<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip---546">TIP - 546</h2>
<ul>
<li><details>
<summary>
(2022a). GraphReg: Dynamical point cloud registration with
geometry-aware graph signal processing. <em>TIP</em>, <em>31</em>,
7449–7464. (<a href="https://doi.org/10.1109/TIP.2022.3223793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a high-accuracy, efficient, and physically induced method for 3D point cloud registration, which is the core of many important 3D vision problems. In contrast to existing physics-based methods that merely consider spatial point information and ignore surface geometry, we explore geometry aware rigid-body dynamics to regulate the particle (point) motion, which results in more precise and robust registration. Our proposed method consists of four major modules. First, we leverage the graph signal processing (GSP) framework to define a new signature, i.e., point response intensity for each point, by which we succeed in describing the local surface variation, resampling keypoints, and distinguishing different particles. Then, to address the shortcomings of current physics-based approaches that are sensitive to outliers, we accommodate the defined point response intensity to median absolute deviation (MAD) in robust statistics and adopt the X84 principle for adaptive outlier depression, ensuring a robust and stable registration. Subsequently, we propose a novel geometric invariant under rigid transformations to incorporate higher-order features of point clouds, which is further embedded for force modeling to guide the correspondence between pairwise scans credibly. Finally, we introduce an adaptive simulated annealing (ASA) method to search for the global optimum and substantially accelerate the registration process. We perform comprehensive experiments to evaluate the proposed method on various datasets captured from range scanners to LiDAR. Results demonstrate that our proposed method outperforms representative state-of-the-art approaches in terms of accuracy and is more suitable for registering large-scale point clouds. Furthermore, it is considerably faster and more robust than most competitors. Our implementation is publicly available at https://github.com/zikai1/GraphReg .},
  archive      = {J_TIP},
  author       = {Mingyang Zhao and Lei Ma and Xiaohong Jia and Dong-Ming Yan and Tiejun Huang},
  doi          = {10.1109/TIP.2022.3223793},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7449-7464},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GraphReg: Dynamical point cloud registration with geometry-aware graph signal processing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advanced scalability for light field image coding.
<em>TIP</em>, <em>31</em>, 7435–7448. (<a
href="https://doi.org/10.1109/TIP.2022.3223787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging, which captures both spatial and angular information, improves user immersion by enabling post-capture actions, such as refocusing and changing view perspective. However, light fields represent very large volumes of data with a lot of redundancy that coding methods try to remove. State-of-the-art coding methods indeed usually focus on improving compression efficiency and overlook other important features in light field compression such as scalability. In this paper, we propose a novel light field image compression method that enables (i) viewport scalability, (ii) quality scalability, (iii) spatial scalability, (iv) random access, and (v) uniform quality distribution among viewports, while keeping compression efficiency high. To this end, light fields in each spatial resolution are divided into sequential viewport layers, and viewports in each layer are encoded using the previously encoded viewports. In each viewport layer, the available viewports are used to synthesize intermediate viewports using a video interpolation deep learning network. The synthesized views are used as virtual reference images to enhance the quality of intermediate views. An image super-resolution method is applied to improve the quality of the lower spatial resolution layer. The super-resolved images are also used as virtual reference images to improve the quality of the higher spatial resolution layer. The proposed structure also improves the flexibility of light field streaming, provides random access to the viewports, and increases error resiliency. The experimental results demonstrate that the proposed method achieves a high compression efficiency and it can adapt to the display type, transmission channel, network condition, processing power, and user needs.},
  archive      = {J_TIP},
  author       = {Hadi Amirpour and Christine Guillemot and Mohammad Ghanbari and Christian Timmerer},
  doi          = {10.1109/TIP.2022.3223787},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7435-7448},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advanced scalability for light field image coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep bilateral filtering network for point-supervised
semantic segmentation in remote sensing images. <em>TIP</em>,
<em>31</em>, 7419–7434. (<a
href="https://doi.org/10.1109/TIP.2022.3222904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation methods based on deep neural networks have achieved great success in recent years. However, training such deep neural networks relies heavily on a large number of images with accurate pixel-level labels, which requires a huge amount of human effort, especially for large-scale remote sensing images. In this paper, we propose a point-based weakly supervised learning framework called the deep bilateral filtering network (DBFNet) for the semantic segmentation of remote sensing images. Compared with pixel-level labels, point annotations are usually sparse and cannot reveal the complete structure of the objects; they also lack boundary information, thus resulting in incomplete prediction within the object and the loss of object boundaries. To address these problems, we incorporate the bilateral filtering technique into deeply learned representations in two respects. First, since a target object contains smooth regions that always belong to the same category, we perform deep bilateral filtering (DBF) to filter the deep features by a nonlinear combination of nearby feature values, which encourages the nearby and similar features to become closer, thus achieving a consistent prediction in the smooth region. In addition, the DBF can distinguish the boundary by enlarging the distance between the features on different sides of the edge, thus preserving the boundary information well. Experimental results on two widely used datasets, the ISPRS 2-D semantic labeling Potsdam and Vaihingen datasets, demonstrate that our proposed DBFNet can achieve a highly competitive performance compared with state-of-the-art fully-supervised methods. Code is available at https://github.com/Luffy03/DBFNet .},
  archive      = {J_TIP},
  author       = {Linshan Wu and Leyuan Fang and Jun Yue and Bob Zhang and Pedram Ghamisi and Min He},
  doi          = {10.1109/TIP.2022.3222904},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7419-7434},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep bilateral filtering network for point-supervised semantic segmentation in remote sensing images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Cluster alignment with target knowledge mining for
unsupervised domain adaptation semantic segmentation. <em>TIP</em>,
<em>31</em>, 7403–7418. (<a
href="https://doi.org/10.1109/TIP.2022.3222634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) carries out knowledge transfer from the labeled source domain to the unlabeled target domain. Existing feature alignment methods in UDA semantic segmentation achieve this goal by aligning the feature distribution between domains. However, these feature alignment methods ignore the domain-specific knowledge of the target domain. In consequence, 1) the correlation among pixels of the target domain is not explored; and 2) the classifier is not explicitly designed for the target domain distribution. To conquer these obstacles, we propose a novel cluster alignment framework, which mines the domain-specific knowledge when performing the alignment. Specifically, we design a multi-prototype clustering strategy to make the pixel features within the same class tightly distributed for the target domain. Subsequently, a contrastive strategy is developed to align the distributions between domains, with the clustered structure maintained. After that, a novel affinity-based normalized cut loss is devised to learn task-specific decision boundaries. Our method enhances the model’s adaptability in the target domain, and can be used as a pre-adaptation for self-training to boost its performance. Sufficient experiments prove the effectiveness of our method against existing state-of-the-art methods on representative UDA benchmarks.},
  archive      = {J_TIP},
  author       = {Shuang Wang and Dong Zhao and Chi Zhang and Yuwei Guo and Qi Zang and Yu Gu and Yi Li and Licheng Jiao},
  doi          = {10.1109/TIP.2022.3222634},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7403-7418},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cluster alignment with target knowledge mining for unsupervised domain adaptation semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). PUFA-GAN: A frequency-aware generative adversarial network
for 3D point cloud upsampling. <em>TIP</em>, <em>31</em>, 7389–7402. (<a
href="https://doi.org/10.1109/TIP.2022.3222918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generative adversarial network for point cloud upsampling, which can not only make the upsampled points evenly distributed on the underlying surface but also efficiently generate clean high frequency regions. The generator of our network includes a dynamic graph hierarchical residual aggregation unit and a hierarchical residual aggregation unit for point feature extraction and upsampling, respectively. The former extracts multiscale point-wise descriptive features, while the latter captures rich feature details with hierarchical residuals. To generate neat edges, our discriminator uses a graph filter to extract and retain high frequency points. The generated high resolution point cloud and corresponding high frequency points help the discriminator learn the global and high frequency properties of the point cloud. We also propose an identity distribution loss function to make sure that the upsampled points remain on the underlying surface of the input low resolution point cloud. To assess the regularity of the upsampled points in high frequency regions, we introduce two evaluation metrics. Objective and subjective results demonstrate that the visual quality of the upsampled point clouds generated by our method is better than that of the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Hao Liu and Hui Yuan and Junhui Hou and Raouf Hamzaoui and Wei Gao},
  doi          = {10.1109/TIP.2022.3222918},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7389-7402},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PUFA-GAN: A frequency-aware generative adversarial network for 3D point cloud upsampling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised learning for textbook question answering.
<em>TIP</em>, <em>31</em>, 7378–7388. (<a
href="https://doi.org/10.1109/TIP.2022.3180563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook Question Answering (TQA) is the task of answering diagram and non-diagram questions given large multi-modal contexts consisting of abundant text and diagrams. Deep text understandings and effective learning of diagram semantics are important for this task due to its specificity. In this paper, we propose a Weakly Supervised learning method for TQA (WSTQ), which regards the incompletely accurate results of essential intermediate procedures for this task as supervision to develop Text Matching (TM) and Relation Detection (RD) tasks and then employs the tasks to motivate itself to learn strong text comprehension and excellent diagram semantics respectively. Specifically, we apply the result of text retrieval to build positive as well as negative text pairs. In order to learn deep text understandings, we first pre-train the text understanding module of WSTQ on TM and then fine-tune it on TQA. We build positive as well as negative relation pairs by checking whether there is any overlap between the items/regions detected from diagrams using object detection. The RD task forces our method to learn the relationships between regions, which are crucial to express the diagram semantics. We train WSTQ on RD and TQA simultaneously, i.e., multitask learning, to obtain effective diagram semantics and then improve the TQA performance. Extensive experiments are carried out on CK12-QA and AI2D to verify the effectiveness of WSTQ. Experimental results show that our method achieves significant accuracy improvements of 5.02\% and 4.12\% on test splits of the above datasets respectively than the current state-of-the-art baseline. We have released our code on https://github.com/dr-majie/WSTQ .},
  archive      = {J_TIP},
  author       = {Jie Ma and Qi Chai and Jingyue Huang and Jun Liu and Yang You and Qinghua Zheng},
  doi          = {10.1109/TIP.2022.3180563},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7378-7388},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly supervised learning for textbook question answering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compact representation and reliable classification learning
for point-level weakly-supervised action localization. <em>TIP</em>,
<em>31</em>, 7363–7377. (<a
href="https://doi.org/10.1109/TIP.2022.3222623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-level weakly-supervised temporal action localization (P-WSTAL) aims to localize temporal extents of action instances and identify the corresponding categories with only a single point label for each action instance for training. Due to the sparse frame-level annotations, most existing models are in the localization-by-classification pipeline. However, there exist two major issues in this pipeline: large intra-action variation due to task gap between classification and localization and noisy classification learning caused by unreliable pseudo training samples. In this paper, we propose a novel framework CRRC-Net, which introduces a co-supervised feature learning module and a probabilistic pseudo label mining module, to simultaneously address the above two issues. Specifically, the co-supervised feature learning module is applied to exploit the complementary information in different modalities for learning more compact feature representations. Furthermore, the probabilistic pseudo label mining module utilizes the feature distances from action prototypes to estimate the likelihood of pseudo samples and rectify their corresponding labels for more reliable classification learning. Comprehensive experiments are conducted on different benchmarks and the experimental results show that our method achieves favorable performance with the state-of-the-art.},
  archive      = {J_TIP},
  author       = {Jie Fu and Junyu Gao and Changsheng Xu},
  doi          = {10.1109/TIP.2022.3222623},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7363-7377},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Compact representation and reliable classification learning for point-level weakly-supervised action localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint learning of salient object detection, depth estimation
and contour extraction. <em>TIP</em>, <em>31</em>, 7350–7362. (<a
href="https://doi.org/10.1109/TIP.2022.3222641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from color independence, illumination invariance and location discrimination attributed by the depth map, it can provide important supplemental information for extracting salient objects in complex environments. However, high-quality depth sensors are expensive and can not be widely applied. While general depth sensors produce the noisy and sparse depth information, which brings the depth-based networks with irreversible interference. In this paper, we propose a novel multi-task and multi-modal filtered transformer (MMFT) network for RGB-D salient object detection (SOD). Specifically, we unify three complementary tasks: depth estimation, salient object detection and contour estimation. The multi-task mechanism promotes the model to learn the task-aware features from the auxiliary tasks. In this way, the depth information can be completed and purified. Moreover, we introduce a multi-modal filtered transformer (MFT) module, which equips with three modality-specific filters to generate the transformer-enhanced feature for each modality. The proposed model works in a depth-free style during the testing phase. Experiments show that it not only significantly surpasses the depth-based RGB-D SOD methods on multiple datasets, but also precisely predicts a high-quality depth map and salient contour at the same time. And, the resulted depth map can help existing RGB-D SOD methods obtain significant performance gain.},
  archive      = {J_TIP},
  author       = {Xiaoqi Zhao and Youwei Pang and Lihe Zhang and Huchuan Lu},
  doi          = {10.1109/TIP.2022.3222641},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7350-7362},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint learning of salient object detection, depth estimation and contour extraction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crafting adversarial perturbations via transformed image
component swapping. <em>TIP</em>, <em>31</em>, 7338–7349. (<a
href="https://doi.org/10.1109/TIP.2022.3204206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks have been demonstrated to fool the deep classification networks. There are two key characteristics of these attacks: firstly, these perturbations are mostly additive noises carefully crafted from the deep neural network itself. Secondly, the noises are added to the whole image, not considering them as the combination of multiple components from which they are made. Motivated by these observations, in this research, we first study the role of various image components and the impact of these components on the classification of the images. These manipulations do not require the knowledge of the networks and external noise to function effectively and hence have the potential to be one of the most practical options for real-world attacks. Based on the significance of the particular image components, we also propose a transferable adversarial attack against unseen deep networks. The proposed attack utilizes the projected gradient descent strategy to add the adversarial perturbation to the manipulated component image. The experiments are conducted on a wide range of networks and four databases including ImageNet and CIFAR-100. The experiments show that the proposed attack achieved better transferability and hence gives an upper hand to an attacker. On the ImageNet database, the success rate of the proposed attack is up to 88.5\%, while the current state-of-the-art attack success rate on the database is 53.8\%. We have further tested the resiliency of the attack against one of the most successful defenses namely adversarial training to measure its strength. The comparison with several challenging attacks shows that: (i) the proposed attack has a higher transferability rate against multiple unseen networks and (ii) it is hard to mitigate its impact. We claim that based on the understanding of the image components, the proposed research has been able to identify a newer adversarial attack unseen so far and unsolvable using the current defense mechanisms.},
  archive      = {J_TIP},
  author       = {Akshay Agarwal and Nalini Ratha and Mayank Vatsa and Richa Singh},
  doi          = {10.1109/TIP.2022.3204206},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7338-7349},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Crafting adversarial perturbations via transformed image component swapping},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention regularized laplace graph for domain adaptation.
<em>TIP</em>, <em>31</em>, 7322–7337. (<a
href="https://doi.org/10.1109/TIP.2022.3216781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In leveraging manifold learning in domain adaptation (DA), graph embedding-based DA methods have shown their effectiveness in preserving data manifold through the Laplace graph. However, current graph embedding DA methods suffer from two issues: 1). they are only concerned with preservation of the underlying data structures in the embedding and ignore sub-domain adaptation, which requires taking into account intra-class similarity and inter-class dissimilarity, thereby leading to negative transfer; 2). manifold learning is proposed across different feature/label spaces separately, thereby hindering unified comprehensive manifold learning. In this paper, starting from our previous DGA-DA, we propose a novel DA method, namely ${A}$ ttention ${R}$ egularized Laplace ${G}$ raph-based ${D}$ omain ${A}$ daptation (ARG-DA), to remedy the aforementioned issues. Specifically, by weighting the importance across different sub-domain adaptation tasks, we propose the ${A}$ ttention ${R}$ egularized Laplace Graph for class aware DA, thereby generating the attention regularized DA. Furthermore, using a specifically designed FEEL strategy, our approach dynamically unifies alignment of the manifold structures across different feature/label spaces, thus leading to comprehensive manifold learning. Comprehensive experiments are carried out to verify the effectiveness of the proposed DA method, which consistently outperforms the state of the art DA methods on 7 standard DA benchmarks, i.e., 37 cross-domain image classification tasks including object, face, and digit images. An in-depth analysis of the proposed DA method is also discussed, including sensitivity, convergence, and robustness.},
  archive      = {J_TIP},
  author       = {Lingkun Luo and Liming Chen and Shiqiang Hu},
  doi          = {10.1109/TIP.2022.3216781},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7322-7337},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention regularized laplace graph for domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MFNet: A novel GNN-based multi-level feature network with
superpixel priors. <em>TIP</em>, <em>31</em>, 7306–7321. (<a
href="https://doi.org/10.1109/TIP.2022.3220057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the superpixel segmentation method aggregates pixels based on similarity, the boundaries of some superpixels indicate the outline of the object and the superpixels provide prerequisites for learning structural-aware features. It is worthwhile to research how to utilize these superpixel priors effectively. In this work, by constructing the graph within superpixel and the graph among superpixels, we propose a novel Multi-level Feature Network (MFNet) based on graph neural network with the above superpixel priors. In our MFNet, we learn three-level features in a hierarchical way: from pixel-level feature to superpixel-level feature, and then to image-level feature. To solve the problem that the existing methods cannot represent superpixels well, we propose a superpixel representation method based on graph neural network, which takes the graph constructed by a single superpixel as input to extract the feature of the superpixel. To reflect the versatility of our MFNet, we apply it to an image-level prediction task and a pixel-level prediction task by designing different prediction modules. An attention linear classifier prediction module is proposed for image-level prediction tasks, such as image classification. An FC-based superpixel prediction module and a Decoder-based pixel prediction module are proposed for pixel-level prediction tasks, such as salient object detection. Our MFNet achieves competitive results on a number of datasets when compared with related methods. The visualization shows that the object boundaries and outline of the saliency maps predicted by our proposed MFNet are more refined and pay more attention to details.},
  archive      = {J_TIP},
  author       = {Shuo Li and Fang Liu and Licheng Jiao and Puhua Chen and Xu Liu and Lingling Li},
  doi          = {10.1109/TIP.2022.3220057},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7306-7321},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MFNet: A novel GNN-based multi-level feature network with superpixel priors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a linear gromov–wasserstein distance. <em>TIP</em>,
<em>31</em>, 7292–7305. (<a
href="https://doi.org/10.1109/TIP.2022.3221286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gromov–Wasserstein distances are generalization of Wasserstein distances, which are invariant under distance preserving transformations. Although a simplified version of optimal transport in Wasserstein spaces, called linear optimal transport (LOT), was successfully used in practice, there does not exist a notion of linear Gromov–Wasserstein distances so far. In this paper, we propose a definition of linear Gromov–Wasserstein distances. We motivate our approach by a generalized LOT model, which is based on barycentric projection maps of transport plans. Numerical examples illustrate that the linear Gromov–Wasserstein distances, similarly as LOT, can replace the expensive computation of pairwise Gromov–Wasserstein distances in applications like shape classification.},
  archive      = {J_TIP},
  author       = {Florian Beier and Robert Beinert and Gabriele Steidl},
  doi          = {10.1109/TIP.2022.3221286},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7292-7305},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {On a linear Gromov–Wasserstein distance},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal collaborative module for efficient action
recognition. <em>TIP</em>, <em>31</em>, 7279–7291. (<a
href="https://doi.org/10.1109/TIP.2022.3221292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient action recognition aims to classify a video clip into a specific action category with a low computational cost. It is challenging since the integrated spatial-temporal calculation (e. g., 3D convolution) introduces intensive operations and increases complexity. This paper explores the feasibility of the integration of channel splitting and filter decoupling for efficient architecture design and feature refinement by proposing a novel spatio-temporal collaborative (STC) module. STC splits the video feature channels into two groups and separately learns spatio-temporal representations in parallel with decoupled convolutional operators. Particularly, STC consists of two computation-efficient blocks, i.e., $\text {S}_{\mathrm{ T}}$ and $\text {T}_{\mathrm{ S}}$ , where they extract either spatial ( ${S}_{\cdot }$ ) or temporal ( ${T}_{\cdot }$ ) features and further refine their features with either temporal ( $\cdot _{T}$ ) or spatial ( $\cdot _{S}$ ) contexts globally. The spatial/temporal context refers to information dynamics aggregated from temporal/spatial axis. To thoroughly examine our method’s performance in video action recognition tasks, we conduct extensive experiments using five video benchmark datasets requiring temporal reasoning. Experimental results show that the proposed STC networks achieve a competitive trade-off between model efficiency and effectiveness.},
  archive      = {J_TIP},
  author       = {Yanbin Hao and Shuo Wang and Yi Tan and Xiangnan He and Zhenguang Liu and Meng Wang},
  doi          = {10.1109/TIP.2022.3221292},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7279-7291},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatio-temporal collaborative module for efficient action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPICE: Semantic pseudo-labeling for image clustering.
<em>TIP</em>, <em>31</em>, 7264–7278. (<a
href="https://doi.org/10.1109/TIP.2022.3221290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The similarity among samples and the discrepancy among clusters are two crucial aspects of image clustering. However, current deep clustering methods suffer from inaccurate estimation of either feature similarity or semantic discrepancy. In this paper, we present a Semantic Pseudo-labeling-based Image ClustEring (SPICE) framework, which divides the clustering network into a feature model for measuring the instance-level similarity and a clustering head for identifying the cluster-level discrepancy. We design two semantics-aware pseudo-labeling algorithms, prototype pseudo-labeling and reliable pseudo-labeling, which enable accurate and reliable self-supervision over clustering. Without using any ground-truth label, we optimize the clustering network in three stages: 1) train the feature model through contrastive learning to measure the instance similarity; 2) train the clustering head with the prototype pseudo-labeling algorithm to identify cluster semantics; and 3) jointly train the feature model and clustering head with the reliable pseudo-labeling algorithm to improve the clustering performance. Extensive experimental results demonstrate that SPICE achieves significant improvements (~10\%) over existing methods and establishes the new state-of-the-art clustering results on six balanced benchmark datasets in terms of three popular metrics. Importantly, SPICE significantly reduces the gap between unsupervised and fully-supervised classification; e.g. there is only 2\% (91.8\% vs 93.8\%) accuracy difference on CIFAR-10. Our code is made publicly available at https://github.com/niuchuangnn/SPICE .},
  archive      = {J_TIP},
  author       = {Chuang Niu and Hongming Shan and Ge Wang},
  doi          = {10.1109/TIP.2022.3221290},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7264-7278},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SPICE: Semantic pseudo-labeling for image clustering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-stage approach toward hyperspectral image
super-resolution. <em>TIP</em>, <em>31</em>, 7252–7263. (<a
href="https://doi.org/10.1109/TIP.2022.3221287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image produces high spectral resolution at the sacrifice of spatial resolution. Without reducing the spectral resolution, improving the resolution in the spatial domain is a very challenging problem. Motivated by the discovery that hyperspectral image exhibits high similarity between adjacent bands in a large spectral range, in this paper, we explore a new structure for hyperspectral image super-resolution (DualSR), leading to a dual-stage design, i.e., coarse stage and fine stage. In coarse stage, five bands with high similarity in a certain spectral range are divided into three groups, and the current band is guided to study the potential knowledge. Under the action of alternative spectral fusion mechanism, the coarse SR image is super-resolved in band-by-band. In order to build model from a global perspective, an enhanced back-projection method via spectral angle constraint is developed in fine stage to learn the content of spatial-spectral consistency, dramatically improving the performance gain. Extensive experiments demonstrate the effectiveness of the proposed coarse stage and fine stage. Besides, our network produces state-of-the-art results against existing works in terms of spatial reconstruction and spectral fidelity. Our code is publicly available at https://github.com/qianngli/DualSR .},
  archive      = {J_TIP},
  author       = {Qiang Li and Yuan Yuan and Xiuping Jia and Qi Wang},
  doi          = {10.1109/TIP.2022.3221287},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7252-7263},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-stage approach toward hyperspectral image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning dense and continuous optical flow from an event
camera. <em>TIP</em>, <em>31</em>, 7237–7251. (<a
href="https://doi.org/10.1109/TIP.2022.3220938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras such as DAVIS can simultaneously output high temporal resolution events and low frame-rate intensity images, which own great potential in capturing scene motion, such as optical flow estimation. Most of the existing optical flow estimation methods are based on two consecutive image frames and can only estimate discrete flow at a fixed time interval. Previous work has shown that continuous flow estimation can be achieved by changing the quantities or time intervals of events. However, they are difficult to estimate reliable dense flow, especially in the regions without any triggered events. In this paper, we propose a novel deep learning-based dense and continuous optical flow estimation framework from a single image with event streams, which facilitates the accurate perception of high-speed motion. Specifically, we first propose an event-image fusion and correlation module to effectively exploit the internal motion from two different modalities of data. Then we propose an iterative update network structure with bidirectional training for optical flow prediction. Therefore, our model can estimate reliable dense flow as two-frame-based methods, as well as estimate temporal continuous flow as event-based methods. Extensive experimental results on both synthetic and real captured datasets demonstrate that our model outperforms existing event-based state-of-the-art methods and our designed baselines for accurate dense and continuous optical flow estimation.},
  archive      = {J_TIP},
  author       = {Zhexiong Wan and Yuchao Dai and Yuxin Mao},
  doi          = {10.1109/TIP.2022.3220938},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7237-7251},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning dense and continuous optical flow from an event camera},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Deformable wiener filter for future video coding.
<em>TIP</em>, <em>31</em>, 7222–7236. (<a
href="https://doi.org/10.1109/TIP.2022.3221278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-loop filters have attracted increasing attention due to the remarkable noise-reduction capability in the hybrid video coding framework. However, the existing in-loop filters in Versatile Video Coding (VVC) mainly take advantage of the image local similarity. Although some non-local based in-loop filters can make up for this shortcoming, the widely-used unsupervised parameter estimation method by non-local filters limits the performance. In view of this, we propose a deformable Wiener Filter (DWF). It combines the local and non-local characteristics and supervisedly trains the filter coefficients based on the Wiener Filter theory. In the filtering process, local adjacent samples and non-local similar samples are first derived for each sample of interest. Then the to-be-filtered samples are classified into specific groups based on the patch-level noise and sample-level characteristics. Samples in each group share the same filter coefficients. After that, the local and non-local reference samples are adaptively fused based on the classification results. Finally, the filtering operation with outlier data constraints is conducted for each to-be-filtered sample. Moreover, the performance of the proposed DWF is analyzed with different reference sample derivation schemes in detail. Simulation results show that the proposed approach achieves 1.16\%, 1.92\%, and 2.67\% bit-rate savings on average compared to the VTM-11.0 for All Intra, Random Access, and Low-Delay B configurations, respectively.},
  archive      = {J_TIP},
  author       = {Xuewei Meng and Chuanmin Jia and Xinfeng Zhang and Shanshe Wang and Siwei Ma},
  doi          = {10.1109/TIP.2022.3221278},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7222-7236},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deformable wiener filter for future video coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confusing image quality assessment: Toward better augmented
reality experience. <em>TIP</em>, <em>31</em>, 7206–7221. (<a
href="https://doi.org/10.1109/TIP.2022.3220404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multimedia technology, Augmented Reality (AR) has become a promising next-generation mobile platform. The primary value of AR is to promote the fusion of digital contents and real-world environments, however, studies on how this fusion will influence the Quality of Experience (QoE) of these two components are lacking. To achieve better QoE of AR, whose two layers are influenced by each other, it is important to evaluate its perceptual quality first. In this paper, we consider AR technology as the superimposition of virtual scenes and real scenes, and introduce visual confusion as its basic theory. A more general problem is first proposed, which is evaluating the perceptual quality of superimposed images, i.e., confusing image quality assessment. A ConFusing Image Quality Assessment (CFIQA) database is established, which includes 600 reference images and 300 distorted images generated by mixing reference images in pairs. Then a subjective quality perception experiment is conducted towards attaining a better understanding of how humans perceive the confusing images. Based on the CFIQA database, several benchmark models and a specifically designed CFIQA model are proposed for solving this problem. Experimental results show that the proposed CFIQA model achieves state-of-the-art performance compared to other benchmark models. Moreover, an extended ARIQA study is further conducted based on the CFIQA study. We establish an ARIQA database to better simulate the real AR application scenarios, which contains 20 AR reference images, 20 background (BG) reference images, and 560 distorted images generated from AR and BG references, as well as the correspondingly collected subjective quality ratings. Three types of full-reference (FR) IQA benchmark variants are designed to study whether we should consider the visual confusion when designing corresponding IQA algorithms. An ARIQA metric is finally proposed for better evaluating the perceptual quality of AR images. Experimental results demonstrate the good generalization ability of the CFIQA model and the state-of-the-art performance of the ARIQA model. The databases, benchmark models, and proposed metrics are available at: https://github.com/DuanHuiyu/ARIQA .},
  archive      = {J_TIP},
  author       = {Huiyu Duan and Xiongkuo Min and Yucheng Zhu and Guangtao Zhai and Xiaokang Yang and Patrick Le Callet},
  doi          = {10.1109/TIP.2022.3220404},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7206-7221},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Confusing image quality assessment: Toward better augmented reality experience},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-attribute subspace clustering via auto-weighted tensor
nuclear norm minimization. <em>TIP</em>, <em>31</em>, 7191–7205. (<a
href="https://doi.org/10.1109/TIP.2022.3220949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-expressiveness based subspace clustering methods have received wide attention for unsupervised learning tasks. However, most existing subspace clustering methods consider data features as a whole and then focus only on one single self-representation. These approaches ignore the intrinsic multi-attribute information embedded in the original data feature and result in one–attribute self-representation. This paper proposes a novel multi-attribute subspace clustering (MASC) model that understands data from multiple attributes. MASC simultaneously learns multiple subspace representations corresponding to each specific attribute by exploiting the intrinsic multi-attribute features drawn from original data. In order to better capture the high-order correlation among multi-attribute representations, we represent them as a tensor in low-rank structure and propose the auto-weighted tensor nuclear norm (AWTNN) as a superior low-rank tensor approximation. Especially, the non-convex AWTNN fully considers the difference between singular values through the implicit and adaptive weights splitting during the AWTNN optimization procedure. We further develop an efficient algorithm to optimize the non-convex and multi-block MASC model and establish the convergence guarantees. A more comprehensive subspace representation can be obtained via aggregating these multi-attribute representations, which can be used to construct a clustering-friendly affinity matrix. Extensive experiments on eight real-world databases reveal that the proposed MASC exhibits superior performance over other subspace clustering methods.},
  archive      = {J_TIP},
  author       = {Jipeng Guo and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TIP.2022.3220949},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7191-7205},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-attribute subspace clustering via auto-weighted tensor nuclear norm minimization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised domain adaptive structure learning.
<em>TIP</em>, <em>31</em>, 7179–7190. (<a
href="https://doi.org/10.1109/TIP.2022.3215889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised domain adaptation (SSDA) is quite a challenging problem requiring methods to overcome both 1) overfitting towards poorly annotated data and 2) distribution shift across domains. Unfortunately, a simple combination of domain adaptation (DA) and semi-supervised learning (SSL) methods often fail to address such two objects because of training data bias towards labeled samples. In this paper, we introduce an adaptive structure learning method to regularize the cooperation of SSL and DA. Inspired by the multi-views learning, our proposed framework is composed of a shared feature encoder network and two classifier networks, trained for contradictory purposes. Among them, one of the classifiers is applied to group target features to improve intra-class density, enlarging the gap of categorical clusters for robust representation learning. Meanwhile, the other classifier, serviced as a regularizer, attempts to scatter the source features to enhance the smoothness of the decision boundary. The iterations of target clustering and source expansion make the target features being well-enclosed inside the dilated boundary of the corresponding source points. For the joint address of cross-domain features alignment and partially labeled data learning, we apply the maximum mean discrepancy (MMD) distance minimization and self-training (ST) to project the contradictory structures into a shared view to make the reliable final decision. The experimental results over the standard SSDA benchmarks, including DomainNet and Office-home, demonstrate both the accuracy and robustness of our method over the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Can Qin and Lichen Wang and Qianqian Ma and Yu Yin and Huan Wang and Yun Fu},
  doi          = {10.1109/TIP.2022.3215889},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7179-7190},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised domain adaptive structure learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Visible-infrared person re-identification with
modality-specific memory network. <em>TIP</em>, <em>31</em>, 7165–7178.
(<a href="https://doi.org/10.1109/TIP.2022.3220408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is challenging due to the large modality discrepancy between visible and infrared images. Existing methods mainly focus on learning modality-shared representations by embedding images from different modalities into a common feature space, in which some discriminative modality information is discarded. Different from these methods, in this paper, we propose a novel Modality-Specific Memory Network (MSMNet) to complete the missing modality information and aggregate visible and infrared modality features into a unified feature space for the VI-ReID task. The proposed model enjoys several merits. First, it can exploit the missing modality information to alleviate the modality discrepancy when only the single-modality input is provided. To the best of our knowledge, this is the first work to exploit the missing modality information completion and alleviate the modality discrepancy with the memory network. Second, to guide the learning process of the memory network, we design three effective learning strategies, including feature consistency, memory representativeness and structural alignment. By incorporating these learning strategies in a unified model, the memory network can be well learned to propagate identity-related information between modalities and boost the VI-ReID performance. Extensive experimental results on two standard benchmarks (SYSU-MM01 and RegDB) demonstrate that the proposed MSMNet performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yulin Li and Tianzhu Zhang and Xiang Liu and Qi Tian and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TIP.2022.3220408},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7165-7178},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visible-infrared person re-identification with modality-specific memory network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent space semantic supervision based on knowledge
distillation for cross-modal retrieval. <em>TIP</em>, <em>31</em>,
7154–7164. (<a href="https://doi.org/10.1109/TIP.2022.3220051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important field in information retrieval, fine-grained cross-modal retrieval has received great attentions from researchers. Existing fine-grained cross-modal retrieval methods made several improvements in capturing the fine-grained interplay between vision and language, failing to consider the fine-grained correspondences between the features in the image latent space and the text latent space respectively, which may lead to inaccurate inference of intra-modal relations or false alignment of cross-modal information. Considering that object detection can get the fine-grained correspondences of image region features and the corresponding semantic features, this paper proposed a novel latent space semantic supervision model based on knowledge distillation (L3S-KD), which trains classifiers supervised by the fine-grained correspondences obtained from an object detection model by using knowledge distillation for image latent space fine-grained alignment, and by the labels of objects and attributes for text latent space fine-grained alignment. Compared with existing fine-grained correspondence matching methods, L3S-KD can learn more accurate semantic similarities for local fragments in image-text pairs. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that the L3S-KD model consistently outperforms state-of-the-art methods for image-text matching.},
  archive      = {J_TIP},
  author       = {Li Zhang and Xiangqian Wu},
  doi          = {10.1109/TIP.2022.3220051},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7154-7164},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Latent space semantic supervision based on knowledge distillation for cross-modal retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quasi-equilibrium feature pyramid network for salient object
detection. <em>TIP</em>, <em>31</em>, 7144–7153. (<a
href="https://doi.org/10.1109/TIP.2022.3220058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern saliency detection models are based on the encoder-decoder framework and they use different strategies to fuse the multi-level features between the encoder and decoder to boost representation power. Motivated by recent work in implicit modelling, we propose to introduce an implicit function to simulate the equilibrium state of the feature pyramid at infinite depths. We question the existence of the ideal equilibrium and thus propose a quasi-equilibrium model by taking the first-order derivative into the black-box root solver using Taylor expansion. It models more realistic convergence states and significantly improves the network performance. We also propose a differentiable edge extractor that directly extracts edges from the saliency masks. By optimizing the extracted edges, the generated saliency masks are naturally optimized on contour constraints and the non-deterministic predictions are removed. We evaluate the proposed methodology on five public datasets and extensive experiments show that our method achieves new state-of-the-art performances on six metrics across datasets.},
  archive      = {J_TIP},
  author       = {Yue Song and Hao Tang and Mengyi Zhao and Nicu Sebe and Wei Wang},
  doi          = {10.1109/TIP.2022.3220058},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7144-7153},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quasi-equilibrium feature pyramid network for salient object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial transformers for weakly supervised object
localization. <em>TIP</em>, <em>31</em>, 7130–7143. (<a
href="https://doi.org/10.1109/TIP.2022.3220055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL) aims at localizing objects with only image-level labels, which has better scalability and practicability than fully supervised methods. However, without pixel-level supervision, existing methods tend to generate rough localization maps, which hinders localization performance. To alleviate this problem, we propose an adversarial transformer network (ATNet), which aims to obtain a well-learned localization model with pixel-level pseudo labels. The proposed ATNet enjoys several merits. First, we design an object transformer ( $G$ ) that can generate localization maps and pseudo labels effectively and dynamically, and a part transformer ( $D$ ) to accurately discriminate detailed local differences between localization maps and pseudo labels. Second, we propose to train $G$ and $D$ via an adversarial process, where $G$ can generate more accurate localization maps approaching pseudo labels to fool $D$ . To the best of our knowledge, this is the first work to explore transformers with adversarial training to obtain a well-learned localization model for WSOL. Extensive experiments with four backbones on two standard benchmarks demonstrate that our ATNet achieves favorable performance against state-of-the-art WSOL methods. Besides, our adversarial training can provide higher robustness against adversarial attacks.},
  archive      = {J_TIP},
  author       = {Meng Meng and Tianzhu Zhang and Zhe Zhang and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TIP.2022.3220055},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7130-7143},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial transformers for weakly supervised object localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SiamHYPER: Learning a hyperspectral object tracker from an
RGB-based tracker. <em>TIP</em>, <em>31</em>, 7116–7129. (<a
href="https://doi.org/10.1109/TIP.2022.3216995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral videos can provide the spatial, spectral, and motion information of targets, which makes it possible to track camouflaged targets that are similar to the background. However, hyperspectral object tracking is a challenging task, due to the huge hyperspectral video data dimension and the “data hungry” problem for the model training. Insufficient training data can seriously interfere with the accuracy and generalization of the tracking models. In this paper, a dual deep Siamese network framework for hyperspectral object tracking (SiamHYPER) is proposed for learning a hyperspectral tracker from a pretrained RGB tracker in the case of the “data hungry” problem. Specifically, in addition to a pretrained RGB-based Siamese tracker, a hyperspectral target-aware module is designed to mine the spectral information during the target prediction, and a spatial-spectral cross-attention module is introduced to further fuse the deep spatial and spectral features extracted from the RGB tracker and the hyperspectral target-aware module. Benefiting from the guidance training of the RGB tracker, a robust hyperspectral object tracker can be trained effectively with only a small number of hyperspectral video samples, to overcome the “data hungry” problem. In the experiments conducted in this study, the SiamHYPER framework was verified using SiamBAN and SiamRPN++, with 13 000 frames of hyperspectral videos for training, and achieved the best performance on the publicly available hyperspectral dataset released as part of the WHISPERS Hyperspectral Object Tracking Challenge. The area under the curve (AUC) of SiamHYPER was increased by nearly 8.9\% and 7.2\%, respectively, when compared with the current state-of-the-art RGB-based and hyperspectral trackers. In addition, the processing speed of SiamHYPER was 19 FPS, which is much higher than that of the current state-of-the-art hyperspectral trackers. The source code is available at zhenliuzhenqi/HOT: Hyperspectral object tracking (github.com).},
  archive      = {J_TIP},
  author       = {Zhenqi Liu and Xinyu Wang and Yanfei Zhong and Meng Shu and Chen Sun},
  doi          = {10.1109/TIP.2022.3216995},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7116-7129},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SiamHYPER: Learning a hyperspectral object tracker from an RGB-based tracker},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised synthetic acoustic image generation for
audio-visual scene understanding. <em>TIP</em>, <em>31</em>, 7102–7115.
(<a href="https://doi.org/10.1109/TIP.2022.3219228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic images are an emergent data modality for multimodal scene understanding. Such images have the peculiarity of distinguishing the spectral signature of the sound coming from different directions in space, thus providing a richer information as compared to that derived from single or binaural microphones. However, acoustic images are typically generated by cumbersome and costly microphone arrays which are not as widespread as ordinary microphones. This paper shows that it is still possible to generate acoustic images from off-the-shelf cameras equipped with only a single microphone and how they can be exploited for audio-visual scene understanding. We propose three architectures inspired by Variational Autoencoder, U-Net and adversarial models, and we assess their advantages and drawbacks. Such models are trained to generate spatialized audio by conditioning them to the associated video sequence and its corresponding monaural audio track. Our models are trained using the data collected by a microphone array as ground truth. Thus they learn to mimic the output of an array of microphones in the very same conditions. We assess the quality of the generated acoustic images considering standard generation metrics and different downstream tasks (classification, cross-modal retrieval and sound localization). We also evaluate our proposed models by considering multimodal datasets containing acoustic images, as well as datasets containing just monaural audio signals and RGB video frames. In all of the addressed downstream tasks we obtain notable performances using the generated acoustic data, when compared to the state of the art and to the results obtained using real acoustic images as input.},
  archive      = {J_TIP},
  author       = {Valentina Sanguineti and Pietro Morerio and Alessio Del Bue and Vittorio Murino},
  doi          = {10.1109/TIP.2022.3219228},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7102-7115},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised synthetic acoustic image generation for audio-visual scene understanding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PFDN: Pyramid feature decoupling network for single image
deraining. <em>TIP</em>, <em>31</em>, 7091–7101. (<a
href="https://doi.org/10.1109/TIP.2022.3219227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring images degraded by rain has attracted more academic attention since rain streaks could reduce the visibility of outdoor scenes. However, most existing deraining methods attempt to remove rain while recovering details in a unified framework, which is an ideal and contradictory target in the image deraining task. Moreover, the relative independence of rain streak features and background features is usually ignored in the feature domain. To tackle these challenges above, we propose an effective Pyramid Feature Decoupling Network (i.e., PFDN) for single image deraining, which could accomplish image deraining and details recovery with the corresponding features. Specifically, the input rainy image features are extracted via a recurrent pyramid module, where the features for the rainy image are divided into two parts, i.e., rain-relevant and rain-irrelevant features. Afterwards, we introduce a novel rain streak removal network for rain-relevant features and remove the rain streak from the rainy image by estimating the rain streak information. Benefiting from lateral outputs, we propose an attention module to enhance the rain-irrelevant features, which could generate spatially accurate and contextually reliable details for image recovery. For better disentanglement, we also enforce multiple causality losses at the pyramid features to encourage the decoupling of rain-relevant and rain-irrelevant features from the high to shallow layers. Extensive experiments demonstrate that our module can well model the rain-relevant information over the domain of the feature. Our framework empowered by PFDN modules significantly outperforms the state-of-the-art methods on single image deraining with multiple widely-used benchmarks, and also shows superiority in the fully-supervised domain.},
  archive      = {J_TIP},
  author       = {Qiang Wang and Gan Sun and Jiahua Dong and Yulun Zhang},
  doi          = {10.1109/TIP.2022.3219227},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7091-7101},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PFDN: Pyramid feature decoupling network for single image deraining},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized meta-FDMixup: Cross-domain few-shot learning
guided by labeled target data. <em>TIP</em>, <em>31</em>, 7078–7090. (<a
href="https://doi.org/10.1109/TIP.2022.3219237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vanilla Few-shot Learning (FSL) learns to build a classifier for a new concept from one or very few target examples, with the general assumption that source and target classes are sampled from the same domain. Recently, the task of Cross-Domain Few-Shot Learning (CD-FSL) aims at tackling the FSL where there is a huge domain shift between the source and target datasets. Extensive efforts on CD-FSL have been made via either directly extending the meta-learning paradigm of vanilla FSL methods, or employing massive unlabeled target data to help learn models. In this paper, we notice that in the CD-FSL task, the few labeled target images have never been explicitly leveraged to inform the model in the training stage. However, such a labeled target example set is very important to bridge the huge domain gap. Critically, this paper advocates a more practical training scenario for CD-FSL. And our key insight is to utilize a few labeled target data to guide the learning of the CD-FSL model. Technically, we propose a novel Generalized Meta-learning based Feature-Disentangled Mixup network, namely GMeta-FDMixup. We make three key contributions of utilizing GMeta-FDMixup to address CD-FSL. Firstly, we present two mixup modules – mixup-P and mixup-M that help facilitate utilizing the unbalanced and disjoint source and target datasets. These two novel modules enable diverse image generation for training the model on the source domain. Secondly, to narrow the domain gap explicitly, we contribute a novel feature disentanglement module that learns to decouple the domain-irrelevant and domain-specific features. By stripping the domain-specific features, we alleviate the negative effects caused by the domain inductive bias. Finally, we repurpose a new contrastive learning module, dubbed ConL. ConL prevents the model from only capturing category-related features via introducing contrastive loss. Thus, the generalization ability on novel categories is improved. Extensive experimental results on two benchmarks show the superiority of our setting and the effectiveness of our method. Code and models will be released.},
  archive      = {J_TIP},
  author       = {Yuqian Fu and Yanwei Fu and Jingjing Chen and Yu-Gang Jiang},
  doi          = {10.1109/TIP.2022.3219237},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7078-7090},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized meta-FDMixup: Cross-domain few-shot learning guided by labeled target data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive online mutual learning bi-decoders for video object
segmentation. <em>TIP</em>, <em>31</em>, 7063–7077. (<a
href="https://doi.org/10.1109/TIP.2022.3219230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges facing video object segmentation (VOS) is the gap between the training and test datasets due to unseen category in test set, as well as object appearance change over time in the video sequence. To overcome such challenges, an adaptive online framework for VOS is developed with bi-decoders mutual learning. We learn object representation per pixel with bi-level attention features in addition to CNN features, and then feed them into mutual learning bi-decoders whose outputs are further fused to obtain the final segmentation result. We design an adaptive online learning mechanism via a deviation correcting trigger such that bi-decoders online mutual learning will be activated when the previous frame is segmented well meanwhile the current frame is segmented relatively worse. Knowledge distillation from the well segmented previous frames, along with mutual learning between bi-decoders, improves generalization ability and robustness of VOS model. Thus, the proposed model adapts to the challenging scenarios including unseen categories, object deformation, and appearance variation during inference. We extensively evaluate our model on widely-used VOS benchmarks including DAVIS-2016, DAVIS-2017, YouTubeVOS-2018, YouTubeVOS-2019, and UVO. Experimental results demonstrate the superiority of the proposed model over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Pinxue Guo and Wei Zhang and Xiaoqiang Li and Wenqiang Zhang},
  doi          = {10.1109/TIP.2022.3219230},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7063-7077},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive online mutual learning bi-decoders for video object segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Canonical correlation analysis with low-rank learning for
image representation. <em>TIP</em>, <em>31</em>, 7048–7062. (<a
href="https://doi.org/10.1109/TIP.2022.3219235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a multivariate data analysis tool, canonical correlation analysis (CCA) has been widely used in computer vision and pattern recognition. However, CCA uses Euclidean distance as a metric, which is sensitive to noise or outliers in the data. Furthermore, CCA demands that the two training sets must have the same number of training samples, which limits the performance of CCA-based methods. To overcome these limitations of CCA, two novel canonical correlation learning methods based on low-rank learning are proposed in this paper for image representation, named robust canonical correlation analysis (robust-CCA) and low-rank representation canonical correlation analysis (LRR-CCA). By introducing two regular matrices, the training sample numbers of the two training datasets can be set as any values without any limitation in the two proposed methods. Specifically, robust-CCA uses low-rank learning to remove the noise in the data and extracts the maximization correlation features from the two learned clean data matrices. The nuclear norm and $L_{1}$ -norm are used as constraints for the learned clean matrices and noise matrices, respectively. LRR-CCA introduces low-rank representation into CCA to ensure that the correlative features can be obtained in low-rank representation. To verify the performance of the proposed methods, five publicly image databases are used to conduct extensive experiments. The experimental results demonstrate the proposed methods outperform state-of-the-art CCA-based and low-rank learning methods.},
  archive      = {J_TIP},
  author       = {Yuwu Lu and Wenjing Wang and Biqing Zeng and Zhihui Lai and Linlin Shen and Xuelong Li},
  doi          = {10.1109/TIP.2022.3219235},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7048-7062},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Canonical correlation analysis with low-rank learning for image representation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature aggregation and propagation network for camouflaged
object detection. <em>TIP</em>, <em>31</em>, 7036–7047. (<a
href="https://doi.org/10.1109/TIP.2022.3217695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to detect/segment camouflaged objects embedded in the environment, which has attracted increasing attention over the past decades. Although several COD methods have been developed, they still suffer from unsatisfactory performance due to the intrinsic similarities between the foreground objects and background surroundings. In this paper, we propose a novel Feature Aggregation and Propagation Network (FAP-Net) for camouflaged object detection. Specifically, we propose a Boundary Guidance Module (BGM) to explicitly model the boundary characteristic, which can provide boundary-enhanced features to boost the COD performance. To capture the scale variations of the camouflaged objects, we propose a Multi-scale Feature Aggregation Module (MFAM) to characterize the multi-scale information from each layer and obtain the aggregated feature representations. Furthermore, we propose a Cross-level Fusion and Propagation Module (CFPM). In the CFPM, the feature fusion part can effectively integrate the features from adjacent layers to exploit the cross-level correlations, and the feature propagation part can transmit valuable context information from the encoder to the decoder network via a gate unit. Finally, we formulate a unified and end-to-end trainable framework where cross-level features can be effectively fused and propagated for capturing rich context information. Extensive experiments on three benchmark camouflaged datasets demonstrate that our FAP-Net outperforms other state-of-the-art COD models. Moreover, our model can be extended to the polyp segmentation task, and the comparison results further validate the effectiveness of the proposed model in segmenting polyps. The source code and results will be released at https://github.com/taozh2017/FAPNet .},
  archive      = {J_TIP},
  author       = {Tao Zhou and Yi Zhou and Chen Gong and Jian Yang and Yu Zhang},
  doi          = {10.1109/TIP.2022.3217695},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7036-7047},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Feature aggregation and propagation network for camouflaged object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth map recovery based on a unified depth boundary
distortion model. <em>TIP</em>, <em>31</em>, 7020–7035. (<a
href="https://doi.org/10.1109/TIP.2022.3216768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth maps acquired by either physical sensors or learning methods are often seriously distorted due to boundary distortion problems, including missing, fake, and misaligned boundaries (compared with RGB images). An RGB-guided depth map recovery method is proposed in this paper to recover true boundaries in seriously distorted depth maps. Therefore, a unified model is first developed to observe all these kinds of distorted boundaries in depth maps. Observing distorted boundaries is equivalent to identifying erroneous regions in distorted depth maps, because depth boundaries are essentially formed by contiguous regions with different intensities. Then, erroneous regions are identified by separately extracting local structures of RGB image and depth map with Gaussian kernels and comparing their similarity on the basis of the SSIM index. A depth map recovery method is then proposed on the basis of the unified model. This method recovers true depth boundaries by iteratively identifying and correcting erroneous regions in recovered depth map based on the unified model and a weighted median filter. Because RGB image generally includes additional textural contents compared with depth maps, texture-copy artifacts problem is further addressed in the proposed method by restricting the model works around depth boundaries in each iteration. Extensive experiments are conducted on five RGB–depth datasets including depth map recovery, depth super-resolution, depth estimation enhancement, and depth completion enhancement. The results demonstrate that the proposed method considerably improves both the quantitative and visual qualities of recovered depth maps in comparison with fifteen competitive methods. Most object boundaries in recovered depth maps are corrected accurately, and kept sharply and well aligned with the ones in RGB images.},
  archive      = {J_TIP},
  author       = {Haotian Wang and Meng Yang and Xuguang Lan and Ce Zhu and Nanning Zheng},
  doi          = {10.1109/TIP.2022.3216768},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7020-7035},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Depth map recovery based on a unified depth boundary distortion model},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking the importance of quantization bias, toward full
low-bit training. <em>TIP</em>, <em>31</em>, 7006–7019. (<a
href="https://doi.org/10.1109/TIP.2022.3216776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization is a promising technique to reduce the computation and storage costs of DNNs. Low-bit ( $\leq8$ bits) precision training remains an open problem due to the difficulty of gradient quantization. In this paper, we find two long-standing misunderstandings of the bias of gradient quantization noise. First, the large bias of gradient quantization noise, instead of the variance, is the key factor of training accuracy loss. Second, the widely used stochastic rounding cannot solve the training crash problem caused by the gradient quantization bias in practice. Moreover, we find that the asymmetric distribution of gradients causes a large bias of gradient quantization noise. Based on our findings, we propose a novel adaptive piecewise quantization method to effectively limit the bias of gradient quantization noise. Accordingly, we propose a new data format, Piecewise Fixed Point (PWF), to present data after quantization. We apply our method to different applications including image classification, machine translation, optical character recognition, and text classification. We achieve approximately $1.9\sim 3.5\times $ speedup compared with full precision training with an accuracy loss of less than 0.5\%. To the best of our knowledge, this is the first work to quantize gradients of all layers to 8 bits in both large-scale CNN and RNN training with negligible accuracy loss.},
  archive      = {J_TIP},
  author       = {Chang Liu and Xishan Zhang and Rui Zhang and Ling Li and Shiyi Zhou and Di Huang and Zhen Li and Zidong Du and Shaoli Liu and Tianshi Chen},
  doi          = {10.1109/TIP.2022.3216776},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {7006-7019},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking the importance of quantization bias, toward full low-bit training},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TransCS: A transformer-based hybrid architecture for image
compressed sensing. <em>TIP</em>, <em>31</em>, 6991–7005. (<a
href="https://doi.org/10.1109/TIP.2022.3217365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Well-known compressed sensing (CS) is widely used in image acquisition and reconstruction. However, accurately reconstructing images from measurements at low sampling rates remains a considerable challenge. In this paper, we propose a novel Transformer-based hybrid architecture (dubbed TransCS) to achieve high-quality image CS. In the sampling module, TransCS adopts a trainable sensing matrix strategy that gains better image reconstruction by learning the structural information from the training images. In the reconstruction module, inspired by the powerful long-distance dependence modelling capacity of the Transformer, a customized iterative shrinkage-thresholding algorithm (ISTA)-based Transformer backbone that iteratively works with gradient descent and soft threshold operation is designed to model the global dependency among image subblocks. Moreover, the auxiliary convolutional neural network (CNN) is introduced to capture the local features of images. Therefore, the proposed hybrid architecture that integrates the customized ISTA-based Transformer backbone with CNN can gain high-performance reconstruction for image compressed sensing. The experimental results demonstrate that our proposed TransCS obtains superior reconstruction quality and noise robustness on several public benchmark datasets compared with other state-of-the-art methods. Our code is available on TransCS.},
  archive      = {J_TIP},
  author       = {Minghe Shen and Hongping Gan and Chao Ning and Yi Hua and Tao Zhang},
  doi          = {10.1109/TIP.2022.3217365},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6991-7005},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TransCS: A transformer-based hybrid architecture for image compressed sensing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defending person detection against adversarial patch attack
by using universal defensive frame. <em>TIP</em>, <em>31</em>,
6976–6990. (<a href="https://doi.org/10.1109/TIP.2022.3217375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person detection has attracted great attention in the computer vision area and is an imperative element in human-centric computer vision. Although the predictive performances of person detection networks have been improved dramatically, they are vulnerable to adversarial patch attacks. Changing the pixels in a restricted region can easily fool the person detection network in safety-critical applications such as autonomous driving and security systems. Despite the necessity of countering adversarial patch attacks, very few efforts have been dedicated to defending person detection against adversarial patch attack. In this paper, we propose a novel defense strategy that defends against an adversarial patch attack by optimizing a defensive frame for person detection. The defensive frame alleviates the effect of the adversarial patch while maintaining person detection performance with clean person. The proposed defensive frame in the person detection is generated with a competitive learning algorithm which makes an iterative competition between detection threatening module and detection shielding module in person detection. Comprehensive experimental results demonstrate that the proposed method effectively defends person detection against adversarial patch attacks.},
  archive      = {J_TIP},
  author       = {Youngjoon Yu and Hong Joo Lee and Hakmin Lee and Yong Man Ro},
  doi          = {10.1109/TIP.2022.3217375},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6976-6990},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Defending person detection against adversarial patch attack by using universal defensive frame},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning spectral cues for multispectral and panchromatic
image fusion. <em>TIP</em>, <em>31</em>, 6964–6975. (<a
href="https://doi.org/10.1109/TIP.2022.3215906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yinghui Xing and Shuyuan Yang and Yan Zhang and Yanning Zhang},
  doi          = {10.1109/TIP.2022.3215906},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6964-6975},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning spectral cues for multispectral and panchromatic image fusion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visible-infrared person re-identification via partially
interactive collaboration. <em>TIP</em>, <em>31</em>, 6951–6963. (<a
href="https://doi.org/10.1109/TIP.2022.3217697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) task aims to retrieve the same person between visible and infrared images. VI-ReID is challenging as the images captured by different spectra present large cross-modality discrepancy. Many methods adopt a two-stream network and design additional constraint conditions to extract shared features for different modalities. However, the interaction between the feature extraction processes of different modalities is rarely considered. In this paper, a partially interactive collaboration method is proposed to exploit the complementary information of different modalities to reduce the modality gap for VI-ReID. Specifically, the proposed method is achieved in a partially interactive-shared architecture: collaborative shallow layers and shared deep layers. The collaborative shallow layers consider the interaction between modality-specific features of different modalities, encouraging the feature extraction processes of different modalities constrain each other to enhance feature representations. The shared deep layers further embed the modality-specific features to a common space to endow them the same identity discriminability. To ensure the interactive collaborative learning implement effectively, the conventional loss and collaborative loss are utilized jointly to train the whole network. Extensive experiments on two publicly available VI-ReID datasets verify the superiority of the proposed PIC method. Specifically, the proposed method achieves a rank-1 accuracy of 83.6\% and 57.5\% on RegDB and SYSU-MM01 datasets, respectively.},
  archive      = {J_TIP},
  author       = {Xiangtao Zheng and Xiumei Chen and Xiaoqiang Lu},
  doi          = {10.1109/TIP.2022.3217697},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6951-6963},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visible-infrared person re-identification via partially interactive collaboration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FineAction: A fine-grained video dataset for temporal action
localization. <em>TIP</em>, <em>31</em>, 6937–6950. (<a
href="https://doi.org/10.1109/TIP.2022.3217368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17\% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond. The dataset is available at https://deeperaction.github.io/datasets/fineaction .},
  archive      = {J_TIP},
  author       = {Yi Liu and Limin Wang and Yali Wang and Xiao Ma and Yu Qiao},
  doi          = {10.1109/TIP.2022.3217368},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6937-6950},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FineAction: A fine-grained video dataset for temporal action localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An untrained neural network prior for light field
compression. <em>TIP</em>, <em>31</em>, 6922–6936. (<a
href="https://doi.org/10.1109/TIP.2022.3217374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models have proven to be effective priors for solving a variety of image processing problems. However, the learning of realistic image priors, based on a large number of parameters, requires a large amount of training data. It has been shown recently, with the so-called deep image prior (DIP), that randomly initialized neural networks can act as good image priors without learning. In this paper, we propose a deep generative model for light fields, which is compact and which does not require any training data other than the light field itself. To show the potential of the proposed generative model, we develop a complete light field compression scheme with quantization-aware learning and entropy coding of the quantized weights. Experimental results show that the proposed method yields very competitive results compared with state-of-the-art light field compression methods, both in terms of PSNR and MS-SSIM metrics.},
  archive      = {J_TIP},
  author       = {Xiaoran Jiang and Jinglei Shi and Christine Guillemot},
  doi          = {10.1109/TIP.2022.3217374},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6922-6936},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An untrained neural network prior for light field compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 6D-ViT: Category-level 6D object pose estimation via
transformer-based instance representation learning. <em>TIP</em>,
<em>31</em>, 6907–6921. (<a
href="https://doi.org/10.1109/TIP.2022.3216980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents 6D vision transformer (6D-ViT), a transformer-based instance representation learning network suitable for highly accurate category-level object pose estimation based on RGB-D images. Specifically, a novel two-stream encoder-decoder framework is dedicated to exploring complex and powerful instance representations from RGB images, point clouds, and categorical shape priors. The whole framework consists of two main branches, named Pixelformer and Pointformer. Pixelformer contains a pyramid transformer encoder with an all-multilayer perceptron (MLP) decoder to extract pixelwise appearance representations from RGB images, while Pointformer relies on a cascaded transformer encoder and an all-MLP decoder to acquire the pointwise geometric characteristics from point clouds. Then, dense instance representations (i.e., correspondence matrix and deformation field) for NOCS model reconstruction are obtained from a multisource aggregation (MSA) network with shape prior, appearance and geometric information as inputs. Finally, the instance 6D pose is computed by solving the similarity transformation between the observed point clouds and the reconstructed NOCS representations. Extensive experiments with synthetic and real-world datasets demonstrate that the proposed framework achieves state-of-the-art performance for both datasets. Code is available at https://github.com/luzzou/6D-ViT .},
  archive      = {J_TIP},
  author       = {Lu Zou and Zhangjin Huang and Naijie Gu and Guoping Wang},
  doi          = {10.1109/TIP.2022.3216980},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6907-6921},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {6D-ViT: Category-level 6D object pose estimation via transformer-based instance representation learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CBNet: A composite backbone network architecture for object
detection. <em>TIP</em>, <em>31</em>, 6893–6906. (<a
href="https://doi.org/10.1109/TIP.2022.3216771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern top-performing object detectors depend heavily on backbone networks, whose advances bring consistent performance gains through exploring more effective network structures. In this paper, we propose a novel and flexible backbone framework, namely CBNet, to construct high-performance detectors using existing open-source pre-trained backbones under the pre-training fine-tuning paradigm. In particular, CBNet architecture groups multiple identical backbones, which are connected through composite connections. Specifically, it integrates the high- and low-level features of multiple identical backbone networks and gradually expands the receptive field to more effectively perform object detection. We also propose a better training strategy with auxiliary supervision for CBNet-based detectors. CBNet has strong generalization capabilities for different backbones and head designs of the detector architecture. Without additional pre-training of the composite backbone, CBNet can be adapted to various backbones (i.e., CNN-based vs. Transformer-based) and head designs of most mainstream detectors (i.e., one-stage vs. two-stage, anchor-based vs. anchor-free-based). Experiments provide strong evidence that, compared with simply increasing the depth and width of the network, CBNet introduces a more efficient, effective, and resource-friendly way to build high-performance backbone networks. Particularly, our CB-Swin-L achieves 59.4\% box AP and 51.6\% mask AP on COCO test-dev under the single-model and single-scale testing protocol, which are significantly better than the state-of-the-art results (i.e., 57.7\% box AP and 50.2\% mask AP) achieved by Swin-L, while reducing the training time by $6\times $ . With multi-scale testing, we push the current best single model result to a new record of 60.1\% box AP and 52.3\% mask AP without using extra training data. Code is available at https://github.com/VDIGPKU/CBNetV2 .},
  archive      = {J_TIP},
  author       = {Tingting Liang and Xiaojie Chu and Yudong Liu and Yongtao Wang and Zhi Tang and Wei Chu and Jingdong Chen and Haibin Ling},
  doi          = {10.1109/TIP.2022.3216771},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6893-6906},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CBNet: A composite backbone network architecture for object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive proposal extension with LSTM network for weakly
supervised object detection. <em>TIP</em>, <em>31</em>, 6879–6892. (<a
href="https://doi.org/10.1109/TIP.2022.3216772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) has attracted more and more attention since it only uses image-level labels and can save huge annotation costs. Most of the WSOD methods use Multiple Instance Learning (MIL) as their basic framework, which regard it as an instance classification problem. However, these methods based on MIL tend to only converge on the most discriminative regions of different instances, rather than their corresponding complete regions, that is, insufficient integrity. Inspired by the human habit of observing things, we propose a new method by comparing the initial proposals and the extended ones to optimize those initial proposals. Specifically, we propose one new strategy for WSOD by involving contrastive proposal extension (CPE), which consists of multiple directional contrastive proposal extensions (D-CPEs), and each D-CPE contains LSTM-based encoders and dual-stream decoders. Firstly, the boundary of initial proposals in MIL is extended to different positions according to well-designed sequential order. Then, the CPE compares the extended proposal and the initial one by extracting the feature semantics of them using the encoders, and calculates the integrity of the initial proposal to optimize its score. These contrastive contextual semantics will guide the basic WSOD to suppress bad proposals and improve the scores of good ones. In addition, a simple dual-stream network is designed as the decoder to constrain the temporal coding of LSTM and improve the performance of WSOD further. Experiments on PASCAL VOC 2007, VOC 2012 and MS-COCO datasets show that our method has achieved the state-of-the-art results.},
  archive      = {J_TIP},
  author       = {Pei Lv and Suqi Hu and Tianran Hao},
  doi          = {10.1109/TIP.2022.3216772},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6879-6892},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contrastive proposal extension with LSTM network for weakly supervised object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based depth denoising &amp; dequantization for point
cloud enhancement. <em>TIP</em>, <em>31</em>, 6863–6878. (<a
href="https://doi.org/10.1109/TIP.2022.3214077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3D point cloud is typically constructed from depth measurements acquired by sensors at one or more viewpoints. The measurements suffer from both quantization and noise corruption. To improve quality, previous works denoise a point cloud a posteriori after projecting the imperfect depth data onto 3D space. Instead, we enhance depth measurements directly on the sensed images a priori, before synthesizing a 3D point cloud. By enhancing near the physical sensing process, we tailor our optimization to our depth formation model before subsequent processing steps that obscure measurement errors. Specifically, we model depth formation as a combined process of signal-dependent noise addition and non-uniform log-based quantization. The designed model is validated (with parameters fitted) using collected empirical data from a representative depth sensor. To enhance each pixel row in a depth image, we first encode intra-view similarities between available row pixels as edge weights via feature graph learning. We next establish inter-view similarities with another rectified depth image via viewpoint mapping and sparse linear interpolation. This leads to a maximum a posteriori (MAP) graph filtering objective that is convex and differentiable. We minimize the objective efficiently using accelerated gradient descent (AGD), where the optimal step size is approximated via Gershgorin circle theorem (GCT). Experiments show that our method significantly outperformed recent point cloud denoising schemes and state-of-the-art image denoising schemes in two established point cloud quality metrics.},
  archive      = {J_TIP},
  author       = {Xue Zhang and Gene Cheung and Jiahao Pang and Yash Sanghvi and Abhiram Gnanasambandam and Stanley H. Chan},
  doi          = {10.1109/TIP.2022.3214077},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6863-6878},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph-based depth denoising &amp; dequantization for point cloud enhancement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Open-source data-driven cross-domain road detection from
very high resolution remote sensing imagery. <em>TIP</em>, <em>31</em>,
6847–6862. (<a href="https://doi.org/10.1109/TIP.2022.3216481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-precision road detection from very high resolution (VHR) remote sensing images has broad application value. However, the most advanced deep learning based methods often fail to identify roads when there is a distribution discrepancy between the training samples and test samples, due to their limited generalization ability. In this paper, to address this problem, an open-source data-driven domain-specific representation (OSM-DOER) framework is proposed for cross-domain road detection. On the one hand, as the spatial structure information of the source and target domains is similar, but the texture information is different, the domain-specific representation (DOER) framework is proposed, which not only aligns the distributions of the spatial structure information, but also learns the domain-specific texture information. Furthermore, in order to enhance the representation of the target domain data distribution, open-source and freely available OpenStreetMap (OSM) road centerline data are utilized to generate target domain samples, which are then used in the network training as the supervised information for the target domain. Finally, to verify the superiority of the proposed OSM-DOER framework, we conducted extensive experiments with the public SpaceNet and DeepGlobe road datasets, and large-scale road datasets from Birmingham in the UK and Shanghai in China. The experimental results demonstrate that the proposed OSM-DOER framework shows obvious advantages over the mainstream road detection methods, and the use of OSM road centerline data has great potential for the road detection task.},
  archive      = {J_TIP},
  author       = {Xiaoyan Lu and Yanfei Zhong and Liangpei Zhang},
  doi          = {10.1109/TIP.2022.3216481},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6847-6862},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Open-source data-driven cross-domain road detection from very high resolution remote sensing imagery},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-view 3D reconstruction via learning correspondence and
dependency of point cloud regions. <em>TIP</em>, <em>31</em>, 6831–6846.
(<a href="https://doi.org/10.1109/TIP.2022.3215024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view 3D reconstruction generally adopts the feature fusion strategy to guide the generation of 3D shape for objects with different views. Empirically, the correspondence learning of object regions across different views enables better feature fusion. However, such idea has not been fully exploited in existing methods. Furthermore, current methods fail to explore the intrinsic dependency among regions within a 3D shape, leading to a rough reconstruction result. To address the above issues, we propose a Dual-View 3D Point Cloud reconstruction architecture named DVPC, which takes two views images as inputs, and progressively generates a refined 3D point cloud. First, a point cloud generation network is assigned to generate a coarse point cloud for each input view. Second, a dual-view point clouds synthesis network is presented in DVPC. It constructs a regional attention mechanism to learn a high-quality correspondence among regions across two coarse point clouds in different views, so that our DVPC can achieve feature fusion accurately. And then it develops a point cloud deformation module to produce a relatively-precise point cloud via establishing the communication between the coarse point cloud and the fused feature. Lastly, a point-region transformer network is devised to model the dependency among regions within the relatively-precise point cloud. With the dependency, the relatively-precise point cloud is refined into a desirable 3D point cloud with rich details. Qualitative and quantitative experiments on the ShapeNet and Pix3D datasets demonstrate that the proposed DVPC outperforms the state-of-the-art methods in terms of reconstruction quality.},
  archive      = {J_TIP},
  author       = {Xin Jia and Shourui Yang and Yunbo Wang and Jianhua Zhang and Yuxin Peng and Shengyong Chen},
  doi          = {10.1109/TIP.2022.3215024},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6831-6846},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-view 3D reconstruction via learning correspondence and dependency of point cloud regions},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SGUIE-net: Semantic attention guided underwater image
enhancement with multi-scale perception. <em>TIP</em>, <em>31</em>,
6816–6830. (<a href="https://doi.org/10.1109/TIP.2022.3216208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the wavelength-dependent light attenuation, refraction and scattering, underwater images usually suffer from color distortion and blurred details. However, due to the limited number of paired underwater images with undistorted images as reference, training deep enhancement models for diverse degradation types is quite difficult. To boost the performance of data-driven approaches, it is essential to establish more effective learning mechanisms that mine richer supervised information from limited training sample resources. In this paper, we propose a novel underwater image enhancement network, called SGUIE-Net, in which we introduce semantic information as high-level guidance via region-wise enhancement feature learning. Accordingly, we propose semantic region-wise enhancement module to better learn local enhancement features for semantic regions with multi-scale perception. After using them as complementary features and feeding them to the main branch, which extracts the global enhancement features on the original image scale, the fused features bring semantically consistent and visually superior enhancements. Extensive experiments on the publicly available datasets and our proposed dataset demonstrate the impressive performance of SGUIE-Net. The code and proposed dataset are available at https://trentqq.github.io/SGUIE-Net.html .},
  archive      = {J_TIP},
  author       = {Qi Qi and Kunqian Li and Haiyong Zheng and Xiang Gao and Guojia Hou and Kun Sun},
  doi          = {10.1109/TIP.2022.3216208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6816-6830},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SGUIE-net: Semantic attention guided underwater image enhancement with multi-scale perception},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CIR-net: Cross-modality interaction and refinement for RGB-d
salient object detection. <em>TIP</em>, <em>31</em>, 6800–6815. (<a
href="https://doi.org/10.1109/TIP.2022.3216198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focusing on the issue of how to effectively capture and utilize cross-modality information in RGB-D salient object detection (SOD) task, we present a convolutional neural network (CNN) model, named CIR-Net, based on the novel cross-modality interaction and refinement. For the cross-modality interaction, 1) a progressive attention guided integration unit is proposed to sufficiently integrate RGB-D feature representations in the encoder stage, and 2) a convergence aggregation structure is proposed, which flows the RGB and depth decoding features into the corresponding RGB-D decoding streams via an importance gated fusion unit in the decoder stage. For the cross-modality refinement, we insert a refinement middleware structure between the encoder and the decoder, in which the RGB, depth, and RGB-D encoder features are further refined by successively using a self-modality attention refinement unit and a cross-modality weighting refinement unit. At last, with the gradually refined features, we predict the saliency map in the decoder stage. Extensive experiments on six popular RGB-D SOD benchmarks demonstrate that our network outperforms the state-of-the-art saliency detectors both qualitatively and quantitatively. The code and results can be found from the link of https://rmcong.github.io/proj_CIRNet.html .},
  archive      = {J_TIP},
  author       = {Runmin Cong and Qinwei Lin and Chen Zhang and Chongyi Li and Xiaochun Cao and Qingming Huang and Yao Zhao},
  doi          = {10.1109/TIP.2022.3216198},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6800-6815},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CIR-net: Cross-modality interaction and refinement for RGB-D salient object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BANet: A blur-aware attention network for dynamic scene
deblurring. <em>TIP</em>, <em>31</em>, 6789–6799. (<a
href="https://doi.org/10.1109/TIP.2022.3216216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image motion blur results from a combination of object motions and camera shakes, and such blurring effect is generally directional and non-uniform. Previous research attempted to solve non-uniform blurs using self-recurrent multi-scale, multi-patch, or multi-temporal architectures with self-attention to obtain decent results. However, using self-recurrent frameworks typically leads to a longer inference time, while inter-pixel or inter-channel self-attention may cause excessive memory usage. This paper proposes a Blur-aware Attention Network (BANet), that accomplishes accurate and efficient deblurring via a single forward pass. Our BANet utilizes region-based self-attention with multi-kernel strip pooling to disentangle blur patterns of different magnitudes and orientations and cascaded parallel dilated convolution to aggregate multi-scale content features. Extensive experimental results on the GoPro and RealBlur benchmarks demonstrate that the proposed BANet performs favorably against the state-of-the-arts in blurred image restoration and can provide deblurred results in real-time.},
  archive      = {J_TIP},
  author       = {Fu-Jen Tsai and Yan-Tsung Peng and Chung-Chi Tsai and Yen-Yu Lin and Chia-Wen Lin},
  doi          = {10.1109/TIP.2022.3216216},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6789-6799},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BANet: A blur-aware attention network for dynamic scene deblurring},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bi-directional pseudo-three-dimensional network for video
frame interpolation. <em>TIP</em>, <em>31</em>, 6773–6788. (<a
href="https://doi.org/10.1109/TIP.2022.3215911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent video frame interpolation methods have employed the curvilinear motion model to accommodate nonlinear motion among frames. The effectiveness of such model often hinges on motion estimation and occlusion detection, and therefore is greatly challenged when these methods are used to handle dynamic scenes that contain complex motions and occlusions. We address the challenges by proposing a bi-directional pseudo-three-dimensional network to exploit the correlation between motion estimation and depth-related occlusion estimation that considers the third dimension: depth. Specifically, the network exploits the correlation by learning shared multi-scale spatiotemporal representations, and by coupling the estimations, in both the past and future directions, to synthesize intermediate frames through a bi-directional pseudo-three-dimensional warping layer, where adaptive convolution kernels are estimated progressively from the coalescence of motion and depth-related occlusion estimations across multiple scales to acquire nonlocal and adaptive neighborhoods. The proposed network utilizes a novel multi-task collaborative learning strategy, which facilitates the supervised learning of video frame interpolation using complementary self-supervisory signals from motion and depth-related occlusion estimations. Across various benchmark datasets, the proposed method outperforms state-of-the-art methods in terms of accuracy, model size and runtime performance.},
  archive      = {J_TIP},
  author       = {Yao Luo and Jinshan Pan and Jinhui Tang},
  doi          = {10.1109/TIP.2022.3215911},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6773-6788},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bi-directional pseudo-three-dimensional network for video frame interpolation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CLAST: Contrastive learning for arbitrary style transfer.
<em>TIP</em>, <em>31</em>, 6761–6772. (<a
href="https://doi.org/10.1109/TIP.2022.3215899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer aims at migrating the style of a reference style painting to a target content image. Existing methods find it challenging to achieve good content fidelity and style migration at the same time. Moreover, they all rely on manually defined content and style, which is of limited universality and robustness. In this paper, we propose to introduce contrastive learning into style transfer, instructing the network to automatically learn to model the structural content and artistic style based on natural contrastive relationships in style transfer. Compared with existing methods, our learned modeling of content and style is more robust and universal. In addition, we further propose instance-wise contrastive style losses and a patch-wise contrastive content loss to guide style transfer. Combining the proposed contrastive losses and two self-reconstruction strategies, we develop a new style transfer framework, which is pluggable and can be flexibly applied to various style transfer modules. Experimental results demonstrate that our method has strong flexibility and synthesizes stylized images with higher quality.},
  archive      = {J_TIP},
  author       = {Xinhao Wang and Wenjing Wang and Shuai Yang and Jiaying Liu},
  doi          = {10.1109/TIP.2022.3215899},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6761-6772},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CLAST: Contrastive learning for arbitrary style transfer},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatially consistent transformer for colorization in
monochrome-color dual-lens system. <em>TIP</em>, <em>31</em>, 6747–6760.
(<a href="https://doi.org/10.1109/TIP.2022.3215910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the colorization problem in monochrome-color dual-lens camera systems, i.e. colorizing the gray image from the monochrome camera using the color image from the color camera as reference. In related methods, cost volume based CNN methods achieve the state-of-the-art results, but they are costly in GPU memory due to building the 4D cost volume. Recently, some slice-wise cross-attention based methods are proposed for related problems. The slice-wise cross-attention has much less costs in GPU memory but directly using them for this colorization problem cannot generate competing results. We make use of the non-local computation property of cross-attention to propose a transformer based method. To overcome the limitations of straight-forward slice-wise cross-attention, we propose the spatially consistent cross-attention (SCCA) block to encourage pixels of slices across different epipolar lines in the gray image to find spatially consistent correspondence with pixels of the reference color image. And, to further reduce the memory cost while keeping the colorization accuracy, we design a pyramid processing strategy to cascade a series of SCCA blocks with smaller slice size and perform the colorization from coarse to fine. To extract more powerful image features, we use several regional self-attention (RSA) blocks with U-style connections. Experimental results show that we outperform the state-of-the-art methods largely on the synthesized datasets of Cityscapes, Sintel, and SceneFlow, and the real monochrome-color dual-lens dataset.},
  archive      = {J_TIP},
  author       = {Xuan Dong and Chang Liu and Xiaoyan Hu and Kang Xu and Weixin Li},
  doi          = {10.1109/TIP.2022.3215910},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6747-6760},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatially consistent transformer for colorization in monochrome-color dual-lens system},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRNet: Double recalibration network for few-shot semantic
segmentation. <em>TIP</em>, <em>31</em>, 6733–6746. (<a
href="https://doi.org/10.1109/TIP.2022.3215905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation aims at learning to segment query images guided by only a few annotated images from the support set. Previous methods rely on mining the feature embedding similarity across the query and the support images to achieve successful segmentation. However, these models tend to perform badly in cases where the query instances have a large variance from the support ones. To enhance model robustness against such intra-class variance, we propose a Double Recalibration Network (DRNet) with two recalibration modules, i.e., the Self-adapted Recalibration (SR) module and the Cross-attended Recalibration (CR) module. In particular, beyond learning robust feature embedding for pixel-wise comparison between support and query as in conventional methods, the DRNet further exploits semantic-aware knowledge embedded in the query image to help segment itself, which we call ‘self-adapted recalibration’. More specifically, DRNet first employs guidance from the support set to roughly predict an incomplete but correct initial object region for the query image, and then reversely uses the feature embedding extracted from the incomplete object region to segment the query image. Also, we devise a CR module to refine the feature representation of the query image by propagating the underlying knowledge embedded in the support image’s foreground to the query. Instead of foreground global pooling, we refine the response at each pixel in the query feature map by attending to all foreground pixels in the support feature map and taking the weighted average by their similarity; meanwhile, feature maps of the query image are also added back to weighted feature maps as a residual connection. Our DRNet can effectively address the intra-class variance under the few-shot setting with such two recalibration modules, and mine more accurate target regions for query images. We conduct extensive experiments on the popular benchmarks PASCAL- $5^{i}$ and COCO- $20^{i}$ . The DRNet with the best configuration achieves the mIoU of $\textbf {63.6}\%$ and $\textbf {64.9}\%$ on PASCAL- $5^{i}$ and $\textbf {44.7}\%$ and $\textbf {49.6}\%$ on COCO- $20^{i}$ for 1-shot and 5-shot settings respectively, significantly outperforming the state-of-the-arts without any bells and whistles. Code is available at: https://github.com/fangzy97/drnet .},
  archive      = {J_TIP},
  author       = {Guangyu Gao and Zhiyuan Fang and Cen Han and Yunchao Wei and Chi Harold Liu and Shuicheng Yan},
  doi          = {10.1109/TIP.2022.3215905},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6733-6746},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DRNet: Double recalibration network for few-shot semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled capsule routing for fast part-object relational
saliency. <em>TIP</em>, <em>31</em>, 6719–6732. (<a
href="https://doi.org/10.1109/TIP.2022.3215887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the Part-Object Relational (POR) saliency underpinned by the Capsule Network (CapsNet) has been demonstrated to be an effective modeling mechanism to improve the saliency detection accuracy. However, it is widely known that the current capsule routing operations have huge computational complexity, which seriously limited the usability of the POR saliency models in real-time applications. To this end, this paper takes an early step towards a fast POR saliency inference by proposing a novel disentangled part-object relational network. Concretely, we disentangle horizontal routing and vertical routing from the original omnidirectional capsule routing, thus generating Disentangled Capsule Routing (DCR). This mechanism enjoys two advantages. On one hand, DCR that disentangles orthogonal 1D (i.e., vertical and horizontal) routing greatly reduces parameters and routing complexity, resulting in much faster inference than omnidirectional 2D routing adopted by existing CapsNets. On the other hand, thanks to the light POR cues explored by DCR, we could conveniently integrate the part-object routing process to different feature layers in CNN, rather than just applying it to the small-scaled one as in previous works. This helps to increase saliency inference accuracy. Compared to previous POR saliency detectors, DPORTNet infers visual saliency $\left ({{5 \sim 9} }\right) \times $ faster, and is more accurate. DPORTNet is available under the open-source license at https://github.com/liuyi1989/DCR .},
  archive      = {J_TIP},
  author       = {Yi Liu and Dingwen Zhang and Nian Liu and Shoukun Xu and Jungong Han},
  doi          = {10.1109/TIP.2022.3215887},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6719-6732},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Disentangled capsule routing for fast part-object relational saliency},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). C2FNet: A coarse-to-fine network for multi-view 3D point
cloud generation. <em>TIP</em>, <em>31</em>, 6707–6718. (<a
href="https://doi.org/10.1109/TIP.2022.3203213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generation of a 3D model of an object from multiple views has a wide range of applications. Different parts of an object would be accurately captured by a particular view or a subset of views in the case of multiple views. In this paper, a novel coarse-to-fine network (C2FNet) is proposed for 3D point cloud generation from multiple views. C2FNet generates subsets of 3D points that are best captured by individual views with the support of other views in a coarse-to-fine way, and then fuses these subsets of 3D points to a whole point cloud. It consists of a coarse generation module where coarse point clouds are constructed from multiple views by exploring the cross-view spatial relations, and a fine generation module where the coarse point cloud features are refined under the guidance of global consistency in appearance and context. Extensive experiments on the benchmark datasets have demonstrated that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jianjun Lei and Jiahui Song and Bo Peng and Wanqing Li and Zhaoqing Pan and Qingming Huang},
  doi          = {10.1109/TIP.2022.3203213},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6707-6718},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {C2FNet: A coarse-to-fine network for multi-view 3D point cloud generation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continual referring expression comprehension via dual
modular memorization. <em>TIP</em>, <em>31</em>, 6694–6706. (<a
href="https://doi.org/10.1109/TIP.2022.3212317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Expression Comprehension (REC) aims to localize an image region of a given object described by a natural-language expression. While promising performance has been demonstrated, existing REC algorithms make a strong assumption that training data feeding into a model are given upfront, which degrades its practicality for real-world scenarios. In this paper, we propose Continual Referring Expression Comprehension (CREC), a new setting for REC, where a model is learning on a stream of incoming tasks. In order to continuously improve the model on sequential tasks without forgetting prior learned knowledge and without repeatedly re-training from a scratch, we propose an effective baseline method named Dual Modular Memorization (DMM), which alleviates the problem of catastrophic forgetting by two memorization modules: Implicit-Memory and Explicit-Memory. Specifically, the former module aims to constrain drastic changes to important parameters learned on old tasks when learning a new task; while the latter module maintains a buffer pool to dynamically select and store representative samples of each seen task for future rehearsal. We create three benchmarks for the new CREC setting, by respectively re-splitting three widely-used REC datasets RefCOCO, RefCOCO+ and RefCOCOg into sequential tasks. Extensive experiments on the constructed benchmarks demonstrate that our DMM method significantly outperforms other alternatives, based on two popular REC backbones. We make the source code and benchmarks publicly available to foster future progress in this field: https://github.com/zackschen/DMM .},
  archive      = {J_TIP},
  author       = {Heng Tao Shen and Cheng Chen and Peng Wang and Lianli Gao and Meng Wang and Jingkuan Song},
  doi          = {10.1109/TIP.2022.3212317},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6694-6706},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Continual referring expression comprehension via dual modular memorization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differentiable SAR renderer and image-based target
reconstruction. <em>TIP</em>, <em>31</em>, 6679–6693. (<a
href="https://doi.org/10.1109/TIP.2022.3215069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forward modeling of wave scattering and radar imaging mechanisms is the key to information extraction from synthetic aperture radar (SAR) images. Like inverse graphics in the optical domain, an inherently-integrated forward-inverse approach would be promising for SAR advanced information retrieval and target reconstruction. This paper presents such an attempt at inverse graphics for SAR imagery. A differentiable SAR renderer (DSR) is developed, which reformulates the mapping and projection algorithm of the SAR imaging mechanism in the differentiable form of probability maps. First-order gradients of the proposed DSR are then analytically derived, which can be back-propagated from rendered image/silhouette to the target geometry and scattering attributes. A 3D inverse target reconstruction algorithm from SAR images is devised. Several simulation and reconstruction experiments are conducted, including targets with and without background, using synthesized data or real measured inverse SAR (ISAR) data by ground radar. Results demonstrate the efficacy of the proposed DSR and its inverse approach.},
  archive      = {J_TIP},
  author       = {Shilei Fu and Feng Xu},
  doi          = {10.1109/TIP.2022.3215069},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6679-6693},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Differentiable SAR renderer and image-based target reconstruction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning a prototype discriminator with RBF for multimodal
image synthesis. <em>TIP</em>, <em>31</em>, 6664–6678. (<a
href="https://doi.org/10.1109/TIP.2022.3214336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal image synthesis has emerged as a viable solution to the modality missing challenge. Most existing approaches employ softmax-based classifiers to provide modal constraints for the generated models. These methods, however, focus on learning to distinguish inter-domain differences while failing to build intra-domain compactness, resulting in inferior synthetic results. To provide sufficient domain-specific constraint, we hereby introduce a novel prototype discriminator for generative adversarial network (PT-GAN) to effectively estimate the missing or noisy modalities. Different from most previous works, we introduce the Radial Basis Function (RBF) network, endowing the discriminator with domain-specific prototypes, to improve the optimization of generative model. Since the prototype learning extracts more discriminative representation of each domain, and emphasizes intra-domain compactness, it reduces the sensitivity of discriminator to pixel changes in generated images. To address this dilemma, we further propose a reconstructive regularization term which connects the discriminator with the generator, thus enhancing its pixel detectability. To this end, the proposed PT-GAN provides not only consistent domain-specific constraints, but also reasonable uncertainty estimation of generated images with the RBF distance. Experimental results show that our method outperforms the state-of-the-art techniques. The source code will be available at: https://github.com/zhiweibi/PT-GAN .},
  archive      = {J_TIP},
  author       = {Zhiwei Bi and Bing Cao and Wangmeng Zuo and Qinghua Hu},
  doi          = {10.1109/TIP.2022.3214336},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6664-6678},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning a prototype discriminator with RBF for multimodal image synthesis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Salient object detection via dynamic scale routing.
<em>TIP</em>, <em>31</em>, 6649–6663. (<a
href="https://doi.org/10.1109/TIP.2022.3214332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research advances in salient object detection (SOD) could largely be attributed to ever-stronger multi-scale feature representation empowered by the deep learning technologies. The existing SOD deep models extract multi-scale features via the off-the-shelf encoders and combine them smartly via various delicate decoders. However, the kernel sizes in this commonly-used thread are usually “fixed”. In our new experiments, we have observed that kernels of small size are preferable in scenarios containing tiny salient objects. In contrast, large kernel sizes could perform better for images with large salient objects. Inspired by this observation, we advocate the “dynamic” scale routing (as a brand-new idea) in this paper. It will result in a generic plug-in that could directly fit the existing feature backbone. This paper’s key technical innovations are two-fold. First, instead of using the vanilla convolution with fixed kernel sizes for the encoder design, we propose the dynamic pyramid convolution (DPConv), which dynamically selects the best-suited kernel sizes w.r.t. the given input. Second, we provide a self-adaptive bidirectional decoder design to accommodate the DPConv-based encoder best. The most significant highlight is its capability of routing between feature scales and their dynamic collection, making the inference process scale-aware. As a result, this paper continues to enhance the current SOTA performance. Both the code and dataset are publicly available at https://github.com/wuzhenyubuaa/DPNet .},
  archive      = {J_TIP},
  author       = {Zhenyu Wu and Shuai Li and Chenglizhao Chen and Hong Qin and Aimin Hao},
  doi          = {10.1109/TIP.2022.3214332},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6649-6663},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Salient object detection via dynamic scale routing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BiN-flow: Bidirectional normalizing flow for robust image
dehazing. <em>TIP</em>, <em>31</em>, 6635–6648. (<a
href="https://doi.org/10.1109/TIP.2022.3214093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing aims to remove haze in images to improve their image quality. However, most image dehazing methods heavily depend on strict prior knowledge and paired training strategy, which would hinder generalization and performance when dealing with unseen scenes. In this paper, to address the above problem, we propose Bidirectional Normalizing Flow (BiN-Flow), which exploits no prior knowledge and constructs a neural network through weakly-paired training with better generalization for image dehazing. Specifically, BiN-Flow designs 1) Feature Frequency Decoupling (FFD) for mining the various texture details through multi-scale residual blocks and 2) Bidirectional Propagation Flow (BPF) for exploiting the one-to-many relationships between hazy and haze-free images using a sequence of invertible Flow. In addition, BiN-Flow constructs a reference mechanism (RM) that uses a small number of paired hazy and haze-free images and a large number of haze-free reference images for weakly-paired training. Essentially, the mutual relationships between hazy and haze-free images could be effectively learned to further improve the generalization and performance for image dehazing. We conduct extensive experiments on five commonly-used datasets to validate the BiN-Flow. The experimental results that BiN-Flow outperforms all state-of-the-art competitors demonstrate the capability and generalization of our BiN-Flow. Besides, our BiN-Flow could produce diverse dehazing images for the same image by considering restoration diversity.},
  archive      = {J_TIP},
  author       = {Yiqiang Wu and Dapeng Tao and Yibing Zhan and Chenyang Zhang},
  doi          = {10.1109/TIP.2022.3214093},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6635-6648},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BiN-flow: Bidirectional normalizing flow for robust image dehazing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Middle-level feature fusion for lightweight RGB-d salient
object detection. <em>TIP</em>, <em>31</em>, 6621–6634. (<a
href="https://doi.org/10.1109/TIP.2022.3214092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB-D salient object detection (SOD) models adopt a two-stream structure to extract the information from the input RGB and depth images. Since they use two subnetworks for unimodal feature extraction and multiple multi-modal feature fusion modules for extracting cross-modal complementary information, these models require a huge number of parameters, thus hindering their real-life applications. To remedy this situation, we propose a novel middle-level feature fusion structure that allows to design a lightweight RGB-D SOD model. Specifically, the proposed structure first employs two shallow subnetworks to extract low- and middle-level unimodal RGB and depth features, respectively. Afterward, instead of integrating middle-level unimodal features multiple times at different layers, we just fuse them once via a specially designed fusion module. On top of that, high-level multi-modal semantic features are further extracted for final salient object detection via an additional subnetwork. This will greatly reduce the network’s parameters. Moreover, to compensate for the performance loss due to parameter deduction, a relation-aware multi-modal feature fusion module is specially designed to effectively capture the cross-modal complementary information during the fusion of middle-level multi-modal features. By enabling the feature-level and decision-level information to interact, we maximize the usage of the fused cross-modal middle-level features and the extracted cross-modal high-level features for saliency prediction. Experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods. Remarkably, our proposed model has only 3.9M parameters and runs at 33 FPS.},
  archive      = {J_TIP},
  author       = {Nianchang Huang and Qiang Jiao and Qiang Zhang and Jungong Han},
  doi          = {10.1109/TIP.2022.3214092},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6621-6634},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Middle-level feature fusion for lightweight RGB-D salient object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-clustering on bipartite graphs for robust model fitting.
<em>TIP</em>, <em>31</em>, 6605–6620. (<a
href="https://doi.org/10.1109/TIP.2022.3214073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph-based methods have been widely applied to model fitting. However, in these methods, association information is invariably lost when data points and model hypotheses are mapped to the graph domain. In this paper, we propose a novel model fitting method based on co-clustering on bipartite graphs (CBG) to estimate multiple model instances in data contaminated with outliers and noise. Model fitting is reformulated as a bipartite graph partition behavior. Specifically, we use a bipartite graph reduction technique to eliminate some insignificant vertices (outliers and invalid model hypotheses), thereby improving the reliability of the constructed bipartite graph and reducing the computational complexity. We then use a co-clustering algorithm to learn a structured optimal bipartite graph with exact connected components for partitioning that can directly estimate the model instances (i.e., post-processing steps are not required). The proposed method fully utilizes the duality of data points and model hypotheses on bipartite graphs, leading to superior fitting performance. Exhaustive experiments show that the proposed CBG method performs favorably when compared with several state-of-the-art fitting methods.},
  archive      = {J_TIP},
  author       = {Shuyuan Lin and Hailing Luo and Yan Yan and Guobao Xiao and Hanzi Wang},
  doi          = {10.1109/TIP.2022.3214073},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6605-6620},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Co-clustering on bipartite graphs for robust model fitting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Color alignment for relative color constancy via
non-standard references. <em>TIP</em>, <em>31</em>, 6591–6604. (<a
href="https://doi.org/10.1109/TIP.2022.3214107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relative colour constancy is an essential requirement for many scientific imaging applications. However, most digital cameras differ in their image formations and native sensor output is usually inaccessible, e.g., in smartphone camera applications. This makes it hard to achieve consistent colour assessment across a range of devices, and that undermines the performance of computer vision algorithms. To resolve this issue, we propose a colour alignment model that considers the camera image formation as a black-box and formulates colour alignment as a three-step process: camera response calibration, response linearisation, and colour matching. The proposed model works with non-standard colour references, i.e., colour patches without knowing the true colour values, by utilising a novel balance-of-linear-distances feature. It is equivalent to determining the camera parameters through an unsupervised process. It also works with a minimum number of corresponding colour patches across the images to be colour aligned to deliver the applicable processing. Three challenging image datasets collected by multiple cameras under various illumination and exposure conditions, including one that imitates uncommon scenes such as scientific imaging, were used to evaluate the model. Performance benchmarks demonstrated that our model achieved superior performance compared to other popular and state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yunfeng Zhao and Stuart Ferguson and Huiyu Zhou and Christopher Elliott and Karen Rafferty},
  doi          = {10.1109/TIP.2022.3214107},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6591-6604},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color alignment for relative color constancy via non-standard references},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pseudo decoder guided light-weight architecture for image
inpainting. <em>TIP</em>, <em>31</em>, 6577–6590. (<a
href="https://doi.org/10.1109/TIP.2022.3213444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is one of the most important and widely used approaches where input image is synthesized at the missing regions. This has various applications like undesired object removal, virtual garment shopping, etc. The methods used for image inpainting may use the knowledge of hole locations to effectively regenerate contents in an image. Existing image inpainting methods give astonishing results with coarse-to-fine architectures or with use of guided information like edges, structures, etc. The coarse-to-fine architectures require umpteen resources leading to high computation cost of the architecture. Other methods with edge or structural information depend on the available models to generate guiding information for inpainting. In this context, we have proposed computationally efficient, light-weight network for image inpainting with very less number of parameters (0.97M) and without any guided information. The proposed architecture consists of the multi-encoder level feature fusion module, pseudo decoder and regeneration decoder. The encoder multi level feature fusion module extracts relevant information from each of the encoder levels to merge structural and textural information from various receptive fields. This information is then processed with pseudo decoder followed by space depth correlation module to assist regeneration decoder for inpainting task. The experiments are performed with different types of masks and compared with the state-of-the-art methods on three benchmark datasets i.e., Paris Street View (PARIS_SV), Places2 and CelebA_HQ. Along with this, the proposed network is tested on high resolution images ( $1024\times1024$ and 2048 $\times2048$ ) and compared with the existing methods. The extensive comparison with state-of-the-art methods, computational complexity analysis, and ablation study prove the effectiveness of the proposed framework for image inpainting.},
  archive      = {J_TIP},
  author       = {Shruti S. Phutke and Subrahmanyam Murala},
  doi          = {10.1109/TIP.2022.3213444},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6577-6590},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pseudo decoder guided light-weight architecture for image inpainting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CVIDS: A collaborative localization and dense mapping
framework for multi-agent based visual-inertial SLAM. <em>TIP</em>,
<em>31</em>, 6562–6576. (<a
href="https://doi.org/10.1109/TIP.2022.3213189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, visual SLAM (Simultaneous Localization And Mapping) has become a hot research topic due to its low costs and wide application scopes. Traditional visual SLAM frameworks are usually designed for single-agent systems, completing both the localization and the mapping with sensors equipped on a single robot or a mobile device. However, the mobility and work capacity of the single agent are usually limited. In reality, robots or mobile devices sometimes may be deployed in the form of clusters, such as drone formations, wearable motion capture systems, and so on. As far as we know, existing SLAM systems designed for multi-agents are still sporadic, and most of them have non-negligible limitations in functions. Specifically, on one hand, most of the existing multi-agent SLAM systems can only extract some key features and build sparse maps. On the other hand, schemes that can reconstruct the environment densely cannot get rid of the dependence on depth sensors, such as RGBD cameras or LiDARs. Systems that can yield high-density maps just with monocular camera suites are temporarily lacking. As an attempt to fill in the research gap to some extent, we design a novel collaborative SLAM system, namely CVIDS (Collaborative Visual-Inertial Dense SLAM), which follows a centralized and loosely coupled framework and can be integrated with any existing Visual-Inertial Odometry (VIO) to accomplish the co-localization and the dense reconstruction. Integrating our proposed robust loop closure detection module and two-stage pose-graph optimization pipeline, the co-localization module of CVIDS can estimate the poses of different agents in a unified coordinate system efficiently from the packed images and local poses sent by the client-ends of different agents. Besides, our motion-based dense mapping module can effectively recover the 3D structures of selected keyframes and then fuse their depth information to the global map for reconstruction. The superior performance of CVIDS is corroborated by both quantitative and qualitative experimental results. To make our results reproducible, the source code has been released at https://cslinzhang.github.io/CVIDS .},
  archive      = {J_TIP},
  author       = {Tianjun Zhang and Lin Zhang and Yang Chen and Yicong Zhou},
  doi          = {10.1109/TIP.2022.3213189},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6562-6576},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CVIDS: A collaborative localization and dense mapping framework for multi-agent based visual-inertial SLAM},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Offline-online associated camera-aware proxies for
unsupervised person re-identification. <em>TIP</em>, <em>31</em>,
6548–6561. (<a href="https://doi.org/10.1109/TIP.2022.3213193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised person re-identification (Re-ID) has received increasing research attention due to its potential for label-free applications. A promising way to address unsupervised Re-ID is clustering-based, which generates pseudo labels by clustering and uses the pseudo labels to train a Re-ID model iteratively. However, most clustering-based methods take each cluster as a pseudo identity class, neglecting the intra-cluster variance mainly caused by the change of cameras. To address this issue, we propose to split each single cluster into multiple proxies according to camera views. The camera-aware proxies explicitly capture local structures within clusters, by which the intra-ID variance and inter-ID similarity can be better tackled. Assisted with the camera-aware proxies, we design two proxy-level contrastive learning losses that are, respectively, based on offline and online association results. The offline association directly associates proxies according to the clustering and splitting results, while the online strategy dynamically associates proxies in terms of up-to-date features to reduce the noise caused by the delayed update of pseudo labels. The combination of two losses enables us to train a desirable Re-ID model. Extensive experiments on three person Re-ID datasets and one vehicle Re-ID dataset show that our proposed approach demonstrates competitive performance with state-of-the-art methods. Code will be available at: https://github.com/Terminator8758/O2CAP .},
  archive      = {J_TIP},
  author       = {Menglin Wang and Jiachen Li and Baisheng Lai and Xiaojin Gong and Xian-Sheng Hua},
  doi          = {10.1109/TIP.2022.3213193},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6548-6561},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Offline-online associated camera-aware proxies for unsupervised person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Toward stable co-saliency detection and object
co-segmentation. <em>TIP</em>, <em>31</em>, 6532–6547. (<a
href="https://doi.org/10.1109/TIP.2022.3212906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel model for simultaneous stable co-saliency detection (CoSOD) and object co-segmentation (CoSEG). To detect co-saliency (segmentation) accurately, the core problem is to well model inter-image relations between an image group. Some methods design sophisticated modules, such as recurrent neural network (RNN), to address this problem. However, order-sensitive problem is the major drawback of RNN, which heavily affects the stability of proposed CoSOD (CoSEG) model. In this paper, inspired by RNN-based model, we first propose a multi-path stable recurrent unit (MSRU), containing dummy orders mechanisms (DOM) and recurrent unit (RU). Our proposed MSRU not only helps CoSOD (CoSEG) model captures robust inter-image relations, but also reduces order-sensitivity, resulting in a more stable inference and training process. Moreover, we design a cross-order contrastive loss (COCL) that can further address order-sensitive problem by pulling close the feature embedding generated from different input orders. We validate our model on five widely used CoSOD datasets (CoCA, CoSOD3k, Cosal2015, iCoseg and MSRC), and three widely used datasets (Internet, iCoseg and PASCAL-VOC) for object co-segmentation, the performance demonstrates the superiority of the proposed approach as compared to the state-of-the-art (SOTA) methods.},
  archive      = {J_TIP},
  author       = {Bo Li and Lv Tang and Senyun Kuang and Mofei Song and Shouhong Ding},
  doi          = {10.1109/TIP.2022.3212906},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6532-6547},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward stable co-saliency detection and object co-segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general dynamic knowledge distillation method for visual
analytics. <em>TIP</em>, <em>31</em>, 6517–6531. (<a
href="https://doi.org/10.1109/TIP.2022.3212905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing knowledge distillation (KD) method normally fixes the weight of the teacher network, and uses the knowledge from the teacher network to guide the training of the student network no-ninteractively, thus it is called static knowledge distillation (SKD). SKD is widely used in model compression on the homologous data and knowledge transfer on the heterogeneous data. However, the teacher network that with fixed-weight constrains the student network to learn knowledge from it. It is worth expecting that the teacher network itself can be continuously optimized to promote the learning ability of the student network dynamically. To overcome this limitation, we propose a novel dynamic knowledge distillation (DKD) method, in which the teacher network and the student network can learn from each other interactively. Importantly, we analyzed the effectiveness of DKD mathematically (see Eq. 4 ), and addressed one crucial issue caused by the continuous change of the teacher network in the dynamic distillation process via designing a valid loss function. We verified the practicality of our DKD by extensive experiments on various visual tasks, e.g. for model compression, we conducted experiments on image classification and object detection. For knowledge transfer, video-based human action recognition is chosen for analysis. The experimental results on benchmark datasets ( i.e. ILSVRC2012, COCO2017, HMDB51, UCF101) demonstrated that the proposed DKD is valid to improve the performance of these visual tasks for a large margin. The source code is publicly available online at https://github.com/errllxj/DKD .},
  archive      = {J_TIP},
  author       = {Zhigang Tu and Xiangjian Liu and Xuan Xiao},
  doi          = {10.1109/TIP.2022.3212905},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6517-6531},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A general dynamic knowledge distillation method for visual analytics},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coarse-to-fine contrastive self-supervised feature learning
for land-cover classification in SAR images with limited labeled data.
<em>TIP</em>, <em>31</em>, 6502–6516. (<a
href="https://doi.org/10.1109/TIP.2022.3211472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive self-supervised learning (CSSL) has achieved promising results in extracting visual features from unlabeled data. Most of the current CSSL methods are used to learn global image features with low-resolution that are not suitable or efficient for pixel-level tasks. In this paper, we propose a coarse-to-fine CSSL framework based on a novel contrasting strategy to address this problem. It consists of two stages, one for encoder pre-training to learn global features and the other for decoder pre-training to derive local features. Firstly, the novel contrasting strategy takes advantage of the spatial structure and semantic meaning of different regions and provides more cues to learn than that relying only on data augmentation. Specifically, a positive pair is built from two nearby patches sampled along the direction of the texture if they fall into the same cluster. A negative pair is generated from different clusters. When the novel contrasting strategy is applied to the coarse-to-fine CSSL framework, global and local features are learned successively by forcing the positive pair close to each other and the negative pair apart in an embedding space. Secondly, a discriminant constraint is incorporated into the per-pixel classification model to maximize the inter-class distance. It makes the classification model more competent at distinguishing between different categories that have similar appearance. Finally, the proposed method is validated on four SAR images for land-cover classification with limited labeled data and substantially improves the experimental results. The effectiveness of the proposed method is demonstrated in pixel-level tasks after comparison with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Meijuan Yang and Licheng Jiao and Fang Liu and Biao Hou and Shuyuan Yang and Yake Zhang and Jianlong Wang},
  doi          = {10.1109/TIP.2022.3211472},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6502-6516},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Coarse-to-fine contrastive self-supervised feature learning for land-cover classification in SAR images with limited labeled data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward understanding and boosting adversarial
transferability from a distribution perspective. <em>TIP</em>,
<em>31</em>, 6487–6501. (<a
href="https://doi.org/10.1109/TIP.2022.3211736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferable adversarial attacks against Deep neural networks (DNNs) have received broad attention in recent years. An adversarial example can be crafted by a surrogate model and then attack the unknown target model successfully, which brings a severe threat to DNNs. The exact underlying reasons for the transferability are still not completely understood. Previous work mostly explores the causes from the model perspective, e.g., decision boundary, model architecture, and model capacity. Here, we investigate the transferability from the data distribution perspective and hypothesize that pushing the image away from its original distribution can enhance the adversarial transferability. To be specific, moving the image out of its original distribution makes different models hardly classify the image correctly, which benefits the untargeted attack, and dragging the image into the target distribution misleads the models to classify the image as the target class, which benefits the targeted attack. Towards this end, we propose a novel method that crafts adversarial examples by manipulating the distribution of the image. We conduct comprehensive transferable attacks against multiple DNNs to demonstrate the effectiveness of the proposed method. Our method can significantly improve the transferability of the crafted attacks and achieves state-of-the-art performance in both untargeted and targeted scenarios, surpassing the previous best method by up to 40\% in some cases. In summary, our work provides new insight into studying adversarial transferability and provides a strong counterpart for future research on adversarial defense.},
  archive      = {J_TIP},
  author       = {Yao Zhu and Yuefeng Chen and Xiaodan Li and Kejiang Chen and Yuan He and Xiang Tian and Bolun Zheng and Yaowu Chen and Qingming Huang},
  doi          = {10.1109/TIP.2022.3211736},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6487-6501},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward understanding and boosting adversarial transferability from a distribution perspective},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrete metric learning for fast image set classification.
<em>TIP</em>, <em>31</em>, 6471–6486. (<a
href="https://doi.org/10.1109/TIP.2022.3212284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image set classification, most existing works focus on exploiting effective latent discriminative features. However, it remains a research gap to efficiently handle this problem. In this paper, benefiting from the superiority of hashing in terms of its computational complexity and memory costs, we present a novel Discrete Metric Learning (DML) approach based on the Riemannian manifold for fast image set classification. The proposed DML jointly learns a metric in the induced space and a compact Hamming space, where efficient classification is carried out. Specifically, each image set is modeled as a point on Riemannian manifold after which the proposed DML minimizes the Hamming distance between similar Riemannian pairs and maximizes the Hamming distance between dissimilar ones by introducing a discriminative Mahalanobis-like matrix. To overcome the shortcoming of DML that relies on the vectorization of Riemannian representations, we further develop Bilinear Discrete Metric Learning (BDML) to directly manipulate the original Riemannian representations and explore the natural matrix structure for high-dimensional data. Different from conventional Riemannian metric learning methods, which require complicated Riemannian optimizations (e.g., Riemannian conjugate gradient), both DML and BDML can be efficiently optimized by computing the geodesic mean between the similarity matrix and inverse of the dissimilarity matrix. Extensive experiments conducted on different visual recognition tasks (face recognition, object recognition, and action recognition) demonstrate that the proposed methods achieve competitive performance in terms of accuracy and efficiency.},
  archive      = {J_TIP},
  author       = {Dong Wei and Xiaobo Shen and Quansen Sun and Xizhan Gao},
  doi          = {10.1109/TIP.2022.3212284},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6471-6486},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discrete metric learning for fast image set classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-driven graph neural network for deep face
super-resolution. <em>TIP</em>, <em>31</em>, 6455–6470. (<a
href="https://doi.org/10.1109/TIP.2022.3212311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the help of convolutional neural networks (CNNs), deep learning-based methods have achieved remarkable performance in face super-resolution (FSR) task. Despite their success, most of the existing methods neglect non-local correlations of face images, leaving much room for improvement. In this paper, we introduce a novel end-to-end trainable attention-driven graph neural network (AD-GNN) for more discriminative feature extraction and feature relation modeling. This is achieved by two major components. The first component is a cross-scale dynamic graph (CDG) block. The CDG block considers cross-scale relationships of patches in distant areas and employs two dynamic graphs to construct enhanced features. The second component is a series of channel attention and spatial dynamic graph (CASDG) blocks. A CASDG block has a channel-wise attention unit and a spatial-aware dynamic graph (SDG) unit. The SDG unit extracts informative features by exploring spatial non-local self-similarity information of the patches using dynamic graph convolution. Using these two components, facial details can be effectively reconstructed with the help of information supplemented by similar but spatially remote patches and structural information of faces. Extensive experiments on two public benchmarks demonstrate the superiority of AD-GNN over the state-of-the-art FSR methods.},
  archive      = {J_TIP},
  author       = {Qiqi Bao and Bowen Gang and Wenming Yang and Jie Zhou and Qingmin Liao},
  doi          = {10.1109/TIP.2022.3212311},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6455-6470},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention-driven graph neural network for deep face super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised outlier detection using memory and contrastive
learning. <em>TIP</em>, <em>31</em>, 6440–6454. (<a
href="https://doi.org/10.1109/TIP.2022.3211476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is to separate anomalous data from inliers in the dataset. Recently, the most deep learning methods of outlier detection leverage an auxiliary reconstruction task by assuming that outliers are more difficult to recover than normal samples (inliers). However, it is not always true in deep auto-encoder (AE) based models. The auto-encoder based detectors may recover certain outliers even if outliers are not in the training data, because they do not constrain the feature learning. Instead, we think outlier detection can be done in the feature space by measuring the distance between outliers’ features and the consistency feature of inliers. To achieve this, we propose an unsupervised outlier detection method using a memory module and a contrastive learning module (MCOD). The memory module constrains the consistency of features, which merely represent the normal data. The contrastive learning module learns more discriminative features, which boosts the distinction between outliers and inliers. Extensive experiments on four benchmark datasets show that our proposed MCOD performs well and outperforms eleven state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Ning Huyan and Dou Quan and Xiangrong Zhang and Xuefeng Liang and Jocelyn Chanussot and Licheng Jiao},
  doi          = {10.1109/TIP.2022.3211476},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6440-6454},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised outlier detection using memory and contrastive learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning transferable parameters for unsupervised domain
adaptation. <em>TIP</em>, <em>31</em>, 6424–6439. (<a
href="https://doi.org/10.1109/TIP.2022.3184848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) enables a learning machine to adapt from a labeled source domain to an unlabeled target domain under the distribution shift. Thanks to the strong representation ability of deep neural networks, recent remarkable achievements in UDA resort to learning domain-invariant features. Intuitively, the goal is that a good feature representation and the hypothesis learned from the source domain can generalize well to the target domain. However, the learning processes of domain-invariant features and source hypotheses inevitably involve domain-specific information that would degrade the generalizability of UDA models on the target domain. The lottery ticket hypothesis proves that only partial parameters are essential for generalization. Motivated by it, we find in this paper that only partial parameters are essential for learning domain-invariant information. Such parameters are termed transferable parameters that can generalize well in UDA. In contrast, the rest parameters tend to fit domain-specific details and often cause the failure of generalization, which are termed untransferable parameters. Driven by this insight, we propose Transferable Parameter Learning (TransPar) to reduce the side effect of domain-specific information in the learning process and thus enhance the memorization of domain-invariant information. Specifically, according to the distribution discrepancy degree, we divide all parameters into transferable and untransferable ones in each training iteration. We then perform separate update rules for the two types of parameters. Extensive experiments on image classification and regression tasks (keypoint detection) show that TransPar outperforms prior arts by non-trivial margins. Moreover, experiments demonstrate that TransPar can be integrated into the most popular deep UDA networks and be easily extended to handle any data distribution shift scenarios.},
  archive      = {J_TIP},
  author       = {Zhongyi Han and Haoliang Sun and Yilong Yin},
  doi          = {10.1109/TIP.2022.3184848},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6424-6439},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning transferable parameters for unsupervised domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampling agnostic feature representation for long-term
person re-identification. <em>TIP</em>, <em>31</em>, 6412–6423. (<a
href="https://doi.org/10.1109/TIP.2022.3207024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is a problem of identifying individuals across non-overlapping cameras. Although remarkable progress has been made in the re-identification problem, it is still a challenging problem due to appearance variations of the same person as well as other people of similar appearance. Some prior works solved the issues by separating features of positive samples from features of negative ones. However, the performances of existing models considerably depend on the characteristics and statistics of the samples used for training. Thus, we propose a novel framework named sampling independent robust feature representation network (SirNet) that learns disentangled feature embedding from randomly chosen samples. A carefully designed sampling independent maximum discrepancy loss is introduced to model samples of the same person as a cluster. As a result, the proposed framework can generate additional hard negatives/positives using the learned features, which results in better discriminability from other identities. Extensive experimental results on large-scale benchmark datasets verify that the proposed model is more effective than prior state-of-the-art models.},
  archive      = {J_TIP},
  author       = {Seongyeop Yang and Byeongkeun Kang and Yeejin Lee},
  doi          = {10.1109/TIP.2022.3207024},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6412-6423},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sampling agnostic feature representation for long-term person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FindNet: Can you find me? Boundary-and-texture enhancement
network for camouflaged object detection. <em>TIP</em>, <em>31</em>,
6396–6411. (<a href="https://doi.org/10.1109/TIP.2022.3189828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged objects share very similar colors but have different semantics with the surroundings. Cognitive scientists observe that both the global contour (i.e., boundary) and the local pattern (i.e., texture) of camouflaged objects are key cues to help humans find them successfully. Inspired by the cognitive scientist’s observation, we propose a novel boundary-and-texture enhancement network (FindNet) for camouflaged object detection (COD) from single images. Different from most of existing COD methods, FindNet embeds both the boundary-and-texture information into the camouflaged object features. The boundary enhancement (BE) module is leveraged to focus on the global contour of the camouflaged object, and the texture enhancement (TE) module is utilized to focus on the local pattern. The enhanced features from BE and TE, which complement each other, are combined to obtain the final prediction. FindNet performs competently on various conditions of COD, including slightly clear boundaries but very similar textures, fuzzy boundaries but slightly differentiated textures, and simultaneous fuzzy boundaries and textures. Experimental results exhibit clear improvements of FindNet over fifteen state-of-the-art methods on four benchmark datasets, in terms of detection accuracy and boundary clearness. The code will be publicly released.},
  archive      = {J_TIP},
  author       = {Peng Li and Xuefeng Yan and Hongwei Zhu and Mingqiang Wei and Xiao-Ping Zhang and Jing Qin},
  doi          = {10.1109/TIP.2022.3189828},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6396-6411},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FindNet: Can you find me? boundary-and-texture enhancement network for camouflaged object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective multimodal encoding for image paragraph
captioning. <em>TIP</em>, <em>31</em>, 6381–6395. (<a
href="https://doi.org/10.1109/TIP.2022.3211467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a regularization-based image paragraph generation method. We propose a novel multimodal encoding generator (MEG) to generate effective multimodal encoding that captures not only an individual sentence but also visual and paragraph-sequential information. By utilizing the encoding generated by MEG, we regularize a paragraph generation model that allows us to improve the results of the captioning model in all the evaluation metrics. With the support of the proposed MEG model for regularization, our paragraph generation model obtains state-of-the-art results on the Stanford paragraph dataset once further optimized with reinforcement learning. Moreover, we perform extensive empirical analysis on the capabilities of MEG encoding. A qualitative visualization based on t-distributed stochastic neighbor embedding (t-SNE) illustrates that sentence encoding generated by MEG captures some level of semantic information. We also demonstrate that the MEG encoding captures meaningful textual and visual information by performing multimodal sentence retrieval tasks and image instance retrieval given a paragraph query.},
  archive      = {J_TIP},
  author       = {Thanh-Son Nguyen and Basura Fernando},
  doi          = {10.1109/TIP.2022.3211467},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6381-6395},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Effective multimodal encoding for image paragraph captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Narrowing the gap: Improved detector training with noisy
location annotations. <em>TIP</em>, <em>31</em>, 6369–6380. (<a
href="https://doi.org/10.1109/TIP.2022.3211468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods require massive of annotated data for optimizing parameters. For example, datasets attached with accurate bounding box annotations are essential for modern object detection tasks. However, labeling with such pixel-wise accuracy is laborious and time-consuming, and elaborate labeling procedures are indispensable for reducing man-made noise, involving annotation review and acceptance testing. In this paper, we focus on the impact of noisy location annotations on the performance of object detection approaches and aim to, on the user side, reduce the adverse effect of the noise. First, noticeable performance degradation is experimentally observed for both one-stage and two-stage detectors when noise is introduced to the bounding box annotations. For instance, our synthesized noise results in performance decrease from 38.9\% AP to 33.6\% AP for FCOS detector on COCO test split, and 37.8\%AP to 33.7\%AP for Faster R-CNN. Second, a self-correction technique based on a Bayesian filter for prediction ensemble is proposed to better exploit the noisy location annotations following a Teacher-Student learning paradigm. Experiments for both synthesized and real-world scenarios consistently demonstrate the effectiveness of our approach, e.g., our method increases the degraded performance of the FCOS detector from 33.6\% AP to 35.6\% AP on COCO.},
  archive      = {J_TIP},
  author       = {Shaoru Wang and Jin Gao and Bing Li and Weiming Hu},
  doi          = {10.1109/TIP.2022.3211468},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6369-6380},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Narrowing the gap: Improved detector training with noisy location annotations},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperated spectral low-rankness prior and deep spatial
prior for HSI unsupervised denoising. <em>TIP</em>, <em>31</em>,
6356–6368. (<a href="https://doi.org/10.1109/TIP.2022.3211471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-driven methods and data-driven methods have been widely developed for hyperspectral image (HSI) denoising. However, there are pros and cons in both model-driven and data-driven methods. To address this issue, we develop a self-supervised HSI denoising method via integrating model-driven with data-driven strategy. The proposed framework simultaneously cooperates the spectral low-rankness prior and deep spatial prior (SLRP-DSP) for HSI self-supervised denoising. SLRP-DSP introduces the Tucker factorization via orthogonal basis and reduced factor, to capture the global spectral low-rankness prior in HSI. Besides, SLRP-DSP adopts a self-supervised way to learn the deep spatial prior. The proposed method doesn’t need a large number of clean HSIs as the label samples. Through the self-supervised learning, SLRP-DSP can adaptively adjust the deep spatial prior from self-spatial information for reduced spatial factor denoising. An alternating iterative optimization framework is developed to exploit the internal low-rankness prior of third-order tensors and the spatial feature extraction capacity of convolutional neural network. Compared with both existing model-driven methods and data-driven methods, experimental results manifest that the proposed SLRP-DSP outperforms on mixed noise removal in different noisy HSIs.},
  archive      = {J_TIP},
  author       = {Qiang Zhang and Qiangqiang Yuan and Meiping Song and Haoyang Yu and Liangpei Zhang},
  doi          = {10.1109/TIP.2022.3211471},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6356-6368},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cooperated spectral low-rankness prior and deep spatial prior for HSI unsupervised denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Plug-and-play regularization using linear solvers.
<em>TIP</em>, <em>31</em>, 6344–6355. (<a
href="https://doi.org/10.1109/TIP.2022.3211473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been tremendous research on the design of image regularizers over the years, from simple Tikhonov and Laplacian to sophisticated sparsity and CNN-based regularizers. Coupled with a model-based loss function, these are typically used for image reconstruction within an optimization framework. The technical challenge is to develop a regularizer that can accurately model realistic images and be optimized efficiently along with the loss function. Motivated by the recent plug-and-play paradigm for image regularization, we construct a quadratic regularizer whose reconstruction capability is competitive with state-of-the-art regularizers. The novelty of the regularizer is that, unlike classical regularizers, the quadratic objective function is derived from the observed data. Since the regularizer is quadratic, we can reduce the optimization to solving a linear system for applications such as superresolution, deblurring, inpainting, etc. In particular, we show that using iterative Krylov solvers, we can converge to the solution in few iterations, where each iteration requires an application of the forward operator and a linear denoiser. The surprising finding is that we can get close to deep learning methods in terms of reconstruction quality. To the best of our knowledge, the possibility of achieving near state-of-the-art performance using a linear solver is novel.},
  archive      = {J_TIP},
  author       = {Pravin Nair and Kunal N. Chaudhury},
  doi          = {10.1109/TIP.2022.3211473},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6344-6355},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Plug-and-play regularization using linear solvers},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimized dual fire attention network and medium-scale fire
classification benchmark. <em>TIP</em>, <em>31</em>, 6331–6343. (<a
href="https://doi.org/10.1109/TIP.2022.3207006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based fire detection systems have been significantly improved by deep models; however, higher numbers of false alarms and a slow inference speed still hinder their practical applicability in real-world scenarios. For a balanced trade-off between computational cost and accuracy, we introduce dual fire attention network (DFAN) to achieve effective yet efficient fire detection. The first attention mechanism highlights the most important channels from the features of an existing backbone model, yielding significantly emphasized feature maps. Then, a modified spatial attention mechanism is employed to capture spatial details and enhance the discrimination potential of fire and non-fire objects. We further optimize the DFAN for real-world applications by discarding a significant number of extra parameters using a meta-heuristic approach, which yields around 50\% higher FPS values. Finally, we contribute a medium-scale challenging fire classification dataset by considering extremely diverse, highly similar fire/non-fire images and imbalanced classes, among many other complexities. The proposed dataset advances the traditional fire detection datasets by considering multiple classes to answer the following question: what is on fire? We perform experiments on four widely used fire detection datasets, and the DFAN provides the best results compared to 21 state-of-the-art methods. Consequently, our research provides a baseline for fire detection over edge devices with higher accuracy and better FPS values, and the proposed dataset extension provides indoor fire classes and a greater number of outdoor fire classes; these contributions can be used in significant future research. Our codes and dataset will be publicly available at https://github.com/tanveer-hussain/DFAN .},
  archive      = {J_TIP},
  author       = {Hikmat Yar and Tanveer Hussain and Mohit Agarwal and Zulfiqar Ahmad Khan and Suneet Kumar Gupta and Sung Wook Baik},
  doi          = {10.1109/TIP.2022.3207006},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6331-6343},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimized dual fire attention network and medium-scale fire classification benchmark},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging geometric structure for label-efficient
semi-supervised scene segmentation. <em>TIP</em>, <em>31</em>,
6320–6330. (<a href="https://doi.org/10.1109/TIP.2022.3208735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label-efficient scene segmentation aims to achieve effective per-pixel classification with reduced labeling effort. Recent approaches for this task focus on leveraging unlabelled images by formulating consistency regularization or pseudo labels for individual pixels. Yet most of these methods ignore the 3D geometric structures naturally conveyed by image scenes, which is free for enhancing training segmentation models with better discrimination of image details. In this work, we present a novel Geometric Structure Refinement (GSR) framework to explicitly exploit the geometric structures of image scenes to enhance the semi-supervised training of segmentation models. In the training phase, we generate initial dense pseudo labels based on fast and coarse annotations, and then utilize the free unsupervised 3D reconstruction of the image scene to calibrate the dense pseudo labels with more reliable details. With the calibrated pseudo groundtruth, we are able to conveniently train any existing image segmentation models without increasing the costs of annotations or modifying the models’ architectures. Moreover, we explore different strategies for allocating labeling effort in semi-supervised scene segmentation, and find that a combination of finely-labeled samples and coarsely-labeled samples performs better than the traditional dense-fine only annotations. Extensive experiments on datasets including Cityscapes and KITTI are conducted to evaluate our proposed methods. The results demonstrate that GSR can be easily applied to boost the performance of existing models like PSPNet, DeepLabv3+, etc with reduced annotations. With half of the annotation effort, GSR achieves 99\% of the accuracy of its fully supervised state-of-the-art counterparts.},
  archive      = {J_TIP},
  author       = {Ping Hu and Stan Sclaroff and Kate Saenko},
  doi          = {10.1109/TIP.2022.3208735},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6320-6330},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Leveraging geometric structure for label-efficient semi-supervised scene segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-guided collaborative counting. <em>TIP</em>,
<em>31</em>, 6306–6319. (<a
href="https://doi.org/10.1109/TIP.2022.3207584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing crowd counting designs usually exploit multi-branch structures to address the scale diversity problem. However, branches in these structures work in a competitive rather than collaborative way. In this paper, we focus on promoting collaboration between branches. Specifically, we propose an attention-guided collaborative counting module (AGCCM) comprising an attention-guided module (AGM) and a collaborative counting module (CCM). The CCM promotes collaboration among branches by recombining each branch’s output into an independent count and joint counts with other branches. The AGM capturing the global attention map through a transformer structure with a pair of foreground-background related loss functions can distinguish the advantages of different branches. The loss functions do not require additional labels and crowd division. In addition, we design two kinds of bidirectional transformers (Bi-Transformers) to decouple the global attention to row attention and column attention. The proposed Bi-Transformers are able to reduce the computational complexity and handle images in any resolution without cropping the image into small patches. Extensive experiments on several public datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art crowd counting methods.},
  archive      = {J_TIP},
  author       = {Hong Mo and Wenqi Ren and Xiong Zhang and Feihu Yan and Zhong Zhou and Xiaochun Cao and Wei Wu},
  doi          = {10.1109/TIP.2022.3207584},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6306-6319},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention-guided collaborative counting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly-supervised salient object detection on light fields.
<em>TIP</em>, <em>31</em>, 6295–6305. (<a
href="https://doi.org/10.1109/TIP.2022.3207605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing salient object detection (SOD) methods are designed for RGB images and do not take advantage of the abundant information provided by light fields. Hence, they may fail to detect salient objects of complex structures and delineate their boundaries. Although some methods have explored multi-view information of light field images for saliency detection, they require tedious pixel-level manual annotations of ground truths. In this paper, we propose a novel weakly-supervised learning framework for salient object detection on light field images based on bounding box annotations. Our method has two major novelties. First, given an input light field image and a bounding-box annotation indicating the salient object, we propose a ground truth label hallucination method to generate a pixel-level pseudo saliency map, to avoid heavy cost of pixel-level annotations. This method generates high quality pseudo ground truth saliency maps to help supervise the training, by exploiting information obtained from the light field (including depths and RGB images). Second, to exploit the multi-view nature of the light field data in learning, we propose a fusion attention module to calibrate the spatial and channel-wise light field representations. It learns to focus on informative features and suppress redundant information from the multi-view inputs. Based on these two novelties, we are able to train a new salient object detector with two branches in a weakly-supervised manner. While the RGB branch focuses on modeling the color contrast in the all-in-focus image for locating the salient objects, the Focal branch exploits the depth and the background spatial redundancy of focal slices for eliminating background distractions. Extensive experiments show that our method outperforms existing weakly-supervised methods and most fully supervised methods.},
  archive      = {J_TIP},
  author       = {Zijian Liang and Pengjie Wang and Ke Xu and Pingping Zhang and Rynson W. H. Lau},
  doi          = {10.1109/TIP.2022.3207605},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6295-6305},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly-supervised salient object detection on light fields},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Multiframe joint enhancement for early interlaced videos.
<em>TIP</em>, <em>31</em>, 6282–6294. (<a
href="https://doi.org/10.1109/TIP.2022.3207003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early interlaced videos usually contain multiple and interlacing and complex compression artifacts, which significantly reduce the visual quality. Although the high-definition reconstruction technology for early videos has made great progress in recent years, related research on deinterlacing is still lacking. Traditional methods mainly focus on simple interlacing mechanism, and cannot deal with the complex artifacts in real-world early videos. Recent interlaced video reconstruction deep deinterlacing models only focus on single frame, while neglecting important temporal information. Therefore, this paper proposes a multiframe deinterlacing network joint enhancement network for early interlaced videos that consists of three modules, i.e., spatial vertical interpolation module, temporal alignment and fusion module, and final refinement module. The proposed method can effectively remove the complex artifacts in early videos by using temporal redundancy of multi-fields. Experimental results demonstrate that the proposed method can recover high quality results for both synthetic dataset and real-world early interlaced videos. At the same time, the method also won the first place in the MSU Deinterlacer Benchmark. The code is available at: https://github.com/anymyb/MFDIN .},
  archive      = {J_TIP},
  author       = {Yang Zhao and Yanbo Ma and Yuan Chen and Wei Jia and Ronggang Wang and Xiaoping Liu},
  doi          = {10.1109/TIP.2022.3207003},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6282-6294},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiframe joint enhancement for early interlaced videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end optimized 360° image compression. <em>TIP</em>,
<em>31</em>, 6267–6281. (<a
href="https://doi.org/10.1109/TIP.2022.3208429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 360° image that offers a 360-degree scenario of the world is widely used in virtual reality and has drawn increasing attention. In 360° image compression, the spherical image is first transformed into a planar image with a projection such as equirectangular projection (ERP) and then saved with the existing codecs. The ERP images that represent different circles of latitude with the same number of pixels suffer from the unbalance sampling problem, resulting in inefficiency using planar compression methods, especially for the deep neural network (DNN) based codecs. To tackle this problem, we introduce a latitude adaptive coding scheme for DNNs by allocating variant numbers of codes for different regions according to the latitude on the sphere. Specifically, taking both the number of allocated codes for each region and their entropy into consideration, we introduce a flexible regional adaptive rate loss for region-wise rate controlling. Latitude adaptive constraints are then introduced to prevent spending too many codes on the over-sampling regions. Furthermore, we introduce viewport-based distortion loss by calculating the average distortion on a set of viewports. We optimize and test our model on a large 360° dataset containing 19,790 images collected from the Internet. The experiment results demonstrate the superiority of the proposed latitude adaptive coding scheme. On the whole, our model outperforms the existing image compression standards, including JPEG, JPEG2000, HEVC Intra Coding, and VVC Intra Coding, and helps to save around 15\% bits compared to the baseline learned image compression model for planar images.},
  archive      = {J_TIP},
  author       = {Mu Li and Jinxing Li and Shuhang Gu and Feng Wu and David Zhang},
  doi          = {10.1109/TIP.2022.3208429},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6267-6281},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end optimized 360° image compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delving deeper into mask utilization in video object
segmentation. <em>TIP</em>, <em>31</em>, 6255–6266. (<a
href="https://doi.org/10.1109/TIP.2022.3208409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the mask utilization of video object segmentation (VOS). The mask here mains the reference masks in the memory bank, i.e., several chosen high-quality predicted masks, which are usually used with the reference frames together. The reference masks depict the edge and contour features of the target object and indicate the boundary of the target against the background, while the reference frames contain the raw RGB information of the whole image. It is obvious that the reference masks could play a significant role in the VOS, but this is not well explored yet. To tackle this, we propose to investigate the mask advantages of both the encoder and the matcher. For the encoder, we provide a unified codebase to integrate and compare eight different mask-fused encoders. Half of them are inherited or summarized from existing methods, and the other half are devised by ourselves. We find the best configuration from our design and give valuable observations from the comparison. Then, we propose a new mask-enhanced matcher to reduce the background distraction and enhance the locality of the matching process. Combining the mask-fused encoder, mask-enhanced matcher and a standard decoder, we formulate a new architecture named MaskVOS, which sufficiently exploits the mask benefits for VOS. Qualitative and quantitative results demonstrate the effectiveness of our method. We hope our exploration could raise the attention of mask utilization in VOS.},
  archive      = {J_TIP},
  author       = {Mengmeng Wang and Jianbiao Mei and Lina Liu and Guanzhong Tian and Yong Liu and Zaisheng Pan},
  doi          = {10.1109/TIP.2022.3208409},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6255-6266},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Delving deeper into mask utilization in video object segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond greedy search: Tracking by multi-agent reinforcement
learning-based beam search. <em>TIP</em>, <em>31</em>, 6239–6254. (<a
href="https://doi.org/10.1109/TIP.2022.3208437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To track the target in a video, current visual trackers usually adopt greedy search for target object localization in each frame, that is, the candidate region with the maximum response score will be selected as the tracking result of each frame. However, we found that this may be not an optimal choice, especially when encountering challenging tracking scenarios such as heavy occlusion and fast motion. In particular, if a tracker drifts, errors will be accumulated and would further make response scores estimated by the tracker unreliable in future frames. To address this issue, we propose to maintain multiple tracking trajectories and apply beam search strategy for visual tracking, so that the trajectory with fewer accumulated errors can be identified. Accordingly, this paper introduces a novel multi-agent reinforcement learning based beam search tracking strategy, termed BeamTracking. It is mainly inspired by the image captioning task, which takes an image as input and generates diverse descriptions using beam search algorithm. Accordingly, we formulate the tracking as a sample selection problem fulfilled by multiple parallel decision-making processes, each of which aims at picking out one sample as their tracking result in each frame. Each maintained trajectory is associated with an agent to perform the decision-making and determine what actions should be taken to update related information. More specifically, using the classification-based tracker as the baseline, we first adopt bi-GRU to encode the target feature, proposal feature, and its response score into a unified state representation. The state feature and greedy search result are then fed into the first agent for independent action selection. Afterwards, the output action and state features are fed into the subsequent agent for diverse results prediction. When all the frames are processed, we select the trajectory with the maximum accumulated score as the tracking result. Extensive experiments on seven popular tracking benchmark datasets validated the effectiveness of the proposed algorithm.},
  archive      = {J_TIP},
  author       = {Xiao Wang and Zhe Chen and Bo Jiang and Jin Tang and Bin Luo and Dacheng Tao},
  doi          = {10.1109/TIP.2022.3208437},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6239-6254},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Beyond greedy search: Tracking by multi-agent reinforcement learning-based beam search},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrast-reconstruction representation learning for
self-supervised skeleton-based action recognition. <em>TIP</em>,
<em>31</em>, 6224–6238. (<a
href="https://doi.org/10.1109/TIP.2022.3207577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is widely used in varied areas, e.g., surveillance and human-machine interaction. Existing models are mainly learned in a supervised manner, thus heavily depending on large-scale labeled data, which could be infeasible when labels are prohibitively expensive. In this paper, we propose a novel Contrast-Reconstruction Representation Learning network (CRRL) that simultaneously captures postures and motion dynamics for unsupervised skeleton-based action recognition. It consists of three parts: Sequence Reconstructor (SER), Contrastive Motion Learner (CML), and Information Fuser (INF). SER learns representation from skeleton coordinate sequence via reconstruction. However the learned representation tends to focus on trivial postural coordinates and be hesitant in motion learning. To enhance the learning of motions, CML performs contrastive learning between the representation learned from coordinate sequences and additional velocity sequences, respectively. Finally, in the INF module, we explore varied strategies to combine SER and CML, and propose to couple postures and motions via a knowledge-distillation based fusion strategy which transfers the motion learning from CML to SER. Experimental results on several benchmarks, i.e., NTU RGB+D 60/120, PKU-MMD, CMU, and NW-UCLA, demonstrate the promise of the our method by outperforming state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Peng Wang and Jun Wen and Chenyang Si and Yuntao Qian and Liang Wang},
  doi          = {10.1109/TIP.2022.3207577},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6224-6238},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contrast-reconstruction representation learning for self-supervised skeleton-based action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-scale single image dehazing via neural augmentation.
<em>TIP</em>, <em>31</em>, 6213–6223. (<a
href="https://doi.org/10.1109/TIP.2022.3207571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based single image dehazing algorithms restore haze-free images with sharp edges and rich details for real-world hazy images at the expense of low PSNR and SSIM values for synthetic hazy images. Data-driven ones restore haze-free images with high PSNR and SSIM values for synthetic hazy images but with low contrast, and even some remaining haze for real-world hazy images. In this paper, a novel single image dehazing algorithm is introduced by combining model-based and data-driven approaches. Both transmission map and atmospheric light are first estimated by the model-based methods, and then refined by dual-scale generative adversarial networks (GANs) based approaches. The resultant algorithm forms a neural augmentation which converges very fast while the corresponding data-driven approach might not converge. Haze-free images are restored by using the estimated transmission map and atmospheric light as well as the Koschmieder’s law. Experimental results indicate that the proposed algorithm can remove haze well from real-world and synthetic hazy images.},
  archive      = {J_TIP},
  author       = {Zhengguo Li and Chaobing Zheng and Haiyan Shu and Shiqian Wu},
  doi          = {10.1109/TIP.2022.3207571},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6213-6223},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-scale single image dehazing via neural augmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boundary TextSpotter: Toward arbitrary-shaped scene text
spotting. <em>TIP</em>, <em>31</em>, 6200–6212. (<a
href="https://doi.org/10.1109/TIP.2022.3206615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reading arbitrary-shaped text in an end-to-end fashion has received particularly growing interested in computer vision. In this paper, we study the problem of scene text spotting, which aims to detect and recognize text from cluttered images simultaneously and propose an end-to-end trainable neural network named Boundary TextSpotter. Different from existing methods that describe the shape of text instance with bounding box or shape mask, Boundary TextSpotter formulates it as a set of boundary points. Besides, the representation of such boundary points provides the order of reading text. Benefiting from the representation on both detection and recognition, Boundary TextSpotter can easily deal with the text of arbitrary shapes. Further, to efficiently detect the boundary points of the text, a single-stage text detector is proposed, which can almost perform at a real-time speed. Experiments on three challenging datasets, including ICDAR2015, Total-Text and CTW1500 demonstrate that the proposed method achieves state-of-the-art or competitive results, meanwhile significantly improving the inference speed.},
  archive      = {J_TIP},
  author       = {Pu Lu and Hao Wang and Shenggao Zhu and Jing Wang and Xiang Bai and Wenyu Liu},
  doi          = {10.1109/TIP.2022.3206615},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6200-6212},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boundary TextSpotter: Toward arbitrary-shaped scene text spotting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional feature embedding by visual clue correspondence
graph for person re-identification. <em>TIP</em>, <em>31</em>,
6188–6199. (<a href="https://doi.org/10.1109/TIP.2022.3206617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Person Re-Identification has made impressive progress, difficult cases like occlusion, change of view-point, and similar clothing still bring great challenges. In order to tackle these challenges, extracting discriminative feature representation is crucial. Most of the existing methods focus on extracting ReID features from individual images separately. However, when matching two images, we propose that the ReID features of a query image should be dynamically adjusted based on the contextual information from the gallery image it matches. We call this type of ReID features conditional feature embedding. In this paper, we propose a novel ReID framework that extracts conditional feature embedding based on the aligned visual clues between image pairs, called Clue Alignment based Conditional Embedding (CACE-Net). CACE-Net applies an attention module to build a detailed correspondence graph between crucial visual clues in image pairs and uses discrepancy-based GCN to embed the obtained complex correspondence information into the conditional features. The experiments show that CACE-Net achieves state-of-the-art performance on three public datasets},
  archive      = {J_TIP},
  author       = {Fufu Yu and Xinyang Jiang and Yifei Gong and Wei-Shi Zheng and Feng Zheng and Xing Sun},
  doi          = {10.1109/TIP.2022.3206617},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6188-6199},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conditional feature embedding by visual clue correspondence graph for person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Screen content video quality assessment model using hybrid
spatiotemporal features. <em>TIP</em>, <em>31</em>, 6175–6187. (<a
href="https://doi.org/10.1109/TIP.2022.3206621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a full-reference video quality assessment (VQA) model is designed for the perceptual quality assessment of the screen content videos (SCVs), called the hybrid spatiotemporal feature-based model (HSFM). The SCVs are of hybrid structure including screen and natural scenes, which are perceived by the human visual system (HVS) with different visual effects. With this consideration, the three dimensional Laplacian of Gaussian (3D-LOG) filter and three dimensional Natural Scene Statistics (3D-NSS) are exploited to extract the screen and natural spatiotemporal features, based on the reference and distorted SCV sequences separately. The similarities of these extracted features are then computed independently, followed by generating the distorted screen and natural quality scores for screen and natural scenes. After that, an adaptive screen and natural quality fusion scheme through the local video activity is developed to combine them for arriving at the final VQA score of the distorted SCV under evaluation. The experimental results on the Screen Content Video Database (SCVD) and Compressed Screen Content Video Quality (CSCVQ) databases have shown that the proposed HSFM is more in line with the perceptual quality assessment of the SCVs perceived by the HVS, compared with a variety of classic and latest IQA/VQA models.},
  archive      = {J_TIP},
  author       = {Huanqiang Zeng and Hailiang Huang and Junhui Hou and Jiuwen Cao and Yongtao Wang and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2022.3206621},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6175-6187},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Screen content video quality assessment model using hybrid spatiotemporal features},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A light field FDL-HCGH feature in scale-disparity space.
<em>TIP</em>, <em>31</em>, 6164–6174. (<a
href="https://doi.org/10.1109/TIP.2022.3202099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computer vision applications rely on feature detection and description, hence the need for computationally efficient and robust 4D light field (LF) feature detectors and descriptors. In this paper, we propose a novel light field feature descriptor based on the Fourier disparity layer representation, for light field imaging applications. After the Harris feature detection in a scale-disparity space, the proposed feature descriptor is then extracted using a circular neighborhood rather than a square neighborhood. It is shown to yield more accurate feature matching, compared with the LiFF LF feature, with a lower computational complexity. In order to evaluate the feature matching performance with the proposed descriptor, we generated a synthetic stereo LF dataset with ground truth matching points. Experimental results with synthetic and real-world dataset show that our solution outperforms existing methods in terms of both feature detection robustness and feature matching accuracy.},
  archive      = {J_TIP},
  author       = {Meng Zhang and Haiyan Jin and Zhaolin Xiao and Christine Guillemot},
  doi          = {10.1109/TIP.2022.3202099},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6164-6174},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A light field FDL-HCGH feature in scale-disparity space},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring spatial correlation for light field saliency
detection: Expansion from a single view. <em>TIP</em>, <em>31</em>,
6152–6163. (<a href="https://doi.org/10.1109/TIP.2022.3205749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous 2D saliency detection methods extract salient cues from a single view and directly predict the expected results. Both traditional and deep-learning-based 2D methods do not consider geometric information of 3D scenes. Therefore the relationship between scene understanding and salient objects cannot be effectively established. This limits the performance of 2D saliency detection in challenging scenes. In this paper, we show for the first time that saliency detection problem can be reformulated as two sub-problems: light field synthesis from a single view and light-field-driven saliency detection. This paper first introduces a high-quality light field synthesis network to produce reliable 4D light field information. Then a novel light-field-driven saliency detection network is proposed, in which a Direction-specific Screening Unit (DSU) is tailored to exploit the spatial correlation among multiple viewpoints. The whole pipeline can be trained in an end-to-end fashion. Experimental results demonstrate that the proposed method outperforms the state-of-the-art 2D, 3D and 4D saliency detection methods. Our code is publicly available at https://github.com/OIPLab-DUT/ESCNet .},
  archive      = {J_TIP},
  author       = {Miao Zhang and Shuang Xu and Yongri Piao and Huchuan Lu},
  doi          = {10.1109/TIP.2022.3205749},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6152-6163},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring spatial correlation for light field saliency detection: Expansion from a single view},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). No-reference image quality assessment by hallucinating
pristine features. <em>TIP</em>, <em>31</em>, 6139–6151. (<a
href="https://doi.org/10.1109/TIP.2022.3205770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a no-reference (NR) image quality assessment (IQA) method via feature level pseudo-reference (PR) hallucination. The proposed quality assessment framework is rooted in the view that the perceptually meaningful features could be well exploited to characterize the visual quality, and the natural image statistical behaviors are exploited in an effort to deliver the accurate predictions. Herein, the PR features from the distorted images are learned by a mutual learning scheme with the pristine reference as the supervision, and the discriminative characteristics of PR features are further ensured with the triplet constraints. Given a distorted image for quality inference, the feature level disentanglement is performed with an invertible neural layer for final quality prediction, leading to the PR and the corresponding distortion features for comparison. The effectiveness of our proposed method is demonstrated on four popular IQA databases, and superior performance on cross-database evaluation also reveals the high generalization capability of our method. The implementation of our method is publicly available on https://github.com/Baoliang93/FPR .},
  archive      = {J_TIP},
  author       = {Baoliang Chen and Lingyu Zhu and Chenqi Kong and Hanwei Zhu and Shiqi Wang and Zhu Li},
  doi          = {10.1109/TIP.2022.3205770},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6139-6151},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference image quality assessment by hallucinating pristine features},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving RGB-d salient object detection via modality-aware
decoder. <em>TIP</em>, <em>31</em>, 6124–6138. (<a
href="https://doi.org/10.1109/TIP.2022.3205747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing RGB-D salient object detection (SOD) methods are primarily focusing on cross-modal and cross-level saliency fusion, which has been proved to be efficient and effective. However, these methods still have a critical limitation, i.e., their fusion patterns – typically the combination of selective characteristics and its variations, are too highly dependent on the network’s non-linear adaptability. In such methods, the balances between RGB and D (Depth) are formulated individually considering the intermediate feature slices, but the relation at the modality level may not be learned properly. The optimal RGB-D combinations differ depending on the RGB-D scenarios, and the exact complementary status is frequently determined by multiple modality-level factors, such as D quality, the complexity of the RGB scene, and degree of harmony between them. Therefore, given the existing approaches, it may be difficult for them to achieve further performance breakthroughs, as their methodologies belong to some methods that are somewhat less modality sensitive. To conquer this problem, this paper presents the Modality-aware Decoder (MaD). The critical technical innovations include a series of feature embedding, modality reasoning, and feature back-projecting and collecting strategies, all of which upgrade the widely-used multi-scale and multi-level decoding process to be modality-aware. Our MaD achieves competitive performance over other state-of-the-art (SOTA) models without using any fancy tricks in the decoder’s design. Codes and results will be publicly available at https://github.com/MengkeSong/MaD .},
  archive      = {J_TIP},
  author       = {Mengke Song and Wenfeng Song and Guowei Yang and Chenglizhao Chen},
  doi          = {10.1109/TIP.2022.3205747},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6124-6138},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving RGB-D salient object detection via modality-aware decoder},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale-consistent fusion: From heterogeneous local sampling
to global immersive rendering. <em>TIP</em>, <em>31</em>, 6109–6123. (<a
href="https://doi.org/10.1109/TIP.2022.3205745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based geometric modeling and novel view synthesis based on sparse large-baseline samplings are challenging but important tasks for emerging multimedia applications such as virtual reality and immersive telepresence. Existing methods fail to produce satisfactory results due to the limitation on inferring reliable depth information over such challenging reference conditions. With the popularization of commercial light field (LF) cameras, capturing LF images (LFIs) is as convenient as taking regular photos, and geometry information can be reliably inferred. This inspires us to use a sparse set of LF captures to render high-quality novel views globally. However, the fusion of LF captures from multiple angles is challenging due to the scale inconsistency caused by various capture settings. To overcome this challenge, we propose a novel scale-consistent volume rescaling algorithm that robustly aligns the disparity probability volumes (DPV) among different captures for scale-consistent global geometry fusion. Based on the fused DPV projected to the target camera frustum, novel learning-based modules (i.e., the attention-guided multi-scale residual fusion module, and the disparity field-guided deep re-regularization module), which comprehensively regularize noisy observations from heterogeneous captures for high-quality rendering of novel LFIs, have been proposed. Both quantitative and qualitative experiments over the Stanford Lytro Multi-view LF dataset show that the proposed method outperforms state-of-the-art methods significantly under different experiment settings for disparity inference and LF synthesis.},
  archive      = {J_TIP},
  author       = {Wenpeng Xing and Jie Chen and Zaifeng Yang and Qiang Wang and Yike Guo},
  doi          = {10.1109/TIP.2022.3205745},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6109-6123},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scale-consistent fusion: From heterogeneous local sampling to global immersive rendering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional feature learning based transformer for
text-based person search. <em>TIP</em>, <em>31</em>, 6097–6108. (<a
href="https://doi.org/10.1109/TIP.2022.3205216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims at retrieving the target person in an image gallery using a descriptive sentence of that person. The core of this task is to calculate a similarity score between the pedestrian image and description, which requires inferring the complex latent correspondence between image sub-regions and textual phrases at different scales. Transformer is an intuitive way to model the complex alignment by its self-attention mechanism. Most previous Transformer-based methods simply concatenate image region features and text features as input and learn a cross-modal representation in a brute force manner. Such weakly supervised learning approaches fail to explicitly build alignment between image region features and text features, causing an inferior feature distribution. In this paper, we present CFLT, Conditional Feature Learning based Transformer. It maps the sub-regions and phrases into a unified latent space and explicitly aligns them by constructing conditional embeddings where the feature of data from one modality is dynamically adjusted based on the data from the other modality. The output of our CFLT is a set of similarity scores for each sub-region or phrase rather than a cross-modal representation. Furthermore, we propose a simple and effective multi-modal re-ranking method named Re-ranking scheme by Visual Conditional Feature (RVCF). Benefit from the visual conditional feature and better feature distribution in our CFLT, the proposed RVCF achieves significant performance improvement. Experimental results show that our CFLT outperforms the state-of-the-art methods by 7.03\% in terms of top-1 accuracy and 5.01\% in terms of top-5 accuracy on the text-based person search dataset.},
  archive      = {J_TIP},
  author       = {Chenyang Gao and Guanyu Cai and Xinyang Jiang and Feng Zheng and Jun Zhang and Yifei Gong and Fangzhou Lin and Xing Sun and Xiang Bai},
  doi          = {10.1109/TIP.2022.3205216},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6097-6108},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conditional feature learning based transformer for text-based person search},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An object point set inductive tracker for multi-object
tracking and segmentation. <em>TIP</em>, <em>31</em>, 6083–6096. (<a
href="https://doi.org/10.1109/TIP.2022.3203607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking and segmentation (MOTS) is a derivative task of multi-object tracking (MOT). The new setting encourages the learning of more discriminative high-quality embeddings. In this paper, we focus on the problem of exploring the relationship between the segmenter and the tracker, and propose an efficient Object Point set Inductive Tracker (OPITrack) based on it. First, we discover that after a single attention layer, the high-dimensional, key point embedding will show feature averaging. To alleviate this phenomenon, we propose an embedding generalization training strategy for sparse training and dense testing. This strategy allows the network to increase randomness in training and encourages the tracker to learn more discriminative features. In addition, to learn the desired embedding space, we propose a general Trip-hard sample augmentation loss. The loss uses patches that are not distinguishable by the segmenter to join the feature learning and force the embedding network to learn the difference between false positives and true positives. Our method was validated on two MOTS benchmark datasets and achieved promising results. In addition, our OPITrack can achieve better performance for the raw model while costing less video memory (VRAM) at training time.},
  archive      = {J_TIP},
  author       = {Yan Gao and Haojun Xu and Yu Zheng and Jie Li and Xinbo Gao},
  doi          = {10.1109/TIP.2022.3203607},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6083-6096},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An object point set inductive tracker for multi-object tracking and segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-aware extended star-tetrix transforms for CFA-sampled
raw camera image compression. <em>TIP</em>, <em>31</em>, 6072–6082. (<a
href="https://doi.org/10.1109/TIP.2022.3205470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Codecs using spectral-spatial transforms efficiently compress raw camera images captured with a color filter array (CFA-sampled raw images) by changing their RGB color space into a decorrelated color space. This study describes two types of spectral-spatial transform, called extended Star-Tetrix transforms (XSTTs), and their edge-aware versions, called edge-aware XSTTs (EXSTTs), with no extra bits (side information) and little extra complexity. They are obtained by (i) extending the Star-Tetrix transform (STT), which is one of the latest spectral-spatial transforms, to a new version of our previously proposed wavelet-based spectral-spatial transform and a simpler version; (ii) considering that each 2D predict step of the wavelet transform is a combination of two 1D diagonal or horizontal-vertical transforms; (iii) weighting the transforms along the edge directions in the images. Compared with XSTTs, the EXSTTs can decorrelate CFA-sampled raw images well: they reduce the difference in energy between the two green components by about 3.38–30.08\% for high-quality camera images and 8.97–14.47\% for mobile phone images. The experiments on JPEG 2000-based lossless and lossy compression of CFA-sampled raw images show better performance than conventional methods. For high-quality camera images, the XSTTs/EXSTTs produce results equal to or better than the conventional methods: especially for images with many edges, the type-I EXSTT improves them by about 0.03–0.19 bpp in average lossless bitrate and the XSTTs improve them by about 0.16–0.96 dB in average Bjøntegaard delta peak signal-to-noise ratio. For mobile phone images, our previous work perform the best, whereas the XSTTs/EXSTTs show similar trends to the case of high-quality camera images.},
  archive      = {J_TIP},
  author       = {Taizo Suzuki and Liping Huang},
  doi          = {10.1109/TIP.2022.3205470},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6072-6082},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Edge-aware extended star-tetrix transforms for CFA-sampled raw camera image compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph convolution RPCA with adaptive graph. <em>TIP</em>,
<em>31</em>, 6062–6071. (<a
href="https://doi.org/10.1109/TIP.2022.3195669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is warmly welcomed in dimensionality reduction and its applications. Due to the high sensitivity of PCA to outliers, a series of PCA methods are proposed to enhance the robustness of PCA. Besides, the representation ability of the existing PCA methods has limitations as well. To enhance the robustness and representation ability of robust PCA, we elaborate a novel Graph Convolution Robust PCA method (GRPCA) to incorporate the manifold structure into PCA. It constructs a sparse graph based on the local connectivity structure of samples. Graph auto-encoder is utilized to solve the robust PCA problem under the low-rank and sparse constraints. With the dual-decoder, GRPCA learns the low-dimensional embeddings that reconstruct the manifold structure and low-rank approximation simultaneously. Furthermore, since the graph suffers from misconnection triggered by occlusions, the local connectivity structure of low-dimensional embeddings is utilized to modify the graph. Our proposed method excels in both the clustering of low-dimensional embeddings and the low-rank recovery. Lastly, extensive experiments conducted on six real-world datasets demonstrated the efficiency and superiority of the proposed GRPCA.},
  archive      = {J_TIP},
  author       = {Rui Zhang and Wenlin Zhang and Pei Li and Xuelong Li},
  doi          = {10.1109/TIP.2022.3195669},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6062-6071},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph convolution RPCA with adaptive graph},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VideoABC: A real-world video dataset for abductive visual
reasoning. <em>TIP</em>, <em>31</em>, 6048–6061. (<a
href="https://doi.org/10.1109/TIP.2022.3205207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of abductive visual reasoning (AVR), which requires vision systems to infer the most plausible explanation for visual observations. Unlike previous work which performs visual reasoning on static images or synthesized scenes, we exploit long-term reasoning from instructional videos that contain a wealth of detailed information about the physical world. We conceptualize two tasks for this emerging and challenging topic. The primary task is AVR, which is based on the initial configuration and desired goal from an instructional video, and the model is expected to figure out what is the most plausible sequence of steps to achieve the goal. In order to avoid trivial solutions based on appearance information rather than reasoning, the second task called AVR++ is constructed, which requires the model to answer why the unselected options are less plausible. We introduce a new dataset called VideoABC, which consists of 46,354 unique steps derived from 11,827 instructional videos, formulated as 13,526 abductive reasoning questions with an average reasoning duration of 51 seconds. Through an adversarial hard hypothesis mining algorithm, non-trivial and high-quality problems are generated efficiently and effectively. To achieve human-level reasoning, we propose a Hierarchical Dual Reasoning Network (HDRNet) to capture the long-term dependencies among steps and observations. We establish a benchmark for abductive visual reasoning, and our method set state-of-the-arts on AVR ( $\sim 74$\%) and AVR++ ( $\sim 45$\%), and humans can easily achieve over 90\% accuracy on these two tasks. The large performance gap reveals the limitation of current video understanding models on temporal reasoning and leaves substantial room for future research on this challenging problem. Our dataset and code are available at https://github.com/wl-zhao/VideoABC .},
  archive      = {J_TIP},
  author       = {Wenliang Zhao and Yongming Rao and Yansong Tang and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TIP.2022.3205207},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6048-6061},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VideoABC: A real-world video dataset for abductive visual reasoning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Video crowd localization with multifocus gaussian
neighborhood attention and a large-scale benchmark. <em>TIP</em>,
<em>31</em>, 6032–6047. (<a
href="https://doi.org/10.1109/TIP.2022.3205210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video crowd localization is a crucial yet challenging task, which aims to estimate exact locations of human heads in the given crowded videos. To model spatial-temporal dependencies of human mobility, we propose a multi-focus Gaussian neighborhood attention (GNA), which can effectively exploit long-range correspondences while maintaining the spatial topological structure of the input videos. In particular, our GNA can also capture the scale variation of human heads well using the equipped multi-focus mechanism. Based on the multi-focus GNA, we develop a unified neural network called GNANet to accurately locate head centers in video clips by fully aggregating spatial-temporal information via a scene modeling module and a context cross-attention module. Moreover, to facilitate future researches in this field, we introduce a large-scale crowd video benchmark named VSCrowd ( https://github.com/HopLee6/VSCrowd ), which consists of 60K+ frames captured in various surveillance scenes and 2M+ head annotations. Finally, we conduct extensive experiments on three datasets including our VSCrowd, and the experiment results show that the proposed method is capable to achieve state-of-the-art performance for both video crowd localization and counting.},
  archive      = {J_TIP},
  author       = {Haopeng Li and Lingbo Liu and Kunlin Yang and Shinan Liu and Junyu Gao and Bin Zhao and Rui Zhang and Jun Hou},
  doi          = {10.1109/TIP.2022.3205210},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6032-6047},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video crowd localization with multifocus gaussian neighborhood attention and a large-scale benchmark},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SR-GNN: Spatial relation-aware graph neural network for
fine-grained image categorization. <em>TIP</em>, <em>31</em>, 6017–6031.
(<a href="https://doi.org/10.1109/TIP.2022.3205215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, a significant progress has been made in deep convolutional neural networks (CNNs)-based image recognition. This is mainly due to the strong ability of such networks in mining discriminative object pose and parts information from texture and shape. This is often inappropriate for fine-grained visual classification (FGVC) since it exhibits high intra-class and low inter-class variances due to occlusions, deformation, illuminations, etc. Thus, an expressive feature representation describing global structural information is a key to characterize an object/scene. To this end, we propose a method that effectively captures subtle changes by aggregating context-aware features from most relevant image-regions and their importance in discriminating fine-grained categories avoiding the bounding-box and/or distinguishable part annotations. Our approach is inspired by the recent advancement in self-attention and graph neural networks (GNNs) approaches to include a simple yet effective relation-aware feature transformation and its refinement using a context-aware attention mechanism to boost the discriminability of the transformed feature in an end-to-end learning process. Our model is evaluated on eight benchmark datasets consisting of fine-grained objects and human-object interactions. It outperforms the state-of-the-art approaches by a significant margin in recognition accuracy.},
  archive      = {J_TIP},
  author       = {Asish Bera and Zachary Wharton and Yonghuai Liu and Nik Bessis and Ardhendu Behera},
  doi          = {10.1109/TIP.2022.3205215},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6017-6031},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SR-GNN: Spatial relation-aware graph neural network for fine-grained image categorization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PointRas: Uncertainty-aware multi-resolution learning for
point cloud segmentation. <em>TIP</em>, <em>31</em>, 6002–6016. (<a
href="https://doi.org/10.1109/TIP.2022.3205208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an uncertainty-aware multi-resolution learning for point cloud segmentation, named PointRas. Most existing works for point cloud segmentation design encoder networks to obtain better representation of local space in point cloud. However, few of them investigate the utilization of features in the lower resolutions produced by encoders and consider the contextual learning between various resolutions in decoder network. To address this, we propose to utilize the descriptive characteristic of point clouds in the lower resolutions. Taking reference to core steps of rasterization in 2D graphics where the properties of pixels in high density are interpolated from a few primitive shapes in rasterization rendering, we use the similar strategy where prediction maps in lower resolution are iteratively regressed and upsampled into higher resolutions. Moreover, to remedy the potential information deficiency of lower-resolution point cloud, we refine the predictions in each resolution under the criterion of uncertainty selection, which notably enhances the representation ability of the point cloud in lower resolutions. Our proposed PointRas module can be incorporated into the backbones of various point cloud segmentation frameworks, and brings only marginal computational cost. We evaluate the proposed method on challenging datasets including ScanNet, S3DIS, NPM3D, STPLS3D and ScanObjectNN, and consistently improve the performance in comparison with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yu Zheng and Xiuwei Xu and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TIP.2022.3205208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {6002-6016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PointRas: Uncertainty-aware multi-resolution learning for point cloud segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Curiosity-driven salient object detection with fragment
attention. <em>TIP</em>, <em>31</em>, 5989–6001. (<a
href="https://doi.org/10.1109/TIP.2022.3203605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep learning based salient object detection methods with attention mechanisms have made great success. However, existing attention mechanisms can be generally separated into two categories. One part chooses to calculate weights indiscriminately, which yields computational redundancy. While one part focuses randomly on a small part of the images, such as hard attention, resulting in incorrectness owing to insufficiently targeted selection of a subset of tokens. To alleviate these problems, we design a Curiosity-driven Network (CNet) and a Curiosity-driven Learning Algorithm (CLA) based on fragment attention (FA) mechanism newly defined in this paper. FA imitates the process of cognition perception driven by human curiosity, and divides the degree of curiosity into three levels, i.e. curious, a little curious and not curious. These three levels correspond to five saliency degrees, including salient and non-salient, likewise salient and likewise non-salient, completely uncertain. With more knowledge gained by the network, CLA transforms the curiosity degree of each pixel to yield enhanced detail-enriched saliency maps. In order to extract more context-aware information of potential salient objects and make a better foundation for CLA, a high-level feature extraction module (HFEM) is further proposed. Based on the much better high-level features extracted by HFEM, FA can classify the curiosity degree for each pixel more reasonably and accurately. Extensive experiments on five popular datasets clearly demonstrate that our method outperforms the state-of-the-art approaches without any pre-processing operations or post-processing operations.},
  archive      = {J_TIP},
  author       = {Zheng Wang and Pengzhi Wang and Yahong Han and Xue Zhang and Meijun Sun and Qi Tian},
  doi          = {10.1109/TIP.2022.3203605},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5989-6001},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Curiosity-driven salient object detection with fragment attention},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Composed image retrieval via explicit erasure and
replenishment with semantic alignment. <em>TIP</em>, <em>31</em>,
5976–5988. (<a href="https://doi.org/10.1109/TIP.2022.3204213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed image retrieval aims at retrieving the desired images, given a reference image and a text piece. To handle this task, two important subprocesses should be modeled reasonably. One is to erase irrelated details of the reference image against the text piece, and the other is to replenish the desired details in the image against the text piece. Nowadays, the existing methods neglect to distinguish between the two subprocesses and implicitly put them together to solve the composed image retrieval task. To explicitly and orderly model the two subprocesses of the task, we propose a novel composed image retrieval method which contains three key components, i.e., Multi-semantic Dynamic Suppression module (MDS), Text-semantic Complementary Selection module (TCS), and Semantic Space Alignment constraints (SSA). Concretely, MDS is to erase irrelated details of the reference image by suppressing its semantic features. TCS aims to select and enhance the semantic features of the text piece and then replenish them to the reference image. In the end, to facilitate the erasure and replenishment subprocesses, SSA aligns the semantics of the two modality features in the final space. Extensive experiments on three benchmark datasets (Shoes, FashionIQ, and Fashion200K) show the superior performance of our approach against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Gangjian Zhang and Shikui Wei and Huaxin Pang and Shuang Qiu and Yao Zhao},
  doi          = {10.1109/TIP.2022.3204213},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5976-5988},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Composed image retrieval via explicit erasure and replenishment with semantic alignment},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stabilizing adversarially learned one-class novelty
detection using pseudo anomalies. <em>TIP</em>, <em>31</em>, 5963–5975.
(<a href="https://doi.org/10.1109/TIP.2022.3204217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, anomaly scores have been formulated using reconstruction loss of the adversarially learned generators and/or classification loss of discriminators. Unavailability of anomaly examples in the training data makes optimization of such networks challenging. Attributed to the adversarial training, performance of such models fluctuates drastically with each training step, making it difficult to halt the training at an optimal point. In the current study, we propose a robust anomaly detection framework that overcomes such instability by transforming the fundamental role of the discriminator from identifying real vs. fake data to distinguishing good vs. bad quality reconstructions. For this purpose, we propose a method that utilizes the current state as well as an old state of the same generator to create good and bad quality reconstruction examples. The discriminator is trained on these examples to detect the subtle distortions that are often present in the reconstructions of anomalous data. In addition, we propose an efficient generic criterion to stop the training of our model, ensuring elevated performance. Extensive experiments performed on six datasets across multiple domains including image and video based anomaly detection, medical diagnosis, and network security, have demonstrated excellent performance of our approach.},
  archive      = {J_TIP},
  author       = {Muhammad Zaigham Zaheer and Jin-Ha Lee and Arif Mahmood and Marcella Astrid and Seung-Ik Lee},
  doi          = {10.1109/TIP.2022.3204217},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5963-5975},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stabilizing adversarially learned one-class novelty detection using pseudo anomalies},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Attribute and state guided structural embedding network for
vehicle re-identification. <em>TIP</em>, <em>31</em>, 5949–5962. (<a
href="https://doi.org/10.1109/TIP.2022.3202370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification (Re-ID) is a crucial task in smart city and intelligent transportation, aiming to match vehicle images across non-overlapping surveillance camera scenarios. However, the images of different vehicles may have small visual discrepancies when they have the same/similar attributes, e.g., the same/similar color, type, and manufacturer. Meanwhile, the images from a vehicle may have large visual discrepancies with different states, e.g., different camera views, vehicle viewpoints, and capture time. In this paper, we propose an attribute and state guided structural embedding network (ASSEN) to achieve discriminative feature learning by attribute-based enhancement and state-based weakening for vehicle Re-ID. First, we propose an attribute-based enhancement and expanding module to enhance the discrimination of vehicle features through identity-related attribute information, and we design an attribute-based expanding loss to increase the feature gap between different vehicles. Second, we design a state-based weakening and shrinking module, which not only weakens the state information that interferes with identification but also reduces the intra-class feature gap by a state-based shrinking loss. Third, we propose a global structural embedding module that exploits the attribute information and state information to explore hierarchical relationships between vehicle features, then we use these relationships for feature embedding to learn more robust vehicle features. Extensive experiments on benchmark datasets VeRi-776, VehicleID, and VERI-Wild demonstrate the superior performance and generalization of the proposed method against state-of-the-art vehicle Re-ID methods. The code is available at https://github.com/ttaalle/fast_assen .},
  archive      = {J_TIP},
  author       = {Hongchao Li and Chenglong Li and Aihua Zheng and Jin Tang and Bin Luo},
  doi          = {10.1109/TIP.2022.3202370},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5949-5962},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attribute and state guided structural embedding network for vehicle re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video question answering with prior knowledge and
object-sensitive learning. <em>TIP</em>, <em>31</em>, 5936–5948. (<a
href="https://doi.org/10.1109/TIP.2022.3205212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Question Answering (VideoQA), which explores spatial-temporal visual information of videos given a linguistic query, has received unprecedented attention over recent years. One of the main challenges lies in locating relevant visual and linguistic information, and therefore various attention-based approaches are proposed. Despite the impressive progress, two aspects are not fully explored by current methods to get proper attention. Firstly, prior knowledge, which in the human cognitive process plays an important role in assisting the reasoning process of VideoQA, is not fully utilized. Secondly, structured visual information (e.g., object) instead of the raw video is underestimated. To address the above two issues, we propose a Prior Knowledge and Object-sensitive Learning (PKOL) by exploring the effect of prior knowledge and learning object-sensitive representations to boost the VideoQA task. Specifically, we first propose a Prior Knowledge Exploring (PKE) module that aims to acquire and integrate prior knowledge into a question feature for feature enriching, where an information retriever is constructed to retrieve related sentences as prior knowledge from the massive corpus. In addition, we propose an Object-sensitive Representation Learning (ORL) module to generate object-sensitive features by interacting object-level features with frame and clip-level features. Our proposed PKOL achieves consistent improvements on three competitive benchmarks (i.e., MSVD-QA, MSRVTT-QA, and TGIF-QA) and gains state-of-the-art performance. The source code is available at https://github.com/zchoi/PKOL .},
  archive      = {J_TIP},
  author       = {Pengpeng Zeng and Haonan Zhang and Lianli Gao and Jingkuan Song and Heng Tao Shen},
  doi          = {10.1109/TIP.2022.3205212},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5936-5948},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video question answering with prior knowledge and object-sensitive learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FlexHDR: Modeling alignment and exposure uncertainties for
flexible HDR imaging. <em>TIP</em>, <em>31</em>, 5923–5935. (<a
href="https://doi.org/10.1109/TIP.2022.3203562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dynamic range (HDR) imaging is of fundamental importance in modern digital photography pipelines and used to produce a high-quality photograph with well exposed regions despite varying illumination across the image. This is typically achieved by merging multiple low dynamic range (LDR) images taken at different exposures. However, over-exposed regions and misalignment errors due to poorly compensated motion result in artefacts such as ghosting. In this paper, we present a new HDR imaging technique that specifically models alignment and exposure uncertainties to produce high quality HDR results. We introduce a strategy that learns to jointly align and assess the alignment and exposure reliability using an HDR-aware, uncertainty-driven attention map that robustly merges the frames into a single high quality HDR image. Further, we introduce a progressive, multi-stage image fusion approach that can flexibly merge any number of LDR images in a permutation-invariant manner. Experimental results show our method can produce better quality HDR images with up to 1.1dB PSNR improvement to the state-of-the-art, and subjective improvements in terms of better detail, colours, and fewer artefacts.},
  archive      = {J_TIP},
  author       = {Sibi Catley-Chandar and Thomas Tanay and Lucas Vandroux and Aleš Leonardis and Gregory Slabaugh and Eduardo Pérez-Pellitero},
  doi          = {10.1109/TIP.2022.3203562},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5923-5935},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FlexHDR: Modeling alignment and exposure uncertainties for flexible HDR imaging},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep fourier ranking quantization for semi-supervised image
retrieval. <em>TIP</em>, <em>31</em>, 5909–5922. (<a
href="https://doi.org/10.1109/TIP.2022.3203612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the extreme label dependence of supervised product quantization methods, the semi-supervised paradigm usually employs massive unlabeled data to assist in regularizing deep networks, thereby improving model performance. However, the existing method focuses on the overall distribution consistency between unlabeled data and class prototypes, while ignoring subtle individual variances between unlabeled instances. Therefore, the local neighborhood structure is not fully explored, which will cause the model to easily overfit in the training set. In this paper, we introduce a new Fourier perspective to alleviate this issue by exploring the semantic relations between unlabeled instances in a self-supervised manner. Specifically, based on Fourier Transform, we first design a Phase Mixing (PM) strategy, which can manipulate the mixing area and values of the phase component between two images to control the proportion of semantic information. In this way, we can construct multi-level similarity neighbors naturally for unlabeled data. Then, a ranking quantization loss is formulated to perceive multi-level semantic variances in neighbor instances, which improves the robustness and generalization of the model. Extensive experiments in three different semi-supervised settings show that our method outperforms existing state-of-the-art methods by averaged 3.95\% improvement on four datasets.},
  archive      = {J_TIP},
  author       = {Pandeng Li and Hongtao Xie and Shaobo Min and Jiannan Ge and Xun Chen and Yongdong Zhang},
  doi          = {10.1109/TIP.2022.3203612},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5909-5922},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep fourier ranking quantization for semi-supervised image retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crosslink-net: Double-branch encoder network via fusing
vertical and horizontal convolutions for medical image segmentation.
<em>TIP</em>, <em>31</em>, 5893–5908. (<a
href="https://doi.org/10.1109/TIP.2022.3203223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate image segmentation plays a crucial role in medical image analysis, yet it faces great challenges caused by various shapes, diverse sizes, and blurry boundaries. To address these difficulties, square kernel-based encoder-decoder architectures have been proposed and widely used, but their performance remains unsatisfactory. To further address these challenges, we present a novel double-branch encoder architecture. Our architecture is inspired by two observations. (1) Since the discrimination of the features learned via square convolutional kernels needs to be further improved, we propose utilizing nonsquare vertical and horizontal convolutional kernels in a double-branch encoder so that the features learned by both branches can be expected to complement each other. (2) Considering that spatial attention can help models to better focus on the target region in a large-sized image, we develop an attention loss to further emphasize the segmentation of small-sized targets. With the above two schemes, we develop a novel double-branch encoder-based segmentation framework for medical image segmentation, namely, Crosslink-Net, and validate its effectiveness on five datasets with experiments. The code is released at https://github.com/Qianyu1226/Crosslink-Net .},
  archive      = {J_TIP},
  author       = {Qian Yu and Lei Qi and Yang Gao and Wuzhang Wang and Yinghuan Shi},
  doi          = {10.1109/TIP.2022.3203223},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5893-5908},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Crosslink-net: Double-branch encoder network via fusing vertical and horizontal convolutions for medical image segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient semi-supervised multimodal hashing with importance
differentiation regression. <em>TIP</em>, <em>31</em>, 5881–5892. (<a
href="https://doi.org/10.1109/TIP.2022.3203216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal hashing learns compact binary hash codes by collaborating heterogeneous multi-modal features at both the model training and online retrieval stages to support large-scale multimedia retrieval. Previous multi-modal hashing methods mainly focus on supervised and unsupervised hashing. The performance of supervised hashing largely relies on the number of labeled data, which is practically expensive to obtain. Unsupervised hashing methods cannot effectively capture the semantic correlations of multi-modal data without any labels for supervision. In this paper, we propose an Efficient Semi-supervised Multi-modal Hashing with Importance Differentiation Regression (ESMH-IDR) model, which can alleviate the existing problems by learning from both labeled and unlabeled data. Specifically, in this paper, we develop an efficient semi-supervised multi-modal hash code learning module. It learns the hash codes for labeled data in an efficient asymmetric way, and simultaneously performs nonlinear regression using the same projection matrix as the labeled samples to preserve the intrinsic data structure of unlabeled data. Besides, different from existing methods, we propose an importance differentiation regression strategy to learn hash functions by specially considering the different importance of hash codes learned from the labeled and unlabeled samples. Finally, we develop an efficient discrete optimization method guaranteed with convergence to iteratively solve the hash optimization problem. Experiments on several public multimedia retrieval datasets demonstrate the superiority of our proposed method on both retrieval effectiveness and efficiency. Our source codes and testing datasets can be obtained at https://github.com/ChaoqunZheng/ESMH .},
  archive      = {J_TIP},
  author       = {Chaoqun Zheng and Lei Zhu and Zheng Zhang and Jingjing Li and Xiaomei Yu},
  doi          = {10.1109/TIP.2022.3203216},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5881-5892},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient semi-supervised multimodal hashing with importance differentiation regression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward better accuracy-efficiency trade-offs: Divide and
co-training. <em>TIP</em>, <em>31</em>, 5869–5880. (<a
href="https://doi.org/10.1109/TIP.2022.3201602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The width of a neural network matters since increasing the width will necessarily increase the model capacity. However, the performance of a network does not improve linearly with the width and soon gets saturated. In this case, we argue that increasing the number of networks (ensemble) can achieve better accuracy-efficiency trade-offs than purely increasing the width. To prove it, one large network is divided into several small ones regarding its parameters and regularization components. Each of these small networks has a fraction of the original one’s parameters. We then train these small networks together and make them see various views of the same data to increase their diversity. During this co-training process, networks can also learn from each other. As a result, small networks can achieve better ensemble performance than the large one with few or no extra parameters or FLOPs, i . e ., achieving better accuracy-efficiency trade-offs. Small networks can also achieve faster inference speed than the large one by concurrent running. All of the above shows that the number of networks is a new dimension of model scaling. We validate our argument with 8 different neural architectures on common benchmarks through extensive experiments.},
  archive      = {J_TIP},
  author       = {Shuai Zhao and Liguang Zhou and Wenxiao Wang and Deng Cai and Tin Lun Lam and Yangsheng Xu},
  doi          = {10.1109/TIP.2022.3201602},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5869-5880},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward better accuracy-efficiency trade-offs: Divide and co-training},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frequency-tuned universal adversarial attacks on texture
recognition. <em>TIP</em>, <em>31</em>, 5856–5868. (<a
href="https://doi.org/10.1109/TIP.2022.3202366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep neural networks (DNNs) have been shown to be susceptible to image-agnostic adversarial attacks on natural image classification problems, the effects of such attacks on DNN-based texture recognition have yet to be explored. As part of our work, we find that limiting the perturbation’s $l_{p}$ norm in the spatial domain may not be a suitable way to restrict the perceptibility of universal adversarial perturbations for texture images. Based on the fact that human perception is affected by local visual frequency characteristics, we propose a frequency-tuned universal attack method to compute universal perturbations in the frequency domain. Our experiments indicate that our proposed method can produce less perceptible perturbations yet with a similar or higher white-box fooling rates on various DNN texture classifiers and texture datasets as compared to existing universal attack techniques. We also demonstrate that our approach can improve the attack robustness against defended models as well as the cross-dataset transferability for texture recognition problems.},
  archive      = {J_TIP},
  author       = {Yingpeng Deng and Lina J. Karam},
  doi          = {10.1109/TIP.2022.3202366},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5856-5868},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Frequency-tuned universal adversarial attacks on texture recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Arbitrary-scale texture generation from coarse-grained
control. <em>TIP</em>, <em>31</em>, 5841–5855. (<a
href="https://doi.org/10.1109/TIP.2022.3201710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep-network based texture synthesis approaches all focus on fine-grained control of texture generation by synthesizing images from exemplars. Since the networks employed by most of these methods are always tied to individual exemplar textures, a large number of individual networks have to be trained when modeling various textures. In this paper, we propose to generate textures directly from coarse-grained control or high-level guidance, such as texture categories, perceptual attributes and semantic descriptions. We fulfill the task by parsing the generation process of a texture into the three-level Bayesian hierarchical model. A coarse-grained signal first determines a distribution over Markov random fields. Then a Markov random field is used to model the distribution of the final output textures. Finally, an output texture is generated from the sampled Markov random field distribution. At the bottom level of the Bayesian hierarchy, the isotropic and ergodic characteristics of the textures favor a construction that consists of a fully convolutional network. The proposed method integrates texture creation and texture synthesis into one pipeline for real-time texture generation, and enables users to readily obtain diverse textures with arbitrary scales from high-level guidance only. Extensive experiments demonstrate that the proposed method is capable of generating plausible textures that are faithful to user-defined control, and achieving impressive texture metamorphosis by interpolation in the learned texture manifold.},
  archive      = {J_TIP},
  author       = {Yanhai Gan and Feng Gao and Junyu Dong and Sheng Chen},
  doi          = {10.1109/TIP.2022.3201710},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5841-5855},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Arbitrary-scale texture generation from coarse-grained control},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstruct dynamic soft-tissue with stereo endoscope based
on a single-layer network. <em>TIP</em>, <em>31</em>, 5828–5840. (<a
href="https://doi.org/10.1109/TIP.2022.3202367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dynamic minimally invasive surgery environments, 3D reconstruction of deformable soft-tissue surfaces with stereo endoscopic images is very challenging. A simple self-supervised stereo reconstruction framework is proposed to address this issue, which bridges the traditional geometric deformable models and the newly revived neural networks. The equivalence between the classical thin plate spline (TPS) model and a single-layer fully-connected or convolutional network is studied. By alternating training of two TPS equivalent networks within the self-supervised framework, disparity priors are learnt from the past stereo frames of target tissues to form an optimized disparity basis, on which disparity maps of subsequent frames can be estimated more accurately without sacrificing computational efficiency and robustness. The proposed method was verified on stereo-endoscopic videos recorded by the da Vinci ® surgical robots.},
  archive      = {J_TIP},
  author       = {Bo Yang and Siyuan Xu and Hongrong Chen and Wenfeng Zheng and Chao Liu},
  doi          = {10.1109/TIP.2022.3202367},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5828-5840},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reconstruct dynamic soft-tissue with stereo endoscope based on a single-layer network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OSLO: On-the-sphere learning for omnidirectional images and
its application to 360-degree image compression. <em>TIP</em>,
<em>31</em>, 5813–5827. (<a
href="https://doi.org/10.1109/TIP.2022.3202357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art 2D image compression schemes rely on the power of convolutional neural networks (CNNs). Although CNNs offer promising perspectives for 2D image compression, extending such models to omnidirectional images is not straightforward. First, omnidirectional images have specific spatial and statistical properties that can not be fully captured by current CNN models. Second, basic mathematical operations composing a CNN architecture, e.g. , translation and sampling, are not well-defined on the sphere. In this paper, we study the learning of representation models for omnidirectional images and propose to use the properties of HEALPix uniform sampling of the sphere to redefine the mathematical tools used in deep learning models for omnidirectional images. In particular, we: i) propose the definition of a new convolution operation on the sphere that keeps the high expressiveness and the low complexity of a classical 2D convolution; ii) adapt standard CNN techniques such as stride, iterative aggregation, and pixel shuffling to the spherical domain; and then iii) apply our new framework to the task of omnidirectional image compression. Our experiments show that our proposed on-the-sphere solution leads to a better compression gain that can save 13.7\% of the bit rate compared to similar learned models applied to equirectangular images. Also, compared to learning models based on graph convolutional networks, our solution supports more expressive filters that can preserve high frequencies and provide a better perceptual quality of the compressed images. Such results demonstrate the efficiency of the proposed framework, which opens new research venues for other omnidirectional vision tasks to be effectively implemented on the sphere manifold.},
  archive      = {J_TIP},
  author       = {Navid Mahmoudian Bidgoli and Roberto G. de A. Azevedo and Thomas Maugey and Aline Roumy and Pascal Frossard},
  doi          = {10.1109/TIP.2022.3202357},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5813-5827},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {OSLO: On-the-sphere learning for omnidirectional images and its application to 360-degree image compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). From pixels to semantics: Self-supervised video object
segmentation with multiperspective feature mining. <em>TIP</em>,
<em>31</em>, 5801–5812. (<a
href="https://doi.org/10.1109/TIP.2022.3201603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing self-supervised methods pose one-shot video object segmentation (O-VOS) as pixel-level matching to enable segmentation mask propagation across frames. However, the two tasks are not fully equivalent since O-VOS is more reliant on semantic correspondence rather than accurate pixel matching. To remedy this issue, we explore a new self-supervised framework that integrates pixel-level correspondence learning with semantic-level adaptation. The pixel-level correspondence learning is performed through photometric reconstruction of adjacent RGB frames during offline training, while semantic-level adaption operates at test-time by enforcing a bi-directional agreement of the predicted segmentation masks. In addition, we further propose a new network architecture with multi-perspective feature mining mechanism which can not only enhance reliable features but also suppress noisy ones to facilitate more robust image matching. By training the network using the proposed self-supervised framework, we achieve state-of-the-art performance on widely adopted datasets, further closing up the gap between self-supervised learning methods and their fully supervised counterparts.},
  archive      = {J_TIP},
  author       = {Ruoqi Li and Yifan Wang and Lijun Wang and Huchuan Lu and Xiaopeng Wei and Qiang Zhang},
  doi          = {10.1109/TIP.2022.3201603},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5801-5812},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {From pixels to semantics: Self-supervised video object segmentation with multiperspective feature mining},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast and efficient shape blending by stable and
analytically invertible finite descriptors. <em>TIP</em>, <em>31</em>,
5788–5800. (<a href="https://doi.org/10.1109/TIP.2022.3199105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a previous work, we have proposed a morphing method based on invertible and stable descriptors that are invariant to Euclidean transformations and to the starting point. The stability guarantees the closeness in the shape sense of the reconstructed intermediate contours. However, this set of descriptors is not defined by a general expression. Here, we propose several sets of stable and invertible finite descriptors expressed by the same formula. Its stability is proven for a subset of this family thanks to the finite-dimension of the invariant space. Such finite dimension results from the Discrete Fourier Transform model that is used instead of Fourier coefficients. Moreover, an analytical general inverse formula is established. The use of the inverse analytical formula and the double utilization of the Fast Fourier Transform ensure an effective blending while being computationally efficient. Finally, we propose a new quantitative criterion for shape morphing. The latter is based on the Euclidean distances between successive curves in a morphing sequence after applying a given registration. Therefore, it allows us to compare the blending results of each set of descriptors. Several experiments are conducted on KIMIA’99 and MPEG-7 datasets. The results highlight the concordance of this criterion with the morphing visual quality and indicate which set of descriptors generates the most appropriate blending.},
  archive      = {J_TIP},
  author       = {Emna Ghorbel and Faouzi Ghorbel and Slim M’Hiri},
  doi          = {10.1109/TIP.2022.3199105},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5788-5800},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A fast and efficient shape blending by stable and analytically invertible finite descriptors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep unrolled low-rank tensor completion for high dynamic
range imaging. <em>TIP</em>, <em>31</em>, 5774–5787. (<a
href="https://doi.org/10.1109/TIP.2022.3201708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major challenge in high dynamic range (HDR) imaging for dynamic scenes is suppressing ghosting artifacts caused by large object motions or poor exposures. Whereas recent deep learning-based approaches have shown significant synthesis performance, interpretation and analysis of their behaviors are difficult and their performance is affected by the diversity of training data. In contrast, traditional model-based approaches yield inferior synthesis performance to learning-based algorithms despite their theoretical thoroughness. In this paper, we propose an algorithm unrolling approach to ghost-free HDR image synthesis algorithm that unrolls an iterative low-rank tensor completion algorithm into deep neural networks to take advantage of the merits of both learning- and model-based approaches while overcoming their weaknesses. First, we formulate ghost-free HDR image synthesis as a low-rank tensor completion problem by assuming the low-rank structure of the tensor constructed from low dynamic range (LDR) images and linear dependency among LDR images. We also define two regularization functions to compensate for modeling inaccuracy by extracting hidden model information. Then, we solve the problem efficiently using an iterative optimization algorithm by reformulating it into a series of subproblems. Finally, we unroll the iterative algorithm into a series of blocks corresponding to each iteration, in which the optimization variables are updated by rigorous closed-form solutions and the regularizers are updated by learned deep neural networks. Experimental results on different datasets show that the proposed algorithm provides better HDR image synthesis performance with superior robustness compared with state-of-the-art algorithms, while using significantly fewer training samples.},
  archive      = {J_TIP},
  author       = {Truong Thanh Nhat Mai and Edmund Y. Lam and Chul Lee},
  doi          = {10.1109/TIP.2022.3201708},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5774-5787},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep unrolled low-rank tensor completion for high dynamic range imaging},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast scalable image restoration using total variation priors
and expectation propagation. <em>TIP</em>, <em>31</em>, 5762–5773. (<a
href="https://doi.org/10.1109/TIP.2022.3202092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a scalable approximate Bayesian method for image restoration using Total Variation (TV) priors, with the ability to offer uncertainty quantification. In contrast to most optimization methods based on maximum a posteriori estimation, we use the Expectation Propagation (EP) framework to approximate minimum mean squared error (MMSE) estimates and marginal (pixel-wise) variances, without resorting to Monte Carlo sampling. For the classical anisotropic TV-based prior, we also propose an iterative scheme to automatically adjust the regularization parameter via Expectation Maximization (EM). Using Gaussian approximating densities with diagonal covariance matrices, the resulting method allows highly parallelizable steps and can scale to large images for denoising, deconvolution, and compressive sensing (CS) problems. The simulation results illustrate that such EP methods can provide a posteriori estimates on par with those obtained via sampling methods but at a fraction of the computational cost. Moreover, EP does not exhibit strong underestimation of posteriori variances, in contrast to variational Bayes alternatives.},
  archive      = {J_TIP},
  author       = {Dan Yao and Stephen McLaughlin and Yoann Altmann},
  doi          = {10.1109/TIP.2022.3202092},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5762-5773},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast scalable image restoration using total variation priors and expectation propagation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Face inverse rendering via hierarchical decoupling.
<em>TIP</em>, <em>31</em>, 5748–5761. (<a
href="https://doi.org/10.1109/TIP.2022.3201466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous face inverse rendering methods often require synthetic data with ground truth and/or professional equipment like a lighting stage. However, a model trained on synthetic data or using pre-defined lighting priors is typically unable to generalize well for real-world situations, due to the gap between synthetic data/lighting priors and real data. Furthermore, for common users, the professional equipment and skill make the task expensive and complex. In this paper, we propose a deep learning framework to disentangle face images in the wild into their corresponding albedo, normal, and lighting components. Specifically, a decomposition network is built with a hierarchical subdivision strategy, which takes image pairs captured from arbitrary viewpoints as input. In this way, our approach can greatly mitigate the pressure from data preparation, and significantly broaden the applicability of face inverse rendering. Extensive experiments are conducted to demonstrate the efficacy of our design, and show its superior performance in face relighting over other state-of-the-art alternatives. Our code is available at https://github.com/AutoHDR/HD-Net.git .},
  archive      = {J_TIP},
  author       = {Meng Wang and Xiaojie Guo and Wenjing Dai and Jiawan Zhang},
  doi          = {10.1109/TIP.2022.3201466},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5748-5761},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Face inverse rendering via hierarchical decoupling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised rigid registration for multimodal retinal
images. <em>TIP</em>, <em>31</em>, 5733–5747. (<a
href="https://doi.org/10.1109/TIP.2022.3201476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to accurately overlay one modality retinal image to another is critical in ophthalmology. Our previous framework achieved the state-of-the-art results for multimodal retinal image registration. However, it requires human-annotated labels due to the supervised approach of the previous work. In this paper, we propose a self-supervised multimodal retina registration method to alleviate the burdens of time and expense to prepare for training data, that is, aiming to automatically register multimodal retinal images without any human annotations. Specially, we focus on registering color fundus images with infrared reflectance and fluorescein angiography images, and compare registration results with several conventional and supervised and unsupervised deep learning methods. From the experimental results, the proposed self-supervised framework achieves a comparable accuracy comparing to the state-of-the-art supervised learning method in terms of registration accuracy and Dice coefficient.},
  archive      = {J_TIP},
  author       = {Cheolhong An and Yiqian Wang and Junkang Zhang and Truong Q. Nguyen},
  doi          = {10.1109/TIP.2022.3201476},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5733-5747},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised rigid registration for multimodal retinal images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep posterior distribution-based embedding for
hyperspectral image super-resolution. <em>TIP</em>, <em>31</em>,
5720–5732. (<a href="https://doi.org/10.1109/TIP.2022.3201478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of hyperspectral (HS) image spatial super-resolution via deep learning. Particularly, we focus on how to embed the high-dimensional spatial-spectral information of HS images efficiently and effectively. Specifically, in contrast to existing methods adopting empirically-designed network modules, we formulate HS embedding as an approximation of the posterior distribution of a set of carefully-defined HS embedding events, including layer-wise spatial-spectral feature extraction and network-level feature aggregation. Then, we incorporate the proposed feature embedding scheme into a source-consistent super-resolution framework that is physically-interpretable, producing PDE-Net, in which high-resolution (HR) HS images are iteratively refined from the residuals between input low-resolution (LR) HS images and pseudo-LR-HS images degenerated from reconstructed HR-HS images via probability-inspired HS embedding. Extensive experiments over three common benchmark datasets demonstrate that PDE-Net achieves superior performance over state-of-the-art methods. Besides, the probabilistic characteristic of this kind of networks can provide the epistemic uncertainty of the network outputs, which may bring additional benefits when used for other HS image-based applications. The code will be publicly available at https://github.com/jinnh/PDE-Net .},
  archive      = {J_TIP},
  author       = {Jinhui Hou and Zhiyu Zhu and Junhui Hou and Huanqiang Zeng and Jinjian Wu and Jiantao Zhou},
  doi          = {10.1109/TIP.2022.3201478},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5720-5732},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep posterior distribution-based embedding for hyperspectral image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D cascade RCNN: High quality object detection in point
clouds. <em>TIP</em>, <em>31</em>, 5706–5719. (<a
href="https://doi.org/10.1109/TIP.2022.3201469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress on 2D object detection has featured Cascade RCNN, which capitalizes on a sequence of cascade detectors to progressively improve proposal quality, towards high-quality object detection. However, there has not been evidence in support of building such cascade structures for 3D object detection, a challenging detection scenario with highly sparse LiDAR point clouds. In this work, we present a simple yet effective cascade architecture, named 3D Cascade RCNN, that allocates multiple detectors based on the voxelized point clouds in a cascade paradigm, pursuing higher quality 3D object detector progressively. Furthermore, we quantitatively define the sparsity level of the points within 3D bounding box of each object as the point completeness score, which is exploited as the task weight for each proposal to guide the learning of each stage detector. The spirit behind is to assign higher weights for high-quality proposals with relatively complete point distribution, while down-weight the proposals with extremely sparse points that often incur noise during training. This design of completeness-aware re-weighting elegantly upgrades the cascade paradigm to be better applicable for the sparse input data, without increasing any FLOP budgets. Through extensive experiments on both the KITTI dataset and Waymo Open Dataset, we validate the superiority of our proposed 3D Cascade RCNN, when comparing to state-of-the-art 3D object detection techniques. The source code is publicly available at https://github.com/caiqi/Cascasde-3D .},
  archive      = {J_TIP},
  author       = {Qi Cai and Yingwei Pan and Ting Yao and Tao Mei},
  doi          = {10.1109/TIP.2022.3201469},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5706-5719},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D cascade RCNN: High quality object detection in point clouds},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Poison ink: Robust and invisible backdoor attack.
<em>TIP</em>, <em>31</em>, 5691–5705. (<a
href="https://doi.org/10.1109/TIP.2022.3201472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research shows deep neural networks are vulnerable to different types of attacks, such as adversarial attacks, data poisoning attacks, and backdoor attacks. Among them, backdoor attacks are the most cunning and can occur in almost every stage of the deep learning pipeline. Backdoor attacks have attracted lots of interest from both academia and industry. However, most existing backdoor attack methods are visible or fragile to some effortless pre-processing such as common data transformations. To address these limitations, we propose a robust and invisible backdoor attack called “Poison Ink”. Concretely, we first leverage the image structures as target poisoning areas and fill them with poison ink (information) to generate the trigger pattern. As the image structure can keep its semantic meaning during the data transformation, such a trigger pattern is inherently robust to data transformations. Then we leverage a deep injection network to embed such input-aware trigger pattern into the cover image to achieve stealthiness. Compared to existing popular backdoor attack methods, Poison Ink outperforms both in stealthiness and robustness. Through extensive experiments, we demonstrate that Poison Ink is not only general to different datasets and network architectures but also flexible for different attack scenarios. Besides, it also has very strong resistance against many state-of-the-art defense techniques.},
  archive      = {J_TIP},
  author       = {Jie Zhang and Chen Dongdong and Qidong Huang and Jing Liao and Weiming Zhang and Huamin Feng and Gang Hua and Nenghai Yu},
  doi          = {10.1109/TIP.2022.3201472},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5691-5705},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Poison ink: Robust and invisible backdoor attack},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group sparsity mixture model and its application on image
denoising. <em>TIP</em>, <em>31</em>, 5677–5690. (<a
href="https://doi.org/10.1109/TIP.2022.3193754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior learning is a fundamental problem in the field of image processing. In this paper, we conduct a detailed study on (1) how to model and learn the prior of the image patch group, which consists of a group of non-local similar image patches, and (2) how to apply the learned prior to the whole image denoising task. To tackle the first problem, we propose a new prior model named Group Sparsity Mixture Model (GSMM). With the bilateral matrix multiplication, the GSMM can model both the local feature of a single patch and the relation among non-local similar patches, and thus it is very suitable for patch group based prior learning. This is supported by the parameter analysis which demonstrates that the learned GSMM successfully captures the inherent strong sparsity embodied in the image patch group. Besides, as a mixture model, GSMM can be used for patch group classification. This makes the image denoising method based on GSMM capable of processing patch groups flexibly. To tackle the second problem, we propose an efficient and effective patch group based image denoising framework, which is plug-and-play and compatible with any patch group prior model. Using this framework, we construct two versions of GSMM based image denoising methods, both of which outperform the competing methods based on other prior models, e . g ., Field of Experts (FoE) and Gaussian Mixture Model (GMM). Also, the better version is competitive with the state-of-the-art model based method WNNM with about $\times 8$ faster average running speed.},
  archive      = {J_TIP},
  author       = {Haosen Liu and Laquan Li and Jiangbo Lu and Shan Tan},
  doi          = {10.1109/TIP.2022.3193754},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5677-5690},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Group sparsity mixture model and its application on image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distance-aware occlusion detection with focused attention.
<em>TIP</em>, <em>31</em>, 5661–5676. (<a
href="https://doi.org/10.1109/TIP.2022.3197984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For humans, understanding the relationships between objects using visual signals is intuitive. For artificial intelligence, however, this task remains challenging. Researchers have made significant progress studying semantic relationship detection, such as human-object interaction detection and visual relationship detection. We take the study of visual relationships a step further from semantic to geometric. In specific, we predict relative occlusion and relative distance relationships. However, detecting these relationships from a single image is challenging. Enforcing focused attention to task-specific regions plays a critical role in successfully detecting these relationships. In this work, (1) we propose a novel three-decoder architecture as the infrastructure for focused attention; 2) we use the generalized intersection box prediction task to effectively guide our model to focus on occlusion-specific regions; 3) our model achieves a new state-of-the-art performance on distance-aware relationship detection. Specifically, our model increases the distance F1-score from 33.8\% to 38.6\% and boosts the occlusion F1-score from 34.4\% to 41.2\%. Our code is publicly available.},
  archive      = {J_TIP},
  author       = {Yang Li and Yucheng Tu and Xiaoxue Chen and Hao Zhao and Guyue Zhou},
  doi          = {10.1109/TIP.2022.3197984},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5661-5676},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Distance-aware occlusion detection with focused attention},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fuzzy sparse deviation regularized robust principal
component analysis. <em>TIP</em>, <em>31</em>, 5645–5660. (<a
href="https://doi.org/10.1109/TIP.2022.3199086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust principal component analysis (RPCA) is a technique that aims to make principal component analysis (PCA) robust to noise samples. The current modeling approaches of RPCA were proposed by analyzing the prior distribution of the reconstruction error terms. However, these methods ignore the influence of samples with large reconstruction errors, as well as the valid information of these samples in principal component space, which will degrade the ability of PCA to extract the principal component of data. In order to solve this problem, Fuzzy sparse deviation regularized robust principal component Analysis (FSD-PCA) is proposed in this paper. First, FSD-PCA learns the principal components by minimizing the square of $\ell _{2}$ -norm-based reconstruction error. Then, FSD-PCA introduces sparse deviation on reconstruction error term to relax the samples with large bias, thus FSD-PCA can process noise and principal components of samples separately as well as improve the ability of FSD-PCA for retaining the principal component information. Finally, FSD-PCA estimates the prior probability of each sample by fuzzy weighting based on the relaxed reconstruction error, which can improve the robustness of the model. The experimental results indicate that the proposed model performs excellent robustness against different types of noise than the state-of-art algorithms, and the sparse deviation term enables FSD-PCA to process noise information and principal component information separately, so FSD-PCA can filter the noise information of an image and restore the corrupted image.},
  archive      = {J_TIP},
  author       = {Yunlong Gao and Tingting Lin and Jinyan Pan and Feiping Nie and Youwei Xie},
  doi          = {10.1109/TIP.2022.3199086},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5645-5660},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fuzzy sparse deviation regularized robust principal component analysis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference-reconstruction variational autoencoder for light
field image reconstruction. <em>TIP</em>, <em>31</em>, 5629–5644. (<a
href="https://doi.org/10.1109/TIP.2022.3197976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field cameras can capture the radiance and direction of light rays by a single exposure, providing a new perspective to photography and 3D geometry perception. However, existing sub-aperture based light field cameras are limited by their sensor resolution to obtain high spatial and angular resolution images simultaneously. In this paper, we propose an inference-reconstruction variational autoencoder (IR-VAE) to reconstruct a dense light field image out of four corner reference views in a light field image. The proposed IR-VAE is comprised of one inference network and one reconstruction network, where the inference network infers novel views from existing reference views and viewpoint conditions, and the reconstruction network reconstructs novel views from a latent variable that contains the information of reference views, novel views, and viewpoints. The conditional latent variable in the inference network is regularized by the latent variable in the reconstruction network to facilitate information flow between the conditional latent variable and novel views. We also propose a statistic distance measurement dubbed the mean local maximum mean discrepancy (MLMMD) to enable the measurement of the statistic distance between two distributions with high-resolution latent variables, which can capture richer information than their low-resolution counterparts. Finally, we propose a viewpoint-dependent indirect view synthesis method to synthesize novel views more efficiently by leveraging adaptive convolution. Experimental results show that our proposed methods outperform state-of-the-art methods on different light field datasets.},
  archive      = {J_TIP},
  author       = {Kang Han and Wei Xiang},
  doi          = {10.1109/TIP.2022.3197976},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5629-5644},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Inference-reconstruction variational autoencoder for light field image reconstruction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconstruction of connected digital lines based on
constrained regularization. <em>TIP</em>, <em>31</em>, 5613–5628. (<a
href="https://doi.org/10.1109/TIP.2022.3197991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach for reconstruction of disconnected digital lines (DDLs) based on a constrained regularization model which ensures connectivity of the digital lines (DLs) in the discrete image plane. The first step in this approach is to determine the order of given pixels of the DDL. To determine connectivity of pixels, we use the usual 8-neighbor connectivity in discrete images. For any neighboring pixels of the DDL that are not connected, we determine a number of new pixel values that need to be reconstructed between these pixels. Next, the integer-valued $x$ - and $y$ -coordinates of the location of the pixels of the DDLs are segregated into two 1D signal vectors. Then the $x$ - and $y$ -coordinates of the missing pixels of the DDLs are estimated using a new constrained regularization. While the solution of this constrained minimization problem provides real values for the $x$ - and $y$ -coordinates of pixels positions, the imposed constraint ensures connectivity of the resulting DLs in the image plane after transforming the computed values from $\mathbb {R}$ to $\mathbb {N}$ . The proposed regularization approach forces connected lines with small curvature. The experimental results demonstrate that the proposed technique improves DL intersection detection, as well. Moreover, this technique has a high potential to be used as a fast approach in binary image inpainting particularly overcoming the shortcomings of conventional methods which cause destruction of thin objects and blurring in the recovered regions.},
  archive      = {J_TIP},
  author       = {Mojtaba Lashgari and Hossein Rabbani and Gerlind Plonka and Ivan Selesnick},
  doi          = {10.1109/TIP.2022.3197991},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5613-5628},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reconstruction of connected digital lines based on constrained regularization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AIParsing: Anchor-free instance-level human parsing.
<em>TIP</em>, <em>31</em>, 5599–5612. (<a
href="https://doi.org/10.1109/TIP.2022.3192989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most state-of-the-art instance-level human parsing models adopt two-stage anchor-based detectors and, therefore, cannot avoid the heuristic anchor box design and the lack of analysis on a pixel level. To address these two issues, we have designed an instance-level human parsing network which is anchor-free and solvable on a pixel level. It consists of two simple sub-networks: an anchor-free detection head for bounding box predictions and an edge-guided parsing head for human segmentation. The anchor-free detector head inherits the pixel-like merits and effectively avoids the sensitivity of hyper-parameters as proved in object detection applications. By introducing the part-aware boundary clue, the edge-guided parsing head is capable to distinguish adjacent human parts from among each other up to 58 parts in a single human instance, even overlapping instances. Meanwhile, a refinement head integrating box-level score and part-level parsing quality is exploited to improve the quality of the parsing results. Experiments on two multiple human parsing datasets ( i.e. , CIHP and LV-MHP-v2.0) and one video instance-level human parsing dataset ( i.e. , VIP) show that our method achieves the best global-level and instance-level performance over state-of-the-art one-stage top-down alternatives.},
  archive      = {J_TIP},
  author       = {Sanyi Zhang and Xiaochun Cao and Guo-Jun Qi and Zhanjie Song and Jie Zhou},
  doi          = {10.1109/TIP.2022.3192989},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5599-5612},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AIParsing: Anchor-free instance-level human parsing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PETR: Rethinking the capability of transformer-based
language model in scene text recognition. <em>TIP</em>, <em>31</em>,
5585–5598. (<a href="https://doi.org/10.1109/TIP.2022.3197981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploration of linguistic information promotes the development of scene text recognition task. Benefiting from the significance in parallel reasoning and global relationship capture, transformer-based language model (TLM) has achieved dominant performance recently. As a decoupled structure from the recognition process, we argue that TLM’s capability is limited by the input low-quality visual prediction. To be specific: 1) The visual prediction with low character-wise accuracy increases the correction burden of TLM. 2) The inconsistent word length between visual prediction and original image provides a wrong language modeling guidance in TLM. In this paper, we propose a Progressive scEne Text Recognizer (PETR) to improve the capability of transformer-based language model by handling above two problems. Firstly, a Destruction Learning Module (DLM) is proposed to consider the linguistic information in the visual context. DLM introduces the recognition of destructed images with disordered patches in the training stage. Through guiding the vision model to restore patch orders and make word-level prediction on the destructed images, visual prediction with high character-wise accuracy is obtained by exploring inner relationship between the local visual patches. Secondly, a new Language Rectification Module (LRM) is proposed to optimize the word length for language guidance rectification. Through progressively implementing LRM in different language modeling steps, a novel progressive rectification network is constructed to handle some extremely challenging cases ( e.g. distortion, occlusion, etc.). By utilizing DLM and LRM, PETR enhances the capability of transformer-based language model from a more general aspect, that is, focusing on the reduction of correction burden and rectification of language modeling guidance. Compared with parallel transformer-based methods, PETR obtains 1.0\% and 0.8\% improvement on regular and irregular datasets respectively while introducing only 1.7M additional parameters. The extensive experiments on both English and Chinese benchmarks demonstrate that PETR achieves the state-of-the-art results.},
  archive      = {J_TIP},
  author       = {Yuxin Wang and Hongtao Xie and Shancheng Fang and Mengting Xing and Jing Wang and Shenggao Zhu and Yongdong Zhang},
  doi          = {10.1109/TIP.2022.3197981},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5585-5598},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PETR: Rethinking the capability of transformer-based language model in scene text recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Convolutional fine-grained classification with
self-supervised target relation regularization. <em>TIP</em>,
<em>31</em>, 5570–5584. (<a
href="https://doi.org/10.1109/TIP.2022.3197931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification can be addressed by deep representation learning under supervision of manually pre-defined targets ( e.g., one-hot or the Hadamard codes). Such target coding schemes are less flexible to model inter-class correlation and are sensitive to sparse and imbalanced data distribution as well. In light of this, this paper introduces a novel target coding scheme – dynamic target relation graphs (DTRG), which, as an auxiliary feature regularization, is a self-generated structural output to be mapped from input images. Specifically, online computation of class-level feature centers is designed to generate cross-category distance in the representation space, which can thus be depicted by a dynamic graph in a non-parametric manner. Explicitly minimizing intra-class feature variations anchored on those class-level centers can encourage learning of discriminative features. Moreover, owing to exploiting inter-class dependency, the proposed target graphs can alleviate data sparsity and imbalanceness in representation learning. Inspired by recent success of the mixup style data augmentation, this paper introduces randomness into soft construction of dynamic target relation graphs to further explore relation diversity of target classes. Experimental results can demonstrate the effectiveness of our method on a number of diverse benchmarks of multiple visual classification, especially achieving the state-of-the-art performance on three popular fine-grained object benchmarks and superior robustness against sparse and imbalanced data. Source codes are made publicly available at https://github.com/AkonLau/DTRG .},
  archive      = {J_TIP},
  author       = {Kangjun Liu and Ke Chen and Kui Jia},
  doi          = {10.1109/TIP.2022.3197931},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5570-5584},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Convolutional fine-grained classification with self-supervised target relation regularization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AAP-MIT: Attentive atrous pyramid network and memory
incorporated transformer for multisentence video description.
<em>TIP</em>, <em>31</em>, 5559–5569. (<a
href="https://doi.org/10.1109/TIP.2022.3195643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating multi-sentence descriptions for video is considered to be the most complex task in computer vision and natural language understanding due to the intricate nature of video-text data. With the recent advances in deep learning approaches, the multi-sentence video description has achieved an impressive progress. However, learning rich temporal context representation of visual sequences and modelling long-term dependencies of natural language descriptions is still a challenging problem. Towards this goal, we propose an Attentive Atrous Pyramid network and Memory Incorporated Transformer (AAP-MIT) for multi-sentence video description. The proposed AAP-MIT incorporates the effective representation of visual scene by distilling the most informative and discriminative spatio-temporal features of video data at multiple granularities and further generates the highly summarized descriptions. Profoundly, we construct AAP-MIT with three major components: i) a temporal pyramid network, which builds the temporal feature hierarchy at multiple scales by convolving the local features at temporal space, ii) a temporal correlation attention to learn the relations among various temporal video segments, and iii) the memory incorporated transformer, which augments the new memory block in language transformer to generate highly descriptive natural language sentences. Finally, the extensive experiments on ActivityNet Captions and YouCookII datasets demonstrate the substantial superiority of AAP-MIT over the existing approaches.},
  archive      = {J_TIP},
  author       = {Jeripothula Prudviraj and Malipatel Indrakaran Reddy and Chalavadi Vishnu and Chalavadi Krishna Mohan},
  doi          = {10.1109/TIP.2022.3195643},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5559-5569},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AAP-MIT: Attentive atrous pyramid network and memory incorporated transformer for multisentence video description},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational bayesian orthogonal nonnegative matrix
factorization over the stiefel manifold. <em>TIP</em>, <em>31</em>,
5543–5558. (<a href="https://doi.org/10.1109/TIP.2022.3194701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is one of the best-known multivariate data analysis techniques. The NMF uniqueness and its rank selection are two major open problems in this field. The solutions uniqueness issue can be addressed by imposing the orthogonality condition on NMF. This constraint yields sparser part-based representations and improved performance in clustering and source separation tasks. However, existing orthogonal NMF algorithms rely mainly on non-probabilistic frameworks that ignore the noise inherent in real-life data and lack variable uncertainties. Thus, in this work, we investigate a new probabilistic formulation of orthogonal NMF (ONMF). In the proposed model, we impose the orthogonality through a directional prior distribution defined on the Stiefel manifold called von Mises-Fisher distribution. This manifold consists of a set of directions that comply with the orthogonality condition that arises in many applications. Moreover, our model involves an automatic relevance determination (ARD) prior to address the model order selection issue. We devised an efficient variational Bayesian inference algorithm to solve the proposed ONMF model, which allows fast processing of large datasets. We evaluated the proposed model, called VBONMF, on the task of blind decomposition of real-world multispectral images of ancient documents. The numerical experiments demonstrate its efficiency and competitiveness compared to the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Abderrahmane Rahiche and Mohamed Cheriet},
  doi          = {10.1109/TIP.2022.3194701},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5543-5558},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational bayesian orthogonal nonnegative matrix factorization over the stiefel manifold},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pattern-based reconstruction of k-level images from cutsets.
<em>TIP</em>, <em>31</em>, 5529–5542. (<a
href="https://doi.org/10.1109/TIP.2022.3196171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a pattern-based approach for reconstructing a K-level image from cutsets, dense samples taken along a family of lines or curves in two- or three-dimensional space, which break the image into blocks, each of which is typically reconstructed independently of the others. The pattern-based approach utilizes statistics of human segmentations to generate a codebook of patterns, each of which represents a pair of a block boundary specification and the corresponding pattern in the block interior. We develop the approach for rectangular cutset topologies and show that it can be extended to general periodic sampling topologies. We also show that, for bilevel cutset reconstruction, the pattern-based can be combined with the previously proposed cutset-MRF approach to substantially reduce the size of the codebook with a slight increase in reconstruction error. In addition, we present an algorithm for segmenting the cutset samples of an original grayscale or color image, followed by reconstruction of the full segmentation field via the pattern-based approach. Experimental results show that the proposed approaches outperform the cutset-MRF approaches in terms of both reconstruction error rate and perceptual quality. Moreover, this is accomplished without any side information about the structure of the block interior. Systematic comparisons of the performance of different sampling topologies are also provided.},
  archive      = {J_TIP},
  author       = {Shengxin Zha and Daizong Tian and Thrasyvoulos N. Pappas},
  doi          = {10.1109/TIP.2022.3196171},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5529-5542},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pattern-based reconstruction of K-level images from cutsets},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed-supervised scene text detection with
expectation-maximization algorithm. <em>TIP</em>, <em>31</em>,
5513–5528. (<a href="https://doi.org/10.1109/TIP.2022.3197987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text detection is an important and challenging task in computer vision. For detecting arbitrarily-shaped texts, most existing methods require heavy data labeling efforts to produce polygon-level text region labels for supervised training. In order to reduce the cost in data labeling, we study mixed-supervised arbitrarily-shaped text detection by combining various weak supervision forms (e.g., image-level tags, coarse, loose and tight bounding boxes), which are far easier to annotate. Whereas the existing weakly-supervised learning methods (such as multiple instance learning) do not promote full object coverage, to approximate the performance of fully-supervised detection, we propose an Expectation-Maximization (EM) based mixed-supervised learning framework to train scene text detector using only a small amount of polygon-level annotated data combined with a large amount of weakly annotated data. The polygon-level labels are treated as latent variables and recovered from the weak labels by the EM algorithm. A new contour-based scene text detector is also proposed to facilitate the use of weak labels in our mixed-supervised learning framework. Extensive experiments on six scene text benchmarks show that (1) using only 10\% strongly annotated data and 90\% weakly annotated data, our method yields comparable performance to that of fully supervised methods, (2) with 100\% strongly annotated data, our method achieves state-of-the-art performance on five scene text benchmarks (CTW1500, Total-Text, ICDAR-ArT, MSRA-TD500, and C-SVT), and competitive results on the ICDAR2015 Dataset. We will make our weakly annotated datasets publicly available.},
  archive      = {J_TIP},
  author       = {Mengbiao Zhao and Wei Feng and Fei Yin and Xu-Yao Zhang and Cheng-Lin Liu},
  doi          = {10.1109/TIP.2022.3197987},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5513-5528},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mixed-supervised scene text detection with expectation-maximization algorithm},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A machine learning approach to design of aperiodic,
clustered-dot halftone screens via direct binary search. <em>TIP</em>,
<em>31</em>, 5498–5512. (<a
href="https://doi.org/10.1109/TIP.2022.3196821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aperiodic, clustered-dot, halftone patterns have recently become popular for commercial printing of continuous-tone images with laser, electrophotographic presses, because of their inherent stability and resistance to moiré artifacts. Halftone screens designed using the multistage, multipass, clustered direct binary search (MS-MP-CLU-DBS) algorithm can yield halftone patterns with very high visual quality. But the characteristics of these halftone patterns depend on three input parameters for which there are no known formulas to choose their values to yield halftone patterns of a certain quality level and scale. Using machine learning methods, two predictors are developed that take as input these three parameters. One predicts the quality level of the halftone pattern. The other one predicts the scale of the halftone pattern. To provide ground truth information for training these predictors, human subjects viewed a large number of halftone patches generated from MS-MP-CLU-DBS-designed screens and assigned each patch to one of four quality levels. For each patch, the location of the peak in the radially averaged power spectrum (RAPS) is calculated as a measure of the scale or effective line frequency of the pattern. Experimental results demonstrate the accuracy of the two predictors and the effectiveness of screen design procedures based on these predictors to generate both monochrome and color high quality halftone images.},
  archive      = {J_TIP},
  author       = {Tal Frank and Jiayin Liu and Shani Gat and Oren Haik and Orel Bat Mor and Itamar Roth and Jan P. Allebach and Yitzhak Yitzhaky},
  doi          = {10.1109/TIP.2022.3196821},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5498-5512},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A machine learning approach to design of aperiodic, clustered-dot halftone screens via direct binary search},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial-temporal pyramid graph reasoning for action
recognition. <em>TIP</em>, <em>31</em>, 5484–5497. (<a
href="https://doi.org/10.1109/TIP.2022.3196175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial-temporal relation reasoning is a significant yet challenging problem for video action recognition. Previous works typically apply local operations like 2D or 3D CNNs to conduct space-time interactions in video sequences, or simply capture space-time long-range relations of a single fixed scale. However, this is inadequate for obtaining a comprehensive action representation. Besides, most models treat all input frames equally for the final classification, without selecting key frames and motion-sensitive regions. This introduces irrelevant video content and hurts the performance of models. In this paper, we propose a generic Spatial-Temporal Pyramid Graph Network (STPG-Net) to adaptively capture long-range spatial-temporal relations in video sequences at multiple scales. Specifically, we design a temporal attention (TA) module and a spatial-temporal attention (STA) module to learn the contribution of each frame and each space-time region to an action at a feature level, respectively. We then apply the selected key information to build spatial-temporal pyramid graphs for long-range relation reasoning and more comprehensive action representation learning. STPG-Net can be flexibly integrated into 2D and 3D backbone networks in a plug-and-play manner. Extensive experiments show that it brings consistent improvements over many challenging baselines on several standard action recognition benchmarks ( i.e. , Something-Something V1 &amp; V2, and FineGym), demonstrating the effectiveness of our approach.},
  archive      = {J_TIP},
  author       = {Tiantian Geng and Feng Zheng and Xiaorong Hou and Ke Lu and Guo-Jun Qi and Ling Shao},
  doi          = {10.1109/TIP.2022.3196175},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5484-5497},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatial-temporal pyramid graph reasoning for action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMDS-net: Model guided spectral-spatial network for
hyperspectral image denoising. <em>TIP</em>, <em>31</em>, 5469–5483. (<a
href="https://doi.org/10.1109/TIP.2022.3196826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) based hyperspectral images (HSIs) denoising approaches directly learn the nonlinear mapping between noisy and clean HSI pairs. They usually do not consider the physical characteristics of HSIs. This drawback makes the models lack interpretability that is key to understanding their denoising mechanism and limits their denoising ability. In this paper, we introduce a novel model-guided interpretable network for HSI denoising to tackle this problem. Fully considering the spatial redundancy, spectral low-rankness, and spectral-spatial correlations of HSIs, we first establish a subspace-based multidimensional sparse (SMDS) model under the umbrella of tensor notation. After that, the model is unfolded into an end-to-end network named SMDS-Net, whose fundamental modules are seamlessly connected with the denoising procedure and optimization of the SMDS model. This makes SMDS-Net convey clear physical meanings, i.e., learning the low-rankness and sparsity of HSIs. Finally, all key variables are obtained by discriminative training. Extensive experiments and comprehensive analysis on synthetic and real-world HSIs confirm the strong denoising ability, strong learning capability, promising generalization ability, and high interpretability of SMDS-Net against the state-of-the-art HSI denoising methods. The source code and data of this article will be made publicly available at https://github.com/bearshng/smds-net for reproducible research.},
  archive      = {J_TIP},
  author       = {Fengchao Xiong and Jun Zhou and Shuyin Tao and Jianfeng Lu and Jiantao Zhou and Yuntao Qian},
  doi          = {10.1109/TIP.2022.3196826},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5469-5483},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SMDS-net: Model guided spectral-spatial network for hyperspectral image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UIF: An objective quality assessment for underwater image
enhancement. <em>TIP</em>, <em>31</em>, 5456–5468. (<a
href="https://doi.org/10.1109/TIP.2022.3196815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to complex and volatile lighting environment, underwater imaging can be readily impaired by light scattering, warping, and noises. To improve the visual quality, Underwater Image Enhancement (UIE) techniques have been widely studied. Recent efforts have also been contributed to evaluate and compare the UIE performances with subjective and objective methods. However, the subjective evaluation is time-consuming and uneconomic for all images, while existing objective methods have limited capabilities for the newly-developed UIE approaches based on deep learning. To fill this gap, we propose an Underwater Image Fidelity (UIF) metric for objective evaluation of enhanced underwater images. By exploiting the statistical features of these images in CIELab space, we present the naturalness, sharpness, and structure indexes. Among them, the naturalness and sharpness indexes represent the visual improvements of enhanced images; the structure index indicates the structural similarity between the underwater images before and after UIE. We combine all indexes with a saliency-based spatial pooling and thus obtain the final UIF metric. To evaluate the proposed metric, we also establish a first-of-its-kind large-scale UIE database with subjective scores, namely Underwater Image Enhancement Database (UIED). Experimental results confirm that the proposed UIF metric outperforms a variety of underwater and general-purpose image quality metrics. The database and source code are available at https://github.com/z21110008/UIF .},
  archive      = {J_TIP},
  author       = {Yannan Zheng and Weiling Chen and Rongfu Lin and Tiesong Zhao and Patrick Le Callet},
  doi          = {10.1109/TIP.2022.3196815},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5456-5468},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UIF: An objective quality assessment for underwater image enhancement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image enhancement with hyper-laplacian
reflectance priors. <em>TIP</em>, <em>31</em>, 5442–5455. (<a
href="https://doi.org/10.1109/TIP.2022.3196546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement aims at improving the visibility and eliminating color distortions of underwater images degraded by light absorption and scattering in water. Recently, retinex variational models show remarkable capacity of enhancing images by estimating reflectance and illumination in a retinex decomposition course. However, ambiguous details and unnatural color still challenge the performance of retinex variational models on underwater image enhancement. To overcome these limitations, we propose a hyper-laplacian reflectance priors inspired retinex variational model to enhance underwater images. Specifically, the hyper-laplacian reflectance priors are established with the $l_{1/2}$ -norm penalty on first-order and second-order gradients of the reflectance. Such priors exploit sparsity-promoting and complete-comprehensive reflectance that is used to enhance both salient structures and fine-scale details and recover the naturalness of authentic colors. Besides, the $l_{2}$ norm is found to be suitable for accurately estimating the illumination. As a result, we turn a complex underwater image enhancement issue into simple subproblems that separately and simultaneously estimate the reflection and the illumination that are harnessed to enhance underwater images in a retinex variational model. We mathematically analyze and solve the optimal solution of each subproblem. In the optimization course, we develop an alternating minimization algorithm that is efficient on element-wise operations and independent of additional prior knowledge of underwater conditions. Extensive experiments demonstrate the superiority of the proposed method in both subjective results and objective assessments over existing methods. The code is available at: https://github.com/zhuangpeixian/HLRP .},
  archive      = {J_TIP},
  author       = {Peixian Zhuang and Jiamin Wu and Fatih Porikli and Chongyi Li},
  doi          = {10.1109/TIP.2022.3196546},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5442-5455},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Underwater image enhancement with hyper-laplacian reflectance priors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end temporal action detection with transformer.
<em>TIP</em>, <em>31</em>, 5427–5441. (<a
href="https://doi.org/10.1109/TIP.2022.3195321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7\% mAP) and HACS Segments (32.09\% mAP). Combined with an extra action classifier, it obtains 36.75\% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR .},
  archive      = {J_TIP},
  author       = {Xiaolong Liu and Qimeng Wang and Yao Hu and Xu Tang and Shiwei Zhang and Song Bai and Xiang Bai},
  doi          = {10.1109/TIP.2022.3195321},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5427-5441},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end temporal action detection with transformer},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Content-aware scalable deep compressed sensing.
<em>TIP</em>, <em>31</em>, 5412–5426. (<a
href="https://doi.org/10.1109/TIP.2022.3195319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To more efficiently address image compressed sensing (CS) problems, we present a novel content-aware scalable network dubbed CASNet which collectively achieves adaptive sampling rate allocation, fine granular scalability and high-quality reconstruction. We first adopt a data-driven saliency detector to evaluate the importance of different image regions and propose a saliency-based block ratio aggregation (BRA) strategy for sampling rate allocation. A unified learnable generating matrix is then developed to produce sampling matrix of any CS ratio with an ordered structure. Being equipped with the optimization-inspired recovery subnet guided by saliency information and a multi-block training scheme preventing blocking artifacts, CASNet jointly reconstructs the image blocks sampled at various sampling rates with one single model. To accelerate training convergence and improve network robustness, we propose an SVD-based initialization scheme and a random transformation enhancement (RTE) strategy, which are extensible without introducing extra parameters. All the CASNet components can be combined and learned end-to-end. We further provide a four-stage implementation for evaluation and practical deployments. Experiments demonstrate that CASNet outperforms other CS networks by a large margin, validating the collaboration and mutual supports among its components and strategies. Codes are available at https://github.com/Guaishou74851/CASNet .},
  archive      = {J_TIP},
  author       = {Bin Chen and Jian Zhang},
  doi          = {10.1109/TIP.2022.3195319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5412-5426},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Content-aware scalable deep compressed sensing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing an illumination-aware network for deep image
relighting. <em>TIP</em>, <em>31</em>, 5396–5411. (<a
href="https://doi.org/10.1109/TIP.2022.3195366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lighting is a determining factor in photography that affects the style, expression of emotion, and even quality of images. Creating or finding satisfying lighting conditions, in reality, is laborious and time-consuming, so it is of great value to develop a technology to manipulate illumination in an image as post-processing. Although previous works have explored techniques based on the physical viewpoint for relighting images, extensive supervisions and prior knowledge are necessary to generate reasonable images, restricting the generalization ability of these works. In contrast, we take the viewpoint of image-to-image translation and implicitly merge ideas of the conventional physical viewpoint. In this paper, we present an Illumination-Aware Network (IAN) which follows the guidance from hierarchical sampling to progressively relight a scene from a single image with high efficiency. In addition, an I llumination- A ware R esidual B lock (IARB) is designed to approximate the physical rendering process and to extract precise descriptors of light sources for further manipulations. We also introduce a depth-guided geometry encoder for acquiring valuable geometry- and structure-related representations once the depth information is available. Experimental results show that our proposed method produces better quantitative and qualitative relighting results than previous state-of-the-art methods. The code and models are publicly available on https://github.com/NK-CS-ZZL/IAN .},
  archive      = {J_TIP},
  author       = {Zuo-Liang Zhu and Zhen Li and Rui-Xun Zhang and Chun-Le Guo and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2022.3195366},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5396-5411},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Designing an illumination-aware network for deep image relighting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dense pixel-level interpretation of dynamic scenes with
video panoptic segmentation. <em>TIP</em>, <em>31</em>, 5383–5395. (<a
href="https://doi.org/10.1109/TIP.2022.3183440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A holistic understanding of dynamic scenes is of fundamental importance in real-world computer vision problems such as autonomous driving, augmented reality and spatio-temporal reasoning. In this paper, we propose a new computer vision benchmark: Video Panoptic Segmentation (VPS). To study this important problem, we present two datasets, Cityscapes-VPS and VIPER together with a new evaluation metric, video panoptic quality (VPQ). We also propose VPSNet++, an advanced video panoptic segmentation network, which simultaneously performs classification, detection, segmentation, and tracking of all identities in videos. Specifically, VPSNet++ builds upon a top-down panoptic segmentation network by adding pixel-level feature fusion head and object-level association head. The former temporally augments the pixel features while the latter performs object tracking. Furthermore, we propose panoptic boundary learning as an auxiliary task, and instance discrimination learning which learns spatio-temporally clustered pixel embedding for individual thing or stuff regions, i.e., exactly the objective of the video panoptic segmentation problem. Our VPSNet++ significantly outperforms the default VPSNet, i.e., FuseTrack baseline, and achieves state-of-the-art results on both Cityscapes-VPS and VIPER datasets. The datasets, metric, and models are publicly available at https://github.com/mcahny/vps .},
  archive      = {J_TIP},
  author       = {Dahun Kim and Sanghyun Woo and Joon-Young Lee and In So Kweon},
  doi          = {10.1109/TIP.2022.3183440},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5383-5395},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dense pixel-level interpretation of dynamic scenes with video panoptic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive boosting for domain adaptation: Toward robust
predictions in scene segmentation. <em>TIP</em>, <em>31</em>, 5371–5382.
(<a href="https://doi.org/10.1109/TIP.2022.3195642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is to transfer the shared knowledge learned from the source domain to a new environment, i.e. , target domain. One common practice is to train the model on both labeled source-domain data and unlabeled target-domain data. Yet the learned models are usually biased due to the strong supervision of the source domain. Most researchers adopt the early-stopping strategy to prevent over-fitting, but when to stop training remains a challenging problem since the lack of the target-domain validation set. In this paper, we propose one efficient bootstrapping method, called Adaboost Student, explicitly learning complementary models during training and liberating users from empirical early stopping. Adaboost Student combines deep model learning with the conventional training strategy, i.e. , adaptive boosting, and enables interactions between learned models and the data sampler. We adopt one adaptive data sampler to progressively facilitate learning on hard samples and aggregate “weak” models to prevent over-fitting. Extensive experiments show that (1) Without the need to worry about the stopping time, AdaBoost Student provides one robust solution by efficient complementary model learning during training. (2) AdaBoost Student is orthogonal to most domain adaptation methods, which can be combined with existing approaches to further improve the state-of-the-art performance. We have achieved competitive results on three widely-used scene segmentation domain adaptation benchmarks.},
  archive      = {J_TIP},
  author       = {Zhedong Zheng and Yi Yang},
  doi          = {10.1109/TIP.2022.3195642},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5371-5382},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive boosting for domain adaptation: Toward robust predictions in scene segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HQ2CL: A high-quality class center learning system for deep
face recognition. <em>TIP</em>, <em>31</em>, 5359–5370. (<a
href="https://doi.org/10.1109/TIP.2022.3195638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefited from the proposals of function losses margin-based, face recognition has achieved significant improvements in recent years. Those losses aim to increase the margin between the different identities to enhance the discriminability. Ideally, the class center of different identities is far from each other, and face samples are compact around the corresponding class center. Hence, it’s very vital to produce a high-quality class center. However, the distribution of training sets determines the class center. With low-quality samples being in the majority, the class center would be close to the samples with little identity information. As a result, it would impair the discriminability of the learned model for those unseen samples. In this work, we propose a High-Quality Class Center Learning system (HQ2CL). This is an effective system and guides the class center to approach the high-quality samples to keep the discriminability. Specifically, HQ2CL introduces a quality-aware scale and margin layer for the identification loss and constructs a new high-quality center loss. We implement the proposed system without additional burden. And we present the experimental evaluation over different face benchmarks. The experimental results show the superiority of our proposed HQ2CL over the state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Xianwei Lv and Chen Yu and Hai Jin and Kai Liu},
  doi          = {10.1109/TIP.2022.3195638},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5359-5370},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HQ2CL: A high-quality class center learning system for deep face recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint specifics and consistency hash learning for
large-scale cross-modal retrieval. <em>TIP</em>, <em>31</em>, 5343–5358.
(<a href="https://doi.org/10.1109/TIP.2022.3195059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the dramatic increase in the amount of multimedia data, cross-modal similarity retrieval has become one of the most popular yet challenging problems. Hashing offers a promising solution for large-scale cross-modal data searching by embedding the high-dimensional data into the low-dimensional similarity preserving Hamming space. However, most existing cross-modal hashing usually seeks a semantic representation shared by multiple modalities, which cannot fully preserve and fuse the discriminative modal-specific features and heterogeneous similarity for cross-modal similarity searching. In this paper, we propose a joint specifics and consistency hash learning method for cross-modal retrieval. Specifically, we introduce an asymmetric learning framework to fully exploit the label information for discriminative hash code learning, where 1) each individual modality can be better converted into a meaningful subspace with specific information, 2) multiple subspaces are semantically connected to capture consistent information, and 3) the integration complexity of different subspaces is overcome so that the learned collaborative binary codes can merge the specifics with consistency. Then, we introduce an alternatively iterative optimization to tackle the specifics and consistency hashing learning problem, making it scalable for large-scale cross-modal retrieval. Extensive experiments on five widely used benchmark databases clearly demonstrate the effectiveness and efficiency of our proposed method on both one-cross-one and one-cross-two retrieval tasks.},
  archive      = {J_TIP},
  author       = {Jianyang Qin and Lunke Fei and Zheng Zhang and Jie Wen and Yong Xu and David Zhang},
  doi          = {10.1109/TIP.2022.3195059},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5343-5358},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint specifics and consistency hash learning for large-scale cross-modal retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised matting-specific portrait enhancement and
generation. <em>TIP</em>, <em>31</em>, 5332–5342. (<a
href="https://doi.org/10.1109/TIP.2022.3194711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We resolve the ill-posed alpha matting problem from a completely different perspective. Given an input portrait image, instead of estimating the corresponding alpha matte, we focus on the other end, to subtly enhance this input so that the alpha matte can be easily estimated by any existing matting models. This is accomplished by exploring the latent space of GAN models. It is demonstrated that interpretable directions can be found in the latent space and they correspond to semantic image transformations. We further explore this property in alpha matting. Particularly, we invert an input portrait into the latent code of StyleGAN, and our aim is to discover whether there is an enhanced version in the latent space which is more compatible with a reference matting model. We optimize multi-scale latent vectors in the latent spaces under four tailored losses, ensuring matting-specificity and subtle modifications on the portrait. We demonstrate that the proposed method can refine real portrait images for arbitrary matting models, boosting the performance of automatic alpha matting by a large margin. In addition, we leverage the generative property of StyleGAN, and propose to generate enhanced portrait data which can be treated as the pseudo GT. It addresses the problem of expensive alpha matte annotation, further augmenting the matting performance of existing models.},
  archive      = {J_TIP},
  author       = {Yangyang Xu and Zeyang Zhou and Shengfeng He},
  doi          = {10.1109/TIP.2022.3194711},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5332-5342},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised matting-specific portrait enhancement and generation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image compression using stochastic-AFD based multisignal
sparse representation. <em>TIP</em>, <em>31</em>, 5317–5331. (<a
href="https://doi.org/10.1109/TIP.2022.3194696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive Fourier decomposition (AFD) is a newly developed signal processing tool that can adaptively decompose any single signal using a Szegö kernel dictionary. To process multiple signals, a novel stochastic-AFD (SAFD) theory was recently proposed. The innovation of this study is twofold. First, a SAFD-based general multi-signal sparse representation learning algorithm is designed and implemented for the first time in the literature, which can be used in many signal and image processing areas. Second, a novel SAFD based image compression framework is proposed. The algorithm design and implementation of the SAFD theory and image compression methods are presented in detail. The proposed compression methods are compared with 13 other state-of-the-art compression methods, including JPEG, JPEG2000, BPG, and other popular deep learning-based methods. The experimental results show that our methods achieve the best balanced performance. The proposed methods are based on single image adaptive sparse representation learning, and they require no pre-training. In addition, the decompression quality or compression efficiency can be easily adjusted by a single parameter, that is, the decomposition level. Our method is supported by a solid mathematical foundation, which has the potential to become a new core technology in image compression.},
  archive      = {J_TIP},
  author       = {Lei Dai and Liming Zhang and Hong Li},
  doi          = {10.1109/TIP.2022.3194696},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5317-5331},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image compression using stochastic-AFD based multisignal sparse representation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted correlation embedding learning for domain
adaptation. <em>TIP</em>, <em>31</em>, 5303–5316. (<a
href="https://doi.org/10.1109/TIP.2022.3193758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation leverages rich knowledge from a related source domain so that it can be used to perform tasks in a target domain. For more knowledge to be obtained under relaxed conditions, domain adaptation methods have been widely used in pattern recognition and image classification. However, most of the existing domain adaptation methods only consider how to minimize different distributions of the source and target domains, which neglects what should be transferred for a specific task and suffers negative transfer by distribution outliers. To address these problems, in this paper, we propose a novel domain adaptation method called weighted correlation embedding learning (WCEL) for image classification. In the WCEL approach, we seamlessly integrated correlation learning, graph embedding, and sample reweighting into a unified learning model. Specifically, we extracted the maximum correlated features from the source and target domains for image classification tasks. In addition, two graphs were designed to preserve the discriminant information from interclass samples and neighborhood relations in intraclass samples. Furthermore, to prevent the negative transfer problem, we developed an efficient sample reweighting strategy to predict the target with different confidence levels. To verify the performance of the proposed method in image classification, extensive experiments were conducted with several benchmark databases, verifying the superiority of the WCEL method over other state-of-the-art domain adaptation algorithms.},
  archive      = {J_TIP},
  author       = {Yuwu Lu and Qi Zhu and Bob Zhang and Zhihui Lai and Xuelong Li},
  doi          = {10.1109/TIP.2022.3193758},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5303-5316},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weighted correlation embedding learning for domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). APSNet: Toward adaptive point sampling for efficient 3D
action recognition. <em>TIP</em>, <em>31</em>, 5287–5302. (<a
href="https://doi.org/10.1109/TIP.2022.3193290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observing that it is still a challenging task to deploy 3D action recognition methods in real-world scenarios, in this work, we investigate the accuracy-efficiency trade-off for 3D action recognition. We first introduce a simple and efficient backbone network structure for 3D action recognition, in which we directly extract the geometry and motion representations from the raw point cloud videos through a set of simple operations ( i.e., coordinate offset generation and mini-PoinNet). Based on the backbone network, we propose an end-to-end optimized network called adaptive point sampling network (APSNet) to achieve the accuracy-efficiency trade-off, which mainly consists of three stages: the coarse feature extraction stage, the decision making stage, and the fine feature extraction stage. In APSNet, we adaptively decide the optimal resolutions ( i.e., the optimal number of points) for each pair of frames based on any input point cloud video under the given computational complexity constraint. Comprehensive experiments on multiple benchmark datasets demonstrate the effectiveness and efficiency of our newly proposed APSNet for 3D action recognition.},
  archive      = {J_TIP},
  author       = {Jiaheng Liu and Jinyang Guo and Dong Xu},
  doi          = {10.1109/TIP.2022.3193290},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5287-5302},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {APSNet: Toward adaptive point sampling for efficient 3D action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Unsupervised high-resolution portrait gaze correction and
animation. <em>TIP</em>, <em>31</em>, 5272–5286. (<a
href="https://doi.org/10.1109/TIP.2022.3191852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a gaze correction and animation method for high-resolution, unconstrained portrait images, which can be trained without the gaze angle and the head pose annotations. Common gaze-correction methods usually require annotating training data with precise gaze, and head pose information. Solving this problem using an unsupervised method remains an open problem, especially for high-resolution face images in the wild, which are not easy to annotate with gaze and head pose labels. To address this issue, we first create two new portrait datasets: CelebGaze ( $256 \times 256$ ) and high-resolution CelebHQGaze ( $512 \times 512$ ). Second, we formulate the gaze correction task as an image inpainting problem, addressed using a Gaze Correction Module (GCM) and a Gaze Animation Module (GAM). Moreover, we propose an unsupervised training strategy, i.e., Synthesis-As-Training, to learn the correlation between the eye region features and the gaze angle. As a result, we can use the learned latent space for gaze animation with semantic interpolation in this space. Moreover, to alleviate both the memory and the computational costs in the training and the inference stage, we propose a Coarse-to-Fine Module (CFM) integrated with GCM and GAM. Extensive experiments validate the effectiveness of our method for both the gaze correction and the gaze animation tasks in both low and high-resolution face datasets in the wild and demonstrate the superiority of our method with respect to the state of the art.},
  archive      = {J_TIP},
  author       = {Jichao Zhang and Jingjing Chen and Hao Tang and Enver Sangineto and Peng Wu and Yan Yan and Nicu Sebe and Wei Wang},
  doi          = {10.1109/TIP.2022.3191852},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5272-5286},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised high-resolution portrait gaze correction and animation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extendable multiple nodes recurrent tracking framework with
RTU++. <em>TIP</em>, <em>31</em>, 5257–5271. (<a
href="https://doi.org/10.1109/TIP.2022.3192706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, tracking-by-detection has become a popular paradigm in Multiple-object tracking (MOT) for its concise pipeline. Many current works first associate the detections to form track proposals and then score proposalns by manual functions to select the best. However, long-term tracking information is lost in this way due to detection failure or heavy occlusion. In this paper, the Extendable Multiple Nodes Tracking framework (EMNT) is introduced to model the association. Instead of detections, EMNT creates four basic types of nodes including correct, false, dummy and termination to generally model the tracking procedure. Further, we propose a General Recurrent Tracking Unit (RTU++) to score track proposals by capturing long-term information. In addition, we present an efficient generation method of simulated tracking data to overcome the dilemma of limited available data in MOT. The experiments show that our methods achieve state-of-the-art performance on MOT17, MOT20 and HiEve benchmarks. Meanwhile, RTU++ can be flexibly plugged into other trackers such as MHT, and bring significant improvements. The additional experiments on MOTS20 and CTMC-v1 also demonstrate the generalization ability of RTU++ trained by simulated data in various scenarios.},
  archive      = {J_TIP},
  author       = {Shuai Wang and Hao Sheng and Da Yang and Yang Zhang and Yubin Wu and Sizhe Wang},
  doi          = {10.1109/TIP.2022.3192706},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5257-5271},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Extendable multiple nodes recurrent tracking framework with RTU++},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Suppressing shot noise using quadratic variable step-size
quantization for the initial acquisition of camera-raw image data.
<em>TIP</em>, <em>31</em>, 5242–5256. (<a
href="https://doi.org/10.1109/TIP.2022.3184256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital images from today’s premium digital cameras have a higher dynamic range than what humans can perceive. Maximumly keeping the image information captured by sensors while reducing the file size of the output raw image data is highly desirable. In this paper, we show that the extractable image information contained in high-quality HDR images taken by premium cameras can be preserved using a bit-depth of only 8 to 9 bits/pixel when the shot-noise-introduced extra entropy is suppressed. The quadratic variable step-size quantization is used to suppress the shot-noise-introduced entropy. The ratio of the quantization step-size of the quadratic quantizer over the shot noise amplitude is introduced as a parameter to measure the relative quantization noise level, and is used to design the quadratic quantizers that maximally suppress the shot noise. Finally, quadratically quantized real CFA camera-raw image data are losslessly compressed. Compared with the lossless compression on linearly quantized data, at least more than 50\% bitrate savings are achieved. Such high bitrate savings without loss of extractable image information are very attractive. Thus, it is highly worthwhile to explore, in the future, hardware implementations of quadratic quantization in image sensors.},
  archive      = {J_TIP},
  author       = {Jianyu Lin and Jianxiao Qin and Shizhu Lu},
  doi          = {10.1109/TIP.2022.3184256},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5242-5256},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Suppressing shot noise using quadratic variable step-size quantization for the initial acquisition of camera-raw image data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral-spatial latent reconstruction for open-set
hyperspectral image classification. <em>TIP</em>, <em>31</em>,
5227–5241. (<a href="https://doi.org/10.1109/TIP.2022.3193747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have produced significant gains for hyperspectral image (HSI) classification in recent years, leading to high impact academic achievements and industrial applications. Despite the success of deep learning-based methods in HSI classification, they still lack the robustness of handling unknown object in open-set environment (OSE). Open-set classification is to deal with the problem of unknown classes that are not included in the training set, while in closed-set environment (CSE), unknown classes will not appear in the test set. The existing open-set classifiers almost entirely rely on the supervision information given by the known classes in the training set, which leads to the specialization of the learned representations into known classes, and makes it easy to classify unknown classes as known classes. To improve the robustness of HSI classification methods in OSE and meanwhile maintain the classification accuracy of known classes, a spectral-spatial latent reconstruction framework which simultaneously conducts spectral feature reconstruction, spatial feature reconstruction and pixel-wise classification in OSE is proposed. By reconstructing the spectral and spatial features of HSI, the learned feature representation is enhanced, so as to retain the spectral-spatial information useful for rejecting unknown classes and distinguishing known classes. The proposed method uses latent representations for spectral-spatial reconstruction, and achieves robust unknown detection without compromising the accuracy of known classes. Experimental results show that the performance of the proposed method outperforms the existing state-of-the-art methods in OSE.},
  archive      = {J_TIP},
  author       = {Jun Yue and Leyuan Fang and Min He},
  doi          = {10.1109/TIP.2022.3193747},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5227-5241},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spectral-spatial latent reconstruction for open-set hyperspectral image classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ingredient-guided region discovery and relationship modeling
for food category-ingredient prediction. <em>TIP</em>, <em>31</em>,
5214–5226. (<a href="https://doi.org/10.1109/TIP.2022.3193763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing the category and its ingredient composition from food images facilitates automatic nutrition estimation, which is crucial to various health relevant applications, such as nutrition intake management and healthy diet recommendation. Since food is composed of ingredients, discovering ingredient-relevant visual regions can help identify its corresponding category and ingredients. Furthermore, various ingredient relationships like co-occurrence and exclusion are also critical for this task. For that, we propose an ingredient-oriented multi-task food category-ingredient joint learning framework for simultaneous food recognition and ingredient prediction. This framework mainly involves learning an ingredient dictionary for ingredient-relevant visual region discovery and building an ingredient-based semantic-visual graph for ingredient relationship modeling. To obtain ingredient-relevant visual regions, we build an ingredient dictionary to capture multiple ingredient regions and obtain the corresponding assignment map, and then pool the region features belonging to the same ingredient to identify the ingredients more accurately and meanwhile improve the classification performance. For ingredient-relationship modeling, we utilize the visual ingredient representations as nodes and the semantic similarity between ingredient embeddings as edges to construct an ingredient graph, and then learn their relationships via the graph convolutional network to make label embeddings and visual features interact with each other to improve the performance. Finally, fused features from both ingredient-oriented region features and ingredient-relationship features are used in the following multi-task category-ingredient joint learning. Extensive evaluation on three popular benchmark datasets (ETH Food-101, Vireo Food-172 and ISIA Food-200) demonstrates the effectiveness of our method. Further visualization of ingredient assignment maps and attention maps also shows the superiority of our method.},
  archive      = {J_TIP},
  author       = {Zhiling Wang and Weiqing Min and Zhuo Li and Liping Kang and Xiaoming Wei and Xiaolin Wei and Shuqiang Jiang},
  doi          = {10.1109/TIP.2022.3193763},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5214-5226},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Ingredient-guided region discovery and relationship modeling for food category-ingredient prediction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep motion prior for weakly-supervised temporal action
localization. <em>TIP</em>, <em>31</em>, 5203–5213. (<a
href="https://doi.org/10.1109/TIP.2022.3193752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context-dependent motion prior, termed as motionness . Specifically, a motion graph is introduced to model motionness based on the local motion carrier ( e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-the-art performance on three challenging benchmarks, including THUMOS’14, ActivityNet v1.2 and v1.3.},
  archive      = {J_TIP},
  author       = {Meng Cao and Can Zhang and Long Chen and Mike Zheng Shou and Yuexian Zou},
  doi          = {10.1109/TIP.2022.3193752},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5203-5213},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep motion prior for weakly-supervised temporal action localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seeking subjectivity in visual emotion distribution
learning. <em>TIP</em>, <em>31</em>, 5189–5202. (<a
href="https://doi.org/10.1109/TIP.2022.3193749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Emotion Analysis (VEA), which aims to predict people’s emotions towards different visual stimuli, has become an attractive research topic recently. Rather than a single label classification task, it is more rational to regard VEA as a Label Distribution Learning (LDL) problem by voting from different individuals. Existing methods often predict visual emotion distribution in a unified network, neglecting the inherent subjectivity in its crowd voting process. In psychology, the Object-Appraisal-Emotion model has demonstrated that each individual’s emotion is affected by his/her subjective appraisal, which is further formed by the affective memory. Inspired by this, we propose a novel Subjectivity Appraise-and-Match Network (SAMNet) to investigate the subjectivity in visual emotion distribution. To depict the diversity in crowd voting process, we first propose the Subjectivity Appraising with multiple branches, where each branch simulates the emotion evocation process of a specific individual. Specifically, we construct the affective memory with an attention-based mechanism to preserve each individual’s unique emotional experience. A subjectivity loss is further proposed to guarantee the divergence between different individuals. Moreover, we propose the Subjectivity Matching with a matching loss, aiming at assigning unordered emotion labels to ordered individual predictions in a one-to-one correspondence with the Hungarian algorithm. Extensive experiments and comparisons are conducted on public visual emotion distribution datasets, and the results demonstrate that the proposed SAMNet consistently outperforms the state-of-the-art methods. Ablation study verifies the effectiveness of our method and visualization proves its interpretability.},
  archive      = {J_TIP},
  author       = {Jingyuan Yang and Jie Li and Leida Li and Xiumei Wang and Yuxuan Ding and Xinbo Gao},
  doi          = {10.1109/TIP.2022.3193749},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5189-5202},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Seeking subjectivity in visual emotion distribution learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HiSA: Hierarchically semantic associating for video temporal
grounding. <em>TIP</em>, <em>31</em>, 5178–5188. (<a
href="https://doi.org/10.1109/TIP.2022.3191841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Temporal Grounding (VTG) aims to locate the time interval in a video that is semantically relevant to a language query. Existing VTG methods interact the query with entangled video features and treat the instances in a dataset independently. However, intra-video entanglement and inter-video connection are rarely considered in these methods, leading to mismatches between the video and language. To this end, we propose a novel method, dubbed Hierarchically Semantic Associating (HiSA), which aims to precisely align the video with language and obtain discriminative representation for further location regression. Specifically, the action factors and background factors are disentangled from adjacent video segments, enforcing precise multimodal interaction and alleviating the intra-video entanglement. In addition, cross-guided contrast is elaborately framed to capture the inter-video connection, which benefits the multimodal understanding to locate the time interval. Extensive experiments on three benchmark datasets demonstrate that our approach significantly outperforms the state-of-the-art methods. The project page is available at: https://github.com/zhexu1997/HiSA .},
  archive      = {J_TIP},
  author       = {Zhe Xu and Da Chen and Kun Wei and Cheng Deng and Hui Xue},
  doi          = {10.1109/TIP.2022.3191841},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5178-5188},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HiSA: Hierarchically semantic associating for video temporal grounding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image quality evaluation in professional HDR/WCG production
questions the need for HDR metrics. <em>TIP</em>, <em>31</em>,
5163–5177. (<a href="https://doi.org/10.1109/TIP.2022.3190706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the quality evaluation of high dynamic range and wide color gamut (HDR/WCG) images, a number of works have concluded that native HDR metrics, such as HDR visual difference predictor (HDR-VDP), HDR video quality metric (HDR-VQM), or convolutional neural network (CNN)-based visibility metrics for HDR content, provide the best results. These metrics consider only the luminance component, but several color difference metrics have been specifically developed for, and validated with, HDR/WCG images. In this paper, we perform subjective evaluation experiments in a professional HDR/WCG production setting, under a real use case scenario. The results are quite relevant in that they show, firstly, that the performance of HDR metrics is worse than that of a classic, simple standard dynamic range (SDR) metric applied directly to the HDR content; and secondly, that the chrominance metrics specifically developed for HDR/WCG imaging have poor correlation with observer scores and are also outperformed by an SDR metric. Based on these findings, we show how a very simple framework for creating color HDR metrics, that uses only luminance SDR metrics, transfer functions, and classic color spaces, is able to consistently outperform, by a considerable margin, state-of-the-art HDR metrics on a varied set of HDR content, for both perceptual quantization (PQ) and Hybrid Log-Gamma (HLG) encoding, luminance and chroma distortions, and on different color spaces of common use.},
  archive      = {J_TIP},
  author       = {Yasuko Sugito and Javier Vazquez-Corral and Trevor Canham and Marcelo Bertalmío},
  doi          = {10.1109/TIP.2022.3190706},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5163-5177},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image quality evaluation in professional HDR/WCG production questions the need for HDR metrics},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-modal graph with meta concepts for video captioning.
<em>TIP</em>, <em>31</em>, 5150–5162. (<a
href="https://doi.org/10.1109/TIP.2022.3192709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning targets interpreting the complex visual contents as text descriptions, which requires the model to fully understand video scenes including objects and their interactions. Prevailing methods adopt off-the-shelf object detection networks to give object proposals and use the attention mechanism to model the relations between objects. They often miss some undefined semantic concepts of the pretrained model and fail to identify exact predicate relationships between objects. In this paper, we investigate an open research task of generating text descriptions for the given videos, and propose Cross-Modal Graph (CMG) with meta concepts for video captioning. Specifically, to cover the useful semantic concepts in video captions, we weakly learn the corresponding visual regions for text descriptions, where the associated visual regions and textual words are named cross-modal meta concepts. We further build meta concept graphs dynamically with the learned cross-modal meta concepts. We also construct holistic video-level and local frame-level video graphs with the predicted predicates to model video sequence structures. We validate the efficacy of our proposed techniques with extensive experiments and achieve state-of-the-art results on two public datasets.},
  archive      = {J_TIP},
  author       = {Hao Wang and Guosheng Lin and Steven C. H. Hoi and Chunyan Miao},
  doi          = {10.1109/TIP.2022.3192709},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5150-5162},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-modal graph with meta concepts for video captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MATR: Multimodal medical image fusion via multiscale
adaptive transformer. <em>TIP</em>, <em>31</em>, 5134–5149. (<a
href="https://doi.org/10.1109/TIP.2022.3193288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the limitations of imaging sensors, it is challenging to obtain a medical image that simultaneously contains functional metabolic information and structural tissue details. Multimodal medical image fusion, an effective way to merge the complementary information in different modalities, has become a significant technique to facilitate clinical diagnosis and surgical navigation. With powerful feature representation ability, deep learning (DL)-based methods have improved such fusion results but still have not achieved satisfactory performance. Specifically, existing DL-based methods generally depend on convolutional operations, which can well extract local patterns but have limited capability in preserving global context information. To compensate for this defect and achieve accurate fusion, we propose a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR. In the proposed method, instead of directly employing vanilla convolution, we introduce an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context. To further model long-range dependencies, an adaptive Transformer is employed to enhance the global semantic extraction capability. Our network architecture is designed in a multiscale fashion so that useful multimodal information can be adequately acquired from the perspective of different scales. Moreover, an objective function composed of a structural loss and a region mutual information loss is devised to construct constraints for information preservation at both the structural-level and the feature-level. Extensive experiments on a mainstream database demonstrate that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation. We also extend the proposed method to address other biomedical image fusion issues, and the pleasing fusion results illustrate that MATR has good generalization capability. The code of the proposed method is available at https://github.com/tthinking/MATR .},
  archive      = {J_TIP},
  author       = {Wei Tang and Fazhi He and Yu Liu and Yansong Duan},
  doi          = {10.1109/TIP.2022.3193288},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5134-5149},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MATR: Multimodal medical image fusion via multiscale adaptive transformer},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PDNet: Toward better one-stage object detection with
prediction decoupling. <em>TIP</em>, <em>31</em>, 5121–5133. (<a
href="https://doi.org/10.1109/TIP.2022.3193223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent one-stage object detectors follow a per-pixel prediction approach that predicts both the object category scores and boundary positions from every single grid location. However, the most suitable positions for inferring different targets, i.e ., the object category and boundaries, are generally different. Predicting all these targets from the same grid location thus may lead to sub-optimal results. In this paper, we analyze the suitable inference positions for object category and boundaries, and propose a prediction-target-decoupled detector named PDNet to establish a more flexible detection paradigm. Our PDNet with the prediction decoupling mechanism encodes different targets separately in different locations. A learnable prediction collection module is devised with two sets of dynamic points, i.e ., dynamic boundary points and semantic points, to collect and aggregate the predictions from the favorable regions for localization and classification. We adopt a two-step strategy to learn these dynamic point positions, where the prior positions are estimated for different targets first, and the network further predicts residual offsets to the positions with better perceptions of the object properties. Extensive experiments on the MS COCO benchmark demonstrate the effectiveness and efficiency of our method. With a single ResNeXt- $64{\times }4\text{d}$ -101-DCN as the backbone, our detector achieves 50.1 AP with single-scale testing, which outperforms the state-of-the-art methods by an appreciable margin under the same experimental settings. Moreover, our detector is highly efficient as a one-stage framework. Our code is public at https://github.com/yangli18/PDNet .},
  archive      = {J_TIP},
  author       = {Li Yang and Yan Xu and Shaoru Wang and Chunfeng Yuan and Ziqi Zhang and Bing Li and Weiming Hu},
  doi          = {10.1109/TIP.2022.3193223},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5121-5133},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PDNet: Toward better one-stage object detection with prediction decoupling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SemiCurv: Semi-supervised curvilinear structure
segmentation. <em>TIP</em>, <em>31</em>, 5109–5120. (<a
href="https://doi.org/10.1109/TIP.2022.3189823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work on curvilinear structure segmentation has mostly focused on backbone network design and loss engineering. The challenge of collecting labelled data, an expensive and labor intensive process, has been overlooked. While labelled data is expensive to obtain, unlabelled data is often readily available. In this work, we propose SemiCurv, a semi-supervised learning (SSL) framework for curvilinear structure segmentation that is able to utilize such unlabelled data to reduce the labelling burden. Our framework addresses two key challenges in formulating curvilinear segmentation in a semi-supervised manner. First, to fully exploit the power of consistency based SSL, we introduce a geometric transformation as strong data augmentation and then align segmentation predictions via a differentiable inverse transformation to enable the computation of pixel-wise consistency. Second, the traditional mean square error (MSE) on unlabelled data is prone to collapsed predictions and this issue exacerbates with severe class imbalance (significantly more background pixels). We propose a N-pair consistency loss to avoid trivial predictions on unlabelled data. We evaluate SemiCurv on six curvilinear segmentation datasets, and find that with no more than 5\% of the labelled data, it achieves close to 95\% of the performance relative to its fully supervised counterpart.},
  archive      = {J_TIP},
  author       = {Xun Xu and Manh Cuong Nguyen and Yasin Yazici and Kangkang Lu and Hlaing Min and Chuan-Sheng Foo},
  doi          = {10.1109/TIP.2022.3189823},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5109-5120},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SemiCurv: Semi-supervised curvilinear structure segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locality-guided global-preserving optimization for robust
feature matching. <em>TIP</em>, <em>31</em>, 5093–5108. (<a
href="https://doi.org/10.1109/TIP.2022.3192993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching is a fundamental problem in many computer vision tasks. This paper proposes a novel effective framework for mismatch removal, named LOcality-guided Global-preserving Optimization (LOGO). To identify inliers from a putative matching set generated by feature descriptor similarity, we introduce a fixed-point progressive approach to optimize a graph-based objective, which represents a two-class assignment problem regarding an affinity matrix containing global structures. We introduce a strategy that a small initial set with a high inlier ratio exploits the topology of the affinity matrix to elicit other inliers based on their reliable geometry, which enhances the robustness to outliers. Geometrically, we provide a locality-guided matching strategy, i.e., using local topology consensus as a criterion to determine the initial set, thus expanding to yield the final feature matching set. In addition, we apply local affine transformations based on reference points to determine the local consensus and similarity scores of nodes and edges, ensuring the validity and generality for various scenarios including complex nonrigid transformations. Extensive experiments demonstrate the effectiveness and robustness of the proposed LOGO, which is competitive with the current state-of-the-art methods. It also exhibits favorable potential for high-level vision tasks, such as essential and fundamental matrix estimation, image registration and loop closure detection.},
  archive      = {J_TIP},
  author       = {Yifan Xia and Jiayi Ma},
  doi          = {10.1109/TIP.2022.3192993},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5093-5108},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Locality-guided global-preserving optimization for robust feature matching},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot learning with class-covariance metric for
hyperspectral image classification. <em>TIP</em>, <em>31</em>,
5079–5092. (<a href="https://doi.org/10.1109/TIP.2022.3192712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, embedding and metric-based few-shot learning (FSL) has been introduced into hyperspectral image classification (HSIC) and achieved impressive progress. To further enhance the performance with few labeled samples, we in this paper propose a novel FSL framework for HSIC with a class-covariance metric (CMFSL). Overall, the CMFSL learns global class representations for each training episode by interactively using training samples from the base and novel classes, and a synthesis strategy is employed on the novel classes to avoid overfitting. During the meta-training and meta-testing, the class labels are determined directly using the Mahalanobis distance measurement rather than an extra classifier. Benefiting from the task-adapted class-covariance estimations, the CMFSL can construct more flexible decision boundaries than the commonly used Euclidean metric. Additionally, a lightweight cross-scale convolutional network (LXConvNet) consisting of 3D and 2D convolutions is designed to thoroughly exploit the spectral-spatial information in the high-frequency and low-frequency scales with low computational complexity. Furthermore, we devise a spectral-prior-based refinement module (SPRM) in the initial stage of feature extraction, which cannot only force the network to emphasize the most informative bands while suppressing the useless ones, but also alleviate the effects of the domain shift between the base and novel categories to learn a collaborative embedding mapping. Extensive experiment results on four benchmark data sets demonstrate that the proposed CMFSL can outperform the state-of-the-art methods with few-shot annotated samples.},
  archive      = {J_TIP},
  author       = {Bobo Xi and Jiaojiao Li and Yunsong Li and Rui Song and Danfeng Hong and Jocelyn Chanussot},
  doi          = {10.1109/TIP.2022.3192712},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5079-5092},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Few-shot learning with class-covariance metric for hyperspectral image classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Castle in the sky: Dynamic sky replacement and harmonization
in videos. <em>TIP</em>, <em>31</em>, 5067–5078. (<a
href="https://doi.org/10.1109/TIP.2022.3192717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a vision-based framework for dynamic sky replacement and harmonization in videos. Different from previous sky editing methods that either focus on static photos or require real-time pose signal from the camera’s inertial measurement units, our method is purely vision-based, without any requirements on the capturing devices, and can be well applied to either online or offline processing scenarios. Our method runs in real-time and is free of manual interactions. We decompose the video sky replacement into several proxy tasks, including motion estimation, sky matting, and image blending. We derive the motion equation of an object at infinity on the image plane under the camera’s motion, and propose “flow propagation”, a novel method for robust motion estimation. We also propose a coarse-to-fine sky matting network to predict accurate sky matte and design image blending to improve the harmonization. Experiments are conducted on videos diversely captured in the wild and show high fidelity and good generalization capability of our framework in both visual quality and lighting/motion dynamics. We also introduce a new method for content-aware image augmentation and proved that this method is beneficial to visual perception in autonomous driving scenarios. Our code and animated results are available at https://github.com/jiupinjia/SkyAR},
  archive      = {J_TIP},
  author       = {Zhengxia Zou and Rui Zhao and Tianyang Shi and Shuang Qiu and Zhenwei Shi},
  doi          = {10.1109/TIP.2022.3192717},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5067-5078},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Castle in the sky: Dynamic sky replacement and harmonization in videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dual-branch self-boosting framework for self-supervised 3D
hand pose estimation. <em>TIP</em>, <em>31</em>, 5052–5066. (<a
href="https://doi.org/10.1109/TIP.2022.3192708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although 3D hand pose estimation has made significant progress in recent years with the development of the deep neural network, most learning-based methods require a large amount of labeled data that is time-consuming to collect. In this paper, we propose a dual-branch self-boosting framework for self-supervised 3D hand pose estimation from depth images. First, we adopt a simple yet effective image-to-image translation technology to generate realistic depth images from synthetic data for network pre-training. Second, we propose a dual-branch network to perform 3D hand model estimation and pixel-wise pose estimation in a decoupled way. Through a part-aware model-fitting loss, the network can be updated according to the fine-grained differences between the hand model and the unlabeled real image. Through an inter-branch loss, the two complementary branches can boost each other continuously during self-supervised learning. Furthermore, we adopt a refinement stage to better utilize the prior structure information in the estimated hand model for a more accurate and robust estimation. Our method outperforms previous self-supervised methods by a large margin without using paired multi-view images and achieves comparable results to strongly supervised methods. Besides, by adopting our regenerated pose annotations, the performance of the skeleton-based gesture recognition is significantly improved.},
  archive      = {J_TIP},
  author       = {Pengfei Ren and Haifeng Sun and Jiachang Hao and Qi Qi and Jingyu Wang and Jianxin Liao},
  doi          = {10.1109/TIP.2022.3192708},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5052-5066},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A dual-branch self-boosting framework for self-supervised 3D hand pose estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistency-regularized region-growing network for semantic
segmentation of urban scenes with point-level annotations. <em>TIP</em>,
<em>31</em>, 5038–5051. (<a
href="https://doi.org/10.1109/TIP.2022.3189825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms have obtained great success in semantic segmentation of very high-resolution (VHR) remote sensing images. Nevertheless, training these models generally requires a large amount of accurate pixel-wise annotations, which is very laborious and time-consuming to collect. To reduce the annotation burden, this paper proposes a consistency-regularized region-growing network (CRGNet) to achieve semantic segmentation of VHR remote sensing images with point-level annotations. The key idea of CRGNet is to iteratively select unlabeled pixels with high confidence to expand the annotated area from the original sparse points. However, since there may exist some errors and noises in the expanded annotations, directly learning from them may mislead the training of the network. To this end, we further propose the consistency regularization strategy, where a base classifier and an expanded classifier are employed. Specifically, the base classifier is supervised by the original sparse annotations, while the expanded classifier aims to learn from the expanded annotations generated by the base classifier with the region-growing mechanism. The consistency regularization is thereby achieved by minimizing the discrepancy between the predictions from both the base and the expanded classifiers. We find such a simple regularization strategy is yet very useful to control the quality of the region-growing mechanism. Extensive experiments on two benchmark datasets demonstrate that the proposed CRGNet significantly outperforms the existing state-of-the-art methods. Codes and pre-trained models are available online ( https://github.com/YonghaoXu/CRGNet ).},
  archive      = {J_TIP},
  author       = {Yonghao Xu and Pedram Ghamisi},
  doi          = {10.1109/TIP.2022.3189825},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5038-5051},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistency-regularized region-growing network for semantic segmentation of urban scenes with point-level annotations},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robust movement quantification algorithm of hyperactivity
detection for ADHD children based on 3D depth images. <em>TIP</em>,
<em>31</em>, 5025–5037. (<a
href="https://doi.org/10.1109/TIP.2022.3185793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention deficit hyperactivity disorder (ADHD) is one of the most common childhood mental disorders. Hyperactivity is a typical symptom of ADHD in children. Clinicians diagnose this symptom by evaluating the children’s activities based on subjective rating scales and clinical experience. In this work, an objective system is proposed to quantify the movements of children with ADHD automatically. This system presents a new movement detection and quantification method based on depth images. A novel salient object extraction method is proposed to segment body regions. In movement detection, we explore a new local search algorithm to detect any potential motions of children based on three newly designed evaluation metrics. In the movement quantification, two parameters are investigated to quantify the participation degree and the displacements of each body part in the movements. This system is tested by a depth dataset of children with ADHD. The movement detection results of this dataset mainly range from 91.0\% to 95.0\%. The movement quantification results of children are consistent with the clinical observations. The public MSR Action 3D dataset is tested to validate the performance of this system.},
  archive      = {J_TIP},
  author       = {Ling He and Fei He and Yuanyuan Li and Xi Xiong and Jing Zhang},
  doi          = {10.1109/TIP.2022.3185793},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5025-5037},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A robust movement quantification algorithm of hyperactivity detection for ADHD children based on 3D depth images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Composition and style attributes guided image aesthetic
assessment. <em>TIP</em>, <em>31</em>, 5009–5024. (<a
href="https://doi.org/10.1109/TIP.2022.3191853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aesthetic quality of an image is defined as the measure or appreciation of the beauty of an image. Aesthetics is inherently a subjective property but there are certain factors that influence it such as, the semantic content of the image, the attributes describing the artistic aspect, the photographic setup used for the shot, etc. In this paper we propose a method for the automatic prediction of the aesthetics of an image that is based on the analysis of the semantic content, the artistic style and the composition of the image. The proposed network includes: a pre-trained network for semantic features extraction (the Backbone); a Multi Layer Perceptron (MLP) network that relies on the Backbone features for the prediction of image attributes (the AttributeNet); a self-adaptive Hypernetwork that exploits the attributes prior encoded into the embedding generated by the AttributeNet to predict the parameters of the target network dedicated to aesthetic estimation (the AestheticNet). Given an image, the proposed multi-network is able to predict: style and composition attributes, and aesthetic score distribution. Results on three benchmark datasets demonstrate the effectiveness of the proposed method, while the ablation study gives a better understanding of the proposed network.},
  archive      = {J_TIP},
  author       = {Luigi Celona and Marco Leonardi and Paolo Napoletano and Alessandro Rozza},
  doi          = {10.1109/TIP.2022.3191853},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {5009-5024},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Composition and style attributes guided image aesthetic assessment},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Fuzzy discriminative block representation learning for
image feature extraction. <em>TIP</em>, <em>31</em>, 4994–5008. (<a
href="https://doi.org/10.1109/TIP.2022.3191846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning is widely used to project high-dimensional data to low-dimensional subspace for feature extraction in image recognition tasks. However, many related methods barely explore the fuzziness and uncertainty between data classes. Besides, the classical unsupervised sparse constraint weakens the evaluation of feature importance and neglects the preservation of discriminant information during sparse representation. To solve these issues, a novel fuzzy discriminative block representation learning (FDBRL) algorithm is proposed for image feature extraction. FDBRL aims to enhance the discriminability of subspace by designing effective constraints for projection learning. Specifically, based on the label information and the fuzzy relation between data, we construct a fuzzy block weight matrix and embed it into the ${l_{2,1}}$ norm regularization term to realize supervised sparse constraint for the representation learning. Next, the low-rank constraint is used to capture the inherent global structure information of data. Finally, we introduce a classification loss term with transformation matrix for joint optimization, such that the projection learning is not limited to number of classes, and the discriminative ability is further improved. Comprehensive experimental results on six benchmarks verify that our method achieves promising performance with other state-of-the-arts in both robustness and effectiveness.},
  archive      = {J_TIP},
  author       = {Yun Wang and Zhenbo Li and Fei Li and Yang Mi and Jun Yue},
  doi          = {10.1109/TIP.2022.3191846},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4994-5008},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fuzzy discriminative block representation learning for image feature extraction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action keypoint network for efficient video recognition.
<em>TIP</em>, <em>31</em>, 4980–4993. (<a
href="https://doi.org/10.1109/TIP.2022.3191461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing redundancy is crucial for improving the efficiency of video recognition models. An effective approach is to select informative content from the holistic video, yielding a popular family of dynamic video recognition methods. However, existing dynamic methods focus on either temporal or spatial selection independently while neglecting a reality that the redundancies are usually spatial and temporal, simultaneously. Moreover, their selected content is usually cropped with fixed shapes ( e.g. , temporally-cropped frames, spatially-cropped patches), while the realistic distribution of informative content can be much more diverse. With these two insights, this paper proposes to integrate temporal and spatial selection into an A ction K eypoint Net work (AK-Net). From different frames and positions, AK-Net selects some informative points scattered in arbitrary-shaped regions as a set of “action keypoints” and then transforms the video recognition into point cloud classification. More concretely, AK-Net has two steps, i.e. , the keypoint selection and the point cloud classification. First, it inputs the video into a baseline network and outputs a feature map from an intermediate layer. We view each pixel on this feature map as a spatial-temporal point and select some informative keypoints using self-attention. Second, AK-Net devises a ranking criterion to arrange the keypoints into an ordered 1D sequence. Since the video is represented with a 1D sequence after the specified layer, AK-Net transforms the subsequent layers into a point cloud classification sub-net by compacting the original 2D convolutional kernels into 1D kernels. Consequentially, AK-Net brings two-fold benefits for efficiency: The keypoint selection step collects informative content within arbitrary shapes and increases the efficiency for modeling spatial-temporal dependencies, while the point cloud classification step further reduces the computational cost by compacting the convolutional kernels. Experimental results show that AK-Net can consistently improve the efficiency and performance of baseline methods on several video recognition benchmarks.},
  archive      = {J_TIP},
  author       = {Xu Chen and Yahong Han and Xiaohan Wang and Yifan Sun and Yi Yang},
  doi          = {10.1109/TIP.2022.3191461},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4980-4993},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action keypoint network for efficient video recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BQN: Busy-quiet net enabled by motion band-pass module for
action recognition. <em>TIP</em>, <em>31</em>, 4966–4979. (<a
href="https://doi.org/10.1109/TIP.2022.3189810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rich video data representation can be realized by means of spatio-temporal frequency analysis. In this research study we show that a video can be disentangled, following the learning of video characteristics according to their spatio-temporal properties, into two complementary information components, dubbed Busy and Quiet. The Busy information characterizes the boundaries of moving regions, moving objects, or regions of change in movement. Meanwhile, the Quiet information encodes global smooth spatio-temporal structures defined by substantial redundancy. We design a trainable Motion Band-Pass Module (MBPM) for separating Busy and Quiet-defined information, in raw video data. We model a Busy-Quiet Net (BQN) by embedding the MBPM into a two-pathway CNN architecture. The efficiency of BQN is determined by avoiding redundancy in the feature spaces defined by the two pathways. While one pathway processes the Busy features, the other processes Quiet features at lower spatio-temporal resolutions reducing both memory and computational costs. Through experiments we show that the proposed MBPM can be used as a plug-in module in various CNN backbone architectures, significantly boosting their performance. The proposed BQN is shown to outperform many recent video models on Something-Something V1, Kinetics400, UCF101 and HMDB51 datasets.},
  archive      = {J_TIP},
  author       = {Guoxi Huang and Adrian G. Bors},
  doi          = {10.1109/TIP.2022.3189810},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4966-4979},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BQN: Busy-quiet net enabled by motion band-pass module for action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SegGroup: Seg-level supervision for 3D instance and semantic
segmentation. <em>TIP</em>, <em>31</em>, 4952–4965. (<a
href="https://doi.org/10.1109/TIP.2022.3190709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing point cloud instance and semantic segmentation methods rely heavily on strong supervision signals, which require point-level labels for every point in the scene. However, such strong supervision suffers from large annotation costs, arousing the need to study efficient annotating. In this paper, we discover that the locations of instances matter for both instance and semantic 3D scene segmentation. By fully taking advantage of locations, we design a weakly-supervised point cloud segmentation method that only requires clicking on one point per instance to indicate its location for annotation. With over-segmentation for pre-processing, we extend these location annotations into segments as seg-level labels. We further design a segment grouping network (SegGroup) to generate point-level pseudo labels under seg-level labels by hierarchically grouping the unlabeled segments into the relevant nearby labeled segments, so that existing point-level supervised segmentation models can directly consume these pseudo labels for training. Experimental results show that our seg-level supervised method (SegGroup) achieves comparable results with the fully annotated point-level supervised methods. Moreover, it outperforms the recent weakly-supervised methods given a fixed annotation budget. Code is available at https://github.com/antao97/SegGroup .},
  archive      = {J_TIP},
  author       = {An Tao and Yueqi Duan and Yi Wei and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TIP.2022.3190709},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4952-4965},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SegGroup: Seg-level supervision for 3D instance and semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From whole video to frames: Weakly-supervised domain
adaptive continuous-time QoE evaluation. <em>TIP</em>, <em>31</em>,
4937–4951. (<a href="https://doi.org/10.1109/TIP.2022.3190711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid increase in video traffic and relatively limited delivery infrastructure, end users often experience dynamically varying quality over time when viewing streaming videos. The user quality-of-experience (QoE) must be continuously monitored to deliver an optimized service. However, modern approaches for continuous-time video QoE estimation require densely annotating the continuous-time QoE labels, which is labor-intensive and time-consuming. To cope with such limitations, we propose a novel weakly-supervised domain adaptation approach for continuous-time QoE evaluation, by making use of a small amount of continuously labeled data in the source domain and abundant weakly-labeled data (only containing the retrospective QoE labels) in the target domain. Specifically, given a pair of videos from source and target domains, effective spatiotemporal segment-level feature representation is first learned by a combination of 2D and 3D convolutional networks. Then, a multi-task prediction framework is developed to simultaneously achieve continuous-time and retrospective QoE predictions, where a quality attentive adaptation approach is investigated to effectively alleviate the domain discrepancy without hampering the prediction performance. This approach is enabled by explicitly attending to the video-level discrimination and segment-level transferability in terms of the domain discrepancy. Experiments on benchmark databases demonstrate that the proposed method significantly improves the prediction performance under the cross-domain setting.},
  archive      = {J_TIP},
  author       = {Leida Li and Pengfei Chen and Weisi Lin and Mai Xu and Guangming Shi},
  doi          = {10.1109/TIP.2022.3190711},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4937-4951},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {From whole video to frames: Weakly-supervised domain adaptive continuous-time QoE evaluation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Twin adversarial contrastive learning for underwater image
enhancement and beyond. <em>TIP</em>, <em>31</em>, 4922–4936. (<a
href="https://doi.org/10.1109/TIP.2022.3190209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images suffer from severe distortion, which degrades the accuracy of object detection performed in an underwater environment. Existing underwater image enhancement algorithms focus on the restoration of contrast and scene reflection. In practice, the enhanced images may not benefit the effectiveness of detection and even lead to a severe performance drop. In this paper, we propose an object-guided twin adversarial contrastive learning based underwater enhancement method to achieve both visual-friendly and task-orientated enhancement. Concretely, we first develop a bilateral constrained closed-loop adversarial enhancement module, which eases the requirement of paired data with the unsupervised manner and preserves more informative features by coupling with the twin inverse mapping. In addition, to confer the restored images with a more realistic appearance, we also adopt the contrastive cues in the training phase. To narrow the gap between visually-oriented and detection-favorable target images, a task-aware feedback module is embedded in the enhancement process, where the coherent gradient information of the detector is incorporated to guide the enhancement towards the detection-pleasing direction. To validate the performance, we allocate a series of prolific detectors into our framework. Extensive experiments demonstrate that the enhanced results of our method show remarkable amelioration in visual quality, the accuracy of different detectors conducted on our enhanced images has been promoted notably. Moreover, we also conduct a study on semantic segmentation to illustrate how object guidance improves high-level tasks. Code and models are available at https://github.com/Jzy2017/TACL},
  archive      = {J_TIP},
  author       = {Risheng Liu and Zhiying Jiang and Shuzhou Yang and Xin Fan},
  doi          = {10.1109/TIP.2022.3190209},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4922-4936},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Twin adversarial contrastive learning for underwater image enhancement and beyond},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive face recognition using adversarial information
network. <em>TIP</em>, <em>31</em>, 4909–4921. (<a
href="https://doi.org/10.1109/TIP.2022.3189830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, face recognition models often degenerate when training data (referred to as source domain) are different from testing data (referred to as target domain). To alleviate this mismatch caused by some factors like pose and skin tone, the utilization of pseudo-labels generated by clustering algorithms is an effective way in unsupervised domain adaptation. However, they always miss some hard positive samples. Supervision on pseudo-labeled samples attracts them towards their prototypes and would cause an intra-domain gap between pseudo-labeled samples and the remaining unlabeled samples within target domain, which results in the lack of discrimination in face recognition. In this paper, considering the particularity of face recognition, we propose a novel adversarial information network (AIN) to address it. First, a novel adversarial mutual information (MI) loss is proposed to alternately minimize MI with respect to the target classifier and maximize MI with respect to the feature extractor. By this min-max manner, the positions of target prototypes are adaptively modified which makes unlabeled images clustered more easily such that intra-domain gap can be mitigated. Second, to assist adversarial MI loss, we utilize a graph convolution network to predict linkage likelihoods between target data and generate pseudo-labels. It leverages valuable information in the context of nodes and can achieve more reliable results. The proposed method is evaluated under two scenarios, i.e., domain adaptation across poses and image conditions, and domain adaptation across faces with different skin tones. Extensive experiments show that AIN successfully improves cross-domain generalization and offers a new state-of-the-art on RFW dataset.},
  archive      = {J_TIP},
  author       = {Mei Wang and Weihong Deng},
  doi          = {10.1109/TIP.2022.3189830},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4909-4921},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive face recognition using adversarial information network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-light enhancement using a plug-and-play retinex model
with shrinkage mapping for illumination estimation. <em>TIP</em>,
<em>31</em>, 4897–4908. (<a
href="https://doi.org/10.1109/TIP.2022.3189805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light photography conditions degrade image quality. This study proposes a novel Retinex-based low-light enhancement method to correctly decompose an input image into reflectance and illumination. Subsequently, we can improve the viewing experience by adjusting the illumination using intensity and contrast enhancement. Because image decomposition is a highly ill-posed problem, constraints must be properly imposed on the optimization framework. To meet the criteria of ideal Retinex decomposition, we design a nonconvex $L_{p}$ norm and apply shrinkage mapping to the illumination layer. In addition, edge-preserving filters are introduced using the plug-and-play technique to improve illumination. Pixel-wise weights based on variance and image gradients are adopted to suppress noise and preserve details in the reflectance layer. We choose the alternating direction method of multipliers (ADMM) to solve the problem efficiently. Experimental results on several challenging low-light datasets show that our proposed method can more effectively enhance image brightness as compared with state-of-the-art methods. In addition to subjective observations, the proposed method also achieved competitive performance in objective image quality assessments.},
  archive      = {J_TIP},
  author       = {Yi-Hsien Lin and Yi-Chang Lu},
  doi          = {10.1109/TIP.2022.3189805},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4897-4908},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-light enhancement using a plug-and-play retinex model with shrinkage mapping for illumination estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action recognition with motion diversification and dynamic
selection. <em>TIP</em>, <em>31</em>, 4884–4896. (<a
href="https://doi.org/10.1109/TIP.2022.3189811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion modeling is crucial in modern action recognition methods. As motion dynamics like moving tempos and action amplitude may vary a lot in different video clips, it poses great challenge on adaptively covering proper motion information. To address this issue, we introduce a Motion Diversification and Selection (MoDS) module to generate diversified spatio-temporal motion features and then select the suitable motion representation dynamically for categorizing the input video. To be specific, we first propose a spatio-temporal motion generation (StMG) module to construct a bank of diversified motion features with varying spatial neighborhood and time range. Then, a dynamic motion selection (DMS) module is leveraged to choose the most discriminative motion feature both spatially and temporally from the feature bank. As a result, our proposed method can make full use of the diversified spatio-temporal motion information, while maintaining computational efficiency at the inference stage. Extensive experiments on five widely-used benchmarks, demonstrate the effectiveness of the method and we achieve state-of-the-art performance on Something-Something V1 &amp; V2 that are of large motion variation.},
  archive      = {J_TIP},
  author       = {Peiqin Zhuang and Yu Guo and Zhipeng Yu and Luping Zhou and Lei Bai and Ding Liang and Zhiyong Wang and Yali Wang and Wanli Ouyang},
  doi          = {10.1109/TIP.2022.3189811},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4884-4896},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action recognition with motion diversification and dynamic selection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local self-expression subspace learning network for motion
capture data. <em>TIP</em>, <em>31</em>, 4869–4883. (<a
href="https://doi.org/10.1109/TIP.2022.3189822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep subspace learning is an important branch of self-supervised learning and has been a hot research topic in recent years, but current methods do not fully consider the individualities of temporal data and related tasks. In this paper, by transforming the individualities of motion capture data and segmentation task as the supervision, we propose the local self-expression subspace learning network. Specifically, considering the temporality of motion data, we use the temporal convolution module to extract temporal features. To implement the local validity of self-expression in temporal tasks, we design the local self-expression layer which only maintains the representation relations with temporally adjacent motion frames. To simulate the interpolatability of motion data in the feature space, we impose a group sparseness constraint on the local self-expression layer to impel the representations only using selected keyframes. Besides, based on the subspace assumption, we propose the subspace projection loss, which is induced from distances of each frame projected to the fitted subspaces, to penalize the potential clustering errors. The superior performances of the proposed model on the segmentation task of synthetic data and three tasks of real motion capture data demonstrate the feature learning ability of our model.},
  archive      = {J_TIP},
  author       = {Guiyu Xia and Peng Xue and Huaijiang Sun and Yubao Sun and Du Zhang and Qingshan Liu},
  doi          = {10.1109/TIP.2022.3189822},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4869-4883},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local self-expression subspace learning network for motion capture data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Under-display camera image enhancement via cascaded curve
estimation. <em>TIP</em>, <em>31</em>, 4856–4868. (<a
href="https://doi.org/10.1109/TIP.2022.3182278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new trend of full-screen devices encourages manufacturers to position a camera behind a screen, i.e., the newly-defined Under-Display Camera (UDC). Therefore, UDC image restoration has been a new realistic single image enhancement problem. In this work, we propose a curve estimation network operating on the hue (H) and saturation (S) channels to perform adaptive enhancement for degraded images captured by UDCs. The proposed network aims to match the complicated relationship between the images captured by under-display and display-free cameras. To extract effective features, we cascade the proposed curve estimation network with sharing weights, and we introduce a spatial and channel attention module in each curve estimation network to exploit attention-aware features. In addition, we learn the curve estimation network in a semi-supervised manner to alleviate the restriction of the requirement for amounts of labeled images and improve the generalization ability for unseen degraded images in various realistic scenes. The semi-supervised network consists of a supervised branch trained on labeled data and an unsupervised branch trained on unlabeled data. To train the proposed model, we build a new dataset comprised of real-world labeled and unlabeled images. Extensive experiments demonstrate that our proposed algorithm performs favorably against state-of-the-art image enhancement methods for UDC images in terms of accuracy and speed, especially on ultra-high-definition (UHD) images.},
  archive      = {J_TIP},
  author       = {Jun Luo and Wenqi Ren and Tao Wang and Chongyi Li and Xiaochun Cao},
  doi          = {10.1109/TIP.2022.3182278},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4856-4868},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Under-display camera image enhancement via cascaded curve estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning semantic-aware local features for long term visual
localization. <em>TIP</em>, <em>31</em>, 4842–4855. (<a
href="https://doi.org/10.1109/TIP.2022.3187565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting robust and discriminative local features from images plays a vital role for long term visual localization, whose challenges are mainly caused by the severe appearance differences between matching images due to the day-night illuminations, seasonal changes, and human activities. Existing solutions resort to jointly learning both keypoints and their descriptors in an end-to-end manner, leveraged on large number of annotations of point correspondence which are harvested from the structure from motion and depth estimation algorithms. While these methods show improved performance over non-deep methods or those two-stage deep methods, i.e. , detection and then description, they are still struggled to conquer the problems encountered in long term visual localization. Since the intrinsic semantics are invariant to the local appearance changes, this paper proposes to learn semantic-aware local features in order to improve robustness of local feature matching for long term localization. Based on a state of the art CNN architecture for local feature learning, i.e. , ASLFeat, this paper leverages on the semantic information from an off-the-shelf semantic segmentation network to learn semantic-aware feature maps. The learned correspondence-aware feature descriptors and semantic features are then merged to form the final feature descriptors, for which the improved feature matching ability has been observed in experiments. In addition, the learned semantics embedded in the features can be further used to filter out noisy keypoints, leading to additional accuracy improvement and faster matching speed. Experiments on two popular long term visual localization benchmarks (Aachen Day and Night v1.1, Robotcar Seasons) and one challenging indoor benchmark (InLoc) demonstrate encouraging improvements of the localization accuracy over its counterpart and other competitive methods.},
  archive      = {J_TIP},
  author       = {Bin Fan and Junjie Zhou and Wensen Feng and Huayan Pu and Yuzhu Yang and Qingqun Kong and Fuchao Wu and Hongmin Liu},
  doi          = {10.1109/TIP.2022.3187565},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4842-4855},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning semantic-aware local features for long term visual localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast generation of superpixels with lattice topology.
<em>TIP</em>, <em>31</em>, 4828–4841. (<a
href="https://doi.org/10.1109/TIP.2022.3188155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serving as an essential step for many applications of image processing, superpixel generation has attracted a lot of attentions. Most existing superpixel generation algorithms focus on the boundary adherence and compactness of the superpixels, but ignore the topological consistency between the superpixels, which severely limites their applications in the subsequent tasks, especially in the CNN based image processing tasks. In this paper, we present a fast lattice superpixel generation algorithm, which can generate superpixels with lattice topology like the original pixels. We also propose a local similarity loss function to improve the segmentation accuracy of the generated lattice superpixels. The whole algorithm is parallelly implemented on GPU. We perform extensive experiments on three datasets (i.e., BSDS500, NYUv2 and VOC) to verify the efficacy of our algorithm. The experimental results show that our method achieves competitive results compared to the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xiao Pan and Yuanfeng Zhou and Yunfeng Zhang and Caiming Zhang},
  doi          = {10.1109/TIP.2022.3188155},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4828-4841},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast generation of superpixels with lattice topology},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Covariance estimation from compressive data partitions using
a projected gradient-based algorithm. <em>TIP</em>, <em>31</em>,
4817–4827. (<a href="https://doi.org/10.1109/TIP.2022.3187285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive covariance estimation has arisen as a class of techniques whose aim is to obtain second-order statistics of stochastic processes from compressive measurements. Recently, these methods have been used in various image processing and communications applications, including denoising, spectrum sensing, and compression. Notice that estimating the covariance matrix from compressive samples leads to ill-posed minimizations with severe performance loss at high compression rates. In this regard, a regularization term is typically aggregated to the cost function to consider prior information about a particular property of the covariance matrix. Hence, this paper proposes an algorithm based on the projected gradient method to recover low-rank or Toeplitz approximations of the covariance matrix from compressive measurements. The proposed algorithm divides the compressive measurements into data subsets projected onto different subspaces and accurately estimates the covariance matrix by solving a single optimization problem assuming that each data subset contains an approximation of the signal statistics. Furthermore, gradient filtering is included at every iteration of the proposed algorithm to minimize the estimation error. The error induced by the proposed splitting approach is analytically derived along with the convergence guarantees of the proposed method. The proposed algorithm estimates the covariance matrix of hyperspectral images from synthetic and real compressive samples. Extensive simulations show that the proposed algorithm can effectively recover the covariance matrix of hyperspectral images from compressive measurements with high compression ratios ( $8-15\%$ approx) in noisy scenarios. Moreover, simulations and theoretical results show that the filtering step reduces the recovery error up to twice the number of eigenvectors. Finally, an optical implementation is proposed, and real measurements are used to validate the theoretical findings.},
  archive      = {J_TIP},
  author       = {Jonathan Monsalve and Juan Ramirez and Iñaki Esnaola and Henry Arguello},
  doi          = {10.1109/TIP.2022.3187285},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4817-4827},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Covariance estimation from compressive data partitions using a projected gradient-based algorithm},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pseudo-pair based self-similarity learning for unsupervised
person re-identification. <em>TIP</em>, <em>31</em>, 4803–4816. (<a
href="https://doi.org/10.1109/TIP.2022.3186746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) is of great importance to video surveillance systems by estimating the similarity between a pair of cross-camera person shorts. Current methods for estimating such similarity require a large number of labeled samples for supervised training. In this paper, we present a pseudo-pair based self-similarity learning approach for unsupervised person re-ID without human annotations. Unlike conventional unsupervised re-ID methods that use pseudo labels based on global clustering, we construct patch surrogate classes as initial supervision, and propose to assign pseudo labels to images through the pairwise gradient-guided similarity separation. This can cluster images in pseudo pairs, and the pseudos can be updated during training. Based on pseudo pairs, we propose to improve the generalization of similarity function via a novel self-similarity learning:it learns local discriminative features from individual images via intra-similarity, and discovers the patch correspondence across images via inter-similarity. The intra-similarity learning is based on channel attention to detect diverse local features from an image. The inter-similarity learning employs a deformable convolution with a non-local block to align patches for cross-image similarity. Experimental results on several re-ID benchmark datasets demonstrate the superiority of the proposed method over the state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Lin Wu and Deyin Liu and Wenying Zhang and Dapeng Chen and Zongyuan Ge and Farid Boussaid and Mohammed Bennamoun and Jialie Shen},
  doi          = {10.1109/TIP.2022.3186746},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4803-4816},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pseudo-pair based self-similarity learning for unsupervised person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View-consistency learning for incomplete multiview
clustering. <em>TIP</em>, <em>31</em>, 4790–4802. (<a
href="https://doi.org/10.1109/TIP.2022.3187562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel general framework for incomplete multi-view clustering by integrating graph learning and spectral clustering. In our model, a tensor low-rank constraint are introduced to learn a stable low-dimensional representation, which encodes the complementary information and takes into account the cluster structure between different views. A corresponding algorithm associated with augmented Lagrangian multipliers is established. In particular, tensor Schatten $p$ -norm is used as a tighter approximation to the tensor rank function. Besides, both consistency and specificity are jointly exploited for subspace representation learning. Extensive experiments on benchmark datasets demonstrate that our model outperforms several baseline methods in incomplete multi-view clustering.},
  archive      = {J_TIP},
  author       = {Ziyu Lv and Quanxue Gao and Xiangdong Zhang and Qin Li and Ming Yang},
  doi          = {10.1109/TIP.2022.3187562},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4790-4802},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {View-consistency learning for incomplete multiview clustering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low-rank tensor decomposition model with factors prior and
total variation for impulsive noise removal. <em>TIP</em>, <em>31</em>,
4776–4789. (<a href="https://doi.org/10.1109/TIP.2022.3169694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration is a long-standing problem in signal processing and low-level computer vision. Previous studies have shown that imposing a low-rank Tucker decomposition (TKD) constraint could produce impressive performances. However, the TKD-based schemes may lead to the overfitting/underfitting problem because of incorrectly predefined ranks. To address this issue, we prove that the $n$ -rank is upper bounded by the rank of each Tucker factor matrix. Using this relationship, we propose a formulation by imposing the nuclear norm regularization on the latent factors of TKD, which can avoid the burden of rank selection and reduce the computational cost when dealing with large-scale tensors. In this formulation, we adopt the Minimax Concave Penalty to remove the impulsive noise instead of the $l_{1}$ -norm which may deviate from both the data-acquisition model and the prior model. Moreover, we employ an anisotropic total variation regularization to explore the piecewise smooth structure in both spatial and spectral domains. To solve this problem, we design the symmetric Gauss-Seidel (sGS) based alternating direction method of multipliers (ADMM) algorithm. Compared to the directly extended ADMM, our algorithm can achieve higher accuracy since more structural information is utilized. Finally, we conduct experiments on the three kinds of datasets, numerical results demonstrate the superiority of the proposed method, especially, the average PSNR of the proposed method can improve about 1~5dB for each noise level of color images.},
  archive      = {J_TIP},
  author       = {Xin Tian and Kun Xie and Hanling Zhang},
  doi          = {10.1109/TIP.2022.3169694},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4776-4789},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A low-rank tensor decomposition model with factors prior and total variation for impulsive noise removal},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MetaAge: Meta-learning personalized age estimators.
<em>TIP</em>, <em>31</em>, 4761–4775. (<a
href="https://doi.org/10.1109/TIP.2022.3188061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different people age in different ways. Learning a personalized age estimator for each person is a promising direction for age estimation given that it better models the personalization of aging processes. However, most existing personalized methods suffer from the lack of large-scale datasets due to the high-level requirements: identity labels and enough samples for each person to form a long-term aging pattern. In this paper, we aim to learn personalized age estimators without the above requirements and propose a meta-learning method named MetaAge for age estimation. Unlike most existing personalized methods that learn the parameters of a personalized estimator for each person in the training set, our method learns the mapping from identity information to age estimator parameters. Specifically, we introduce a personalized estimator meta-learner, which takes identity features as the input and outputs the parameters of customized estimators. In this way, our method learns the meta knowledge without the above requirements and seamlessly transfers the learned meta knowledge to the test set, which enables us to leverage the existing large-scale age datasets without any additional annotations. Extensive experimental results on three benchmark datasets including MORPH II, ChaLearn LAP 2015 and ChaLearn LAP 2016 databases demonstrate that our MetaAge significantly boosts the performance of existing personalized methods and outperforms the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Wanhua Li and Jiwen Lu and Abudukelimu Wuerkaixi and Jianjiang Feng and Jie Zhou},
  doi          = {10.1109/TIP.2022.3188061},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4761-4775},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MetaAge: Meta-learning personalized age estimators},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A temporal-aware relation and attention network for temporal
action localization. <em>TIP</em>, <em>31</em>, 4746–4760. (<a
href="https://doi.org/10.1109/TIP.2022.3182866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization is currently an active research topic in computer vision and machine learning due to its usage in smart surveillance. It is a challenging problem since the categories of the actions must be classified in untrimmed videos and the start and end of the actions need to be accurately found. Although many temporal action localization methods have been proposed, they require substantial amounts of computational resources for the training and inference processes. To solve these issues, in this work, a novel temporal-aware relation and attention network (abbreviated as TRA) is proposed for the temporal action localization task. TRA has an anchor-free and end-to-end architecture that fully uses temporal-aware information. Specifically, a temporal self-attention module is first designed to determine the relationship between different temporal positions, and more weight is given to features within the actions. Then, a multiple temporal aggregation module is constructed to aggregate the temporal domain information. Finally, a graph relation module is designed to obtain the aggregated graph features, which are used to refine the boundaries and classification results. Most importantly, these three modules are jointly explored in a unified framework, and temporal awareness is always fully used. Extensive experiments demonstrate that the proposed method can outperform all state-of-the-art methods on the THUMOS14 dataset with an average mAP that reaches 67.6\% and obtain a comparable result on the ActivityNet1.3 dataset with an average mAP that reaches 34.4\%. Compared with A2Net (TIP20), PCG-TAL (TIP21), and AFSD (CVPR21) TRA can achieve improvements of 11.7\%, 4.4\%, and 1.8\%, respectively on the THUMOS14 dataset.},
  archive      = {J_TIP},
  author       = {Yibo Zhao and Hua Zhang and Zan Gao and Weili Guan and Jie Nie and Anan Liu and Meng Wang and Shengyong Chen},
  doi          = {10.1109/TIP.2022.3182866},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4746-4760},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A temporal-aware relation and attention network for temporal action localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Partially supervised compatibility modeling. <em>TIP</em>,
<em>31</em>, 4733–4745. (<a
href="https://doi.org/10.1109/TIP.2022.3187290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion Compatibility Modeling (FCM), which aims to automatically evaluate whether a given set of fashion items makes a compatible outfit, has attracted increasing research attention. Recent studies have demonstrated the benefits of conducting the item representation disentanglement towards FCM. Although these efforts have achieved prominent progress, they still perform unsatisfactorily, as they mainly investigate the visual content of fashion items, while overlooking the semantic attributes of items (e.g., color and pattern), which could largely boost the model performance and interpretability. To address this issue, we propose to comprehensively explore the visual content and attributes of fashion items towards FCM. This problem is non-trivial considering the following challenges: a) how to utilize the irregular attribute labels of items to partially supervise the attribute-level representation learning of fashion items; b) how to ensure the intact disentanglement of attribute-level representations; and c) how to effectively sew the multiple granulairites (i.e, coarse-grained item-level and fine-grained attribute-level) information to enable performance improvement and interpretability. To address these challenges, in this work, we present a partially supervised outfit compatibility modeling scheme (PS-OCM). In particular, we first devise a partially supervised attribute-level embedding learning component to disentangle the fine-grained attribute embeddings from the entire visual feature of each item. We then introduce a disentangled completeness regularizer to prevent the information loss during disentanglement. Thereafter, we design a hierarchical graph convolutional network, which seamlessly integrates the attribute- and item-level compatibility modeling, and enables the explainable compatibility reasoning. Extensive experiments on the real-world dataset demonstrate that our PS-OCM significantly outperforms the state-of-the-art baselines. We have released our source codes and well-trained models to benefit other researchers ( https://site2750.wixsite.com/ps-ocm ).},
  archive      = {J_TIP},
  author       = {Weili Guan and Haokun Wen and Xuemeng Song and Chun Wang and Chung-Hsing Yeh and Xiaojun Chang and Liqiang Nie},
  doi          = {10.1109/TIP.2022.3187290},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4733-4745},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Partially supervised compatibility modeling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical superpixel segmentation by parallel CRTrees
labeling. <em>TIP</em>, <em>31</em>, 4719–4732. (<a
href="https://doi.org/10.1109/TIP.2022.3187563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a hierarchical superpixel segmentation by representing an image as a hierarchy of 1-nearest neighbor (1-NN) graphs with pixels/superpixels denoting the graph vertices. The 1-NN graphs are built from the pixel/superpixel adjacent matrices to ensure connectivity. To determine the next-level superpixel hierarchy, inspired by FINCH clustering, the weakly connected components (WCCs) of the 1-NN graph are labeled as superpixels. We reveal that the WCCs of a 1-NN graph consist of a forest of cycle-root-trees (CRTrees). The forest-like structure inspires us to propose a two-stage parallel CRTrees labeling which first links the child vertices to the cycle-roots and then labels all the vertices by the cycle-roots. We also propose an inter-inner superpixel distance penalization and a Lab color lightness penalization base on the property that the distance of a CRTree decreases monotonically from the child to root vertices. Experiments show the parallel CRTrees labeling is several times faster than recent advanced sequential and parallel connected components labeling algorithms. The proposed hierarchical superpixel segmentation has comparable performance to the best performer ETPS (state-of-the-arts) on the BSDS500, NYUV2, and Fash datasets. At the same time, it can achieve 200FPS for 480P video streams.},
  archive      = {J_TIP},
  author       = {Tingman Yan and Xiaolin Huang and Qunfei Zhao},
  doi          = {10.1109/TIP.2022.3187563},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4719-4732},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical superpixel segmentation by parallel CRTrees labeling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupled splines for sparse curve fitting. <em>TIP</em>,
<em>31</em>, 4707–4718. (<a
href="https://doi.org/10.1109/TIP.2022.3187286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate as an inverse problem the construction of sparse parametric continuous curve models that fit a sequence of contour points. Our prior is incorporated as a regularization term that encourages rotation invariance and sparsity. We prove that an optimal solution to the inverse problem is a closed curve with spline components. We then show how to efficiently solve the task using B-splines as basis functions. We extend our problem formulation to curves made of two distinct components with complementary smoothness properties and solve it using hybrid splines. We illustrate the performance of our model on contours of different smoothness. Our experimental results show that we can faithfully reconstruct any general contour using few parameters, even in the presence of imprecisions in the measurements.},
  archive      = {J_TIP},
  author       = {Icíar Lloréns Jover and Thomas Debarre and Shayan Aziznejad and Michael Unser},
  doi          = {10.1109/TIP.2022.3187286},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4707-4718},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Coupled splines for sparse curve fitting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring language hierarchy for video grounding.
<em>TIP</em>, <em>31</em>, 4693–4706. (<a
href="https://doi.org/10.1109/TIP.2022.3187288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The understanding of language plays a key role in video grounding, where a target moment is localized according to a text query. From a biological point of view, language is naturally hierarchical, with the main clause (predicate phrase) providing coarse semantics and modifiers providing detailed descriptions. In video grounding, moments described by the main clause may exist in multiple clips of a long video, including both the ground-truth and background clips. Therefore, in order to correctly discriminate the ground-truth clip from the background ones, this co-existence leads to the negligence of the main clause, and concentrate the model on the modifiers that provide discriminative information on distinguishing the target proposal from the others. We first demonstrate this phenomenon empirically, and propose a Hierarchical Language Network (HLN) that exploits the language hierarchy, as well as a new learning approach called Multi-Instance Positive-Unlabelled Learning (MI-PUL) to alleviate the above problem. Specifically, in HLN, the localization is performed on various layers of the language hierarchy, so that the attention can be paid to different parts of the sentences, rather than only discriminative ones. Furthermore, MI-PUL allows the model to localize background clips that can be possibly described by the main clause, even without manual annotations. Therefore, the union of the two proposed components enhances the learning of the main clause, which is of critical importance in video grounding. Finally, we evaluate that our proposed HLN can plug into the current methods and improve their performance. Extensive experiments on challenging datasets show HLN significantly improve the state-of-the-art methods, especially achieving 6.15\% gain in terms of $Recall 1\text{@}IoU0.5$ on the TACoS dataset.},
  archive      = {J_TIP},
  author       = {Xinpeng Ding and Nannan Wang and Shiwei Zhang and Ziyuan Huang and Xiaomeng Li and Mingqian Tang and Tongliang Liu and Xinbo Gao},
  doi          = {10.1109/TIP.2022.3187288},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4693-4706},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring language hierarchy for video grounding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pose2UV: Single-shot multiperson mesh recovery with deep UV
prior. <em>TIP</em>, <em>31</em>, 4679–4692. (<a
href="https://doi.org/10.1109/TIP.2022.3187294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we focus on the task of multi-person mesh recovery from a single color image, where the key issue is to tackle the pixel-level ambiguities caused by inter-person occlusions. Overall, there are two main technical challenges when addressing the ambiguities: how to extract valid target features under occlusions and how to reconstruct reasonable human meshes with only a handful of body cues? To deal with these problems, our key idea is to utilize the predicted 2D poses to locate and separate the target person, and reconstruct them with a novel learning-based UV prior. Specifically, we propose a visible pose-mask module to help extract valid target features, then train a dense body mesh prior to promote reconstructing natural mesh represented by the UV position map. To evaluate the performance of our proposed method under occlusions, we further build an in-the-wild 3D multi-person benchmark named as 3DMPB. Experimental results demonstrate that our method achieves state-of-the-art compared with previous methods. The dataset, codes are publicly available on our website.},
  archive      = {J_TIP},
  author       = {Buzhen Huang and Tianshu Zhang and Yangang Wang},
  doi          = {10.1109/TIP.2022.3187294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4679-4692},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pose2UV: Single-shot multiperson mesh recovery with deep UV prior},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothed-NUV priors for imaging. <em>TIP</em>, <em>31</em>,
4663–4678. (<a href="https://doi.org/10.1109/TIP.2022.3186749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variations of $L1$ -regularization including, in particular, total variation regularization, have hugely improved computational imaging. However, sharper edges and fewer staircase artifacts can be achieved with convex-concave regularizers. We present a new class of such regularizers using normal priors with unknown variance (NUV), which include smoothed versions of the logarithm function and smoothed versions of $Lp$ norms with $p\leq 1$ . All NUV priors allow variational representations that lead to efficient algorithms for image reconstruction by iterative reweighted descent. A preferred such algorithm is iterative reweighted coordinate descent, which has no parameters (in particular, no step size to control) and is empirically robust and efficient. The proposed priors and algorithms are demonstrated with applications to tomography. We also note that the proposed priors come with built-in edge detection, which is demonstrated by an application to image segmentation.},
  archive      = {J_TIP},
  author       = {Boxiao Ma and Nour Zalmai and Hans-Andrea Loeliger},
  doi          = {10.1109/TIP.2022.3186749},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4663-4678},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Smoothed-NUV priors for imaging},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning feature recovery transformer for occluded person
re-identification. <em>TIP</em>, <em>31</em>, 4651–4662. (<a
href="https://doi.org/10.1109/TIP.2022.3186759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major issue that challenges person re-identification (Re-ID) is the ubiquitous occlusion over the captured persons. There are two main challenges for the occluded person Re-ID problem, $i.e.$ , the interference of noise during feature matching and the loss of pedestrian information brought by the occlusions. In this paper, we propose a new approach called Feature Recovery Transformer (FRT) to address the two challenges simultaneously, which mainly consists of visibility graph matching and feature recovery transformer. To reduce the interference of the noise during feature matching, we mainly focus on visible regions that appear in both images and develop a visibility graph to calculate the similarity. In terms of the second challenge, based on the developed graph similarity, for each query image, we propose a recovery transformer that exploits the feature sets of its $k$ -nearest neighbors in the gallery to recover the complete features. Extensive experiments across different person Re-ID datasets, including occluded, partial and holistic datasets, demonstrate the effectiveness of FRT. Specifically, FRT significantly outperforms state-of-the-art results by at least 6.2\% Rank- 1 accuracy and 7.2\% mAP scores on the challenging Occluded-Duke dataset.},
  archive      = {J_TIP},
  author       = {Boqiang Xu and Lingxiao He and Jian Liang and Zhenan Sun},
  doi          = {10.1109/TIP.2022.3186759},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4651-4662},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning feature recovery transformer for occluded person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). CRS-CONT: A well-trained general encoder for facial
expression analysis. <em>TIP</em>, <em>31</em>, 4637–4650. (<a
href="https://doi.org/10.1109/TIP.2022.3186536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing facial expression recognition (FER) methods train encoders with different large-scale training data for specific FER applications. In this paper, we propose a new task in this field. This task aims to pre-train a general encoder to extract any facial expression representations without fine-tuning. To tackle this task, we extend the self-supervised contrastive learning to pre-train a general encoder for facial expression analysis. To be specific, given a batch of facial expressions, some positive and negative pairs are firstly constructed based on coarse-grained labels and a FER-specified data augmentation strategy. Secondly, we propose the coarse-contrastive (CRS-CONT) learning, where the features of positive pairs are pulled together, while pushed away from the features of negative pairs. Moreover, one key event is that the excessive constraint on the coarse-grained feature distribution will affect fine-grained FER applications. To address this, a weight vector is designed to control the optimization of the CRS-CONT learning. As a result, a well-trained general encoder with frozen weights could preferably adapt to different facial expressions and realize the linear evaluation on any target datasets. Extensive experiments on both in- the-wild and in- the-lab FER datasets show that our method provides superior or comparable performance against state-of-the-art FER methods, especially on unseen facial expressions and cross-dataset evaluation. We hope that this work will help to reduce the training burden and develop a new solution against the fully-supervised feature learning with fine-grained labels. Code and the general encoder will be publicly available at https://github.com/hangyu94/CRS-CONT},
  archive      = {J_TIP},
  author       = {Hangyu Li and Nannan Wang and Xi Yang and Xinbo Gao},
  doi          = {10.1109/TIP.2022.3186536},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4637-4650},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CRS-CONT: A well-trained general encoder for facial expression analysis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multistage framework with mean subspace computation and
recursive feedback for online unsupervised domain adaptation.
<em>TIP</em>, <em>31</em>, 4622–4636. (<a
href="https://doi.org/10.1109/TIP.2022.3186537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the Online Unsupervised Domain Adaptation (OUDA) problem and propose a novel multi-stage framework to solve real-world situations when the target data are unlabeled and arriving online sequentially in batches. Most of the traditional manifold-based methods on the OUDA problem focus on transforming each arriving target data to the source domain without sufficiently considering the temporal coherency and accumulative statistics among the arriving target data. In order to project the data from the source and the target domains to a common subspace and manipulate the projected data in real-time, our proposed framework institutes a novel method, called an Incremental Computation of Mean-Subspace (ICMS) technique, which computes an approximation of mean-target subspace on a Grassmann manifold and is proven to be a close approximate to the Karcher mean. Furthermore, the transformation matrix computed from the mean-target subspace is applied to the next target data in the recursive-feedback stage, aligning the target data closer to the source domain. The computation of transformation matrix and the prediction of next-target subspace leverage the performance of the recursive-feedback stage by considering the cumulative temporal dependency among the flow of the target subspace on the Grassmann manifold. The labels of the transformed target data are predicted by the pre-trained source classifier, then the classifier is updated by the transformed data and predicted labels. Extensive experiments on six datasets were conducted to investigate in depth the effect and contribution of each stage in our proposed framework and its performance over previous approaches in terms of classification accuracy and computational speed. In addition, the experiments on traditional manifold-based learning models and neural-network-based learning models demonstrated the applicability of our proposed framework for various types of learning models.},
  archive      = {J_TIP},
  author       = {Jihoon Moon and Debasmit Das and C. S. George Lee},
  doi          = {10.1109/TIP.2022.3186537},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4622-4636},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multistage framework with mean subspace computation and recursive feedback for online unsupervised domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hilbert space filling curve based scan-order for point cloud
attribute compression. <em>TIP</em>, <em>31</em>, 4609–4621. (<a
href="https://doi.org/10.1109/TIP.2022.3186532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is a set of three-dimensional points in arbitrary order, which is a popular representation of 3D scene in autonomous navigation and immersive applications in recent years. Compression becomes an inevitable issue due to the huge data volume of point cloud. In order to effectively compress attributes of those points, proper reordering is important. The existing voxel-based point cloud attributes compression scheme uses a naive scan for points reordering. In this paper, we theoretically analyzed 3C properties of point cloud, i.e., Compactness, Clustering and Correlation, of different scan-orders defined by different space filling curves and disclosed that the Hilbert curve can provide the best spatial correlation preservation compared with Z-order and Gray-coded curves. It is also statistically verified that the Hilbert curve always has the best ability of attributes correlation preservation for point clouds with different sparsity. We also proposed a fast and iterative Hilbert address code generation method to implement points reordering. The Hilbert scan-order could be combined with various point cloud attribute coding methods. Experiments show that the correlation preservation feature of the proposed scan-order can bring us 6.1\% and 6.5\% coding gain for prediction and transform coding, respectively.},
  archive      = {J_TIP},
  author       = {Jiafeng Chen and Lu Yu and Wenyi Wang},
  doi          = {10.1109/TIP.2022.3186532},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4609-4621},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hilbert space filling curve based scan-order for point cloud attribute compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MSA-net: Establishing reliable correspondences by multiscale
attention network. <em>TIP</em>, <em>31</em>, 4598–4608. (<a
href="https://doi.org/10.1109/TIP.2022.3186535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel multi-scale attention based network (called MSA-Net) for feature matching problems. Current deep networks based feature matching methods suffer from limited effectiveness and robustness when applied to different scenarios, due to random distributions of outliers and insufficient information learning. To address this issue, we propose a multi-scale attention block to enhance the robustness to outliers, for improving the representational ability of the feature map. In addition, we also design a novel context channel refine block and a context spatial refine block to mine the information context with less parameters along channel and spatial dimensions, respectively. The proposed MSA-Net is able to effectively infer the probability of correspondences being inliers with less parameters. Extensive experiments on outlier removal and relative pose estimation have shown the performance improvements of our network over current state-of-the-art methods with less parameters on both outdoor and indoor datasets. Notably, our proposed network achieves an 11.7\% improvement at error threshold 5° without RANSAC than the state-of-the-art method on relative pose estimation task when trained on YFCC100M dataset.},
  archive      = {J_TIP},
  author       = {Linxin Zheng and Guobao Xiao and Ziwei Shi and Shiping Wang and Jiayi Ma},
  doi          = {10.1109/TIP.2022.3186535},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4598-4608},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MSA-net: Establishing reliable correspondences by multiscale attention network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic instance domain adaptation. <em>TIP</em>,
<em>31</em>, 4585–4597. (<a
href="https://doi.org/10.1109/TIP.2022.3186531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing studies on unsupervised domain adaptation (UDA) assume that each domain’s training samples come with domain labels (e.g., painting, photo). Samples from each domain are assumed to follow the same distribution and the domain labels are exploited to learn domain-invariant features via feature alignment. However, such an assumption often does not hold true—there often exist numerous finer-grained domains (e.g., dozens of modern painting styles have been developed, each differing dramatically from those of the classic styles). Therefore, forcing feature distribution alignment across each artificially-defined and coarse-grained domain can be ineffective. In this paper, we address both single-source and multi-source UDA from a completely different perspective, which is to view each instance as a fine domain . Feature alignment across domains is thus redundant. Instead, we propose to perform dynamic instance domain adaptation (DIDA). Concretely, a dynamic neural network with adaptive convolutional kernels is developed to generate instance-adaptive residuals to adapt domain-agnostic deep features to each individual instance. This enables a shared classifier to be applied to both source and target domain data without relying on any domain annotation. Further, instead of imposing intricate feature alignment losses, we adopt a simple semi-supervised learning paradigm using only a cross-entropy loss for both labeled source and pseudo labeled target data. Our model, dubbed DIDA-Net, achieves state-of-the-art performance on several commonly used single-source and multi-source UDA datasets including Digits, Office-Home, DomainNet, Digit-Five, and PACS.},
  archive      = {J_TIP},
  author       = {Zhongying Deng and Kaiyang Zhou and Da Li and Junjun He and Yi-Zhe Song and Tao Xiang},
  doi          = {10.1109/TIP.2022.3186531},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4585-4597},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic instance domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FOVQA: Blind foveated video quality assessment.
<em>TIP</em>, <em>31</em>, 4571–4584. (<a
href="https://doi.org/10.1109/TIP.2022.3185738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous blind or No Reference (NR) Image / video quality assessment (IQA/VQA) models largely rely on features drawn from natural scene statistics (NSS), but under the assumption that the image statistics are stationary in the spatial domain. Several of these models are quite successful on standard pictures. However, in Virtual Reality (VR) applications, foveated video compression is regaining attention, and the concept of space-variant quality assessment is of interest, given the availability of increasingly high spatial and temporal resolution contents and practical ways of measuring gaze direction. Distortions from foveated video compression increase with increased eccentricity, implying that the natural scene statistics are space-variant. Towards advancing the development of foveated compression / streaming algorithms, we have devised a no-reference (NR) foveated video quality assessment model, called FOVQA, which is based on new models of space-variant natural scene statistics (NSS) and natural video statistics (NVS). Specifically, we deploy a space-variant generalized Gaussian distribution (SV-GGD) model and a space-variant asynchronous generalized Gaussian distribution (SV-AGGD) model of mean subtracted contrast normalized (MSCN) coefficients and products of neighboring MSCN coefficients, respectively. We devise a foveated video quality predictor that extracts radial basis features, and other features that capture perceptually annoying rapid quality fall-offs. We find that FOVQA achieves state-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as compared with other leading Foveated IQA / VQA models. we have made our implementation of FOVQA available at: https://live.ece.utexas.edu/research/Quality/FOVQA.zip .},
  archive      = {J_TIP},
  author       = {Yize Jin and Anjul Patney and Richard Webb and Alan C. Bovik},
  doi          = {10.1109/TIP.2022.3185738},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4571-4584},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FOVQA: Blind foveated video quality assessment},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning implicit class knowledge for RGB-d co-salient
object detection with transformers. <em>TIP</em>, <em>31</em>,
4556–4570. (<a href="https://doi.org/10.1109/TIP.2022.3185550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D co-salient object detection aims to segment co-occurring salient objects when given a group of relevant images and depth maps. Previous methods often adopt separate pipeline and use hand-crafted features, being hard to capture the patterns of co-occurring salient objects and leading to unsatisfactory results. Using end-to-end CNN models is a straightforward idea, but they are less effective in exploiting global cues due to the intrinsic limitation. Thus, in this paper, we alternatively propose an end-to-end transformer-based model which uses class tokens to explicitly capture implicit class knowledge to perform RGB-D co-salient object detection, denoted as CTNet. Specifically, we first design adaptive class tokens for individual images to explore intra-saliency cues and then develop common class tokens for the whole group to explore inter-saliency cues. Besides, we also leverage the complementary cues between RGB images and depth maps to promote the learning of the above two types of class tokens. In addition, to promote model evaluation, we construct a challenging and large-scale benchmark dataset, named RGBD CoSal1k, which collects 106 groups containing 1000 pairs of RGB-D images with complex scenarios and diverse appearances. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TIP},
  author       = {Ni Zhang and Junwei Han and Nian Liu},
  doi          = {10.1109/TIP.2022.3185550},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4556-4570},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning implicit class knowledge for RGB-D co-salient object detection with transformers},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning calibrated class centers for few-shot
classification by pair-wise similarity. <em>TIP</em>, <em>31</em>,
4543–4555. (<a href="https://doi.org/10.1109/TIP.2022.3184813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based methods achieve promising performance on few-shot classification by learning clusters on support samples and generating shared decision boundaries for query samples. However, existing methods ignore the inaccurate class center approximation introduced by the limited number of support samples, which consequently leads to biased inference. Therefore, in this paper, we propose to reduce the approximation error by class center calibration. Specifically, we introduce the so-called Pair-wise Similarity Module (PSM) to generate calibrated class centers adapted to the query sample by capturing the semantic correlations between the support and the query samples, as well as enhancing the discriminative regions on support representation. It is worth noting that the proposed PSM is a simple plug-and-play module and can be inserted into most metric-based few-shot learning models. Through extensive experiments in metric-based models, we demonstrate that the module significantly improves the performance of conventional few-shot classification methods on four few-shot image classification benchmark datasets. Codes are available at: https://github.com/PRIS-CV/Pair-wise-Similarity-module .},
  archive      = {J_TIP},
  author       = {Yurong Guo and Ruoyi Du and Xiaoxu Li and Jiyang Xie and Zhanyu Ma and Yuan Dong},
  doi          = {10.1109/TIP.2022.3184813},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4543-4555},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning calibrated class centers for few-shot classification by pair-wise similarity},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). A self-supervised residual feature learning model for
multifocus image fusion. <em>TIP</em>, <em>31</em>, 4527–4542. (<a
href="https://doi.org/10.1109/TIP.2022.3184250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion (MFIF) attempts to achieve an “all-focused” image from multiple source images with the same scene but different focused objects. Given the lack of multi-focus image sets for network training, we propose a self-supervised residual feature learning model in this paper. The model consists of a feature extraction network and a fusion module. We select image super-resolution as a pretext task in the MFIF field, which is supported by a new residual gradient prior discovered by our theoretical study for low- and high-resolution (LR-HR) image pairs, as well as for multi-focus images. In the pretext task, our network’s training set is LR-HR image pairs generated from natural images, and HR images can be regarded as pseudo-labels of LR images. In the fusion task, the trained network extracts residual features of multi-focus images firstly. Secondly, the fusion module, consisting of an activity level measurement and a new boundary refinement method, is leveraged for the features to generated decision maps. Experimental results, both subjective evaluations and objective evaluations, demonstrate that our approach outperforms other state-of-the-art fusion algorithms.},
  archive      = {J_TIP},
  author       = {Zeyu Wang and Xiongfei Li and Haoran Duan and Xiaoli Zhang},
  doi          = {10.1109/TIP.2022.3184250},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4527-4542},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A self-supervised residual feature learning model for multifocus image fusion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disparity-aware reference frame generation network for
multiview video coding. <em>TIP</em>, <em>31</em>, 4515–4526. (<a
href="https://doi.org/10.1109/TIP.2022.3183436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview video coding (MVC) aims to compress the multiview video through the elimination of video redundancies, where the quality of the reference frame directly affects the compression efficiency. In this paper, we propose a deep virtual reference frame generation method based on a disparity-aware reference frame generation network (DAG-Net) to transform the disparity relationship between different viewpoints and generate a more reliable reference frame. The proposed DAG-Net consists of a multi-level receptive field module, a disparity-aware alignment module, and a fusion reconstruction module. First, a multi-level receptive field module is designed to enlarge the receptive field, and extract the multi-scale deep features of the temporal and inter-view reference frames. Then, a disparity-aware alignment module is proposed to learn the disparity relationship, and perform disparity shift on the inter-view reference frame to align it with the temporal reference frame. Finally, a fusion reconstruction module is utilized to fuse the complementary information and generate a more reliable virtual reference frame. Experiments demonstrate that the proposed reference frame generation method achieves superior performance for multiview video coding.},
  archive      = {J_TIP},
  author       = {Jianjun Lei and Zongqian Zhang and Zhaoqing Pan and Dong Liu and Xiangrui Liu and Ying Chen and Nam Ling},
  doi          = {10.1109/TIP.2022.3183436},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4515-4526},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Disparity-aware reference frame generation network for multiview video coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Grammar-induced wavelet network for human parsing.
<em>TIP</em>, <em>31</em>, 4502–4514. (<a
href="https://doi.org/10.1109/TIP.2022.3181486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods of human parsing still face a challenge: how to extract the accurate foreground from similar or cluttered scenes effectively. In this paper, we propose a Grammar-induced Wavelet Network (GWNet), to deal with the challenge. GWNet mainly consists of two modules, including a blended grammar-induced module and a wavelet prediction module. We design the blended grammar-induced module to exploit the relationship of different human parts and the inherent hierarchical structure of a human body by means of grammar rules in both cascaded and paralleled manner. In this way, conspicuous parts, which are easily distinguished from the background, can amend the segmentation of inconspicuous ones, improving the foreground extraction. We also design a Part-aware Convolutional Recurrent Neural Network (PCRNN) to pass messages which are generated by grammar rules. To further improve the performance, we propose a wavelet prediction module to capture the basic structure and the edge details of a person by decomposing the low-frequency and high-frequency components of features. The low-frequency component can represent the smooth structures and the high-frequency components can describe the fine details. We conduct extensive experiments to evaluate GWNet on PASCAL-Person-Part, LIP, and PPSS datasets. GWNet obtains state-of-the-art performance on these human parsing datasets.},
  archive      = {J_TIP},
  author       = {Xiaomei Zhang and Yingying Chen and Ming Tang and Zhen Lei and Jinqiao Wang},
  doi          = {10.1109/TIP.2022.3181486},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4502-4514},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Grammar-induced wavelet network for human parsing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast run-length compression of point cloud geometry.
<em>TIP</em>, <em>31</em>, 4490–4501. (<a
href="https://doi.org/10.1109/TIP.2022.3185541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in popularity of point-cloud-oriented applications has triggered the development of specialized compression algorithms. In this paper, a novel algorithm is developed for the lossless geometry compression of voxelized point clouds following an intra-frame design. The encoded voxels are arranged into runs and are encoded through a single-pass application directly on the voxel domain. This is done without representing the point cloud via an octree nor rendering the voxel space through an occupancy matrix, therefore decreasing the memory requirements of the method. Each run is compressed using a context-adaptive arithmetic encoder yielding state-of-the-art compression results, with gains of up to 15\% over TMC13 , MPEG’s standard for point cloud geometry compression. Several proposed contributions accelerate the calculations of each run’s probability limits prior to arithmetic encoding. As a result, the encoder attains a low computational complexity described by a linear relation to the number of occupied voxels leading to an average speedup of 1.8 over TMC13 in encoding speeds. Various experiments are conducted assessing the proposed algorithm’s state-of-the-art performance in terms of compression ratio and encoding speeds.},
  archive      = {J_TIP},
  author       = {Dion E. O. Tzamarias and Kevin Chow and Ian Blanes and Joan Serra-Sagristà},
  doi          = {10.1109/TIP.2022.3185541},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4490-4501},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast run-length compression of point cloud geometry},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Actor and action modular network for text-based video
segmentation. <em>TIP</em>, <em>31</em>, 4474–4489. (<a
href="https://doi.org/10.1109/TIP.2022.3185487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based video segmentation aims to segment an actor in video sequences by specifying the actor and its performing action with a textual query. Previous methods fail to explicitly align the video content with the textual query in a fine-grained manner according to the actor and its action, due to the problem of semantic asymmetry . The semantic asymmetry implies that two modalities contain different amounts of semantic information during the multi-modal fusion process. To alleviate this problem, we propose a novel actor and action modular network that individually localizes the actor and its action in two separate modules. Specifically, we first learn the actor-/action-related content from the video and textual query, and then match them in a symmetrical manner to localize the target tube. The target tube contains the desired actor and action which is then fed into a fully convolutional network to predict segmentation masks of the actor. Our method also establishes the association of objects cross multiple frames with the proposed temporal proposal aggregation mechanism. This enables our method to segment the video effectively and keep the temporal consistency of predictions. The whole model is allowed for joint learning of the actor-action matching and segmentation, as well as achieves the state-of-the-art performance for both single-frame segmentation and full video segmentation on A2D Sentences and J-HMDB Sentences datasets.},
  archive      = {J_TIP},
  author       = {Jianhua Yang and Yan Huang and Kai Niu and Linjiang Huang and Zhanyu Ma and Liang Wang},
  doi          = {10.1109/TIP.2022.3185487},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4474-4489},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Actor and action modular network for text-based video segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed x-ray image separation for artworks with concealed
designs. <em>TIP</em>, <em>31</em>, 4458–4473. (<a
href="https://doi.org/10.1109/TIP.2022.3185488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on X-ray images (X-radiographs) of paintings with concealed sub-surface designs ( e.g. , deriving from reuse of the painting support or revision of a composition by the artist), which therefore include contributions from both the surface painting and the concealed features. In particular, we propose a self-supervised deep learning-based image separation approach that can be applied to the X-ray images from such paintings to separate them into two hypothetical X-ray images. One of these reconstructed images is related to the X-ray image of the concealed painting, while the second one contains only information related to the X-ray image of the visible painting. The proposed separation network consists of two components: the analysis and the synthesis sub-networks. The analysis sub-network is based on learned coupled iterative shrinkage thresholding algorithms (LCISTA) designed using algorithm unrolling techniques, and the synthesis sub-network consists of several linear mappings. The learning algorithm operates in a totally self-supervised fashion without requiring a sample set that contains both the mixed X-ray images and the separated ones. The proposed method is demonstrated on a real painting with concealed content, Do na Isabel de Porcel by Francisco de Goya, to show its effectiveness.},
  archive      = {J_TIP},
  author       = {Wei Pu and Jun-Jie Huang and Barak Sober and Nathan Daly and Catherine Higgitt and Ingrid Daubechies and Pier Luigi Dragotti and Miguel R. D. Rodrigues},
  doi          = {10.1109/TIP.2022.3185488},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4458-4473},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mixed X-ray image separation for artworks with concealed designs},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Action shuffling for weakly supervised temporal
localization. <em>TIP</em>, <em>31</em>, 4447–4457. (<a
href="https://doi.org/10.1109/TIP.2022.3185485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised action localization is a challenging task with extensive applications, which aims to identify actions and the corresponding temporal intervals with only video-level annotations available. This paper analyzes the order-sensitive and location-insensitive properties of actions, and embodies them into a self-augmented learning framework to improve the weakly supervised action localization performance. To be specific, we propose a novel two-branch network architecture with intra/inter-action shuffling, referred to as ActShufNet. The intra-action shuffling branch lays out a self-supervised order prediction task to augment the video representation with inner-video relevance, whereas the inter-action shuffling branch imposes a reorganizing strategy on the existing action contents to augment the training set without resorting to any external resources. Furthermore, the global-local adversarial training is presented to enhance the model’s robustness to irrelevant noises. Extensive experiments are conducted on three benchmark datasets, and the results clearly demonstrate the efficacy of the proposed method.},
  archive      = {J_TIP},
  author       = {Xiao-Yu Zhang and Haichao Shi and Changsheng Li and Xinchu Shi},
  doi          = {10.1109/TIP.2022.3185485},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4447-4457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action shuffling for weakly supervised temporal localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical random walker segmentation for large volumetric
biomedical images. <em>TIP</em>, <em>31</em>, 4431–4446. (<a
href="https://doi.org/10.1109/TIP.2022.3185551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The random walker method for image segmentation is a popular tool for semi-automatic image segmentation, especially in the biomedical field. However, its linear asymptotic run time and memory requirements make application to 3D datasets of increasing sizes impractical. We propose a hierarchical framework that, to the best of our knowledge, is the first attempt to overcome these restrictions for the random walker algorithm and achieves sublinear run time and constant memory complexity. The goal of this framework is– rather than improving the segmentation quality compared to the baseline method– to make interactive segmentation on out-of-core datasets possible. The method is evaluated quantitatively on synthetic data and the CT-ORG dataset where the expected improvements in algorithm run time while maintaining high segmentation quality are confirmed. The incremental (i.e., interaction update) run time is demonstrated to be in seconds on a standard PC even for volumes of hundreds of gigabytes in size. In a small case study the applicability to large real world from current biomedical research is demonstrated. An implementation of the presented method is publicly available in version 5.2 of the widely used volume rendering and processing software Voreen ( https://www.uni-muenster.de/Voreen/ ).},
  archive      = {J_TIP},
  author       = {Dominik Drees and Florian Eilers and Xiaoyi Jiang},
  doi          = {10.1109/TIP.2022.3185551},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4431-4446},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical random walker segmentation for large volumetric biomedical images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting fast adversarial training with learnable
adversarial initialization. <em>TIP</em>, <em>31</em>, 4417–4430. (<a
href="https://doi.org/10.1109/TIP.2022.3184255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT) has been demonstrated to be effective in improving model robustness by leveraging adversarial examples for training. However, most AT methods are in face of expensive time and computational cost for calculating gradients at multiple steps in generating adversarial examples. To boost training efficiency, fast gradient sign method (FGSM) is adopted in fast AT methods by calculating gradient only once. Unfortunately, the robustness is far from satisfactory. One reason may arise from the initialization fashion. Existing fast AT generally uses a random sample-agnostic initialization, which facilitates the efficiency yet hinders a further robustness improvement. Up to now, the initialization in fast AT is still not extensively explored. In this paper, focusing on image classification, we boost fast AT with a sample-dependent adversarial initialization, i.e. , an output from a generative network conditioned on a benign image and its gradient information from the target network. As the generative network and the target network are optimized jointly in the training phase, the former can adaptively generate an effective initialization with respect to the latter, which motivates gradually improved robustness. Experimental evaluations on four benchmark databases demonstrate the superiority of our proposed method over state-of-the-art fast AT methods, as well as comparable robustness to advanced multi-step AT methods. The code is released at https://github.com//jiaxiaojunQAQ//FGSM-SDI .},
  archive      = {J_TIP},
  author       = {Xiaojun Jia and Yong Zhang and Baoyuan Wu and Jue Wang and Xiaochun Cao},
  doi          = {10.1109/TIP.2022.3184255},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4417-4430},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting fast adversarial training with learnable adversarial initialization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JPD-SE: High-level semantics for joint perception-distortion
enhancement in image compression. <em>TIP</em>, <em>31</em>, 4405–4416.
(<a href="https://doi.org/10.1109/TIP.2022.3180208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While humans can effortlessly transform complex visual scenes into simple words and the other way around by leveraging their high-level understanding of the content, conventional or the more recent learned image compression codecs do not seem to utilize the semantic meanings of visual content to their full potential. Moreover, they focus mostly on rate-distortion and tend to underperform in perception quality especially in low bitrate regime, and often disregard the performance of downstream computer vision algorithms, which is a fast-growing consumer group of compressed images in addition to human viewers. In this paper, we (1) present a generic framework that can enable any image codec to leverage high-level semantics and (2) study the joint optimization of perception quality and distortion. Our idea is that given any codec, we utilize high-level semantics to augment the low-level visual features extracted by it and produce essentially a new, semantic-aware codec. We propose a three-phase training scheme that teaches semantic-aware codecs to leverage the power of semantic to jointly optimize rate-perception-distortion (R-PD) performance. As an additional benefit, semantic-aware codecs also boost the performance of downstream computer vision algorithms. To validate our claim, we perform extensive empirical evaluations and provide both quantitative and qualitative results.},
  archive      = {J_TIP},
  author       = {Shiyu Duan and Huaijin Chen and Jinwei Gu},
  doi          = {10.1109/TIP.2022.3180208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4405-4416},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {JPD-SE: High-level semantics for joint perception-distortion enhancement in image compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data acquisition and preparation for dual-reference deep
learning of image super-resolution. <em>TIP</em>, <em>31</em>,
4393–4404. (<a href="https://doi.org/10.1109/TIP.2022.3184819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning based image super-resolution (SR) methods depend on how accurately the paired low and high resolution images for training characterize the sampling process of real cameras. Low and high resolution (LR $\sim $ HR) image pairs synthesized by degradation models (e.g., bicubic downsampling) deviate from those in reality; thus the synthetically-trained DCNN SR models work disappointingly when being applied to real-world images. To address this issue, we propose a novel data acquisition process to shoot a large set of LR $\sim $ HR image pairs using real cameras. The images are displayed on an ultra-high quality screen and captured at different resolutions. The resulting LR $\sim $ HR image pairs can be aligned at very high sub-pixel precision by a novel spatial-frequency dual-domain registration method, and hence they provide more appropriate training data for the learning task of super-resolution. Moreover, the captured HR image and the original digital image offer dual references to strengthen supervised learning. Experimental results show that training a super-resolution DCNN by our LR $\sim $ HR dataset achieves higher image quality than training it by other datasets in the literature. Moreover, the proposed screen-capturing data collection process can be automated; it can be carried out for any target camera with ease and low cost, offering a practical way of tailoring the training of a DCNN SR model separately to each of the given cameras.},
  archive      = {J_TIP},
  author       = {Yanhui Guo and Xiaolin Wu and Xiao Shu},
  doi          = {10.1109/TIP.2022.3184819},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4393-4404},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Data acquisition and preparation for dual-reference deep learning of image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WINNet: Wavelet-inspired invertible network for image
denoising. <em>TIP</em>, <em>31</em>, 4377–4392. (<a
href="https://doi.org/10.1109/TIP.2022.3184845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising aims to restore a clean image from an observed noisy one. Model-based image denoising approaches can achieve good generalization ability over different noise levels and are with high interpretability. Learning-based approaches are able to achieve better results, but usually with weaker generalization ability and interpretability. In this paper, we propose a wavelet-inspired invertible network (WINNet) to combine the merits of the wavelet-based approaches and learning-based approaches. The proposed WINNet consists of $K$ -scale of lifting inspired invertible neural networks (LINNs) and sparsity-driven denoising networks together with a noise estimation network. The network architecture of LINNs is inspired by the lifting scheme in wavelets. LINNs are used to learn a non-linear redundant transform with perfect reconstruction property to facilitate noise removal. The denoising network implements a sparse coding process for denoising. The noise estimation network estimates the noise level from the input image which will be used to adaptively adjust the soft-thresholds in LINNs. The forward transform of LINNs produces a redundant multi-scale representation for denoising. The denoised image is reconstructed using the inverse transform of LINNs with the denoised detail channels and the original coarse channel. The simulation results show that the proposed WINNet method is highly interpretable and has strong generalization ability to unseen noise levels. It also achieves competitive results in the non-blind/blind image denoising and in image deblurring.},
  archive      = {J_TIP},
  author       = {Jun-Jie Huang and Pier Luigi Dragotti},
  doi          = {10.1109/TIP.2022.3184845},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4377-4392},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {WINNet: Wavelet-inspired invertible network for image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational deep image restoration. <em>TIP</em>,
<em>31</em>, 4363–4376. (<a
href="https://doi.org/10.1109/TIP.2022.3183835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new variational inference framework for image restoration and a convolutional neural network (CNN) structure that can solve the restoration problems described by the proposed framework. Earlier CNN-based image restoration methods primarily focused on network architecture design or training strategy with non-blind scenarios where the degradation models are known or assumed. For a step closer to real-world applications, CNNs are also blindly trained with the whole dataset, including diverse degradations. However, the conditional distribution of a high-quality image given a diversely degraded one is too complicated to be learned by a single CNN. Therefore, there have also been some methods that provide additional prior information to train a CNN. Unlike previous approaches, we focus more on the objective of restoration based on the Bayesian perspective and how to reformulate the objective. Specifically, our method relaxes the original posterior inference problem to better manageable sub-problems and thus behaves like a divide-and-conquer scheme. As a result, the proposed framework boosts the performance of several restoration problems compared to the previous ones. Specifically, our method delivers state-of-the-art performance on Gaussian denoising, real-world noise reduction, blind image super-resolution, and JPEG compression artifacts reduction. Our code and more details are available on our project page, https://github.com/JWSoh/VDIR .},
  archive      = {J_TIP},
  author       = {Jae Woong Soh and Nam Ik Cho},
  doi          = {10.1109/TIP.2022.3183835},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4363-4376},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational deep image restoration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MetaLabelNet: Learning to generate soft-labels from
noisy-labels. <em>TIP</em>, <em>31</em>, 4352–4362. (<a
href="https://doi.org/10.1109/TIP.2022.3183841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets commonly have noisy labels, which negatively affects the performance of deep neural networks (DNNs). In order to address this problem, we propose a label noise robust learning algorithm, in which the base classifier is trained on soft-labels that are produced according to a meta-objective. In each iteration, before conventional training, the meta-training loop updates soft-labels so that resulting gradients updates on the base classifier would yield minimum loss on meta-data. Soft-labels are generated from extracted features of data instances, and the mapping function is learned by a single layer perceptron (SLP) network, which is called MetaLabelNet. Following, base classifier is trained by using these generated soft-labels. These iterations are repeated for each batch of training data. Our algorithm uses a small amount of clean data as meta-data, which can be obtained effortlessly for many cases. We perform extensive experiments on benchmark datasets with both synthetic and real-world noises. Results show that our approach outperforms existing baselines. The source code of the proposed model is available at https://github.com/gorkemalgan/MetaLabelNet .},
  archive      = {J_TIP},
  author       = {Görkem Algan and Ilkay Ulusoy},
  doi          = {10.1109/TIP.2022.3183841},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4352-4362},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MetaLabelNet: Learning to generate soft-labels from noisy-labels},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IMU-assisted online video background identification.
<em>TIP</em>, <em>31</em>, 4336–4351. (<a
href="https://doi.org/10.1109/TIP.2022.3183442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguishing between dynamic foreground objects and a mostly static background is a fundamental problem in many computer vision and computer graphics tasks. This paper presents a novel online video background identification method with the assistance of inertial measurement unit (IMU). Based on the fact that the background motion of a video essentially reflects the 3D camera motion, we leverage IMU data to realize a robust camera motion estimation for identifying background feature points by only investigating a few historical frames. We observe that the displacement of the 2D projection of a scene point caused by camera rotation is depth-invariant, and the rotation estimation by using IMU data can be quite accurate. We thus propose to analyze 2D feature points by decomposing the 2D motion into two components: rotation projection and translation projection. In our method, after establishing the 3D camera rotations, we generate the depth-relevant 2D feature point movement induced by the camera 3D translation. Then, by examining the disparity between inter-frame offset and the projection of estimated 3D camera motion, we can identify the background feature points. In the experiments, our online method is able to run at 30FPS with only 1 frame latency and outperforms state-of-the-art background identification and other relevant methods. Our method directly leads to a better camera motion estimation, which is beneficial to many applications like online video stabilization, SLAM, image stitching, etc .},
  archive      = {J_TIP},
  author       = {Jian-Xiang Rong and Lei Zhang and Hua Huang and Fang-Lue Zhang},
  doi          = {10.1109/TIP.2022.3183442},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4336-4351},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IMU-assisted online video background identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowing what to learn: A metric-oriented focal mechanism for
image captioning. <em>TIP</em>, <em>31</em>, 4321–4335. (<a
href="https://doi.org/10.1109/TIP.2022.3183434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite considerable progress, image captioning still suffers from the huge difference in quality between easy and hard examples, which is left unexploited in existing methods. To address this issue, we explore the hard example mining in image captioning, and propose a simple yet effective mechanism to instruct the model to pay more attention to hard examples, thereby improving the performance in both general and complex scenarios. We first propose a novel learning strategy, termed Metric-oriented Focal Mechanism (MFM), for hard example mining in image captioning. Differing from the existing strategies for classification tasks, MFM can adopt the generative metrics of image captioning to measure the difficulties of examples, and then up-weight the rewards of hard examples during training. To make MFM applicable to different datasets without tedious parameter tuning, we further introduce an adaptive reward metric called Effective CIDEr (ECIDEr), which considers the data distribution of easy and hard examples during reward estimation. Extensive experiments are conducted on the MS COCO benchmark, and the results show that while maintaining the performance on simple examples, MFM can significantly improve the quality of captions for hard examples. The ECIDEr-based MFM is equipped on the current SOTA method, e.g. , DLCT (Luo et al. , 2021), which outperforms all existing methods and achieves new state-of-the-art performance on both the off-line and on- line testing, i.e. , 134.3 CIDEr for the off-line testing and 136.1 for the on- line testing of MSCOCO. To validate the generalization ability of ECIDEr-based MFM, we also apply it to another dataset, namely Flickr30k, and superior performance gains can also be obtained.},
  archive      = {J_TIP},
  author       = {Jiayi Ji and Yiwei Ma and Xiaoshuai Sun and Yiyi Zhou and Yongjian Wu and Rongrong Ji},
  doi          = {10.1109/TIP.2022.3183434},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4321-4335},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Knowing what to learn: A metric-oriented focal mechanism for image captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DUT: Learning video stabilization by simply watching
unstable videos. <em>TIP</em>, <em>31</em>, 4306–4320. (<a
href="https://doi.org/10.1109/TIP.2022.3182887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous deep learning-based video stabilizers require a large scale of paired unstable and stable videos for training, which are difficult to collect. Traditional trajectory-based stabilizers, on the other hand, divide the task into several sub-tasks and tackle them subsequently, which are fragile in textureless and occluded regions regarding the usage of hand-crafted features. In this paper, we attempt to tackle the video stabilization problem in a deep unsupervised learning manner, which borrows the divide-and-conquer idea from traditional stabilizers while leveraging the representation power of DNNs to handle the challenges in real-world scenarios. Technically, DUT is composed of a trajectory estimation stage and a trajectory smoothing stage. In the trajectory estimation stage, we first estimate the motion of keypoints, initialize and refine the motion of grids via a novel multi-homography estimation strategy and a motion refinement network, respectively, and get the grid-based trajectories via temporal association. In the trajectory smoothing stage, we devise a novel network to predict dynamic smoothing kernels for trajectory smoothing, which can well adapt to trajectories with different dynamic patterns. We exploit the spatial and temporal coherence of keypoints and grid vertices to formulate the training objectives, resulting in an unsupervised training scheme. Experiment results on public benchmarks show that DUT outperforms state-of-the-art methods both qualitatively and quantitatively. The source code is available at https://github.com/Annbless/DUTCode .},
  archive      = {J_TIP},
  author       = {Yufei Xu and Jing Zhang and Stephen J. Maybank and Dacheng Tao},
  doi          = {10.1109/TIP.2022.3182887},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4306-4320},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DUT: Learning video stabilization by simply watching unstable videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DCT2net: An interpretable shallow CNN for image denoising.
<em>TIP</em>, <em>31</em>, 4292–4305. (<a
href="https://doi.org/10.1109/TIP.2022.3181488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work tackles the issue of noise removal from images, focusing on the well-known DCT image denoising algorithm. The latter, stemming from signal processing, has been well studied over the years. Though very simple, it is still used in crucial parts of state-of-the-art “traditional” denoising algorithms such as BM3D. For a few years however, deep convolutional neural networks (CNN), especially DnCNN, have outperformed their traditional counterparts, making signal processing methods less attractive. In this paper, we demonstrate that a DCT denoiser can be seen as a shallow CNN and thereby its original linear transform can be tuned through gradient descent in a supervised manner, improving considerably its performance. This gives birth to a fully interpretable CNN called DCT2net. To deal with remaining artifacts induced by DCT2net, an original hybrid solution between DCT and DCT2net is proposed combining the best that these two methods can offer; DCT2net is selected to process non-stationary image patches while DCT is optimal for piecewise smooth patches. Experiments on artificially noisy images demonstrate that two-layer DCT2net provides comparable results to BM3D and is as fast as DnCNN algorithm.},
  archive      = {J_TIP},
  author       = {Sébastien Herbreteau and Charles Kervrann},
  doi          = {10.1109/TIP.2022.3181488},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4292-4305},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DCT2net: An interpretable shallow CNN for image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting monocular 3D human pose estimation with part aware
attention. <em>TIP</em>, <em>31</em>, 4278–4291. (<a
href="https://doi.org/10.1109/TIP.2022.3182269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D human pose estimation is challenging due to depth ambiguity. Convolution-based and Graph-Convolution-based methods have been developed to extract 3D information from temporal cues in motion videos. Typically, in the lifting-based methods, most recent works adopt the transformer to model the temporal relationship of 2D keypoint sequences. These previous works usually consider all the joints of a skeleton as a whole and then calculate the temporal attention based on the overall characteristics of the skeleton. Nevertheless, the human skeleton exhibits obvious part-wise inconsistency of motion patterns. It is therefore more appropriate to consider each part’s temporal behaviors separately. To deal with such part-wise motion inconsistency, we propose the Part Aware Temporal Attention module to extract the temporal dependency of each part separately. Moreover, the conventional attention mechanism in 3D pose estimation usually calculates attention within a short time interval. This indicates that only the correlation within the temporal context is considered. Whereas, we find that the part-wise structure of the human skeleton is repeating across different periods, actions, and even subjects. Therefore, the part-wise correlation at a distance can be utilized to further boost 3D pose estimation. We thus propose the Part Aware Dictionary Attention module to calculate the attention for the part-wise features of input in a dictionary, which contains multiple 3D skeletons sampled from the training set. Extensive experimental results show that our proposed part aware attention mechanism helps a transformer-based model to achieve state-of-the-art 3D pose estimation performance on two widely used public datasets. The codes and the trained models are released at https://github.com/thuxyz19/3D-HPE-PAA .},
  archive      = {J_TIP},
  author       = {Youze Xue and Jiansheng Chen and Xiangming Gu and Huimin Ma and Hongbing Ma},
  doi          = {10.1109/TIP.2022.3182269},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4278-4291},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting monocular 3D human pose estimation with part aware attention},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive language-customized visual feature learning for
one-stage visual grounding. <em>TIP</em>, <em>31</em>, 4266–4277. (<a
href="https://doi.org/10.1109/TIP.2022.3181516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual grounding is a task to localize an object described by a sentence in an image. Conventional visual grounding methods extract visual and linguistic features isolatedly and then perform cross-modal interaction in a post-fusion manner. We argue that this post-fusion mechanism does not fully utilize the information in two modalities. Instead, it is more desired to perform cross-modal interaction during the extraction process of the visual and linguistic feature. In this paper, we propose a language-customized visual feature learning mechanism where linguistic information guides the extraction of visual feature from the very beginning. We instantiate the mechanism as a one-stage framework named Progressive Language-customized Visual feature learning (PLV). Our proposed PLV consists of a Progressive Language-customized Visual Encoder (PLVE) and a grounding module. We customize the visual feature with linguistic guidance at each stage of the PLVE by Channel-wise Language-guided Interaction Modules (CLIM). Our proposed PLV outperforms conventional state-of-the-art methods with large margins across five visual grounding datasets without pre-training on object detection datasets, while achieving real-time speed. The source code is available in the supplementary material.},
  archive      = {J_TIP},
  author       = {Yue Liao and Aixi Zhang and Zhiyuan Chen and Tianrui Hui and Si Liu},
  doi          = {10.1109/TIP.2022.3181516},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4266-4277},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive language-customized visual feature learning for one-stage visual grounding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rotation-invariant attention network for hyperspectral image
classification. <em>TIP</em>, <em>31</em>, 4251–4265. (<a
href="https://doi.org/10.1109/TIP.2022.3177322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) classification refers to identifying land-cover categories of pixels based on spectral signatures and spatial information of HSIs. In recent deep learning-based methods, to explore the spatial information of HSIs, the HSI patch is usually cropped from original HSI as the input. And $3 \times 3$ convolution is utilized as a key component to capture spatial features for HSI classification. However, the $3 \times 3$ convolution is sensitive to the spatial rotation of inputs, which results in that recent methods perform worse in rotated HSIs. To alleviate this problem, a rotation-invariant attention network (RIAN) is proposed for HSI classification. First, a center spectral attention (CSpeA) module is designed to avoid the influence of other categories of pixels to suppress redundant spectral bands. Then, a rectified spatial attention (RSpaA) module is proposed to replace $3 \times 3$ convolution for extracting rotation-invariant spectral-spatial features from HSI patches. The CSpeA module, the $1 \times 1$ convolution and the RSpaA module are utilized to build the proposed RIAN for HSI classification. Experimental results demonstrate that RIAN is invariant to the spatial rotation of HSIs and has superior performance, e.g., achieving an overall accuracy of 86.53\% (1.04\% improvement) on the Houston database. The codes of this work are available at https://github.com/spectralpublic/RIAN .},
  archive      = {J_TIP},
  author       = {Xiangtao Zheng and Hao Sun and Xiaoqiang Lu and Wei Xie},
  doi          = {10.1109/TIP.2022.3177322},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4251-4265},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rotation-invariant attention network for hyperspectral image classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised person re-identification with stochastic
training strategy. <em>TIP</em>, <em>31</em>, 4240–4250. (<a
href="https://doi.org/10.1109/TIP.2022.3181811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (re-ID) has attracted increasing research interests because of its scalability and possibility for real-world applications. State-of-the-art unsupervised re-ID methods usually follow a clustering-based strategy, which generates pseudo labels by clustering and maintains a memory to store instance features and represent the centroid of the clusters for contrastive learning. This approach suffers two problems. First, the centroid generated by unsupervised learning may not be a perfect prototype. Forcing images to get closer to the centroid emphasizes the result of clustering, which could accumulate clustering errors during iterations. Second, previous instance memory based methods utilize features updated at different training iterations to represent one centroid, these features are inconsistent due to the change of encoder. To this end, we propose an unsupervised re-ID approach with a stochastic learning strategy. Specifically, we adopt a stochastic updated memory, where a random instance from a cluster is used to update the cluster-level memory for contrastive learning. In this way, the relationship between randomly selected pair of images are learned to avoid the training bias caused by unreliable pseudo labels. By picking a sole last seen sample to directly update each cluster center, the stochastic memory is also always up-to-date for classifying to keep the consistency. Besides, to relieve the issue of camera variance, a unified distance matrix is proposed during clustering, where the distance bias from different camera domains is reduced and the variances of identities are emphasized. Our proposed method outperforms the state-of-the-arts in all the common unsupervised and UDA re-ID tasks. The code will be available at https://github.com/lithium770/Unsupervised-Person-re-ID-with-Stochastic-Training-Strategy .},
  archive      = {J_TIP},
  author       = {Tianyang Liu and Yutian Lin and Bo Du},
  doi          = {10.1109/TIP.2022.3181811},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4240-4250},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised person re-identification with stochastic training strategy},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complementary data augmentation for cloth-changing person
re-identification. <em>TIP</em>, <em>31</em>, 4227–4239. (<a
href="https://doi.org/10.1109/TIP.2022.3183469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the challenging person re-identification (Re-ID) task under the cloth-changing scenario, where the same identity (ID) suffers from uncertain cloth changes. To learn cloth- and ID-invariant features, it is crucial to collect abundant training data with varying clothes, which is difficult in practice. To alleviate the reliance on rich data collection, we reinforce the feature learning process by designing powerful complementary data augmentation strategies, including positive and negative data augmentation. Specifically, the positive augmentation fulfills the ID space by randomly patching the person images with different clothes, simulating rich appearance to enhance the robustness against clothes variations. For negative augmentation, its basic idea is to randomly generate out-of-distribution synthetic samples by combining various appearance and posture factors from real samples. The designed strategies seamlessly reinforce the feature learning without additional information introduction. Extensive experiments conducted on both cloth-changing and -unchanging tasks demonstrate the superiority of our proposed method, consistently improving the accuracy over various baselines.},
  archive      = {J_TIP},
  author       = {Xuemei Jia and Xian Zhong and Mang Ye and Wenxuan Liu and Wenxin Huang},
  doi          = {10.1109/TIP.2022.3183469},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4227-4239},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Complementary data augmentation for cloth-changing person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). SPU-net: Self-supervised point cloud upsampling by
coarse-to-fine reconstruction with self-projection optimization.
<em>TIP</em>, <em>31</em>, 4213–4226. (<a
href="https://doi.org/10.1109/TIP.2022.3182266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of point cloud upsampling aims to acquire dense and uniform point sets from sparse and irregular point sets. Although significant progress has been made with deep learning models, state-of-the-art methods require ground-truth dense point sets as the supervision, which makes them limited to be trained under synthetic paired training data and not suitable to be under real-scanned sparse data. However, it is expensive and tedious to obtain large numbers of paired sparse-dense point sets as supervision from real-scanned sparse data. To address this problem, we propose a self-supervised point cloud upsampling network, named SPU-Net, to capture the inherent upsampling patterns of points lying on the underlying object surface. Specifically, we propose a coarse-to-fine reconstruction framework, which contains two main components: point feature extraction and point feature expansion, respectively. In the point feature extraction, we integrate the self-attention module with the graph convolution network (GCN) to capture context information inside and among local regions simultaneously. In the point feature expansion, we introduce a hierarchically learnable folding strategy to generate upsampled point sets with learnable 2D grids. Moreover, to further optimize the noisy points in the generated point sets, we propose a novel self-projection optimization associated with uniform and reconstruction terms as a joint loss to facilitate the self-supervised point cloud upsampling. We conduct various experiments on both synthetic and real-scanned datasets, and the results demonstrate that we achieve comparable performances to state-of-the-art supervised methods.},
  archive      = {J_TIP},
  author       = {Xinhai Liu and Xinchen Liu and Yu-Shen Liu and Zhizhong Han},
  doi          = {10.1109/TIP.2022.3182266},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4213-4226},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SPU-net: Self-supervised point cloud upsampling by coarse-to-fine reconstruction with self-projection optimization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel discrete convolutions on adaptive particle
representations of images. <em>TIP</em>, <em>31</em>, 4197–4212. (<a
href="https://doi.org/10.1109/TIP.2022.3181487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present data structures and algorithms for native implementations of discrete convolution operators over Adaptive Particle Representations (APR) of images on parallel computer architectures. The APR is a content-adaptive image representation that locally adapts the sampling resolution to the image signal. It has been developed as an alternative to pixel representations for large, sparse images as they typically occur in fluorescence microscopy. It has been shown to reduce the memory and runtime costs of storing, visualizing, and processing such images. This, however, requires that image processing natively operates on APRs, without intermediately reverting to pixels. Designing efficient and scalable APR-native image processing primitives, however, is complicated by the APR’s irregular memory structure. Here, we provide the algorithmic building blocks required to efficiently and natively process APR images using a wide range of algorithms that can be formulated in terms of discrete convolutions. We show that APR convolution naturally leads to scale-adaptive algorithms that efficiently parallelize on multi-core CPU and GPU architectures. We quantify the speedups in comparison to pixel-based algorithms and convolutions on evenly sampled data. We achieve pixel-equivalent throughputs of up to 1TB/s on a single Nvidia GeForce RTX 2080 gaming GPU, requiring up to two orders of magnitude less memory than a pixel-based implementation.},
  archive      = {J_TIP},
  author       = {Joel Jonsson and Bevan L. Cheeseman and Suryanarayana Maddu and Krzysztof Gonciarz and Ivo F. Sbalzarini},
  doi          = {10.1109/TIP.2022.3181487},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4197-4212},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Parallel discrete convolutions on adaptive particle representations of images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained classification via categorical memory networks.
<em>TIP</em>, <em>31</em>, 4186–4196. (<a
href="https://doi.org/10.1109/TIP.2022.3181492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the desire to exploit patterns shared across classes, we present a simple yet effective class-specific memory module for fine-grained feature learning. The memory module stores the prototypical feature representation for each category as a moving average. We hypothesize that the combination of similarities with respect to each category is itself a useful discriminative cue. To detect these similarities, we use attention as a querying mechanism. The attention scores with respect to each class prototype are used as weights to combine prototypes via weighted sum, producing a uniquely tailored response feature representation for a given input. The original and response features are combined to produce an augmented feature for classification. We integrate our class-specific memory module into a standard convolutional neural network, yielding a Categorical Memory Network. Our memory module significantly improves accuracy over baseline CNNs, achieving competitive accuracy with state-of-the-art methods on four benchmarks, including CUB-200-2011, Stanford Cars, FGVC Aircraft, and NABirds.},
  archive      = {J_TIP},
  author       = {Weijian Deng and Joshua Marsh and Stephen Gould and Liang Zheng},
  doi          = {10.1109/TIP.2022.3181492},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4186-4196},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained classification via categorical memory networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicate correlation learning for scene graph generation.
<em>TIP</em>, <em>31</em>, 4173–4185. (<a
href="https://doi.org/10.1109/TIP.2022.3181511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a typical Scene Graph Generation (SGG) method in image understanding, there usually exists a large gap in the performance of the predicates’ head classes and tail classes. This phenomenon is mainly caused by the semantic overlap between different predicates as well as the long-tailed data distribution. In this paper, a Predicate Correlation Learning (PCL) method for SGG is proposed to address the above problems by taking the correlation between predicates into consideration. To measure the semantic overlap between highly correlated predicate classes, a Predicate Correlation Matrix (PCM) is defined to quantify the relationship between predicate pairs, which is dynamically updated to remove the matrix’s long-tailed bias. In addition, PCM is integrated into a predicate correlation loss function ( $L_{PC}$ ) to reduce discouraging gradients of unannotated classes. The proposed method is evaluated on several benchmarks, where the performance of the tail classes is significantly improved when built on existing methods.},
  archive      = {J_TIP},
  author       = {Leitian Tao and Li Mi and Nannan Li and Xianhang Cheng and Yaosi Hu and Zhenzhong Chen},
  doi          = {10.1109/TIP.2022.3181511},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4173-4185},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Predicate correlation learning for scene graph generation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CERL: A unified optimization framework for light enhancement
with realistic noise. <em>TIP</em>, <em>31</em>, 4162–4172. (<a
href="https://doi.org/10.1109/TIP.2022.3180213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images captured in the real world are inevitably corrupted by sensor noise. Such noise is spatially variant and highly dependent on the underlying pixel intensity, deviating from the oversimplified assumptions in conventional denoising. Existing light enhancement methods either overlook the important impact of real-world noise during enhancement, or treat noise removal as a separate pre- or post-processing step. We present C oordinated E nhancement for R eal-world L ow-light Noisy Images (CERL), that seamlessly integrates light enhancement and noise suppression parts into a unified and physics-grounded optimization framework. For the real low-light noise removal part, we customize a self-supervised denoising model that can easily be adapted without referring to clean ground-truth images. For the light enhancement part, we also improve the design of a state-of-the-art backbone. The two parts are then joint formulated into one principled plug-and-play optimization. Our approach is compared against state-of-the-art low-light enhancement methods both qualitatively and quantitatively. Besides standard benchmarks, we further collect and test on a new realistic low-light mobile photography dataset (RLMP), whose mobile-captured photos display heavier realistic noise than those taken by high-quality cameras. CERL consistently produces the most visually pleasing and artifact-free results across all experiments. Our RLMP dataset and codes are available at: https://github.com/VITA-Group/CERL .},
  archive      = {J_TIP},
  author       = {Zeyuan Chen and Yifan Jiang and Dong Liu and Zhangyang Wang},
  doi          = {10.1109/TIP.2022.3180213},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4162-4172},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CERL: A unified optimization framework for light enhancement with realistic noise},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image quality assessment using contrastive learning.
<em>TIP</em>, <em>31</em>, 4149–4161. (<a
href="https://doi.org/10.1109/TIP.2022.3181496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of obtaining image quality representations in a self-supervised manner. We use prediction of distortion type and degree as an auxiliary task to learn features from an unlabeled image dataset containing a mixture of synthetic and realistic distortions. We then train a deep Convolutional Neural Network (CNN) using a contrastive pairwise objective to solve the auxiliary problem. We refer to the proposed training framework and resulting deep IQA model as the CONTRastive Image QUality Evaluator (CONTRIQUE). During evaluation, the CNN weights are frozen and a linear regressor maps the learned representations to quality scores in a No-Reference (NR) setting. We show through extensive experiments that CONTRIQUE achieves competitive performance when compared to state-of-the-art NR image quality models, even without any additional fine-tuning of the CNN backbone. The learned representations are highly robust and generalize well across images afflicted by either synthetic or authentic distortions. Our results suggest that powerful quality representations with perceptual relevance can be obtained without requiring large labeled subjective image quality datasets. The implementations used in this paper are available at https://github.com/pavancm/CONTRIQUE .},
  archive      = {J_TIP},
  author       = {Pavan C. Madhusudana and Neil Birkbeck and Yilin Wang and Balu Adsumilli and Alan C. Bovik},
  doi          = {10.1109/TIP.2022.3181496},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4149-4161},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image quality assessment using contrastive learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PU-dense: Sparse tensor-based point cloud geometry
upsampling. <em>TIP</em>, <em>31</em>, 4133–4148. (<a
href="https://doi.org/10.1109/TIP.2022.3180904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increased popularity of augmented and virtual reality experiences, the interest in capturing high-resolution real-world point clouds has never been higher. Loss of details and irregularities in point cloud geometry can occur during the capturing, processing, and compression pipeline. It is essential to address these challenges by being able to upsample a low Level-of-Detail (LoD) point cloud into a high LoD point cloud. Current upsampling methods suffer from several weaknesses in handling point cloud upsampling, especially in dense real-world photo-realistic point clouds. In this paper, we present a novel geometry upsampling technique, PU-Dense, which can process a diverse set of point clouds including synthetic mesh-based point clouds, real-world high-resolution point clouds, real-world indoor LiDAR scanned objects, as well as outdoor dynamically acquired LiDAR-based point clouds. PU-Dense employs a 3D multiscale architecture using sparse convolutional networks that hierarchically reconstruct an upsampled point cloud geometry via progressive rescaling and multiscale feature extraction. The framework employs a UNet type architecture that downscales the point cloud to a bottleneck and then upscales it to a higher level-of-detail (LoD) point cloud. PU-Dense introduces a novel Feature Extraction Unit that incorporates multiscale spatial learning by employing filters at multiple sampling rates and receptive fields. The architecture is memory efficient and is driven by a binary voxel occupancy classification loss that allows it to process high-resolution dense point clouds with millions of points during inference time. Qualitative and quantitative experimental results show that our method significantly outperforms the state-of-the-art approaches by a large margin while having much lower inference time complexity. We further test our dataset on high-resolution photo-realistic datasets. In addition, our method can handle noisy data well. We further show that our approach is memory efficient compared to the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Anique Akhtar and Zhu Li and Geert Van der Auwera and Li Li and Jianle Chen},
  doi          = {10.1109/TIP.2022.3180904},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4133-4148},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PU-dense: Sparse tensor-based point cloud geometry upsampling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Point cloud video super-resolution via partial point
coupling and graph smoothness. <em>TIP</em>, <em>31</em>, 4117–4132. (<a
href="https://doi.org/10.1109/TIP.2022.3166644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud (PC) is a collection of discrete geometric samples of a physical object in 3D space. A PC video consists of temporal frames evenly spaced in time, each containing a static PC at one time instant. PCs in adjacent frames typically do not have point-to-point (P2P) correspondence, and thus exploiting temporal redundancy for PC restoration across frames is difficult. In this paper, we focus on the super-resolution (SR) problem for PC video: increase point density of PCs in video frames while preserving salient geometric features consistently across time. We accomplish this with two ideas. First, we establish partial P2P coupling between PCs of adjacent frames by interpolating interior points in a low-resolution PC patch in frame $t$ and translating them to a corresponding patch in frame $t+1$ , via a motion model computed by iterative closest point (ICP). Second, we promote piecewise smoothness in 3D geometry in each patch using feature graph Laplacian regularizer (FGLR) in an easily computable quadratic form. The two ideas translate to an unconstrained quadratic programming (QP) problem with a system of linear equations as solution—one where we ensure the numerical stability by upper-bounding the condition number of the coefficient matrix. Finally, to improve the accuracy of the ICP motion model, we re-sample points in a super-resolved patch at time $t$ to better match a low-resolution patch at time $t+1$ via bipartite graph matching after each SR iteration. Experimental results show temporally consistent super-resolved PC videos generated by our scheme, outperforming SR competitors that optimized on a per-frame basis, in two established PC metrics.},
  archive      = {J_TIP},
  author       = {Chinthaka Dinesh and Gene Cheung and Ivan V. Bajić},
  doi          = {10.1109/TIP.2022.3166644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4117-4132},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Point cloud video super-resolution via partial point coupling and graph smoothness},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Motion-driven visual tempo learning for video-based action
recognition. <em>TIP</em>, <em>31</em>, 4104–4116. (<a
href="https://doi.org/10.1109/TIP.2022.3180585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action visual tempo characterizes the dynamics and the temporal scale of an action, which is helpful to distinguish human actions that share high similarities in visual dynamics and appearance. Previous methods capture the visual tempo either by sampling raw videos with multiple rates, which require a costly multi-layer network to handle each rate, or by hierarchically sampling backbone features, which rely heavily on high-level features that miss fine-grained temporal dynamics. In this work, we propose a Temporal Correlation Module (TCM), which can be easily embedded into the current action recognition backbones in a plug-in-and-play manner, to extract action visual tempo from low-level backbone features at single-layer remarkably. Specifically, our TCM contains two main components: a Multi-scale Temporal Dynamics Module (MTDM) and a Temporal Attention Module (TAM). MTDM applies a correlation operation to learn pixel-wise fine-grained temporal dynamics for both fast-tempo and slow-tempo. TAM adaptively emphasizes expressive features and suppresses inessential ones via analyzing the global information across various tempos. Extensive experiments conducted on several action recognition benchmarks, e.g. Something-Something V1&amp;V2, Kinetics-400, UCF-101, and HMDB-51, have demonstrated that the proposed TCM is effective to promote the performance of the existing video-based action recognition models for a large margin. The source code is publicly released at https://github.com/zphyix/TCM .},
  archive      = {J_TIP},
  author       = {Yuanzhong Liu and Junsong Yuan and Zhigang Tu},
  doi          = {10.1109/TIP.2022.3180585},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4104-4116},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Motion-driven visual tempo learning for video-based action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-guided image de-raining using time-lapse data.
<em>TIP</em>, <em>31</em>, 4090–4103. (<a
href="https://doi.org/10.1109/TIP.2022.3180561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of single image de-raining, that is, the task of recovering clean and rain-free background scenes from a single image obscured by a rainy artifact. Although recent advances adopt real-world time-lapse data to overcome the need for paired rain-clean images, they are limited to fully exploit the time-lapse data. The main cause is that, in terms of network architectures, they could not capture long-term rain streak information in the time-lapse data during training owing to the lack of memory components. To address this problem, we propose a novel network architecture combining the time-lapse data and, the memory network that explicitly helps to capture long-term rain streak information. Our network comprises the encoder-decoder networks and a memory network. The features extracted from the encoder are read and updated in the memory network that contains several memory items to store rain streak-aware feature representations. With the read/update operation, the memory network retrieves relevant memory items in terms of the queries, enabling the memory items to represent the various rain streaks included in the time-lapse data. To boost the discriminative power of memory features, we also present a novel background selective whitening (BSW) loss for capturing only rain streak information in the memory network by erasing the background information. Experimental results on standard benchmarks demonstrate the effectiveness and superiority of our approach.},
  archive      = {J_TIP},
  author       = {Jaehoon Cho and Seungryong Kim and Kwanghoon Sohn},
  doi          = {10.1109/TIP.2022.3180561},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4090-4103},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Memory-guided image de-raining using time-lapse data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ACE: Anchor-free corner evolution for real-time
arbitrarily-oriented object detection. <em>TIP</em>, <em>31</em>,
4076–4089. (<a href="https://doi.org/10.1109/TIP.2022.3167919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects with different orientations are ubiquitous in the real world ( e.g. , texts/hands in the scene image, objects in the aerial image, etc. ), and the widely-used axis-aligned bounding box does not compactly enclose the oriented objects. Thus arbitrarily-oriented object detection has attracted rising attention in recent years. In this paper, we propose a novel and effective model to detect arbitrarily-oriented objects. Instead of directly predicting the angles of oriented bounding boxes like most existing methods, we evolve the axis-aligned bounding box to the oriented quadrilateral box with the assistance of dynamically gathering contour information. More specifically, we first obtain the axis-aligned bounding box in an anchor-free manner. After that, we set the key points based on the sampled contour points of the axis-aligned bounding box. To improve the localization performance, we enrich the feature representations of these key points by exploiting a dynamic information gathering mechanism. This technique propagates the geometrical and semantic information along the sampled contour points, and fuses the information from the semantic neighbors of each sampled point, which varies for different locations. Finally, we estimate the offsets between the axis-aligned bounding box key points and the oriented quadrilateral box corner points. Extensive experiments on two frequently-used aerial image benchmarks HRSC2016 and DOTA, as well as scene text/hand datasets ICDAR2015, TD500, and Oxford-Hand, demonstrate the effectiveness and advantage of our proposed model.},
  archive      = {J_TIP},
  author       = {Pengwen Dai and Siyuan Yao and Zekun Li and Sanyi Zhang and Xiaochun Cao},
  doi          = {10.1109/TIP.2022.3167919},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4076-4089},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ACE: Anchor-free corner evolution for real-time arbitrarily-oriented object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VPU: A video-based point cloud upsampling framework.
<em>TIP</em>, <em>31</em>, 4062–4075. (<a
href="https://doi.org/10.1109/TIP.2022.3166627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new patch-based framework called VPU for the video-based point cloud upsampling task by effectively exploiting temporal dependency among multiple consecutive point cloud frames, in which each frame consists of a set of unordered, sparse and irregular 3D points. Rather than adopting the sophisticated motion estimation strategy in video analysis, we propose a new spatio-temporal aggregation (STA) module to effectively extract , align and aggregate rich local geometric clues from consecutive frames at the feature level. By more reliably summarizing spatio-temporally consistent and complementary knowledge from multiple frames in the resultant local structural features, our method better infers the local geometry distributions at the current frame. In addition, our STA module can be readily incorporated with various existing single frame-based point upsampling methods ( e.g. , PU-Net, MPU, PU-GAN and PU-GCN). Comprehensive experiments on multiple point cloud sequence datasets demonstrate our video-based point cloud upsampling framework achieves substantial performance improvement over its single frame-based counterparts.},
  archive      = {J_TIP},
  author       = {Kaisiyuan Wang and Lu Sheng and Shuhang Gu and Dong Xu},
  doi          = {10.1109/TIP.2022.3166627},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4062-4075},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VPU: A video-based point cloud upsampling framework},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Fine-grained multilevel fusion for anti-occlusion monocular
3D object detection. <em>TIP</em>, <em>31</em>, 4050–4061. (<a
href="https://doi.org/10.1109/TIP.2022.3180210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep fine-grained multi-level fusion architecture for monocular 3D object detection, with an additionally designed anti-occlusion optimization process. Conventional monocular 3D object detection methods usually leverage geometry constraints such as keypoints, object shape relationships, and 3D to 2D optimizations to offset the lack of accurate depth information. However, these methods still struggle against directly extracting rich information for fusion from the depth estimation. To solve the problem, we integrate the monocular 3D features with the pseudo-LiDAR filter generation network between fine-grained multi-level layers. Our network utilizes the inherent multi-scale and promotes depth and semantic information flow in different stages. The new architecture can obtain features that incorporate more reliable depth information. At the same time, the problem of occlusion among objects is prevalent in natural scenes yet remains unsolved mainly. We propose a novel loss function that aims at alleviating the problem of occlusion. Extensive experiments have proved that the framework demonstrates a competitive performance, especially for the complex scenes with occlusion.},
  archive      = {J_TIP},
  author       = {He Liu and Huaping Liu and Yikai Wang and Fuchun Sun and Wenbing Huang},
  doi          = {10.1109/TIP.2022.3180210},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4050-4061},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained multilevel fusion for anti-occlusion monocular 3D object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sinkhorn adversarial attack and defense. <em>TIP</em>,
<em>31</em>, 4039–4049. (<a
href="https://doi.org/10.1109/TIP.2022.3180207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks have been extensively investigated in the recent past. Quite interestingly, a majority of these attacks primarily work in the $l_{p}$ space. In this work, we propose a novel approach for generating adversarial samples using Wasserstein distance. Unlike previous approaches, we use an unbalanced optimal transport formulation which is naturally suited for images. We first compute an adversarial sample using a gradient step and then project the resultant image into Wasserstein ball with respect to original sample. The attack introduces perturbation in the form of pixel mass distribution which is guided by a cost metric. Elaborate experiments on MNIST, Fashion-MNIST, CIFAR-10 and Tiny ImageNet demonstrate a sharp decrease in the performance of state-of-art classifiers. We also perform experiments with adversarially trained classifiers and show that our system achieves superior performance in terms of adversarial defense against several state-of-art attacks. Our code and pre-trained models are available at https://bit.ly/2SQBR4E .},
  archive      = {J_TIP},
  author       = {A. V. Subramanyam},
  doi          = {10.1109/TIP.2022.3180207},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4039-4049},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sinkhorn adversarial attack and defense},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neighbor2Neighbor: A self-supervised framework for deep
image denoising. <em>TIP</em>, <em>31</em>, 4023–4038. (<a
href="https://doi.org/10.1109/TIP.2022.3176533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, image denoising has benefited a lot from deep neural networks. However, these models need large amounts of noisy-clean image pairs for supervision. Although there have been attempts in training denoising networks with only noisy images, existing self-supervised algorithms suffer from inefficient network training, heavy computational burden, or dependence on noise modeling. In this paper, we proposed a self-supervised framework named Neighbor2Neighbor for deep image denoising. We develop a theoretical motivation and prove that by designing specific samplers for training image pairs generation from only noisy images, we can train a self-supervised denoising network similar to the network trained with clean images supervision. Besides, we propose a regularizer in the perspective of optimization to narrow the optimization gap between the self-supervised denoiser and the supervised denoiser. We present a very simple yet effective self-supervised training scheme based on the theoretical understandings: training image pairs are generated by random neighbor sub-samplers, and denoising networks are trained with a regularized loss. Moreover, we propose a training strategy named BayerEnsemble to adapt the Neighbor2Neighbor framework in raw image denoising. The proposed Neighbor2Neighbor framework can enjoy the progress of state-of-the-art supervised denoising networks in network architecture design. It also avoids heavy dependence on the assumption of the noise distribution. We evaluate the Neighbor2Neighbor framework through extensive experiments, including synthetic experiments with different noise distributions and real-world experiments under various scenarios. The code is available online: https://github.com/TaoHuang2018/Neighbor2Neighbor .},
  archive      = {J_TIP},
  author       = {Tao Huang and Songjiang Li and Xu Jia and Huchuan Lu and Jianzhuang Liu},
  doi          = {10.1109/TIP.2022.3176533},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4023-4038},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Neighbor2Neighbor: A self-supervised framework for deep image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multisubject task-related fMRI data processing via a
two-stage generalized canonical correlation analysis. <em>TIP</em>,
<em>31</em>, 4011–4022. (<a
href="https://doi.org/10.1109/TIP.2022.3159125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional magnetic resonance imaging (fMRI) is one of the most popular methods for studying the human brain. Task-related fMRI data processing aims to determine which brain areas are activated when a specific task is performed and is usually based on the Blood Oxygen Level Dependent (BOLD) signal. The background BOLD signal also reflects systematic fluctuations in regional brain activity which are attributed to the existence of resting-state brain networks. We propose a new fMRI data generating model which takes into consideration the existence of common task-related and resting-state components. We first estimate the common task-related temporal component, via two successive stages of generalized canonical correlation analysis and, then, we estimate the common task-related spatial component, leading to a task-related activation map. The experimental tests of our method with synthetic data reveal that we are able to obtain very accurate temporal and spatial estimates even at very low Signal to Noise Ratio (SNR), which is usually the case in fMRI data processing. The tests with real-world fMRI data show significant advantages over standard procedures based on General Linear Models (GLMs).},
  archive      = {J_TIP},
  author       = {Paris A. Karakasis and Athanasios P. Liavas and Nicholas D. Sidiropoulos and Panagiotis G. Simos and Efrosini Papadaki},
  doi          = {10.1109/TIP.2022.3159125},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {4011-4022},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multisubject task-related fMRI data processing via a two-stage generalized canonical correlation analysis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater image enhancement via minimal color loss and
locally adaptive contrast enhancement. <em>TIP</em>, <em>31</em>,
3997–4010. (<a href="https://doi.org/10.1109/TIP.2022.3177129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images typically suffer from color deviations and low visibility due to the wavelength-dependent light absorption and scattering. To deal with these degradation issues, we propose an efficient and robust underwater image enhancement method, called MLLE. Specifically, we first locally adjust the color and details of an input image according to a minimum color loss principle and a maximum attenuation map-guided fusion strategy. Afterward, we employ the integral and squared integral maps to compute the mean and variance of local image blocks, which are used to adaptively adjust the contrast of the input image. Meanwhile, a color balance strategy is introduced to balance the color differences between channel a and channel b in the CIELAB color space. Our enhanced results are characterized by vivid color, improved contrast, and enhanced details. Extensive experiments on three underwater image enhancement datasets demonstrate that our method outperforms the state-of-the-art methods. Our method is also appealing in its fast processing speed within 1s for processing an image of size $1024\times 1024 \times 3$ on a single CPU. Experiments further suggest that our method can effectively improve the performance of underwater image segmentation, keypoint detection, and saliency detection. The project page is available at https://li-chongyi.github.io/proj},
  archive      = {J_TIP},
  author       = {Weidong Zhang and Peixian Zhuang and Hai-Han Sun and Guohou Li and Sam Kwong and Chongyi Li},
  doi          = {10.1109/TIP.2022.3177129},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3997-4010},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Underwater image enhancement via minimal color loss and locally adaptive contrast enhancement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised neural rendering for image hazing.
<em>TIP</em>, <em>31</em>, 3987–3996. (<a
href="https://doi.org/10.1109/TIP.2022.3177321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hazing aims to render a hazy image from a given clean one, which could be applied to a variety of practical applications such as gaming, filming, photographic filtering, and image dehazing. To generate plausible haze, we study two less-touched but challenging problems in hazy image rendering, namely, i) how to estimate the transmission map from a single image without auxiliary information, and ii) how to adaptively learn the airlight from exemplars, i.e. , unpaired real hazy images. To this end, we propose a neural rendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN is a knowledge-driven neural network which estimates the transmission map by leveraging a new prior, i.e. , there exists the structure similarity ( e.g. , contour and luminance) between the transmission map and the input clean image. To adaptively learn the airlight, we build a neural module based on another new prior, i.e. , the rendered hazy image and the exemplar are similar in the airlight distribution. To the best of our knowledge, this could be the first attempt to deeply render hazy images in an unsupervised fashion. Compared with existing haze generation methods, HazeGEN renders the hazy images in an unsupervised, learnable, and controllable manner, thus avoiding the labor-intensive efforts in paired data collection and the domain-shift issue in haze generation. Extensive experiments show the promising performance of our method comparing with some baselines in both qualitative and quantitative comparisons. The code is available at https://github.com/XLearning-SCU .},
  archive      = {J_TIP},
  author       = {Boyun Li and Yijie Lin and Jinfeng Bai and Peng Hu and Jiancheng Lv and Xi Peng},
  doi          = {10.1109/TIP.2022.3177321},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3987-3996},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised neural rendering for image hazing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relation-based associative joint location for human pose
estimation in videos. <em>TIP</em>, <em>31</em>, 3973–3986. (<a
href="https://doi.org/10.1109/TIP.2022.3177959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based human pose estimation (VHPE) is a vital yet challenging task. While deep learning algorithms have made tremendous progress for the VHPE, lots of these approaches to this task implicitly model the long-range interaction between joints by expanding the receptive field of the convolution or designing a graph manually. Unlike prior methods, we design a lightweight and plug-and-play joint relation extractor (JRE) to explicitly and automatically model the associative relationship between joints. The JRE takes the pseudo heatmaps of joints as input and calculates their similarity. In this way, the JRE can flexibly learn the correlation between any two joints, allowing it to learn the rich spatial configuration of human poses. Furthermore, the JRE can infer invisible joints according to the correlation between joints, which is beneficial for locating occluded joints. Then, combined with temporal semantic continuity modeling, we propose a Relation-based Pose Semantics Transfer Network (RPSTN) for video-based human pose estimation. Specifically, to capture the temporal dynamics of poses, the pose semantic information of the current frame is transferred to the next with a joint relation guided pose semantics propagator (JRPSP). The JRPSP can transfer the pose semantic features from the non-occluded frame to the occluded frame. The proposed RPSTN achieves state-of-the-art or competitive results on the video-based Penn Action, Sub-JHMDB, PoseTrack2018, and HiEve datasets. Moreover, the proposed JRE improves the performance of backbones on the image-based COCO2017 dataset. Code is available at https://github.com/YHDang/pose-estimation .},
  archive      = {J_TIP},
  author       = {Yonghao Dang and Jianqin Yin and Shaojie Zhang},
  doi          = {10.1109/TIP.2022.3177959},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3973-3986},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Relation-based associative joint location for human pose estimation in videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph jigsaw learning for cartoon face recognition.
<em>TIP</em>, <em>31</em>, 3961–3972. (<a
href="https://doi.org/10.1109/TIP.2022.3177952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cartoon face recognition is challenging as they typically have smooth color regions and emphasized edges, the key to recognizing cartoon faces is to precisely perceive their sparse and critical shape patterns. However, it is quite difficult to learn a shape-oriented representation for cartoon face recognition with convolutional neural networks (CNNs). To mitigate this issue, we propose the GraphJigsaw that constructs jigsaw puzzles at various stages in the classification network and solves the puzzles with the graph convolutional network (GCN) in a progressive manner. Solving the puzzles requires the model to spot the shape patterns of the cartoon faces as the texture information is quite limited. The key idea of GraphJigsaw is constructing a jigsaw puzzle by randomly shuffling the intermediate convolutional feature maps in the spatial dimension and exploiting the GCN to reason and recover the correct layout of the jigsaw fragments in a self-supervised manner. The proposed GraphJigsaw avoids training the classification model with the deconstructed images that would introduce noisy patterns and are harmful for the final classification. Specially, GraphJigsaw can be incorporated at various stages in a top-down manner within the classification model, which facilitates propagating the learned shape patterns gradually. GraphJigsaw does not rely on any extra manual annotation during the training process and incorporates no extra computation burden at inference time. Both quantitative and qualitative experimental results have verified the feasibility of our proposed GraphJigsaw, which consistently outperforms other face recognition or jigsaw-based methods on two popular cartoon face datasets with considerable improvements.},
  archive      = {J_TIP},
  author       = {Yong Li and Lingjie Lao and Zhen Cui and Shiguang Shan and Jian Yang},
  doi          = {10.1109/TIP.2022.3177952},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3961-3972},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph jigsaw learning for cartoon face recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional hyper-network for blind super-resolution with
multiple degradations. <em>TIP</em>, <em>31</em>, 3949–3960. (<a
href="https://doi.org/10.1109/TIP.2022.3176526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the single-image super-resolution (SISR) methods have achieved great success on the single degradation, they still suffer performance drop with multiple degrading effects in real scenarios. Recently, some blind and non-blind models for multiple degradations have been explored. However, these methods usually degrade significantly for distribution shifts between the training and test data. Towards this end, we propose a novel conditional hyper-network framework for super-resolution with multiple degradations (named CMDSR), which helps the SR framework learn how to adapt to changes in the degradation distribution of input. We extract degradation prior at the task-level with the proposed ConditionNet, which will be used to adapt the parameters of the basic SR network (BaseNet). Specifically, the ConditionNet of our framework first learns the degradation prior from a support set, which is composed of a series of degraded image patches from the same task. Then the adaptive BaseNet rapidly shifts its parameters according to the conditional features. Moreover, in order to better extract degradation prior, we propose a task contrastive loss to shorten the inner-task distance and enlarge the cross-task distance between task-level features. Without predefining degradation maps, our blind framework can conduct one single parameter update to yield considerable improvement in SR results. Extensive experiments demonstrate the effectiveness of CMDSR over various blind, and even several non-blind methods. The flexible BaseNet structure also reveals that CMDSR can be a general framework for a large series of SISR models. Our code is available at https://github.com/guanghaoyin/CMDSR .},
  archive      = {J_TIP},
  author       = {Guanghao Yin and Wei Wang and Zehuan Yuan and Wei Ji and Dongdong Yu and Shouqian Sun and Tat-Seng Chua and Changhu Wang},
  doi          = {10.1109/TIP.2022.3176526},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3949-3960},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conditional hyper-network for blind super-resolution with multiple degradations},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PNRNet: Physically-inspired neural rendering for any-to-any
relighting. <em>TIP</em>, <em>31</em>, 3935–3948. (<a
href="https://doi.org/10.1109/TIP.2022.3177311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing any-to-any relighting methods suffer from the task-aliasing effects and the loss of local details in the image generation process, such as shading and attached-shadow. In this paper, we present PNRNet, a novel neural architecture that decomposes the any-to-any relighting task into three simpler sub-tasks, i.e. lighting estimation, color temperature transfer, and lighting direction transfer, to avoid the task-aliasing effects. These sub-tasks are easy to learn and can be trained with direct supervisions independently. To better preserve local shading and attached-shadow details, we propose a parallel multi-scale network that incorporates multiple physical attributes to model local illuminations for lighting direction transfer. We also introduce a simple yet effective color temperature transfer network to learn a pixel-level non-linear function which allows color temperature adjustment beyond the predefined color temperatures and generalizes well to real images. Extensive experiments demonstrate that our proposed approach achieves better results quantitatively and qualitatively than prior works.},
  archive      = {J_TIP},
  author       = {Zhongyun Hu and Ntumba Elie Nsampi and Xue Wang and Qing Wang},
  doi          = {10.1109/TIP.2022.3177311},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3935-3948},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PNRNet: Physically-inspired neural rendering for any-to-any relighting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual cluster grounding for image captioning. <em>TIP</em>,
<em>31</em>, 3920–3934. (<a
href="https://doi.org/10.1109/TIP.2022.3177318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al. , 2014) and MS COCO datasets (Lin et al. , 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.},
  archive      = {J_TIP},
  author       = {Wenhui Jiang and Minwei Zhu and Yuming Fang and Guangming Shi and Xiaowei Zhao and Yang Liu},
  doi          = {10.1109/TIP.2022.3177318},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3920-3934},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual cluster grounding for image captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion feature aggregation for video-based person
re-identification. <em>TIP</em>, <em>31</em>, 3908–3919. (<a
href="https://doi.org/10.1109/TIP.2022.3175593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most video-based person re-identification (re-id) methods only focus on appearance features but neglect motion features. In fact, motion features can help to distinguish the target persons that are hard to be identified only by appearance features. However, most existing temporal information modeling methods cannot extract motion features effectively or efficiently for v ideo-based re-id. In this paper, we propose a more efficient Motion Feature Aggregation (MFA) method to model and aggregate motion information in the feature map level for video-based re-id. The proposed MFA consists of (i) a coarse-grained motion learning module, which extracts coarse-grained motion features based on the position changes of body parts over time, and (ii) a fine-grained motion learning module, which extracts fine-grained motion features based on the appearance changes of body parts over time. These two modules can model motion information from different granularities and are complementary to each other. It is easy to combine the proposed method with existing network architectures for end-to-end training. Extensive experiments on four widely used datasets demonstrate that the motion features extracted by MFA are crucial complements to appearance features for video-based re-id, especially for the scenario with large appearance changes. Besides, the results on LS-VID, the current largest publicly available video-based re-id dataset, surpass the state-of-the-art methods by a large margin. The code is available at: https://github.com/guxinqian/Simple-ReID .},
  archive      = {J_TIP},
  author       = {Xinqian Gu and Hong Chang and Bingpeng Ma and Shiguang Shan},
  doi          = {10.1109/TIP.2022.3175593},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3908-3919},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Motion feature aggregation for video-based person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subjective and objective quality of experience of free
viewpoint videos. <em>TIP</em>, <em>31</em>, 3896–3907. (<a
href="https://doi.org/10.1109/TIP.2022.3177127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free viewpoint videos (FVVs) provide immersive experiences for end-users, and they have been applied in many applications, such as movies, sports, and TV shows. However, the development of quantifying the quality of experience (QoE) of FVVs is still relatively slow due to the high costs of data collection and limited public databases. In this paper, we conduct a comprehensive study on FVV QoE. First, we construct the largest, to the best of our knowledge, FVV QoE database called Youku-FVV from two complex real scenarios, i . e ., entertainment and sports. Specifically, Youku-FVV originates from the videos captured by dozens of real cameras arranged annularly. We use these videos to generate virtual viewpoints, which make up FVVs together with real views. In constructing the FVV QoE database, we consider both internal and external influencing factors of QoE, which correspond to FVV generation and playback, respectively. Besides, we make an initial attempt to train an efficient no reference FVV QoE prediction model using this database, where several sparse frame sampling strategies are validated. And we demonstrate the feasibility of striving for the balance between effectiveness and efficiency of FVV QoE prediction. The proposed FVV QoE database and source codes are publicly available at https://github.com/QTJiebin/FVV_QoE},
  archive      = {J_TIP},
  author       = {Jiebin Yan and Jing Li and Yuming Fang and Zhaohui Che and Xue Xia and Yang Liu},
  doi          = {10.1109/TIP.2022.3177127},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3896-3907},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Subjective and objective quality of experience of free viewpoint videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast multiview clustering with spectral embedding.
<em>TIP</em>, <em>31</em>, 3884–3895. (<a
href="https://doi.org/10.1109/TIP.2022.3176223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering has been a hot topic in unsupervised learning owing to its remarkable clustering effectiveness and well-defined framework. Despite this, due to its high computation complexity, it is unable of handling large-scale or high-dimensional data, particularly multi-view large-scale data. To address this issue, in this paper, we propose a fast multi-view clustering algorithm with spectral embedding (FMCSE), which speeds up both the spectral embedding and spectral analysis stages of multi-view spectral clustering. Furthermore, unlike conventional spectral clustering, FMCSE can acquire all sample categories directly after optimization without extra k-means, which can significantly enhance efficiency. Moreover, we also provide a fast optimization strategy for solving the FMCSE model, which divides the optimization problem into three decoupled small-scale sub-problems that can be solved in a few iteration steps. Finally, extensive experiments on a variety of real-world datasets (including large-scale and high-dimensional datasets) show that, when compared to other state-of-the-art fast multi-view clustering baselines, FMCSE can maintain comparable or even better clustering effectiveness while significantly improving clustering efficiency.},
  archive      = {J_TIP},
  author       = {Ben Yang and Xuetao Zhang and Feiping Nie and Fei Wang},
  doi          = {10.1109/TIP.2022.3176223},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3884-3895},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast multiview clustering with spectral embedding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-local robust quaternion matrix completion for
large-scale color image and video inpainting. <em>TIP</em>, <em>31</em>,
3868–3883. (<a href="https://doi.org/10.1109/TIP.2022.3176133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image nonlocal self-similarity (NSS) prior refers to the fact that a local patch often has many nonlocal similar patches to it across the image and has been widely applied in many recently proposed machining learning algorithms for image processing. However, there is no theoretical analysis on its working principle in the literature. In this paper, we discover a potential causality between NSS and low-rank property of color images, which is also available to grey images. A new patch group based NSS prior scheme is proposed to learn explicit NSS models of natural color images. The numerical low-rank property of patched matrices is also rigorously proved. The NSS-based QMC algorithm computes an optimal low-rank approximation to the high-rank color image, resulting in high PSNR and SSIM measures and particularly the better visual quality. A new tensor NSS-based QMC method is also presented to solve the color video inpainting problem based on quaternion tensor representation. The numerical experiments on color images and videos indicate the advantages of NSS-based QMC over the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zhigang Jia and Qiyu Jin and Michael K. Ng and Xi-Le Zhao},
  doi          = {10.1109/TIP.2022.3176133},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3868-3883},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Non-local robust quaternion matrix completion for large-scale color image and video inpainting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). X-invariant contrastive augmentation and representation
learning for semi-supervised skeleton-based action recognition.
<em>TIP</em>, <em>31</em>, 3852–3867. (<a
href="https://doi.org/10.1109/TIP.2022.3175605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised skeleton-based action recognition is a challenging problem due to insufficient labeled data. For addressing this problem, some representative methods leverage contrastive learning to obtain more features from the pre-augmented skeleton actions. Such methods usually adopt a two-stage way: first randomly augment samples, and then learn their representations via contrastive learning. Since skeleton samples have already been randomly augmented, the representation ability of the subsequent contrastive learning is limited due to the inconsistency between the augmentations and representations. Thus, we propose a novel X-invariant Contrastive Augmentation and Representation learning (X-CAR) framework to thoroughly obtain rotate-shear-scale (X for short) invariant features by learning augmentations and representations of skeleton sequences in a one-stage way. In X-CAR, a new Adaptive-combination Augmentation (AA) mechanism is designed to rotate, shear, and scale the skeletons by learnable controlling factors in an adaptive way rather than a random way. Here, such controlling factors are also learned in the whole contrastive learning process, which can facilitate the consistency between the learned augmentations and representations of skeleton sequences. In addition, we relax the pre-definition of positive and negative samples to avoid the confusing allocation of ambiguous samples, and present a new Pull-Push Contrastive Loss (PPCL) to pull the augmenting skeleton close to the original skeleton, while push far away from the other skeletons. Experimental results on both NTU RGB+D and North-Western UCLA datasets show that the proposed X-CAR achieves better accuracy compared with other competitive methods in the semi-supervised scenario.},
  archive      = {J_TIP},
  author       = {Binqian Xu and Xiangbo Shu and Yan Song},
  doi          = {10.1109/TIP.2022.3175605},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3852-3867},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {X-invariant contrastive augmentation and representation learning for semi-supervised skeleton-based action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Superpixel guided deformable convolution network for
hyperspectral image classification. <em>TIP</em>, <em>31</em>,
3838–3851. (<a href="https://doi.org/10.1109/TIP.2022.3176537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are widely used in the field of hyperspectral image classification because of their excellent nonlinear feature extraction ability. However, as the sampling position of the regular convolution kernel is unchangeable, the regular convolution cannot distinctively extract the spatial and spectral information around the central pixel, which makes the classification results at the boundaries of ground objects over-smoothed and the classification performance degraded. Thus, we propose a novel superpixel guided deformable convolution network (SGDCN) for hyperspectral image classification. Firstly, the superpixel region fusion filter (SRF-Filter) is designed to fuse the initial superpixel region segmented by the simple linear iterative clustering (SLIC), making the fused superpixel region have a high homogeneity and also contain spatial features of diverse scales. Then, the superpixel guided deformable convolution (SGD-Conv) is proposed to make the shape of deformable convolution consistent with the real shape of land covers, and the SGD-Conv can extract pure neighborhood spatial-spectral features. Finally, a superpixel joint bilateral filter (SPJBF) is designed to solve the pixel-level and region-level misclassification problem, which can effectively utilize the superpixel region’s homogeneity and improve the classification accuracy. Experiments on three HSI datasets indicate that the SGDCN can obtain better classification performance when compared with other twelve state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Chunhui Zhao and Wenxiang Zhu and Shou Feng},
  doi          = {10.1109/TIP.2022.3176537},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3838-3851},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Superpixel guided deformable convolution network for hyperspectral image classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CKDF: Cascaded knowledge distillation framework for robust
incremental learning. <em>TIP</em>, <em>31</em>, 3825–3837. (<a
href="https://doi.org/10.1109/TIP.2022.3176130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, owing to the superior performances, knowledge distillation-based (kd-based) methods with the exemplar rehearsal have been widely applied in class incremental learning (CIL). However, we discover that they suffer from the feature uncalibration problem, which is caused by directly transferring knowledge from the old model immediately to the new model when learning a new task. As the old model confuses the feature representations between the learned and new classes, the kd loss and the classification loss used in kd-based methods are heterogeneous. This is detrimental if we learn the existing knowledge from the old model directly in the way as in typical kd-based methods. To tackle this problem, the feature calibration network (FCN) is proposed, which is used to calibrate the existing knowledge to alleviate the feature representation confusion of the old model. In addition, to relieve the task-recency bias of FCN caused by the limited storage memory in CIL, we propose a novel image-feature hybrid sample rehearsal strategy to train FCN by splitting the memory budget to store the image-and-feature exemplars of the previous tasks. As feature embeddings of images have much lower-dimensions, this allows us to store more samples to train FCN. Based on these two improvements, we propose the Cascaded Knowledge Distillation Framework (CKDF) including three main stages. The first stage is used to train FCN to calibrate the existing knowledge of the old model. Then, the new model is trained simultaneously by transferring knowledge from the calibrated teacher model through the knowledge distillation strategy and learning new classes. Finally, after completing the new task learning, the feature exemplars of previous tasks are updated. Importantly, we demonstrate that the proposed CKDF is a general framework that can be applied to various kd-based methods. Experimental results show that our method achieves state-of-the-art performances on several CIL benchmarks.},
  archive      = {J_TIP},
  author       = {Kunchi Li and Jun Wan and Shan Yu},
  doi          = {10.1109/TIP.2022.3176130},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3825-3837},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CKDF: Cascaded knowledge distillation framework for robust incremental learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BP-EVD: Forward block-output propagation for efficient video
denoising. <em>TIP</em>, <em>31</em>, 3809–3824. (<a
href="https://doi.org/10.1109/TIP.2022.3176210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising videos in real-time is critical in many applications, including robotics and medicine, where varying-light conditions, miniaturized sensors, and optics can substantially compromise image quality. This work proposes the first video denoising method based on a deep neural network that achieves state-of-the-art performance on dynamic scenes while running in real-time on VGA video resolution with no frame latency. The backbone of our method is a novel, remarkably simple, temporal network of cascaded blocks with forward block output propagation. We train our architecture with short, long, and global residual connections by minimizing the restoration loss of pairs of frames, leading to a more effective training across noise levels. It is robust to heavy noise following Poisson-Gaussian noise statistics. The algorithm is evaluated on RAW and RGB data. We propose a denoising algorithm that requires no future frames to denoise a current frame, reducing its latency considerably. The visual and quantitative results show that our algorithm achieves state-of-the-art performance among efficient algorithms, achieving from two-fold to two-orders-of-magnitude speed-ups on standard benchmarks for video denoising.},
  archive      = {J_TIP},
  author       = {Piotr Kopa Ostrowski and Efklidis Katsaros and Daniel Węsierski and Anna Jezierska},
  doi          = {10.1109/TIP.2022.3176210},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3809-3824},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BP-EVD: Forward block-output propagation for efficient video denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised nonlinear transform-based tensor nuclear
norm for multi-dimensional image recovery. <em>TIP</em>, <em>31</em>,
3793–3808. (<a href="https://doi.org/10.1109/TIP.2022.3176220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transform-based tensor nuclear norm (TNN) minimization methods have received increasing attention for recovering third-order tensors in multi-dimensional imaging problems. The main idea of these methods is to perform the linear transform along the third mode of third-order tensors and then minimize the nuclear norm of frontal slices of the transformed tensor. The main aim of this paper is to propose a nonlinear multilayer neural network to learn a nonlinear transform by solely using the observed tensor in a self-supervised manner. The proposed network makes use of the low-rank representation of the transformed tensor and data-fitting between the observed tensor and the reconstructed tensor to learn the nonlinear transform. Extensive experimental results on different data and different tasks including tensor completion, background subtraction, robust tensor completion, and snapshot compressive imaging demonstrate the superior performance of the proposed method over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yi-Si Luo and Xi-Le Zhao and Tai-Xiang Jiang and Yi Chang and Michael K. Ng and Chao Li},
  doi          = {10.1109/TIP.2022.3176220},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3793-3808},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised nonlinear transform-based tensor nuclear norm for multi-dimensional image recovery},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint representation learning and keypoint detection for
cross-view geo-localization. <em>TIP</em>, <em>31</em>, 3780–3792. (<a
href="https://doi.org/10.1109/TIP.2022.3175601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the cross-view geo-localization problem to match images from different viewpoints. The key motivation underpinning this task is to learn a discriminative viewpoint-invariant visual representation. Inspired by the human visual system for mining local patterns, we propose a new framework called RK-Net to jointly learn the discriminative Representation and detect salient Keypoints with a single Network. Specifically, we introduce a Unit Subtraction Attention Module (USAM) that can automatically discover representative keypoints from feature maps and draw attention to the salient regions. USAM contains very few learning parameters but yields significant performance improvement and can be easily plugged into different networks. We demonstrate through extensive experiments that (1) by incorporating USAM, RK-Net facilitates end-to-end joint learning without the prerequisite of extra annotations. Representation learning and keypoint detection are two highly-related tasks. Representation learning aids keypoint detection. Keypoint detection, in turn, enriches the model capability against large appearance changes caused by viewpoint variants. (2) USAM is easy to implement and can be integrated with existing methods, further improving the state-of-the-art performance. We achieve competitive geo-localization accuracy on three challenging datasets, i . e ., University-1652, CVUSA and CVACT. Our code is available at https://github.com/AggMan96/RK-Net .},
  archive      = {J_TIP},
  author       = {Jinliang Lin and Zhedong Zheng and Zhun Zhong and Zhiming Luo and Shaozi Li and Yi Yang and Nicu Sebe},
  doi          = {10.1109/TIP.2022.3175601},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3780-3792},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint representation learning and keypoint detection for cross-view geo-localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A spatial and geometry feature-based quality assessment
model for the light field images. <em>TIP</em>, <em>31</em>, 3765–3779.
(<a href="https://doi.org/10.1109/TIP.2022.3175619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new full-reference image quality assessment (IQA) model for performing perceptual quality evaluation on light field (LF) images, called the spatial and geometry feature-based model (SGFM). Considering that the LF image describe both spatial and geometry information of the scene, the spatial features are extracted over the sub-aperture images (SAIs) by using contourlet transform and then exploited to reflect the spatial quality degradation of the LF images, while the geometry features are extracted across the adjacent SAIs based on 3D-Gabor filter and then explored to describe the viewing consistency loss of the LF images. These schemes are motivated and designed based on the fact that the human eyes are more interested in the scale, direction, contour from the spatial perspective and viewing angle variations from the geometry perspective. These operations are applied to the reference and distorted LF images independently. The degree of similarity can be computed based on the above-measured quantities for jointly arriving at the final IQA score of the distorted LF image. Experimental results on three commonly-used LF IQA datasets show that the proposed SGFM is more in line with the quality assessment of the LF images perceived by the human visual system (HVS), compared with multiple classical and state-of-the-art IQA models.},
  archive      = {J_TIP},
  author       = {Hailiang Huang and Huanqiang Zeng and Junhui Hou and Jing Chen and Jianqing Zhu and Kai-Kuang Ma},
  doi          = {10.1109/TIP.2022.3175619},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3765-3779},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A spatial and geometry feature-based quality assessment model for the light field images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly alignment-free RGBT salient object detection with
deep correlation network. <em>TIP</em>, <em>31</em>, 3752–3764. (<a
href="https://doi.org/10.1109/TIP.2022.3176540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT Salient Object Detection (SOD) focuses on common salient regions of a pair of visible and thermal infrared images. Existing methods perform on the well-aligned RGBT image pairs, but the captured image pairs are always unaligned and aligning them requires much labor cost. To handle this problem, we propose a novel deep correlation network (DCNet), which explores the correlations across RGB and thermal modalities, for weakly alignment-free RGBT SOD. In particular, DCNet includes a modality alignment module based on the spatial affine transformation, the feature-wise affine transformation and the dynamic convolution to model the strong correlation of two modalities. Moreover, we propose a novel bi-directional decoder model, which combines the coarse-to-fine and fine-to-coarse processes for better feature enhancement. In particular, we design a modality correlation ConvLSTM by adding the first two components of modality alignment module and a global context reinforcement module into ConvLSTM, which is used to decode hierarchical features in both top-down and button-up manners. Extensive experiments on three public benchmark datasets show the remarkable performance of our method against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Zhengzheng Tu and Zhun Li and Chenglong Li and Jin Tang},
  doi          = {10.1109/TIP.2022.3176540},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3752-3764},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly alignment-free RGBT salient object detection with deep correlation network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SceneSketcher-v2: Fine-grained scene-level sketch-based
image retrieval using adaptive GCNs. <em>TIP</em>, <em>31</em>,
3737–3751. (<a href="https://doi.org/10.1109/TIP.2022.3175403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based image retrieval (SBIR) is a long-standing research topic in computer vision. Existing methods mainly focus on category-level or instance-level image retrieval. This paper investigates the fine-grained scene-level SBIR problem where a free-hand sketch depicting a scene is used to retrieve desired images. This problem is useful yet challenging mainly because of two entangled facts: 1) achieving an effective representation of the input query data and scene-level images is difficult as it requires to model the information across multiple modalities such as object layout, relative size and visual appearances, and 2) there is a great domain gap between the query sketch input and target images. We present SceneSketcher-v2, a Graph Convolutional Network (GCN) based architecture to address these challenges. SceneSketcher-v2 employs a carefully designed graph convolution network to fuse the multi-modality information in the query sketch and target images and uses a triplet training process and end-to-end training manner to alleviate the domain gap. Extensive experiments demonstrate SceneSketcher-v2 outperforms state-of-the-art scene-level SBIR models with a significant margin.},
  archive      = {J_TIP},
  author       = {Fang Liu and Xiaoming Deng and Changqing Zou and Yu-Kun Lai and Keqi Chen and Ran Zuo and Cuixia Ma and Yong-Jin Liu and Hongan Wang},
  doi          = {10.1109/TIP.2022.3175403},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3737-3751},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SceneSketcher-v2: Fine-grained scene-level sketch-based image retrieval using adaptive GCNs},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DO-conv: Depthwise over-parameterized convolutional layer.
<em>TIP</em>, <em>31</em>, 3726–3736. (<a
href="https://doi.org/10.1109/TIP.2022.3175432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional layers are the core building blocks of Convolutional Neural Networks (CNNs). In this paper, we propose to augment a convolutional layer with an additional depthwise convolution, where each input channel is convolved with a different 2D kernel. The composition of the two convolutions constitutes an over-parameterization, since it adds learnable parameters, while the resulting linear operation can be expressed by a single convolution layer. We refer to this depthwise over-parameterized convolutional layer as DO-Conv, which is a novel way of over-parameterization. We show with extensive experiments that the mere replacement of conventional convolutional layers with DO-Conv layers boosts the performance of CNNs on many classical vision tasks, such as image classification, detection, and segmentation. Moreover, in the inference phase, the depthwise convolution is folded into the conventional convolution, reducing the computation to be exactly equivalent to that of a convolutional layer without over-parameterization. As DO-Conv introduces performance gains without incurring any computational complexity increase for inference, we advocate it as an alternative to the conventional convolutional layer. We open sourced an implementation of DO-Conv in Tensorflow, PyTorch and GluonCV at https://github.com/yangyanli/DO-Conv .},
  archive      = {J_TIP},
  author       = {Jinming Cao and Yangyan Li and Mingchao Sun and Ying Chen and Dani Lischinski and Daniel Cohen-Or and Baoquan Chen and Changhe Tu},
  doi          = {10.1109/TIP.2022.3175432},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3726-3736},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DO-conv: Depthwise over-parameterized convolutional layer},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data augmentation using bitplane information recombination
model. <em>TIP</em>, <em>31</em>, 3713–3725. (<a
href="https://doi.org/10.1109/TIP.2022.3175429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning heavily depend on the quantity and quality of training data. But in many fields, well-annotated data are so difficult to collect, which makes the data scale hard to meet the needs of network training. To deal with this issue, a novel data augmentation method using the bitplane information recombination model (termed as BIRD) is proposed in this paper. Considering each bitplane can provide different structural information at different levels of detail, this method divides the internal hierarchical structure of a given image into different bitplanes, and reorganizes them by bitplane extraction, bitplane selection and bitplane recombination, to form an augmented data with different image details. This method can generate up to 62 times of the training data, for a given 8-bits image. In addition, this generalized method is model free, parameter free and easy to combine with various neural networks, without changing the original annotated data. Taking the task of target detection for remotely sensed images and classification for natural images as an example, experimental results on DOTA dataset and CIFAR-100 dataset demonstrated that, our proposed method is not only effective for data augmentation, but also helpful to improve the accuracy of target detection and image classification.},
  archive      = {J_TIP},
  author       = {Huan Zhang and Zhiyi Xu and Xiaolin Han and Weidong Sun},
  doi          = {10.1109/TIP.2022.3175429},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3713-3725},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Data augmentation using bitplane information recombination model},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward top-down just noticeable difference estimation of
natural images. <em>TIP</em>, <em>31</em>, 3697–3712. (<a
href="https://doi.org/10.1109/TIP.2022.3174398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just noticeable difference (JND) of natural images refers to the maximum pixel intensity change magnitude that typical human visual system (HVS) cannot perceive. Existing efforts on JND estimation mainly dedicate to modeling the diverse masking effects in either/both spatial or/and frequency domains, and then fusing them into an overall JND estimate. In this work, we turn to a dramatically different way to address this problem with a top-down design philosophy. Instead of explicitly formulating and fusing different masking effects in a bottom-up way, the proposed JND estimation model dedicates to first predicting a critical perceptual lossless (CPL) counterpart of the original image and then calculating the difference map between the original image and the predicted CPL image as the JND map. We conduct subjective experiments to determine the critical points of 500 images and find that the distribution of cumulative normalized KLT coefficient energy values over all 500 images at these critical points can be well characterized by a Weibull distribution. Given a testing image, its corresponding critical point is determined by a simple weighted average scheme where the weights are determined by a fitted Weibull distribution function. The performance of the proposed JND model is evaluated explicitly with direct JND prediction and implicitly with two applications including JND-guided noise injection and JND-guided image compression. Experimental results have demonstrated that our proposed JND model can achieve better performance than several latest JND models. In addition, we also compare the proposed JND model with existing visual difference predicator (VDP) metrics in terms of the capability in distortion detection and discrimination. The results indicate that our JND model also has a good performance in this task. The code of this work are available at https://github.com/Zhentao-Liu/KLT-JND .},
  archive      = {J_TIP},
  author       = {Qiuping Jiang and Zhentao Liu and Shiqi Wang and Feng Shao and Weisi Lin},
  doi          = {10.1109/TIP.2022.3174398},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3697-3712},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward top-down just noticeable difference estimation of natural images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive regression and classification for dense object
detector. <em>TIP</em>, <em>31</em>, 3684–3696. (<a
href="https://doi.org/10.1109/TIP.2022.3174391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In object detection, enhancing feature representation using localization information has been revealed as a crucial procedure to improve detection performance. However, the localization information ( i.e. , regression feature and regression offset) captured by the regression branch is still not well utilized. In this paper, we propose a simple but effective method called Interactive Regression and Classification (IRC) to better utilize localization information. Specifically, we propose Feature Aggregation Module (FAM) and Localization Attention Module (LAM) to leverage localization information to the classification branch during forward propagation. Furthermore, the classifier also guides the learning of the regression branch during backward propagation, to guarantee that the localization information is beneficial to both regression and classification. Thus, the regression and classification branches are learned in an interactive manner. Our method can be easily integrated into anchor-based and anchor-free object detectors without increasing computation cost. With our method, the performance is significantly improved on many popular dense object detectors, including RetinaNet, FCOS, ATSS, PAA, GFL, GFLV2, OTA, GA-RetinaNet, RepPoints, BorderDet and VFNet. Based on ResNet-101 backbone, IRC achieves 47.2\% AP on COCO test-dev, surpassing the previous state-of-the-art PAA (44.8\% AP), GFL (45.0\% AP) and without sacrificing the efficiency both in training and inference. Moreover, our best model (Res2Net-101-DCN) can achieve a single-model single-scale AP of 51.4\%.},
  archive      = {J_TIP},
  author       = {Linmao Zhou and Hong Chang and Bingpeng Ma and Shiguang Shan},
  doi          = {10.1109/TIP.2022.3174391},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3684-3696},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Interactive regression and classification for dense object detector},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recovering realistic details for magnification-arbitrary
image super-resolution. <em>TIP</em>, <em>31</em>, 3669–3683. (<a
href="https://doi.org/10.1109/TIP.2022.3174393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of implicit neural representations (INR) has shown the potential to represent images in a continuous form by mapping pixel coordinates to RGB values. Recent work is capable of recovering arbitrary-resolution images from the continuous representations of the input low-resolution (LR) images. However, it can only super-resolve blurry images and lacks the ability to generate perceptual-pleasant details. In this paper, we propose implicit pixel flow (IPF) to model the coordinate dependency between the blurry INR distribution and the sharp real-world distribution. For each pixel near the blurry edges, IPF assigns offsets for the coordinates of the pixel so that the original RGB values can be replaced by the RGB values of a neighboring pixel which are more appropriate to form sharper edges. By modifying the relationship between the INR-domain coordinates and the image-domain pixels via IPF, we convert the original blurry INR distribution to a sharp one. Specifically, we adopt convolutional neural networks to extract continuous flow representations and employ multi-layer perceptrons to build the implicit function for calculating pixel flow. In addition, we propose a new double constraint module to search for more stable and optimal pixel flows during training. To the best of our knowledge, this is the first method to recover perceptually-pleasant details for magnification-arbitrary single image super-resolution. Experimental results on public benchmark datasets demonstrate that we successfully restore shape edges and satisfactory textures from continuous image representations.},
  archive      = {J_TIP},
  author       = {Cheng Ma and Peiqi Yu and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TIP.2022.3174393},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3669-3683},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Recovering realistic details for magnification-arbitrary image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented multimodality fusion for generalized zero-shot
sketch-based visual retrieval. <em>TIP</em>, <em>31</em>, 3657–3668. (<a
href="https://doi.org/10.1109/TIP.2022.3173815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot sketch-based image retrieval (ZS-SBIR) has attracted great attention recently, due to the potential application of sketch-based retrieval under zero-shot scenarios, where the categories of query sketches and gallery photos are not observed in the training stage. However, it is still under insufficient exploration for the general and practical scenario when the query sketches and gallery photos contain both seen and unseen categories. Such a problem is defined as generalized zero-shot sketch-based image retrieval (GZS-SBIR), which is the focus of this work. To this end, we propose a novel Augmented Multi-modality Fusion (AMF) framework to generalize seen concepts to unobserved ones efficiently. Specifically, a novel knowledge discovery module named cross-domain augmentation is designed in both visual and semantic space to mimic novel knowledge unseen from the training stage, which is the key to handling the GZS-SBIR challenge. Moreover, a triplet domain alignment module is proposed to couple the cross-domain distribution between photo and sketch in visual space. To enhance the robustness of our model, we explore embedding propagation to refine both visual and semantic features by removing undesired noise. Eventually, visual-semantic fusion representations are concatenated for further domain discrimination and task-specific recognition, which tend to trigger the cross-domain alignment in both visual and semantic feature space. Experimental evaluations are conducted on popular ZS-SBIR benchmarks as well as a new evaluation protocol designed for GZS-SBIR from DomainNet dataset with more diverse sub-domains, and the promising results demonstrate the superiority of the proposed solution over other baselines. The source code is available at https://github.com/scottjingtt/AMF_GZS_SBIR.git .},
  archive      = {J_TIP},
  author       = {Taotao Jing and Haifeng Xia and Jihun Hamm and Zhengming Ding},
  doi          = {10.1109/TIP.2022.3173815},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3657-3668},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Augmented multimodality fusion for generalized zero-shot sketch-based visual retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video quality model of compression, resolution and frame
rate adaptation based on space-time regularities. <em>TIP</em>,
<em>31</em>, 3644–3656. (<a
href="https://doi.org/10.1109/TIP.2022.3173810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to accurately predict the visual quality of videos subjected to various combinations of dimension reduction protocols is of high interest to the streaming video industry, given rapid increases in frame resolutions and frame rates. In this direction, we have developed a video quality predictor that is sensitive to spatial, temporal, or space-time subsampling combined with compression. Our predictor is based on new models of space-time natural video statistics (NVS). Specifically, we model the statistics of divisively normalized difference between neighboring frames that are relatively displaced. In an extensive empirical study, we found that those paths of space-time displaced frame differences that provide maximal regularity against our NVS model generally align best with motion trajectories. Motivated by this, we built a new video quality prediction engine that extracts NVS features that represent how space-time directional regularities are disturbed by space-time distortions. Based on parametric models of these regularities, we compute features that are used to train a regressor that can accurately predict perceptual quality. As a stringent test of the new model, we apply it to the difficult problem of predicting the quality of videos subjected not only to compression, but also to downsampling in space and/or time. We show that the new quality model achieves state-of-the-art (SOTA) prediction performance on the new ETRI-LIVE Space-Time Subsampled Video Quality (STSVQ) and also on the AVT-VQDB-UHD-1 database.},
  archive      = {J_TIP},
  author       = {Dae Yeol Lee and Jongho Kim and Hyunsuk Ko and Alan C. Bovik},
  doi          = {10.1109/TIP.2022.3173810},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3644-3656},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video quality model of compression, resolution and frame rate adaptation based on space-time regularities},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual aligned siamese dense regression tracker. <em>TIP</em>,
<em>31</em>, 3630–3643. (<a
href="https://doi.org/10.1109/TIP.2022.3166638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anchor or anchor-free based Siamese trackers have achieved the astonishing advancement. However, their parallel regression and classification branches lack the tracked target information link and interaction, and the corresponding independent optimization maybe lead to task-misalignment, such as the reliable classification prediction with imprecisely localization and vice versa. To address this problem, we develop a general Siamese dense regression tracker (SDRT) with both task and feature alignments. It consists of two cooperative and mutual-guidance core branches: dense local regression with RepPoint representation, the global and local multi-classifier fusion with aligned features. They complement and boost each other to constrain the results with well-localized followed to also be well-classified. Specifically, a dense local regression with RepPoint representation, directly estimates and averages multiple dense local bounding box offsets for accurate localization. And then, the refined bounding boxes can be used to learn the global and local affine alignment features for reliable multi-classifier fusion. The classified scores in turn guide the assigned positive bounding boxes for the regression task. The mutual guidance operations can bridge the connection between classification and regression substantially, since the assigned labels of one task depend on the prediction quality of the other task. The proposed tracking module is general, and it can boost both the anchor or anchor-free based Siamese trackers to some extent. The extensive tracking comparisons on six tracking benchmarks verify its favorable and competitive performance over states-of-the-arts tracking modules.},
  archive      = {J_TIP},
  author       = {Baojie Fan and Hui Zhang and Yang Cong and Yandong Tang and Huijie Fan and Jiandong Tian},
  doi          = {10.1109/TIP.2022.3166638},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3630-3643},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual aligned siamese dense regression tracker},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Dual mixture model based CNN for image denoising.
<em>TIP</em>, <em>31</em>, 3618–3629. (<a
href="https://doi.org/10.1109/TIP.2022.3173814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Gaussian residual error and noise are common in the real applications, and they can be efficiently addressed by some non-quadratic fidelity terms in the classic variational method. However, they have not been well integrated into the architectures design in the convolutional neural networks (CNN) based image denoising method. In this paper, we propose a deep learning approach to handle non-Gaussian residual error. Our method is developed on an universal approximation property for the probability density functions of the non-Gaussian error/noise. By considering the duality of the maximum likelihood estimation for the non-Gaussian error, an adaptive weighting strategy can be derived for image fidelity. To get a good image prior, a learnable regularizer is adopted. Solving such a problem iteratively can be unrolled as a weighted residual CNN architecture. The main advantage of our method is that the weighted residual block can well handle the non-Gaussian residual, especially for the noise with non-uniformly spatial distribution. Numerical results show that it has better performance on non-Gaussian noise (e.g. Gaussian mixture, random-valued impulse noise) removal than the related existing methods.},
  archive      = {J_TIP},
  author       = {Zhuoxiao Li and Faqiang Wang and Li Cui and Jun Liu},
  doi          = {10.1109/TIP.2022.3173814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3618-3629},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual mixture model based CNN for image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluster-guided asymmetric contrastive learning for
unsupervised person re-identification. <em>TIP</em>, <em>31</em>,
3606–3617. (<a href="https://doi.org/10.1109/TIP.2022.3173163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-ID) aims to match pedestrian images from different camera views in an unsupervised setting. Existing methods for unsupervised person Re-ID are usually built upon the pseudo labels from clustering. However, the result of clustering depends heavily on the quality of the learned features, which are overwhelmingly dominated by colors in images. In this paper, we attempt to suppress the negative dominating influence of colors to learn more effective features for unsupervised person Re-ID. Specifically, we propose a Cluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised person Re-ID, in which clustering result is leveraged to guide the feature learning in a properly designed asymmetric contrastive learning framework. In CACL, both instance-level and cluster-level contrastive learning are employed to help the siamese network learn discriminant features with respect to the clustering result within and between different data augmentation views, respectively. In addition, we also present a cluster refinement method, and validate that the cluster refinement step helps CACL significantly. Extensive experiments conducted on three benchmark datasets demonstrate the superior performance of our proposal.},
  archive      = {J_TIP},
  author       = {Mingkun Li and Chun-Guang Li and Jun Guo},
  doi          = {10.1109/TIP.2022.3173163},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3606-3617},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cluster-guided asymmetric contrastive learning for unsupervised person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiview spectral clustering with bipartite graph.
<em>TIP</em>, <em>31</em>, 3591–3605. (<a
href="https://doi.org/10.1109/TIP.2022.3171411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view spectral clustering has become appealing due to its good performance in capturing the correlations among all views. However, on one hand, many existing methods usually require a quadratic or cubic complexity for graph construction or eigenvalue decomposition of Laplacian matrix; on the other hand, they are inefficient and unbearable burden to be applied to large scale data sets, which can be easily obtained in the era of big data. Moreover, the existing methods cannot encode the complementary information between adjacency matrices, i.e. , similarity graphs of views and the low-rank spatial structure of adjacency matrix of each view. To address these limitations, we develop a novel multi-view spectral clustering model. Our model well encodes the complementary information by Schatten $p$ -norm regularization on the third tensor whose lateral slices are composed of the adjacency matrices of the corresponding views. To further improve the computational efficiency, we leverage anchor graphs of views instead of full adjacency matrices of the corresponding views, and then present a fast model that encodes the complementary information embedded in anchor graphs of views by Schatten $p$ -norm regularization on the tensor bipartite graph. Finally, an efficient alternating algorithm is derived to optimize our model. The constructed sequence was proved to converge to the stationary KKT point. Extensive experimental results indicate that our method has good performance.},
  archive      = {J_TIP},
  author       = {Haizhou Yang and Quanxue Gao and Wei Xia and Ming Yang and Xinbo Gao},
  doi          = {10.1109/TIP.2022.3171411},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3591-3605},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiview spectral clustering with bipartite graph},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained image quality caption with hierarchical
semantics degradation. <em>TIP</em>, <em>31</em>, 3578–3590. (<a
href="https://doi.org/10.1109/TIP.2022.3171445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image quality assessment (BIQA), which is capable of precisely and automatically estimating human perceived image quality with no pristine image for comparison, attracts extensive attention and is of wide applications. Recently, many existing BIQA methods commonly represent image quality with a quantitative value, which is inconsistent with human cognition. Generally, human beings are good at perceiving image quality in terms of semantic description rather than quantitative value. Moreover, cognition is a needs-oriented task where humans are able to extract image contents with local to global semantics as they need. The mediocre quality value represents coarse or holistic image quality and fails to reflect degradation on hierarchical semantics. In this paper, to comply with human cognition, a novel quality caption model is inventively proposed to measure fine-grained image quality with hierarchical semantics degradation. Research on human visual system indicates there are hierarchy and reverse hierarchy correlations between hierarchical semantics. Meanwhile, empirical evidence shows that there are also bi-directional degradation dependencies between them. Thus, a novel bi-directional relationship-based network (BDRNet) is proposed for semantics degradation description, through adaptively exploring those correlations and degradation dependencies in a bi-directional manner. Extensive experiments demonstrate that our method outperforms the state-of-the-arts in terms of both evaluation performance and generalization ability.},
  archive      = {J_TIP},
  author       = {Wen Yang and Jinjian Wu and Shiwei Tian and Leida Li and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TIP.2022.3171445},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3578-3590},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained image quality caption with hierarchical semantics degradation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I2Transformer: Intra- and inter-relation embedding
transformer for TV show captioning. <em>TIP</em>, <em>31</em>,
3565–3577. (<a href="https://doi.org/10.1109/TIP.2022.3159472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TV show captioning aims to generate a linguistic sentence based on the video and its associated subtitle. Compared to purely video-based captioning, the subtitle can provide the captioning model with useful semantic clues such as actors’ sentiments and intentions. However, the effective use of subtitle is also very challenging, because it is the pieces of scrappy information and has semantic gap with visual modality. To organize the scrappy information together and yield a powerful omni-representation for all the modalities, an efficient captioning model requires understanding video contents, subtitle semantics, and the relations in between. In this paper, we propose an Intra- and Inter-relation Embedding Transformer (I 2 Transformer), consisting of an Intra-relation Embedding Block (IAE) and an Inter-relation Embedding Block (IEE) under the framework of a Transformer. First, the IAE captures the intra-relation in each modality via constructing the learnable graphs. Then, IEE learns the cross attention gates, and selects useful information from each modality based on their inter-relations, so as to derive the omni-representation as the input to the Transformer. Experimental results on the public dataset show that the I 2 Transformer achieves the state-of-the-art performance. We also evaluate the effectiveness of the IAE and IEE on two other relevant tasks of video with text inputs, i.e. , TV show retrieval and video-guided machine translation. The encouraging performance further validates that the IAE and IEE blocks have a good generalization ability. The code is available at https://github.com/tuyunbin/I2Transformer .},
  archive      = {J_TIP},
  author       = {Yunbin Tu and Liang Li and Li Su and Shengxiang Gao and Chenggang Yan and Zheng-Jun Zha and Zhengtao Yu and Qingming Huang},
  doi          = {10.1109/TIP.2022.3159472},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3565-3577},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {I2Transformer: Intra- and inter-relation embedding transformer for TV show captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal unrolled robust PCA for background foreground
separation. <em>TIP</em>, <em>31</em>, 3553–3564. (<a
href="https://doi.org/10.1109/TIP.2022.3172851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background foreground separation (BFS) is a popular computer vision problem where dynamic foreground objects are separated from the static background of a scene. Typically, this is performed using consumer cameras because of their low cost, human interpretability, and high resolution. Yet, cameras and the BFS algorithms that process their data have common failure modes due to lighting changes, highly reflective surfaces, and occlusion. One solution is to incorporate an additional sensor modality that provides robustness to such failure modes. In this paper, we explore the ability of a cost-effective radar system to augment the popular Robust PCA technique for BFS. We apply the emerging technique of algorithm unrolling to yield real-time computation, feedforward inference, and strong generalization in comparison with traditional deep learning methods. We benchmark on the RaDICaL dataset to demonstrate both quantitative improvements of incorporating radar data and qualitative improvements that confirm robustness to common failure modes of image-based methods.},
  archive      = {J_TIP},
  author       = {Spencer Markowitz and Corey Snyder and Yonina C. Eldar and Minh N. Do},
  doi          = {10.1109/TIP.2022.3172851},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3553-3564},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multimodal unrolled robust PCA for background foreground separation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Deepfake forensics via an adversarial game. <em>TIP</em>,
<em>31</em>, 3541–3552. (<a
href="https://doi.org/10.1109/TIP.2022.3172845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the progress in AI-based facial forgery ( i.e ., deepfake), people are concerned about its abuse. Albeit effort has been made for training models to recognize such forgeries, existing models suffer from poor generalization to unseen forgery technologies and high sensitivity to changes in image/video quality. In this paper, we advocate robust training for improving the generalization ability. We believe training with samples that are adversarially crafted to attack the classification models improves the generalization ability considerably. Considering that AI-based face manipulation often leads to high-frequency artifacts that can be easily spotted (by models) yet difficult to generalize, we further propose a new adversarial training method that attempts to blur out these artifacts, by introducing pixel-wise Gaussian blurring. Plenty of empirical evidence show that, with adversarial training, models are forced to learn more discriminative and generalizable features. Our code: https://github.com/ah651/deepfake_adv .},
  archive      = {J_TIP},
  author       = {Zhi Wang and Yiwen Guo and Wangmeng Zuo},
  doi          = {10.1109/TIP.2022.3172845},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3541-3552},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deepfake forensics via an adversarial game},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised foggy scene understanding via self
spatial-temporal label diffusion. <em>TIP</em>, <em>31</em>, 3525–3540.
(<a href="https://doi.org/10.1109/TIP.2022.3172208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding foggy image sequence in driving scene is critical for autonomous driving, but it remains a challenging task due to the difficulty in collecting and annotating real-world images of adverse weather. Recently, self-training strategy has been considered as a powerful solution for unsupervised domain adaptation, which iteratively adapts the model from the source domain to the target domain by generating target pseudo labels and re-training the model. However, the selection of confident pseudo labels inevitably suffers from the conflict between sparsity and accuracy, both of which will lead to suboptimal models. To tackle this problem, we exploit the characteristics of the foggy image sequence of driving scenes to densify the confident pseudo labels. Specifically, based on the two discoveries of local spatial similarity and adjacent temporal correspondence of the sequential image data, we propose a novel Target-Domain driven pseudo label Diffusion (TDo-Dif) scheme. It employs superpixels and optical flows to identify the spatial similarity and temporal correspondence, respectively, and then diffuses the confident but sparse pseudo labels within a superpixel or a temporal corresponding pair linked by the flow. Moreover, to ensure the feature similarity of the diffused pixels, we introduce local spatial similarity loss and temporal contrastive loss in the model re-training stage. Experimental results show that our TDo-Dif scheme helps the adaptive model achieve 51.92\% and 53.84\% mean intersection-over-union (mIoU) on two publicly available natural foggy datasets (Foggy Zurich and Foggy Driving), which exceeds the state-of-the-art unsupervised domain adaptive semantic segmentation methods. The proposed method can also be applied to non-sequential images in the target domain by considering only spatial similarity.},
  archive      = {J_TIP},
  author       = {Liang Liao and Wenyi Chen and Jing Xiao and Zheng Wang and Chia-Wen Lin and Shin’ichi Satoh},
  doi          = {10.1109/TIP.2022.3172208},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3525-3540},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised foggy scene understanding via self spatial-temporal label diffusion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GraFT: Graph filtered temporal dictionary learning for
functional neural imaging. <em>TIP</em>, <em>31</em>, 3509–3524. (<a
href="https://doi.org/10.1109/TIP.2022.3171414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical imaging of calcium signals in the brain has enabled researchers to observe the activity of hundreds-to-thousands of individual neurons simultaneously. Current methods predominantly use morphological information, typically focusing on expected shapes of cell bodies, to better identify neurons in the field-of-view. The explicit shape constraints limit the applicability of automated cell identification to other important imaging scales with more complex morphologies, e.g., dendritic or widefield imaging. Specifically, fluorescing components may be broken up, incompletely found, or merged in ways that do not accurately describe the underlying neural activity. Here we present Graph Filtered Temporal Dictionary (GraFT), a new approach that frames the problem of isolating independent fluorescing components as a dictionary learning problem. Specifically, we focus on the time-traces—the main quantity used in scientific discovery—and learn a time trace dictionary with the spatial maps acting as the presence coefficients encoding which pixels the time-traces are active in. Furthermore, we present a novel graph filtering model which redefines connectivity between pixels in terms of their shared temporal activity, rather than spatial proximity. This model greatly eases the ability of our method to handle data with complex non-local spatial structure. We demonstrate important properties of our method, such as robustness to morphology, simultaneously detecting different neuronal types, and implicitly inferring number of neurons, on both synthetic data and real data examples. Specifically, we demonstrate applications of our method to calcium imaging both at the dendritic, somatic, and widefield scales.},
  archive      = {J_TIP},
  author       = {Adam S. Charles and Nathan Cermak and Rifqi O. Affan and Benjamin B. Scott and Jackie Schiller and Gal Mishne},
  doi          = {10.1109/TIP.2022.3171414},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3509-3524},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GraFT: Graph filtered temporal dictionary learning for functional neural imaging},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MA-GANet: A multi-attention generative adversarial network
for defocus blur detection. <em>TIP</em>, <em>31</em>, 3494–3508. (<a
href="https://doi.org/10.1109/TIP.2022.3171424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background clutters pose challenges to defocus blur detection. Existing approaches often produce artifact predictions in background areas with clutter and relatively low confident predictions in boundary areas. In this work, we tackle the above issues from two perspectives. Firstly, inspired by the recent success of self-attention mechanism, we introduce channel-wise and spatial-wise attention modules to attentively aggregate features at different channels and spatial locations to obtain more discriminative features. Secondly, we propose a generative adversarial training strategy to suppress spurious and low reliable predictions. This is achieved by utilizing a discriminator to identify predicted defocus map from ground-truth ones. As such, the defocus network (generator) needs to produce ‘realistic’ defocus map to minimize discriminator loss. We further demonstrate that the generative adversarial training allows exploiting additional unlabeled data to improve performance, a.k.a. semi-supervised learning, and we provide the first benchmark on semi-supervised defocus detection. Finally, we demonstrate that the existing evaluation metrics for defocus detection generally fail to quantify the robustness with respect to thresholding. For a fair and practical evaluation, we introduce an effective yet efficient $AUF_\beta $ metric. Extensive experiments on three public datasets verify the superiority of the proposed methods compared against state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Zeyu Jiang and Xun Xu and Le Zhang and Chao Zhang and Chuan Sheng Foo and Ce Zhu},
  doi          = {10.1109/TIP.2022.3171424},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3494-3508},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MA-GANet: A multi-attention generative adversarial network for defocus blur detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Siamese-SR: A siamese super-resolution model for boosting
resolution of digital rock images for improved petrophysical property
estimation. <em>TIP</em>, <em>31</em>, 3479–3493. (<a
href="https://doi.org/10.1109/TIP.2022.3172211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Rock Physics leverages advances in digital image acquisition and analysis techniques to create 3D digital images of rock samples, which are used for computational modeling and simulations to predict petrophysical properties of interest. However, the accuracy of the predictions is crucially dependent on the quality of the digital images, which is currently limited by the resolution of the micro-CT scanning technology. We have proposed a novel Deep Learning based Super-Resolution model called Siamese-SR to digitally boost the resolution of Digital Rock images whilst retaining the texture and providing optimal de-noising. The Siamese-SR model consists of a generator which is adversarially trained with a relativistic and a siamese discriminator utilizing Materials In Context (MINC) loss estimator. This model has been demonstrated to improve the resolution of sandstone rock images acquired using micro-CT scanning by a factor of 2. Another key highlight of our work is that for the evaluation of the super-resolution performance, we propose to move away from image-based metrics such as Structural Similarity (SSIM) and Peak Signal to Noise Ratio (PSNR) because they do not correlate well with expert geological and petrophysical evaluations. Instead, we propose to subject the super-resolved images to the next step in the Digital Rock workflow to calculate a crucial petrophysical property of interest, viz. porosity and use it as a metric for evaluation of our proposed Siamese-SR model against several other existing super-resolution methods like SRGAN, ESRGAN, EDSR and SPSR. Furthermore, we also use Local Attribution Maps to show how our proposed Siamese-SR model focuses optimally on edge-semantics, which is what leads to improvement in the image-based porosity prediction, the permeability prediction from Multiple Relaxation Time Lattice Boltzmann Method (MRTLBM) flow simulations as well as the prediction of other petrophysical properties of interest derived from Mercury Injection Capillary Pressure (MICP) simulations.},
  archive      = {J_TIP},
  author       = {Vishal R. Ahuja and Utkarsh Gupta and Shivani R. Rapole and Nishank Saxena and Ronny Hofmann and Ruarri J. Day-Stirrat and Jaya Prakash and Phaneendra K. Yalavarthy},
  doi          = {10.1109/TIP.2022.3172211},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3479-3493},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Siamese-SR: A siamese super-resolution model for boosting resolution of digital rock images for improved petrophysical property estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics-coupled neural network magnetic resonance electrical
property tomography (MREPT) for conductivity reconstruction.
<em>TIP</em>, <em>31</em>, 3463–3478. (<a
href="https://doi.org/10.1109/TIP.2022.3172220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrical property (EP) of human tissues is a quantitative biomarker that facilitates early diagnosis of cancerous tissues. Magnetic resonance electrical properties tomography (MREPT) is an imaging modality that reconstructs EPs by the radio-frequency field in an MRI system. MREPT reconstructs EPs by solving analytic models numerically based on Maxwell’s equations. Most MREPT methods suffer from artifacts caused by inaccuracy of the hypotheses behind the models, and/or numerical errors. These artifacts can be mitigated by adding coefficients to stabilize the models, however, the selection of such coefficient has been empirical, which limit its medical application. Alternatively, end-to-end Neural networks-based MREPT (NN-MREPT) learns to reconstruct the EPs from training samples, circumventing Maxwell’s equations. However, due to its pattern-matching nature, it is difficult for NN-MREPT to produce accurate reconstructions for new samples. In this work, we proposed a physics-coupled NN for MREPT (PCNN-MREPT), in which an analytic model, cr-MREPT, works with diffusion and convection coefficients, learned by NNs from the difference between the reconstructed and ground-truth EPs to reduce artifacts. With two simulated datasets, three generalization experiments in which test samples deviate gradually from the training samples, and one noise-robustness experiment were conducted. The results show that the proposed PCNN-MREPT achieves higher accuracy than two representative analytic methods. Moreover, compared with an end-to-end NN-MREPT, the proposed method attained higher accuracy in two critical generalization tests. This is an important step to practical MREPT medical diagnoses.},
  archive      = {J_TIP},
  author       = {Adan Jafet Garcia Inda and Shao Ying Huang and Nevrez İmamoğlu and Wenwei Yu},
  doi          = {10.1109/TIP.2022.3172220},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3463-3478},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Physics-coupled neural network magnetic resonance electrical property tomography (MREPT) for conductivity reconstruction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised meta learning with multiview constraints for
hyperspectral image small sample set classification. <em>TIP</em>,
<em>31</em>, 3449–3462. (<a
href="https://doi.org/10.1109/TIP.2022.3169689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulties of obtaining sufficient labeled samples have always been one of the factors hindering deep learning models from obtaining high accuracy in hyperspectral image (HSI) classification. To reduce the dependence of deep learning models on training samples, meta learning methods have been introduced, effectively improving the classification accuracy in small sample set scenarios. However, the existing methods based on meta learning still need to construct a labeled source data set with several pre-collected HSIs, and must utilize a large number of labeled samples for meta-training, which is actually time-consuming and labor-intensive. To solve this problem, this paper proposes a novel unsupervised meta learning method with multiview constraints for HSI small sample set classification. Specifically, the proposed method first builds an unlabeled source data set using unlabeled HSIs. Then, multiple spatial-spectral multiview features of each unlabeled sample are generated to construct tasks for unsupervised meta learning. Finally, the designed residual relation network is used for meta-training and small sample set classification based on the voting strategy. Compared with existing supervised meta learning methods for HSI classification, our method can only utilize HSIs without any label for unsupervised meta learning, which significantly reduces the number of requisite labeled samples in the whole classification process. To verify the effectiveness of the proposed method, extensive experiments are carried out on 8 public HSIs in the cross-domain and in-domain classification scenarios. The statistical results demonstrate that, compared with existing supervised meta learning methods and other advanced classification models, the proposed method can achieve competitive or better classification performance in small sample set scenarios.},
  archive      = {J_TIP},
  author       = {Kuiliang Gao and Bing Liu and Xuchu Yu and Anzhu Yu},
  doi          = {10.1109/TIP.2022.3169689},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3449-3462},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised meta learning with multiview constraints for hyperspectral image small sample set classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature-aligned video raindrop removal with temporal
constraints. <em>TIP</em>, <em>31</em>, 3440–3448. (<a
href="https://doi.org/10.1109/TIP.2022.3170726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing adherent raindrop removal methods focus on the detection of the raindrop locations, and then use inpainting techniques or generative networks to recover the background behind raindrops. Yet, as adherent raindrops are diverse in sizes and appearances, the detection is challenging for both single image and video. Moreover, unlike rain streaks, adherent raindrops tend to cover the same area in several frames. Addressing these problems, our method employs a two-stage video-based raindrop removal method. The first stage is the single image module, which generates initial clean results. The second stage is the multiple frame module, which further refines the initial results using temporal constraints, namely, by utilizing multiple input frames in our process and applying temporal consistency between adjacent output frames. Our single image module employs a raindrop removal network to generate initial raindrop removal results, and create a mask representing the differences between the input and initial output. Once the masks and initial results for consecutive frames are obtained, our multiple-frame module aligns the frames in both the image and feature levels and then obtains the clean background. Our method initially employs optical flow to align the frames, and then utilizes deformable convolution layers further to achieve feature-level frame alignment. To remove small raindrops and recover correct backgrounds, a target frame is predicted from adjacent frames. A series of unsupervised losses are proposed so that our second stage, which is the video raindrop removal module, can self-learn from video data without ground truths. Experimental results on real videos demonstrate the state-of-art performance of our method both quantitatively and qualitatively.},
  archive      = {J_TIP},
  author       = {Wending Yan and Lu Xu and Wenhan Yang and Robby T. Tan},
  doi          = {10.1109/TIP.2022.3170726},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3440-3448},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Feature-aligned video raindrop removal with temporal constraints},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive attribute and structure subspace clustering
network. <em>TIP</em>, <em>31</em>, 3430–3439. (<a
href="https://doi.org/10.1109/TIP.2022.3171421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep self-expressiveness-based subspace clustering methods have demonstrated effectiveness. However, existing works only consider the attribute information to conduct the self-expressiveness, limiting the clustering performance. In this paper, we propose a novel adaptive attribute and structure subspace clustering network (AASSC-Net) to simultaneously consider the attribute and structure information in an adaptive graph fusion manner. Specifically, we first exploit an auto-encoder to represent input data samples with latent features for the construction of an attribute matrix. We also construct a mixed signed and symmetric structure matrix to capture the local geometric structure underlying data samples. Then, we perform self-expressiveness on the constructed attribute and structure matrices to learn their affinity graphs separately. Finally, we design a novel attention-based fusion module to adaptively leverage these two affinity graphs to construct a more discriminative affinity graph. Extensive experimental results on commonly used benchmark datasets demonstrate that our AASSC-Net significantly outperforms state-of-the-art methods. In addition, we conduct comprehensive ablation studies to discuss the effectiveness of the designed modules. The code is publicly available at https://github.com/ZhihaoPENG-CityU/AASSC-Net .},
  archive      = {J_TIP},
  author       = {Zhihao Peng and Hui Liu and Yuheng Jia and Junhui Hou},
  doi          = {10.1109/TIP.2022.3171421},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3430-3439},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive attribute and structure subspace clustering network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical prototype refinement with progressive
inter-categorical discrimination maximization for few-shot learning.
<em>TIP</em>, <em>31</em>, 3414–3429. (<a
href="https://doi.org/10.1109/TIP.2022.3170727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based few-shot learning categorizes unseen query instances by measuring their distance to the categories appearing in the given support set. To facilitate distance measurement, prototypes are used to approximate the representations of categories. However, we find prototypical representations are generally not discriminative enough to represent the discrepancy of inter-categorical distribution of queries, thereby limiting the classification accuracy. To overcome this issue, we propose a new Progressive Hierarchical-Refinement (PHR) method, which effectively refines the discrimination of prototypes by conducting the Progressive Discrimination Maximization strategy based on the hierarchical feature representations. Specifically, we first encode supports and queries into the representation space of spatial level, global level, and semantic level. Then, the refining coefficients are constructed by exploring the metric information contained in these hierarchical embedding spaces simultaneously. Under the guidance of the refining coefficients, the meta-refining loss progressively maximizes the discrimination degree of inter-categorical prototypical representations. In addition, the refining vectors are adopted to further enhance the representations of prototypes. In this way, the metric-based classification can be more accurate. Our PHR method shows the competitive performance on the miniImagenet, CIFAR-FS, FC100, and CUB datasets. Moreover, PHR presents good compatibility. It can be incorporated with other few-shot learning models, making them more accurate.},
  archive      = {J_TIP},
  author       = {Yuan Zhou and Yanrong Guo and Shijie Hao and Richang Hong},
  doi          = {10.1109/TIP.2022.3170727},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3414-3429},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical prototype refinement with progressive inter-categorical discrimination maximization for few-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attribute artifacts removal for geometry-based point cloud
compression. <em>TIP</em>, <em>31</em>, 3399–3413. (<a
href="https://doi.org/10.1109/TIP.2022.3170722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometry-based point cloud compression (G-PCC) can achieve remarkable compression efficiency for point clouds. However, it still leads to serious attribute compression artifacts, especially under low bitrate scenarios. In this paper, we propose a Multi-Scale Graph Attention Network (MS-GAT) to remove the artifacts of point cloud attributes compressed by G-PCC. We first construct a graph based on point cloud geometry coordinates and then use the Chebyshev graph convolutions to extract features of point cloud attributes. Considering that one point may be correlated with points both near and far away from it, we propose a multi-scale scheme to capture the short- and long-range correlations between the current point and its neighboring and distant points. To address the problem that various points may have different degrees of artifacts caused by adaptive quantization, we introduce the quantization step per point as an extra input to the proposed network. We also incorporate a weighted graph attentional layer into the network to pay special attention to the points with more attribute artifacts. To the best of our knowledge, this is the first attribute artifacts removal method for G-PCC. We validate the effectiveness of our method over various point clouds. Objective comparison results show that our proposed method achieves an average of 9.74\% BD-rate reduction compared with Predlift and 10.13\% BD-rate reduction compared with RAHT. Subjective comparison results present that visual artifacts such as color shifting, blurring, and quantization noise are reduced.},
  archive      = {J_TIP},
  author       = {Xihua Sheng and Li Li and Dong Liu and Zhiwei Xiong},
  doi          = {10.1109/TIP.2022.3170722},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3399-3413},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attribute artifacts removal for geometry-based point cloud compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards lightweight transformer via group-wise
transformation for vision-and-language tasks. <em>TIP</em>, <em>31</em>,
3386–3398. (<a href="https://doi.org/10.1109/TIP.2021.3139234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e. , Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer . LW-Transformer applies Group-wise Transformation to reduce both the parameters and computations of Transformer, while also preserving its two main properties, i.e. , the efficient attention modeling on diverse subspaces of MHA, and the expanding-scaling feature transformation of FFN. We apply LW-Transformer to a set of Transformer-based networks, and quantitatively measure them on three vision-and-language tasks and six benchmark datasets. Experimental results show that while saving a large number of parameters and computations, LW-Transformer achieves very competitive performance against the original Transformer networks for vision-and-language tasks. To examine the generalization ability, we apply LW-Transformer to the task of image classification, and build its network based on a recently proposed image Transformer called Swin-Transformer, where the effectiveness can be also confirmed.},
  archive      = {J_TIP},
  author       = {Gen Luo and Yiyi Zhou and Xiaoshuai Sun and Yan Wang and Liujuan Cao and Yongjian Wu and Feiyue Huang and Rongrong Ji},
  doi          = {10.1109/TIP.2021.3139234},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3386-3398},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards lightweight transformer via group-wise transformation for vision-and-language tasks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-manifold deep discriminative cross-modal hashing for
medical image retrieval. <em>TIP</em>, <em>31</em>, 3371–3385. (<a
href="https://doi.org/10.1109/TIP.2022.3171081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefitting from the low storage cost and high retrieval efficiency, hash learning has become a widely used retrieval technology to approximate nearest neighbors. Within it, the cross-modal medical hashing has attracted an increasing attention in facilitating efficiently clinical decision. However, there are still two main challenges in weak multi-manifold structure perseveration across multiple modalities and weak discriminability of hash code. Specifically, existing cross-modal hashing methods focus on pairwise relations within two modalities, and ignore underlying multi-manifold structures across over 2 modalities. Then, there is little consideration about discriminability, i.e., any pair of hash codes should be different. In this paper, we propose a novel hashing method named multi-manifold deep discriminative cross-modal hashing (MDDCH) for large-scale medical image retrieval. The key point is multi-modal manifold similarity which integrates multiple sub-manifolds defined on heterogeneous data to preserve correlation among instances, and it can be measured by three-step connection on corresponding hetero-manifold. Then, we propose discriminative item to make each hash code encoded by hash functions be different, which improves discriminative performance of hash code. Besides, we introduce Gaussian-binary Restricted Boltzmann Machine to directly output hash codes without using any continuous relaxation. Experiments on three benchmark datasets (AIBL, Brain and SPLP) show that our proposed MDDCH achieves comparative performance to recent state-of-the-art hashing methods. Additionally, diagnostic evaluation from professional physicians shows that all the retrieved medical images describe the same object and illness as the queried image.},
  archive      = {J_TIP},
  author       = {Liming Xu and Xianhua Zeng and Bochuan Zheng and Weisheng Li},
  doi          = {10.1109/TIP.2022.3171081},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3371-3385},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-manifold deep discriminative cross-modal hashing for medical image retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spot-adaptive knowledge distillation. <em>TIP</em>,
<em>31</em>, 3359–3370. (<a
href="https://doi.org/10.1109/TIP.2022.3170728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has become a well established paradigm for compressing deep neural networks. The typical way of conducting knowledge distillation is to train the student network under the supervision of the teacher network to harness the knowledge at one or multiple spots ( i.e. , layers) in the teacher network. The distillation spots, once specified, will not change for all the training samples, throughout the whole distillation process. In this work, we argue that distillation spots should be adaptive to training samples and distillation epochs. We thus propose a new distillation strategy, termed spot-adaptive KD (SAKD), to adaptively determine the distillation spots in the teacher network per sample, at every training iteration during the whole distillation period. As SAKD actually focuses on “where to distill” instead of “what to distill” that is widely investigated by most existing works, it can be seamlessly integrated into existing distillation methods to further improve their performance. Extensive experiments with 10 state-of-the-art distillers are conducted to demonstrate the effectiveness of SAKD for improving their distillation performance, under both homogeneous and heterogeneous distillation settings. Code is available at https://github.com/zju-vipa/spot-adaptive-pytorch .},
  archive      = {J_TIP},
  author       = {Jie Song and Ying Chen and Jingwen Ye and Mingli Song},
  doi          = {10.1109/TIP.2022.3170728},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3359-3370},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spot-adaptive knowledge distillation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CAST: Learning both geometric and texture style transfers
for effective caricature generation. <em>TIP</em>, <em>31</em>,
3347–3358. (<a href="https://doi.org/10.1109/TIP.2022.3154238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a photo of a subject, ability to generate a caricature image that captures distinct characteristics of the subject but with certain exaggeration of their prominent features is of fundamental importance to image processing and facial recognition. There are two main challenges in this task: shape exaggeration and style transfer. The former morphs and exaggerates key facial features of the subject, while the latter generates caricature images in a certain artistic style. In this paper, we propose a CAricature Style Transfer (CAST) framework for caricature generation. There are two modules in the proposed framework. The first is a geometric warping module. Different from the existing style transfer methods, we incorporate the Whitening and Coloring Transformation (WCT) in the geometric style transfer. The WCT is learned on photo and caricature landmarks or the caricature landmark space of a specific artist and is capable of transforming input photo landmarks to caricature landmarks. The second module is a texture style rendering module. We propose a new style transfer method by considering a semantic region-aligned style transfer via affinity constraint. Given a reference caricature image as the style reference, this module is capable of transferring styles between the same or similar semantic regions in caricatures and photos. Furthermore, it can transfer visual attributes of the reference caricatures (such as mouth shape and expressions) to the output caricatures. Experiments have shown desirable effects of the proposed method in transferring both the geometric and artistic texture styles of caricatures. Both qualitative and quantitative results show that the CAST framework is more effective compared than the state-of-the-art caricature generation methods.},
  archive      = {J_TIP},
  author       = {Jing Huo and Xiangde Liu and Wenbin Li and Yang Gao and Hujun Yin and Jiebo Luo},
  doi          = {10.1109/TIP.2022.3154238},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3347-3358},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CAST: Learning both geometric and texture style transfers for effective caricature generation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid dynamic contrast and probability distillation for
unsupervised person re-id. <em>TIP</em>, <em>31</em>, 3334–3346. (<a
href="https://doi.org/10.1109/TIP.2022.3169693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (Re-Id) has attracted increasing attention due to its practical application in the read-world video surveillance system. The traditional unsupervised Re-Id are mostly based on the method alternating between clustering and fine-tuning with the classification or metric learning objectives on the grouped clusters. However, since person Re-Id is an open-set problem, the clustering based methods often leave out lots of outlier instances or group the instances into the wrong clusters, thus they can not make full use of the training samples as a whole. To solve these problems, we present the hybrid dynamic cluster contrast and probability distillation algorithm. It formulates the unsupervised Re-Id problem into an unified local-to-global dynamic contrastive learning and self-supervised probability distillation framework. Specifically, the proposed method can make the best of the self-supervised signals of all the clustered and un-clustered instances, from both the instances’ self-contrastive level and the probability distillation respectives, in the memory-based non-parametric manner. Besides, the proposed hybrid local-to-global contrastive learning can take full advantage of the informative and valuable training examples for effective and robust training. Extensive experiment results show that the proposed method achieves superior performances to state-of-the-art methods, under both the purely unsupervised and unsupervised domain adaptation experiment settings. Our source code is released in https://github.com/zjy2050/HDCRL-ReID .},
  archive      = {J_TIP},
  author       = {De Cheng and Jingyu Zhou and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TIP.2022.3169693},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3334-3346},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hybrid dynamic contrast and probability distillation for unsupervised person re-id},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An individual-difference-aware model for cross-person gaze
estimation. <em>TIP</em>, <em>31</em>, 3322–3333. (<a
href="https://doi.org/10.1109/TIP.2022.3171416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method on refining cross-person gaze prediction task with eye/face images only by explicitly modelling the person-specific differences. Specifically, we first assume that we can obtain some initial gaze prediction results with existing method, which we refer to as InitNet, and then introduce three modules, the Validity Module (VM), Self-Calibration (SC) and Person-specific Transform (PT) module. By predicting the reliability of current eye/face images, VM is able to identify invalid samples, e.g. eye blinking images, and reduce their effects in modelling process. SC and PT module then learn to compensate for the differences on valid samples only. The former models the translation offsets by bridging the gap between initial predictions and dataset-wise distribution. And the later learns more general person-specific transformation by incorporating the information from existing initial predictions of the same person. We validate our ideas on three publicly available datasets, EVE, XGaze, and MPIIGaze dataset. We demonstrate that our proposed method outperforms the SOTA methods significantly on all of them, e.g. respectively 21.7\%, 36.0\%, and 32.9\% relative performance improvements. We are the winner of the GAZE 2021 EVE Challenge and our code can be found here https://github.com/bjj9/EVE_SCPT .},
  archive      = {J_TIP},
  author       = {Jun Bao and Buyu Liu and Jun Yu},
  doi          = {10.1109/TIP.2022.3171416},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3322-3333},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An individual-difference-aware model for cross-person gaze estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lossless white balance for improved lossless CFA image and
video compression. <em>TIP</em>, <em>31</em>, 3309–3321. (<a
href="https://doi.org/10.1109/TIP.2022.3169687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color filter array is a spatial multiplexing of pixel-sized filters fabricated over pixel sensors in most color image sensors. The state-of-the-art lossless coding techniques of raw sensor data captured by such sensors leverage spatial or cross-color correlation using lifting schemes. In this paper, we propose a lifting-based lossless white balance algorithm. When applied to the raw sensor data, the spatial bandwidth of the implied chrominance signals decreases. We propose to use this white balance as a pre-processing step to lossless CFA subsampled image/video compression, improving the overall coding efficiency of the raw sensor data.},
  archive      = {J_TIP},
  author       = {Yeejin Lee and Keigo Hirakawa},
  doi          = {10.1109/TIP.2022.3169687},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3309-3321},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Lossless white balance for improved lossless CFA image and video compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual alternating direction method of multipliers for inverse
imaging. <em>TIP</em>, <em>31</em>, 3295–3308. (<a
href="https://doi.org/10.1109/TIP.2022.3167915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse imaging covers a wide range of imaging applications, including super-resolution, deblurring, and compressive sensing. We propose a novel scheme to solve such problems by combining duality and the alternating direction method of multipliers (ADMM). In addition to a conventional ADMM process, we introduce a second one that solves the dual problem to find the estimated nontrivial lower bound of the objective function, and the related iteration results are used in turn to guide the primal iterations. We call this D-ADMM, and show that it converges to the global minimum when the regularization function is convex and the optimization problem has at least one optimizer. Furthermore, we show how the scheme can give rise to two specific algorithms, called D-ADMM-L2 and D-ADMM-TV, by having different regularization functions. We compare D-ADMM-TV with other methods on image super-resolution and demonstrate comparable or occasionally slightly better quality results. This paves the way of incorporating advanced operators and strategies designed for basic ADMM into the D-ADMM method as well to further improve the performances of those methods.},
  archive      = {J_TIP},
  author       = {Li Song and Zhou Ge and Edmund Y. Lam},
  doi          = {10.1109/TIP.2022.3167915},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3295-3308},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual alternating direction method of multipliers for inverse imaging},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unintentional action localization via counterfactual
examples. <em>TIP</em>, <em>31</em>, 3281–3294. (<a
href="https://doi.org/10.1109/TIP.2022.3166278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do humans localize unintentional action like “ A boy falls down while playing skateboard ”? Cognitive science shows that an 18-month-old baby understands the intention by observing the actions and comparing the feedback. Motivated by this evidence, we propose a causal inference approach that constructs a video pool containing intentional knowledge, conducts the counterfactual intervention to observe intentional action, and compares the unintentional action with intentional action to achieve localization. Specifically, we first build a video pool, where each video contains the same action content as an original unintentional action video. Then we conduct the counterfactual intervention to generate counterfactual examples. We further maximize the difference between the predictions of factual unintentional action and counterfactual intentional action to train the model. By disentangling the effects of different clues on the model prediction, we encourage the model to highlight the intention clue and alleviate the negative effect brought by the training bias of the action content clue. We evaluate our approach on a public unintentional action dataset and achieve consistent improvements on both unintentional action recognition and localization tasks.},
  archive      = {J_TIP},
  author       = {Jinglin Xu and Guangyi Chen and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TIP.2022.3166278},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3281-3294},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unintentional action localization via counterfactual examples},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). E2Style: Improve the efficiency and effectiveness of
StyleGAN inversion. <em>TIP</em>, <em>31</em>, 3267–3280. (<a
href="https://doi.org/10.1109/TIP.2022.3167305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of StyleGAN inversion, which plays an essential role in enabling the pretrained StyleGAN to be used for real image editing tasks. The goal of StyleGAN inversion is to find the exact latent code of the given image in the latent space of StyleGAN. This problem has a high demand for quality and efficiency. Existing optimization-based methods can produce high-quality results, but the optimization often takes a long time. On the contrary, forward-based methods are usually faster but the quality of their results is inferior. In this paper, we present a new feed-forward network “E2Style” for StyleGAN inversion, with significant improvement in terms of efficiency and effectiveness. In our inversion network, we introduce: 1) a shallower backbone with multiple efficient heads across scales; 2) multi-layer identity loss and multi-layer face parsing loss to the loss function; and 3) multi-stage refinement. Combining these designs together forms an effective and efficient method that exploits all benefits of optimization-based and forward-based methods. Quantitative and qualitative results show that our E2Style performs better than existing forward-based methods and comparably to state-of-the-art optimization-based methods while maintaining the high efficiency as well as forward-based methods. Moreover, a number of real image editing applications demonstrate the efficacy of our E2Style. Our code is available at https://github.com/wty-ustc/e2style},
  archive      = {J_TIP},
  author       = {Tianyi Wei and Dongdong Chen and Wenbo Zhou and Jing Liao and Weiming Zhang and Lu Yuan and Gang Hua and Nenghai Yu},
  doi          = {10.1109/TIP.2022.3167305},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3267-3280},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {E2Style: Improve the efficiency and effectiveness of StyleGAN inversion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised monocular depth estimation with multiscale
perception. <em>TIP</em>, <em>31</em>, 3251–3266. (<a
href="https://doi.org/10.1109/TIP.2022.3167307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting 3D information from a single optical image is very attractive. Recently emerging self-supervised methods can learn depth representations without using ground truth depth maps as training data by transforming the depth prediction task into an image synthesis task. However, existing methods rely on a differentiable bilinear sampler for image synthesis, which results in each pixel in a synthetic image being derived from only four pixels in the source image and causes each pixel in the depth map to perceive only a few pixels in the source image. In addition, when calculating the photometric error between a synthetic image and its corresponding target image, existing methods only consider the photometric error within a small neighborhood of each single pixel and therefore ignore correlations between larger areas, which causes the model to tend to fall into the local optima for small patches. In order to extend the perceptual area of the depth map over the source image, we propose a novel multi-scale method that downsamples the predicted depth map and performs image synthesis at different resolutions, which enables each pixel in the depth map to perceive more pixels in the source image and improves the performance of the model. As for the locality of photometric error, we propose a structural similarity (SSIM) pyramid loss to allow the model to sense the difference between images in multiple areas of different sizes. Experimental results show that our method achieves superior performance on both outdoor and indoor benchmarks.},
  archive      = {J_TIP},
  author       = {Yourun Zhang and Maoguo Gong and Jianzhao Li and Mingyang Zhang and Fenlong Jiang and Hongyu Zhao},
  doi          = {10.1109/TIP.2022.3167307},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3251-3266},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised monocular depth estimation with multiscale perception},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pro-UIGAN: Progressive face hallucination from occluded
thumbnails. <em>TIP</em>, <em>31</em>, 3236–3250. (<a
href="https://doi.org/10.1109/TIP.2022.3167280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the task of hallucinating an authentic high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed Pro-UIGAN, which exploits facial geometry priors to replenish and upsample ( $8\times $ ) the occluded and tiny faces ( $16\times 16$ pixels). Pro-UIGAN iteratively (1) estimates facial geometry priors for low-resolution (LR) faces and (2) acquires non-occluded HR face images under the guidance of the estimated priors. Our multi-stage hallucination network upsamples and inpaints occluded LR faces via a coarse-to-fine fashion, significantly reducing undesirable artifacts and blurriness. Specifically, we design a novel cross-modal attention module for facial priors estimation, in which an input face and its landmark features are formulated as queries and keys, respectively. Such a design encourages joint feature learning across the input facial and landmark features, and deep feature correspondences will be discovered by attention. Thus, facial appearance features and facial geometry priors are learned in a mutually beneficial manner. Extensive experiments show that our Pro-UIGAN attains visually pleasing completed HR faces, thus facilitating downstream tasks, i.e., face alignment, face parsing, face recognition as well as expression classification.},
  archive      = {J_TIP},
  author       = {Yang Zhang and Xin Yu and Xiaobo Lu and Ping Liu},
  doi          = {10.1109/TIP.2022.3167280},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3236-3250},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pro-UIGAN: Progressive face hallucination from occluded thumbnails},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-batch hard example mining with pseudo large batch for
ID vs. Spot face recognition. <em>TIP</em>, <em>31</em>, 3224–3235. (<a
href="https://doi.org/10.1109/TIP.2021.3137005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our daily life, a large number of activities require identity verification, e.g., ePassport gates. Most of those verification systems recognize who you are by matching the ID document photo (ID face) to your live face image (spot face). The ID vs. Spot (IvS) face recognition is different from general face recognition where each dataset usually contains a small number of subjects and sufficient images for each subject. In IvS face recognition, the datasets usually contain massive class numbers (million or more) while each class only has two image samples (one ID face and one spot face), which makes it very challenging to train an effective model (e.g., excessive demand on GPU memory if conducting the classification on such massive classes, hardly capture the effective features for bisample data of each identity, etc.). To avoid the excessive demand on GPU memory, a two-stage training method is developed, where we first train the model on the dataset in general face recognition (e.g., MS-Celeb-1M) and then employ the metric learning losses (e.g., triplet and quadruplet losses) to learn the features on IvS data with million classes. To extract more effective features for IvS face recognition, we propose two novel algorithms to enhance the network by selecting harder samples for training. Firstly, a Cross-Batch Hard Example Mining (CB-HEM) is proposed to select the hard triplets from not only the current mini-batch but also past dozens of mini-batches (for convenience, we use batch to denote a mini-batch in the following), which can significantly expand the space of sample selection. Secondly, a Pseudo Large Batch (PLB) is proposed to virtually increase the batch size with a fixed GPU memory. The proposed PLB and CB-HEM can be employed simultaneously to train the network, which dramatically expands the selecting space by hundreds of times, where the very hard sample pairs especially the hard negative pairs can be selected for training to enhance the discriminative capability. Extensive comparative evaluations conducted on multiple IvS benchmarks demonstrate the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Zichang Tan and Ajian Liu and Jun Wan and Hao Liu and Zhen Lei and Guodong Guo and Stan Z. Li},
  doi          = {10.1109/TIP.2021.3137005},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3224-3235},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-batch hard example mining with pseudo large batch for ID vs. spot face recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention guided global enhancement and local refinement
network for semantic segmentation. <em>TIP</em>, <em>31</em>, 3211–3223.
(<a href="https://doi.org/10.1109/TIP.2022.3166673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The encoder-decoder architecture is widely used as a lightweight semantic segmentation network. However, it struggles with a limited performance compared to a well-designed Dilated-FCN model for two major problems. First, commonly used upsampling methods in the decoder such as interpolation and deconvolution suffer from a local receptive field, unable to encode global contexts. Second, low-level features may bring noises to the network decoder through skip connections for the inadequacy of semantic concepts in early encoder layers. To tackle these challenges, a Global Enhancement Method is proposed to aggregate global information from high-level feature maps and adaptively distribute them to different decoder layers, alleviating the shortage of global contexts in the upsampling process. Besides, aLocal Refinement Module is developed by utilizing the decoder features as the semantic guidance to refine the noisy encoder features before the fusion of these two (the decoder features and the encoder features). Then, the two methods are integrated into a Context Fusion Block, and based on that, a novel Attention guided Global enhancement and Local refinement Network (AGLN) is elaborately designed. Extensive experiments on PASCAL Context, ADE20K, and PASCAL VOC 2012 datasets have demonstrated the effectiveness of the proposed approach. In particular, with a vanilla ResNet-101 backbone, AGLN achieves the state-of-the-art result (56.23\% mean IOU) on the PASCAL Context dataset. The code is available at https://github.com/zhasen1996/AGLN .},
  archive      = {J_TIP},
  author       = {Jiangyun Li and Sen Zha and Chen Chen and Meng Ding and Tianxiang Zhang and Hong Yu},
  doi          = {10.1109/TIP.2022.3166673},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3211-3223},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention guided global enhancement and local refinement network for semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CSR-net: Learning adaptive context structure representation
for robust feature correspondence. <em>TIP</em>, <em>31</em>, 3197–3210.
(<a href="https://doi.org/10.1109/TIP.2022.3166284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching, which refers to identifying and then corresponding the same or similar visual pattern from two or more images, is a key technique in any image processing task that requires establishing good correspondences between images. Given potential correspondences (matches) in two scenes, a novel whole-part deep learning framework, termed as Context Structure Representation Network (CSR-Net), is designed to infer the probabilities of arbitrary correspondences being inliers. Traditional approaches commonly build the local relation between correspondences by manually engineered criteria. Different from existing attempts, the main idea of our work is to learn explicitly neighborhood structure of each correspondence, allowing us to formulate the matching problem into a dynamic local structure consensus evaluation in an end-to-end fashion. For this purpose, we propose a permutation-invariant STructure Representation (STR) learning module, which can easily merge different types of networks into a unified architecture to deal with sparse matches directly. By the collaborative use of STR, we introduce a Context-Aware Attention (CAA) mechanism to adaptively re-calibrate structure features via a rotation-invariant context aware encoding and simple feature gating, thus arising the ability of fine-grained patterns recognition. Moreover, to further weaken the cost of establishing reliable correspondences, the CSR-Net is formulated as whole-part consensus learning, where the aim of whole level is compensating rigid transformations. In order to demonstrate our CSR-Net can effectively boost the baselines, we intensively experiment on image matching and other visual tasks. The results of the experiment confirm that the matching performances of CSR-Net have significantly improved over nine state-of-the-art competitors.},
  archive      = {J_TIP},
  author       = {Jiaxuan Chen and Shuang Chen and Xiaoxian Chen and Yuan Dai and Yang Yang},
  doi          = {10.1109/TIP.2022.3166284},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3197-3210},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CSR-net: Learning adaptive context structure representation for robust feature correspondence},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking the competition between detection and ReID in
multiobject tracking. <em>TIP</em>, <em>31</em>, 3182–3196. (<a
href="https://doi.org/10.1109/TIP.2022.3165376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to balanced accuracy and speed, one-shot models which jointly learn detection and identification embeddings, have drawn great attention in multi-object tracking (MOT). However, the inherent differences and relations between detection and re-identification (ReID) are unconsciously overlooked because of treating them as two isolated tasks in the one-shot tracking paradigm. This leads to inferior performance compared with existing two-stage methods. In this paper, we first dissect the reasoning process for these two tasks, which reveals that the competition between them inevitably would destroy task-dependent representations learning. To tackle this problem, we propose a novel reciprocal network (REN) with a self-relation and cross-relation design so that to impel each branch to better learn task-dependent representations. The proposed model aims to alleviate the deleterious tasks competition, meanwhile improve the cooperation between detection and ReID. Furthermore, we introduce a scale-aware attention network (SAAN) that prevents semantic level misalignment to improve the association capability of ID embeddings. By integrating the two delicately designed networks into a one-shot online MOT system, we construct a strong MOT tracker, namely CSTrack. Our tracker achieves the state-of-the-art performance on MOT16, MOT17 and MOT20 datasets, without other bells and whistles. Moreover, CSTrack is efficient and runs at 16.4 FPS on a single modern GPU, and its lightweight version even runs at 34.6 FPS. The complete code has been released at https://github.com/JudasDie/SOTS},
  archive      = {J_TIP},
  author       = {Chao Liang and Zhipeng Zhang and Xue Zhou and Bing Li and Shuyuan Zhu and Weiming Hu},
  doi          = {10.1109/TIP.2022.3165376},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3182-3196},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking the competition between detection and ReID in multiobject tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Three-dimensional cubic barcodes. <em>TIP</em>, <em>31</em>,
3166–3181. (<a href="https://doi.org/10.1109/TIP.2021.3120049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider three-dimensional cubic barcodes, consisting of smaller cubes, each built from one of two possible materials and carry one bit of information. To retrieve the information stored in the barcode, we measure a 2D projection of the barcode using a penetrating wave such as X-rays, either using parallel-beam or cone-beam scanners from an unknown direction. We derive a theoretical representation of this scanning process and show that for a known barcode pose with respect to the scanner, the projection operator is linear and can be easily inverted. Moreover, we provide a method to estimate the unknown pose of the barcode from a single 2D scan. We also propose coding schemes to correct errors and ambiguities in the reconstruction process. Finally, we test our designed barcode and reconstruction algorithms with several simulations, as well as a real-world barcode acquired with an X-ray cone-beam scanner, as a proof of concept.},
  archive      = {J_TIP},
  author       = {Golnoosh Elhami and Adam Scholefield and Martin Vetterli},
  doi          = {10.1109/TIP.2021.3120049},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3166-3181},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Three-dimensional cubic barcodes},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real image denoising with a locally-adaptive bitonic filter.
<em>TIP</em>, <em>31</em>, 3151–3165. (<a
href="https://doi.org/10.1109/TIP.2022.3164532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image noise removal is a common problem with many proposed solutions. The current standard is set by learning-based approaches, however these are not appropriate in all scenarios, perhaps due to lack of training data or the need for predictability in novel circumstances. The bitonic filter is a non-learning-based filter for removing noise from signals, with a mathematical morphology (ranking) framework in which the signal is postulated to be locally bitonic (having only one minimum or maximum) over some domain of finite extent. A novel version of this filter is developed in this paper, with a domain that is locally-adaptive to the signal, and other adjustments to allow application to real image sensor noise. These lead to significant improvements in noise reduction performance at no cost to processing times. The new bitonic filter performs better than the block-matching 3D filter for high levels of additive white Gaussian noise. It also surpasses this and other more recent non-learning-based filters for two public data sets containing real image noise at various levels. This is despite an additional adjustment to the block-matching filter, which leads to significantly better performance than has previously been cited on these data sets. The new bitonic filter has a signal-to-noise ratio 2.4dB lower than the best learning-based techniques when they are optimally trained. However, the performance gap is closed completely when these techniques are trained on data sets not directly related to the benchmark data. This demonstrates what can be achieved with a predictable, explainable, entirely local technique, which makes no assumptions of repeating patterns either within an image or across images, and hence creates residual images which are well behaved even in very high noise. Since the filter does not require training, it can still be used in situations where training is either difficult or inappropriate.},
  archive      = {J_TIP},
  author       = {Graham Treece},
  doi          = {10.1109/TIP.2022.3164532},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3151-3165},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Real image denoising with a locally-adaptive bitonic filter},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised structure-texture separation network for oracle
character recognition. <em>TIP</em>, <em>31</em>, 3137–3150. (<a
href="https://doi.org/10.1109/TIP.2022.3165989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oracle bone script is the earliest-known Chinese writing system of the Shang dynasty and is precious to archeology and philology. However, real-world scanned oracle data are rare and few experts are available for annotation which make the automatic recognition of scanned oracle characters become a challenging task. Therefore, we aim to explore unsupervised domain adaptation to transfer knowledge from handprinted oracle data, which are easy to acquire, to scanned domain. We propose a structure-texture separation network (STSN), which is an end-to-end learning framework for joint disentanglement, transformation, adaptation and recognition. First, STSN disentangles features into structure (glyph) and texture (noise) components by generative models, and then aligns handprinted and scanned data in structure feature space such that the negative influence caused by serious noises can be avoided when adapting. Second, transformation is achieved via swapping the learned textures across domains and a classifier for final classification is trained to predict the labels of the transformed scanned characters. This not only guarantees the absolute separation, but also enhances the discriminative ability of the learned features. Extensive experiments on Oracle-241 dataset show that STSN outperforms other adaptation methods and successfully improves recognition performance on scanned data even when they are contaminated by long burial and careless excavation.},
  archive      = {J_TIP},
  author       = {Mei Wang and Weihong Deng and Cheng-Lin Liu},
  doi          = {10.1109/TIP.2022.3165989},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3137-3150},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Unsupervised structure-texture separation network for oracle character recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EDN: Salient object detection via extremely-downsampled
network. <em>TIP</em>, <em>31</em>, 3125–3136. (<a
href="https://doi.org/10.1109/TIP.2022.3164550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress on salient object detection (SOD) mainly benefits from multi-scale learning, where the high-level and low-level features collaborate in locating salient objects and discovering fine details, respectively. However, most efforts are devoted to low-level feature learning by fusing multi-scale features or enhancing boundary representations. High-level features, which although have long proven effective for many other tasks, yet have been barely studied for SOD. In this paper, we tap into this gap and show that enhancing high-level features is essential for SOD as well. To this end, we introduce an Extremely-Downsampled Network (EDN), which employs an extreme downsampling technique to effectively learn a global view of the whole image, leading to accurate salient object localization. To accomplish better multi-level feature fusion, we construct the Scale-Correlated Pyramid Convolution (SCPC) to build an elegant decoder for recovering object details from the above extreme downsampling. Extensive experiments demonstrate that EDN achieves state-of-the-art performance with real-time speed. Our efficient EDN-Lite also achieves competitive performance with a speed of 316fps. Hence, this work is expected to spark some new thinking in SOD. Code is available at https://github.com/yuhuan-wu/EDN .},
  archive      = {J_TIP},
  author       = {Yu-Huan Wu and Yun Liu and Le Zhang and Ming-Ming Cheng and Bo Ren},
  doi          = {10.1109/TIP.2022.3164550},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3125-3136},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {EDN: Salient object detection via extremely-downsampled network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised visual saliency prediction. <em>TIP</em>,
<em>31</em>, 3111–3124. (<a
href="https://doi.org/10.1109/TIP.2022.3158064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of current deep saliency models heavily depends on large amounts of annotated human fixation data to fit the highly non-linear mapping between the stimuli and visual saliency. Such fully supervised data-driven approaches are annotation-intensive and often fail to consider the underlying mechanisms of visual attention. In contrast, in this paper, we introduce a model based on various cognitive theories of visual saliency, which learns visual attention patterns in a weakly supervised manner. Our approach incorporates insights from cognitive science as differentiable submodules, resulting in a unified, end-to-end trainable framework. Specifically, our model encapsulates the following important components motivated from biological vision. (a) As scene semantics are closely related to visually attentive regions, our model encodes discriminative spatial information for scene understanding through spatial visual semantics embedding . (b) To model the objectness factors in visual attention deployment, we incorporate object-level semantics embedding and object relation information. (c) Considering the “winner-take-all” mechanism in visual stimuli processing, we model the competition mechanism among objects with softmax based neural attention. (d) Lastly, a conditional center prior is learned to mimic the spatial distribution bias of visual attention. Furthermore, we propose novel loss functions to utilize supervision cues from image-level semantics , saliency prior knowledge , and self-information compression . Experiments show that our method achieves promising results, and even outperforms many of its fully supervised counterparts. Overall, our weakly supervised saliency method makes an essential step towards reducing the annotation budget of current approaches, as well as providing a more comprehensive understanding of the visual attention mechanism. Our code is available at: https://github.com/ashleylqx/WeakFixation.git .},
  archive      = {J_TIP},
  author       = {Qiuxia Lai and Tianfei Zhou and Salman Khan and Hanqiu Sun and Jianbing Shen and Ling Shao},
  doi          = {10.1109/TIP.2022.3158064},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3111-3124},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly supervised visual saliency prediction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep hierarchical vision transformer for hyperspectral and
LiDAR data classification. <em>TIP</em>, <em>31</em>, 3095–3110. (<a
href="https://doi.org/10.1109/TIP.2022.3162964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we develop a novel deep hierarchical vision transformer (DHViT) architecture for hyperspectral and light detection and ranging (LiDAR) data joint classification. Current classification methods have limitations in heterogeneous feature representation and information fusion of multi-modality remote sensing data (e.g., hyperspectral and LiDAR data), these shortcomings restrict the collaborative classification accuracy of remote sensing data. The proposed deep hierarchical vision transformer architecture utilizes both the powerful modeling capability of long-range dependencies and strong generalization ability across different domains of the transformer network, which is based exclusively on the self-attention mechanism. Specifically, the spectral sequence transformer is exploited to handle the long-range dependencies along the spectral dimension from hyperspectral images, because all diagnostic spectral bands contribute to the land cover classification. Thereafter, we utilize the spatial hierarchical transformer structure to extract hierarchical spatial features from hyperspectral and LiDAR data, which are also crucial for classification. Furthermore, the cross attention (CA) feature fusion pattern could adaptively and dynamically fuse heterogeneous features from multi-modality data, and this contextual aware fusion mode further improves the collaborative classification performance. Comparative experiments and ablation studies are conducted on three benchmark hyperspectral and LiDAR datasets, and the DHViT model could yield an average overall classification accuracy of 99.58\%, 99.55\%, and 96.40\% on three datasets, respectively, which sufficiently certify the effectiveness and superior performance of the proposed method.},
  archive      = {J_TIP},
  author       = {Zhixiang Xue and Xiong Tan and Xuchu Yu and Bing Liu and Anzhu Yu and Pengqiang Zhang},
  doi          = {10.1109/TIP.2022.3162964},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3095-3110},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep hierarchical vision transformer for hyperspectral and LiDAR data classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic temporal modeling for unintentional action
localization. <em>TIP</em>, <em>31</em>, 3081–3094. (<a
href="https://doi.org/10.1109/TIP.2022.3163544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have the inherent advantage of understanding action intention, while it is an enormous challenge to train the machine to localize unintentional action in videos due to the lack of reliable annotations for stable training. The annotations of unintentional action are unreliable since different annotators are affected by their subjective appraisals and intrinsic ambiguity, which brings heavy difficulties for the training. To address this issue, we propose a probabilistic framework for unintentional action localization by modeling the uncertainty of annotations. Our framework consists of two main components, including Temporal Label Aggregation (TLA) and Dense Probabilistic Localization (DPL). We first formulate each annotated failure moment as a temporal label distribution. Then we propose a TLA component to aggregate temporal label distributions of different failure moments in an online manner and generate dense probabilistic supervision. Based on TLA, We further develop a DPL component to jointly train three heads (i.e., probabilistic dense classification, probabilistic temporal detection, and probabilistic regression) with different supervision granularities and make them highly collaborative. We evaluate our approach on the largest unintentional action dataset OOPS and demonstrate that our approach can achieve significant improvement over the baseline and state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jinglin Xu and Guangyi Chen and Nuoxing Zhou and Wei-Shi Zheng and Jiwen Lu},
  doi          = {10.1109/TIP.2022.3163544},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3081-3094},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Probabilistic temporal modeling for unintentional action localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A no-reference stereoscopic image quality assessment network
based on binocular interaction and fusion mechanisms. <em>TIP</em>,
<em>31</em>, 3066–3080. (<a
href="https://doi.org/10.1109/TIP.2022.3164537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary society full of stereoscopic images, how to assess visual quality of 3D images has attracted an increasing attention in field of Stereoscopic Image Quality Assessment (SIQA). Compared with 2D-IQA, SIQA is more challenging because some complicated features of Human Visual System (HVS), such as binocular interaction and binocular fusion, must be considered. In this paper, considering both binocular interaction and fusion mechanisms of the HVS, a hierarchical no-reference stereoscopic image quality assessment network (StereoIF-Net) is proposed to simulate the whole quality perception of 3D visual signals in human cortex, including two key modules: BIM and BFM. In particular, Binocular Interaction Modules (BIMs) are constructed to simulate binocular interaction in V2-V5 visual cortex regions, in which a novel cross convolution is designed to explore the interaction details in each region. In the BIMs, different output channel numbers are designed to imitate various receptive fields in V2-V5. Furthermore, a Binocular Fusion Module (BFM) with automatic learned weights is proposed to model binocular fusion of the HVS in higher cortex layers. The verification experiments are conducted on the LIVE 3D, IVC and Waterloo-IVC SIQA databases and three indices including PLCC, SROCC and RMSE are employed to evaluate the assessment consistency between StereoIF-Net and the HVS. The proposed StereoIF-Net achieves almost the best results compared with advanced SIQA methods. Specifically, the metric values on LIVE 3D, IVC and WIVC-I are the best, and are the second-best on the WIVC-II.},
  archive      = {J_TIP},
  author       = {Jianwei Si and Baoxiang Huang and Huan Yang and Weisi Lin and Zhenkuan Pan},
  doi          = {10.1109/TIP.2022.3164537},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3066-3080},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A no-reference stereoscopic image quality assessment network based on binocular interaction and fusion mechanisms},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exemplar-based, semantic guided zero-shot visual
recognition. <em>TIP</em>, <em>31</em>, 3056–3065. (<a
href="https://doi.org/10.1109/TIP.2021.3120319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot recognition has been a hot topic in recent years. Since no direct supervision is available, researchers use semantic information as the bridge instead. However, most zero-shot recognition methods jointly model images on the class level without considering the distinctive character of each image. To solve this problem, in this paper, we propose a novel exemplar-based, semantic guided zero-shot recognition method (EBSG). Both visual and semantic information of each image is used. We train visual sub-model to separate each image from the other images of different classes. We also train semantic sub-model to separate this image from the other images described with different semantics. We concatenate the outputs of visual and semantic sub-models to represent images. Image classification model is then learned by measuring visual similarity and semantic consistency of both source and target images. We conduct zero-shot recognition experiments on four widely used datasets. Experimental results show the effectiveness of the proposed EBSG method.},
  archive      = {J_TIP},
  author       = {Chunjie Zhang and Chao Liang and Yao Zhao},
  doi          = {10.1109/TIP.2021.3120319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3056-3065},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exemplar-based, semantic guided zero-shot visual recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MC-GCN: A multi-scale contrastive graph convolutional
network for unconstrained face recognition with image sets.
<em>TIP</em>, <em>31</em>, 3046–3055. (<a
href="https://doi.org/10.1109/TIP.2022.3163851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Multi-scale Contrastive Graph Convolutional Network (MC-GCN) method is proposed for unconstrained face recognition with image sets, which takes a set of media (orderless images and videos) as a face subject instead of single media (an image or video). Due to factors such as illumination, posture, media source, etc., there are huge intra-set variances in a face set, and the importance of different face prototypes varies considerably. How to model the attention mechanism according to the relationship between prototypes or images in a set is the main content of this paper. In this work, we formulate a framework based on graph convolutional network (GCN), which considers face prototypes as nodes to build relations. Specifically, we first present a multi-scale graph module to learn the relationship between prototypes at multiple scales. Moreover, a Contrastive Graph Convolutional (CGC) block is introduced to build attention control model, which focuses on those frames with similar prototypes (contrastive information) between pair of sets instead of simply evaluating the frame quality. The experiments on IJB-A, YouTube Face, and an animal face dataset clearly demonstrate that our proposed MC-GCN outperforms the state-of-the-art methods significantly.},
  archive      = {J_TIP},
  author       = {Xiao Shi and Xiujuan Chai and Jiake Xie and Tan Sun},
  doi          = {10.1109/TIP.2022.3163851},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3046-3055},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MC-GCN: A multi-scale contrastive graph convolutional network for unconstrained face recognition with image sets},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QA-filter: A QP-adaptive convolutional neural network filter
for video coding. <em>TIP</em>, <em>31</em>, 3032–3045. (<a
href="https://doi.org/10.1109/TIP.2022.3152627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN)-based filters have achieved great success in video coding. However, in most previous works, individual models were needed for each quantization parameter (QP) band, which is impractical due to limited storage resources. To explore this, our work consists of two parts. First, we propose a frequency and spatial QP-adaptive mechanism (FSQAM), which can be directly applied to the (vanilla) convolution to help any CNN filter handle different quantization noise. From the frequency domain, a FQAM that introduces the quantization step (Qstep) into the convolution is proposed. When the quantization noise increases, the ability of the CNN filter to suppress noise improves. Moreover, SQAM is further designed to compensate for the FQAM from the spatial domain. Second, based on FSQAM, a QP-adaptive CNN filter called QA-Filter that can be used under a wide range of QP is proposed. By factorizing the mixed features to high-frequency and low-frequency parts with the pair of pooling and upsampling operations, the QA-Filter and FQAM can promote each other to obtain better performance. Compared to the H.266/VVC baseline, average 5.25\% and 3.84\% BD-rate reductions for luma are achieved by QA-Filter with default all-intra (AI) and random-access (RA) configurations, respectively. Additionally, an up to 9.16\% BD-rate reduction is achieved on the luma of sequence BasketballDrill . Besides, FSQAM achieves measurably better BD-rate performance compared with the previous QP map method.},
  archive      = {J_TIP},
  author       = {Chao Liu and Heming Sun and Jiro Katto and Xiaoyang Zeng and Yibo Fan},
  doi          = {10.1109/TIP.2022.3152627},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3032-3045},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {QA-filter: A QP-adaptive convolutional neural network filter for video coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relational reasoning over spatial-temporal graphs for video
summarization. <em>TIP</em>, <em>31</em>, 3017–3031. (<a
href="https://doi.org/10.1109/TIP.2022.3163855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a dynamic graph modeling approach to learn spatial-temporal representations for video summarization. Most existing video summarization methods extract image-level features with ImageNet pre-trained deep models. Differently, our method exploits object-level and relation-level information to capture spatial-temporal dependencies. Specifically, our method builds spatial graphs on the detected object proposals. Then, we construct a temporal graph by using the aggregated representations of spatial graphs. Afterward, we perform relational reasoning over spatial and temporal graphs with graph convolutional networks and extract spatial-temporal representations for importance score prediction and key shot selection. To eliminate relation clutters caused by densely connected nodes, we further design a self-attention edge pooling module, which disregards meaningless relations of graphs. We conduct extensive experiments on two popular benchmarks, including the SumMe and TVSum datasets. Experimental results demonstrate that the proposed method achieves superior performance against state-of-the-art video summarization methods.},
  archive      = {J_TIP},
  author       = {Wencheng Zhu and Yucheng Han and Jiwen Lu and Jie Zhou},
  doi          = {10.1109/TIP.2022.3163855},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3017-3031},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Relational reasoning over spatial-temporal graphs for video summarization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised multi-category counting networks for
automatic check-out. <em>TIP</em>, <em>31</em>, 3004–3016. (<a
href="https://doi.org/10.1109/TIP.2022.3163527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practical task of Automatic Check-Out (ACO) is to accurately predict the presence and count of each product in an arbitrary product combination. Beyond the large-scale and the fine-grained nature of product categories as its main challenges, products are always continuously updated in realistic check-out scenarios, which is also required to be solved in an ACO system. Previous work in this research line almost depends on the supervisions of labor-intensive bounding boxes of products by performing a detection paradigm. While, in this paper, we propose a Self-Supervised Multi-Category Counting (S2MC2) network to leverage the point-level supervisions of products in check-out images to both lower the labeling cost and be able to return ACO predictions in a class incremental setting. Specifically, as a backbone, our S2MC2 is built upon a counting module in a class-agnostic counting fashion. Also, it consists of several crucial components including an attention module for capturing fine-grained patterns and a domain adaptation module for reducing the domain gap between single product images as training and check-out images as test. Furthermore, a self-supervised approach is utilized in S2MC2 to initialize the parameters of its backbone for better performance. By conducting comprehensive experiments on the large-scale automatic check-out dataset RPC, we demonstrate that our proposed S2MC2 achieves superior accuracy in both traditional and incremental settings of ACO tasks over the competing baselines.},
  archive      = {J_TIP},
  author       = {Hao Chen and Yangzhun Zhou and Jun Li and Xiu-Shen Wei and Liang Xiao},
  doi          = {10.1109/TIP.2022.3163527},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {3004-3016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised multi-category counting networks for automatic check-out},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local semantic correlation modeling over graph neural
networks for deep feature embedding and image retrieval. <em>TIP</em>,
<em>31</em>, 2988–3003. (<a
href="https://doi.org/10.1109/TIP.2022.3163571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep feature embedding aims to learn discriminative features or feature embeddings for image samples which can minimize their intra-class distance while maximizing their inter-class distance. Recent state-of-the-art methods have been focusing on learning deep neural networks with carefully designed loss functions. In this work, we propose to explore a new approach to deep feature embedding. We learn a graph neural network to characterize and predict the local correlation structure of images in the feature space. Based on this correlation structure, neighboring images collaborate with each other to generate and refine their embedded features based on local linear combination. Graph edges learn a correlation prediction network to predict the correlation scores between neighboring images. Graph nodes learn a feature embedding network to generate the embedded feature for a given image based on a weighted summation of neighboring image features with the correlation scores as weights. Our extensive experimental results under the image retrieval settings demonstrate that our proposed method outperforms the state-of-the-art methods by a large margin, especially for top-1 recalls.},
  archive      = {J_TIP},
  author       = {Shichao Kan and Yigang Cen and Yang Li and Mladenovic Vladimir and Zhihai He},
  doi          = {10.1109/TIP.2022.3163571},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2988-3003},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local semantic correlation modeling over graph neural networks for deep feature embedding and image retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Asynchronous spatio-temporal memory network for continuous
event-based object detection. <em>TIP</em>, <em>31</em>, 2975–2987. (<a
href="https://doi.org/10.1109/TIP.2022.3162962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras, offering extremely high temporal resolution and high dynamic range, have brought a new perspective to addressing common object detection challenges (e.g., motion blur and low light). However, how to learn a better spatio-temporal representation and exploit rich temporal cues from asynchronous events for object detection still remains an open issue. To address this problem, we propose a novel asynchronous spatio-temporal memory network (ASTMNet) that directly consumes asynchronous events instead of event images prior to processing, which can well detect objects in a continuous manner. Technically, ASTMNet learns an asynchronous attention embedding from the continuous event stream by adopting an adaptive temporal sampling strategy and a temporal attention convolutional module. Besides, a spatio-temporal memory module is designed to exploit rich temporal cues via a lightweight yet efficient inter-weaved recurrent-convolutional architecture. Empirically, it shows that our approach outperforms the state-of-the-art methods using the feed-forward frame-based detectors on three datasets by a large margin (i.e., 7.6\% in the KITTI Simulated Dataset, 10.8\% in the Gen1 Automotive Dataset, and 10.5\% in the 1Mpx Detection Dataset). The results demonstrate that event cameras can perform robust object detection even in cases where conventional cameras fail, e.g., fast motion and challenging light conditions.},
  archive      = {J_TIP},
  author       = {Jianing Li and Jia Li and Lin Zhu and Xijie Xiang and Tiejun Huang and Yonghong Tian},
  doi          = {10.1109/TIP.2022.3162962},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2975-2987},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Asynchronous spatio-temporal memory network for continuous event-based object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). CrabNet: Fully task-specific feature learning for one-stage
object detection. <em>TIP</em>, <em>31</em>, 2962–2974. (<a
href="https://doi.org/10.1109/TIP.2022.3162099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is usually solved by learning a deep architecture involving classification and localization tasks, where feature learning for these two tasks is shared using the same backbone model. Recent works have shown that suitable disentanglement of classification and localization tasks has the great potential to improve performance of object detection. Despite the promising performance, existing feature disentanglement methods usually suffer from two limitations. First, most of them only focus on the disentangled proposals or predication heads for classification and localization tasks after RPN. While little consideration has been given to that the features for these two different tasks actually are obtained by a shared backbone model before RPN. Second, they are suggested for two-stage objectors and are not applicable to one-stage methods. To overcome these limitations, this paper presents a novel fully task-specific feature learning method for one-stage object detection. Specifically, our method first learns disentangled features for classification and localization tasks using two separated backbone models, where auxiliary classification and localization heads are inserted at the end of the two backbone models for providing a fully task-specific features for classification and localization. Then, a feature interaction module is developed for aligning and fusing task-specific features, which are further used to produce the final detection result. Experiments on MS COCO show that our proposed method (dubbed CrabNet) can achieve clear improvement over counterparts with increasing limited inference time, while performing favorably against state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Hao Wang and Qilong Wang and Hongzhi Zhang and Qinghua Hu and Wangmeng Zuo},
  doi          = {10.1109/TIP.2022.3162099},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2962-2974},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CrabNet: Fully task-specific feature learning for one-stage object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task interaction learning for spatiospectral image
super-resolution. <em>TIP</em>, <em>31</em>, 2950–2961. (<a
href="https://doi.org/10.1109/TIP.2022.3161834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High spatial resolution and high spectral resolution images (HR-HSIs) are widely applied in geosciences, medical diagnosis, and beyond. However, how to get images with both high spatial resolution and high spectral resolution is still a problem to be solved. In this paper, we present a deep spatial-spectral feature interaction network (SSFIN) for reconstructing an HR-HSI from a low-resolution multispectral image (LR-MSI), e.g. , RGB image. In particular, we introduce two auxiliary tasks, i.e. , spatial super-resolution (SR) and spectral SR to help the network recover the HR-HSI better. Since higher spatial resolution can provide more detailed information about image texture and structure, and richer spectrum can provide more attribute information, we propose a spatial-spectral feature interaction block (SSFIB) to make the spatial SR task and the spectral SR task benefit each other. Therefore, we can make full use of the rich spatial and spectral information extracted from the spatial SR task and spectral SR task, respectively. Moreover, we use a weight decay strategy (for the spatial and spectral SR tasks) to train the SSFIN, so that the model can gradually shift attention from the auxiliary tasks to the primary task. Both quantitative and visual results on three widely used HSI datasets demonstrate that the proposed method achieves a considerable gain compared to other state-of-the-art methods. Source code is available at https://github.com/junjun-jiang/SSFIN .},
  archive      = {J_TIP},
  author       = {Qing Ma and Junjun Jiang and Xianming Liu and Jiayi Ma},
  doi          = {10.1109/TIP.2022.3161834},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2950-2961},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-task interaction learning for spatiospectral image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universal background subtraction based on arithmetic
distribution neural network. <em>TIP</em>, <em>31</em>, 2934–2949. (<a
href="https://doi.org/10.1109/TIP.2022.3162961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a universal background subtraction framework based on the Arithmetic Distribution Neural Network (ADNN) for learning the distributions of temporal pixels. In our ADNN model, the arithmetic distribution operations are utilized to introduce the arithmetic distribution layers, including the product distribution layer and the sum distribution layer. Furthermore, in order to improve the accuracy of the proposed approach, an improved Bayesian refinement model based on neighboring information, with a GPU implementation, is incorporated. In the forward pass and backpropagation of the proposed arithmetic distribution layers, histograms are considered as probability density functions rather than matrices. Thus, the proposed approach is able to utilize the probability information of the histogram and achieve promising results with a very simple architecture compared to traditional convolutional neural networks. Evaluations using standard benchmarks demonstrate the superiority of the proposed approach compared to state-of-the-art traditional and deep learning methods. To the best of our knowledge, this is the first method to propose network layers based on arithmetic distribution operations for learning distributions during background subtraction.},
  archive      = {J_TIP},
  author       = {Chenqiu Zhao and Kangkang Hu and Anup Basu},
  doi          = {10.1109/TIP.2022.3162961},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2934-2949},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Universal background subtraction based on arithmetic distribution neural network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive glass segmentation. <em>TIP</em>, <em>31</em>,
2920–2933. (<a href="https://doi.org/10.1109/TIP.2022.3162709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glass is very common in the real world. Influenced by the uncertainty about the glass region and the varying complex scenes behind the glass, the existence of glass poses severe challenges to many computer vision tasks, making glass segmentation as an important computer vision task. Glass does not have its own visual appearances but only transmit/reflect the appearances of its surroundings, making it fundamentally different from other common objects. To address such a challenging task, existing methods typically explore and combine useful cues from different levels of features in the deep network. As there exists a characteristic gap between level-different features, i.e. , deep layer features embed more high-level semantics and are better at locating the target objects while shallow layer features have larger spatial sizes and keep richer and more detailed low-level information, fusing these features naively thus would lead to a sub-optimal solution. In this paper, we approach the effective features fusion towards accurate glass segmentation in two steps. First, we attempt to bridge the characteristic gap between different levels of features by developing a Discriminability Enhancement (DE) module which enables level-specific features to be a more discriminative representation, alleviating the features incompatibility for fusion. Second, we design a Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful information in the fusion process by highlighting the common and exploring the difference between level-different features. Combining these two steps, we construct a P rogressive G lass S egmentation Net work ( PGSNet ) which uses multiple DE and FEBF modules to progressively aggregate features from high-level to low-level, implementing a coarse-to-fine glass segmentation. In addition, we build the first home-scene-oriented glass segmentation dataset for advancing household robot applications and in-depth research on this topic. Extensive experiments demonstrate that our method outperforms 26 cutting-edge models on three challenging datasets under four standard metrics. The code and dataset will be made publicly available.},
  archive      = {J_TIP},
  author       = {Letian Yu and Haiyang Mei and Wen Dong and Ziqi Wei and Li Zhu and Yuxin Wang and Xin Yang},
  doi          = {10.1109/TIP.2022.3162709},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2920-2933},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive glass segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Chasing the tail in monocular 3D human reconstruction with
prototype memory. <em>TIP</em>, <em>31</em>, 2907–2919. (<a
href="https://doi.org/10.1109/TIP.2022.3154606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved remarkable progress in single-image 3D human reconstruction. However, existing methods still fall short in predicting rare poses. The reason is that most of the current models perform regression based on a single human prototype, which is similar to common poses while far from the rare poses. In this work, we 1) identify and analyze this learning obstacle and 2) propose a prototype memory-augmented network, PM-Net, that effectively improves performances of predicting rare poses. The core of our framework is a memory module that learns and stores a set of 3D human prototypes capturing local distributions for either common poses or rare poses. With this formulation, the regression starts from a better initialization, which is relatively easier to converge. Extensive experiments on several widely employed datasets demonstrate the proposed framework’s effectiveness compared to other state-of-the-art methods. Notably, our approach significantly improves the models’ performances on rare poses while generating comparable results on other samples.},
  archive      = {J_TIP},
  author       = {Yu Rong and Ziwei Liu and Chen Change Loy},
  doi          = {10.1109/TIP.2022.3154606},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2907-2919},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Chasing the tail in monocular 3D human reconstruction with prototype memory},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explored normalized cut with random walk refining term for
image segmentation. <em>TIP</em>, <em>31</em>, 2893–2906. (<a
href="https://doi.org/10.1109/TIP.2022.3162475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Normalized Cut (NCut) model is a popular graph-based model for image segmentation. But it suffers from the excessive normalization problem and weakens the small object and twig segmentation. In this paper, we propose an Explored Normalized Cut (ENCut) model that establishes a balance graph model by adopting a meaningful-loop and a k-step random walk, which reduces the energy of small salient region, so as to enhance the small object segmentation. To improve the twig segmentation, our ENCut model is further enhanced by a new Random Walk Refining Term (RWRT) that adds local attention to our model with the help of an un-supervising random walk. Finally, a move-making based strategy is developed to efficiently solve the ENCut model with RWRT. Experiments on three standard datasets indicate that our model can achieve state-of-the-art results among the NCut-based segmentation models.},
  archive      = {J_TIP},
  author       = {Lei Zhu and Xuejing Kang and Lizhu Ye and Anlong Ming},
  doi          = {10.1109/TIP.2022.3162475},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2893-2906},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Explored normalized cut with random walk refining term for image segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Birds of a feather flock together: Category-divergence
guidance for domain adaptive segmentation. <em>TIP</em>, <em>31</em>,
2878–2892. (<a href="https://doi.org/10.1109/TIP.2022.3162471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. Present UDA models focus on alleviating the domain shift by minimizing the feature discrepancy between the source domain and the target domain but usually ignore the class confusion problem. In this work, we propose an Inter-class Separation and Intra-class Aggregation (ISIA) mechanism. It encourages the cross-domain representative consistency between the same categories and differentiation among diverse categories. In this way, the features belonging to the same categories are aligned together and the confusable categories are separated. By measuring the align complexity of each category, we design an Adaptive-weighted Instance Matching (AIM) strategy to further optimize the instance-level adaptation. Based on our proposed methods, we also raise a hierarchical unsupervised domain adaptation framework for cross-domain semantic segmentation task. Through performing the image-level, feature-level, category-level and instance-level alignment, our method achieves a stronger generalization performance of the model from the source domain to the target domain. In two typical cross-domain semantic segmentation tasks, i.e., GTA $5\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $ Cityscapes, our method achieves the state-of-the-art segmentation accuracy. We also build two cross-domain semantic segmentation datasets based on the publicly available data, i.e., remote sensing building segmentation and road segmentation, for domain adaptive segmentation. Our code, models and datasets are available at https://github.com/HibiscusYB/BAFFT .},
  archive      = {J_TIP},
  author       = {Bo Yuan and Danpei Zhao and Shuai Shao and Zehuan Yuan and Changhu Wang},
  doi          = {10.1109/TIP.2022.3162471},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2878-2892},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Birds of a feather flock together: Category-divergence guidance for domain adaptive segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CM-net: Concentric mask based arbitrary-shaped text
detection. <em>TIP</em>, <em>31</em>, 2864–2877. (<a
href="https://doi.org/10.1109/TIP.2022.3141844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently fast arbitrary-shaped text detection has become an attractive research topic. However, most existing methods are non-real-time, which may fall short in intelligent systems. Although a few real-time text methods are proposed, the detection accuracy is far behind non-real-time methods. To improve the detection accuracy and speed simultaneously, we propose a novel fast and accurate text detection framework, namely CM-Net, which is constructed based on a new text representation method and a multi-perspective feature (MPF) module. The former can fit arbitrary-shaped text contours by concentric mask (CM) in an efficient and robust way. The latter encourages the network to learn more CM-related discriminative features from multiple perspectives and brings no extra computational cost. Benefiting the advantages of CM and MPF, the proposed CM-Net only needs to predict one CM of the text instance to rebuild the text contour and achieves the best balance between detection accuracy and speed compared with previous works. Moreover, to ensure that multi-perspective features are effectively learned, the multi-factor constraints loss is proposed. Extensive experiments demonstrate the proposed CM is efficient and robust to fit arbitrary-shaped text instances, and also validate the effectiveness of MPF and constraints loss for discriminative text features recognition. Furthermore, experimental results show that the proposed CM-Net is superior to existing state-of-the-art (SOTA) real-time text detection methods in both detection speed and accuracy on MSRA-TD500, CTW1500, Total-Text, and ICDAR2015 datasets.},
  archive      = {J_TIP},
  author       = {Chuang Yang and Mulin Chen and Zhitong Xiong and Yuan Yuan and Qi Wang},
  doi          = {10.1109/TIP.2022.3141844},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2864-2877},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CM-net: Concentric mask based arbitrary-shaped text detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient sampling-based attention network for semantic
segmentation. <em>TIP</em>, <em>31</em>, 2850–2863. (<a
href="https://doi.org/10.1109/TIP.2022.3162101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention is widely explored to model long-range dependencies in semantic segmentation. However, this operation computes pair-wise relationships between the query point and all other points, leading to prohibitive complexity. In this paper, we propose an efficient Sampling-based Attention Network which combines a novel sample method with an attention mechanism for semantic segmentation. Specifically, we design a Stochastic Sampling-based Attention Module (SSAM) to capture the relationships between the query point and a stochastic sampled representative subset from a global perspective, where the sampled subset is selected by a Stochastic Sampling Module. Compared to self-attention, our SSAM achieves comparable segmentation performance while significantly reducing computational redundancy. In addition, with the observation that not all pixels are interested in the contextual information, we design a Deterministic Sampling-based Attention Module (DSAM) to sample features from a local region for obtaining the detailed information. Extensive experiments demonstrate that our proposed method can compete or perform favorably against the state-of-the-art methods on the Cityscapes, ADE20K, COCO Stuff, and PASCAL Context datasets.},
  archive      = {J_TIP},
  author       = {Xingjian He and Jing Liu and Weining Wang and Hanqing Lu},
  doi          = {10.1109/TIP.2022.3162101},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2850-2863},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An efficient sampling-based attention network for semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object-agnostic transformers for video referring
segmentation. <em>TIP</em>, <em>31</em>, 2839–2849. (<a
href="https://doi.org/10.1109/TIP.2022.3161832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video referring segmentation focuses on segmenting out the object in a video based on the corresponding textual description. Previous works have primarily tackled this task by devising two crucial parts, an intra-modal module for context modeling and an inter-modal module for heterogeneous alignment. However, there are two essential drawbacks of this approach: (1) it lacks joint learning of context modeling and heterogeneous alignment, leading to insufficient interactions among input elements; (2) both modules require task-specific expert knowledge to design, which severely limits the flexibility and generality of prior methods. To address these problems, we here propose a novel Object-Agnostic Transformer-based Network, called OATNet, that simultaneously conducts intra-modal and inter-modal learning for video referring segmentation, without the aid of object detection or category-specific pixel labeling. More specifically, we first directly feed the sequence of textual tokens and visual tokens (pixels rather than detected object bounding boxes) into a multi-modal encoder, where context and alignment are simultaneously and effectively explored. We then design a novel cascade segmentation network to decouple our task into coarse-grained segmentation and fine-grained refinement. Moreover, considering the difficulty of samples, a more balanced metric is provided to better diagnose the performance of the proposed method. Extensive experiments on two popular datasets, A2D Sentences and J-HMDB Sentences, demonstrate that our proposed approach noticeably outperforms state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Xu Yang and Hao Wang and De Xie and Cheng Deng and Dacheng Tao},
  doi          = {10.1109/TIP.2022.3161832},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2839-2849},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Object-agnostic transformers for video referring segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint local and nonlocal progressive prediction for
versatile video coding. <em>TIP</em>, <em>31</em>, 2824–2838. (<a
href="https://doi.org/10.1109/TIP.2022.3161831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the latest video coding standard, namely Versatile Video Coding (VVC), more directional intra modes and reference lines have been utilized to improve prediction efficiency. However, complex content still cannot be predicted well with only the adjacent reference samples. Although nonlocal prediction has been proposed to further improve the prediction efficiency in existing algorithms, explicit signalling or matching error potentially limits the coding efficiency. To address these issues, we propose a joint local and nonlocal progressive prediction scheme, targeting at improving nonlocal prediction accuracy without additional signalling. Specifically, template matching based prediction (TMP) is conducted firstly to derive an initial nonlocal predictor. Based on the first prediction and previously decoded reconstruction information, a local template, including inner textures and neighboring reconstruction, is carefully designed. With the local template involved in nonlocal matching process, a more accurate nonlocal predictor can be found progressively in the second prediction. Finally, the coefficients from the two predictions are fused and transmitted in bitstreams. In this way, more accurate nonlocal predictor can be derived implicitly with local information instead of being explicitly signalled. Experimental results on the reference software VTM-9.0 of VVC show that the method achieves 1.02\% BD-Rate reduction for natural sequences and 2.31\% BD-Rate reduction for screen content videos under all intra (AI) configuration.},
  archive      = {J_TIP},
  author       = {Meng Lei and Falei Luo and Xinfeng Zhang and Shanshe Wang and Siwei Ma},
  doi          = {10.1109/TIP.2022.3161831},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2824-2838},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Joint local and nonlocal progressive prediction for versatile video coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conceptual compression via deep structure and texture
synthesis. <em>TIP</em>, <em>31</em>, 2809–2823. (<a
href="https://doi.org/10.1109/TIP.2022.3159477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing compression methods typically focus on the removal of signal-level redundancies, while the potential and versatility of decomposing visual data into compact conceptual components still lack further study. To this end, we propose a novel conceptual compression framework that encodes visual data into compact structure and texture representations, then decodes in a deep synthesis fashion, aiming to achieve better visual reconstruction quality, flexible content manipulation, and potential support for various vision tasks. In particular, we propose to compress images by a dual-layered model consisting of two complementary visual features: 1) structure layer represented by structural maps and 2) texture layer characterized by low-dimensional deep representations. At the encoder side, the structural maps and texture representations are individually extracted and compressed, generating the compact, interpretable, inter-operable bitstreams. During the decoding stage, a hierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm where the textures are rendered into the decoded structural maps, leading to high-quality reconstruction with remarkable visual realism. Extensive experiments on diverse images have demonstrated the superiority of our framework with lower bitrates, higher reconstruction quality, and increased versatility towards visual analysis and content manipulation tasks.},
  archive      = {J_TIP},
  author       = {Jianhui Chang and Zhenghui Zhao and Chuanmin Jia and Shiqi Wang and Lingbo Yang and Qi Mao and Jian Zhang and Siwei Ma},
  doi          = {10.1109/TIP.2022.3159477},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2809-2823},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Conceptual compression via deep structure and texture synthesis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrete and parameter-free multiple kernel k-means.
<em>TIP</em>, <em>31</em>, 2796–2808. (<a
href="https://doi.org/10.1109/TIP.2022.3141612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple kernel $k$ -means (MKKM) and its variants utilize complementary information from different sources, achieving better performance than kernel $k$ -means (KKM). However, the optimization procedures of most previous works comprise two stages, learning the continuous relaxation matrix and obtaining the discrete one by extra discretization procedures. Such a two-stage strategy gives rise to a mismatched problem and severe information loss. Even worse, most existing MKKM methods overlook the correlation among prespecified kernels, which leads to the fusion of mutually redundant kernels and bad effects on the diversity of information sources, finally resulting in unsatisfying results. To address these issues, we elaborate a novel Discrete and Parameter-free Multiple Kernel $k$ -means (DPMKKM) model solved by an alternative optimization method, which can directly obtain the cluster assignment results without subsequent discretization procedure. Moreover, DPMKKM can measure the correlation among kernels by implicitly introducing a regularization term, which is able to enhance kernel fusion by reducing redundancy and improving diversity. Noteworthily, the time complexity of optimization algorithm is successfully reduced, through masterly utilizing of coordinate descent technique, which contributes to higher algorithm efficiency and broader applications. What’s more, our proposed model is parameter-free avoiding intractable hyperparameter tuning, which makes it feasible in practical applications. Lastly, extensive experiments conducted on a number of real-world datasets illustrated the effectiveness and superiority of the proposed DPMKKM model.},
  archive      = {J_TIP},
  author       = {Rong Wang and Jitao Lu and Yihang Lu and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TIP.2022.3141612},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2796-2808},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discrete and parameter-free multiple kernel k-means},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DetPoseNet: Improving multi-person pose estimation via
coarse-pose filtering. <em>TIP</em>, <em>31</em>, 2782–2795. (<a
href="https://doi.org/10.1109/TIP.2022.3161081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human detection and pose estimation are essential for understanding human activities in images and videos. Mainstream multi-human pose estimation methods take a top-down approach, where human detection is first performed, then each detected person bounding box is fed into a pose estimation network. This top-down approach suffers from the early commitment of initial detections in crowded scenes and other cases with ambiguities or occlusions, leading to pose estimation failures. We propose the DetPoseNet, an end-to-end multi-human detection and pose estimation framework in a unified three-stage network. Our method consists of a coarse-pose proposal extraction sub-net, a coarse-pose based proposal filtering module, and a multi-scale pose refinement sub-net. The coarse-pose proposal sub-net extracts whole-body bounding boxes and body keypoint proposals in a single shot. The coarse-pose filtering step based on the person and keypoint proposals can effectively rule out unlikely detections, thus improving subsequent processing. The pose refinement sub-net performs cascaded pose estimation on each refined proposal region. Multi-scale supervision and multi-scale regression are used in the pose refinement sub-net to simultaneously strengthen context feature learning. Structure-aware loss and keypoint masking are applied to further improve the pose refinement robustness. Our framework is flexible to accept most existing top-down pose estimators as the role of the pose refinement sub-net in our approach. Experiments on COCO and OCHuman datasets demonstrate the effectiveness of the proposed framework. The proposed method is computationally efficient (5-6x speedup) in estimating multi-person poses with refined bounding boxes in sub-seconds.},
  archive      = {J_TIP},
  author       = {Lipeng Ke and Ming-Ching Chang and Honggang Qi and Siwei Lyu},
  doi          = {10.1109/TIP.2022.3161081},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2782-2795},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DetPoseNet: Improving multi-person pose estimation via coarse-pose filtering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure guided deep neural network for unsupervised active
learning. <em>TIP</em>, <em>31</em>, 2767–2781. (<a
href="https://doi.org/10.1109/TIP.2022.3161076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised active learning has become an active research topic in the machine learning and computer vision communities, whose goal is to choose a subset of representative samples to be labeled in an unsupervised setting. Most of existing approaches rely on shallow linear models by assuming that each sample can be well approximated by the span (i.e., the set of all linear combinations) of the selected samples, and then take these selected samples as the representative ones for manual labeling. However, the data do not necessarily conform to the linear models in many real-world scenarios, and how to model nonlinearity of data often becomes the key point of unsupervised active learning. Moreover, the existing works often aim to well reconstruct the whole dataset, while ignore the important cluster structure, especially for imbalanced data. In this paper, we present a novel deep unsupervised active learning framework. The proposed method can explicitly learn a nonlinear embedding to map each input into a latent space via a deep neural network, and introduce a selection block to select the representative samples in the learnt latent space through a self-supervised learning strategy. In the selection block, we aim to not only preserve the global structure of the data, but also capture the cluster structure of the data in order to well handle the data imbalance issue during sample selection. Meanwhile, we take advantage of the clustering result to provide self-supervised information to guide the above processes. Finally, we attempt to preserve the local structure of the data, such that the data embedding becomes more precise and the model performance can be further improved. Extensive experimental results on several publicly available datasets clearly demonstrate the effectiveness of our method, compared with the state-of-the-arts.},
  archive      = {J_TIP},
  author       = {Changsheng Li and Handong Ma and Ye Yuan and Guoren Wang and Dong Xu},
  doi          = {10.1109/TIP.2022.3161076},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2767-2781},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure guided deep neural network for unsupervised active learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised adaptive similarity matrix hashing. <em>TIP</em>,
<em>31</em>, 2755–2766. (<a
href="https://doi.org/10.1109/TIP.2022.3158092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compact hash codes can facilitate large-scale multimedia retrieval, significantly reducing storage and computation. Most hashing methods learn hash functions based on the data similarity matrix, which is predefined by supervised labels or a distance metric type. However, this predefined similarity matrix cannot accurately reflect the real similarity relationship among images, which results in poor retrieval performance of hashing methods, especially in multi-label datasets and zero-shot datasets that are highly dependent on similarity relationships. Toward this end, this study proposes a new supervised hashing method called supervised adaptive similarity matrix hashing (SASH) via feature-label space consistency. SASH not only learns the similarity matrix adaptively, but also extracts the label correlations by maintaining consistency between the feature and the label space. This correlation information is then used to optimize the similarity matrix. The experiments on three large normal benchmark datasets (including two multi-label datasets) and three large zero-shot benchmark datasets show that SASH has an excellent performance compared with several state-of-the-art techniques.},
  archive      = {J_TIP},
  author       = {Yang Shi and Xiushan Nie and Xingbo Liu and Li Zou and Yilong Yin},
  doi          = {10.1109/TIP.2022.3158092},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2755-2766},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Supervised adaptive similarity matrix hashing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable image coding for humans and machines. <em>TIP</em>,
<em>31</em>, 2739–2754. (<a
href="https://doi.org/10.1109/TIP.2022.3160602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, and increasingly so in the future, much of the captured visual content will not be seen by humans. Instead, it will be used for automated machine vision analytics and may require occasional human viewing. Examples of such applications include traffic monitoring, visual surveillance, autonomous navigation, and industrial machine vision. To address such requirements, we develop an end-to-end learned image codec whose latent space is designed to support scalability from simpler to more complicated tasks. The simplest task is assigned to a subset of the latent space (the base layer), while more complicated tasks make use of additional subsets of the latent space, i.e., both the base and enhancement layer(s). For the experiments, we establish a 2-layer and a 3-layer model, each of which offers input reconstruction for human vision, plus machine vision task(s), and compare them with relevant benchmarks. The experiments show that our scalable codecs offer 37\%–80\% bitrate savings on machine vision tasks compared to best alternatives, while being comparable to state-of-the-art image codecs in terms of input reconstruction.},
  archive      = {J_TIP},
  author       = {Hyomin Choi and Ivan V. Bajić},
  doi          = {10.1109/TIP.2022.3160602},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2739-2754},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scalable image coding for humans and machines},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long short-term relation transformer with global gating for
video captioning. <em>TIP</em>, <em>31</em>, 2726–2738. (<a
href="https://doi.org/10.1109/TIP.2022.3158546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning aims to generate a natural language sentence to describe the main content of a video. Since there are multiple objects in videos, taking full exploration of the spatial and temporal relationships among them is crucial for this task. The previous methods wrap the detected objects as input sequences, and leverage vanilla self-attention or graph neural network to reason about visual relations. This cannot make full use of the spatial and temporal nature of a video, and suffers from the problems of redundant connections, over-smoothing, and relation ambiguity. In order to address the above problems, in this paper we construct a long short-term graph (LSTG) that simultaneously captures short-term spatial semantic relations and long-term transformation dependencies. Further, to perform relational reasoning over the LSTG, we design a global gated graph reasoning module (G3RM), which introduces a global gating based on global context to control information propagation between objects and alleviate relation ambiguity. Finally, by introducing G3RM into Transformer instead of self-attention, we propose the long short-term relation transformer (LSRT) to fully mine objects’ relations for caption generation. Experiments on MSVD and MSR-VTT datasets show that the LSRT achieves superior performance compared with state-of-the-art methods. The visualization results indicate that our method alleviates problem of over-smoothing and strengthens the ability of relational reasoning.},
  archive      = {J_TIP},
  author       = {Liang Li and Xingyu Gao and Jincan Deng and Yunbin Tu and Zheng-Jun Zha and Qingming Huang},
  doi          = {10.1109/TIP.2022.3158546},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2726-2738},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Long short-term relation transformer with global gating for video captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R-PointHop: A green, accurate, and unsupervised point cloud
registration method. <em>TIP</em>, <em>31</em>, 2710–2725. (<a
href="https://doi.org/10.1109/TIP.2022.3160609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the recent PointHop classification method, an unsupervised 3D point cloud registration method, called R-PointHop, is proposed in this work. R-PointHop first determines a local reference frame (LRF) for every point using its nearest neighbors and finds local attributes. Next, R-PointHop obtains local-to-global hierarchical features by point downsampling, neighborhood expansion, attribute construction and dimensionality reduction steps. Thus, point correspondences are built in hierarchical feature space using the nearest neighbor rule. Afterwards, a subset of salient points with good correspondence is selected to estimate the 3D transformation. The use of the LRF allows for invariance of the hierarchical features of points with respect to rotation and translation, thus making R-PointHop more robust at building point correspondence, even when the rotation angles are large. Experiments are conducted on the 3DMatch, ModelNet40, and Stanford Bunny datasets, which demonstrate the effectiveness of R-PointHop for 3D point cloud registration. R-PointHop’s model size and training time are an order of magnitude smaller than those of deep learning methods, and its registration errors are smaller, making it a green and accurate solution. Our codes are available on GitHub ( https://github.com/pranavkdm/R-PointHop ).},
  archive      = {J_TIP},
  author       = {Pranav Kadam and Min Zhang and Shan Liu and C. -C. Jay Kuo},
  doi          = {10.1109/TIP.2022.3160609},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2710-2725},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {R-PointHop: A green, accurate, and unsupervised point cloud registration method},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guided filter network for semantic image segmentation.
<em>TIP</em>, <em>31</em>, 2695–2709. (<a
href="https://doi.org/10.1109/TIP.2022.3160399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing publicly available datasets with pixel-level labels contain limited categories, and it is difficult to generalize to the real world containing thousands of categories. In this paper, we propose an approach to generate object masks with detailed pixel-level structures/boundaries automatically to enable semantic image segmentation of thousands of targets in the real world without manually labelling. A Guided Filter Network (GFN) is first developed to learn the segmentation knowledge from an existed dataset, and such GFN then transfers the learned segmentation knowledge to generate initial coarse object masks for the target images. These coarse object masks are treated as pseudo labels to self-optimize the GFN iteratively in the target images. Our experiments on six image sets have demonstrated that our proposed approach can generate object masks with detailed pixel-level structures/boundaries, whose quality is comparable to the manually-labelled ones. Our proposed approach also achieves better performance on semantic image segmentation than most existing weakly-supervised, semi-supervised, and domain adaptation approaches under the same experimental conditions.},
  archive      = {J_TIP},
  author       = {Xiang Zhang and Wanqing Zhao and Wei Zhang and Jinye Peng and Jianping Fan},
  doi          = {10.1109/TIP.2022.3160399},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2695-2709},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Guided filter network for semantic image segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistage spatio-temporal networks for robust sketch
recognition. <em>TIP</em>, <em>31</em>, 2683–2694. (<a
href="https://doi.org/10.1109/TIP.2022.3160240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch recognition relies on two types of information, namely, spatial contexts like the local structures in images and temporal contexts like the orders of strokes. Existing methods usually adopt convolutional neural networks (CNNs) to model spatial contexts, and recurrent neural networks (RNNs) for temporal contexts. However, most of them combine spatial and temporal features with late fusion or single-stage transformation, which is prone to losing the informative details in sketches. To tackle this problem, we propose a novel framework that aims at the multi-stage interactions and refinements of spatial and temporal features. Specifically, given a sketch represented by a stroke array, we first generate a temporal-enriched image (TEI), which is a pseudo-color image retaining the temporal order of strokes, to overcome the difficulty of CNNs in leveraging temporal information. We then construct a dual-branch network, in which a CNN branch and a RNN branch are adopted to process the stroke array and the TEI respectively. In the early stages of our network, considering the limited ability of RNNs in capturing spatial structures, we utilize multiple enhancement modules to enhance the stroke features with the TEI features. While in the last stage of our network, we propose a spatio-temporal enhancement module that refines stroke features and TEI features in a joint feature space. Furthermore, a bidirectional temporal-compatible unit that adaptively merges features in opposite temporal orders, is proposed to help RNNs tackle abrupt strokes. Comprehensive experimental results on QuickDraw and TU-Berlin demonstrate that the proposed method is a robust and efficient solution for sketch recognition.},
  archive      = {J_TIP},
  author       = {Hanhui Li and Xudong Jiang and Boliang Guan and Ruomei Wang and Nadia Magnenat Thalmann},
  doi          = {10.1109/TIP.2022.3160240},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2683-2694},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multistage spatio-temporal networks for robust sketch recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). One sketch for all: One-shot personalized sketch
segmentation. <em>TIP</em>, <em>31</em>, 2673–2682. (<a
href="https://doi.org/10.1109/TIP.2022.3160076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first one-shot personalized sketch segmentation method. We aim to segment all sketches belonging to the same category provisioned with a single sketch with a given part annotation while (i) preserving the parts semantics embedded in the exemplar, and (ii) being robust to input style and abstraction. We refer to this scenario as personalized . With that, we importantly enable a much-desired personalization capability for downstream fine-grained sketch analysis tasks. To train a robust segmentation module, we deform the exemplar sketch to each of the available sketches of the same category. Our method generalizes to sketches not observed during training. Our central contribution is a sketch-specific hierarchical deformation network. Given a multi-level sketch-strokes encoding obtained via a graph convolutional network, our method estimates rigid-body transformation from the target to the exemplar, on the upper level. Finer deformation from the exemplar to the globally warped target sketch is further obtained through stroke-wise deformations, on the lower-level. Both levels of deformation are guided by mean squared distances between the keypoints learned without supervision, ensuring that the stroke semantics are preserved. We evaluate our method against the state-of-the-art segmentation and perceptual grouping baselines re-purposed for the one-shot setting and against two few-shot 3D shape segmentation methods. We show that our method outperforms all the alternatives by more than 10\% on average. Ablation studies further demonstrate that our method is robust to personalization : changes in input part semantics and style differences.},
  archive      = {J_TIP},
  author       = {Anran Qi and Yulia Gryaditskaya and Tao Xiang and Yi-Zhe Song},
  doi          = {10.1109/TIP.2022.3160076},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2673-2682},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {One sketch for all: One-shot personalized sketch segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-guided progressive neural texture fusion for high
dynamic range image restoration. <em>TIP</em>, <em>31</em>, 2661–2672.
(<a href="https://doi.org/10.1109/TIP.2022.3160070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) imaging via multi-exposure fusion is an important task for most modern imaging platforms. In spite of recent developments in both hardware and algorithm innovations, challenges remain over content association ambiguities caused by saturation, motion, and various artifacts introduced during multi-exposure fusion such as ghosting, noise, and blur. In this work, we propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion) HDR restoration model which aims to address these issues within one framework. An efficient two-stream structure is proposed which separately focuses on texture feature transfer over saturated regions and multi-exposure tonal and texture feature fusion. A neural feature transfer mechanism is proposed which establishes spatial correspondence between different exposures based on multi-scale VGG features in the masked saturated HDR domain for discriminative contextual clues over the ambiguous image areas. A progressive texture blending module is designed to blend the encoded two-stream features in a multi-scale and progressive manner. In addition, we introduce several novel attention mechanisms, i.e., the motion attention module detects and suppresses the content discrepancies among the reference images; the saturation attention module facilitates differentiating the misalignment caused by saturation from those caused by motion; and the scale attention module ensures texture blending consistency between different coder/decoder scales. We carry out comprehensive qualitative and quantitative evaluations and ablation studies, which validate that these novel modules work coherently under the same framework and outperform state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Jie Chen and Zaifeng Yang and Tsz Nam Chan and Hui Li and Junhui Hou and Lap-Pui Chau},
  doi          = {10.1109/TIP.2022.3160070},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2661-2672},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attention-guided progressive neural texture fusion for high dynamic range image restoration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet-based texture reformation network for image
super-resolution. <em>TIP</em>, <em>31</em>, 2647–2660. (<a
href="https://doi.org/10.1109/TIP.2022.3160072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most reference-based image super-resolution (RefSR) methods directly leverage the raw features extracted from a pretrained VGG encoder to transfer the matched texture information from a reference image to a low-resolution image. We argue that simply operating on these raw features neglects the influence of irrelevant and redundant information and the importance of abundant high-frequency representations, leading to undesirable texture matching and transfer results. Taking the advantages of wavelet transformation, which represents the contextual and textural information of features at different scales, we propose a Wavelet-based Texture Reformation Network (WTRN) for RefSR. We first decompose the extracted texture features into low-frequency and high-frequency sub-bands and conduct feature matching on the low-frequency component. Based on the correlation map obtained from the feature matching process, we then separately swap and transfer wavelet-domain features at different stages of the network. Furthermore, a wavelet-based texture adversarial loss is proposed to make the network generate more visually plausible textures. Experiments on four benchmark datasets demonstrate that our proposed method outperforms previous RefSR methods both quantitatively and qualitatively. The source code is available at https://github.com/zskuang58/WTRN-TIP .},
  archive      = {J_TIP},
  author       = {Zhen Li and Zeng-Sheng Kuang and Zuo-Liang Zhu and Hong-Peng Wang and Xiu-Li Shao},
  doi          = {10.1109/TIP.2022.3160072},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2647-2660},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Wavelet-based texture reformation network for image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Removing atmospheric turbulence via deep adversarial
learning. <em>TIP</em>, <em>31</em>, 2633–2646. (<a
href="https://doi.org/10.1109/TIP.2022.3158547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring images degraded due to atmospheric turbulence is challenging as it consists of several distortions. Several deep learning methods have been proposed to minimize atmospheric distortions that consist of a single-stage deep network. However, we find that a single-stage deep network is insufficient to remove the mixture of distortions caused by atmospheric turbulence. We propose a two-stage deep adversarial network that minimizes atmospheric turbulence to mitigate this. The first stage reduces the geometrical distortion and the second stage minimizes the image blur. We improve our network by adding channel attention and a proposed sub-pixel mechanism, which utilizes the information between the channels and further reduces the atmospheric turbulence at the finer level. Unlike previous methods, our approach neither uses any prior knowledge about atmospheric turbulence conditions at inference time nor requires the fusion of multiple images to get a single restored image. Our final restoration models DT-GAN+ and DTD-GAN+ outperform the general state-of-the-art image-to-image translation models and baseline restoration models. We synthesize turbulent image datasets to train the restoration models. Additionally, we also curate a natural turbulent dataset from YouTube to show the generalisability of the proposed model. We perform extensive experiments on restored images by utilizing them for downstream tasks such as classification, pose estimation, semantic keypoint estimation, and depth estimation. We observe that our restored images outperform turbulent images in downstream tasks by a significant margin demonstrating the restoration model’s applicability in real-world problems.},
  archive      = {J_TIP},
  author       = {Shyam Nandan Rai and C. V. Jawahar},
  doi          = {10.1109/TIP.2022.3158547},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2633-2646},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Removing atmospheric turbulence via deep adversarial learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BooDet: Gradient boosting object detection with additive
learning-based prediction aggregation. <em>TIP</em>, <em>31</em>,
2620–2632. (<a href="https://doi.org/10.1109/TIP.2022.3157453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the community of object detection has witnessed remarkable progress with the development of deep neural networks. But the detection performance still suffers from the dilemma between complex networks and single-vector predictions. In this paper, we propose a novel approach to boost the object detection performance based on aggregating predictions. First, we propose a unified module with adjustable hyper-structure to generate multiple predictions from a single detection network. Second, we formulate the additive learning for aggregating predictions, which reduces the classification and regression losses by progressively adding the prediction values. Based on the gradient Boosting strategy, the optimization of the additional predictions is further modeled as weighted regression problems to fit the Newton-descent directions. By aggregating multiple predictions from a single network, we propose the BooDet approach which can Boo tstrap the classification and bounding box regression for high-performance object Det ection. In particular, we plug the BooDet into Cascade R-CNN for object detection. Extensive experiments show that the proposed approach is quite effective to improve object detection. We obtain a 1.3\%~2.0\% improvement over the strong baseline Cascade R-CNN on COCO val dataset. We achieve 56.5\% AP on the COCO test-dev dataset with only bounding box annotations.},
  archive      = {J_TIP},
  author       = {Ya-Li Li and Shengjin Wang},
  doi          = {10.1109/TIP.2022.3157453},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2620-2632},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BooDet: Gradient boosting object detection with additive learning-based prediction aggregation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IDART: An improved discrete tomography algorithm for
reconstructing images with multiple gray levels. <em>TIP</em>,
<em>31</em>, 2608–2619. (<a
href="https://doi.org/10.1109/TIP.2022.3152632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete algebraic reconstruction technique has many advantages in computed tomography and electron tomography. However, the number of gray levels and the absolute gray values that should be known in advance are typically not available in experiments especially when there are many gray levels in the image. In this paper, we report an automatic discrete tomography reconstruction algorithm to improve its feasibility in practice, without needing to know these two parameters. In our algorithm, the number of gray levels is estimated by labeling the connected components in the tomogram and the absolute values of them are determined by the modal value of each domain. The proposed algorithm was extensively validated on both simulated and experimental datasets. The results show that our algorithm can accurately recover not only the morphology but also the gray levels of the interested objects, even in the images with multiple gray levels. It is demonstrated that the presented algorithm is robust for eliminating missing wedge artifacts and tolerable for noisy data.},
  archive      = {J_TIP},
  author       = {Yutao He and Wenquan Ming and Ruohan Shen and Jianghua Chen},
  doi          = {10.1109/TIP.2022.3152632},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2608-2619},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IDART: An improved discrete tomography algorithm for reconstructing images with multiple gray levels},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonparametric bayesian regression and classification on
manifolds, with applications to 3D cochlear shapes. <em>TIP</em>,
<em>31</em>, 2598–2607. (<a
href="https://doi.org/10.1109/TIP.2022.3147971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced shape analysis studies such as regression and classification need to be performed on curved manifolds, where often, there is a lack of standard statistical formulations. To overcome these limitations, we introduce a novel machine-learning method on the shape space of curves that avoids direct inference on infinite-dimensional spaces and instead performs Bayesian inference with spherical Gaussian processes decomposition. As an application, we study the shape of the cochlear spiral-shaped cavity within the petrous part of the temporal bone. This problem is particularly challenging due to the relationship between shape and gender, especially in children. Experimental results for both synthetic and real data show improved performance compared to state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {A. Fradi and C. Samir and J. Braga and S. H. Joshi and J-M. Loubes},
  doi          = {10.1109/TIP.2022.3147971},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2598-2607},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Nonparametric bayesian regression and classification on manifolds, with applications to 3D cochlear shapes},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal remote sensing image matching considering
co-occurrence filter. <em>TIP</em>, <em>31</em>, 2584–2597. (<a
href="https://doi.org/10.1109/TIP.2022.3157450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image feature matching methods cannot obtain satisfactory results for multi-modal remote sensing images (MRSIs) in most cases because different imaging mechanisms bring significant nonlinear radiation distortion differences (NRD) and complicated geometric distortion. The key to MRSI matching is trying to weakening or eliminating the NRD and extract more edge features. This paper introduces a new robust MRSI matching method based on co-occurrence filter (CoF) space matching (CoFSM). Our algorithm has three steps: (1) a new co-occurrence scale space based on CoF is constructed, and the feature points in the new scale space are extracted by the optimized image gradient; (2) the gradient location and orientation histogram algorithm is used to construct a 152-dimensional log-polar descriptor, which makes the multi-modal image description more robust; and (3) a position-optimized Euclidean distance function is established, which is used to calculate the displacement error of the feature points in the horizontal and vertical directions to optimize the matching distance function. The optimization results then are rematched, and the outliers are eliminated using a fast sample consensus algorithm. We performed comparison experiments on our CoFSM method with the scale-invariant feature transform (SIFT), upright-SIFT, PSO-SIFT, and radiation-variation insensitive feature transform (RIFT) methods using a multi-modal image dataset. The algorithms of each method were comprehensively evaluated both qualitatively and quantitatively. Our experimental results show that our proposed CoFSM method can obtain satisfactory results both in the number of corresponding points and the accuracy of its root mean square error. The average number of obtained matches is namely 489.52 of CoFSM, and 412.52 of RIFT. As mentioned earlier, the matching effect of the proposed method was significantly greater than the three state-of-art methods. Our proposed CoFSM method achieved good effectiveness and robustness. Executable programs of CoFSM and MRSI datasets are published: https://skyearth.org/publication/project/CoFSM/},
  archive      = {J_TIP},
  author       = {Yongxiang Yao and Yongjun Zhang and Yi Wan and Xinyi Liu and Xiaohu Yan and Jiayuan Li},
  doi          = {10.1109/TIP.2022.3157450},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2584-2597},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-modal remote sensing image matching considering co-occurrence filter},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SST: Spatial and semantic transformers for multi-label image
recognition. <em>TIP</em>, <em>31</em>, 2570–2583. (<a
href="https://doi.org/10.1109/TIP.2022.3148867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image recognition has attracted considerable research attention and achieved great success in recent years. Capturing label correlations is an effective manner to advance the performance of multi-label image recognition. Two types of label correlations were principally studied, i.e., the spatial and semantic correlations. However, in the literature, previous methods considered only either of them. In this work, inspired by the great success of Transformer, we propose a plug-and-play module, named the Spatial and Semantic Transformers (SST), to simultaneously capture spatial and semantic correlations in multi-label images. Our proposal is mainly comprised of two independent transformers, aiming to capture the spatial and semantic correlations respectively. Specifically, our Spatial Transformer is designed to model the correlations between features from different spatial positions, while the Semantic Transformer is leveraged to capture the co-existence of labels without manually defined rules. Other than methodological contributions, we also prove that spatial and semantic correlations complement each other and deserve to be leveraged simultaneously in multi-label image recognition. Benefitting from the Transformer’s ability to capture long-range correlations, our method remarkably outperforms state-of-the-art methods on four popular multi-label benchmark datasets. In addition, extensive ablation studies and visualizations are provided to validate the essential components of our method.},
  archive      = {J_TIP},
  author       = {Zhao-Min Chen and Quan Cui and Borui Zhao and Renjie Song and Xiaoqin Zhang and Osamu Yoshie},
  doi          = {10.1109/TIP.2022.3148867},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2570-2583},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SST: Spatial and semantic transformers for multi-label image recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local intensity order transformation for robust curvilinear
object segmentation. <em>TIP</em>, <em>31</em>, 2557–2569. (<a
href="https://doi.org/10.1109/TIP.2022.3155954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of curvilinear structures is important in many applications, such as retinal blood vessel segmentation for early detection of vessel diseases and pavement crack segmentation for road condition evaluation and maintenance. Currently, deep learning-based methods have achieved impressive performance on these tasks. Yet, most of them mainly focus on finding powerful deep architectures but ignore capturing the inherent curvilinear structure feature ( e.g. , the curvilinear structure is darker than the context) for a more robust representation. In consequence, the performance usually drops a lot on cross-datasets, which poses great challenges in practice. In this paper, we aim to improve the generalizability by introducing a novel local intensity order transformation (LIOT). Specifically, we transfer a gray-scale image into a contrast-invariant four-channel image based on the intensity order between each pixel and its nearby pixels along with the four (horizontal and vertical) directions. This results in a representation that preserves the inherent characteristic of the curvilinear structure while being robust to contrast changes. Cross-dataset evaluation on three retinal blood vessel segmentation datasets demonstrates that LIOT improves the generalizability of some state-of-the-art methods. Additionally, the cross-dataset evaluation between retinal blood vessel segmentation and pavement crack segmentation shows that LIOT is able to preserve the inherent characteristic of curvilinear structure with large appearance gaps. An implementation of the proposed method is available at https://github.com/TY-Shi/LIOT .},
  archive      = {J_TIP},
  author       = {Tianyi Shi and Nicolas Boutry and Yongchao Xu and Thierry Géraud},
  doi          = {10.1109/TIP.2022.3155954},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2557-2569},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Local intensity order transformation for robust curvilinear object segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ShaTure: Shape and texture deformation for human pose and
attribute transfer. <em>TIP</em>, <em>31</em>, 2541–2556. (<a
href="https://doi.org/10.1109/TIP.2022.3157146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel end-to-end pose transfer framework to transform a source person image to an arbitrary pose with controllable attributes. Due to the spatial misalignment caused by occlusions and multi-viewpoints, maintaining high-quality shape and texture appearance is still a challenging problem for pose-guided person image synthesis. Without considering the deformation of shape and texture, existing solutions on controllable pose transfer still cannot generate high-fidelity texture for the target image. To solve this problem, we design a new image reconstruction decoder – ShaTure which formulates shape and texture in a braiding manner. It can interchange discriminative features in both feature-level space and pixel-level space so that the shape and texture can be mutually fine-tuned. In addition, we develop a new bottleneck module – Adaptive Style Selector (AdaSS) Module which can enhance the multi-scale feature extraction capability by self-recalibration of the feature map through channel-wise attention. Both quantitative and qualitative results show that the proposed framework has superiority compared with the state-of-the-art human pose and attribute transfer methods. Detailed ablation studies report the effectiveness of each contribution, which proves the robustness and efficacy of the proposed framework.},
  archive      = {J_TIP},
  author       = {Wing-Yin Yu and Lai-Man Po and Jingjing Xiong and Yuzhi Zhao and Pengfei Xian},
  doi          = {10.1109/TIP.2022.3157146},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2541-2556},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ShaTure: Shape and texture deformation for human pose and attribute transfer},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TSGB: Target-selective gradient backprop for probing CNN
visual saliency. <em>TIP</em>, <em>31</em>, 2529–2540. (<a
href="https://doi.org/10.1109/TIP.2022.3157149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explanation for deep neural networks has drawn extensive attention in the deep learning community over the past few years. In this work, we study the visual saliency, a.k.a. visual explanation, to interpret convolutional neural networks. Compared to iteration based saliency methods, single backward pass based saliency methods benefit from faster speed, and they are widely used in downstream visual tasks. Thus, we focus on single backward pass based methods. However, existing methods in this category struggle to successfully produce fine-grained saliency maps concentrating on specific target classes. That said, producing faithful saliency maps satisfying both target-selectiveness and fine-grainedness using a single backward pass is a challenging problem in the field. To mitigate this problem, we revisit the gradient flow inside the network, and find that the entangled semantics and original weights may disturb the propagation of target-relevant saliency. Inspired by those observations, we propose a novel visual saliency method, termed Target-Selective Gradient Backprop (TSGB), which leverages rectification operations to effectively emphasize target classes and further efficiently propagate the saliency to the image space, thereby generating target-selective and fine-grained saliency maps. The proposed TSGB consists of two components, namely, TSGB-Conv and TSGB-FC, which rectify the gradients for convolutional layers and fully-connected layers, respectively. Extensive qualitative and quantitative experiments on the ImageNet and Pascal VOC datasets show that the proposed method achieves more accurate and reliable results than the other competitive methods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB},
  archive      = {J_TIP},
  author       = {Lin Cheng and Pengfei Fang and Yanjie Liang and Liao Zhang and Chunhua Shen and Hanzi Wang},
  doi          = {10.1109/TIP.2022.3157149},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2529-2540},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TSGB: Target-selective gradient backprop for probing CNN visual saliency},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot domain adaptation via mixup optimal transport.
<em>TIP</em>, <em>31</em>, 2518–2528. (<a
href="https://doi.org/10.1109/TIP.2022.3157139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to learn a classification model for the target domain without any labeled samples by transferring the knowledge from the source domain with sufficient labeled samples. The source and the target domains usually share the same label space but are with different data distributions. In this paper, we consider a more difficult but insufficient-explored problem named as few-shot domain adaptation, where a classifier should generalize well to the target domain given only a small number of examples in the source domain. In such a problem, we recast the link between the source and target samples by a mixup optimal transport model. The mixup mechanism is integrated into optimal transport to perform the few-shot adaptation by learning the cross-domain alignment matrix and domain-invariant classifier simultaneously to augment the source distribution and align the two probability distributions. Moreover, spectral shrinkage regularization is deployed to improve the transferability and discriminability of the mixup optimal transport model by utilizing all singular eigenvectors. Experiments conducted on several domain adaptation tasks demonstrate the effectiveness of our proposed model dealing with the few-shot domain adaptation problem compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Bingrong Xu and Zhigang Zeng and Cheng Lian and Zhengming Ding},
  doi          = {10.1109/TIP.2022.3157139},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2518-2528},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Few-shot domain adaptation via mixup optimal transport},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Portal vein and hepatic vein segmentation in multi-phase MR
images using flow-guided change detection. <em>TIP</em>, <em>31</em>,
2503–2517. (<a href="https://doi.org/10.1109/TIP.2022.3157136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting portal vein (PV) and hepatic vein (HV) from magnetic resonance imaging (MRI) scans is important for hepatic tumor surgery. Compared with single phase-based methods, multiple phases-based methods have better scalability in distinguishing HV and PV by exploiting multi-phase information. However, these methods just coarsely extract HV and PV from different phase images. In this paper, we propose a unified framework to automatically and robustly segment 3D HV and PV from multi-phase MR images, which considers both the change and appearance caused by the vascular flow event to improve segmentation performance. Firstly, inspired by change detection, flow-guided change detection (FGCD) is designed to detect the changed voxels related to hepatic venous flow by generating hepatic venous phase map and clustering the map. The FGCD uniformly deals with HV and PV clustering by the proposed shared clustering, thus making the appearance correlated with portal venous flow robustly delineate without increasing framework complexity. Then, to refine vascular segmentation results produced by both HV and PV clustering, interclass decision making (IDM) is proposed by combining the overlapping region discrimination and neighborhood direction consistency. Finally, our framework is evaluated on multi-phase clinical MR images of the public dataset (TCGA) and local hospital dataset. The quantitative and qualitative evaluations show that our framework outperforms the existing methods.},
  archive      = {J_TIP},
  author       = {Qing Guo and Hong Song and Jingfan Fan and Danni Ai and Yuanjin Gao and Xiaoling Yu and Jian Yang},
  doi          = {10.1109/TIP.2022.3157136},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2503-2517},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Portal vein and hepatic vein segmentation in multi-phase MR images using flow-guided change detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image analysis by fractional-order gaussian-hermite moments.
<em>TIP</em>, <em>31</em>, 2488–2502. (<a
href="https://doi.org/10.1109/TIP.2022.3156380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moments and moment invariants are effective feature descriptors. They have widespread applications in the field of image processing. The recent researches show that fractional-order moments have notable image representation ability. Hermite polynomials are defined over the interval from negative infinity to positive one. Such unboundedness prevents us from developing fractional-order Gaussian-Hermite moments via the existing ideas or approaches. In this paper, we propose fractional-order Gaussian-Hermite moments by forcing the definition domain of Hermite polynomials to be a bounded interval, meanwhile, resorting to a value-decreasing standard deviation to maintain the orthogonality. Moreover, we successfully develop contrast, translation and rotation invariants from the proposed moments based on the inherent properties of Hermite polynomials. The reconstructions of different types of images demonstrate that the proposed moments have more superior image representation ability to the most existing popular orthogonal moments. Besides, the salient performance in invariant image recognition, noise robustness and region-of-interest feature extraction reflect that these moments and their invariants possess the stronger discrimination power and the better noise robustness in comparison with the existing orthogonal moments. Furthermore, both complexity analysis and time consumption indicate that the proposed moments and their invariants are easy to implement, they are suitable for practical engineering applications.},
  archive      = {J_TIP},
  author       = {Bo Yang and Xiaojuan Shi and Xiaofeng Chen},
  doi          = {10.1109/TIP.2022.3156380},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2488-2502},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image analysis by fractional-order gaussian-hermite moments},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-agnostic temporal regularizer for object localization
using motion fields. <em>TIP</em>, <em>31</em>, 2478–2487. (<a
href="https://doi.org/10.1109/TIP.2022.3155947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analysis often requires locating and tracking target objects. In some applications, the localization system has access to the full video, which allows fine-grain motion information to be estimated. This paper proposes capturing this information through motion fields and using it to improve the localization results. The learned motion fields act as a model-agnostic temporal regularizer that can be used with any localization system based on keypoints. Unlike optical flow-based strategies, our motion fields are estimated from the model domain, based on the trajectories described by the object keypoints. Therefore, they are not affected by poor imaging conditions. The benefits of the proposed strategy are shown on three applications: 1) segmentation of cardiac magnetic resonance; 2) facial model alignment; and 3) vehicle tracking. In each case, combining popular localization methods with the proposed regularizer leads to improvement in overall accuracies and reduces gross errors.},
  archive      = {J_TIP},
  author       = {Carlos Santiago and Daniela O. Medley and Jorge S. Marques and Jacinto C. Nascimento},
  doi          = {10.1109/TIP.2022.3155947},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2478-2487},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model-agnostic temporal regularizer for object localization using motion fields},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TSA-SCC: Text semantic-aware screen content coding with
ultra low bitrate. <em>TIP</em>, <em>31</em>, 2463–2477. (<a
href="https://doi.org/10.1109/TIP.2022.3152003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid growth of web conferences, remote screen sharing, and online games, screen content has become an important type of internet media information and over 90\% of online media interactions are screen based. Meanwhile, as the main component in the screen content, textual information averagely takes up over 40\% of the whole image on various commonly used screen content datasets. However, it is difficult to compress the textual information by using the traditional coding schemes as HEVC, which assumes strong spatial and temporal correlations within the image/video. State-of-the-art screen content coding (SCC) standard as HEVC-SCC still adopts a block-based coding framework and does not consider the text semantics for compression, thus inevitably blurring texts at a lower bitrate. In this paper, we propose a general text semantic-aware screen content coding scheme (TSA-SCC) for ultra low bitrate setting. This method detects the abrupt picture in a screen content video (or image), recognizes textual information (including word, position, font type, font size and font color) in the abrupt picture based on neural networks, and encodes texts with text coding tools. The other pictures as well as the background image after removing texts from the abrupt picture via inpainting, are encoded with HEVC-SCC. Compared with HEVC-SCC, the proposed method TSA-SCC reduces bitrate by up to $3\times $ at a similar compression quality. Moreover, TSA-SCC achieves much better visual quality with less bitrate consumption when encoding the screen content video/image at ultra low bitrates.},
  archive      = {J_TIP},
  author       = {Tong Tang and Ling Li and Xiaoyu Wu and Ruizhi Chen and Haochen Li and Guo Lu and Limin Cheng},
  doi          = {10.1109/TIP.2022.3152003},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2463-2477},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TSA-SCC: Text semantic-aware screen content coding with ultra low bitrate},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VidSfM: Robust and accurate structure-from-motion for
monocular videos. <em>TIP</em>, <em>31</em>, 2449–2462. (<a
href="https://doi.org/10.1109/TIP.2022.3156375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of smartphones, larger collection of videos with high quality is available, which makes the scale of scene reconstruction increase dramatically. However, high-resolution video produces more match outliers, and high frame rate video brings more redundant images. To solve these problems, a tailor-made framework is proposed to realize an accurate and robust structure-from-motion based on monocular videos. The key ideas include two points: one is to use the spatial and temporal continuity of video sequences to improve the accuracy and robustness of reconstruction; the other is to use the redundancy of video sequences to improve the efficiency and scalability of system. Our technical contributions include an adaptive way to identify accurate loop matching pairs, a cluster-based camera registration algorithm, a local rotation averaging scheme to verify the pose estimate and a local images extension strategy to reboot the incremental reconstruction. In addition, our system can integrate data from different video sequences, allowing multiple videos to be simultaneously reconstructed. Extensive experiments on both indoor and outdoor monocular videos demonstrate that our method outperforms the state-of-the-art approaches in robustness, accuracy and scalability.},
  archive      = {J_TIP},
  author       = {Hainan Cui and Diantao Tu and Fulin Tang and Pengfei Xu and Hongmin Liu and Shuhan Shen},
  doi          = {10.1109/TIP.2022.3156375},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2449-2462},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VidSfM: Robust and accurate structure-from-motion for monocular videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-rank high-order tensor completion with applications in
visual data. <em>TIP</em>, <em>31</em>, 2433–2448. (<a
href="https://doi.org/10.1109/TIP.2022.3155949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, tensor Singular Value Decomposition (t-SVD)-based low-rank tensor completion (LRTC) has achieved unprecedented success in addressing various pattern analysis issues. However, existing studies mostly focus on third-order tensors while order- $d$ ( $d\geq 4$ ) tensors are commonly encountered in real-world applications, like fourth-order color videos, fourth-order hyper-spectral videos, fifth-order light-field images, and sixth-order bidirectional texture functions. Aiming at addressing this critical issue, this paper establishes an order- $d$ tensor recovery framework including the model, algorithm and theories by innovatively developing a novel algebraic foundation for order- $d$ t-SVD, thereby achieving exact completion for any order- $d$ low t-SVD rank tensors with missing values with an overwhelming probability. Emperical studies on synthetic data and real-world visual data illustrate that compared with other state-of-the-art recovery frameworks, the proposed one achieves highly competitive performance in terms of both qualitative and quantitative metrics. In particular, as the observed data density becomes low, i.e., about 10\%, the proposed recovery framework is still significantly better than its peers. The code of our algorithm is released at https://github.com/Qinwenjinswu/TIP-Code},
  archive      = {J_TIP},
  author       = {Wenjin Qin and Hailin Wang and Feng Zhang and Jianjun Wang and Xin Luo and Tingwen Huang},
  doi          = {10.1109/TIP.2022.3155949},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2433-2448},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Low-rank high-order tensor completion with applications in visual data},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep interactive image matting with feature propagation.
<em>TIP</em>, <em>31</em>, 2421–2432. (<a
href="https://doi.org/10.1109/TIP.2022.3155958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting has attracted growing interest in recent years for its wide applications in numerous vision tasks. Most previous image matting methods rely on trimaps as auxiliary input to define the foreground, background and unknown region. However, trimaps involve fussy manual annotation efforts and are expensive to be obtained in practice. Thus, it is hard and inflexible to update user’s input or achieve real-time interaction with trimaps. Although some automatic matting approaches discard trimaps, they can only be applied to some certain scenarios, like human matting, which limits their versatility. In this work, we employ clicks as interactive behaviours for image matting, to indicate the user-defined foreground, background and unknown region, and propose a click-based deep interactive image matting (DIIM) approach. Compared with trimaps, clicks provide sparse information and are much easier and more flexible, especially for novice users. Based on clicks, users can perform interactive operations and gradually correct the errors until they are satisfied with the prediction. What’s more, we propose a recurrent alpha feature propagation and a full-resolution extraction module to enhance the alpha matte estimation from high-level and low-level respectively. Experimental results show that the proposed click-based deep interactive image matting approach achieves promising performance on image matting datasets.},
  archive      = {J_TIP},
  author       = {Henghui Ding and Hui Zhang and Chang Liu and Xudong Jiang},
  doi          = {10.1109/TIP.2022.3155958},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2421-2432},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep interactive image matting with feature propagation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image inpainting with local and global refinement.
<em>TIP</em>, <em>31</em>, 2405–2420. (<a
href="https://doi.org/10.1109/TIP.2022.3152624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting has made remarkable progress with recent advances in deep learning. Popular networks mainly follow an encoder-decoder architecture (sometimes with skip connections) and possess sufficiently large receptive field, i.e. , larger than the image resolution. The receptive field refers to the set of input pixels that are path-connected to a neuron. For image inpainting task, however, the size of surrounding areas needed to repair different kinds of missing regions are different, and the very large receptive field is not always optimal, especially for the local structures and textures. In addition, a large receptive field tends to involve more undesired completion results, which will disturb the inpainting process. Based on these insights, we rethink the process of image inpainting from a different perspective of receptive field, and propose a novel three-stage inpainting framework with local and global refinement. Specifically, we first utilize an encoder-decoder network with skip connection to achieve coarse initial results. Then, we introduce a shallow deep model with small receptive field to conduct the local refinement, which can also weaken the influence of distant undesired completion results. Finally, we propose an attention-based encoder-decoder network with large receptive field to conduct the global refinement. Experimental results demonstrate that our method outperforms the state of the arts on three popular publicly available datasets for image inpainting. Our local and global refinement network can be directly inserted into the end of any existing networks to further improve their inpainting performance. Code is available at https://github.com/weizequan/LGNet.git .},
  archive      = {J_TIP},
  author       = {Weize Quan and Ruisong Zhang and Yong Zhang and Zhifeng Li and Jue Wang and Dong-Ming Yan},
  doi          = {10.1109/TIP.2022.3152624},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2405-2420},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image inpainting with local and global refinement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive joint low-light enhancement and noise removal
for raw images. <em>TIP</em>, <em>31</em>, 2390–2404. (<a
href="https://doi.org/10.1109/TIP.2022.3155948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light imaging on mobile devices is typically challenging due to insufficient incident light coming through the relatively small aperture, resulting in low image quality. Most of the previous works on low-light imaging focus either only on a single task such as illumination adjustment, color enhancement, or noise removal; or on a joint illumination adjustment and denoising task that heavily relies on short-long exposure image pairs from specific camera models. These approaches are less practical and generalizable in real-world settings where camera-specific joint enhancement and restoration is required. In this paper, we propose a low-light imaging framework that performs joint illumination adjustment, color enhancement, and denoising to tackle this problem. Considering the difficulty in model-specific data collection and the ultra-high definition of the captured images, we design two branches: a coefficient estimation branch and a joint operation branch. The coefficient estimation branch works in a low-resolution space and predicts the coefficients for enhancement via bilateral learning, whereas the joint operation branch works in a full-resolution space and progressively performs joint enhancement and denoising. In contrast to existing methods, our framework does not need to recollect massive data when adapted to another camera model, which significantly reduces the efforts required to fine-tune our approach for practical usage. Through extensive experiments, we demonstrate its great potential in real-world low-light imaging applications.},
  archive      = {J_TIP},
  author       = {Yucheng Lu and Seung-Won Jung},
  doi          = {10.1109/TIP.2022.3155948},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2390-2404},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive joint low-light enhancement and noise removal for raw images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TDPN: Texture and detail-preserving network for single image
super-resolution. <em>TIP</em>, <em>31</em>, 2375–2389. (<a
href="https://doi.org/10.1109/TIP.2022.3154614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) using deep convolutional neural networks (CNNs) achieves the state-of-the-art performance. Most existing SISR models mainly focus on pursuing high peak signal-to-noise ratio (PSNR) and neglect textures and details. As a result, the recovered images are often perceptually unpleasant. To address this issue, in this paper, we propose a texture and detail-preserving network (TDPN), which focuses not only on local region feature recovery but also on preserving textures and details. Specifically, the high-resolution image is recovered from its corresponding low-resolution input in two branches. First, a multi-reception field based branch is designed to let the network fully learn local region features by adaptively selecting local region features in different reception fields. Then, a texture and detail-learning branch supervised by the textures and details decomposed from the ground-truth high resolution image is proposed to provide additional textures and details for the super-resolution process to improve the perceptual quality. Finally, we introduce a gradient loss into the SISR field and define a novel hybrid loss to strengthen boundary information recovery and to avoid overly smooth boundary in the final recovered high-resolution image caused by using only the MAE loss. More importantly, the proposed method is model-agnostic, which can be applied to most off-the-shelf SISR networks. The experimental results on public datasets demonstrate the superiority of our TDPN on most state-of-the-art SISR methods in PSNR, SSIM and perceptual quality. We will share our code on https://github.com/tocaiqing/TDPN .},
  archive      = {J_TIP},
  author       = {Qing Cai and Jinxing Li and Huafeng Li and Yee-Hong Yang and Feng Wu and David Zhang},
  doi          = {10.1109/TIP.2022.3154614},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2375-2389},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TDPN: Texture and detail-preserving network for single image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video reenactment as inductive bias for content-motion
disentanglement. <em>TIP</em>, <em>31</em>, 2365–2374. (<a
href="https://doi.org/10.1109/TIP.2022.3153140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent components within low-dimensional representations are essential inputs in several downstream tasks, and provide explanations over the observed data. Video-based disentangled factors of variation provide low-dimensional representations that can be identified and used to feed task-specific models. We introduce MTC-VAE, a self-supervised motion-transfer VAE model to disentangle motion and content from videos. Unlike previous work on video content-motion disentanglement, we adopt a chunk-wise modeling approach and take advantage of the motion information contained in spatiotemporal neighborhoods. Our model yields independent per-chunk representations that preserve temporal consistency. Hence, we reconstruct whole videos in a single forward-pass. We extend the ELBO’s log-likelihood term and include a Blind Reenactment Loss as an inductive bias to leverage motion disentanglement, under the assumption that swapping motion features yields reenactment between two videos. We evaluate our model with recently-proposed disentanglement metrics and show that it outperforms a variety of methods for video motion-content disentanglement. Experiments on video reenactment show the effectiveness of our disentanglement in the input space where our model outperforms the baselines in reconstruction quality and motion alignment.},
  archive      = {J_TIP},
  author       = {Juan F. Hernández Albarracín and Adín Ramírez Rivera},
  doi          = {10.1109/TIP.2022.3153140},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2365-2374},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video reenactment as inductive bias for content-motion disentanglement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structure-aware positional transformer for visible-infrared
person re-identification. <em>TIP</em>, <em>31</em>, 2352–2364. (<a
href="https://doi.org/10.1109/TIP.2022.3141868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is a cross-modality retrieval problem, which aims at matching the same pedestrian between the visible and infrared cameras. Due to the existence of pose variation, occlusion, and huge visual differences between the two modalities, previous studies mainly focus on learning image-level shared features. Since they usually learn a global representation or extract uniformly divided part features, these methods are sensitive to misalignments. In this paper, we propose a structure-aware positional transformer (SPOT) network to learn semantic-aware sharable modality features by utilizing the structural and positional information. It consists of two main components: attended structure representation (ASR) and transformer-based part interaction (TPI). Specifically, ASR models the modality-invariant structure feature for each modality and dynamically selects the discriminative appearance regions under the guidance of the structure information. TPI mines the part-level appearance and position relations with a transformer to learn discriminative part-level modality features. With a weighted combination of ASR and TPI, the proposed SPOT explores the rich contextual and structural information, effectively reducing cross-modality difference and enhancing the robustness against misalignments. Extensive experiments indicate that SPOT is superior to the state-of-the-art methods on two cross-modal datasets. Notably, the Rank-1/mAP value on the SYSU-MM01 dataset has improved by 8.43\%/6.80\%.},
  archive      = {J_TIP},
  author       = {Cuiqun Chen and Mang Ye and Meibin Qi and Jingjing Wu and Jianguo Jiang and Chia-Wen Lin},
  doi          = {10.1109/TIP.2022.3141868},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2352-2364},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-aware positional transformer for visible-infrared person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RVFace: Reliable vector guided softmax loss for face
recognition. <em>TIP</em>, <em>31</em>, 2337–2351. (<a
href="https://doi.org/10.1109/TIP.2022.3154293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has witnessed significant progress with the advances of deep convolutional neural networks (CNNs), and the central task of which is how to improve the feature discrimination. To this end, several margin-based ( e.g. , angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from four issues: 1) They are based on the assumption of well-cleaned training sets, without considering the consequence of noisy labels inherently existing in most of face recognition datasets; 2) They ignore the importance of informative ( e.g., semi-hard) features mining for discriminative learning; 3) They encourage the feature margin only from the perspective of ground truth class, without realizing the discriminability from other non-ground truth classes; and 4) They set the feature margin between different classes to be same and fixed, which may not adapt the situation of unbalanced data in different classes very well. To cope with these issues, this paper develops a novel loss function, which explicitly estimates the noisy labels to drop them and adaptively emphasizes the semi-hard feature vectors from the remaining reliable ones to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative features for face recognition. To the best of our knowledge, this is the first attempt to inherit the advantages of feature-based noisy labels detection, feature mining and feature margin into a unified loss function. Extensive experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives. Our source code is available at http://www.cbsr.ia.ac.cn/users/xiaobowang/ .},
  archive      = {J_TIP},
  author       = {Xiaobo Wang and Shuo Wang and Yanyan Liang and Liang Gu and Zhen Lei},
  doi          = {10.1109/TIP.2022.3154293},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2337-2351},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RVFace: Reliable vector guided softmax loss for face recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DMRA: Depth-induced multi-scale recurrent attention network
for RGB-d saliency detection. <em>TIP</em>, <em>31</em>, 2321–2336. (<a
href="https://doi.org/10.1109/TIP.2022.3154931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel depth-induced multi-scale recurrent attention network for RGB-D saliency detection, named as DMRA. It achieves dramatic performance especially in complex scenarios. There are four main contributions of our network that are experimentally demonstrated to have significant practical merits. First, we design an effective depth refinement block using residual connections to fully extract and fuse cross-modal complementary cues from RGB and depth streams. Second, depth cues with abundant spatial information are innovatively combined with multi-scale contextual features for accurately locating salient objects. Third, a novel recurrent attention module inspired by Internal Generative Mechanism of human brain is designed to generate more accurate saliency results via comprehensively learning the internal semantic relation of the fused feature and progressively optimizing local details with memory-oriented scene understanding. Finally, a cascaded hierarchical feature fusion strategy is designed to promote efficient information interaction of multi-level contextual features and further improve the contextual representability of model. In addition, we introduce a new real-life RGB-D saliency dataset containing a variety of complex scenarios that has been widely used as a benchmark dataset in recent RGB-D saliency detection research. Extensive empirical experiments demonstrate that our method can accurately identify salient objects and achieve appealing performance against 18 state-of-the-art RGB-D saliency models on nine benchmark datasets.},
  archive      = {J_TIP},
  author       = {Wei Ji and Ge Yan and Jingjing Li and Yongri Piao and Shunyu Yao and Miao Zhang and Li Cheng and Huchuan Lu},
  doi          = {10.1109/TIP.2022.3154931},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2321-2336},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DMRA: Depth-induced multi-scale recurrent attention network for RGB-D saliency detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sample-centric feature generation for semi-supervised
few-shot learning. <em>TIP</em>, <em>31</em>, 2309–2320. (<a
href="https://doi.org/10.1109/TIP.2022.3154938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised few-shot learning aims to improve the model generalization ability by means of both limited labeled data and widely-available unlabeled data. Previous works attempt to model the relations between the few-shot labeled data and extra unlabeled data, by performing a label propagation or pseudo-labeling process using an episodic training strategy. However, the feature distribution represented by the pseudo-labeled data itself is coarse-grained, meaning that there might be a large distribution gap between the pseudo-labeled data and the real query data. To this end, we propose a sample-centric feature generation (SFG) approach for semi-supervised few-shot image classification. Specifically, the few-shot labeled samples from different classes are initially trained to predict pseudo-labels for the potential unlabeled samples. Next, a semi-supervised meta-generator is utilized to produce derivative features centering around each pseudo-labeled sample, enriching the intra-class feature diversity. Meanwhile, the sample-centric generation constrains the generated features to be compact and close to the pseudo-labeled sample, ensuring the inter-class feature discriminability. Further, a reliability assessment (RA) metric is developed to weaken the influence of generated outliers on model learning. Extensive experiments validate the effectiveness of the proposed feature generation approach on challenging one- and few-shot image classification benchmarks.},
  archive      = {J_TIP},
  author       = {Bo Zhang and Hancheng Ye and Gang Yu and Bin Wang and Yike Wu and Jiayuan Fan and Tao Chen},
  doi          = {10.1109/TIP.2022.3154938},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2309-2320},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sample-centric feature generation for semi-supervised few-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive contourlet fusion clustering for SAR image change
detection. <em>TIP</em>, <em>31</em>, 2295–2308. (<a
href="https://doi.org/10.1109/TIP.2022.3154922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel unsupervised change detection method called adaptive Contourlet fusion clustering based on adaptive Contourlet fusion and fast non-local clustering is proposed for multi-temporal synthetic aperture radar (SAR) images. A binary image indicating changed regions is generated by a novel fuzzy clustering algorithm from a Contourlet fused difference image. Contourlet fusion uses complementary information from different types of difference images. For unchanged regions, the details should be restrained while highlighted for changed regions. Different fusion rules are designed for low frequency band and high frequency directional bands of Contourlet coefficients. Then a fast non-local clustering algorithm (FNLC) is proposed to classify the fused image to generate changed and unchanged regions. In order to reduce the impact of noise while preserve details of changed regions, not only local but also non-local information are incorporated into the FNLC in a fuzzy way. Experiments on both small and large scale datasets demonstrate the state-of-the-art performance of the proposed method in real applications.},
  archive      = {J_TIP},
  author       = {Wenhua Zhang and Licheng Jiao and Fang Liu and Shuyuan Yang and Jia Liu},
  doi          = {10.1109/TIP.2022.3154922},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2295-2308},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive contourlet fusion clustering for SAR image change detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single image super-resolution quality assessment: A
real-world dataset, subjective studies, and an objective metric.
<em>TIP</em>, <em>31</em>, 2279–2294. (<a
href="https://doi.org/10.1109/TIP.2022.3154588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous single image super-resolution (SISR) algorithms have been proposed during the past years to reconstruct a high-resolution (HR) image from its low-resolution (LR) observation. However, how to fairly compare the performance of different SISR algorithms/results remains a challenging problem. So far, the lack of comprehensive human subjective study on large-scale real-world SISR datasets and accurate objective SISR quality assessment metrics makes it unreliable to truly understand the performance of different SISR algorithms. We in this paper make efforts to tackle these two issues. Firstly, we construct a real-world SISR quality dataset (i.e., RealSRQ ) and conduct human subjective studies to compare the performance of the representative SISR algorithms. Secondly, we propose a new objective metric, i.e., KLTSRQA , based on the Karhunen-Loéve Transform (KLT) to evaluate the quality of SISR images in a no-reference (NR) manner. Experiments on our constructed RealSRQ and the latest synthetic SISR quality dataset (i.e., QADS ) have demonstrated the superiority of our proposed KLTSRQA metric, achieving higher consistency with human subjective scores than relevant existing NR image quality assessment (NR-IQA) metrics. The dataset and the code will be made available at https://github.com/Zhentao-Liu/RealSRQ-KLTSRQA .},
  archive      = {J_TIP},
  author       = {Qiuping Jiang and Zhentao Liu and Ke Gu and Feng Shao and Xinfeng Zhang and Hantao Liu and Weisi Lin},
  doi          = {10.1109/TIP.2022.3154588},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2279-2294},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Single image super-resolution quality assessment: A real-world dataset, subjective studies, and an objective metric},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GMLight: Lighting estimation via geometric distribution
approximation. <em>TIP</em>, <em>31</em>, 2268–2278. (<a
href="https://doi.org/10.1109/TIP.2022.3151997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring the scene illumination from a single image is an essential yet challenging task in computer vision and computer graphics. Existing works estimate lighting by regressing representative illumination parameters or generating illumination maps directly. However, these methods often suffer from poor accuracy and generalization. This paper presents Geometric Mover’s Light (GMLight), a lighting estimation framework that employs a regression network and a generative projector for effective illumination estimation. We parameterize illumination scenes in terms of the geometric light distribution, light intensity, ambient term, and auxiliary depth, which can be estimated by a regression network. Inspired by the earth mover’s distance, we design a novel geometric mover’s loss to guide the accurate regression of light distribution parameters. With the estimated light parameters, the generative projector synthesizes panoramic illumination maps with realistic appearance and high-frequency details. Extensive experiments show that GMLight achieves accurate illumination estimation and superior fidelity in relighting for 3D object insertion. The codes are available at https://github.com/fnzhan/Illumination-Estimation},
  archive      = {J_TIP},
  author       = {Fangneng Zhan and Yingchen Yu and Changgong Zhang and Rongliang Wu and Wenbo Hu and Shijian Lu and Feiying Ma and Xuansong Xie and Ling Shao},
  doi          = {10.1109/TIP.2022.3151997},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2268-2278},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {GMLight: Lighting estimation via geometric distribution approximation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning residual color for novel view synthesis.
<em>TIP</em>, <em>31</em>, 2257–2267. (<a
href="https://doi.org/10.1109/TIP.2022.3154242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene Representation Networks (SRN) have been proven as a powerful tool for novel view synthesis in recent works. They learn a mapping function from the world coordinates of spatial points to radiance color and the scene’s density using a fully connected network. However, scene texture contains complex high-frequency details in practice that is hard to be memorized by a network with limited parameters, leading to disturbing blurry effects when rendering novel views. In this paper, we propose to learn ‘residual color’ instead of ‘radiance color’ for novel view synthesis, i.e., the residuals between surface color and reference color. Here the reference color is calculated based on spatial color priors, which are extracted from input view observations. The beauty of such a strategy lies in that the residuals between radiance color and reference are close to zero for most spatial points thus are easier to learn. A novel view synthesis system that learns the residual color using SRN is presented in this paper. Experiments on public datasets demonstrate that the proposed method achieves competitive performance in preserving high-resolution details, leading to visually more pleasant results than the state of the arts.},
  archive      = {J_TIP},
  author       = {Lei Han and Dawei Zhong and Lin Li and Kai Zheng and Lu Fang},
  doi          = {10.1109/TIP.2022.3154242},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2257-2267},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning residual color for novel view synthesis},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep ranking exemplar-based dynamic scene deblurring.
<em>TIP</em>, <em>31</em>, 2245–2256. (<a
href="https://doi.org/10.1109/TIP.2022.3142518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic scene deblurring is a challenging problem as it is difficult to be modeled mathematically. Benefiting from the deep convolutional neural networks, this problem has been significantly advanced by the end-to-end network architectures. However, the success of these methods is mainly due to simply stacking network layers. In addition, the methods based on the end-to-end network architectures usually estimate latent images in a regression way which does not preserve the structural details. In this paper, we propose an exemplar-based method to solve dynamic scene deblurring problem. To explore the properties of the exemplars, we propose a siamese encoder network and a shallow encoder network to respectively extract input features and exemplar features and then develop a rank module to explore useful features for better blur removing, where the rank modules are applied to the last three layers of encoder, respectively. The proposed method can be further extended to the way of multi-scale, which enables to recover more texture from the exemplar. Extensive experiments show that our method achieves significant improvements in both quantitative and qualitative evaluations.},
  archive      = {J_TIP},
  author       = {Yaowei Li and Jinshan Pan and Ye Luo and Jianwei Lu},
  doi          = {10.1109/TIP.2022.3142518},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2245-2256},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep ranking exemplar-based dynamic scene deblurring},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bias-eliminated semantic refinement for any-shot learning.
<em>TIP</em>, <em>31</em>, 2229–2244. (<a
href="https://doi.org/10.1109/TIP.2022.3152631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training samples are scarce, the semantic embedding technique, i . e ., describing class labels with attributes, provides a condition to generate visual features for unseen objects by transferring the knowledge from seen objects. However, semantic descriptions are usually obtained in an external paradigm, such as manual annotation, resulting in weak consistency between descriptions and visual features. In this paper, we refine the coarse-grained semantic description for any-shot learning tasks, i . e ., zero-shot learning (ZSL), generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new model, namely, the semantic refinement Wasserstein generative adversarial network (SRWGAN) model, is designed with the proposed multihead representation and hierarchical alignment techniques. Unlike conventional methods, semantic refinement is performed with the aim of identifying a bias-eliminated condition for disjoint-class feature generation and is applicable in both inductive and transductive settings. We extensively evaluate model performance on six benchmark datasets and observe state-of-the-art results for any-shot learning; e . g ., we obtain 70.2\% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset and 82.2\% harmonic accuracy for the Oxford Flowers (FLO) dataset in the standard GZSL setting. Various visualizations are also provided to show the bias-eliminated generation of SRWGAN. Our code is available. 1},
  archive      = {J_TIP},
  author       = {Liangjun Feng and Chunhui Zhao and Xi Li},
  doi          = {10.1109/TIP.2022.3152631},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2229-2244},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bias-eliminated semantic refinement for any-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Occlusion-aware unsupervised learning of depth from 4-d
light fields. <em>TIP</em>, <em>31</em>, 2216–2228. (<a
href="https://doi.org/10.1109/TIP.2022.3154288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation is a fundamental issue in 4-D light field processing and analysis. Although recent supervised learning-based light field depth estimation methods have significantly improved the accuracy and efficiency of traditional optimization-based ones, these methods rely on the training over light field data with ground-truth depth maps which are challenging to obtain or even unavailable for real-world light field data. Besides, due to the inevitable gap (or domain difference) between real-world and synthetic data, they may suffer from serious performance degradation when generalizing the models trained with synthetic data to real-world data. By contrast, we propose an unsupervised learning-based method, which does not require ground-truth depth as supervision during training. Specifically, based on the basic knowledge of the unique geometry structure of light field data, we present an occlusion-aware strategy to improve the accuracy on occlusion areas, in which we explore the angular coherence among subsets of the light field views to estimate initial depth maps, and utilize a constrained unsupervised loss to learn their corresponding reliability for final depth prediction. Additionally, we adopt a multi-scale network with a weighted smoothness loss to handle the textureless areas. Experimental results on synthetic data show that our method can significantly shrink the performance gap between the previous unsupervised method and supervised ones, and produce depth maps with comparable accuracy to traditional methods with obviously reduced computational cost. Moreover, experiments on real-world datasets show that our method can avoid the domain shift problem presented in supervised methods, demonstrating the great potential of our method. The code will be publicly available at https://github.com/jingjin25/LFDE-OccUnNet .},
  archive      = {J_TIP},
  author       = {Jing Jin and Junhui Hou},
  doi          = {10.1109/TIP.2022.3154288},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2216-2228},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Occlusion-aware unsupervised learning of depth from 4-D light fields},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identity-quantity harmonic multi-object tracking.
<em>TIP</em>, <em>31</em>, 2201–2215. (<a
href="https://doi.org/10.1109/TIP.2022.3154286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data association problem of multi-object tracking (MOT) aims to assign IDentity (ID) labels to detections and infer a complete trajectory for each target. Most existing methods assume that each detection corresponds to a unique target and thus cannot handle situations when multiple targets occur in a single detection due to detection failure in crowded scenes. To relax this strong assumption for practical applications, we formulate the MOT as a Maximizing An Identity-Quantity Posterior (MAIQP) problem on the basis of associating each detection with an identity and a quantity characteristic and then provide solutions to tackle two key problems arising. Firstly, a local target quantification module is introduced to count the number of targets within one detection. Secondly, we propose an identity-quantity harmony mechanism to reconcile the two characteristics. On this basis, we develop a novel Identity-Quantity HArmonic Tracking (IQHAT) framework that allows assigning multiple ID labels to detections containing several targets. Through extensive experimental evaluations on five benchmark datasets, we demonstrate the superiority of the proposed method.},
  archive      = {J_TIP},
  author       = {Yuhang He and Xing Wei and Xiaopeng Hong and Wei Ke and Yihong Gong},
  doi          = {10.1109/TIP.2022.3154286},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2201-2215},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Identity-quantity harmonic multi-object tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Learning feature channel weighting for real-time visual
tracking. <em>TIP</em>, <em>31</em>, 2190–2200. (<a
href="https://doi.org/10.1109/TIP.2022.3153170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the siamese convolutional neural network plays an important role in the field of visual tracking, which can obtain high tracking accuracy and good real-time performance. However, the requirement of offline training a specific neural network results in the hardware source and time consumption. In order to improve the tracking efficiency and save computation resources, we adopt pre-trained densely connected neural network to extract robust target features. Since the pre-trained model is mainly used for classification task, it is not appropriate to directly adopt these deep features for visual tracking. We design a regression network to measure the importance of each channel to the target, and then propose a weighting fusion strategy to select the suitable features for visual tracking. Besides, we provide deep analysis about the proposed channel weighting method to demonstrate the superiority of this method through visualization of feature heatmaps. Extensive experiments on four classical benckmarks show that compared with state-of-the-art methods, our algorithm achieves the best results on several standard indicators and comparable results on other indicators.},
  archive      = {J_TIP},
  author       = {Zhetao Li and Jie Zhang and Yanchun Li and Jiang Zhu and Saiqin Long and Dengfeng Xue and Longfei Fan},
  doi          = {10.1109/TIP.2022.3153170},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2190-2200},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning feature channel weighting for real-time visual tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Learning-based rate control for video-based point cloud
compression. <em>TIP</em>, <em>31</em>, 2175–2189. (<a
href="https://doi.org/10.1109/TIP.2022.3152065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to limited transmission resources and storage capacity, efficient rate control is important in Video-based Point Cloud Compression (V-PCC). In this paper, we propose a learning-based rate control method to improve the rate-distortion (RD) performance of V-PCC. A low-latency synchronous rate control structure is designed to reduce the overhead of pre-coding. The basic unit (BU) parameters are predicted accurately based on our proposed CNN-LSTM neural network, instead of the online updating approach, which can be inaccurate due to low consistency between adjacent 2D frames in V-PCC. When determining the quantization parameters for the BU, a patch-based clipping method is proposed to avoid unnecessary clipping. This approach is able to improve the RD performance and subjective dynamic point cloud quality. Experiments show that our proposed rate control method outperforms present approaches.},
  archive      = {J_TIP},
  author       = {Taiyu Wang and Fan Li and Pamela C. Cosman},
  doi          = {10.1109/TIP.2022.3152065},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2175-2189},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning-based rate control for video-based point cloud compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personality assessment based on multimodal attention network
learning with category-based mean square error. <em>TIP</em>,
<em>31</em>, 2162–2174. (<a
href="https://doi.org/10.1109/TIP.2022.3152049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality analysis is widely used in occupational aptitude tests and entrance psychological tests. However, answering hundreds of questions at once seems to be a burden. Inspired by personality psychology, we propose a multimodal attention network with Category-based mean square error (CBMSE) for personality assessment. With this method, we can obtain information about one’s behaviour from his or her daily videos, including his or her gaze distribution, speech features, and facial expression changes, to accurately determine personality traits. In particular, we propose a new approach to implementing an attention mechanism based on the facial Region of No Interest (RoNI), which can achieve higher accuracy and reduce the number of network parameters. Simultaneously, we use CBMSE, a loss function with a higher penalty for the fuzzy boundary in personality assessment, to help the network distinguish boundary data. After effective data fusion, this method achieves an average prediction accuracy of 92.07\%, which is higher than any other state-of-the-art model on the dataset of the ChaLearn Looking at People challenge in association with ECCV 2016.},
  archive      = {J_TIP},
  author       = {Xiao Sun and Jie Huang and Shixin Zheng and Xuanheng Rao and Meng Wang},
  doi          = {10.1109/TIP.2022.3152049},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2162-2174},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Personality assessment based on multimodal attention network learning with category-based mean square error},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised RGB-d salient object detection with
prediction consistency training and active scribble boosting.
<em>TIP</em>, <em>31</em>, 2148–2161. (<a
href="https://doi.org/10.1109/TIP.2022.3151999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D salient object detection (SOD) has attracted increasingly more attention as it shows more robust results in complex scenes compared with RGB SOD. However, state-of-the-art RGB-D SOD approaches heavily rely on a large amount of pixel-wise annotated data for training. Such densely labeled annotations are often labor-intensive and costly. To reduce the annotation burden, we investigate RGB-D SOD from a weakly supervised perspective. More specifically, we use annotator-friendly scribble annotations as supervision signals for model training. Since scribble annotations are much sparser compared to ground-truth masks, some critical object structure information might be neglected. To preserve such structure information, we explicitly exploit the complementary edge information from two modalities ( i.e. , RGB and depth). Specifically, we leverage the dual-modal edge guidance and introduce a new network architecture with a dual-edge detection module and a modality-aware feature fusion module. In order to use the useful information of unlabeled pixels, we introduce a prediction consistency training scheme by comparing the predictions of two networks optimized by different strategies. Moreover, we develop an active scribble boosting strategy to provide extra supervision signals with negligible annotation cost, leading to significant SOD performance improvement. Extensive experiments on seven benchmarks validate the superiority of our proposed method. Remarkably, the proposed method with scribble annotations achieves competitive performance in comparison to fully supervised state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yunqiu Xu and Xin Yu and Jing Zhang and Linchao Zhu and Dadong Wang},
  doi          = {10.1109/TIP.2022.3151999},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2148-2161},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weakly supervised RGB-D salient object detection with prediction consistency training and active scribble boosting},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A geodesic translation model for spherical video
compression. <em>TIP</em>, <em>31</em>, 2136–2147. (<a
href="https://doi.org/10.1109/TIP.2022.3152059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical video coding is critical to the success of many virtual reality and related applications. This paper focuses on an important class of spherical videos whose dynamics involve camera motion. A common approach to spherical video coding is to project from the sphere onto a plane (or planes), where a standard video coder is applied. The projection induces warping resulting in complex non-linear motion in the projected domain that severely comprises the performance of motion models in standard coders. To overcome this shortcoming, we propose a new motion model that captures the motion field on the sphere, and capitalizes on insights into the perceived motion on the sphere due to camera translation. Specifically, surrounding static points are perceived as moving along their respective geodesics, which all intersect at the points where the camera velocity vector intersects the sphere. We analyze the rate of translation along geodesics and its dependence on the elevation of a pixel on the sphere with respect to the camera velocity vector. The analysis leads to a motion vector modulation scheme that perfectly captures the perceived motion of each pixel. Complementary to the new motion model, we propose a search grid tailored to capture expected geodesic motion on the sphere for effective motion estimation. The proposed method yields significant bit-rate savings over employing standard HEVC after projection, which validates its efficacy.},
  archive      = {J_TIP},
  author       = {Bharath Vishwanath and Tejaswi Nanjundaswamy and Kenneth Rose},
  doi          = {10.1109/TIP.2022.3152059},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2136-2147},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A geodesic translation model for spherical video compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-source unsupervised domain adaptation via pseudo
target domain. <em>TIP</em>, <em>31</em>, 2122–2135. (<a
href="https://doi.org/10.1109/TIP.2022.3152052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation (MDA) aims to transfer knowledge from multiple source domains to an unlabeled target domain. MDA is a challenging task due to the severe domain shift, which not only exists between target and source but also exists among diverse sources. Prior studies on MDA either estimate a mixed distribution of source domains or combine multiple single-source models, but few of them delve into the relevant information among diverse source domains. For this reason, we propose a novel MDA approach, termed Pseudo Target for MDA (PTMDA). Specifically, PTMDA maps each group of source and target domains into a group-specific subspace using adversarial learning with a metric constraint, and constructs a series of pseudo target domains correspondingly. Then we align the remainder source domains with the pseudo target domain in the subspace efficiently, which allows to exploit additional structured source information through the training on pseudo target domain and improves the performance on the real target domain. Besides, to improve the transferability of deep neural networks (DNNs), we replace the traditional batch normalization layer with an effective matching normalization layer, which enforces alignments in latent layers of DNNs and thus gains further promotion. We give theoretical analysis showing that PTMDA as a whole can reduce the target error bound and leads to a better approximation of the target risk in MDA settings. Extensive experiments demonstrate PTMDA’s effectiveness on MDA tasks, as it outperforms state-of-the-art methods in most experimental settings.},
  archive      = {J_TIP},
  author       = {Chuan-Xian Ren and Yong-Hui Liu and Xi-Wen Zhang and Ke-Kun Huang},
  doi          = {10.1109/TIP.2022.3152052},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2122-2135},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-source unsupervised domain adaptation via pseudo target domain},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate 3D reconstruction of dynamic objects by
spatial-temporal multiplexing and motion-induced error elimination.
<em>TIP</em>, <em>31</em>, 2106–2121. (<a
href="https://doi.org/10.1109/TIP.2022.3150297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) reconstruction of dynamic objects has broad applications, including object recognition and robotic manipulation. However, achieving high-accuracy reconstruction and robustness to motion simultaneously is a challenging task. In this paper, we present a novel method for 3D reconstruction of dynamic objectS, whose main features are as follows. Firstly, a structured-light multiplexing method is developed that only requires 3 patterns to achieve high-accuracy encoding. Fewer projected patterns require shorter image acquisition time, thus, the object motion is reduced in each reconstruction cycle. The three patterns, i.e. spatial-temporally encoded patterns, are generated by embedding a specifically designed spatial-coded texture map into the temporal-encoded three-step phase-shifting fringes. A temporal codeword and three spatial codewords are extracted from the composite patterns using a proposed extraction algorithm. The two types of codewords are utilized separately in stereo matching: the temporal codeword ensures the high accuracy, while the spatial codewords are responsible for removing phase ambiguity. Secondly, we aim to eliminate the reconstruction error induced by motion between frames abbreviated as motion induced error (MiE). Instead of assuming the object to be static when acquiring the 3 images, we derive the motion of projection pixels among frames. Using the extracted spatial codewords, correspondences between different frames are found, i.e. pixels with the same codewords are traceable in the image sequences. Therefore, we can obtain the phase map at each image-acquisition moment without being affected by the object motion. Then the object surfaces corresponding to all the images can be recovered. Experimental results validate the high reconstruction accuracy and precision of the proposed method for dynamic objects with different motion speeds. Comparative experiments show that the presented method demonstrates superior performance with various types of motion, including translation in different directions and deformation.},
  archive      = {J_TIP},
  author       = {Congying Sui and Kejing He and Congyi Lyu and Yun-Hui Liu},
  doi          = {10.1109/TIP.2022.3150297},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2106-2121},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Accurate 3D reconstruction of dynamic objects by spatial-temporal multiplexing and motion-induced error elimination},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft exemplar highlighting for cross-view image-based
geo-localization. <em>TIP</em>, <em>31</em>, 2094–2105. (<a
href="https://doi.org/10.1109/TIP.2022.3152046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of ground-to-aerial image geo-localization is to determine the location of a ground query image by matching it against a reference database consisting of aerial/satellite images. This task is highly challenging due to the large appearance difference caused by extreme changes in viewpoint and orientation. In this work, we show that the training difficulty is an important cue that can be leveraged to improve metric learning on cross-view images. More specifically, we propose a new Soft Exemplar Highlighting (SEH) loss to achieve online soft selection of exemplars. Adaptive weights are generated for exemplars by measuring their associated training difficulty using distance rectified logistic regression. These weights are then constrained to remove simple exemplars from training and truncate the large weights of extremely hard exemplars to escape from the trap with a local optimal solution. We further use the proposed SEH loss to train two mainstream convolutional neural networks for ground-to-aerial image-based geo-localization. Experimental results on two benchmark cross-view image datasets demonstrate that the proposed method achieves significant improvements in feature discriminativeness and outperforms the state-of-the-art image-based geo-localization methods.},
  archive      = {J_TIP},
  author       = {Yulan Guo and Michael Choi and Kunhong Li and Farid Boussaid and Mohammed Bennamoun},
  doi          = {10.1109/TIP.2022.3152046},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2094-2105},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Soft exemplar highlighting for cross-view image-based geo-localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive feature learning for facade parsing with
occlusions. <em>TIP</em>, <em>31</em>, 2081–2093. (<a
href="https://doi.org/10.1109/TIP.2022.3152004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep models for facade parsing often fail in classifying pixels in heavily occluded regions of facade images due to the difficulty in feature representation of these pixels. In this paper, we solve facade parsing with occlusions by progressive feature learning. To this end, we locate the regions contaminated by occlusions via Bayesian uncertainty evaluation on categorizing each pixel in these regions. Then, guided by the uncertainty, we propose an occlusion-immune facade parsing architecture in which we progressively re-express the features of pixels in each contaminated region from easy to hard. Specifically, the outside pixels, which have reliable context from visible areas, are re-expressed at early stages; the inner pixels are processed at late stages when their surroundings have been decontaminated at the earlier stages. In addition, at each stage, instead of using regular square convolution kernels, we design a context enhancement module (CEM) with directional strip kernels, which can aggregate structural context to re-express facade pixels. Extensive experiments on popular facade datasets demonstrate that the proposed method achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Wenguang Ma and Shibiao Xu and Wei Ma and Xiaopeng Zhang and Hongbin Zha},
  doi          = {10.1109/TIP.2022.3152004},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2081-2093},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive feature learning for facade parsing with occlusions},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). High-order correlation preserved incomplete multi-view
subspace clustering. <em>TIP</em>, <em>31</em>, 2067–2080. (<a
href="https://doi.org/10.1109/TIP.2022.3147046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering aims to exploit the information of multiple incomplete views to partition data into their clusters. Existing methods only utilize the pair-wise sample correlation and pair-wise view correlation to improve the clustering performance but neglect the high-order correlation of samples and that of views. To address this issue, we propose a high-order correlation preserved incomplete multi-view subspace clustering (HCP-IMSC) method which effectively recovers the missing views of samples and the subspace structure of incomplete multi-view data. Specifically, multiple affinity matrices constructed from the incomplete multi-view data are treated as a third-order low rank tensor with a tensor factorization regularization which preserves the high-order view correlation and sample correlation. Then, a unified affinity matrix can be obtained by fusing the view-specific affinity matrices in a self-weighted manner. A hypergraph is further constructed from the unified affinity matrix to preserve the high-order geometrical structure of the data with incomplete views. Then, the samples with missing views are restricted to be reconstructed by their neighbor samples under the hypergraph-induced hyper-Laplacian regularization. Furthermore, the learning of view-specific affinity matrices as well as the unified one, tensor factorization, and hyper-Laplacian regularization are integrated into a unified optimization framework. An iterative algorithm is designed to solve the resultant model. Experimental results on various benchmark datasets indicate the superiority of the proposed method. The code is implemented by using MATLAB R2018a and MindSpore library: https://github.com/ChangTang/HCP-IMSC},
  archive      = {J_TIP},
  author       = {Zhenglai Li and Chang Tang and Xiao Zheng and Xinwang Liu and Wei Zhang and En Zhu},
  doi          = {10.1109/TIP.2022.3147046},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2067-2080},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-order correlation preserved incomplete multi-view subspace clustering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta PID attention network for flexible and efficient
real-world noisy image denoising. <em>TIP</em>, <em>31</em>, 2053–2066.
(<a href="https://doi.org/10.1109/TIP.2022.3150294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep convolutional neural networks for real-world noisy image denoising have shown a huge boost in performance by training a well-engineered network over external image pairs. However, most of these methods are generally trained with supervision. Once the testing data is no longer compatible with the training conditions, they can exhibit poor generalization and easily result in severe overfitting or degrading performances. To tackle this barrier, we propose a novel denoising algorithm, dubbed as Meta PID Attention Network (MPA-Net). Our MPA-Net is built based upon stacking Meta PID Attention Modules (MPAMs). In each MPAM, we utilize a second-order attention module (SAM) to exploit the channel-wise feature correlations with second-order statistics, which are then adaptively updated via a proportional-integral-derivative (PID) guided meta-learning framework. This learning framework exerts the unique property of the PID controller and meta-learning scheme to dynamically generate filter weights for beneficial update of the extracted features within a feedback control system. Moreover, the dynamic nature of the framework enables the generated weights to be flexibly tweaked according to the input at test time. Thus, MPAM not only achieves discriminative feature learning, but also facilitates a robust generalization ability on distinct noises for real images. Extensive experiments on ten datasets are conducted to inspect the effectiveness of the proposed MPA-Net quantitatively and qualitatively, which demonstrates both its superior denoising performance and promising generalization ability that goes beyond those of the state-of-the-art denoising methods.},
  archive      = {J_TIP},
  author       = {Ruijun Ma and Shuyi Li and Bob Zhang and Haifeng Hu},
  doi          = {10.1109/TIP.2022.3150294},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2053-2066},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Meta PID attention network for flexible and efficient real-world noisy image denoising},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). User-guided deep human image matting using arbitrary
trimaps. <em>TIP</em>, <em>31</em>, 2040–2052. (<a
href="https://doi.org/10.1109/TIP.2022.3150295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting is widely studied for accurate foreground extraction. Most algorithms, including deep-learning based solutions, require a carefully edited trimap. Recent works attempt to combine the segmentation stage and matting stage in one CNN model, but errors occurring at the segmentation stage lead to unsatisfactory matte. We propose a user-guided approach for practical human matting. More precisely, we provide a good automatic initial matting and a natural way of interaction that reduces the workload of drawing trimaps and allows users to guide the matting in ambiguous situation. We also combine the segmentation and matting stage in an end-to-end CNN architecture and introduce a residual-learning module to support convenient stroke-based interaction. The proposed model learns to propagate the input trimap and modify the deep image features, which can efficiently correct the segmentation errors. Our model supports arbitrary forms of trimaps from carefully edited to totally unknown maps. Our model also allows users to choose from different foreground estimations according to their preference. We collected a large human matting dataset consisting of 12K real-world human images with complex background and human-object relations. The proposed model is trained on the new dataset with a novel trimap generation strategy that enables the model to tackle different test situations and highly improves the interaction efficiency. Our method outperforms other state-of-the-art automatic methods and achieve competitive accuracy when high-quality trimaps are provided. Experiments indicate that our interactive matting strategy is superior to separately estimating the trimap and alpha matte using two models.},
  archive      = {J_TIP},
  author       = {Xiaonan Fang and Song-Hai Zhang and Tao Chen and Xian Wu and Ariel Shamir and Shi-Min Hu},
  doi          = {10.1109/TIP.2022.3150295},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2040-2052},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {User-guided deep human image matting using arbitrary trimaps},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perceptually unimportant information reduction and cosine
similarity-based quality assessment of 3D-synthesized images.
<em>TIP</em>, <em>31</em>, 2027–2039. (<a
href="https://doi.org/10.1109/TIP.2022.3147981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality assessment of 3D-synthesized images has traditionally been based on detecting specific categories of distortions such as stretching, black-holes, blurring, etc. However, such approaches have limitations in accurately detecting distortions entirely in 3D synthesized images affecting their performance. This work proposes an algorithm to efficiently detect the distortions and subsequently evaluate the perceptual quality of 3D synthesized images. The process of generation of 3D synthesized images produces a few pixel shift between reference and 3D synthesized image, and hence they are not properly aligned with each other. To address this, we propose using morphological operation (opening) in the residual image to reduce perceptually unimportant information between the reference and the distorted 3D synthesized image. The residual image suppresses the perceptually unimportant information and highlights the geometric distortions which significantly affect the overall quality of 3D synthesized images. We utilized the information present in the residual image to quantify the perceptual quality measure and named this algorithm as Perceptually Unimportant Information Reduction (PU-IR) algorithm. At the same time, the residual image cannot capture the minor structural and geometric distortions due to the usage of erosion operation. To address this, we extract the perceptually important deep features from the pre-trained VGG-16 architectures on the Laplacian pyramid. The distortions in 3D synthesized images are present in patches, and the human visual system perceives even the small levels of these distortions. With this view, to compare these deep features between reference and distorted image, we propose using cosine similarity and named this algorithm as Deep Features extraction and comparison using Cosine Similarity (DF-CS) algorithm. The cosine similarity is based upon their similarity rather than computing the magnitude of the difference of deep features. Finally, the pooling is done to obtain the objective quality scores using simple multiplication to both PU-IR and DF-CS algorithms. Our source code is available online: https://github.com/sadbhawnathakur/3D-Image-Quality-Assessment .},
  archive      = {J_TIP},
  author       = {Sadbhawna and Vinit Jakhetiya and Shubham Chaudhary and Badri Narayan Subudhi and Weisi Lin and Sharath Chandra Guntuku},
  doi          = {10.1109/TIP.2022.3147981},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2027-2039},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptually unimportant information reduction and cosine similarity-based quality assessment of 3D-synthesized images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep demosaicing for polarimetric filter array cameras.
<em>TIP</em>, <em>31</em>, 2017–2026. (<a
href="https://doi.org/10.1109/TIP.2022.3150296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarisation Filter Array (PFA) cameras allow the analysis of light polarisation state in a simple and cost-effective manner. Such filter arrays work as the Bayer pattern for colour cameras, sharing similar advantages and drawbacks. Among the others, the raw image must be demosaiced considering the local variations of the PFA and the characteristics of the imaged scene. Non-linear effects, like the cross-talk among neighbouring pixels, are difficult to explicitly model and suggest the potential advantage of a data-driven learning approach. However, the PFA cannot be removed from the sensor, making it difficult to acquire the ground-truth polarization state for training. In this work we propose a novel CNN-based model which directly demosaics the raw camera image to a per-pixel Stokes vector. Our contribution is twofold. First, we propose a network architecture composed by a sequence of Mosaiced Convolutions operating coherently with the local arrangement of the different filters. Second, we introduce a new method, employing a consumer LCD screen, to effectively acquire real-world data for training. The process is designed to be invariant by monitor gamma and external lighting conditions. We extensively compared our method against algorithmic and learning-based demosaicing techniques, obtaining a consistently lower error especially in terms of polarisation angle.},
  archive      = {J_TIP},
  author       = {Mara Pistellato and Filippo Bergamasco and Tehreem Fatima and Andrea Torsello},
  doi          = {10.1109/TIP.2022.3150296},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2017-2026},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep demosaicing for polarimetric filter array cameras},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial reinforcement learning with object-scene
relational graph for video captioning. <em>TIP</em>, <em>31</em>,
2004–2016. (<a href="https://doi.org/10.1109/TIP.2022.3148868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing video captioning methods usually ignore the important fine-grained semantic attributes, the video diversity, as well as the association and motion state between objects within and between frames. Thus, they cannot adapt to small sample data sets. To solve the above problems, this paper proposes a novel video captioning model and an adversarial reinforcement learning strategy. Firstly, an object-scene relational graph model is designed based on the object detector and scene segmenter to express the association features. The graph is encoded by the graph neural network to enrich the expression of visual features. Meanwhile, a trajectory-based feature representation model is designed to replace the previous data-driven method to extract motion and attribute information, so as to analyze the object motion in the time domain and establish the connection between the visual content and language under small data sets. Finally, an adversarial reinforcement learning strategy and a multi- branch discriminator are designed to learn the relationship between the visual content and corresponding words so that rich language knowledge is integrated into the model. Experimental results on three standard datasets and one small sample dataset indicate that our proposed method achieves state-of-the-art performance. Also, the ablation experiments and visualization results verify the effectiveness of proposed each strategy.},
  archive      = {J_TIP},
  author       = {Xia Hua and Xinqing Wang and Ting Rui and Faming Shao and Dong Wang},
  doi          = {10.1109/TIP.2022.3148868},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {2004-2016},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial reinforcement learning with object-scene relational graph for video captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differential transform for video-based plenoptic point cloud
coding. <em>TIP</em>, <em>31</em>, 1994–2003. (<a
href="https://doi.org/10.1109/TIP.2022.3146641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud compression has been studied in standard bodies and we are here concerned with the Moving Picture Experts Group video-based point cloud compression (V-PCC) solution. Plenoptic point clouds (PPC) is a novel volumetric data representation wherein points are associated with colors in all viewing directions to improve realism. It is sampled as a number ( $N_{c}$ ) of attribute colors per point. We propose a new method for the efficient video-based compression of PPC that is backwards compatible with the existing single-color V-PCC decoder. V-PCC generates three image atlases which are encoded using an image/video encoder. We assume there may be a reference color which is to be encoded as the main payload. We generate $N_{c}+3$ atlases and we produce $N_{c}$ differential images against the reference color image. Those difference images are pixel-wise transformed using an $N_{c}$ -point discrete cosine transform, generating $N_{c}$ transformed atlases which are encoded, forming the secondary payload. Such secondary information is the plenoptic enhancement to the point cloud. If there is no reference attribute, we skip the differences and use the lowest frequency of the transformed atlases as the main payload. Results are presented that show an unrivaled performance of the proposed method.},
  archive      = {J_TIP},
  author       = {Diogo C. Garcia and Camilo Dorea and Renan U. B. Ferreira and Davi R. Freitas and Ricardo L. de Queiroz and Rogerio Higa and Ismael Seidel and Vanessa Testoni},
  doi          = {10.1109/TIP.2022.3146641},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1994-2003},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Differential transform for video-based plenoptic point cloud coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TCGL: Temporal contrastive graph for self-supervised video
representation learning. <em>TIP</em>, <em>31</em>, 1978–1993. (<a
href="https://doi.org/10.1109/TIP.2022.3147032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video self-supervised learning is a challenging task, which requires significant expressive power from the model to leverage rich spatial-temporal knowledge and generate effective supervisory signals from large amounts of unlabeled videos. However, existing methods fail to increase the temporal diversity of unlabeled videos and ignore elaborately modeling multi-scale temporal dependencies in an explicit way. To overcome these limitations, we take advantage of the multi-scale temporal dependencies within videos and propose a novel video self-supervised learning framework named Temporal Contrastive Graph Learning (TCGL), which jointly models the inter-snippet and intra-snippet temporal dependencies for temporal representation learning with a hybrid graph contrastive learning strategy. Specifically, a Spatial-Temporal Knowledge Discovering (STKD) module is first introduced to extract motion-enhanced spatial-temporal representations from videos based on the frequency domain analysis of discrete cosine transform. To explicitly model multi-scale temporal dependencies of unlabeled videos, our TCGL integrates the prior knowledge about the frame and snippet orders into graph structures, i.e., the intra-/inter-snippet Temporal Contrastive Graphs (TCG). Then, specific contrastive learning modules are designed to maximize the agreement between nodes in different graph views. To generate supervisory signals for unlabeled videos, we introduce an Adaptive Snippet Order Prediction (ASOP) module which leverages the relational knowledge among video snippets to learn the global context representation and recalibrate the channel-wise features adaptively. Experimental results demonstrate the superiority of our TCGL over the state-of-the-art methods on large-scale action recognition and video retrieval benchmarks. The code is publicly available at https://github.com/YangLiu9208/TCGL .},
  archive      = {J_TIP},
  author       = {Yang Liu and Keze Wang and Lingbo Liu and Haoyuan Lan and Liang Lin},
  doi          = {10.1109/TIP.2022.3147032},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1978-1993},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {TCGL: Temporal contrastive graph for self-supervised video representation learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new non-linear hyperbolic-parabolic coupled PDE model for
image despeckling. <em>TIP</em>, <em>31</em>, 1963–1977. (<a
href="https://doi.org/10.1109/TIP.2022.3149230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a non-linear hyperbolic-parabolic coupled Partial Differential Equation (PDE) based model for image despeckling. Here, a separate equation is used to calculate the edge variable, which improves the quality of edge information in the despeckled images. The existence of the weak solution of the present system is achieved via Schauder fixed point theorem. We used a generalized weighted average finite-difference scheme and the Gauss-Seidel iterative technique to solve the coupled system. Numerical studies are reported to show the effectiveness of the proposed approach with respect to standard PDE-based and nonlocal methods available in the literature. Numerical experiments are performed over gray-level images degraded by artificial speckle noise. Additionally, we investigate the noise removal efficiency of the proposed algorithm when applied to real synthetic aperture radar (SAR) and Ultrasound images. Overall, our study confirms that in most cases, the present model performs better than the other PDE-based models and shows competitive performance with the nonlocal technique. To the best of our knowledge, the proposed despeckling approach is the first work that utilizes the advantage of the non-linear coupled hyperbolic-parabolic PDEs for image despeckling.},
  archive      = {J_TIP},
  author       = {Sudeb Majee and Rajendra K. Ray and Ananta K. Majee},
  doi          = {10.1109/TIP.2022.3149230},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1963-1977},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A new non-linear hyperbolic-parabolic coupled PDE model for image despeckling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust object detection via adversarial novel style
exploration. <em>TIP</em>, <em>31</em>, 1949–1962. (<a
href="https://doi.org/10.1109/TIP.2022.3146017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep object detection models trained on clean images may not generalize well on degraded images due to the well-known domain shift issue. This hinders their application in real-life scenarios such as video surveillance and autonomous driving. Though domain adaptation methods can adapt the detection model from a labeled source domain to an unlabeled target domain, they struggle in dealing with open and compound degradation types. In this paper, we attempt to address this problem in the context of object detection by proposing a robust object Detector via Adversarial Novel Style Exploration (DANSE). Technically, DANSE first disentangles images into domain-irrelevant content representation and domain-specific style representation under an adversarial learning framework. Then, it explores the style space to discover diverse novel degradation styles that are complementary to those of the target domain images by leveraging a novelty regularizer and a diversity regularizer. The clean source domain images are transferred into these discovered styles by using a content-preserving regularizer to ensure realism. These transferred source domain images are combined with the target domain images and used to train a robust degradation-agnostic object detection model via adversarial domain adaptation. Experiments on both synthetic and real benchmark scenarios confirm the superiority of DANSE over state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Wen Wang and Jing Zhang and Wei Zhai and Yang Cao and Dacheng Tao},
  doi          = {10.1109/TIP.2022.3146017},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1949-1962},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust object detection via adversarial novel style exploration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LASOR: Learning accurate 3D human pose and shape via
synthetic occlusion-aware data and neural mesh rendering. <em>TIP</em>,
<em>31</em>, 1938–1948. (<a
href="https://doi.org/10.1109/TIP.2022.3149229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in the task of human pose and shape estimation is occlusion, including self-occlusions, object-human occlusions, and inter-person occlusions. The lack of diverse and accurate pose and shape training data becomes a major bottleneck, especially for scenes with occlusions in the wild. In this paper, we focus on the estimation of human pose and shape in the case of inter-person occlusions, while also handling object-human occlusions and self-occlusion. We propose a novel framework that synthesizes occlusion-aware silhouette and 2D keypoints data and directly regress to the SMPL pose and shape parameters. A neural 3D mesh renderer is exploited to enable silhouette supervision on the fly, which contributes to great improvements in shape estimation. In addition, keypoints-and-silhouette-driven training data in panoramic viewpoints are synthesized to compensate for the lack of viewpoint diversity in any existing dataset. Experimental results show that we are among the state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose estimation accuracy. The proposed method evidently outperforms Mesh Transformer, 3DCrowdNet and ROMP in terms of shape estimation. Top performance is also achieved on SSP-3D in terms of shape prediction accuracy. Demo and code will be available at https://igame-lab.github.io/LASOR/ .},
  archive      = {J_TIP},
  author       = {Kaibing Yang and Renshu Gu and Maoyu Wang and Masahiro Toyoura and Gang Xu},
  doi          = {10.1109/TIP.2022.3149229},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1938-1948},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LASOR: Learning accurate 3D human pose and shape via synthetic occlusion-aware data and neural mesh rendering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient hypergraph approach to robust point cloud
resampling. <em>TIP</em>, <em>31</em>, 1924–1937. (<a
href="https://doi.org/10.1109/TIP.2022.3149225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient processing and feature extraction of large-scale point clouds are important in related computer vision and cyber-physical systems. This work investigates point cloud resampling based on hypergraph signal processing (HGSP) to better explore the underlying relationship among different points in the point cloud and to extract contour-enhanced features. Specifically, we design hypergraph spectral filters to capture multilateral interactions among the signal nodes of point clouds and to better preserve their surface outlines. Without the need and the computation to first construct the underlying hypergraph, our low complexity approach directly estimates hypergraph spectrum of point clouds by leveraging hypergraph stationary processes from the observed 3D coordinates. Evaluating the proposed resampling methods with several metrics, our test results validate the high efficacy of hypergraph characterization of point clouds and demonstrate the robustness of hypergraph-based resampling under noisy observations.},
  archive      = {J_TIP},
  author       = {Qinwen Deng and Songyang Zhang and Zhi Ding},
  doi          = {10.1109/TIP.2022.3149225},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1924-1937},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An efficient hypergraph approach to robust point cloud resampling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic context-aware image style transfer. <em>TIP</em>,
<em>31</em>, 1911–1923. (<a
href="https://doi.org/10.1109/TIP.2022.3149237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide semantic image style transfer results which are consistent with human perception, transferring styles of semantic regions of the style image to their corresponding semantic regions of the content image is necessary. However, when the object categories between the content and style images are not the same, it is difficult to match semantic regions between two images for semantic image style transfer. To solve the semantic matching problem and guide the semantic image style transfer based on matched regions, we propose a novel semantic context-aware image style transfer method by performing semantic context matching followed by a hierarchical local-to-global network architecture. The semantic context matching aims to obtain the corresponding regions between the content and style images by using context correlations of different object categories. Based on the matching results, we retrieve semantic context pairs where each pair is composed of two semantically matched regions from the content and style images. To achieve semantic context-aware style transfer, a hierarchical local-to-global network architecture, which contains two sub-networks including the local context network and the global context network, is proposed. The former focuses on style transfer for each semantic context pair from the style image to the content image, and generates a local style transfer image storing the detailed style feature representations for corresponding semantic regions. The latter aims to derive the stylized image by considering the content, the style, and the intermediate local style transfer images, so that inconsistency between different corresponding semantic regions can be addressed and solved. The experimental results show that the stylized results using our method are more consistent with human perception compared with the state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yi-Sheng Liao and Chun-Rong Huang},
  doi          = {10.1109/TIP.2022.3149237},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1911-1923},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic context-aware image style transfer},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general gaussian heatmap label assignment for
arbitrary-oriented object detection. <em>TIP</em>, <em>31</em>,
1895–1910. (<a href="https://doi.org/10.1109/TIP.2022.3148874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many arbitrary-oriented object detection (AOOD) methods have been proposed and attracted widespread attention in many fields. However, most of them are based on anchor-boxes or standard Gaussian heatmaps. Such label assignment strategy may not only fail to reflect the shape and direction characteristics of arbitrary-oriented objects, but also have high parameter-tuning efforts. In this paper, a novel AOOD method called General Gaussian Heatmap Label Assignment (GGHL) is proposed. Specifically, an anchor-free object-adaptation label assignment (OLA) strategy is presented to define the positive candidates based on two-dimensional (2D) oriented Gaussian heatmaps, which reflect the shape and direction features of arbitrary-oriented objects. Based on OLA, an oriented-bounding-box (OBB) representation component (ORC) is developed to indicate OBBs and adjust the Gaussian center prior weights to fit the characteristics of different objects adaptively through neural network learning. Moreover, a joint-optimization loss (JOL) with area normalization and dynamic confidence weighting is designed to refine the misalign optimal results of different subtasks. Extensive experiments on public datasets demonstrate that the proposed GGHL improves the AOOD performance with low parameter-tuning and time costs. Furthermore, it is generally applicable to most AOOD methods to improve their performance including lightweight models on embedded platforms.},
  archive      = {J_TIP},
  author       = {Zhanchao Huang and Wei Li and Xiang-Gen Xia and Ran Tao},
  doi          = {10.1109/TIP.2022.3148874},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1895-1910},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A general gaussian heatmap label assignment for arbitrary-oriented object detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Siamese implicit region proposal network with compound
attention for visual tracking. <em>TIP</em>, <em>31</em>, 1882–1894. (<a
href="https://doi.org/10.1109/TIP.2022.3148876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, siamese-based trackers have achieved significant successes. However, those trackers are restricted by the difficulty of learning consistent feature representation with the object. To address the above challenge, this paper proposes a novel siamese implicit region proposal network with compound attention for visual tracking. First, an implicit region proposal (IRP) module is designed by combining a novel pixel-wise correlation method. This module can aggregate feature information of different regions that are similar to the pre-defined anchor boxes in Region Proposal Network. To this end, the adaptive feature receptive fields then can be obtained by linear fusion of features from different regions. Second, a compound attention module including a channel and non-local attention is raised to assist the IRP module to perform a better perception of the scale and shape of the object. The channel attention is applied for mining the discriminative information of the object to handle the background clutters of the template, while non-local attention is trained to aggregate the contextual information to learn the semantic range of the object. Finally, experimental results demonstrate that the proposed tracker achieves state-of-the-art performance on six challenging benchmark tests, including VOT-2018, VOT-2019, OTB-100, GOT-10k, LaSOT, and TrackingNet. Further, our obtained results demonstrate that the proposed approach can be run at an average speed of 72 FPS in real time.},
  archive      = {J_TIP},
  author       = {Sixian Chan and Jian Tao and Xiaolong Zhou and Cong Bai and Xiaoqin Zhang},
  doi          = {10.1109/TIP.2022.3148876},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1882-1894},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Siamese implicit region proposal network with compound attention for visual tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intra- and inter-slice contrastive learning for point
supervised OCT fluid segmentation. <em>TIP</em>, <em>31</em>, 1870–1881.
(<a href="https://doi.org/10.1109/TIP.2022.3148814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {OCT fluid segmentation is a crucial task for diagnosis and therapy in ophthalmology. The current convolutional neural networks (CNNs) supervised by pixel-wise annotated masks achieve great success in OCT fluid segmentation. However, requiring pixel-wise masks from OCT images is time-consuming, expensive and expertise needed. This paper proposes an Intra- and inter-Slice Contrastive Learning Network (ISCLNet) for OCT fluid segmentation with only point supervision. Our ISCLNet learns visual representation by designing contrastive tasks that exploit the inherent similarity or dissimilarity from unlabeled OCT data. Specifically, we propose an intra-slice contrastive learning strategy to leverage the fluid-background similarity and the retinal layer-background dissimilarity. Moreover, we construct an inter-slice contrastive learning architecture to learn the similarity of adjacent OCT slices from one OCT volume. Finally, an end-to-end model combining intra- and inter-slice contrastive learning processes learns to segment fluid under the point supervision. The experimental results on two public OCT fluid segmentation datasets (i.e., AI Challenger and RETOUCH) demonstrate that the ISCLNet bridges the gap between fully-supervised and weakly-supervised OCT fluid segmentation and outperforms other well-known point-supervised segmentation methods.},
  archive      = {J_TIP},
  author       = {Xingxin He and Leyuan Fang and Mingkui Tan and Xiangdong Chen},
  doi          = {10.1109/TIP.2022.3148814},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1870-1881},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Intra- and inter-slice contrastive learning for point supervised OCT fluid segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). See360: Novel panoramic view interpolation. <em>TIP</em>,
<em>31</em>, 1857–1869. (<a
href="https://doi.org/10.1109/TIP.2022.3148819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present See360, which is a versatile and efficient framework for 360° panoramic view interpolation using latent space viewpoint estimation. Most of the existing view rendering approaches only focus on indoor or synthetic 3D environments and render new views of small objects. In contrast, we suggest to tackle camera-centered view synthesis as a 2D affine transformation without using point clouds or depth maps, which enables an effective 360° panoramic scene exploration. Given a pair of reference images, the See360 model learns to render novel views by a proposed novel Multi-Scale Affine Transformer (MSAT), enabling the coarse-to-fine feature rendering. We also propose a Conditional Latent space AutoEncoder (C-LAE) to achieve view interpolation at any arbitrary angle. To show the versatility of our method, we introduce four training datasets, namely UrbanCity360, Archinterior360, HungHom360 and Lab360, which are collected from indoor and outdoor environments for both real and synthetic rendering. Experimental results show that the proposed method is generic enough to achieve real-time rendering of arbitrary views for all four datasets. In addition, our See360 model can be applied to view synthesis in the wild: with only a short extra training time (approximately 10 mins), and is able to render unknown real-world scenes. The superior performance of See360 opens up a promising direction for camera-centered view rendering and 360° panoramic view interpolation.},
  archive      = {J_TIP},
  author       = {Zhi-Song Liu and Marie-Paule Cani and Wan-Chi Siu},
  doi          = {10.1109/TIP.2022.3148819},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1857-1869},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {See360: Novel panoramic view interpolation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature preserving non-rigid iterative weighted closest
point and semi-curvature registration. <em>TIP</em>, <em>31</em>,
1841–1856. (<a href="https://doi.org/10.1109/TIP.2022.3148822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preserving features of a surface as characteristic local shape properties captured e.g. by curvature, during non-rigid registration is always difficult where finding meaningful correspondences, assuring the robustness and the convergence of the algorithm while maintaining the quality of mesh are often challenges due to the high degrees of freedom and the sensitivity to features of the source surface. In this paper, we present a non-rigid registration method utilizing a newly defined semi-curvature, which is inspired by the definition of the Gaussian curvature. In the procedure of establishing the correspondences, for each point on the source surface, a corresponding point on the target surface is selected using a dynamic weighted criterion defined on the distance and the semi-curvature. We reformulate the cost function as a combination of the semi-curvature, the stiffness, and the distance terms, and ensure to penalize errors of both the distance and the semi-curvature terms in a guaranteed stable region. For a robust and efficient optimization process, we linearize the semi-curvature term, where the region of attraction is defined and the stability of the approach is proven. Experimental results show that features of the local areas on the original surface with higher curvature values are better preserved in comparison with the conventional methods. In comparison with the other methods, this leads to, on average, 75\%, 8\% and 82\% improvement in terms of quality of correspondences selection, quality of surface after registration, and time spent of the convergence process respectively, mainly due to that the semi-curvature term logically increases the constraints and dependency of each point on the neighboring vertices based on the point’s degree of curvature.},
  archive      = {J_TIP},
  author       = {Farzam Tajdari and Toon Huysmans and Yusheng Yang and Yu Song},
  doi          = {10.1109/TIP.2022.3148822},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1841-1856},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Feature preserving non-rigid iterative weighted closest point and semi-curvature registration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view multi-human association with deep assignment
network. <em>TIP</em>, <em>31</em>, 1830–1840. (<a
href="https://doi.org/10.1109/TIP.2021.3139178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the same persons across different views plays an important role in many vision applications. In this paper, we study this important problem, denoted as Multi-view Multi-Human Association (MvMHA), on multi-view images that are taken by different cameras at the same time. Different from previous works on human association across two views, this paper is focused on more general and challenging scenarios of more than two views, and none of these views are fixed or priorly known. In addition, each involved person may be present in all the views or only a subset of views, which are also not priorly known. We develop a new end-to-end deep-network based framework to address this problem. First, we use an appearance-based deep network to extract the feature of each detected subject on each image. We then compute pairwise-similarity scores between all the detected subjects and construct a comprehensive affinity matrix. Finally, we propose a Deep Assignment Network (DAN) to transform the affinity matrix into an assignment matrix, which provides a binary assignment result for MvMHA. We build both a synthetic dataset and a real image dataset to verify the effectiveness of the proposed method. We also test the trained network on other three public datasets, resulting in very good cross-domain performance.},
  archive      = {J_TIP},
  author       = {Ruize Han and Yun Wang and Haomin Yan and Wei Feng and Song Wang},
  doi          = {10.1109/TIP.2021.3139178},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1830-1840},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-view multi-human association with deep assignment network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pre-demosaic graph-based light field image compression.
<em>TIP</em>, <em>31</em>, 1816–1829. (<a
href="https://doi.org/10.1109/TIP.2022.3145242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An unfocused plenoptic light field (LF) camera places an array of microlenses in front of an image sensor in order to separately capture different directional rays arriving at an image pixel. Using a conventional Bayer pattern, data captured at each pixel is a single color component (R, G or B). The sensed data then undergoes demosaicking (interpolation of RGB components per pixel) and conversion to an array of sub-aperture images (SAIs). In this paper, we propose a new LF image coding scheme based on graph lifting transform (GLT), where the acquired sensor data are coded in the original captured form without pre-processing. Specifically, we directly map raw sensed color data to the SAIs, resulting in sparsely distributed color pixels on 2D grids, and perform demosaicking at the receiver after decoding. To exploit spatial correlation among the sparse pixels, we propose a novel intra-prediction scheme, where the prediction kernel is determined according to the local gradient estimated from already coded neighboring pixel blocks. We then connect the pixels by forming a graph, modeling the prediction residuals statistically as a Gaussian Markov Random Field (GMRF). The optimal edge weights are computed via a graph learning method using a set of training SAIs. The residual data is encoded via low-complexity GLT. Experiments show that at high PSNRs—important for archiving and instant storage scenarios—our method outperformed significantly a conventional light field image coding scheme with demosaicking followed by High Efficiency Video Coding (HEVC).},
  archive      = {J_TIP},
  author       = {Yung-Hsuan Chao and Haoran Hong and Gene Cheung and Antonio Ortega},
  doi          = {10.1109/TIP.2022.3145242},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1816-1829},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pre-demosaic graph-based light field image compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A three-stage self-training framework for semi-supervised
semantic segmentation. <em>TIP</em>, <em>31</em>, 1805–1815. (<a
href="https://doi.org/10.1109/TIP.2022.3144036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has been widely investigated in the community, in which state-of-the-art techniques are based on supervised models. Those models have reported unprecedented performance at the cost of requiring a large set of high quality segmentation masks for training. Obtaining such annotations is highly expensive and time consuming, in particular, in semantic segmentation where pixel-level annotations are required. In this work, we address this problem by proposing a holistic solution framed as a self-training framework for semi-supervised semantic segmentation. The key idea of our technique is the extraction of the pseudo-mask information on unlabelled data whilst enforcing segmentation consistency in a multi-task fashion. We achieve this through a three-stage solution. Firstly, a segmentation network is trained using the labelled data only and rough pseudo-masks are generated for all images. Secondly, we decrease the uncertainty of the pseudo-mask by using a multi-task model that enforces consistency and that exploits the rich statistical information of the data. Finally, the segmentation model is trained by taking into account the information of the higher quality pseudo-masks. We compare our approach against existing semi-supervised semantic segmentation methods and demonstrate state-of-the-art performance with extensive experiments.},
  archive      = {J_TIP},
  author       = {Rihuan Ke and Angelica I. Aviles-Rivero and Saurabh Pandey and Saikumar Reddy and Carola-Bibiane Schönlieb},
  doi          = {10.1109/TIP.2022.3144036},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1805-1815},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A three-stage self-training framework for semi-supervised semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph convolutional dictionary selection with l₂,ₚ norm for
video summarization. <em>TIP</em>, <em>31</em>, 1789–1804. (<a
href="https://doi.org/10.1109/TIP.2022.3146012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Summarization (VS) has become one of the most effective solutions for quickly understanding a large volume of video data. Dictionary selection with self representation and sparse regularization has demonstrated its promise for VS by formulating the VS problem as a sparse selection task on video frames. However, existing dictionary selection models are generally designed only for data reconstruction, which results in the neglect of the inherent structured information among video frames. In addition, the sparsity commonly constrained by $L_{2,1}$ norm is not strong enough, which causes the redundancy of keyframes, i.e., similar keyframes are selected. Therefore, to address these two issues, in this paper we propose a general framework called graph convolutional dictionary selection with $L_{2,p}$ ( $0&amp;lt; p\leq 1$ ) norm (GCDS $_{2,p}$ ) for both keyframe selection and skimming based summarization. Firstly, we incorporate graph embedding into dictionary selection to generate the graph embedding dictionary, which can take the structured information depicted in videos into account. Secondly, we propose to use $L_{2,p}$ ( $0&amp;lt; p\leq 1$ ) norm constrained row sparsity, in which $p$ can be flexibly set for two forms of video summarization. For keyframe selection, $0&amp;lt; p&amp;lt; 1$ can be utilized to select diverse and representative keyframes; and for skimming, $p=1$ can be utilized to select key shots. In addition, an efficient iterative algorithm is devised to optimize the proposed model, and the convergence is theoretically proved. Experimental results including both keyframe selection and skimming based summarization on four benchmark datasets demonstrate the effectiveness and superiority of the proposed method.},
  archive      = {J_TIP},
  author       = {Mingyang Ma and Shaohui Mei and Shuai Wan and Zhiyong Wang and Xian-Sheng Hua and David Dagan Feng},
  doi          = {10.1109/TIP.2022.3146012},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1789-1804},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Graph convolutional dictionary selection with l₂,ₚ norm for video summarization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diverse complementary part mining for weakly supervised
object localization. <em>TIP</em>, <em>31</em>, 1774–1788. (<a
href="https://doi.org/10.1109/TIP.2022.3145238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly Supervised Object Localization (WSOL) aims to localize objects with only image-level labels, which has better scalability and practicability than fully supervised methods in the actual deployment. However, a common limitation for available techniques based on classification networks is that they only highlight the most discriminative part of the object, not the entire object. To alleviate this problem, we propose a novel end-to-end part discovery model (PDM) to learn multiple discriminative object parts in a unified network for accurate object localization and classification. The proposed PDM enjoys several merits. First, to the best of our knowledge, it is the first work to directly model diverse and robust object parts by exploiting part diversity, compactness, and importance jointly for WSOL. Second, three effective mechanisms including diversity, compactness, and importance learning mechanisms are designed to learn robust object parts. Therefore, our model can exploit complementary spatial information and local details from the learned object parts, which help to produce precise bounding boxes and discriminate different object categories. Extensive experiments on two standard benchmarks demonstrate that our PDM performs favorably against state-of-the-art WSOL approaches.},
  archive      = {J_TIP},
  author       = {Meng Meng and Tianzhu Zhang and Wenfei Yang and Jian Zhao and Yongdong Zhang and Feng Wu},
  doi          = {10.1109/TIP.2022.3145238},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1774-1788},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diverse complementary part mining for weakly supervised object localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video super-resolution via a spatio-temporal alignment
network. <em>TIP</em>, <em>31</em>, 1761–1773. (<a
href="https://doi.org/10.1109/TIP.2022.3146625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network based video super-resolution (SR) models have achieved significant progress in recent years. Existing deep video SR methods usually impose optical flow to wrap the neighboring frames for temporal alignment. However, accurate estimation of optical flow is quite difficult, which tends to produce artifacts in the super-resolved results. To address this problem, we propose a novel end-to-end deep convolutional network that dynamically generates the spatially adaptive filters for the alignment, which are constituted by the local spatio-temporal channels of each pixel. Our method avoids generating explicit motion compensation and utilizes spatio-temporal adaptive filters to achieve the operation of alignment, which effectively fuses the multi-frame information and improves the temporal consistency of the video. Capitalizing on the proposed adaptive filter, we develop a reconstruction network and take the aligned frames as input to restore the high-resolution frames. In addition, we employ residual modules embedded with channel attention as the basic unit to extract more informative features for video SR. Both quantitative and qualitative evaluation results on three public video datasets demonstrate that the proposed method performs favorably against state-of-the-art super-resolution methods in terms of clearness and texture details.},
  archive      = {J_TIP},
  author       = {Weilei Wen and Wenqi Ren and Yinghuan Shi and Yunfeng Nie and Jingang Zhang and Xiaochun Cao},
  doi          = {10.1109/TIP.2022.3146625},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1761-1773},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video super-resolution via a spatio-temporal alignment network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating quantitative metrics of tone-mapped images.
<em>TIP</em>, <em>31</em>, 1751–1760. (<a
href="https://doi.org/10.1109/TIP.2022.3146640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subjective evaluation of tone-mapped images is tedious and time-consuming; therefore, it is desirable to have algorithms for automatic quality assessment. Many full-reference and blind metrics have been developed for this purpose, but their performance is generally evaluated on limited benchmark datasets. This leaves a possibility that the observed performance of the metric could be due to overfitting, and it might indeed not perform well for all scenes. In this work, we propose a novel framework using population-based metaheuristics to evaluate the performance of these metrics without requiring any subjectively evaluated reference dataset. The proposed algorithm does not modify the individual image pixels, instead, the tone-mapping curve is modified to synthesize realistic tone-mapped images for evaluation. Moreover, it is not required to know the underlying model of the evaluated metric, which is treated just like a black box and can be replaced by any other metric seamlessly. Therefore, any new metrics designed in the future can also be easily evaluated by simply replacing just one module in the proposed evaluation framework. We evaluate six existing metrics and synthesize images to which the metrics fail to assign appropriate scores for visual quality. We also propose a method to rank the relative performance of evaluated metrics, through a competition in which each metric tries to find the errors in the scores given by other metrics.},
  archive      = {J_TIP},
  author       = {Ishtiaq Rasool Khan and Theyab A. Alotaibi and Asif Siddiq and Farid Bourennani},
  doi          = {10.1109/TIP.2022.3146640},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1751-1760},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Evaluating quantitative metrics of tone-mapped images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stretching artifacts identification for quality assessment
of 3D-synthesized views. <em>TIP</em>, <em>31</em>, 1737–1750. (<a
href="https://doi.org/10.1109/TIP.2022.3145997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Quality Assessment (QA) algorithms consider identifying “black-holes” to assess perceptual quality of 3D-synthesized views. However, advancements in rendering and inpainting techniques have made black-hole artifacts near obsolete. Further, 3D-synthesized views frequently suffer from stretching artifacts due to occlusion that in turn affect perceptual quality. Existing QA algorithms are found to be inefficient in identifying these artifacts, as has been seen by their performance on the IETR dataset. We found, empirically, that there is a relationship between the number of blocks with stretching artifacts in view and the overall perceptual quality. Building on this observation, we propose a Convolutional Neural Network (CNN) based algorithm that identifies the blocks with stretching artifacts and incorporates the number of blocks with the stretching artifacts to predict the quality of 3D-synthesized views. To address the challenge with existing 3D-synthesized views dataset, which has few samples, we collect images from other related datasets to increase the sample size and increase generalization while training our proposed CNN-based algorithm. The proposed algorithm identifies blocks with stretching distortions and subsequently fuses them to predict perceptual quality without reference, achieving improvement in performance compared to existing no-reference QA algorithms that are not trained on the IETR dataset. The proposed algorithm can also identify the blocks with stretching artifacts efficiently, which can further be used in downstream applications to improve the quality of 3D views. Our source code is available online: https://github.com/sadbhawnathakur/3D-Image-Quality-Assessment .},
  archive      = {J_TIP},
  author       = {Sadbhawna and Vinit Jakhetiya and Deebha Mumtaz and Badri Narayan Subudhi and Sharath Chandra Guntuku},
  doi          = {10.1109/TIP.2022.3145997},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1737-1750},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Stretching artifacts identification for quality assessment of 3D-synthesized views},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative style learning for cross-domain image
captioning. <em>TIP</em>, <em>31</em>, 1723–1736. (<a
href="https://doi.org/10.1109/TIP.2022.3145158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cross-domain image captioning, which is trained on a source domain and generalized to other domains, usually faces the large domain shift problem. Although prior work has attempted to leverage both paired source and unpaired target data to minimize this shift, the performance is still unsatisfactory. One main reason lies in the large discrepancy in language expression between two domains, where diverse language styles are adopted to describe an image from different views, resulting in different semantic descriptions for an image. To tackle this problem, this paper proposes a Style-based Cross-domain Image Captioner (SCIC) which incorporates the discriminative style information into the encoder-decoder framework, and interprets an image as a special sentence according to external style instructions. Technically, we design a novel “Instruction-based LSTM”, which adds the instruct gate to collect a style instruction, and then outputs a specified format according to that instruction. Two objectives are designed to train I-LSTM: 1) generating correct image descriptions and 2) generating correct styles, thus the model is expected to accurately capture the semantic meanings of an image by the special caption as well as understand the syntactic structure of the caption. We use MS-COCO as the source domain, and Oxford-102, CUB-200, Flickr30k as the target domains. Experimental results demonstrate that our model consistently outperforms the previous methods, and the style information incorporating with I-LSTM significantly improves the performance, with 5\% CIDEr improvements at least on all datasets.},
  archive      = {J_TIP},
  author       = {Jin Yuan and Shuai Zhu and Shuyin Huang and Hanwang Zhang and Yaoqiang Xiao and Zhiyong Li and Meng Wang},
  doi          = {10.1109/TIP.2022.3145158},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1723-1736},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Discriminative style learning for cross-domain image captioning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lossless coding of light fields based on 4D minimum rate
predictors. <em>TIP</em>, <em>31</em>, 1708–1722. (<a
href="https://doi.org/10.1109/TIP.2022.3146009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common representations of light fields use four-dimensional data structures, where a given pixel is closely related not only to its spatial neighbours within the same view, but also to its angular neighbours, co-located in adjacent views. Such structure presents increased redundancy between pixels, when compared with regular single-view images. Then, these redundancies are exploited to obtain compressed representations of the light field, using prediction algorithms specifically tailored to estimate pixel values based on both spatial and angular references. This paper proposes new encoding schemes which take advantage of the four-dimensional light field data structures to improve the coding performance of Minimum Rate Predictors. The proposed methods expand previous research on lossless coding beyond the current state-of-the-art. The experimental results, obtained using both traditional datasets and others more challenging, show bit-rate savings no smaller than 10\%, when compared with existing methods for lossless light field compression.},
  archive      = {J_TIP},
  author       = {João M. Santos and Lucas A. Thomaz and Pedro A. Amado Assunção and Luís A. da Silva Cruz and Luís Távora and Sérgio M. M. de Faria},
  doi          = {10.1109/TIP.2022.3146009},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1708-1722},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Lossless coding of light fields based on 4D minimum rate predictors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting intra-slice and inter-slice redundancy for
learning-based lossless volumetric image compression. <em>TIP</em>,
<em>31</em>, 1697–1707. (<a
href="https://doi.org/10.1109/TIP.2022.3140608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D volumetric image processing has attracted increasing attention in the last decades, in which one major research area is to develop efficient lossless volumetric image compression techniques to better store and transmit such images with massive amount of information. In this work, we propose the first end-to-end optimized learning framework for losslessly compressing 3D volumetric data. Our approach builds upon a hierarchical compression scheme by additionally introducing the intra-slice auxiliary features and estimating the entropy model based on both intra-slice and inter-slice latent priors. Specifically, we first extract the hierarchical intra-slice auxiliary features through multi-scale feature extraction modules. Then, an Intra-slice and Inter-slice Conditional Entropy Coding module is proposed to fuse the intra-slice and inter-slice information from different scales as the context information. Based on such context information, we can predict the distributions for both intra-slice auxiliary features and the slice images. To further improve the lossless compression performance, we also introduce two new gating mechanisms called Intra-Gate and Inter-Gate to generate the optimal feature representations for better information fusion. Eventually, we can produce the bitstream for losslessly compressing volumetric images based on the estimated entropy model. Different from the existing lossless volumetric image codecs, our end-to-end optimized framework jointly learns both intra-slice auxiliary features at different scales for each slice and inter-slice latent features from previously encoded slices for better entropy estimation. The extensive experimental results indicate that our framework outperforms the state-of-the-art hand-crafted lossless volumetric image codecs ( e.g., JP3D) and the learning-based lossless image compression method on four volumetric image benchmarks for losslessly compressing both 3D Medical Images and Hyper-Spectral Images.},
  archive      = {J_TIP},
  author       = {Zhenghao Chen and Shuhang Gu and Guo Lu and Dong Xu},
  doi          = {10.1109/TIP.2022.3140608},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1697-1707},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting intra-slice and inter-slice redundancy for learning-based lossless volumetric image compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-attentional spatio-temporal semantic graph networks
for video question answering. <em>TIP</em>, <em>31</em>, 1684–1696. (<a
href="https://doi.org/10.1109/TIP.2022.3142526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rich spatio-temporal visual content and complex multimodal relations, Video Question Answering (VideoQA) has become a challenging task and attracted increasing attention. Current methods usually leverage visual attention, linguistic attention, or self-attention to uncover latent correlations between video content and question semantics. Although these methods exploit interactive information between different modalities to improve comprehension ability, inter- and intra-modality correlations cannot be effectively integrated in a uniform model. To address this problem, we propose a novel VideoQA model called Cross-Attentional Spatio-Temporal Semantic Graph Networks (CASSG). Specifically, a multi-head multi-hop attention module with diversity and progressivity is first proposed to explore fine-grained interactions between different modalities in a crossing manner. Then, heterogeneous graphs are constructed from the cross-attended video frames, clips, and question words, in which the multi-stream spatio-temporal semantic graphs are designed to synchronously reasoning inter- and intra-modality correlations. Last, the global and local information fusion method is proposed to coalesce the local reasoning vector learned from multi-stream spatio-temporal semantic graphs and the global vector learned from another branch to infer the answer. Experimental results on three public VideoQA datasets confirm the effectiveness and superiority of our model compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Yun Liu and Xiaoming Zhang and Feiran Huang and Bo Zhang and Zhoujun Li},
  doi          = {10.1109/TIP.2022.3142526},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1684-1696},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-attentional spatio-temporal semantic graph networks for video question answering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fine-grained hashing with double filtering. <em>TIP</em>,
<em>31</em>, 1671–1683. (<a
href="https://doi.org/10.1109/TIP.2022.3145159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained hashing is a new topic in the field of hashing-based retrieval and has not been well explored up to now. In this paper, we raise three key issues that fine-grained hashing should address simultaneously, i.e., fine-grained feature extraction, feature refinement as well as a well-designed loss function. In order to address these issues, we propose a novel Fine-graIned haSHing method with a double-filtering mechanism and a proxy-based loss function, FISH for short. Specifically, the double-filtering mechanism consists of two modules, i.e., Space Filtering module and Feature Filtering module, which address the fine-grained feature extraction and feature refinement issues, respectively. Thereinto, the Space Filtering module is designed to highlight the critical regions in images and help the model to capture more subtle and discriminative details; the Feature Filtering module is the key of FISH and aims to further refine extracted features by supervised re- weighting and enhancing. Moreover, the proxy-based loss is adopted to train the model by preserving similarity relationships between data instances and proxy-vectors of each class rather than other data instances, further making FISH much efficient and effective. Experimental results demonstrate that FISH achieves much better retrieval performance compared with state-of-the-art fine-grained hashing methods, and converges very fast. The source code is publicly available: https://github.com/chenzhenduo/FISH .},
  archive      = {J_TIP},
  author       = {Zhen-Duo Chen and Xin Luo and Yongxin Wang and Shanqing Guo and Xin-Shun Xu},
  doi          = {10.1109/TIP.2022.3145159},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1671-1683},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fine-grained hashing with double filtering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blind and compact denoising network based on noise order
learning. <em>TIP</em>, <em>31</em>, 1657–1670. (<a
href="https://doi.org/10.1109/TIP.2022.3145160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lightweight blind image denoiser, called blind compact denoising network (BCDNet), is proposed in this paper to achieve excellent trade-offs between performance and network complexity. With only 330K parameters, the proposed BCDNet is composed of the compact denoising network (CDNet) and the guidance network (GNet). From a noisy image, GNet extracts a guidance feature, which encodes the severity of the noise. Then, using the guidance feature, CDNet filters the image adaptively according to the severity to remove the noise effectively. Moreover, by reducing the number of parameters without compromising the performance, CDNet achieves denoising not only effectively but also efficiently. Experimental results show that the proposed BCDNet yields state-of-the-art or competitive denoising performances on various datasets while requiring significantly fewer parameters.},
  archive      = {J_TIP},
  author       = {Keunsoo Ko and Yeong Jun Koh and Chang-Su Kim},
  doi          = {10.1109/TIP.2022.3145160},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1657-1670},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Blind and compact denoising network based on noise order learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Relative pose estimation for light field cameras based on
LF-point-LF-point correspondence model. <em>TIP</em>, <em>31</em>,
1641–1656. (<a href="https://doi.org/10.1109/TIP.2022.3144891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a relative pose estimation algorithm for micro-lens array (MLA)-based conventional light field (LF) cameras. First, by employing the matched LF-point pairs, we establish the LF-point-LF-point correspondence model to represent the correlation between LF features of the same 3D scene point in a pair of LFs. Then, we employ the proposed correspondence model to estimate the relative camera pose, which includes a linear solution and a non-linear optimization on manifold. Unlike prior related algorithms, which estimated relative poses based on the recovered depths of scene points, we adopt the estimated disparities to avoid the inaccuracy in recovering depths due to the ultra-small baseline between sub-aperture images of LF cameras. Experimental results on both simulated and real scene data have demonstrated the effectiveness of the proposed algorithm compared with classical as well as state-of-art relative pose estimation algorithms.},
  archive      = {J_TIP},
  author       = {Saiping Zhang and Dongyang Jin and Yuchao Dai and Fuzheng Yang},
  doi          = {10.1109/TIP.2022.3144891},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1641-1656},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Relative pose estimation for light field cameras based on LF-point-LF-point correspondence model},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BIGPrior: Toward decoupling learned prior hallucination and
data fidelity in image restoration. <em>TIP</em>, <em>31</em>,
1628–1640. (<a href="https://doi.org/10.1109/TIP.2022.3143006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic image-restoration algorithms use a variety of priors, either implicitly or explicitly. Their priors are hand-designed and their corresponding weights are heuristically assigned. Hence, deep learning methods often produce superior image restoration quality. Deep networks are, however, capable of inducing strong and hardly predictable hallucinations. Networks implicitly learn to be jointly faithful to the observed data while learning an image prior; and the separation of original data and hallucinated data downstream is then not possible. This limits their wide-spread adoption in image restoration. Furthermore, it is often the hallucinated part that is victim to degradation-model overfitting. We present an approach with decoupled network-prior based hallucination and data fidelity terms. We refer to our framework as the Bayesian Integration of a Generative Prior (BIGPrior). Our method is rooted in a Bayesian framework and tightly connected to classic restoration methods. In fact, it can be viewed as a generalization of a large family of classic restoration algorithms. We use network inversion to extract image prior information from a generative network. We show that, on image colorization, inpainting and denoising, our framework consistently improves the inversion results. Our method, though partly reliant on the quality of the generative network inversion, is competitive with state-of-the-art supervised and task-specific restoration methods. It also provides an additional metric that sets forth the degree of prior reliance per pixel relative to data fidelity.},
  archive      = {J_TIP},
  author       = {Majed El Helou and Sabine Süsstrunk},
  doi          = {10.1109/TIP.2022.3143006},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1628-1640},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BIGPrior: Toward decoupling learned prior hallucination and data fidelity in image restoration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VCRNet: Visual compensation restoration network for
no-reference image quality assessment. <em>TIP</em>, <em>31</em>,
1613–1627. (<a href="https://doi.org/10.1109/TIP.2022.3144892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided by the free-energy principle, generative adversarial networks (GAN)-based no-reference image quality assessment (NR-IQA) methods have improved the image quality prediction accuracy. However, the GAN cannot well handle the restoration task for the free-energy principle-guided NR-IQA methods, especially for the severely destroyed images, which results in that the quality reconstruction relationship between the distorted image and its restored image cannot be accurately built. To address this problem, a visual compensation restoration network (VCRNet)-based NR-IQA method is proposed, which uses a non-adversarial model to efficiently handle the distorted image restoration task. The proposed VCRNet consists of a visual restoration network and a quality estimation network. To accurately build the quality reconstruction relationship between the distorted image and its restored image, a visual compensation module, an optimized asymmetric residual block, and an error map-based mixed loss function, are proposed for increasing the restoration capability of the visual restoration network. For further addressing the NR-IQA problem of severely destroyed images, the multi-level restoration features which are obtained from the visual restoration network are used for the image quality estimation. To prove the effectiveness of the proposed VCRNet, seven representative IQA databases are used, and experimental results show that the proposed VCRNet achieves the state-of-the-art image quality prediction accuracy. The implementation of the proposed VCRNet has been released at https://github.com/NUIST-Videocoding/VCRNet .},
  archive      = {J_TIP},
  author       = {Zhaoqing Pan and Feng Yuan and Jianjun Lei and Yuming Fang and Xiao Shao and Sam Kwong},
  doi          = {10.1109/TIP.2022.3144892},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1613-1627},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VCRNet: Visual compensation restoration network for no-reference image quality assessment},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-constraint adversarial networks for unsupervised
image-to-image translation. <em>TIP</em>, <em>31</em>, 1601–1612. (<a
href="https://doi.org/10.1109/TIP.2022.3144886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised image-to-image translation aims to learn the mapping from an input image in a source domain to an output image in a target domain without paired training dataset. Recently, remarkable progress has been made in translation due to the development of generative adversarial networks (GANs). However, existing methods suffer from the training instability as gradients passing from discriminator to generator become less informative when the source and target domains exhibit sufficiently large discrepancies in appearance or shape. To handle this challenging problem, in this paper, we propose a novel multi-constraint adversarial model (MCGAN) for image translation in which multiple adversarial constraints are applied at generator’s multi-scale outputs by a single discriminator to pass gradients to all the scales simultaneously and assist generator training for capturing large discrepancies in appearance between two domains. We further notice that the solution to regularize generator is helpful in stabilizing adversarial training, but results may have unreasonable structure or blurriness due to less context information flow from discriminator to generator. Therefore, we adopt dense combinations of the dilated convolutions at discriminator for supporting more information flow to generator. With extensive experiments on three public datasets, cat-to-dog, horse-to-zebra, and apple-to-orange, our method significantly improves state-of-the-arts on all datasets.},
  archive      = {J_TIP},
  author       = {Divya Saxena and Tarun Kulshrestha and Jiannong Cao and Shing-Chi Cheung},
  doi          = {10.1109/TIP.2022.3144886},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1601-1612},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-constraint adversarial networks for unsupervised image-to-image translation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imposing semantic consistency of local descriptors for
few-shot learning. <em>TIP</em>, <em>31</em>, 1587–1600. (<a
href="https://doi.org/10.1109/TIP.2022.3143692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning suffers from the scarcity of labeled training data. Regarding local descriptors of an image as representations for the image could greatly augment existing labeled training data. Existing local descriptor based few-shot learning methods have taken advantage of this fact but ignore that the semantics exhibited by local descriptors may not be relevant to the image semantic. In this paper, we deal with this issue from a new perspective of imposing semantic consistency of local descriptors of an image. Our proposed method consists of three modules. The first one is a local descriptor extractor module, which can extract a large number of local descriptors in a single forward pass. The second one is a local descriptor compensator module, which compensates the local descriptors with the image-level representation, in order to align the semantics between local descriptors and the image semantic. The third one is a local descriptor based contrastive loss function, which supervises the learning of the whole pipeline, with the aim of making the semantics carried by the local descriptors of an image relevant and consistent with the image semantic. Theoretical analysis demonstrates the generalization ability of our proposed method. Comprehensive experiments conducted on benchmark datasets indicate that our proposed method achieves the semantic consistency of local descriptors and the state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Jun Cheng and Fusheng Hao and Liu Liu and Dacheng Tao},
  doi          = {10.1109/TIP.2022.3143692},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1587-1600},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Imposing semantic consistency of local descriptors for few-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video summarization through reinforcement learning with a 3D
spatio-temporal u-net. <em>TIP</em>, <em>31</em>, 1573–1586. (<a
href="https://doi.org/10.1109/TIP.2022.3143699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent video summarization algorithms allow to quickly convey the most relevant information in videos through the identification of the most essential and explanatory content while removing redundant video frames. In this paper, we introduce the 3DST-UNet-RL framework for video summarization. A 3D spatio-temporal U-Net is used to efficiently encode spatio-temporal information of the input videos for downstream reinforcement learning (RL). An RL agent learns from spatio-temporal latent scores and predicts actions for keeping or rejecting a video frame in a video summary. We investigate if real/inflated 3D spatio-temporal CNN features are better suited to learn representations from videos than commonly used 2D image features. Our framework can operate in both, a fully unsupervised mode and a supervised training mode. We analyse the impact of prescribed summary lengths and show experimental evidence for the effectiveness of 3DST-UNet-RL on two commonly used general video summarization benchmarks. We also applied our method on a medical video summarization task. The proposed video summarization method has the potential to save storage costs of ultrasound screening videos as well as to increase efficiency when browsing patient video data during retrospective analysis or audit without loosing essential information.},
  archive      = {J_TIP},
  author       = {Tianrui Liu and Qingjie Meng and Jun-Jie Huang and Athanasios Vlontzos and Daniel Rueckert and Bernhard Kainz},
  doi          = {10.1109/TIP.2022.3143699},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1573-1586},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video summarization through reinforcement learning with a 3D spatio-temporal U-net},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted feature fusion of convolutional neural network and
graph attention network for hyperspectral image classification.
<em>TIP</em>, <em>31</em>, 1559–1572. (<a
href="https://doi.org/10.1109/TIP.2022.3144017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN), such as Graph Attention Networks (GAT), are two classic neural network models, which are applied to the processing of grid data and graph data respectively. They have achieved outstanding performance in hyperspectral images (HSIs) classification field, which have attracted great interest. However, CNN has been facing the problem of small samples and GNN has to pay a huge computational cost, which restrict the performance of the two models. In this paper, we propose Weighted Feature Fusion of Convolutional Neural Network and Graph Attention Network (WFCG) for HSI classification, by using the characteristics of superpixel-based GAT and pixel-based CNN, which proved to be complementary. We first establish GAT with the help of superpixel-based encoder and decoder modules. Then we combined the attention mechanism to construct CNN. Finally, the features are weighted fusion with the characteristics of two neural network models. Rigorous experiments on three real-world HSI data sets show WFCG can fully explore the high-dimensional feature of HSI, and obtain competitive results compared to other state-of-the art methods.},
  archive      = {J_TIP},
  author       = {Yanni Dong and Quanwei Liu and Bo Du and Liangpei Zhang},
  doi          = {10.1109/TIP.2022.3144017},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1559-1572},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Weighted feature fusion of convolutional neural network and graph attention network for hyperspectral image classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refined one-stage oriented object detection method for
remote sensing images. <em>TIP</em>, <em>31</em>, 1545–1558. (<a
href="https://doi.org/10.1109/TIP.2022.3143690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-class object detection in remote sensing images plays an important role in many applications but remains a challenging task because of scale imbalance and arbitrary orientations of the objects with extreme aspect ratios. In this paper, the Asymmetric Feature Pyramid Network (AFPN), Dynamic Feature Alignment (DFA) module, and Area-IoU regression loss are proposed on the basis of a one-stage cascaded detection method for the detection of multi-class objects with arbitrary orientations in remote sensing images. The designed asymmetric convolutional block is embedded into the AFPN for handling objects with extreme aspect ratios and improving the space representation with ignorable increases in calculation. The DFA module is proposed to dynamically align mismatched features, which are caused by the deviation between predefined anchors and arbitrarily oriented predicted boxes. The refined Area-IoU regression loss, which reconciles two new regression loss functions, the area-guided regression loss and IoU-guided regression loss, is proposed to simultaneously solve the scale imbalance problem and angle sensitivity problem. Experiments on three publicly available datasets, DOTA, HRSC2016, and ICDAR2015, show the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Liping Hou and Ke Lu and Jian Xue},
  doi          = {10.1109/TIP.2022.3143690},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1545-1558},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Refined one-stage oriented object detection method for remote sensing images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Attentive WaveBlock: Complementarity-enhanced mutual
networks for unsupervised domain adaptation in person re-identification
and beyond. <em>TIP</em>, <em>31</em>, 1532–1544. (<a
href="https://doi.org/10.1109/TIP.2022.3140614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) for person re-identification is challenging because of the huge gap between the source and target domain. A typical self-training method is to use pseudo-labels generated by clustering algorithms to iteratively optimize the model on the target domain. However, a drawback to this is that noisy pseudo-labels generally cause trouble in learning. To address this problem, a mutual learning method by dual networks has been developed to produce reliable soft labels. However, as the two neural networks gradually converge, their complementarity is weakened and they likely become biased towards the same kind of noise. This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity and further depress noise in the pseudo-labels. Specifically, we first introduce a parameter-free module, the WaveBlock, which creates a difference between features learned by two networks by waving blocks of feature maps differently. Then, an attention mechanism is leveraged to enlarge the difference created and discover more complementary features. Furthermore, two kinds of combination strategies, i.e. pre-attention and post-attention, are explored. Experiments demonstrate that the proposed method achieves state-of-the-art performance with significant improvements on multiple UDA person re-identification tasks. We also prove the generality of the proposed method by applying it to vehicle re-identification and image classification tasks. Our codes and models are available at: AWB.},
  archive      = {J_TIP},
  author       = {Wenhao Wang and Fang Zhao and Shengcai Liao and Ling Shao},
  doi          = {10.1109/TIP.2022.3140614},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1532-1544},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Attentive WaveBlock: Complementarity-enhanced mutual networks for unsupervised domain adaptation in person re-identification and beyond},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Information symmetry matters: A modal-alternating
propagation network for few-shot learning. <em>TIP</em>, <em>31</em>,
1520–1531. (<a href="https://doi.org/10.1109/TIP.2022.3143005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic information provides intra-class consistency and inter-class discriminability beyond visual concepts, which has been employed in Few-Shot Learning (FSL) to achieve further gains. However, semantic information is only available for labeled samples but absent for unlabeled samples, in which the embeddings are rectified unilaterally by guiding the few labeled samples with semantics. Therefore, it is inevitable to bring a cross-modal bias between semantic-guided samples and nonsemantic-guided samples, which results in an information asymmetry problem. To address this problem, we propose a Modal-Alternating Propagation Network (MAP-Net) to supplement the absent semantic information of unlabeled samples, which builds information symmetry among all samples in both visual and semantic modalities. Specifically, the MAP-Net transfers the neighbor information by the graph propagation to generate the pseudo-semantics for unlabeled samples guided by the completed visual relationships and rectify the feature embeddings. In addition, due to the large discrepancy between visual and semantic modalities, we design a Relation Guidance (RG) strategy to guide the visual relation vectors via semantics so that the propagated information is more beneficial. Extensive experimental results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011, SUN Attribute Database and Oxford 102 Flower, have demonstrated that our proposed method achieves promising performance and outperforms the state-of-the-art approaches, which indicates the necessity of information symmetry.},
  archive      = {J_TIP},
  author       = {Zhong Ji and Zhishen Hou and Xiyao Liu and Yanwei Pang and Jungong Han},
  doi          = {10.1109/TIP.2022.3143005},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1520-1531},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Information symmetry matters: A modal-alternating propagation network for few-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modality self-distillation for weakly supervised
temporal action localization. <em>TIP</em>, <em>31</em>, 1504–1519. (<a
href="https://doi.org/10.1109/TIP.2021.3137649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging task of high-level video understanding, Weakly-supervised Temporal Action Localization (WTAL) has attracted increasing attention in recent years. However, due to the weak supervisions of whole-video classification labels, it is challenging to accurately determine action instance boundaries. To address this issue, pseudo-label-based methods [Alwassel et al. (2019), Luo et al. (2020), and Zhai et al. (2020)] were proposed to generate snippet-level pseudo labels from classification results. In spite of the promising performance, these methods hardly take full advantages of multiple modalities, i.e. , RGB and optical flow sequences, to generate high quality pseudo labels. Most of them ignored how to mitigate the label noise, which hinders the capability of the network on learning discriminative feature representations. To address these challenges, we propose a Multi-Modality Self-Distillation (MMSD) framework, which contains two single-modal streams and a fused-modal stream to perform multi-modality knowledge distillation and multi-modality self-voting. On the one hand, multi-modality knowledge distillation improves snippet-level classification performance by transferring knowledge between single-modal streams and a fused-modal stream. On the other hand, multi-modality self-voting mitigates the label noise in a modality voting manner according to the reliability and complementarity of the streams. Experimental results on THUMOS14 and ActivityNet1.3 datasets demonstrate the effectiveness of our method and superior performance over state-of-the-art approaches. Our code is available at https://github.com/LeonHLJ/MMSD .},
  archive      = {J_TIP},
  author       = {Linjiang Huang and Liang Wang and Hongsheng Li},
  doi          = {10.1109/TIP.2021.3137649},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1504-1519},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-modality self-distillation for weakly supervised temporal action localization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning discrete representations from reference images for
large scale factor image super-resolution. <em>TIP</em>, <em>31</em>,
1490–1503. (<a href="https://doi.org/10.1109/TIP.2022.3142999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) task aims to recover high-resolution (HR) images from degraded low-resolution (LR) images, which has achieved great progress due to the recent advances of deep neural networks. Due to severe information loss of the LR images, it is more challenging to reconstruct high quality HR images at large scale factors, i . e ., higher than $4\times $ . Traditional reference image based SR methods usually perform patch matching to locate detailed texture from HR reference images which could provide fine details from similar image contents. But it suffers from difficulties in achieving good matching in the largely downscaled image space or feature space due to the ill-posed nature between LR and HR mapping. In this paper, we tackle this problem by exploiting fine details contained in reference HR images. Inspired by vector quantization (VQ), we propose a simple yet effective auto-encoder convolutional neural network (CNN) module to learn discrete representations of images. Furthermore, we propose to progressively learn pairs of cross-scale discrete feature representations using paired LR and HR reference images. The coarser scale of the discrete representation is responsible for encoding the global image structure while the paired finer scale of the discrete representation takes charge of capturing missing details in the finer image scale. During inference, continuous features of the test LR image are used as queries to retrieve finer scale discrete representations (value) by searching the nearest coarser scale discrete representations (key). Then, the queries and retrieved values are combined to progressively recover the HR image. Experimental results indicate that when compared with the state-of-the-art image SR models, the proposed method can achieve advanced performance in terms of both objective quality and subjective quality. The code will be available on URL: https://github.com/sunwj/refsr .},
  archive      = {J_TIP},
  author       = {Wanjie Sun and Zhenzhong Chen},
  doi          = {10.1109/TIP.2022.3142999},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1490-1503},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning discrete representations from reference images for large scale factor image super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards disentangling latent space for unsupervised semantic
face editing. <em>TIP</em>, <em>31</em>, 1475–1489. (<a
href="https://doi.org/10.1109/TIP.2022.3142527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial attributes in StyleGAN generated images are entangled in the latent space which makes it very difficult to independently control a specific attribute without affecting the others. Supervised attribute editing requires annotated training data which is difficult to obtain and limits the editable attributes to those with labels. Therefore, unsupervised attribute editing in an disentangled latent space is key to performing neat and versatile semantic face editing. In this paper, we present a new technique termed Structure-Texture Independent Architecture with Weight Decomposition and Orthogonal Regularization (STIA-WO) to disentangle the latent space for unsupervised semantic face editing. By applying STIA-WO to GAN, we have developed a StyleGAN termed STGAN-WO which performs weight decomposition through utilizing the style vector to construct a fully controllable weight matrix to regulate image synthesis, and employs orthogonal regularization to ensure each entry of the style vector only controls one independent feature matrix. To further disentangle the facial attributes, STGAN-WO introduces a structure-texture independent architecture which utilizes two independently and identically distributed (i.i.d.) latent vectors to control the synthesis of the texture and structure components in a disentangled way. Unsupervised semantic editing is achieved by moving the latent code in the coarse layers along its orthogonal directions to change texture related attributes or changing the latent code in the fine layers to manipulate structure related ones. We present experimental results which show that our new STGAN-WO can achieve better attribute editing than state of the art methods.},
  archive      = {J_TIP},
  author       = {Kanglin Liu and Gaofeng Cao and Fei Zhou and Bozhi Liu and Jiang Duan and Guoping Qiu},
  doi          = {10.1109/TIP.2022.3142527},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1475-1489},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards disentangling latent space for unsupervised semantic face editing},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to compare relation: Semantic alignment for
few-shot learning. <em>TIP</em>, <em>31</em>, 1462–1474. (<a
href="https://doi.org/10.1109/TIP.2022.3142530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is a fundamental and challenging problem since it requires recognizing novel categories from only a few examples. The objects for recognition have multiple variants and can locate anywhere in images. Directly comparing query images with example images can not handle content misalignment. The representation and metric for comparison are critical but challenging to learn due to the scarcity and wide variation of the samples in few-shot learning. In this paper, we present a novel semantic alignment model to compare relations, which is robust to content misalignment. We propose to add two key ingredients to existing few-shot learning frameworks for better feature and metric learning ability. First, we introduce a semantic alignment loss to align the relation statistics of the features from samples that belong to the same category. And second, local and global mutual information maximization is introduced, allowing for representations that contain locally-consistent and intra-class shared information across structural locations in an image. Furthermore, we introduce a principled approach to weigh multiple loss functions by considering the homoscedastic uncertainty of each stream. We conduct extensive experiments on several few-shot learning datasets. Experimental results show that the proposed method is capable of comparing relations with semantic alignment strategies, and achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Congqi Cao and Yanning Zhang},
  doi          = {10.1109/TIP.2022.3142530},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1462-1474},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning to compare relation: Semantic alignment for few-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring local detail perception for scene sketch semantic
segmentation. <em>TIP</em>, <em>31</em>, 1447–1461. (<a
href="https://doi.org/10.1109/TIP.2022.3142511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to explore the fine-grained perception ability of deep models for the newly proposed scene sketch semantic segmentation task. Scene sketches are abstract drawings containing multiple related objects. It plays a vital role in daily communication and human-computer interaction. The study has only recently started due to a main obstacle of the absence of large-scale datasets. The currently available dataset SketchyScene is composed of clip art-style edge maps, which lacks abstractness and diversity. To drive further research, we contribute two new large-scale datasets based on real hand-drawn object sketches. A general automatic scene sketch synthesis process is developed to assist with new dataset composition. Furthermore, we propose to enhancing local detail perception in deep models to realize accurate stroke-oriented scene sketch segmentation. Due to the inherent differences between hand-drawn sketches and natural images, extreme low-level local features of strokes are incorporated to improve detail discrimination. Stroke masks are also integrated into model training to guide the learning attention. Extensive experiments are conducted on three large-scale scene sketch datasets. Our method achieves state-of-the-art performance under four evaluation metrics and yields meaningful interpretability via visual analytics.},
  archive      = {J_TIP},
  author       = {Ce Ge and Haifeng Sun and Yi-Zhe Song and Zhanyu Ma and Jianxin Liao},
  doi          = {10.1109/TIP.2022.3142511},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1447-1461},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring local detail perception for scene sketch semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast ORB-SLAM without keypoint descriptors. <em>TIP</em>,
<em>31</em>, 1433–1446. (<a
href="https://doi.org/10.1109/TIP.2021.3136710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indirect methods for visual SLAM are gaining popularity due to their robustness to environmental variations. ORB-SLAM2 (Mur-Artal and Tardós, 2017) is a benchmark method in this domain, however, it consumes significant time for computing descriptors that never get reused unless a frame is selected as a keyframe. To overcome these problems, we present FastORB-SLAM which is light-weight and efficient as it tracks keypoints between adjacent frames without computing descriptors. To achieve this, a two stage descriptor-independent keypoint matching method is proposed based on sparse optical flow. In the first stage, we predict initial keypoint correspondences via a simple but effective motion model and then robustly establish the correspondences via pyramid-based sparse optical flow tracking. In the second stage, we leverage the constraints of the motion smoothness and epipolar geometry to refine the correspondences. In particular, our method computes descriptors only for keyframes. We test FastORB-SLAM on TUM and ICL-NUIM RGB-D datasets and compare its accuracy and efficiency to nine existing RGB-D SLAM methods. Qualitative and quantitative results show that our method achieves state-of-the-art accuracy and is about twice as fast as the ORB-SLAM2.},
  archive      = {J_TIP},
  author       = {Qiang Fu and Hongshan Yu and Xiaolong Wang and Zhengeng Yang and Yong He and Hong Zhang and Ajmal Mian},
  doi          = {10.1109/TIP.2021.3136710},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1433-1446},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast ORB-SLAM without keypoint descriptors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Target detection with unconstrained linear mixture model and
hierarchical denoising autoencoder in hyperspectral imagery.
<em>TIP</em>, <em>31</em>, 1418–1432. (<a
href="https://doi.org/10.1109/TIP.2022.3141843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imagery with very high spectral resolution provides a new insight for subtle nuances identification of similar substances. However, hyperspectral target detection faces significant challenges of intraclass dissimilarity and interclass similarity due to the unavoidable interference caused by atmosphere, illumination, and sensor noise. In order to effectively alleviate these spectral inconsistencies, this paper proposes a novel target detection method without strict assumptions on data distribution based on an unconstrained linear mixture model and deep learning. Our proposed detector firstly reduces interference via a specifically designed deep-learning-based hierarchical denoising autoencoder, and then carries out accurate detection with a two-step subspace projection, aiming at background suppression and target enhancement. Additionally, to generate representative background and reliable target samples required in the detection procedure, an efficient spatial-spectral unified endmember extraction method has been developed. Performance comparison with several state-of-the-art detection methods and further analysis on four real-world hyperspectral images demonstrate the effectiveness and efficiency of our proposed target detector.},
  archive      = {J_TIP},
  author       = {Yunsong Li and Yanzi Shi and Keyan Wang and Bobo Xi and Jiaojiao Li and Paolo Gamba},
  doi          = {10.1109/TIP.2022.3141843},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1418-1432},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Target detection with unconstrained linear mixture model and hierarchical denoising autoencoder in hyperspectral imagery},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delving deeper into pixel prior for box-supervised semantic
segmentation. <em>TIP</em>, <em>31</em>, 1406–1417. (<a
href="https://doi.org/10.1109/TIP.2022.3141878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) based on bounding box annotations has attracted considerable recent attention and has achieved promising performance. However, most of existing methods focus on generation of high-quality pseudo labels for segmented objects using box indicators, but they fail to fully explore and exploit prior from bounding box annotations, which limits performance of WSSS methods, especially for fine parts and boundaries. To overcome above issues, this paper proposes a novel Pixel-as-Instance Prior (PIP) for WSSS methods by delving deeper into pixel prior from bounding box annotations. Specifically, the proposed PIP is built on two important observations on pixels around bounding boxes. First, since objects are usually irregularity and tightly close to bounding boxes (dubbed irregular-filling prior), so each row or column of bounding boxes basically have at least one pixel belonging to foreground objects and background, respectively. Second, pixels near the bounding boxes tend to be highly ambiguous and more difficult to classify (dubbed label-ambiguity prior). To implement our PIP, a constrained loss alike multiple instance learning (MIL) and a labeling-balance loss are developed to jointly train WSSS models, which regards each pixel as a weighted positive or negative instance while considering more effective prior (i.e., irregular-filling and label-ambiguity priors) from bounding box annotations in an efficient way. Note that our PIP can be flexibly integrated with various WSSS methods, while clearly improving their performance with negligible computational overload in training stage. The experiments are conducted on most widely used PASCAL VOC 2012 and Cityscapes benchmarks, and the results show that our PIP has a good ability to improve performance of various WSSS methods, while achieving very competitive results.},
  archive      = {J_TIP},
  author       = {Tianqi Ma and Qilong Wang and Hongzhi Zhang and Wangmeng Zuo},
  doi          = {10.1109/TIP.2022.3141878},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1406-1417},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Delving deeper into pixel prior for box-supervised semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Towards low light enhancement with RAW images.
<em>TIP</em>, <em>31</em>, 1391–1405. (<a
href="https://doi.org/10.1109/TIP.2022.3140610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we make the first benchmark effort to elaborate on the superiority of using RAW images in the low light enhancement and develop a novel alternative route to utilize RAW images in a more flexible and practical way. Inspired by a full consideration on the typical image processing pipeline, we are inspired to develop a new evaluation framework, Factorized Enhancement Model ( FEM ), which decomposes the properties of RAW images into measurable factors and provides a tool for exploring how properties of RAW images affect the enhancement performance empirically. The empirical benchmark results show that the Linearity of data and Exposure Time recorded in meta-data play the most critical role, which brings distinct performance gains in various measures over the approaches taking the sRGB images as input. With the insights obtained from the benchmark results in mind, a RAW-guiding Exposure Enhancement Network (REENet) is developed, which makes trade-offs between the advantages and inaccessibility of RAW images in real applications in a way of using RAW images only in the training phase. REENet projects sRGB images into linear RAW domains to apply constraints with corresponding RAW images to reduce the difficulty of modeling training. After that, in the testing phase, our REENet does not rely on RAW images. Experimental results demonstrate not only the superiority of REENet to state-of-the-art sRGB-based methods and but also the effectiveness of the RAW guidance and all components.},
  archive      = {J_TIP},
  author       = {Haofeng Huang and Wenhan Yang and Yueyu Hu and Jiaying Liu and Ling-Yu Duan},
  doi          = {10.1109/TIP.2022.3140610},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1391-1405},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Towards low light enhancement with RAW images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fractional super-resolution of voxelized point clouds.
<em>TIP</em>, <em>31</em>, 1380–1390. (<a
href="https://doi.org/10.1109/TIP.2022.3141611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to super-resolve voxelized point clouds downsampled by a fractional factor, using lookup-tables (LUT) constructed from self-similarities from their own downsampled neighborhoods. The proposed method was developed to densify and to increase the precision of voxelized point clouds, and can be used, for example, as improve compression and rendering. We super-resolve the geometry, but we also interpolate texture by averaging colors from adjacent neighbors, for completeness. Our technique, as we understand, is the first specifically developed for intra-frame super-resolution of voxelized point clouds, for arbitrary resampling scale factors. We present extensive test results over different point clouds, showing the effectiveness of the proposed approach against baseline methods.},
  archive      = {J_TIP},
  author       = {Tomás M. Borges and Diogo C. Garcia and Ricardo L. de Queiroz},
  doi          = {10.1109/TIP.2022.3141611},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1380-1390},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fractional super-resolution of voxelized point clouds},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Feature map distillation of thin nets for low-resolution
object recognition. <em>TIP</em>, <em>31</em>, 1364–1379. (<a
href="https://doi.org/10.1109/TIP.2022.3141255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent video surveillance is an important computer vision application in natural environments. Since detected objects under surveillance are usually low-resolution and noisy, their accurate recognition represents a huge challenge. Knowledge distillation is an effective method to deal with it, but existing related work usually focuses on reducing the channel count of a student network, not feature map size. As a result, they cannot transfer “privilege information” hidden in feature maps of a wide and deep teacher network into a thin and shallow student one, leading to the latter’s poor performance. To address this issue, we propose a Feature Map Distillation (FMD) framework under which the feature map size of teacher and student networks is different. FMD consists of two main components: Feature Decoder Distillation (FDD) and Feature Map Consistency-enforcement (FMC). FDD reconstructs the shallow texture features of a thin student network to approximate the corresponding samples in a teacher network, which allows the high-resolution ones to directly guide the learning of the shallow features of the student network. FMC makes the size and direction of each deep feature map consistent between student and teacher networks, which constrains each pair of feature maps to produce the same feature distribution. FDD and FMC allow a thin student network to learn rich “privilege information” in feature maps of a wide teacher network. The overall performance of FMD is verified in multiple recognition tasks by comparing it with state-of-the-art knowledge distillation methods on low-resolution and noisy objects.},
  archive      = {J_TIP},
  author       = {Zhenhua Huang and Shunzhi Yang and MengChu Zhou and Zhetao Li and Zheng Gong and Yunwen Chen},
  doi          = {10.1109/TIP.2022.3141255},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1364-1379},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Feature map distillation of thin nets for low-resolution object recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Variational single nighttime image haze removal with a gray
haze-line prior. <em>TIP</em>, <em>31</em>, 1349–1363. (<a
href="https://doi.org/10.1109/TIP.2022.3141252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influenced by glowing effects, nighttime haze removal is a challenging ill-posed task. Existing nighttime dehazing methods usually result in glowing artifacts, color shifts, overexposure, and noise amplification. Thus, through statistical and theoretical analyses, we propose a simple and effective gray haze-line prior (GHLP) to identify accurate hazy feature areas. This prior demonstrates that haze is concentrated on the haze line in the RGB color space and can be accurately projected into the gray component in the Y channel of the YUV color space. Based on this prior, we establish a new unified nighttime haze removal framework and then decompose a nighttime hazy image into color and gray components in the YUV color space. Glowing color correction and haze removal are two important consecutive steps in the nighttime dehazing process. The glowing color correction method is designed to separately remove glow in the color component and enhance illumination in the gray component. After obtaining a refined nighttime hazy image, we propose a new structure-aware variational framework to simultaneously estimate the inverted scene radiance and the transmission in the gray component. This approach can not only recover the high-quality nighttime scene radiance but also preserve the significant structural information and intrinsic color of the scene. Quantitative and qualitative comparisons validate the excellent effectiveness of the proposed nighttime dehazing method against previous state-of-the-art methods. In addition, the proposed approach can be extended to achieve image enhancement for inclement weather scenes, such as sandstorm scenes and extreme daytime hazy scenes.},
  archive      = {J_TIP},
  author       = {Wenhui Wang and Anna Wang and Chen Liu},
  doi          = {10.1109/TIP.2022.3141252},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1349-1363},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational single nighttime image haze removal with a gray haze-line prior},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progressive transfer learning. <em>TIP</em>, <em>31</em>,
1340–1348. (<a href="https://doi.org/10.1109/TIP.2022.3141258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model fine-tuning is a widely used transfer learning approach in person Re-identification (ReID) applications, which fine-tuning a pre-trained feature extraction model into the target scenario instead of training a model from scratch. It is challenging due to the significant variations inside the target scenario, e.g., different camera viewpoint, illumination changes, and occlusion. These variations result in a gap between each mini-batch’s distribution and the whole dataset’s distribution when using mini-batch training. In this paper, we study model fine-tuning from the perspective of the aggregation and utilization of the dataset’s global information when using mini-batch training. Specifically, we introduce a novel network structure called Batch-related Convolutional Cell (BConv-Cell), which progressively collects the dataset’s global information into a latent state and uses it to rectify the extracted feature. Based on BConv-Cells, we further proposed the Progressive Transfer Learning (PTL) method to facilitate the model fine-tuning process by jointly optimizing BConv-Cells and the pre-trained ReID model. Empirical experiments show that our proposal can greatly improve the ReID model’s performance on MSMT17, Market-1501, CUHK03, and DukeMTMC-reID datasets. Moreover, we extend our proposal to the general image classification task. The experiments in several image classification benchmark datasets demonstrate that our proposal can significantly improve baseline models’ performance. The code has been released at https://github.com/ZJULearning/PTL},
  archive      = {J_TIP},
  author       = {Zhengxu Yu and Dong Shen and Zhongming Jin and Jianqiang Huang and Deng Cai and Xian-Sheng Hua},
  doi          = {10.1109/TIP.2022.3141258},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1340-1348},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Progressive transfer learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal convolutional dictionary learning. <em>TIP</em>,
<em>31</em>, 1325–1339. (<a
href="https://doi.org/10.1109/TIP.2022.3141251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional dictionary learning has become increasingly popular in signal and image processing for its ability to overcome the limitations of traditional patch-based dictionary learning. Although most studies on convolutional dictionary learning mainly focus on the unimodal case, real-world image processing tasks usually involve images from multiple modalities, e.g., visible and near-infrared (NIR) images. Thus, it is necessary to explore convolutional dictionary learning across different modalities. In this paper, we propose a novel multi-modal convolutional dictionary learning algorithm, which efficiently correlates different image modalities and fully considers neighborhood information at the image level. In this model, each modality is represented by two convolutional dictionaries, in which one dictionary is for common feature representation and the other is for unique feature representation. The model is constrained by the requirement that the convolutional sparse representations (CSRs) for the common features should be the same across different modalities, considering that these images are captured from the same scene. We propose a new training method based on the alternating direction method of multipliers (ADMM) to alternatively learn the common and unique dictionaries in the discrete Fourier transform (DFT) domain. We show that our model converges in less than 20 iterations between the convolutional dictionary updating and the CSRs calculation. The effectiveness of the proposed dictionary learning algorithm is demonstrated on various multimodal image processing tasks, achieves better performance than both dictionary learning methods and deep learning based methods with limited training data.},
  archive      = {J_TIP},
  author       = {Fangyuan Gao and Xin Deng and Mai Xu and Jingyi Xu and Pier Luigi Dragotti},
  doi          = {10.1109/TIP.2022.3141251},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1325-1339},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-modal convolutional dictionary learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting non-local priors via self-convolution for
highly-efficient image restoration. <em>TIP</em>, <em>31</em>,
1311–1324. (<a href="https://doi.org/10.1109/TIP.2022.3140918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing effective priors is critical to solving ill-posed inverse problems in image processing and computational imaging. Recent works focused on exploiting non-local similarity by grouping similar patches for image modeling, and demonstrated state-of-the-art results in many image restoration applications. However, compared to classic methods based on filtering or sparsity, non-local algorithms are more time-consuming, mainly due to the highly inefficient block matching step, i.e., distance between every pair of overlapping patches needs to be computed. In this work, we propose a novel Self-Convolution operator to exploit image non-local properties in a unified framework. We prove that the proposed Self-Convolution based formulation can generalize the commonly-used non-local modeling methods, as well as produce results equivalent to standard methods, but with much cheaper computation. Furthermore, by applying Self-Convolution, we propose an effective multi-modality image restoration scheme, which is much more efficient than conventional block matching for non-local modeling. Experimental results demonstrate that (1) Self-Convolution with fast Fourier transform implementation can significantly speed up most of the popular non-local image restoration algorithms, with two-fold to nine-fold faster block matching, and (2) the proposed online multi-modality image restoration scheme achieves superior denoising results than competing methods in both efficiency and effectiveness on RGB-NIR images. The code for this work is publicly available at https://github.com/GuoLanqing/Self-Convolution .},
  archive      = {J_TIP},
  author       = {Lanqing Guo and Zhiyuan Zha and Saiprasad Ravishankar and Bihan Wen},
  doi          = {10.1109/TIP.2022.3140918},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1311-1324},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploiting non-local priors via self-convolution for highly-efficient image restoration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Implicitly selected transform for AVS3. <em>TIP</em>,
<em>31</em>, 1298–1310. (<a
href="https://doi.org/10.1109/TIP.2021.3137659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transform coding removes redundancy by de-correlating the residual data, which plays a crucial role in video coding. Since different transform cores can adapt to different video content and residual data, multiple cores transform has been proposed to improve the coding performance. However, the overhead for representing the transform type is inevitable. This paper intensively studies the statistical characteristics of transform coefficients. Theoretical analysis shows that the implicit method in the Implicitly Selected Transform (IST) is superior to the explicit signalling method. As such, we propose the parity adjustment scheme which seamlessly cooperates with the rate-distortion optimization quantization for IST. Furthermore, we propose two combination methods, size-based and number-based, to optimize IST. Moreover, a restriction region of IST is applied to reduce the complexity. Experimental results on HPM-6.0, which is the reference software of the AVS3 video coding standard, show that our proposed method can achieve 1.76\% and 0.76\% BD-Rate savings on average under AI and RA configurations, respectively, along with negligible decoding time variations. The comparison results between the proposed method and explicit signalling method illustrate that the proposed method can achieve better coding gain than the latter. Our method has been adopted in AVS3.},
  archive      = {J_TIP},
  author       = {Yuhuai Zhang and Kai Zhang and Li Zhang and Shanshe Wang and Wen Gao},
  doi          = {10.1109/TIP.2021.3137659},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1298-1310},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Implicitly selected transform for AVS3},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning discriminative cross-modality features for RGB-d
saliency detection. <em>TIP</em>, <em>31</em>, 1285–1297. (<a
href="https://doi.org/10.1109/TIP.2022.3140606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to explore useful information from depth is the key success of the RGB-D saliency detection methods. While the RGB and depth images are from different domains, a modality gap will lead to unsatisfactory results for simple feature concatenation. Towards better performance, most methods focus on bridging this gap and designing different cross-modal fusion modules for features, while ignoring explicitly extracting some useful consistent information from them. To overcome this problem, we develop a simple yet effective RGB-D saliency detection method by learning discriminative cross-modality features based on the deep neural network. The proposed method first learns modality-specific features for RGB and depth inputs. And then we separately calculate the correlations of every pixel-pair in a cross-modality consistent way, i.e., the distribution ranges are consistent for the correlations calculated based on features extracted from RGB (RGB correlation) or depth inputs (depth correlation). From different perspectives, color or spatial, the RGB and depth correlations end up at the same point to depict how tightly each pixel-pair is related. Secondly, to complemently gather RGB and depth information, we propose a novel correlation-fusion to fuse RGB and depth correlations, resulting in a cross-modality correlation. Finally, the features are refined with both long-range cross-modality correlations and local depth correlations to predict salient maps. In which, the long-range cross-modality correlation provides context information for accurate localization, and the local depth correlation keeps good subtle structures for fine segmentation. In addition, a lightweight DepthNet is designed for efficient depth feature extraction. We solve the proposed network in an end-to-end manner. Both quantitative and qualitative experimental results demonstrate the proposed algorithm achieves favorable performance against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Fengyun Wang and Jinshan Pan and Shoukun Xu and Jinhui Tang},
  doi          = {10.1109/TIP.2022.3140606},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1285-1297},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning discriminative cross-modality features for RGB-D saliency detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust deep 3D blood vessel segmentation using structural
priors. <em>TIP</em>, <em>31</em>, 1271–1284. (<a
href="https://doi.org/10.1109/TIP.2021.3139241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has enabled significant improvements in the accuracy of 3D blood vessel segmentation. Open challenges remain in scenarios where labeled 3D segmentation maps for training are severely limited, as is often the case in practice, and in ensuring robustness to noise. Inspired by the observation that 3D vessel structures project onto 2D image slices with informative and unique edge profiles, we propose a novel deep 3D vessel segmentation network guided by edge profiles. Our network architecture comprises a shared encoder and two decoders that learn segmentation maps and edge profiles jointly. 3D context is mined in both the segmentation and edge prediction branches by employing bidirectional convolutional long-short term memory (BCLSTM) modules. 3D features from the two branches are concatenated to facilitate learning of the segmentation map. As a key contribution, we introduce new regularization terms that: a) capture the local homogeneity of 3D blood vessel volumes in the presence of biomarkers; and b) ensure performance robustness to domain-specific noise by suppressing false positive responses. Experiments on benchmark datasets with ground truth labels reveal that the proposed approach outperforms state-of-the-art techniques on standard measures such as DICE overlap and mean Intersection-over-Union. The performance gains of our method are even more pronounced when training is limited. Furthermore, the computational cost of our network inference is among the lowest compared with state-of-the-art.},
  archive      = {J_TIP},
  author       = {Xuelu Li and Raja Bala and Vishal Monga},
  doi          = {10.1109/TIP.2021.3139241},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1271-1284},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust deep 3D blood vessel segmentation using structural priors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient 3D point cloud feature learning for large-scale
place recognition. <em>TIP</em>, <em>31</em>, 1258–1270. (<a
href="https://doi.org/10.1109/TIP.2021.3136714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud based retrieval for place recognition is still a challenging problem since the drastic appearance changes of scenes due to seasonal or artificial changes in the environments. Existing deep learning based global descriptors for the retrieval task usually consume a large amount of computational resources ( $e.g$ ., memory), which may not be suitable for the cases of limited hardware resources. In this paper, we develop an efficient point cloud learning network (EPC-Net) to generate global descriptors of point clouds for place recognition. While obtaining good performance, it can greatly reduce computational memory and inference time. First, we propose a lightweight but effective neural network module, called ProxyConv, to aggregate the local geometric features of point clouds. We leverage the adjacency matrix and proxy points to simplify the original edge convolution for lower memory consumption. Then, we design a lightweight grouped VLAD network to form global descriptors for retrieval. Compared with the original VLAD network, we propose a grouped fully connected layer to decompose the high-dimensional vectors into a group of low-dimensional vectors, which can reduce the number of parameters of the network and maintain the discrimination of the feature vector. Finally, we further develop a simple version of EPC-Net, called EPC-Net-L, which consists of two ProxyConv modules and one max pooling layer to aggregate global descriptors. By distilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative global descriptors for retrieval. Extensive experiments on the Oxford dataset and three in-house datasets demonstrate that our method achieves good results with lower parameters, FLOPs, GPU memory, and shorter inference time. Our code is available at https://github.com/fpthink/EPC-Net .},
  archive      = {J_TIP},
  author       = {Le Hui and Mingmei Cheng and Jin Xie and Jian Yang and Ming-Ming Cheng},
  doi          = {10.1109/TIP.2021.3136714},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1258-1270},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient 3D point cloud feature learning for large-scale place recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NL-CALIC soft decoding using strict constrained
wide-activated recurrent residual network. <em>TIP</em>, <em>31</em>,
1243–1257. (<a href="https://doi.org/10.1109/TIP.2021.3136608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a normalized Tanh activate strategy and a lightweight wide-activate recurrent structure to solve three key challenges of the soft-decoding of near-lossless codes: 1. How to add an effective strict constrained peak absolute error (PAE) boundary to the network; 2. An end-to-end solution that is suitable for different quantization steps (compression ratios). 3. Simple structure that favors the GPU and FPGA implementation. To this end, we propose a Wide-activated Recurrent structure with a normalized Tanh activate strategy for Soft-Decoding (WRSD). Experiments demonstrate the effectiveness of the proposed WRSD technique that WRSD outperforms better than the state-of-the-art soft decoders with less than 5&amp;#x0025; number of parameters, and every computation node of WRSD requires less than 64KB storage for the parameters which can be easily cached by most of the current consumer-level GPUs. Source code is available at https://github.com/dota-109/WRSD},
  archive      = {J_TIP},
  author       = {Yi Niu and Chang Liu and Mingming Ma and Fu Li and Zhiwen Chen and Guangming Shi},
  doi          = {10.1109/TIP.2021.3136608},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1243-1257},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NL-CALIC soft decoding using strict constrained wide-activated recurrent residual network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pro-PULSE: Learning progressive encoders of latent semantics
in GANs for photo upsampling. <em>TIP</em>, <em>31</em>, 1230–1242. (<a
href="https://doi.org/10.1109/TIP.2022.3140603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art photo upsampling method, PULSE, demonstrates that a sharp, high-resolution (HR) version of a given low-resolution (LR) input can be obtained by exploring the latent space of generative models. However, mapping an extreme LR input (16 2 ) directly to an HR image (1024 2 ) is too ambiguous to preserve faithful local facial semantics. In this paper, we propose an enhanced upsampling approach, Pro-PULSE, that addresses the issues of semantic inconsistency and optimization complexity. Our idea is to learn an encoder that progressively constructs the HR latent codes in the extended $\mathcal {W}+$ latent space of StyleGAN. This design divides the complex $64\times $ upsampling problem into several steps, and therefore small-scale facial semantics can be inherited from one end to the other. In particular, we train two encoders, the base encoder maps latent vectors in $\mathcal {W}$ space and serves as a foundation of the HR latent vector, while the second scale-specific encoder performed in $\mathcal {W}+$ space gradually replaces the previous vector produced by the base encoder at each scale. This process produces intermediate side-outputs, which injects deep supervision into the training of encoder. Extensive experiments demonstrate superiorities over the latest latent space exploration methods, in terms of efficiency, quantitative quality metrics, and qualitative visual results.},
  archive      = {J_TIP},
  author       = {Yang Zhou and Yangyang Xu and Yong Du and Qiang Wen and Shengfeng He},
  doi          = {10.1109/TIP.2022.3140603},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1230-1242},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pro-PULSE: Learning progressive encoders of latent semantics in GANs for photo upsampling},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-guided image dehazing using progressive feature fusion.
<em>TIP</em>, <em>31</em>, 1217–1229. (<a
href="https://doi.org/10.1109/TIP.2022.3140609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an effective image dehazing algorithm which explores useful information from the input hazy image itself as the guidance for the haze removal. The proposed algorithm first uses a deep pre-dehazer to generate an intermediate result, and takes it as the reference image due to the clear structures it contains. To better explore the guidance information in the generated reference image, it then develops a progressive feature fusion module to fuse the features of the hazy image and the reference image. Finally, the image restoration module takes the fused features as input to use the guidance information for better clear image restoration. All the proposed modules are trained in an end-to-end fashion, and we show that the proposed deep pre-dehazer with progressive feature fusion module is able to help haze removal. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods on the widely-used dehazing benchmark datasets as well as real-world hazy images.},
  archive      = {J_TIP},
  author       = {Haoran Bai and Jinshan Pan and Xinguang Xiang and Jinhui Tang},
  doi          = {10.1109/TIP.2022.3140609},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1217-1229},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-guided image dehazing using progressive feature fusion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Video moment retrieval with cross-modal neural architecture
search. <em>TIP</em>, <em>31</em>, 1204–1216. (<a
href="https://doi.org/10.1109/TIP.2022.3140611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of video moment retrieval (VMR) is to retrieve the specific video moment from an untrimmed video, according to a textual query. It is a challenging task that requires effective modeling of complex cross-modal matching relationship. Recent efforts primarily model the cross-modal interactions by hand-crafted network architectures. Despite their effectiveness, they rely heavily on expert experience to select architectures and have numerous hyperparameters that need to be carefully tuned, which significantly limit their applications in real-world scenarios. How to design flexible architectures for modeling cross-modal interactions with less manual effort is crucial for the task of VMR but has received limited attention so far. To address this issue, we present a novel VMR approach that automatically searches for an optimal architecture to learn cross-modal matching relationship. Specifically, we develop a cross-modal architecture searching method. It first searches for repeatable cell network architectures based on a directed acyclic graph, which performs operation sampling over a customized task-specific operation set. Then, we adaptively modulate the edge importance in the graph by a query-aware attention network, which performs edge sampling softly in the searched cell. Different from existing neural architecture search methods, our approach can effectively exploit the query information to reach query-conditioned architectures for modeling cross modal matching. Extensive experiments on three benchmark datasets show that our approach can not only significantly outperform the state-of-the-art methods but also run more efficiently and robustly than manually crafted network architectures.},
  archive      = {J_TIP},
  author       = {Xun Yang and Shanshan Wang and Jian Dong and Jianfeng Dong and Meng Wang and Tat-Seng Chua},
  doi          = {10.1109/TIP.2022.3140611},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1204-1216},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Video moment retrieval with cross-modal neural architecture search},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-oriented convex bilevel optimization with latent
feasibility. <em>TIP</em>, <em>31</em>, 1190–1203. (<a
href="https://doi.org/10.1109/TIP.2022.3140607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper firstly proposes a convex bilevel optimization paradigm to formulate and optimize popular learning and vision problems in real-world scenarios. Different from conventional approaches, which directly design their iteration schemes based on given problem formulation, we introduce a task-oriented energy as our latent constraint which integrates richer task information. By explicitly re- characterizing the feasibility, we establish an efficient and flexible algorithmic framework to tackle convex models with both shrunken solution space and powerful auxiliary (based on domain knowledge and data distribution of the task). In theory, we present the convergence analysis of our latent feasibility re- characterization based numerical strategy. We also analyze the stability of the theoretical convergence under computational error perturbation. Extensive numerical experiments are conducted to verify our theoretical findings and evaluate the practical performance of our method on different applications.},
  archive      = {J_TIP},
  author       = {Risheng Liu and Long Ma and Xiaoming Yuan and Shangzhi Zeng and Jin Zhang},
  doi          = {10.1109/TIP.2022.3140607},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1190-1203},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Task-oriented convex bilevel optimization with latent feasibility},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometry-aware deep video deblurring via recurrent feature
refinement. <em>TIP</em>, <em>31</em>, 1176–1189. (<a
href="https://doi.org/10.1109/TIP.2021.3137019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blurring in videos is a frequent phenomenon in real-world video data owing to camera shake or object movement at different scene depths. Hence, video deblurring is an ill-posed problem that requires understanding of geometric and temporal information. Traditional model-based optimization methods first define a degradation model and then solve an optimization problem to recover the latent frames with a variational model for additional external information, such as optical flow, segmentation, depth, or camera movement. Recent deep-learning-based approaches learn from numerous training pairs of blurred and clean latent frames, with the powerful representation ability of deep convolutional neural networks. Although deep models have achieved remarkable performances without the explicit model, existing deep methods do not utilize geometrical information as strong priors. Therefore, they cannot handle extreme blurring caused by large camera shake or scene depth variations. In this paper, we propose a geometry-aware deep video deblurring method via a recurrent feature refinement module that exploits optimization-based and deep-learning-based schemes. In addition to the off-the-shelf deep geometry estimation modules, we design an effective fusion module for geometrical information with deep video features. Specifically, similar to model-based optimization, our proposed module recurrently refines video features as well as geometrical information to restore more precise latent frames. To evaluate the effectiveness and generalization of our framework, we perform tests on eight baseline networks whose structures are motivated by the previous research. The experimental results show that our framework offers greater performances than the eight baselines and produces state-of-the-art performance on four video deblurring benchmark datasets.},
  archive      = {J_TIP},
  author       = {Taeoh Kim and Sangyoun Lee},
  doi          = {10.1109/TIP.2021.3137019},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1176-1189},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry-aware deep video deblurring via recurrent feature refinement},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quality assessment of DIBR-synthesized views based on
sparsity of difference of closings and difference of gaussians.
<em>TIP</em>, <em>31</em>, 1161–1175. (<a
href="https://doi.org/10.1109/TIP.2021.3139238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images synthesized using depth-image-based-rendering (DIBR) techniques may suffer from complex structural distortions. The goal of the primary visual cortex and other parts of brain is to reduce redundancies of input visual signal in order to discover the intrinsic image structure, and thus create sparse image representation. Human visual system (HVS) treats images on several scales and several levels of resolution when perceiving the visual scene. With an attempt to emulate the properties of HVS, we have designed the no-reference model for the quality assessment of DIBR-synthesized views. To extract a higher-order structure of high curvature which corresponds to distortion of shapes to which the HVS is highly sensitive, we define a morphological oriented Difference of Closings (DoC) operator and use it at multiple scales and resolutions. DoC operator nonlinearly removes redundancies and extracts fine grained details, texture of an image local structure and contrast to which HVS is highly sensitive. We introduce a new feature based on sparsity of DoC band. To extract perceptually important low-order structural information (edges), we use the non-oriented Difference of Gaussians (DoG) operator at different scales and resolutions. Measure of sparsity is calculated for DoG bands to get scalar features. To model the relationship between the extracted features and subjective scores, the general regression neural network (GRNN) is used. Quality predictions by the proposed DoC-DoG-GRNN model show higher compatibility with perceptual quality scores in comparison to the tested state-of-the-art metrics when evaluated on four benchmark datasets with synthesized views, IRCCyN/IVC image/video dataset, MCL-3D stereoscopic image dataset and IST image dataset.},
  archive      = {J_TIP},
  author       = {Dragana D. Sandić-Stanković and Dragan D. Kukolj and Patrick Le Callet},
  doi          = {10.1109/TIP.2021.3139238},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1161-1175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Quality assessment of DIBR-synthesized views based on sparsity of difference of closings and difference of gaussians},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Big-hypergraph factorization neural network for survival
prediction from whole slide image. <em>TIP</em>, <em>31</em>, 1149–1160.
(<a href="https://doi.org/10.1109/TIP.2021.3139229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival prediction for patients based on histopa- thological whole-slide images (WSIs) has attracted increasing attention in recent years. Due to the massive pixel data in a single WSI, fully exploiting cell-level structural information (e.g., stromal/tumor microenvironment) from the gigapixel WSI is challenging. Most of the current studies resolve the problem by sampling limited image patches to construct a graph-based model (e.g., hypergraph). However, the sampling scale is a critical bottleneck since it is a fundamental obstacle of broadening samples for transductive learning. To overcome the limitation of the sampling scale for constructing a big hypergraph model, we propose a factorization neural network that embeds the correlation among large-scale vertices and hyperedges into two low-dimensional latent semantic spaces separately, empowering the dense sampling. Thanks to the compressed low-dimensional correlation embedding, the hypergraph convolutional layers generate the high-order global representation for each WSI. To minimize the effect of the uncertainty data as well as to achieve the metric-driven learning, we also propose a multi-level ranking supervision to enable the network learning by a queue of patients on the global horizon. Extensive experiments are conducted on three public carcinoma datasets (i.e., LUSC, GBM, and NLST), and the quantitative results demonstrate the proposed method outperforms state-of-the-art methods across-the-board.},
  archive      = {J_TIP},
  author       = {Donglin Di and Jun Zhang and Fuqiang Lei and Qi Tian and Yue Gao},
  doi          = {10.1109/TIP.2021.3139229},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1149-1160},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Big-hypergraph factorization neural network for survival prediction from whole slide image},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SSL++: Improving self-supervised learning by mitigating the
proxy task-specificity problem. <em>TIP</em>, <em>31</em>, 1134–1148.
(<a href="https://doi.org/10.1109/TIP.2021.3135470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep convolutional networks (ConvNets) generally relies on a massive amount of well-labeled data, which is labor-intensive and time-consuming to collect and annotate in many scenarios. To eliminate such limitation, self-supervised learning (SSL) is recently proposed. Specifically, by solving a pre-designed proxy task, SSL is capable of capturing general-purpose features without requiring human supervision. Existing efforts focus obsessively on designing a particular proxy task but ignore the semanticity of samples that are advantageous to downstream tasks, resulting in the inherent limitation that the learned features are specific to the proxy task, namely the proxy task-specificity of features. In this work, to improve the generalizability of features learned by existing SSL methods, we present a novel self-supervised framework SSL&amp;#x002B;&amp;#x002B; to incorporate the proxy task-independent semanticity of samples into the representation learning process. Technically, SSL&amp;#x002B;&amp;#x002B; aims to leverage the complementarity, between the low-level generic features learned by a proxy task and the high-level semantic features newly learned by the generated semantic pseudo-labels, to mitigate the task-specificity and improve the generalizability of features. Extensive experiments show that SSL&amp;#x002B;&amp;#x002B; performs favorably against the state-of-the-art approaches on the established and latest SSL benchmarks.},
  archive      = {J_TIP},
  author       = {Song Chen and Jing-Hao Xue and Jianlong Chang and Jianzhong Zhang and Jufeng Yang and Qi Tian},
  doi          = {10.1109/TIP.2021.3135470},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1134-1148},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SSL&amp;#x002B;&amp;#x002B;: Improving self-supervised learning by mitigating the proxy task-specificity problem},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MetaCloth: Learning unseen tasks of dense fashion landmark
detection from a few samples. <em>TIP</em>, <em>31</em>, 1120–1133. (<a
href="https://doi.org/10.1109/TIP.2021.3131033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advanced methods for fashion landmark detection are mainly driven by training convolutional neural networks on large-scale fashion datasets, which has a large number of annotated landmarks. However, such large-scale annotations are difficult and expensive to obtain in real-world applications, thus models that can generalize well from a small amount of labelled data are desired. We investigate this problem of few-shot fashion landmark detection, where only a few labelled samples are available for an unseen task. This work proposes a novel framework named MetaCloth via meta-learning, which is able to learn unseen tasks of dense fashion landmark detection with only a few annotated samples. Unlike previous meta-learning work that focus on solving &amp;#x201C; $N$ -way $K$ -shot&amp;#x201D; tasks, where each task predicts $N$ number of classes by training with $K$ annotated samples for each class ( $N$ is fixed for all seen and unseen tasks), a task in MetaCloth detects $N$ different landmarks for different clothing categories using $K$ samples, where $N$ varies across tasks, because different clothing categories usually have various number of landmarks. Therefore, numbers of parameters are various for different seen and unseen tasks in MetaCloth. MetaCloth is carefully designed to dynamically generate different numbers of parameters for different tasks, and learn a generalizable feature extraction network from a few annotated samples with a set of good initialization parameters. Extensive experiments show that MetaCloth outperforms its counterparts by a large margin.},
  archive      = {J_TIP},
  author       = {Yuying Ge and Ruimao Zhang and Ping Luo},
  doi          = {10.1109/TIP.2021.3131033},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1120-1133},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MetaCloth: Learning unseen tasks of dense fashion landmark detection from a few samples},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting RGB-d saliency detection by leveraging unlabeled
RGB images. <em>TIP</em>, <em>31</em>, 1107–1119. (<a
href="https://doi.org/10.1109/TIP.2021.3139232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep models for RGB-D salient object detection (SOD) often requires a large number of labeled RGB-D images. However, RGB-D data is not easily acquired, which limits the development of RGB-D SOD techniques. To alleviate this issue, we present a Dual-Semi RGB-D Salient Object Detection Network (DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency detection. We first devise a depth decoupling convolutional neural network (DDCNN), which contains a depth estimation branch and a saliency detection branch. The depth estimation branch is trained with RGB-D images and then used to estimate the pseudo depth maps for all unlabeled RGB images to form the paired data. The saliency detection branch is used to fuse the RGB feature and depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned as the backbone in a teacher-student framework for semi-supervised learning. Moreover, we also introduce a consistency loss on the intermediate attention and saliency maps for the unlabeled data, as well as a supervised depth and saliency loss for labeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art methods both quantitatively and qualitatively. We also demonstrate that our semi-supervised DS-Net can further improve the performance, even when using an RGB image with the pseudo depth map.},
  archive      = {J_TIP},
  author       = {Xiaoqiang Wang and Lei Zhu and Siliang Tang and Huazhu Fu and Ping Li and Fei Wu and Yi Yang and Yueting Zhuang},
  doi          = {10.1109/TIP.2021.3139232},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1107-1119},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Boosting RGB-D saliency detection by leveraging unlabeled RGB images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multi-scale feature learning for defocus blur
estimation. <em>TIP</em>, <em>31</em>, 1097–1106. (<a
href="https://doi.org/10.1109/TIP.2021.3139243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an edge-based defocus blur estimation method from a single defocused image. We first distinguish edges that lie at depth discontinuities (called depth edges, for which the blur estimate is ambiguous) from edges that lie at approximately constant depth regions (called pattern edges, for which the blur estimate is well-defined). Then, we estimate the defocus blur amount at pattern edges only, and explore an interpolation scheme based on guided filters that prevents data propagation across the detected depth edges to obtain a dense blur map with well-defined object boundaries. Both tasks (edge classification and blur estimation) are performed by deep convolutional neural networks (CNNs) that share weights to learn meaningful local features from multi-scale patches centered at edge locations. Experiments on naturally defocused images show that the proposed method presents qualitative and quantitative results that outperform state-of-the-art (SOTA) methods, with a good compromise between running time and accuracy.},
  archive      = {J_TIP},
  author       = {Ali Karaali and Naomi Harte and Claudio R. Jung},
  doi          = {10.1109/TIP.2021.3139243},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1097-1106},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep multi-scale feature learning for defocus blur estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving face-based age estimation with attention-based
dynamic patch fusion. <em>TIP</em>, <em>31</em>, 1084–1096. (<a
href="https://doi.org/10.1109/TIP.2021.3139226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of convolutional neural networks (CNNs), recent works on face-based age estimation employ these networks as the backbone. However, state-of-the-art CNN-based methods treat each facial region equally, thus entirely ignoring the importance of some facial patches that may contain rich age-specific information. In this paper, we propose a face-based age estimation framework, called Attention-based Dynamic Patch Fusion (ADPF). In ADPF, two separate CNNs are implemented, namely the AttentionNet and the FusionNet. The AttentionNet dynamically locates and ranks age-specific patches by employing a novel Ranking-guided Multi-Head Hybrid Attention (RMHHA) mechanism. The FusionNet uses the discovered patches along with the facial image to predict the age of the subject. Since the proposed RMHHA mechanism ranks the discovered patches based on their importance, the length of the learning path of each patch in the FusionNet is proportional to the amount of information it carries (the longer, the more important). ADPF also introduces a novel diversity loss to guide the training of the AttentionNet and reduce the overlap among patches so that the diverse and important patches are discovered. Through extensive experiments, we show that our proposed framework outperforms state-of-the-art methods on several age estimation benchmark datasets.},
  archive      = {J_TIP},
  author       = {Haoyi Wang and Victor Sanchez and Chang-Tsun Li},
  doi          = {10.1109/TIP.2021.3139226},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1084-1096},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Improving face-based age estimation with attention-based dynamic patch fusion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward real-world category-level articulation pose
estimation. <em>TIP</em>, <em>31</em>, 1072–1083. (<a
href="https://doi.org/10.1109/TIP.2021.3138644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human life is populated with articulated objects. Current Category-level Articulation Pose Estimation (CAPE) methods are studied under the single-instance setting with a fixed kinematic structure for each category. Considering these limitations, we aim to study the problem of estimating part-level 6D pose for multiple articulated objects with unknown kinematic structures in a single RGB-D image, and reform this problem setting for real-world environments and suggest a CAPE-Real (CAPER) task setting. This setting allows varied kinematic structures within a semantic category, and multiple instances to co-exist in an observation of real world. To support this task, we build an articulated model repository ReArt-48 and present an efficient dataset generation pipeline, which contains Fast Articulated Object Modeling (FAOM) and Semi-Authentic MixEd Reality Technique (SAMERT). Accompanying the pipeline, we build a large-scale mixed reality dataset ReArtMix and a real world dataset ReArtVal. Accompanying the CAPER problem and the dataset, we propose an effective framework that exploits RGB-D input to estimate part-level pose for multiple instances in a single forward pass. In our method, we introduce object detection from RGB-D input to handle the multi-instance problem and segment each instance into several parts. To address the unknown kinematic structure issue, we propose an Articulation Parsing Network to analyze the structure of detected instance, and also build a Pair Articulation Pose Estimation module to estimate per-part 6D pose as well as joint property from connected part pairs. Extensive experiments demonstrate that the proposed method can achieve good performance on CAPER, CAPE and instance-level Robot Arm pose estimation problems. We believe it could serve as a strong baseline for future research on the CAPER task. The datasets and codes in our work will be made publicly available.},
  archive      = {J_TIP},
  author       = {Liu Liu and Han Xue and Wenqiang Xu and Haoyuan Fu and Cewu Lu},
  doi          = {10.1109/TIP.2021.3138644},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1072-1083},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward real-world category-level articulation pose estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive selection of reference frames for video object
segmentation. <em>TIP</em>, <em>31</em>, 1057–1071. (<a
href="https://doi.org/10.1109/TIP.2021.3137660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation is a challenging task in computer vision because the appearances of target objects might change drastically along the time in the video. To solve this problem, space-time memory (STM) networks are exploited to make use of the information from all the intermediate frames between the first frame and the current frame in the video. However, fully using the information from all the memory frames may make STM not practical for long videos. To overcome this issue, a novel method is developed in this paper to select the reference frames adaptively. First, an adaptive selection criterion is introduced to choose the reference frames with similar appearance and precise mask estimation, which can efficiently capture the rich information of the target object and overcome the challenges of appearance changes, occlusion, and model drift. Secondly, bi-matching (bi-scale and bi-direction) is conducted to obtain more robust correlations for objects of various scales and prevents multiple similar objects in the current frame from being mismatched with the same target object in the reference frame. Thirdly, a novel edge refinement technique is designed by using an edge detection network to obtain smooth edges from the outputs of edge confidence maps, where the edge confidence is quantized into ten sub-intervals to generate smooth edges step by step. Experimental results on the challenging benchmark datasets DAVIS-2016, DAVIS-2017, YouTube-VOS, and a Long-Video dataset have demonstrated the effectiveness of our proposed approach to video object segmentation.},
  archive      = {J_TIP},
  author       = {Lingyi Hong and Wei Zhang and Liangyu Chen and Wenqiang Zhang and Jianping Fan},
  doi          = {10.1109/TIP.2021.3137660},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1057-1071},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive selection of reference frames for video object segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual prior learning for blind and blended image restoration.
<em>TIP</em>, <em>31</em>, 1042–1056. (<a
href="https://doi.org/10.1109/TIP.2021.3135482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised single image restoration approach, Deep Image Prior (DIP), aims to restore images by learning enough raw image statistic priors from the corrupted observation. However, it is not uncommon that an image is contaminated by the multiple unknown distortions. Thus it is hard to disentangle the clean and the hybrid distortion signals by solely relying on image prior learning to restore the images. To overcome this problem, we propose the Dual Prior Learning (DPL) method by taking both image and distortion priors into account. DPL goes beyond DIP by considering an additional step to explicitly learn the blended distortion prior. Furthermore, to coordinate the learning of two priors and avoid them learning the same knowledge, we exploit unpaired training data to enforce a weakly supervision in an adversarial manner to encourage disentangling two priors. Extensive experiments show the effectiveness and appealing performance of the proposed DPL on restoring images with challenging unknown blended distortions.},
  archive      = {J_TIP},
  author       = {Xin Jin and Li Zhang and Chaowei Shan and Xin Li and Zhibo Chen},
  doi          = {10.1109/TIP.2021.3135482},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1042-1056},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual prior learning for blind and blended image restoration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Study of the subjective and objective quality of high motion
live streaming videos. <em>TIP</em>, <em>31</em>, 1027–1041. (<a
href="https://doi.org/10.1109/TIP.2021.3136723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video livestreaming is gaining prevalence among video streaming service s, especially for the delivery of live, high motion content such as sport ing events. The quality of the se livestreaming videos can be adversely affected by any of a wide variety of events, including capture artifacts, and distortions incurred during coding and transmission. High motion content can cause or exacerbate many kinds of distortion, such as motion blur and stutter. Because of this, the development of objective Video Quality Assessment (VQA) algorithms that can predict the perceptual quality of high motion, live streamed videos is greatly desired. Important resources for developing these algorithms are appropriate databases that exemplify the kinds of live streaming video distortions encountered in practice. Towards making progress in this direction, we built a video quality database specifically designed for live streaming VQA research. The new video database is called the Laboratory for Image and Video Engineering (LIVE) Livestream Database. The LIVE Livestream Database includes 315 videos of 45 source sequences from 33 original contents impaired by 6 types of distortions. We also performed a subjective quality study using the new database, whereby more than 12,000 human opinions were gathered from 40 subjects. We demonstrate the usefulness of the new resource by performing a holistic evaluation of the performance of current state-of-the-art (SOTA) VQA models. We envision that researchers will find the dataset to be useful for the development, testing, and comparison of future VQA models. The LIVE Livestream database is being made publicly available for these purposes at https://live.ece . utexas.edu/research/LIVE_APV_Study/apv_index.html},
  archive      = {J_TIP},
  author       = {Zaixi Shang and Joshua Peter Ebenezer and Yongjun Wu and Hai Wei and Sriram Sethuraman and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3136723},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1027-1041},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Study of the subjective and objective quality of high motion live streaming videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangling task-oriented representations for unsupervised
domain adaptation. <em>TIP</em>, <em>31</em>, 1012–1026. (<a
href="https://doi.org/10.1109/TIP.2021.3136615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance.},
  archive      = {J_TIP},
  author       = {Pingyang Dai and Peixian Chen and Qiong Wu and Xiaopeng Hong and Qixiang Ye and Qi Tian and Chia-Wen Lin and Rongrong Ji},
  doi          = {10.1109/TIP.2021.3136615},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1012-1026},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Disentangling task-oriented representations for unsupervised domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometry sensitive cross-modal reasoning for composed query
based image retrieval. <em>TIP</em>, <em>31</em>, 1000–1011. (<a
href="https://doi.org/10.1109/TIP.2021.3138302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed Query Based Image Retrieval ( CQBIR ) aims at retrieving images relevant to a composed query containing a reference image with a requested modification expressed via a textual sentence. Compared with the conventional image retrieval which takes one modality as query to retrieve relevant data of another modality, CQBIR poses great challenge over the semantic gap between the reference image and modification text in the composed query. To solve the challenge, previous methods either resort to feature composition that cannot model interactions in the query or explore inter-modal attention while ignoring the spatial structure and visual-semantic relationship. In this paper, we propose a geometry sensitive cross-modal reasoning network for CQBIR by jointly modeling the geometric information of the image and the visual-semantic relationship between the reference image and modification text in the query. Specifically, it contains two key components: a geometry sensitive inter-modal attention module (GS-IMA) and a text-guided visual reasoning module (TG-VR). The GS-IMA introduces the spatial structure into the inter-modal attention in both implicit and explicit manners. The TG-VR models the unequal semantics not included in the reference image to guide further visual reasoning. As a result, our method can learn effective feature for the composed query which does not exhibit literal alignment. Comprehensive experimental results on three standard benchmarks demonstrate that the proposed model performs favorably against state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Feifei Zhang and Mingliang Xu and Changsheng Xu},
  doi          = {10.1109/TIP.2021.3138302},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1000-1011},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geometry sensitive cross-modal reasoning for composed query based image retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensor completion via complementary global, local, and
nonlocal priors. <em>TIP</em>, <em>31</em>, 984–999. (<a
href="https://doi.org/10.1109/TIP.2021.3138325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Completing missing entries in multidimensional visual data is a typical ill-posed problem that requires appropriate exploitation of prior information of the underlying data. Commonly used priors can be roughly categorized into three classes: global tensor low-rankness, local properties, and nonlocal self-similarity (NSS); most existing works utilize one or two of them to implement completion. Naturally, there arises an interesting question: can one concurrently make use of multiple priors in a unified way, such that they can collaborate with each other to achieve better performance? This work gives a positive answer by formulating a novel tensor completion framework which can simultaneously take advantage of the global-local-nonlocal priors. In the proposed framework, the tensor train (TT) rank is adopted to characterize the global correlation; meanwhile, two Plug-and-Play (PnP) denoisers, including a convolutional neural network (CNN) denoiser and the color block-matching and 3 D filtering (CBM3D) denoiser, are incorporated to preserve local details and exploit NSS, respectively. Then, we design a proximal alternating minimization algorithm to efficiently solve this model under the PnP framework. Under mild conditions, we establish the convergence guarantee of the proposed algorithm. Extensive experiments show that these priors organically benefit from each other to achieve state-of-the-art performance both quantitatively and qualitatively.},
  archive      = {J_TIP},
  author       = {Xi-Le Zhao and Jing-Hua Yang and Tian-Hui Ma and Tai-Xiang Jiang and Michael K. Ng and Ting-Zhu Huang},
  doi          = {10.1109/TIP.2021.3138325},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {984-999},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Tensor completion via complementary global, local, and nonlocal priors},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end rate-distortion optimized learned hierarchical
bi-directional video compression. <em>TIP</em>, <em>31</em>, 974–983.
(<a href="https://doi.org/10.1109/TIP.2021.3138300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional video compression (VC) methods are based on motion compensated transform coding, and the steps of motion estimation, mode and quantization parameter selection, and entropy coding are optimized individually due to the combinatorial nature of the end-to-end optimization problem. Learned VC allows end-to-end rate-distortion (R-D) optimized training of nonlinear transform, motion and entropy model simultaneously. Most works on learned VC consider end-to-end optimization of a sequential video codec based on R-D loss averaged over pairs of successive frames. It is well-known in conventional VC that hierarchical, bi-directional coding outperforms sequential compression because of its ability to use both past and future reference frames. This paper proposes a learned hierarchical bi-directional video codec (LHBDC) that combines the benefits of hierarchical motion-compensated prediction and end-to-end optimization. Experimental results show that we achieve the best R-D results that are reported for learned VC schemes to date in both PSNR and MS-SSIM. Compared to conventional video codecs, the R-D performance of our end-to-end optimized codec outperforms those of both x265 and SVT-HEVC encoders (“veryslow” preset) in PSNR and MS-SSIM as well as HM 16.23 reference software in MS-SSIM. We present ablation studies showing performance gains due to proposed novel tools such as learned masking, flow-field subsampling, and temporal flow vector prediction. The models and instructions to reproduce our results can be found in https://github.com/makinyilmaz/LHBDC/ .},
  archive      = {J_TIP},
  author       = {M. Akın Yılmaz and A. Murat Tekalp},
  doi          = {10.1109/TIP.2021.3138300},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {974-983},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {End-to-end rate-distortion optimized learned hierarchical bi-directional video compression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defending against multiple and unforeseen adversarial
videos. <em>TIP</em>, <em>31</em>, 962–973. (<a
href="https://doi.org/10.1109/TIP.2021.3137648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial robustness of deep neural networks has been actively investigated. However, most existing defense approaches are limited to a specific type of adversarial perturbations. Specifically, they often fail to offer resistance to multiple attack types simultaneously, i.e., they lack multi-perturbation robustness. Furthermore, compared to image recognition problems, the adversarial robustness of video recognition models is relatively unexplored. While several studies have proposed how to generate adversarial videos, only a handful of approaches about defense strategies have been published in the literature. In this paper, we propose one of the first defense strategies against multiple types of adversarial videos for video recognition. The proposed method, referred to as MultiBN, performs adversarial training on multiple adversarial video types using multiple independent batch normalization (BN) layers with a learning-based BN selection module. With a multiple BN structure, each BN brach is responsible for learning the distribution of a single perturbation type and thus provides more precise distribution estimations. This mechanism benefits dealing with multiple perturbation types. The BN selection module detects the attack type of an input video and sends it to the corresponding BN branch, making MultiBN fully automatic and allowing end-to-end training. Compared to present adversarial training approaches, the proposed MultiBN exhibits stronger multi-perturbation robustness against different and even unforeseen adversarial video types, ranging from Lp-bounded attacks and physically realizable attacks. This holds true on different datasets and target models. Moreover, we conduct an extensive analysis to study the properties of the multiple BN structure.},
  archive      = {J_TIP},
  author       = {Shao-Yuan Lo and Vishal M. Patel},
  doi          = {10.1109/TIP.2021.3137648},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {962-973},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Defending against multiple and unforeseen adversarial videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep listwise triplet hashing for fine-grained image
retrieval. <em>TIP</em>, <em>31</em>, 949–961. (<a
href="https://doi.org/10.1109/TIP.2021.3137653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is a practical approach for the approximate nearest neighbor search. Deep hashing methods, which train deep networks to generate compact and similarity-preserving binary codes for entities (e.g. images), have received lots of attention in the information retrieval community. A representative stream of deep hashing methods is triplet-based hashing that learns hashing models from triplets of data. The existing triplet-based hashing methods only consider triplets that are in the form of $(q,q^{+},q^{-})$ , where $q$ and $q^{+}$ are in the same class and $q$ and $q^{-}$ are in different classes. However, the number of possible triplets is approximately the cube of training examples, triplets used in the existing methods are only a small fraction of all possible triplets. This motivates us to develop a new triplet-based hashing method that adopts many more triplets in training phase. We propose Deep Listwise Triplet Hashing (DLTH) that introduces more triplets into batch-based training and a novel listwise triplet loss to capture the relative similarity in new triplets. This method has a pipeline of two steps. In Step 1, we propose a novel way to generate triplets from the soft class labels obtained by knowledge distillation module, where the triplets in the form of $(q,q^{+},q^{-})$ are a subset of the newly obtained triplets. In Step 2, we develop a novel listwise triplet loss to train the hashing network, which seeks to capture the relative similarity between images in triplets according to soft labels. We conduct comprehensive image retrieval experiments on four benchmark datasets. The experimental results show that the proposed method has superior performances over state-of-the-art baselines.},
  archive      = {J_TIP},
  author       = {Yuchen Liang and Yan Pan and Hanjiang Lai and Wei Liu and Jian Yin},
  doi          = {10.1109/TIP.2021.3137653},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {949-961},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep listwise triplet hashing for fine-grained image retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A subjective and objective study of space-time subsampled
video quality. <em>TIP</em>, <em>31</em>, 934–948. (<a
href="https://doi.org/10.1109/TIP.2021.3137658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video dimensions are continuously increasing to provide more realistic and immersive experiences to global streaming and social media viewers. However, increments in video parameters such as spatial resolution and frame rate are inevitably associated with larger data volumes. Transmitting increasingly voluminous videos through limited bandwidth networks in a perceptually optimal way is a current challenge affecting billions of viewers. One recent practice adopted by video service providers is space-time resolution adaptation in conjunction with video compression. Consequently, it is important to understand how different levels of space-time subsampling and compression affect the perceptual quality of videos. Towards making progress in this direction, we constructed a large new resource, called the ETRI-LIVE Space-Time Subsampled Video Quality (ETRI-LIVE STSVQ) database, containing 437 videos generated by applying various levels of combined space-time subsampling and video compression on 15 diverse video contents. We also conducted a large-scale human study on the new dataset, collecting about 15,000 subjective judgments of video quality. We provide a rate-distortion analysis of the collected subjective scores, enabling us to investigate the perceptual impact of space-time subsampling at different bit rates. We also evaluated and compare the performance of leading video quality models on the new database. The new ETRI-LIVE STSVQ database is being made freely available at ( https://live.ece.utexas.edu/research/ETRI-LIVE_STSVQ/index.html ).},
  archive      = {J_TIP},
  author       = {Dae Yeol Lee and Somdyuti Paul and Christos G. Bampis and Hyunsuk Ko and Jongho Kim and Se Yoon Jeong and Blake Homan and Alan C. Bovik},
  doi          = {10.1109/TIP.2021.3137658},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {934-948},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A subjective and objective study of space-time subsampled video quality},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unified pansharpening model based on band-adaptive
gradient and detail correction. <em>TIP</em>, <em>31</em>, 918–933. (<a
href="https://doi.org/10.1109/TIP.2021.3137020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening is used to fuse a panchromatic (PAN) image with a multispectral (MS) image to obtain a high-spatial-resolution multispectral (HRMS) image. Traditional pansharpening methods face difficulties in obtaining accurate details and have low computational efficiency. In this study, a unified pansharpening model based on the band-adaptive gradient and detail correction is proposed. First, a spectral fidelity constraint is designed by keeping each band of the HRMS image consistent with that of the MS image. Then, a band-adaptive gradient correction model is constructed by exploring the gradient relationship between a PAN image and each band of the MS image, so as to adaptively obtain an accurate spatial structure for the estimated HRMS image. To refine the spatial details, a detail correction constraint is defined based on the parameter transfer by designing a reduced-scale parameter acquisition model. Finally, a unified model is constructed based on the gradient and detail corrections, which is then solved by an alternating direction multiplier method. Both reduced-scale and full-scale experiments are conducted on several datasets. Compared with state-of-the-art pansharpening methods, the proposed method can achieve the best results in terms of fusion quality and has high efficiency. Specifically, our method improves the SAM and ERGAS metrics by 17.6\% and 21.2\% respectively compared to the traditional approach with the best average values, and improves these two metrics by 4.3\% and 10.3\% respectively compared to the learning-based approach with the best average values.},
  archive      = {J_TIP},
  author       = {Hangyuan Lu and Yong Yang and Shuying Huang and Wei Tu and Weiguo Wan},
  doi          = {10.1109/TIP.2021.3137020},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {918-933},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A unified pansharpening model based on band-adaptive gradient and detail correction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limb pose aware networks for monocular 3D pose estimation.
<em>TIP</em>, <em>31</em>, 906–917. (<a
href="https://doi.org/10.1109/TIP.2021.3136613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of monocular 3D pose estimation, the estimation errors of limb joints (i.e., wrist, ankle, etc) with a higher degree of freedom(DOF) are larger than that of others (i.e., hip, thorax, etc). Specifically, errors may accumulate along the physiological structure of human body parts, and trajectories of joints with higher DOF bring in higher complexity. To address this problem, we propose a limb pose aware framework, involving a kinematic constraint aware network as well as a trajectory aware temporal module, to improve the 3D prediction accuracy of limb joint positions. Two kinematic constraints named relative bone angles and absolute bone angles are introduced in this paper, the former being used for building the angular relation between adjacent bones and the latter for building the angular relation between bones and the camera plane. As a joint result of two constraints, our work suppresses errors accumulated along limbs. Furthermore, we propose a trajectory-aware network, named as Hierarchical Transformer, which takes temporal trajectories of joints as input and generates fused trajectory estimation as a result. The Hierarchical Transformer consists of Transformer Encoder blocks and aims at improving the performance of fusing temporal features. Under the effect of kinematic constraints and trajectory network, we alleviate the problem of errors accumulated along limbs and achieve promising results. Most of the off-the-shelf 2D pose estimators can be easily integrated into our framework. We perform extensive experiments on public datasets and validate the effectiveness of the framework. The ablation studies show the strength of each individual sub-module.},
  archive      = {J_TIP},
  author       = {Lele Wu and Zhenbo Yu and Yijiang Liu and Qingshan Liu},
  doi          = {10.1109/TIP.2021.3136613},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {906-917},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Limb pose aware networks for monocular 3D pose estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intra- and inter-pair consistency for semi-supervised gland
segmentation. <em>TIP</em>, <em>31</em>, 894–905. (<a
href="https://doi.org/10.1109/TIP.2021.3136716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate gland segmentation in histology tissue images is a critical but challenging task. Although deep models have demonstrated superior performance in medical image segmentation, they commonly require a large amount of annotated data, which are hard to obtain due to the extensive labor costs and expertise required. In this paper, we propose an intra- and inter-pair consistency-based semi-supervised (I 2 CS) model that can be trained on both labeled and unlabeled histology images for gland segmentation. Considering that each image contains glands and hence different images could potentially share consistent semantics in the feature space, we introduce a novel intra- and inter-pair consistency module to explore such consistency for learning with unlabeled data. It first characterizes the pixel-level relation between a pair of images in the feature space to create an attention map that highlights the regions with the same semantics but on different images. Then, it imposes a consistency constraint on the attention maps obtained from multiple image pairs, and thus filters low-confidence attention regions to generate refined attention maps that are then merged with original features to improve their representation ability. In addition, we also design an object-level loss to address the issues caused by touching glands. We evaluated our model against several recent gland segmentation methods and three typical semi-supervised methods on the GlaS and CRAG datasets. Our results not only demonstrate the effectiveness of the proposed due consistency module and Obj-Dice loss, but also indicate that the proposed I 2 CS model achieves state-of-the-art gland segmentation performance on both benchmarks.},
  archive      = {J_TIP},
  author       = {Yutong Xie and Jianpeng Zhang and Zhibin Liao and Johan Verjans and Chunhua Shen and Yong Xia},
  doi          = {10.1109/TIP.2021.3136716},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {894-905},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Intra- and inter-pair consistency for semi-supervised gland segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ECSU-net: An embedded clustering sliced u-net coupled with
fusing strategy for efficient intervertebral disc segmentation and
classification. <em>TIP</em>, <em>31</em>, 880–893. (<a
href="https://doi.org/10.1109/TIP.2021.3136619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic vertebra segmentation from computed tomography (CT) image is the very first and a decisive stage in vertebra analysis for computer-based spinal diagnosis and therapy support system. However, automatic segmentation of vertebra remains challenging due to several reasons, including anatomic complexity of spine, unclear boundaries of the vertebrae associated with spongy and soft bones. Based on 2D U-Net, we have proposed an Embedded Clustering Sliced U-Net (ECSU-Net). ECSU-Net comprises of three modules named segmentation, intervertebral disc extraction (IDE) and fusion. The segmentation module follows an instance embedding clustering approach, where our three sliced sub-nets use axis of CT images to generate a coarse 2D segmentation along with embedding space with the same size of the input slices. Our IDE module is designed to classify vertebra and find the inter-space between two slices of segmented spine. Our fusion module takes the coarse segmentation (2D) and outputs the refined 3D results of vertebra. A novel adaptive discriminative loss (ADL) function is introduced to train the embedding space for clustering. In the fusion strategy, three modules are integrated via a learnable weight control component, which adaptively sets their contribution. We have evaluated classical and deep learning methods on Spineweb dataset-2. ECSU-Net has provided comparable performance to previous neural network based algorithms achieving the best segmentation dice score of 95.60\% and classification accuracy of 96.20\%, while taking less time and computation resources.},
  archive      = {J_TIP},
  author       = {Anam Nazir and Muhammad Nadeem Cheema and Bin Sheng and Ping Li and Huating Li and Guangtao Xue and Jing Qin and Jinman Kim and David Dagan Feng},
  doi          = {10.1109/TIP.2021.3136619},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {880-893},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {ECSU-net: An embedded clustering sliced U-net coupled with fusing strategy for efficient intervertebral disc segmentation and classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). 3D layout estimation via weakly supervised learning of
plane parameters from 2D segmentation. <em>TIP</em>, <em>31</em>,
868–879. (<a href="https://doi.org/10.1109/TIP.2021.3131025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of 3D layout estimation in an indoor scene is to predict the holistic 3D structural information of the scene from an RGB image. It is costly to obtain the ground truth 3D layout, and this issue severely restricts the learning based 3D layout estimation approaches. In this paper, we present a novel weakly supervised learning framework that is able to learn the 3D layout effectively with 2D layout segmentation mask as supervision. We employ a deep neural network to predict the plane parameters and camera intrinsic parameters in the image. Based on the predicted plane instances, the 3D layout as well as the corresponding depth map and 2D segmentation can be generated. The key objectives for learning meaningful plane parameters are the label consistency of layout segmentation and depth consistency of border pixels from adjacent planes, with which the ground truth 2D layout segmentation is able to supervise the learning of the 3D layout. We further incorporate 3D geometric reasoning and prior knowledge in the learning process to ensure that the learned 3D layout is realistic and reasonable. Experimental results show that our method can produce accurate 3D layout estimates by weakly supervised learning.},
  archive      = {J_TIP},
  author       = {Weidong Zhang and Youmei Zhang and Ran Song and Ying Liu and Wei Zhang},
  doi          = {10.1109/TIP.2021.3131025},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {868-879},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3D layout estimation via weakly supervised learning of plane parameters from 2D segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep RED unfolding network for image restoration.
<em>TIP</em>, <em>31</em>, 852–867. (<a
href="https://doi.org/10.1109/TIP.2021.3136623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep unfolding network (DUN) provides an efficient framework for image restoration. It consists of a regularization module and a data fitting module. In existing DUN models, it is common to directly use a deep convolution neural network (DCNN) as the regularization module, and perform data fitting before regularization in each iteration/stage. In this work, we present a DUN by incorporating a new regularization module, and putting the regularization module before the data fitting module. The proposed regularization model is deducted by using the regularization by denoing (RED) and plugging in it a newly designed DCNN. For the data fitting module, we use the closed-form solution with Faster Fourier Transform (FFT). The resulted DRED-DUN model has some major advantages. First, the regularization model inherits the flexibility of learned image-adaptive and interpretability of RED. Second, the DRED-DUN model is an end-to-end trainable DUN, which learns the regularization network and other parameters jointly, thus leads to better restoration performance than the plug-and-play framework. Third, extensive experiments show that, our proposed model significantly outperforms the-state-of-the-art model-based methods and learning based methods in terms of PSNR indexes as well as the visual effects. In particular, our method has much better capability in recovering salient image components such as edges and small scale textures.},
  archive      = {J_TIP},
  author       = {Shengjiang Kong and Weiwei Wang and Xiangchu Feng and Xixi Jia},
  doi          = {10.1109/TIP.2021.3136623},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {852-867},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep RED unfolding network for image restoration},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SODAR: Exploring locally aggregated learning of mask
representations for instance segmentation. <em>TIP</em>, <em>31</em>,
839–851. (<a href="https://doi.org/10.1109/TIP.2021.3135717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent state-of-the-art one-stage instance segmentation model SOLO divides the input image into a grid and directly predicts per grid cell object masks with fully-convolutional networks, yielding comparably good performance as traditional two-stage Mask R-CNN yet enjoying much simpler architecture and higher efficiency. We observe SOLO generates similar masks for an object at nearby grid cells, and these neighboring predictions can complement each other as some may better segment certain object part, most of which are however directly discarded by non-maximum-suppression. Motivated by the observed gap, we develop a novel learning-based aggregation method that improves upon SOLO by leveraging the rich neighboring information while maintaining the architectural efficiency. The resulting model is named SODAR. Unlike the original per grid cell object masks, SODAR is implicitly supervised to learn mask representations that encode geometric structure of nearby objects and complement adjacent representations with context. The aggregation method further includes two novel designs: 1) a mask interpolation mechanism that enables the model to generate much fewer mask representations by sharing neighboring representations among nearby grid cells, and thus saves computation and memory; 2) a deformable neighbour sampling mechanism that allows the model to adaptively adjust neighbor sampling locations thus gathering mask representations with more relevant context and achieving higher performance. SODAR significantly improves the instance segmentation performance, e.g. , it outperforms a SOLO model with ResNet-101 backbone by 2.2 AP on COCO test set, with only about 3\% additional computation. We further show consistent performance gain with the SOLOv2 model.},
  archive      = {J_TIP},
  author       = {Tao Wang and Jun Hao Liew and Yu Li and Yunpeng Chen and Jiashi Feng},
  doi          = {10.1109/TIP.2021.3135717},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {839-851},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SODAR: Exploring locally aggregated learning of mask representations for instance segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-step registration on multi-modal retinal images via deep
neural networks. <em>TIP</em>, <em>31</em>, 823–838. (<a
href="https://doi.org/10.1109/TIP.2021.3135708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal retinal image registration plays an important role in the ophthalmological diagnosis process. The conventional methods lack robustness in aligning multi-modal images of various imaging qualities. Deep-learning methods have not been widely developed for this task, especially for the coarse-to-fine registration pipeline. To handle this task, we propose a two-step method based on deep convolutional networks, including a coarse alignment step and a fine alignment step. In the coarse alignment step, a global registration matrix is estimated by three sequentially connected networks for vessel segmentation, feature detection and description, and outlier rejection, respectively. In the fine alignment step, a deformable registration network is set up to find pixel-wise correspondence between a target image and a coarsely aligned image from the previous step to further improve the alignment accuracy. Particularly, an unsupervised learning framework is proposed to handle the difficulties of inconsistent modalities and lack of labeled training data for the fine alignment step. The proposed framework first changes multi-modal images into a same modality through modality transformers, and then adopts photometric consistency loss and smoothness loss to train the deformable registration network. The experimental results show that the proposed method achieves state-of-the-art results in Dice metrics and is more robust in challenging cases.},
  archive      = {J_TIP},
  author       = {Junkang Zhang and Yiqian Wang and Ji Dai and Melina Cavichini and Dirk-Uwe G. Bartsch and William R. Freeman and Truong Q. Nguyen and Cheolhong An},
  doi          = {10.1109/TIP.2021.3135708},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {823-838},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Two-step registration on multi-modal retinal images via deep neural networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep stereo matching with hysteresis attention and
supervised cost volume construction. <em>TIP</em>, <em>31</em>, 812–822.
(<a href="https://doi.org/10.1109/TIP.2021.3135485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching disparity prediction for rectified image pairs is of great importance to many vision tasks such as depth sensing and autonomous driving. Previous work on the end-to-end unary trained networks follows the pipeline of feature extraction, cost volume construction, matching cost aggregation, and disparity regression. In this paper, we propose a deep neural network architecture for stereo matching aiming at improving the first and second stages of the matching pipeline. Specifically, we show a network design inspired by hysteresis comparator in the circuit as our attention mechanism. Our attention module is multiple-block and generates an attentive feature directly from the input. The cost volume is constructed in a supervised way. We try to use data-driven to find a good balance between informativeness and compactness of extracted feature maps. The proposed approach is evaluated on several benchmark datasets. Experimental results demonstrate that our method outperforms previous methods on SceneFlow, KITTI 2012, and KITTI 2015 datasets.},
  archive      = {J_TIP},
  author       = {Kai Zeng and Yaonan Wang and Jianxu Mao and Caiping Liu and Weixing Peng and Yin Yang},
  doi          = {10.1109/TIP.2021.3135485},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {812-822},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep stereo matching with hysteresis attention and supervised cost volume construction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Group-wise learning for weakly supervised semantic
segmentation. <em>TIP</em>, <em>31</em>, 799–811. (<a
href="https://doi.org/10.1109/TIP.2021.3132834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring sufficient ground-truth supervision to train deep visual models has been a bottleneck over the years due to the data-hungry nature of deep learning. This is exacerbated in some structured prediction tasks, such as semantic segmentation, which require pixel-level annotations. This work addresses weakly supervised semantic segmentation (WSSS), with the goal of bridging the gap between image-level annotations and pixel-level segmentation. To achieve this, we propose, for the first time, a novel group-wise learning framework for WSSS. The framework explicitly encodes semantic dependencies in a group of images to discover rich semantic context for estimating more reliable pseudo ground-truths, which are subsequently employed to train more effective segmentation models. In particular, we solve the group-wise learning within a graph neural network (GNN), wherein input images are represented as graph nodes, and the underlying relations between a pair of images are characterized by graph edges. We then formulate semantic mining as an iterative reasoning process which propagates the common semantics shared by a group of images to enrich node representations. Moreover, in order to prevent the model from paying excessive attention to common semantics, we further propose a graph dropout layer to encourage the graph model to capture more accurate and complete object responses. With the above efforts, our model lays the foundation for more sophisticated and flexible group-wise semantic mining. We conduct comprehensive experiments on the popular PASCAL VOC 2012 and COCO benchmarks, and our model yields state-of-the-art performance. In addition, our model shows promising performance in weakly supervised object localization (WSOL) on the CUB-200-2011 dataset, demonstrating strong generalizability. Our code is available at: https://github.com/Lixy1997/Group-WSSS .},
  archive      = {J_TIP},
  author       = {Tianfei Zhou and Liulei Li and Xueyi Li and Chun-Mei Feng and Jianwu Li and Ling Shao},
  doi          = {10.1109/TIP.2021.3132834},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {799-811},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Group-wise learning for weakly supervised semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locality-aware channel-wise dropout for occluded face
recognition. <em>TIP</em>, <em>31</em>, 788–798. (<a
href="https://doi.org/10.1109/TIP.2021.3132827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition remains a challenging task in unconstrained scenarios, especially when faces are partially occluded. To improve the robustness against occlusion, augmenting the training images with artificial occlusions has been proved as a useful approach. However, these artificial occlusions are commonly generated by adding a black rectangle or several object templates including sunglasses, scarfs and phones, which cannot well simulate the realistic occlusions. In this paper, based on the argument that the occlusion essentially damages a group of neurons, we propose a novel and elegant occlusion-simulation method via dropping the activations of a group of neurons in some elaborately selected channel. Specifically, we first employ a spatial regularization to encourage each feature channel to respond to local and different face regions. Then, the locality-aware channel-wise dropout (LCD) is designed to simulate occlusions by dropping out a few feature channels. The proposed LCD can encourage its succeeding layers to minimize the intra-class feature variance caused by occlusions, thus leading to improved robustness against occlusion. In addition, we design an auxiliary spatial attention module by learning a channel-wise attention vector to reweight the feature channels, which improves the contributions of non-occluded regions. Extensive experiments on various benchmarks show that the proposed method outperforms state-of-the-art methods with a remarkable improvement.},
  archive      = {J_TIP},
  author       = {Mingjie He and Jie Zhang and Shiguang Shan and Xiao Liu and Zhongqin Wu and Xilin Chen},
  doi          = {10.1109/TIP.2021.3132827},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {788-798},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Locality-aware channel-wise dropout for occluded face recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural reference synthesis for inter frame coding.
<em>TIP</em>, <em>31</em>, 773–787. (<a
href="https://doi.org/10.1109/TIP.2021.3134465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes the neural reference synthesis (NRS) to generate high-fidelity reference block for motion estimation and motion compensation (MEMC) in inter frame coding. The NRS is comprised of two submodules: one for reconstruction enhancement and the other for reference generation. Although numerous methods have been developed in the past for these two submodules using either handcrafted rules or deep convolutional neural network (CNN) models, they basically deal with them separately, resulting in limited coding gains. By contrast, the NRS proposes to optimize them collaboratively. It first develops two CNN-based models, namely EnhNet and GenNet. The EnhNet only uses spatial correlations within the current frame for reconstruction enhancement and the GenNet is then augmented by further aggregating temporal correlations across multiple frames for reference synthesis. However, a direct concatenation of EnhNet and GenNet without considering the complex temporal reference dependency across inter frames would implicitly induce iterative CNN processing and cause the data overfitting problem, leading to visually-disturbing artifacts and oversmoothed pixels. To tackle this problem, the NRS applies a new training strategy to coordinate the EnhNet and GenNet for more robust and generalizable models, and also devises a lightweight multi-level R-D (rate-distortion) selection policy for the encoder to adaptively choose reference blocks generated from the proposed NRS model or conventional coding process. Our NRS not only offers state-of-the-art coding gains, e.g. , &gt;10\% BD-Rate (Bjøntegaard Delta Rate) reduction against the High Efficiency Video Coding (HEVC) anchor for a variety of common test video sequences encoded at a wide bit range in both low-delay and random access settings, but also greatly reduces the complexity relative to existing learning-based methods by utilizing more lightweight DNNs. All models are made publicly accessible at https://github.com/IVC-Projects/NRS for reproducible research.},
  archive      = {J_TIP},
  author       = {Dandan Ding and Xiang Gao and Chenran Tang and Zhan Ma},
  doi          = {10.1109/TIP.2021.3134465},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {773-787},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Neural reference synthesis for inter frame coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Better than reference in low-light image enhancement:
Conditional re-enhancement network. <em>TIP</em>, <em>31</em>, 759–772.
(<a href="https://doi.org/10.1109/TIP.2021.3135473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images suffer from severe noise, low brightness, low contrast, etc. In previous researches, many image enhancement methods have been proposed, but few methods can deal with these problems simultaneously. In this paper, to solve these problems simultaneously, we propose a low-light image enhancement method that can be combined with supervised learning and previous HSV (Hue, Saturation, Value) or Retinex model-based image enhancement methods. First, we analyse the relationship between the HSV color space and the Retinex theory, and show that the V channel (V channel in HSV color space, equals the maximum channel in RGB color space) of the enhanced image can well represent the contrast and brightness enhancement process. Then, a data-driven conditional re-enhancement network (denoted as CRENet) is proposed. The network takes low-light images as input and the enhanced V channel (V channel of the enhanced image) as a condition during testing, and then it can re-enhance the contrast and brightness of the low-light image and at the same time reduce noise and color distortion. In addition, it takes 23 ms to process a color image with the resolution 400*600 on a 1080Ti GPU. Finally, some comparative experiments are implemented to prove the effectiveness of the method. The results show that the method proposed in this paper can significantly improve the quality of the enhanced image, and by combining it with other image contrast enhancement methods, the final enhancement result can even be better than the reference image in contrast and brightness when the contrast and brightness of the reference are not good.},
  archive      = {J_TIP},
  author       = {Yu Zhang and Xiaoguang Di and Bin Zhang and Ruihang Ji and Chunhui Wang},
  doi          = {10.1109/TIP.2021.3135473},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {759-772},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Better than reference in low-light image enhancement: Conditional re-enhancement network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-part learning for fine-grained image classification.
<em>TIP</em>, <em>31</em>, 748–758. (<a
href="https://doi.org/10.1109/TIP.2021.3135477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent techniques have achieved remarkable improvements depended on mining subtle yet distinctive features for fine-grained visual classification (FGVC). While prior works directly combine discriminative features extracted from different parts, we argue that the potential interactions between different parts and their abilities to category predictions should be taken into consideration, which enables significant parts to contribute more to the decision of the sub-category. To this end, we present a Cross-Part Convolutional Neural Network (CP-CNN) in a weakly supervised manner to explore cross-learning among multi-regional features. Specifically, the context transformer is implemented to encourage joint feature learning across different parts under the guidance of a navigator. The part with the highest confidence is regarded as a navigator to deliver distinguishing characteristics to the others with lower confidence while the complementary information is retained. To locate discriminative but subtle parts precisely, a part proposal generator (PPG) is designed with the feature enhancement blocks, through which complex scale variations caused by the viewpoint diversity can be effectively alleviated. Extensive experiments on three benchmark datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Man Liu and Chunjie Zhang and Huihui Bai and Riquan Zhang and Yao Zhao},
  doi          = {10.1109/TIP.2021.3135477},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {748-758},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-part learning for fine-grained image classification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Adaptive rate block compressive sensing based on
statistical characteristics estimation. <em>TIP</em>, <em>31</em>,
734–747. (<a href="https://doi.org/10.1109/TIP.2021.3135476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some video compressive sensing (CS) applications, the sparsity of original signals is unknown to the sampling device. The computing power, memory space and power consumption of the sampling device are also limited, which makes it difficult to achieve adaptive rate compressive sensing (ARCS). A new blocked ARCS method for surveillance videos is proposed, which fully considers the limitations mentioned above. By observing the result of CS measurement, the statistical characteristics of the original signal are estimated. The sparsity of the original signal is reasonably estimated by using these statistical characteristics. Therefore, blocks can be divided into more classes with higher accuracy. The proposed method has the advantages of low computational complexity, small memory footprint and low power consumption, which makes it suitable for implementing in applications such as wireless video sensor networks (WVSN) and single pixel cameras (SPC). The experiment results show that the proposed method can well adapt to the change of sparsity, allocate appropriate sampling rate for each block, effectively reduce the sampling rate, and improve the quality of the reconstructed image. Meanwhile, the amount of calculation in the sampling process is much lower, and the sampling speed is obviously accelerated. The overall performance of the proposed method is better than the previous state-of-the-art method.},
  archive      = {J_TIP},
  author       = {Jianming Wang and Wei Wang and Jianhua Chen},
  doi          = {10.1109/TIP.2021.3135476},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {734-747},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive rate block compressive sensing based on statistical characteristics estimation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bidirectional mapping coupled GAN for generalized zero-shot
learning. <em>TIP</em>, <em>31</em>, 721–733. (<a
href="https://doi.org/10.1109/TIP.2021.3135480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bidirectional mapping-based generalized zero-shot learning (GZSL) methods rely on the quality of synthesized features to recognize seen and unseen data. Therefore, learning a joint distribution of seen-unseen classes and preserving the distinction between seen-unseen classes is crucial for GZSL methods. However, existing methods only learn the underlying distribution of seen data, although unseen class semantics are available in the GZSL problem setting. Most methods neglect retaining seen-unseen classes distinction and use the learned distribution to recognize seen and unseen data. Consequently, they do not perform well. In this work, we utilize the available unseen class semantics alongside seen class semantics and learn joint distribution through a strong visual-semantic coupling. We propose a bidirectional mapping coupled generative adversarial network (BMCoGAN) by extending the concept of the coupled generative adversarial network into a bidirectional mapping model. We further integrate a Wasserstein generative adversarial optimization to supervise the joint distribution learning. We design a loss optimization for retaining distinctive information of seen-unseen classes in the synthesized features and reducing bias towards seen classes, which pushes synthesized seen features towards real seen features and pulls synthesized unseen features away from real seen features. We evaluate BMCoGAN on benchmark datasets and demonstrate its superior performance against contemporary methods.},
  archive      = {J_TIP},
  author       = {Tasfia Shermin and Shyh Wei Teng and Ferdous Sohel and Manzur Murshed and Guojun Lu},
  doi          = {10.1109/TIP.2021.3135480},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {721-733},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bidirectional mapping coupled GAN for generalized zero-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EMDQ: Removal of image feature mismatches in real-time.
<em>TIP</em>, <em>31</em>, 706–720. (<a
href="https://doi.org/10.1109/TIP.2021.3134456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel method for removing image feature mismatches in real-time that can handle both rigid and smooth deforming environments. Image distortion, parallax and object deformation may cause the pixel coordinates of feature matches to have non-rigid deformations, which cannot be represented using a single analytical rigid transformation. To solve this problem, we propose an algorithm based on the re-weighting and 1-point RANSAC strategy (R1P-RNSC), which operates under the assumption that a non-rigid deformation can be approximately represented by multiple rigid transformations. R1P-RNSC is fast but suffers from the drawback that local smoothing information cannot be considered, thus limiting its accuracy. To solve this problem, we propose a non-parametric algorithm based on the expectation-maximization algorithm and the dual quaternion-based representation (EMDQ). EMDQ generates dense and smooth deformation fields by interpolating among the feature matches, simultaneously removing mismatches that are inconsistent with the deformation field. It relies on the rigid transformations obtained by R1P-RNSC to improve its accuracy. The experimental results demonstrate that EMDQ has superior accuracy compared to other state-of-the-art mismatch removal methods. The ability to build correspondences for all image pixels using the dense deformation field is another contribution of this paper.},
  archive      = {J_TIP},
  author       = {Haoyin Zhou and Jagadeesan Jayender},
  doi          = {10.1109/TIP.2021.3134456},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {706-720},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {EMDQ: Removal of image feature mismatches in real-time},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generic reversible visible watermarking via regularized
graph fourier transform coding. <em>TIP</em>, <em>31</em>, 691–705. (<a
href="https://doi.org/10.1109/TIP.2021.3134466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible visible watermarking (RVW) is an active copyright protection mechanism. It not only transparently superimposes copyright patterns on specific positions of digital images or video frames to declare the copyright ownership information, but also completely erases the visible watermark image and thus enables restoring the original host image without any distortion. However, existing RVW algorithms mostly construct the reversible mapping mechanism for a specific visible watermarking scheme, which is not versatile. Hence, we propose a generic RVW framework to accommodate various visible watermarking schemes. In particular, we obtain a reconstruction data packet—the compressed difference image between the watermarked image and the original host image, which is embedded into the watermarked image via any conventional reversible data hiding method to facilitate the blind recovery of the host image. The key is to achieve compact compression of the difference image for efficient embedding of the reconstruction data packet. To this end, we propose regularized Graph Fourier Transform (GFT) coding, where the difference image is smoothed via the graph Laplacian regularizer for more efficient compression and then encoded by multi-resolution GFTs in an approximately optimal manner. Experimental results show that the proposed framework has much better versatility than state-of-the-art methods. Due to the small amount of auxiliary information to be embedded, the visual quality of the watermarked image is also higher.},
  archive      = {J_TIP},
  author       = {Wenfa Qi and Sirui Guo and Wei Hu},
  doi          = {10.1109/TIP.2021.3134466},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {691-705},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generic reversible visible watermarking via regularized graph fourier transform coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial shape learning for building extraction in VHR
remote sensing images. <em>TIP</em>, <em>31</em>, 678–690. (<a
href="https://doi.org/10.1109/TIP.2021.3134455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building extraction in VHR RSIs remains a challenging task due to occlusion and boundary ambiguity problems. Although conventional convolutional neural networks (CNNs) based methods are capable of exploiting local texture and context information, they fail to capture the shape patterns of buildings, which is a necessary constraint in the human recognition. To address this issue, we propose an adversarial shape learning network (ASLNet) to model the building shape patterns that improve the accuracy of building segmentation. In the proposed ASLNet, we introduce the adversarial learning strategy to explicitly model the shape constraints, as well as a CNN shape regularizer to strengthen the embedding of shape features. To assess the geometric accuracy of building segmentation results, we introduced several object-based quality assessment metrics. Experiments on two open benchmark datasets show that the proposed ASLNet improves both the pixel-based accuracy and the object-based quality measurements by a large margin. The code is available at: https://github.com/ggsDing/ASLNet .},
  archive      = {J_TIP},
  author       = {Lei Ding and Hao Tang and Yahui Liu and Yilei Shi and Xiao Xiang Zhu and Lorenzo Bruzzone},
  doi          = {10.1109/TIP.2021.3134455},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {678-690},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adversarial shape learning for building extraction in VHR remote sensing images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced standard compatible image compression framework
based on auxiliary codec networks. <em>TIP</em>, <em>31</em>, 664–677.
(<a href="https://doi.org/10.1109/TIP.2021.3134473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep neural network-based research to enhance image compression performance can be divided into three categories: learnable codecs, postprocessing networks, and compact representation networks. The learnable codec has been designed for end-to-end learning beyond the conventional compression modules. The postprocessing network increases the quality of decoded images using example-based learning. The compact representation network is learned to reduce the capacity of an input image, reducing the bit rate while maintaining the quality of the decoded image. However, these approaches are not compatible with existing codecs or are not optimal for increasing coding efficiency. Specifically, it is difficult to achieve optimal learning in previous studies using a compact representation network due to the inaccurate consideration of the codecs. In this paper, we propose a novel standard compatible image compression framework based on auxiliary codec networks (ACNs). In addition, ACNs are designed to imitate image degradation operations of the existing codec, which delivers more accurate gradients to the compact representation network. Therefore, compact representation and postprocessing networks can be learned effectively and optimally. We demonstrate that the proposed framework based on the JPEG and High Efficiency Video Coding standard substantially outperforms existing image compression algorithms in a standard compatible manner.},
  archive      = {J_TIP},
  author       = {Hanbin Son and Taeoh Kim and Hyeongmin Lee and Sangyoun Lee},
  doi          = {10.1109/TIP.2021.3134473},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {664-677},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Enhanced standard compatible image compression framework based on auxiliary codec networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-resolution depth maps imaging via attention-based
hierarchical multi-modal fusion. <em>TIP</em>, <em>31</em>, 648–663. (<a
href="https://doi.org/10.1109/TIP.2021.3131041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance. The most challenging issue for guided DSR is how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhancement block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency.},
  archive      = {J_TIP},
  author       = {Zhiwei Zhong and Xianming Liu and Junjun Jiang and Debin Zhao and Zhiwen Chen and Xiangyang Ji},
  doi          = {10.1109/TIP.2021.3131041},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {648-663},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {High-resolution depth maps imaging via attention-based hierarchical multi-modal fusion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Effective prediction modes design for adaptive compression
with application in video coding. <em>TIP</em>, <em>31</em>, 636–647.
(<a href="https://doi.org/10.1109/TIP.2021.3134454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive prediction is an important tool for efficient compression of non-stationary signals. A common approach to achieve adaptivity is to switch between a set of prediction modes, designed to capture variations in signal statistics. The design poses several challenges including: i) catastrophic instability due to statistical mismatch driven by propagation through the prediction loop, and ii) severe non-convexity of the cost surface that is often riddled with poor local minima. Motivated by these challenges, this paper presents a near-optimal method for designing prediction modes for adaptive compression. The proposed method builds on a stable, open-loop platform, but with a subterfuge that ensures that the design is asymptotically optimized for closed-loop operation. The non-convexity is handled by deterministic annealing, a powerful optimization tool to avoid poor local minima. To demonstrate the impact of the proposed approach on practical applications, we consider adaptive, transform-domain predictor design for enhancing standard video coding. Experimental results validate the benefits of the proposed design in terms of significant performance gains for both predictive compression systems in general and video coding in particular.},
  archive      = {J_TIP},
  author       = {Bharath Vishwanath and Tejaswi Nanjundaswamy and Kenneth Rose},
  doi          = {10.1109/TIP.2021.3134454},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {636-647},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Effective prediction modes design for adaptive compression with application in video coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from pixel-level label noise: A new perspective for
semi-supervised semantic segmentation. <em>TIP</em>, <em>31</em>,
623–635. (<a href="https://doi.org/10.1109/TIP.2021.3134142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses semi-supervised semantic segmentation by exploiting a small set of images with pixel-level annotations (strong supervisions) and a large set of images with only image-level annotations (weak supervisions). Most existing approaches aim to generate accurate pixel-level labels from weak supervisions. However, we observe that those generated labels still inevitably contain noisy labels. Motivated by this observation, we present a novel perspective and formulate this task as a problem of learning with pixel-level label noise. Existing noisy label methods, nevertheless, mainly aim at image-level tasks, which can not capture the relationship between neighboring labels in one image. Therefore, we propose a graph-based label noise detection and correction framework to deal with pixel-level noisy labels. In particular, for the generated pixel-level noisy labels from weak supervisions by Class Activation Map (CAM), we train a clean segmentation model with strong supervisions to detect the clean labels from these noisy labels according to the cross-entropy loss. Then, we adopt a superpixel-based graph to represent the relations of spatial adjacency and semantic similarity between pixels in one image. Finally we correct the noisy labels using a Graph Attention Network (GAT) supervised by detected clean labels. We comprehensively conduct experiments on PASCAL VOC 2012, PASCAL-Context, MS-COCO and Cityscapes datasets. The experimental results show that our proposed semi-supervised method achieves the state-of-the-art performances and even outperforms the fully-supervised models on PASCAL VOC 2012 and MS-COCO datasets in some cases.},
  archive      = {J_TIP},
  author       = {Rumeng Yi and Yaping Huang and Qingji Guan and Mengyang Pu and Runsheng Zhang},
  doi          = {10.1109/TIP.2021.3134142},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {623-635},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Learning from pixel-level label noise: A new perspective for semi-supervised semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive affinity for associations in multi-target
multi-camera tracking. <em>TIP</em>, <em>31</em>, 612–622. (<a
href="https://doi.org/10.1109/TIP.2021.3131936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data associations in multi-target multi-camera tracking (MTMCT) usually estimate affinity directly from re-identification (re-ID) feature distances. However, we argue that it might not be the best choice given the difference in matching scopes between re-ID and MTMCT problems. Re-ID systems focus on global matching , which retrieves targets from all cameras and all times. In contrast, data association in tracking is a local matching problem, since its candidates only come from neighboring locations and time frames. In this paper, we design experiments to verify such misfit between global re-ID feature distances and local matching in tracking, and propose a simple yet effective approach to adapt affinity estimations to corresponding matching scopes in MTMCT. Instead of trying to deal with all appearance changes, we tailor the affinity metric to specialize in ones that might emerge during data associations. To this end, we introduce a new data sampling scheme with temporal windows originally used for data associations in tracking. Minimizing the mismatch, the adaptive affinity module brings significant improvements over global re-ID distance, and produces competitive performance on CityFlow and DukeMTMC datasets.},
  archive      = {J_TIP},
  author       = {Yunzhong Hou and Zhongdao Wang and Shengjin Wang and Liang Zheng},
  doi          = {10.1109/TIP.2021.3131936},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {612-622},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive affinity for associations in multi-target multi-camera tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Universal adversarial patch attack for automatic checkout
using perceptual and attentional bias. <em>TIP</em>, <em>31</em>,
598–611. (<a href="https://doi.org/10.1109/TIP.2021.3127849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples are inputs with imperceptible perturbations that easily mislead deep neural networks (DNNs). Recently, adversarial patch, with noise confined to a small and localized patch, has emerged for its easy feasibility in real-world scenarios. However, existing strategies failed to generate adversarial patches with strong generalization ability due to the ignorance of the inherent biases of models. In other words, the adversarial patches are always input-specific and fail to attack images from all classes or different models, especially unseen classes and black-box models. To address the problem, this paper proposes a bias-based framework to generate universal adversarial patches with strong generalization ability, which exploits the perceptual bias and attentional bias to improve the attacking ability. Regarding the perceptual bias, since DNNs are strongly biased towards textures, we exploit the hard examples which convey strong model uncertainties and extract a textural patch prior from them by adopting the style similarities. The patch prior is closer to decision boundaries and would promote attacks across classes. As for the attentional bias, motivated by the fact that different models share similar attention patterns towards the same image, we exploit this bias by confusing the model-shared similar attention patterns. Thus, the generated adversarial patches can obtain stronger transferability among different models. Taking Automatic Check-out (ACO) as the typical scenario, extensive experiments including white-box/black-box settings in both digital-world (RPC, the largest ACO related dataset) and physical-world scenario (Taobao and JD, the world’s largest online shopping platforms) are conducted. Experimental results demonstrate that our proposed framework outperforms state-of-the-art adversarial patch attack methods.},
  archive      = {J_TIP},
  author       = {Jiakai Wang and Aishan Liu and Xiao Bai and Xianglong Liu},
  doi          = {10.1109/TIP.2021.3127849},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {598-611},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Universal adversarial patch attack for automatic checkout using perceptual and attentional bias},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Person foreground segmentation by learning multi-domain
networks. <em>TIP</em>, <em>31</em>, 585–597. (<a
href="https://doi.org/10.1109/TIP.2021.3097169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separating the dominant person from the complex background is significant to the human-related research and photo-editing based applications. Existing segmentation algorithms are either too general to separate the person region accurately, or not capable of achieving real-time speed. In this paper, we introduce the multi-domain learning framework into a novel baseline model to construct the Multi-domain TriSeNet Networks for the real-time single person image segmentation. We first divide training data into different subdomains based on the characteristics of single person images, then apply a multi-branch Feature Fusion Module (FFM) to decouple the networks into the domain-independent and the domain-specific layers. To further enhance the accuracy, a self-supervised learning strategy is proposed to dig out domain relations during training. It helps transfer domain-specific knowledge by improving predictive consistency among different FFM branches. Moreover, we create a large-scale single person image segmentation dataset named MSSP20k, which consists of 22,100 pixel-level annotated images in the real world. The MSSP20k dataset is more complex and challenging than existing public ones in terms of scalability and variety. Experiments show that our Multi-domain TriSeNet outperforms state-of-the-art approaches on both public and the newly built datasets with real-time speed.},
  archive      = {J_TIP},
  author       = {Zhiyuan Liang and Kan Guo and Xiaobo Li and Xiaogang Jin and Jianbing Shen},
  doi          = {10.1109/TIP.2021.3097169},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {585-597},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Person foreground segmentation by learning multi-domain networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic neural network for lossy-to-lossless image coding.
<em>TIP</em>, <em>31</em>, 569–584. (<a
href="https://doi.org/10.1109/TIP.2021.3132825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifting-based wavelet transform has been extensively used for efficient compression of various types of visual data. Generally, the performance of such coding schemes strongly depends on the lifting operators used, namely the prediction and update filters. Unlike conventional schemes based on linear filters, we propose, in this paper, to learn these operators by exploiting neural networks. More precisely, a classical Fully Connected Neural Network (FCNN) architecture is firstly employed to perform the prediction and update. Then, we propose to improve this FCNN-based Lifting Scheme (LS) in order to better take into account the input image to be encoded. Thus, a novel dynamical FCNN model is developed, making the learning process adaptive to the input image contents for which two adaptive learning techniques are proposed. While the first one resorts to an iterative algorithm where the computation of two kinds of variables is performed in an alternating manner, the second learning method aims to learn the model parameters directly through a reformulation of the loss function. Experimental results carried out on various test images show the benefits of the proposed approaches in the context of lossy and lossless image compression.},
  archive      = {J_TIP},
  author       = {Tassnim Dardouri and Mounir Kaaniche and Amel Benazza-Benyahia and Jean-Christophe Pesquet},
  doi          = {10.1109/TIP.2021.3132825},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {569-584},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic neural network for lossy-to-lossless image coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Fast parameter-free multi-view subspace clustering with
consensus anchor guidance. <em>TIP</em>, <em>31</em>, 556–568. (<a
href="https://doi.org/10.1109/TIP.2021.3131941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering has attracted intensive attention to effectively fuse multi-view information by exploring appropriate graph structures. Although existing works have made impressive progress in clustering performance, most of them suffer from the cubic time complexity which could prevent them from being efficiently applied into large-scale applications. To improve the efficiency, anchor sampling mechanism has been proposed to select vital landmarks to represent the whole data. However, existing anchor selecting usually follows the heuristic sampling strategy, e.g. $k$ -means or uniform sampling. As a result, the procedures of anchor selecting and subsequent subspace graph construction are separated from each other which may adversely affect clustering performance. Moreover, the involved hyper-parameters further limit the application of traditional algorithms. To address these issues, we propose a novel subspace clustering method termed Fast Parameter-free Multi-view Subspace Clustering with Consensus Anchor Guidance (FPMVS-CAG). Firstly, we jointly conduct anchor selection and subspace graph construction into a unified optimization formulation. By this way, the two processes can be negotiated with each other to promote clustering quality. Moreover, our proposed FPMVS-CAG is proved to have linear time complexity with respect to the sample number. In addition, FPMVS-CAG can automatically learn an optimal anchor subspace graph without any extra hyper-parameters. Extensive experimental results on various benchmark datasets demonstrate the effectiveness and efficiency of the proposed method against the existing state-of-the-art multi-view subspace clustering competitors. These merits make FPMVS-CAG more suitable for large-scale subspace clustering. The code of FPMVS-CAG is publicly available at https://github.com/wangsiwei2010/FPMVS-CAG .},
  archive      = {J_TIP},
  author       = {Siwei Wang and Xinwang Liu and Xinzhong Zhu and Pei Zhang and Yi Zhang and Feng Gao and En Zhu},
  doi          = {10.1109/TIP.2021.3131941},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {556-568},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast parameter-free multi-view subspace clustering with consensus anchor guidance},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage copy-move forgery detection with self deep
matching and proposal SuperGlue. <em>TIP</em>, <em>31</em>, 541–555. (<a
href="https://doi.org/10.1109/TIP.2021.3132828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy-move forgery detection identifies a tampered image by detecting pasted and source regions in the same image. In this paper, we propose a novel two-stage framework specially for copy-move forgery detection. The first stage is a backbone self deep matching network, and the second stage is named as Proposal SuperGlue. In the first stage, atrous convolution and skip matching are incorporated to enrich spatial information and leverage hierarchical features. Spatial attention is built on self-correlation to reinforce the ability to find appearance similar regions. In the second stage, Proposal SuperGlue is proposed to remove false-alarmed regions and remedy incomplete regions. Specifically, a proposal selection strategy is designed to enclose highly suspected regions based on proposal generation and backbone score maps. Then, pairwise matching is conducted among candidate proposals by deep learning based keypoint extraction and matching, i.e., SuperPoint and SuperGlue. Integrated score map generation and refinement methods are designed to integrate results of both stages and obtain optimized results. Our two-stage framework unifies end-to-end deep matching and keypoint matching by obtaining highly suspected proposals, and opens a new gate for deep learning research in copy-move forgery detection. Experiments on publicly available datasets demonstrate the effectiveness of our two-stage framework.},
  archive      = {J_TIP},
  author       = {Yaqi Liu and Chao Xia and Xiaobin Zhu and Shengwei Xu},
  doi          = {10.1109/TIP.2021.3132828},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {541-555},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Two-stage copy-move forgery detection with self deep matching and proposal SuperGlue},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward scalable and unified example-based explanation and
outlier detection. <em>TIP</em>, <em>31</em>, 525–540. (<a
href="https://doi.org/10.1109/TIP.2021.3127847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When neural networks are employed for high-stakes decision-making, it is desirable that they provide explanations for their prediction in order for us to understand the features that have contributed to the decision. At the same time, it is important to flag potential outliers for in-depth verification by domain experts. In this work we propose to unify two differing aspects of explainability with outlier detection. We argue for a broader adoption of prototype-based student networks capable of providing an example-based explanation for their prediction and at the same time identify regions of similarity between the predicted sample and the examples. The examples are real prototypical cases sampled from the training set via a novel iterative prototype replacement algorithm. Furthermore, we propose to use the prototype similarity scores for identifying outliers. We compare performance in terms of the classification, explanation quality and outlier detection of our proposed network with baselines. We show that our prototype-based networks extending beyond similarity kernels deliver meaningful explanations and promising outlier detection results without compromising classification accuracy.},
  archive      = {J_TIP},
  author       = {Penny Chong and Ngai-Man Cheung and Yuval Elovici and Alexander Binder},
  doi          = {10.1109/TIP.2021.3127847},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {525-540},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward scalable and unified example-based explanation and outlier detection},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). JigsawGAN: Auxiliary learning for solving jigsaw puzzles
with generative adversarial networks. <em>TIP</em>, <em>31</em>,
513–524. (<a href="https://doi.org/10.1109/TIP.2021.3120052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a solution based on Generative Adversarial Network (GAN) for solving jigsaw puzzles. The problem assumes that an image is divided into equal square pieces, and asks to recover the image according to information provided by the pieces. Conventional jigsaw puzzle solvers often determine the relationships based on the boundaries of pieces, which ignore the important semantic information. In this paper, we propose JigsawGAN, a GAN-based auxiliary learning method for solving jigsaw puzzles with unpaired images (with no prior knowledge of the initial images). We design a multi-task pipeline that includes, (1) a classification branch to classify jigsaw permutations, and (2) a GAN branch to recover features to images in correct orders. The classification branch is constrained by the pseudo-labels generated according to the shuffled pieces. The GAN branch concentrates on the image semantic information, where the generator produces the natural images to fool the discriminator, while the discriminator distinguishes whether a given image belongs to the synthesized or the real target domain. These two branches are connected by a flow-based warp module that is applied to warp features to correct the order according to the classification results. The proposed method can solve jigsaw puzzles more efficiently by utilizing both semantic information and boundary information simultaneously. Qualitative and quantitative comparisons against several representative jigsaw puzzle solvers demonstrate the superiority of our method.},
  archive      = {J_TIP},
  author       = {Ru Li and Shuaicheng Liu and Guangfu Wang and Guanghui Liu and Bing Zeng},
  doi          = {10.1109/TIP.2021.3120052},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {513-524},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {JigsawGAN: Auxiliary learning for solving jigsaw puzzles with generative adversarial networks},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral-spatial boundary detection in hyperspectral images.
<em>TIP</em>, <em>31</em>, 499–512. (<a
href="https://doi.org/10.1109/TIP.2021.3131942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method for boundary detection in close-range hyperspectral images. This method can effectively predict the boundaries of objects of similar colour but different materials. To effectively extract the material information in the image, the spatial distribution of the spectral responses of different materials or endmembers is first estimated by hyperspectral unmixing. The resulting abundance map represents the fraction of each endmember spectra at each pixel. The abundance map is used as a supportive feature such that the spectral signature and the abundance vector for each pixel are fused to form a new spectral feature vector. Then different spectral similarity measures are adopted to construct a sparse spectral-spatial affinity matrix that characterizes the similarity between the spectral feature vectors of neighbouring pixels within a local neighborhood. After that, a spectral clustering method is adopted to produce eigenimages. Finally, the boundary map is constructed from the most informative eigenimages. We created a new HSI dataset and use it to compare the proposed method with four alternative methods, one for hyperspectral image and three for RGB image. The results exhibit that our method outperforms the alternatives and can cope with several scenarios that methods based on colour images cannot handle.},
  archive      = {J_TIP},
  author       = {Suhad Lateef Al-Khafaji and Jun Zhou and Xiao Bai and Yuntao Qian and Alan Wee-Chung Liew},
  doi          = {10.1109/TIP.2021.3131942},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {499-512},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spectral-spatial boundary detection in hyperspectral images},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CartoonLossGAN: Learning surface and coloring of images for
cartoonization. <em>TIP</em>, <em>31</em>, 485–498. (<a
href="https://doi.org/10.1109/TIP.2021.3130539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cartoonization as a special type of artistic style transfer is a difficult image processing task. The current existing artistic style transfer methods cannot generate satisfactory cartoon-style images due to that artistic style images often have delicate strokes and rich hierarchical color changes while cartoon-style images have smooth surfaces without obvious color changes, and sharp edges. To this end, we propose a cartoon loss based generative adversarial network (CartoonLossGAN) for cartoonization. Particularly, we first reuse the encoder part of the discriminator to build a compact generative adversarial network (GAN) based cartoonization architecture. Then we propose a novel cartoon loss function for the architecture. It can imitate the process of sketching to learn the smooth surface of the cartoon image, and imitate the coloring process to learn the coloring of the cartoon image. Furthermore, we also propose an initialization strategy, which is used in the scenario of reusing the discriminator to make our model training easier and more stable. Extensive experimental results demonstrate that our proposed CartoonLossGAN can generate fantastic cartoon-style images, and outperforms four representative methods.},
  archive      = {J_TIP},
  author       = {Yongsheng Dong and Wei Tan and Dacheng Tao and Lintao Zheng and Xuelong Li},
  doi          = {10.1109/TIP.2021.3130539},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {485-498},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {CartoonLossGAN: Learning surface and coloring of images for cartoonization},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable distributed hashing for approximate nearest
neighbor search. <em>TIP</em>, <em>31</em>, 472–484. (<a
href="https://doi.org/10.1109/TIP.2021.3130528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has been widely applied to the large-scale approximate nearest neighbor search problem owing to its high efficiency and low storage requirement. Most investigations concentrate on learning hashing methods in a centralized setting. However, in existing big data systems, data is often stored across different nodes. In some situations, data is even collected in a distributed manner. A straightforward way to solve this problem is to aggregate all the data into the fusion center to obtain the search result (aggregating method). However, this strategy is not feasible because of the prohibitive communication cost. Although a few distributed hashing methods have been proposed to reduce this cost, they only focus on designing a distributed algorithm for a specific global optimization objective without considering scalability. Moreover, existing distributed hashing methods aim at finding a distributed solution to hashing, meanwhile avoiding accuracy loss, rather than improving accuracy. To address these challenges, we propose a Scalable Distributed Hashing (SDisH) model in which most existing hashing methods can be extended to process distributed data with no changes. Furthermore, to improve accuracy, we utilize the search radius as a global variable across different nodes to achieve a global optimum search result for every iteration. In addition, a voting algorithm is presented based on the results produced by multiple iterations to further reduce search errors. Theoretical analyses of communication, computation, and accuracy demonstrate the superiority of the proposed model. Numerical simulations on three large-scale and two relatively small benchmark datasets also show that the SDisH model achieves up to 44.75\% and 10.23\% accuracy gains compared to the aggregating method and state-of-the-art distributed hashing methods, respectively.},
  archive      = {J_TIP},
  author       = {Yuan Cao and Junwei Liu and Heng Qi and Jie Gui and Keqiu Li and Jieping Ye and Chao Liu},
  doi          = {10.1109/TIP.2021.3130528},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {472-484},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Scalable distributed hashing for approximate nearest neighbor search},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive self-supervised pre-training for video quality
assessment. <em>TIP</em>, <em>31</em>, 458–471. (<a
href="https://doi.org/10.1109/TIP.2021.3130536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video quality assessment (VQA) task is an ongoing small sample learning problem due to the costly effort required for manual annotation. Since existing VQA datasets are of limited scale, prior research tries to leverage models pre-trained on ImageNet to mitigate this kind of shortage. Nonetheless, these well-trained models targeting on image classification task can be sub-optimal when applied on VQA data from a significantly different domain. In this paper, we make the first attempt to perform self-supervised pre-training for VQA task built upon contrastive learning method, targeting at exploiting the plentiful unlabeled video data to learn feature representation in a simple-yet-effective way. Specifically, we implement this idea by first generating distorted video samples with diverse distortion characteristics and visual contents based on the proposed distortion augmentation strategy. Afterwards, we conduct contrastive learning to capture quality-aware information by maximizing the agreement on feature representations of future frames and their corresponding predictions in the embedding space. In addition, we further introduce distortion prediction task as an additional learning objective to push the model towards discriminating different distortion categories of the input video. Solving these prediction tasks jointly with the contrastive learning not only provides stronger surrogate supervision signals, but also learns the shared knowledge among the prediction tasks. Extensive experiments demonstrate that our approach sets a new state-of-the-art in self-supervised learning for VQA task. Our results also underscore that the learned pre-trained model can significantly benefit the existing learning based VQA models. Source code is available at https://github.com/cpf0079/CSPT .},
  archive      = {J_TIP},
  author       = {Pengfei Chen and Leida Li and Jinjian Wu and Weisheng Dong and Guangming Shi},
  doi          = {10.1109/TIP.2021.3130536},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {458-471},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Contrastive self-supervised pre-training for video quality assessment},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic facial expression recognition under partial
occlusion with optical flow reconstruction. <em>TIP</em>, <em>31</em>,
446–457. (<a href="https://doi.org/10.1109/TIP.2021.3129120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video facial expression recognition is useful for many applications and received much interest lately. Although some methods give good results in controlled environments (no occlusion), recognition in the presence of partial facial occlusion remains a challenging task. To handle facial occlusions, methods based on the reconstruction of the occluded part of the face have been proposed. These methods are mainly based on the texture or the geometry of the face. However, the similarity of the face movement between different persons doing the same expression seems to be a real asset for the reconstruction. In this paper we exploit this asset and propose a new method based on an auto-encoder with skip connections to reconstruct the occluded part of the face in the optical flow domain. To the best of our knowledge, this is the first work that directly reconstructs the movement for facial expression recognition. We validated our approach in the controlled CK+ datasets on which different occlusions were generated. Our experiments show that the proposed method reduces the gap in the recognition accuracy between occluded and unoccluded situations. We also compare our approach with existing state-of-the-art approaches. In order to lay the basis of a reproducible and fair comparison in the future, we also propose a new experimental protocol that includes occlusion generation and reconstruction evaluation.},
  archive      = {J_TIP},
  author       = {Delphine Poux and Benjamin Allaert and Nacim Ihaddadene and Ioan Marius Bilasco and Chaabane Djeraba and Mohammed Bennamoun},
  doi          = {10.1109/TIP.2021.3129120},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {446-457},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic facial expression recognition under partial occlusion with optical flow reconstruction},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward unaligned guided thermal super-resolution.
<em>TIP</em>, <em>31</em>, 433–445. (<a
href="https://doi.org/10.1109/TIP.2021.3130538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermography is a useful imaging technique as it works well in poor visibility conditions. High-resolution thermal imaging sensors are usually expensive and this limits the general applicability of such imaging systems. Many thermal cameras are accompanied by a high-resolution visible-range camera, which can be used as a guide to super-resolve the low-resolution thermal images. However, the thermal and visible images form a stereo pair and the difference in their spectral range makes it very challenging to pixel-wise align the two images. The existing guided super-resolution (GSR) methods are based on aligned image pairs and hence are not appropriate for this task. In this paper, we attempt to remove the necessity of pixel-to-pixel alignment for GSR by proposing two models: the first one employs a correlation-based feature-alignment loss to reduce the misalignment in the feature-space itself and the second model includes a misalignment-map estimation block as a part of an end-to-end framework that adequately aligns the input images for performing guided super-resolution. We conduct multiple experiments to compare our methods with existing state-of-the-art single and guided super-resolution techniques and show that our models are better suited for the task of unaligned guided super-resolution from very low-resolution thermal images.},
  archive      = {J_TIP},
  author       = {Honey Gupta and Kaushik Mitra},
  doi          = {10.1109/TIP.2021.3130538},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {433-445},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward unaligned guided thermal super-resolution},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-induced generalization error bound for
information-theoretic representation learning in source-data-free
unsupervised domain adaptation. <em>TIP</em>, <em>31</em>, 419–432. (<a
href="https://doi.org/10.1109/TIP.2021.3130530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many unsupervised domain adaptation (UDA) methods have been developed and have achieved promising results in various pattern recognition tasks. However, most existing methods assume that raw source data are available in the target domain when transferring knowledge from the source to the target domain. Due to the emerging regulations on data privacy, the availability of source data cannot be guaranteed when applying UDA methods in a new domain. The lack of source data makes UDA more challenging, and most existing methods are no longer applicable. To handle this issue, this paper analyzes the cross-domain representations in source-data-free unsupervised domain adaptation (SF-UDA). A new theorem is derived to bound the target-domain prediction error using the trained source model instead of the source data. On the basis of the proposed theorem, information bottleneck theory is introduced to minimize the generalization upper bound of the target-domain prediction error, thereby achieving domain adaptation. The minimization is implemented in a variational inference framework using a newly developed latent alignment variational autoencoder (LA-VAE). The experimental results show good performance of the proposed method in several cross-dataset classification tasks without using source data. Ablation studies and feature visualization also validate the effectiveness of our method in SF-UDA.},
  archive      = {J_TIP},
  author       = {Baoyao Yang and Hao-Wei Yeh and Tatsuya Harada and Pong C. Yuen},
  doi          = {10.1109/TIP.2021.3130530},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {419-432},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Trajectory grouping with curvature regularization for
tubular structure tracking. <em>TIP</em>, <em>31</em>, 405–418. (<a
href="https://doi.org/10.1109/TIP.2021.3131940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tubular structure tracking is a crucial task in the fields of computer vision and medical image analysis. The minimal paths-based approaches have exhibited their strong ability in tracing tubular structures, by which a tubular structure can be naturally modeled as a minimal geodesic path computed with a suitable geodesic metric. However, existing minimal paths-based tracing approaches still suffer from difficulties such as the shortcuts and short branches combination problems, especially when dealing with the images involving complicated tubular tree structures or background. In this paper, we introduce a new minimal paths-based model for minimally interactive tubular structure centerline extraction in conjunction with a perceptual grouping scheme. Basically, we take into account the prescribed tubular trajectories and curvature-penalized geodesic paths to seek suitable shortest paths. The proposed approach can benefit from the local smoothness prior on tubular structures and the global optimality of the used graph-based path searching scheme. Experimental results on both synthetic and real images prove that the proposed model indeed obtains outperformance comparing with the state-of-the-art minimal paths-based tubular structure tracing algorithms.},
  archive      = {J_TIP},
  author       = {Li Liu and Da Chen and Minglei Shu and Baosheng Li and Huazhong Shu and Michel Paques and Laurent D. Cohen},
  doi          = {10.1109/TIP.2021.3131940},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {405-418},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Trajectory grouping with curvature regularization for tubular structure tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LasHeR: A large-scale high-diversity benchmark for RGBT
tracking. <em>TIP</em>, <em>31</em>, 392–404. (<a
href="https://doi.org/10.1109/TIP.2021.3130533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT tracking receives a surge of interest in the computer vision community, but this research field lacks a large-scale and high-diversity benchmark dataset, which is essential for both the training of deep RGBT trackers and the comprehensive evaluation of RGBT tracking methods. To this end, we present a La rge- ${s}$ cale ${H}$ igh-diversity $\text{b}{e}$ nchmark for short-term ${R}$ GBT tracking (LasHeR) in this work. LasHeR consists of 1224 visible and thermal infrared video pairs with more than 730K frame pairs in total. Each frame pair is spatially aligned and manually annotated with a bounding box, making the dataset well and densely annotated. LasHeR is highly diverse capturing from a broad range of object categories, camera viewpoints, scene complexities and environmental factors across seasons, weathers, day and night. We conduct a comprehensive performance evaluation of 12 RGBT tracking algorithms on the LasHeR dataset and present detailed analysis. In addition, we release the unaligned version of LasHeR to attract the research interest for alignment-free RGBT tracking, which is a more practical task in real-world applications. The datasets and evaluation protocols are available at: https://github.com/mmic-lcl/Datasets-and-benchmark-code .},
  archive      = {J_TIP},
  author       = {Chenglong Li and Wanlin Xue and Yaqing Jia and Zhichen Qu and Bin Luo and Jin Tang and Dengdi Sun},
  doi          = {10.1109/TIP.2021.3130533},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {392-404},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LasHeR: A large-scale high-diversity benchmark for RGBT tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative refining for person re-identification with
label noise. <em>TIP</em>, <em>31</em>, 379–391. (<a
href="https://doi.org/10.1109/TIP.2021.3131937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing person re-identification (Re-ID) methods usually rely heavily on large-scale thoroughly annotated training data. However, label noise is unavoidable due to inaccurate person detection results or annotation errors in real scenes. It is extremely challenging to learn a robust Re-ID model with label noise since each identity has very limited annotated training samples. To avoid fitting to the noisy labels, we propose to learn a prefatory model using a large learning rate at the early stage with a self-label refining strategy, in which the labels and network are jointly optimized. To further enhance the robustness, we introduce an online co-refining (CORE) framework with dynamic mutual learning, where networks and label predictions are online optimized collaboratively by distilling the knowledge from other peer networks. Moreover, it also reduces the negative impact of noisy labels using a favorable selective consistency strategy. CORE has two primary advantages: it is robust to different noise types and unknown noise ratios; it can be easily trained without much additional effort on the architecture design. Extensive experiments on Re-ID and image classification demonstrate that CORE outperforms its counterparts by a large margin under both practical and simulated noise settings. Notably, it also improves the state-of-the-art unsupervised Re-ID performance under standard settings. Code is available at https://github.com/mangye16/ReID-Label-Noise .},
  archive      = {J_TIP},
  author       = {Mang Ye and He Li and Bo Du and Jianbing Shen and Ling Shao and Steven C. H. Hoi},
  doi          = {10.1109/TIP.2021.3131937},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {379-391},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Collaborative refining for person re-identification with label noise},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correcting face distortion in wide-angle videos.
<em>TIP</em>, <em>31</em>, 366–378. (<a
href="https://doi.org/10.1109/TIP.2021.3131047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video blogs and selfies are popular social media formats, which are often captured by wide-angle cameras to show human subjects and expanded background. Unfortunately, due to perspective projection, faces near corners and edges exhibit apparent distortions that stretch and squish the facial features, resulting in poor video quality. In this work, we present a video warping algorithm to correct these distortions. Our key idea is to apply stereographic projection locally on the facial regions. We formulate a mesh warp problem using spatial-temporal energy minimization and minimize background deformation using a line-preservation term to maintain the straight edges in the background. To address temporal coherency, we constrain the temporal smoothness on the warping meshes and facial trajectories through the latent variables. For performance evaluation, we develop a wide-angle video dataset with a wide range of focal lengths. The user study shows that 83.9\% of users prefer our algorithm over other alternatives based on perspective projection. The video results can be found at https://www.wslai.net/publications/video_face_correction/ .},
  archive      = {J_TIP},
  author       = {Wei-Sheng Lai and Yichang Shih and Chia-Kai Liang and Ming-Hsuan Yang},
  doi          = {10.1109/TIP.2021.3131047},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {366-378},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Correcting face distortion in wide-angle videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seeing like a human: Asynchronous learning with dynamic
progressive refinement for person re-identification. <em>TIP</em>,
<em>31</em>, 352–365. (<a
href="https://doi.org/10.1109/TIP.2021.3128330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning discriminative and rich features is an important research task for person re-identification. Previous studies have attempted to capture global and local features at the same time and layer of the model in a non-interactive manner, which are called synchronous learning. However, synchronous learning leads to high similarity, and further defects in model performance. To this end, we propose asynchronous learning based on the human visual perception mechanism. Asynchronous learning emphasizes the time asynchrony and space asynchrony of feature learning and achieves mutual promotion and cyclical interaction for feature learning. Furthermore, we design a dynamic progressive refinement module to improve local features with the guidance of global features. The dynamic property allows this module to adaptively adjust the network parameters according to the input image, in both the training and testing stage. The progressive property narrows the semantic gap between the global and local features, which is due to the guidance of global features. Finally, we have conducted several experiments on four datasets, including Market1501, CUHK03, DukeMTMC-ReID, and MSMT17. The experimental results show that asynchronous learning can effectively improve feature discrimination and achieve strong performance.},
  archive      = {J_TIP},
  author       = {Quan Zhang and Jianhuang Lai and Zhanxiang Feng and Xiaohua Xie},
  doi          = {10.1109/TIP.2021.3128330},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {352-365},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Seeing like a human: Asynchronous learning with dynamic progressive refinement for person re-identification},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Two-branch attention network via efficient semantic
coupling for one-shot learning. <em>TIP</em>, <em>31</em>, 341–351. (<a
href="https://doi.org/10.1109/TIP.2021.3124668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, Convolutional Neural Networks (CNNs) have achieved remarkable advancement for the tasks of one-shot image classification. However, the lack of effective attention modeling has limited its performance. In this paper, we propose a Two-branch ( C ontent-aware and P osition-aware) A ttention (CPA) Network via an Efficient Semantic Coupling module for attention modeling. Specifically, we harness content-aware attention to model the characteristic features (e.g., color, shape, texture) as well as position-aware attention to model the spatial position weights. In addition, we exploit support images to improve the learning of attention for the query images. Similarly, we also use query images to enhance the attention model of the support set. Furthermore, we design a local-global optimizing framework that further improves the recognition accuracy. The extensive experiments on four common datasets (miniImageNet, tieredImageNet, CUB-200-2011, CIFAR-FS) with three popular networks (DPGN, RelationNet and IFSL) demonstrate that our devised CPA module equipped with local-global T wo-stream framework (CPAT) can achieve state-of-the-art performance, with a significant improvement in accuracy of 3.16\% on CUB-200-2011 in particular.},
  archive      = {J_TIP},
  author       = {Jun Li and Duorui Wang and Xianglong Liu and Zhiping Shi and Meng Wang},
  doi          = {10.1109/TIP.2021.3124668},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {341-351},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Two-branch attention network via efficient semantic coupling for one-shot learning},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-branch deconvolutional network with application in
stereo matching. <em>TIP</em>, <em>31</em>, 327–340. (<a
href="https://doi.org/10.1109/TIP.2021.3131048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deconvolutional networks have attracted extensive attention and have been successfully applied in the field of computer vision. In this paper we propose a novel two-branch deconvolutional network (TBDN) that can improve the performance of conventional deconvolutional networks and reduce the computational complexity. A feasible iterative algorithm is designed to solve the optimization problem for the TBDN model, and a theoretical analysis of the convergence and computational complexity for the algorithm is also provided. The application of the TBDN in stereo matching is presented by constructing a disparity estimation network. Extensive experimental results on four commonly used datasets demonstrate the efficiency and effectiveness of the proposed TBDN.},
  archive      = {J_TIP},
  author       = {Chunbo Cheng and Hong Li and Liming Zhang},
  doi          = {10.1109/TIP.2021.3131048},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {327-340},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Two-branch deconvolutional network with application in stereo matching},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sub-region localized hashing for fine-grained image
retrieval. <em>TIP</em>, <em>31</em>, 314–326. (<a
href="https://doi.org/10.1109/TIP.2021.3131042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image hashing is challenging due to the difficulties of capturing discriminative local information to generate hash codes. On the one hand, existing methods usually extract local features with the dense attention mechanism by focusing on dense local regions, which cannot contain diverse local information for fine-grained hashing. On the other hand, hash codes of the same class suffer from large intra-class variation of fine-grained images. To address the above problems, this work proposes a novel sub-Region Localized Hashing (sRLH) to learn intra-class compact and inter-class separable hash codes that also contain diverse subtle local information for efficient fine-grained image retrieval. Specifically, to localize diverse local regions, a sub-region localization module is developed to learn discriminative local features by locating the peaks of non-overlap sub-regions in the feature map. Different from localizing dense local regions, these peaks can guide the sub-region localization module to capture multifarious local discriminative information by paying close attention to dispersive local regions. To mitigate intra-class variations, hash codes of the same class are enforced to approach one common binary center. Meanwhile, the gram-schmidt orthogonalization is performed on the binary centers to make the hash codes inter-class separable. Extensive experimental results on four widely used fine-grained image retrieval datasets demonstrate the superiority of sRLH to several state-of-the-art methods. The source code of sRLH will be released at https://github.com/ZhangYajie-NJUST/sRLH.git .},
  archive      = {J_TIP},
  author       = {Xinguang Xiang and Yajie Zhang and Lu Jin and Zechao Li and Jinhui Tang},
  doi          = {10.1109/TIP.2021.3131042},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {314-326},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Sub-region localized hashing for fine-grained image retrieval},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust perturbation for visual explanation: Cross-checking
mask optimization to avoid class distortion. <em>TIP</em>, <em>31</em>,
301–313. (<a href="https://doi.org/10.1109/TIP.2021.3130526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the outstanding performance of the deep neural networks (DNNs), considerable research efforts have been devoted to finding ways to understand the decision of DNNs structures. In the computer vision domain, visualizing the attribution map is one of the most intuitive and understandable ways to achieve human-level interpretation. Among them, perturbation-based visualization can explain the “black box” property of the given network by optimizing perturbation masks that alter the network prediction of the target class the most. However, existing perturbation methods could make unexpected changes to network predictions after applying a perturbation mask to the input image, resulting in a loss of robustness and fidelity of the perturbation mechanisms. In this paper, we define class distortion as the unexpected changes of the network prediction during the perturbation process. To handle that, we propose a novel visual interpretation framework, Robust Perturbation, which shows robustness against the unexpected class distortion during the mask optimization. With a new cross-checking mask optimization strategy, our proposed framework perturbs the target prediction of the network while upholding the non-target predictions, providing more reliable and accurate visual explanations. We evaluate our framework on three different public datasets through extensive experiments. Furthermore, we propose a new metric for class distortion evaluation. In both quantitative and qualitative experiments, tackling the class distortion problem turns out to enhance the quality and fidelity of the visual explanation in comparison with the existing perturbation-based methods.},
  archive      = {J_TIP},
  author       = {Junho Kim and Seongyeop Kim and Seong Tae Kim and Yong Man Ro},
  doi          = {10.1109/TIP.2021.3130526},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {301-313},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust perturbation for visual explanation: Cross-checking mask optimization to avoid class distortion},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Camouflaged instance segmentation in-the-wild: Dataset,
method, and benchmark suite. <em>TIP</em>, <em>31</em>, 287–300. (<a
href="https://doi.org/10.1109/TIP.2021.3130490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper pushes the envelope on decomposing camouflaged regions in an image into meaningful components, namely, camouflaged instances. To promote the new task of camouflaged instance segmentation of in-the-wild images, we introduce a dataset, dubbed CAMO++, that extends our preliminary CAMO dataset (camouflaged object segmentation) in terms of quantity and diversity. The new dataset substantially increases the number of images with hierarchical pixel-wise ground truths. We also provide a benchmark suite for the task of camouflaged instance segmentation. In particular, we present an extensive evaluation of state-of-the-art instance segmentation methods on our newly constructed CAMO++ dataset in various scenarios. We also present a camouflage fusion learning (CFL) framework for camouflaged instance segmentation to further improve the performance of state-of-the-art methods. The dataset, model, evaluation suite, and benchmark will be made publicly available on our project page.},
  archive      = {J_TIP},
  author       = {Trung-Nghia Le and Yubo Cao and Tan-Cong Nguyen and Minh-Quan Le and Khanh-Duy Nguyen and Thanh-Toan Do and Minh-Triet Tran and Tam V. Nguyen},
  doi          = {10.1109/TIP.2021.3130490},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {287-300},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Camouflaged instance segmentation in-the-wild: Dataset, method, and benchmark suite},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational abnormal behavior detection with motion
consistency. <em>TIP</em>, <em>31</em>, 275–286. (<a
href="https://doi.org/10.1109/TIP.2021.3130545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal crowd behavior detection has recently attracted increasing attention due to its wide applications in computer vision research areas. However, it is still an extremely challenging task due to the great variability of abnormal behavior coupled with huge ambiguity and uncertainty of video contents. To tackle these challenges, we propose a new probabilistic framework named variational abnormal behavior detection (VABD), which can detect abnormal crowd behavior in video sequences. We make three major contributions: (1) We develop a new probabilistic latent variable model that combines the strengths of the U-Net and conditional variational auto-encoder, which also are the backbone of our model; (2) We propose a motion loss based on an optical flow network to impose the motion consistency of generated video frames and input video frames; (3) We embed a Wasserstein generative adversarial network at the end of the backbone network to enhance the framework performance. VABD can accurately discriminate abnormal video frames from video sequences. Experimental results on UCSD, CUHK Avenue, IITB-Corridor, and ShanghaiTech datasets show that VABD outperforms the state-of-the-art algorithms on abnormal crowd behavior detection. Without data augmentation, our VABD achieves 72.24\% in terms of AUC on IITB-Corridor, which surpasses the state-of-the-art methods by nearly 5\%.},
  archive      = {J_TIP},
  author       = {Jing Li and Qingwang Huang and Yingjun Du and Xiantong Zhen and Shengyong Chen and Ling Shao},
  doi          = {10.1109/TIP.2021.3130545},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {275-286},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Variational abnormal behavior detection with motion consistency},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Completely blind quality assessment of user generated video
content. <em>TIP</em>, <em>31</em>, 263–274. (<a
href="https://doi.org/10.1109/TIP.2021.3130541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the challenging problem of completely blind video quality assessment (BVQA) of user generated content (UGC). The challenge is twofold since the quality prediction model is oblivious of human opinion scores, and there are no well-defined distortion models for UGC content. Our solution is inspired by a recent computational neuroscience model which hypothesizes that the human visual system (HVS) transforms a natural video input to follow a straighter temporal trajectory in the perceptual domain. A bandpass filter based computational model of the lateral geniculate nucleus (LGN) and V1 regions of the HVS was used to validate the perceptual straightening hypothesis. We hypothesize that distortions in natural videos lead to loss in straightness (or increased curvature) in their transformed representations in the HVS. We provide extensive empirical evidence to validate our hypothesis. We quantify the loss in straightness as a measure of temporal quality, and show that this measure delivers acceptable quality prediction performance on its own. Further, the temporal quality measure is combined with a state-of-the-art blind spatial (image) quality metric to design a blind video quality predictor that we call STraightness Evaluation Metric (STEM). STEM is shown to deliver state-of-the-art performance over the class of BVQA algorithms on five UGC VQA datasets including KoNViD-1K, LIVE-Qualcomm, LIVE-VQC, CVD and YouTube-UGC. Importantly, our solution is completely blind i.e., training-free, generalizes very well, is explainable, has few tunable parameters, and is simple and easy to implement.},
  archive      = {J_TIP},
  author       = {Parimala Kancharla and Sumohana S. Channappayya},
  doi          = {10.1109/TIP.2021.3130541},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {263-274},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Completely blind quality assessment of user generated video content},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and accurate stitching for 360° dual-fisheye
images and videos. <em>TIP</em>, <em>31</em>, 251–262. (<a
href="https://doi.org/10.1109/TIP.2021.3130531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Back-to-back dual-fisheye cameras are the most cost-effective devices to capture 360° visual content. However, image and video stitching for such cameras often suffer from the effect of fisheye distortion, photometric inconsistency between the two views, and non-collocated optical centers. In this paper, we present algorithms for geometric calibration, photometric compensation, and seamless stitching to address these issues for back-to-back dual-fisheye cameras. Specifically, we develop a co-centric trajectory model for geometric calibration to characterize both intrinsic and extrinsic parameters of the fisheye camera to fifth-order precision, a photometric correction model for intensity and color compensation to provide efficient and accurate local color transfer, and a mesh deformation model along with an adaptive seam carving method for image stitching to reduce geometric distortion and ensure optimal spatiotemporal alignment. The stitching algorithm and the compensation algorithm can run efficiently for $1920\times 960$ images. Quantitative evaluation of geometric distortion, color discontinuity, jitter, and ghost artifact of the resulting image and video shows that our solution outperforms the state-of-the-art techniques.},
  archive      = {J_TIP},
  author       = {I-Chan Lo and Kuang-Tsu Shih and Homer H. Chen},
  doi          = {10.1109/TIP.2021.3130531},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {251-262},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Efficient and accurate stitching for 360° dual-fisheye images and videos},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Triple-level model inferred collaborative network
architecture for video deraining. <em>TIP</em>, <em>31</em>, 239–250.
(<a href="https://doi.org/10.1109/TIP.2021.3128327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video deraining is an important issue for outdoor vision systems and has been investigated extensively. However, designing optimal architectures by the aggregating model formation and data distribution is a challenging task for video deraining. In this paper, we develop a model-guided triple-level optimization framework to deduce network architecture with cooperating optimization and auto-searching mechanism, named Triple-level Model Inferred Cooperating Searching (TMICS), for dealing with various video rain circumstances. In particular, to mitigate the problem that existing methods cannot cover various rain streaks distribution, we first design a hyper-parameter optimization model about task variable and hyper-parameter. Based on the proposed optimization model, we design a collaborative structure for video deraining. This structure includes Dominant Network Architecture (DNA) and Companionate Network Architecture (CNA) that is cooperated by introducing an Attention-based Averaging Scheme (AAS). To better explore inter-frame information from videos, we introduce a macroscopic structure searching scheme that searches from Optical Flow Module (OFM) and Temporal Grouping Module (TGM) to help restore latent frame. In addition, we apply the differentiable neural architecture searching from a compact candidate set of task-specific operations to discover desirable rain streaks removal architectures automatically. Extensive experiments on various datasets demonstrate that our model shows significant improvements in fidelity and temporal consistency over the state-of-the-art works. Source code is available at https://github.com/vis-opt-group/TMICS .},
  archive      = {J_TIP},
  author       = {Pan Mu and Zhu Liu and Yaohua Liu and Risheng Liu and Xin Fan},
  doi          = {10.1109/TIP.2021.3128327},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {239-250},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Triple-level model inferred collaborative network architecture for video deraining},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loss re-scaling VQA: Revisiting the language prior problem
from a class-imbalance view. <em>TIP</em>, <em>31</em>, 227–238. (<a
href="https://doi.org/10.1109/TIP.2021.3128322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have pointed out that many well-developed Visual Question Answering (VQA) models are heavily affected by the language prior problem. It refers to making predictions based on the co-occurrence pattern between textual questions and answers instead of reasoning upon visual contents. To tackle this problem, most existing methods focus on strengthening the visual feature learning capability to reduce this text shortcut influence on model decisions. However, few efforts have been devoted to analyzing its inherent cause and providing an explicit interpretation. It thus lacks a good guidance for the research community to move forward in a purposeful way, resulting in model construction perplexity towards overcoming this non-trivial problem. In this paper, we propose to interpret the language prior problem in VQA from a class-imbalance view. Concretely, we design a novel interpretation scheme whereby the loss of mis-predicted frequent and sparse answers from the same question type is distinctly exhibited during the late training phase. It explicitly reveals why the VQA model tends to produce a frequent yet obviously wrong answer, to a given question whose right answer is sparse in the training set. Based upon this observation, we further propose a novel loss re-scaling approach to assign different weights to each answer according to the training data statistics for estimating the final loss. We apply our approach into six strong baselines and the experimental results on two VQA-CP benchmark datasets evidently demonstrate its effectiveness. In addition, we also justify the validity of the class imbalance interpretation scheme on other computer vision tasks, such as face recognition and image classification.},
  archive      = {J_TIP},
  author       = {Yangyang Guo and Liqiang Nie and Zhiyong Cheng and Qi Tian and Min Zhang},
  doi          = {10.1109/TIP.2021.3128322},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {227-238},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Loss re-scaling VQA: Revisiting the language prior problem from a class-imbalance view},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defocus image deblurring network with defocus map estimation
as auxiliary task. <em>TIP</em>, <em>31</em>, 216–226. (<a
href="https://doi.org/10.1109/TIP.2021.3127850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from the object motion blur, the defocus blur is caused by the limitation of the cameras’ depth of field. The defocus amount can be characterized by the parameter of point spread function and thus forms a defocus map. In this paper, we propose a new network architecture called Defocus Image Deblurring Auxiliary Learning Net (DID-ANet), which is specifically designed for single image defocus deblurring by using defocus map estimation as auxiliary task to improve the deblurring result. To facilitate the training of the network, we build a novel and large-scale dataset for single image defocus deblurring, which contains the defocus images, the defocus maps and the all-sharp images. To the best of our knowledge, the new dataset is the first large-scale defocus deblurring dataset for training deep networks. Moreover, the experimental results demonstrate that the proposed DID-ANet outperforms the state-of-the-art methods for both tasks of defocus image deblurring and defocus map estimation, both quantitatively and qualitatively. The dataset, code, and model is available on GitHub: https://github.com/xytmhy/DID-ANet-Defocus-Deblurring .},
  archive      = {J_TIP},
  author       = {Haoyu Ma and Shaojun Liu and Qingmin Liao and Juncheng Zhang and Jing-Hao Xue},
  doi          = {10.1109/TIP.2021.3127850},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {216-226},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Defocus image deblurring network with defocus map estimation as auxiliary task},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical representation network with auxiliary tasks for
video captioning and video question answering. <em>TIP</em>,
<em>31</em>, 202–215. (<a
href="https://doi.org/10.1109/TIP.2021.3120867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, integrating vision and language for in-depth video understanding e.g., video captioning and video question answering, has become a promising direction for artificial intelligence. However, due to the complexity of video information, it is challenging to extract a video feature that can well represent multiple levels of concepts i . e ., objects, actions and events. Meanwhile, content completeness and syntactic consistency play an important role in high-quality language-related video understanding. Motivated by these, we propose a novel framework, named Hierarchical Representation Network with Auxiliary Tasks (HRNAT), for learning multi-level representations and obtaining syntax-aware video captions. Specifically, the Cross-modality Matching Task enables the learning of hierarchical representation of videos, guided by the three-level representation of languages. The Syntax-guiding Task and the Vision-assist Task contribute to generating descriptions which are not only globally similar to the video content, but also syntax-consistent to the ground-truth description. The key components of our model are general and they can be readily applied to both video captioning and video question answering tasks. Performances for the above tasks on several benchmark datasets validate the effectiveness and superiority of our proposed method compared with the state-of-the-art methods. Codes and models are also released https://github.com/riesling00/HRNAT .},
  archive      = {J_TIP},
  author       = {Lianli Gao and Yu Lei and Pengpeng Zeng and Jingkuan Song and Meng Wang and Heng Tao Shen},
  doi          = {10.1109/TIP.2021.3120867},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {202-215},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical representation network with auxiliary tasks for video captioning and video question answering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Color image recovery using low-rank quaternion matrix
completion algorithm. <em>TIP</em>, <em>31</em>, 190–201. (<a
href="https://doi.org/10.1109/TIP.2021.3128321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new color image representation tool, quaternion has achieved excellent results in color image processing problems. In this paper, we propose a novel low-rank quaternion matrix completion algorithm to recover missing data of a color image. Motivated by two kinds of low-rank approximation approaches (low-rank decomposition and nuclear norm minimization) in traditional matrix-based methods, we combine the two approaches in our quaternion matrix-based model. Furthermore, the nuclear norm of the quaternion matrix is replaced by the sum of the Frobenius norm of its two low-rank factor quaternion matrices. Based on the relationship between the quaternion matrix and its equivalent complex matrix, the problem eventually is converted from the quaternion number domain to the complex number domain. An alternating minimization method is applied to solve the model. Simulation results on color image recovery show the superior performance and efficiency of the proposed algorithm over some tensor-based and quaternion-based ones.},
  archive      = {J_TIP},
  author       = {Jifei Miao and Kit Ian Kou},
  doi          = {10.1109/TIP.2021.3128321},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {190-201},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Color image recovery using low-rank quaternion matrix completion algorithm},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Divergent angular representation for open set image
recognition. <em>TIP</em>, <em>31</em>, 176–189. (<a
href="https://doi.org/10.1109/TIP.2021.3128318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open set recognition (OSR) models need not only discriminate between known classes but also detect unknown class samples unavailable during training. One promising approach is to learn discriminative representations over known classes with strong intra-class similarity and inter-class discrepancy. Then, the powerful class discrimination learned from the known classes can be extended to known and unknown classes. Without appropriate regularization, however, the model may learn representations trivially, collapsing unknown class representations to the known class ones. To resolve this problem, we propose Divergent Angular Representation (DivAR) based on two approaches. Firstly, DivAR maximizes its representational discrimination between known classes via a highly discriminative loss. Secondly, to ensure separation between known and unknown classes in the representation space, DivAR boosts the directional variation of representations over global samples. In addition, self-supervision is leveraged to improve the representation’s robustness and extend DivAR to one-class classification. Moreover, unlike other OSR methods that require an extra machinery for inference, DivAR learns and infers in a single module. Extensive experiments on generic image datasets demonstrate the plausibility and effectiveness of DivAR for both OSR and One-Class Classification (OCC) problems.},
  archive      = {J_TIP},
  author       = {Jaewoo Park and Cheng Yaw Low and Andrew Beng Jin Teoh},
  doi          = {10.1109/TIP.2021.3128318},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {176-189},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Divergent angular representation for open set image recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feedback graph convolutional network for skeleton-based
action recognition. <em>TIP</em>, <em>31</em>, 164–175. (<a
href="https://doi.org/10.1109/TIP.2021.3129117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has attracted considerable attention since the skeleton data is more robust to the dynamic circumstances and complicated backgrounds than other modalities. Recently, many researchers have used the Graph Convolutional Network (GCN) to model spatial-temporal features of skeleton sequences by an end-to-end optimization. However, conventional GCNs are feedforward networks for which it is impossible for the shallower layers to access semantic information in the high-level layers. In this paper, we propose a novel network, named Feedback Graph Convolutional Network (FGCN). This is the first work that introduces a feedback mechanism into GCNs for action recognition. Compared with conventional GCNs, FGCN has the following advantages: (1) A multi-stage temporal sampling strategy is designed to extract spatial-temporal features for action recognition in a coarse to fine process; (2) A Feedback Graph Convolutional Block (FGCB) is proposed to introduce dense feedback connections into the GCNs. It transmits the high-level semantic features to the shallower layers and conveys temporal information stage by stage to model video level spatial-temporal features for action recognition; (3) The FGCN model provides predictions on-the-fly. In the early stages, its predictions are relatively coarse. These coarse predictions are treated as priors to guide the feature learning in later stages, to obtain more accurate predictions. Extensive experiments on three datasets, NTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed FGCN is effective for action recognition. It achieves the state-of-the-art performance on all three datasets.},
  archive      = {J_TIP},
  author       = {Hao Yang and Dan Yan and Li Zhang and Yunda Sun and Dong Li and Stephen J. Maybank},
  doi          = {10.1109/TIP.2021.3129117},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {164-175},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Feedback graph convolutional network for skeleton-based action recognition},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A prototypical knowledge oriented adaptation framework for
semantic segmentation. <em>TIP</em>, <em>31</em>, 149–163. (<a
href="https://doi.org/10.1109/TIP.2021.3128311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prevalent family of fully convolutional networks are capable of learning discriminative representations and producing structural prediction in semantic segmentation tasks. However, such supervised learning methods require a large amount of labeled data and show inability of learning cross-domain invariant representations, giving rise to overfitting performance on the source dataset. Domain adaptation, a transfer learning technique that demonstrates strength on aligning feature distributions, can improve the performance of learning methods by providing inter-domain discrepancy alleviation. Recently introduced output-space based adaptation methods provide significant advances on cross-domain semantic segmentation tasks, however, a lack of consideration for intra-domain divergence of domain discrepancy remains prone to over-adaptation results on the target domain. To address the problem, we first leverage prototypical knowledge on the target domain to relax its hard domain label to a continuous domain space, where pixel-wise domain adaptation is developed upon a soft adversarial loss. The development of prototypical knowledge allows to elaborate specific adaptation strategies on under-aligned regions and well-aligned regions of the target domain. Furthermore, aiming to achieve better adaptation performance, we employ a unilateral discriminator to alleviate implicit uncertainty on prototypical knowledge. At last, we theoretically and experimentally demonstrate that the proposed prototypical knowledge oriented adaptation approach provides effective guidance on distribution alignment and alleviation on over-adaptation. The proposed approach shows competitive performance with state-of-the-art methods on two cross-domain segmentation tasks.},
  archive      = {J_TIP},
  author       = {Haitao Tian and Shiru Qu and Pierre Payeur},
  doi          = {10.1109/TIP.2021.3128311},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {149-163},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A prototypical knowledge oriented adaptation framework for semantic segmentation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge tracing using gaussian process regression.
<em>TIP</em>, <em>31</em>, 138–148. (<a
href="https://doi.org/10.1109/TIP.2021.3128329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel edge tracing algorithm using Gaussian process regression. Our edge-based segmentation algorithm models an edge of interest using Gaussian process regression and iteratively searches the image for edge pixels in a recursive Bayesian scheme. This procedure combines local edge information from the image gradient and global structural information from posterior curves, sampled from the model’s posterior predictive distribution, to sequentially build and refine an observation set of edge pixels. This accumulation of pixels converges the distribution to the edge of interest. Hyperparameters can be tuned by the user at initialisation and optimised given the refined observation set. This tunable approach does not require any prior training and is not restricted to any particular type of imaging domain. Due to the model’s uncertainty quantification, the algorithm is robust to artefacts and occlusions which degrade the quality and continuity of edges in images. Our approach also has the ability to efficiently trace edges in image sequences by using previous-image edge traces as a priori information for consecutive images. Various applications to medical imaging and satellite imaging are used to validate the technique and comparisons are made with two commonly used edge tracing algorithms.},
  archive      = {J_TIP},
  author       = {Jamie Burke and Stuart King},
  doi          = {10.1109/TIP.2021.3128329},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {138-148},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Edge tracing using gaussian process regression},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Euclidean distance approximations from replacement product
graphs. <em>TIP</em>, <em>31</em>, 125–137. (<a
href="https://doi.org/10.1109/TIP.2021.3128319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new chamfering paradigm, locally connecting pixels to produce path distances that approximate Euclidean space by building a small network (a replacement product) inside each pixel. These “ $RE$ -grid graphs” maintain near-Euclidean polygonal distance contours even in noisy data sets, making them useful tools for approximation when exact numerical solutions are unobtainable or impractical. The $RE$ -grid graph creates a modular global architecture with lower pixel-to-pixel valency and simplified topology at the cost of increased computational complexity due to its internal structure. We present an introduction to chamfering replacement products with a number of case study examples to demonstrate the potential of these graphs for path-finding in high frequency and low resolution image spaces which motivate further study. Possible future applications include morphology, watershed segmentation, halftoning, neural network design, anisotropic image processing, image skeletonization, dendritic shaping, and cellular automata.},
  archive      = {J_TIP},
  author       = {T. Arthur Terlep and Mark R. Bell and Thomas M. Talavage and Douglas L. Smith},
  doi          = {10.1109/TIP.2021.3128319},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {125-137},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Euclidean distance approximations from replacement product graphs},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Passive non-line-of-sight imaging using optimal transport.
<em>TIP</em>, <em>31</em>, 110–124. (<a
href="https://doi.org/10.1109/TIP.2021.3128312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive non-line-of-sight (NLOS) imaging has drawn great attention in recent years. However, all existing methods are in common limited to simple hidden scenes, low-quality reconstruction, and small-scale datasets. In this paper, we propose NLOS-OT, a novel passive NLOS imaging framework based on manifold embedding and optimal transport, to reconstruct high-quality complicated hidden scenes. NLOS-OT converts the high-dimensional reconstruction task to a low-dimensional manifold mapping through optimal transport, alleviating the ill-posedness in passive NLOS imaging. Besides, we create the first large-scale passive NLOS imaging dataset, NLOS-Passive, which includes 50 groups and more than 3,200,000 images. NLOS-Passive collects target images with different distributions and their corresponding observed projections under various conditions, which can be used to evaluate the performance of passive NLOS imaging algorithms. It is shown that the proposed NLOS-OT framework achieves much better performance than the state-of-the-art methods on NLOS-Passive. We believe that the NLOS-OT framework together with the NLOS-Passive dataset is a big step and can inspire many ideas towards the development of learning-based passive NLOS imaging. Codes and dataset are publicly available ( https://github.com/ruixv/NLOS-OT ).},
  archive      = {J_TIP},
  author       = {Ruixu Geng and Yang Hu and Zhi Lu and Cong Yu and Houqiang Li and Hengyu Zhang and Yan Chen},
  doi          = {10.1109/TIP.2021.3128312},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {110-124},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Passive non-line-of-sight imaging using optimal transport},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Remote sensing scene classification via multi-branch local
attention network. <em>TIP</em>, <em>31</em>, 99–109. (<a
href="https://doi.org/10.1109/TIP.2021.3127851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing scene classification (RSSC) is a hotspot and play very important role in the field of remote sensing image interpretation in recent years. With the recent development of the convolutional neural networks, a significant breakthrough has been made in the classification of remote sensing scenes. Many objects form complex and diverse scenes through spatial combination and association, which makes it difficult to classify remote sensing image scenes. The problem of insufficient differentiation of feature representations extracted by Convolutional Neural Networks (CNNs) still exists, which is mainly due to the characteristics of similarity for inter-class images and diversity for intra-class images. In this paper, we propose a remote sensing image scene classification method via Multi-Branch Local Attention Network (MBLANet), where Convolutional Local Attention Module (CLAM) is embedded into all down-sampling blocks and residual blocks of ResNet backbone. CLAM contains two submodules, Convolutional Channel Attention Module (CCAM) and Local Spatial Attention Module (LSAM). The two submodules are placed in parallel to obtain both channel and spatial attentions, which helps to emphasize the main target in the complex background and improve the ability of feature representation. Extensive experiments on three benchmark datasets show that our method is better than state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Si-Bao Chen and Qing-Song Wei and Wen-Zhong Wang and Jin Tang and Bin Luo and Zu-Yuan Wang},
  doi          = {10.1109/TIP.2021.3127851},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {99-109},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Remote sensing scene classification via multi-branch local attention network},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). M5L: Multi-modal multi-margin metric learning for RGBT
tracking. <em>TIP</em>, <em>31</em>, 85–98. (<a
href="https://doi.org/10.1109/TIP.2021.3125504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying hard samples in the course of RGBT tracking is a quite challenging problem. Existing methods only focus on enlarging the boundary between positive and negative samples, but ignore the relations of multilevel hard samples, which are crucial for the robustness of hard sample classification. To handle this problem, we propose a novel Multi-Modal Multi-Margin Metric Learning framework named M 5 L for RGBT tracking. In particular, we divided all samples into four parts including normal positive, normal negative, hard positive and hard negative ones, and aim to leverage their relations to improve the robustness of feature embeddings, e.g., normal positive samples are closer to the ground truth than hard positive ones. To this end, we design a multi-modal multi-margin structural loss to preserve the relations of multilevel hard samples in the training stage. In addition, we introduce an attention-based fusion module to achieve quality-aware integration of different source data. Extensive experiments on large-scale datasets testify that our framework clearly improves the tracking performance and performs favorably the state-of-the-art RGBT trackers.},
  archive      = {J_TIP},
  author       = {Zhengzheng Tu and Chun Lin and Wei Zhao and Chenglong Li and Jin Tang},
  doi          = {10.1109/TIP.2021.3125504},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {85-98},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {M5L: Multi-modal multi-margin metric learning for RGBT tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A domain gap aware generative adversarial network for
multi-domain image translation. <em>TIP</em>, <em>31</em>, 72–84. (<a
href="https://doi.org/10.1109/TIP.2021.3125266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent image-to-image translation models have shown great success in mapping local textures between two domains. Existing approaches rely on a cycle-consistency constraint that supervises the generators to learn an inverse mapping. However, learning the inverse mapping introduces extra trainable parameters and it is unable to learn the inverse mapping for some domains. As a result, they are ineffective in the scenarios where (i) multiple visual image domains are involved; (ii) both structure and texture transformations are required; and (iii) semantic consistency is preserved. To solve these challenges, the paper proposes a unified model to translate images across multiple domains with significant domain gaps. Unlike previous models that constrain the generators with the ubiquitous cycle-consistency constraint to achieve the content similarity, the proposed model employs a perceptual self-regularization constraint. With a single unified generator, the model can maintain consistency over the global shapes as well as the local texture information across multiple domains. Extensive qualitative and quantitative evaluations demonstrate the effectiveness and superior performance over state-of-the-art models. It is more effective in representing shape deformation in challenging mappings with significant dataset variation across multiple domains.},
  archive      = {J_TIP},
  author       = {Wenju Xu and Guanghui Wang},
  doi          = {10.1109/TIP.2021.3125266},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {72-84},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A domain gap aware generative adversarial network for multi-domain image translation},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). View-wise versus cluster-wise weight: Which is better for
multi-view clustering? <em>TIP</em>, <em>31</em>, 58–71. (<a
href="https://doi.org/10.1109/TIP.2021.3128323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighted multi-view clustering (MVC) aims to combine the complementary information of multi-view data (such as image data with different types of features) in a weighted manner to obtain a consistent clustering result. However, when the cluster-wise weights across views are vastly different, most existing weighted MVC methods may fail to fully utilize the complementary information, because they are based on view-wise weight learning and can not learn the fine-grained cluster-wise weights. Additionally, extra parameters are needed for most of them to control the weight distribution sparsity or smoothness, which are hard to tune without prior knowledge. To address these issues, in this paper we propose a novel and effective Cluster-weighted mUlti-view infoRmation bottlEneck (CURE) clustering algorithm, which can automatically learn the cluster-wise weights to discover the discriminative clusters across multiple views and thus can enhance the clustering performance by properly exploiting the cluster-level complementary information. To learn the cluster-wise weights, we design a new weight learning scheme by exploring the relation between the mutual information of the joint distribution of a specific cluster (containing a group of data samples) and the weight of this cluster. Finally, a novel draw-and-merge method is presented to solve the optimization problem. Experimental results on various multi-view datasets show the superiority and effectiveness of our cluster-wise weighted CURE over several state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Shizhe Hu and Zhengzheng Lou and Yangdong Ye},
  doi          = {10.1109/TIP.2021.3128323},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {58-71},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {View-wise versus cluster-wise weight: Which is better for multi-view clustering?},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AVLSM: Adaptive variational level set model for image
segmentation in the presence of severe intensity inhomogeneity and high
noise. <em>TIP</em>, <em>31</em>, 43–57. (<a
href="https://doi.org/10.1109/TIP.2021.3127848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensity inhomogeneity and noise are two common issues in images but inevitably lead to significant challenges for image segmentation and is particularly pronounced when the two issues simultaneously appear in one image. As a result, most existing level set models yield poor performance when applied to this images. To this end, this paper proposes a novel hybrid level set model, named adaptive variational level set model (AVLSM) by integrating an adaptive scale bias field correction term and a denoising term into one level set framework, which can simultaneously correct the severe inhomogeneous intensity and denoise in segmentation. Specifically, an adaptive scale bias field correction term is first defined to correct the severe inhomogeneous intensity by adaptively adjusting the scale according to the degree of intensity inhomogeneity while segmentation. More importantly, the proposed adaptive scale truncation function in the term is model-agnostic, which can be applied to most off-the-shelf models and improves their performance for image segmentation with severe intensity inhomogeneity. Then, a denoising energy term is constructed based on the variational model, which can remove not only common additive noise but also multiplicative noise often occurred in medical image during segmentation. Finally, by integrating the two proposed energy terms into a variational level set framework, the AVLSM is proposed. The experimental results on synthetic and real images demonstrate the superiority of AVLSM over most state-of-the-art level set models in terms of accuracy, robustness and running time.},
  archive      = {J_TIP},
  author       = {Qing Cai and Yiming Qian and Sanping Zhou and Jinxing Li and Yee-Hong Yang and Feng Wu and David Zhang},
  doi          = {10.1109/TIP.2021.3127848},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {43-57},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AVLSM: Adaptive variational level set model for image segmentation in the presence of severe intensity inhomogeneity and high noise},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Spatio-temporal correlation guided geometric partitioning
for versatile video coding. <em>TIP</em>, <em>31</em>, 30–42. (<a
href="https://doi.org/10.1109/TIP.2021.3126420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric partitioning has attracted increasing attention by its remarkable motion field description capability in the hybrid video coding framework. However, the existing geometric partitioning (GEO) scheme in Versatile Video Coding (VVC) causes a non-negligible burden for signaling the side information. Consequently, the coding efficiency is limited. In view of this, we propose a spatio-temporal correlation guided geometric partitioning (STGEO) scheme to efficiently describe the object information in the motion field of video coding. The proposed method can economize the bits consumed for side information signaling, including the partitioning mode and motion information. We firstly analyze the characteristics of partitioning mode decision and motion vector selection in a statistically-sound way. Based on the observed spatio-temporal correlation, we design a mode prediction and coding method to reduce the overhead for representing the above mentioned side information. The main idea is to predict the STGEO modes and motion candidates that have higher selection possibilities, which can guide the entropy coding, i.e., representing the predicted high-probability modes and motion candidates with fewer bits. In particular, the high-probability STGEO modes are predicted based on the edge information and history modes of adjacent STGEO-coded blocks. The corresponding motion information is represented by the index in a merge candidate list, which is adaptively inferred based on the off-line trained merge candidate selection probability. Simulation results show that the proposed approach achieves 0.95\% and 1.98\% bit-rate savings on average compared to VTM-8.0 without GEO for Random Access and Low-Delay B configurations, respectively.},
  archive      = {J_TIP},
  author       = {Xuewei Meng and Chuanmin Jia and Xinfeng Zhang and Shanshe Wang and Siwei Ma},
  doi          = {10.1109/TIP.2021.3126420},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {30-42},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatio-temporal correlation guided geometric partitioning for versatile video coding},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel hybrid level set model for non-rigid object contour
tracking. <em>TIP</em>, <em>31</em>, 15–29. (<a
href="https://doi.org/10.1109/TIP.2021.3112051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing trackers use bounding boxes for object tracking. However, the background contained in the bounding box inevitably decreases the accuracy of the target model, which affects the performance of the tracker and is particularly pronounced for non-rigid objects. To address the above issue, this paper proposes a novel hybrid level set model, which can robustly address the issue of topology changing, occlusions and abrupt motion in non-rigid object tracking by accurately tracking the object contour. In particular, an appearance model is first obtained by repeatedly training and relabeling the initial labeled frame using competing one-class SVMs. Then, by integrating the trained appearance model, an edge detector and image spatial information into the level set model, a new hybrid level set model is presented, which accurately locates the object contour and feeds back to the competing one-class SVMs to update the appearance model of the next frame. In addition, a motion model is defined to predict the accurate location of the object when occlusion and abrupt motion occur in the next frame. Finally, the experimental results on state-of-the-art benchmarks demonstrate the feasibility and effectiveness of the proposed model and the superiority of the proposed method over existing trackers in terms of accuracy and robustness.},
  archive      = {J_TIP},
  author       = {Qing Cai and Huiying Liu and Yiming Qian and Sanping Zhou and Jinjun Wang and Yee-Hong Yang},
  doi          = {10.1109/TIP.2021.3112051},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {15-29},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A novel hybrid level set model for non-rigid object contour tracking},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised structured subspace learning for multi-view
clustering. <em>TIP</em>, <em>31</em>, 1–14. (<a
href="https://doi.org/10.1109/TIP.2021.3128325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims at simultaneously obtaining a consensus underlying subspace across multiple views and conducting clustering on the learned consensus subspace, which has gained a variety of interest in image processing. In this paper, we propose the Semi-supervised Structured Subspace Learning algorithm for clustering data points from Multiple sources (SSSL-M). We explicitly extend the traditional multi-view clustering with a semi-supervised manner and then build an anti-block-diagonal indicator matrix with small amount of supervisory information to pursue the block-diagonal structure of the shared affinity matrix. SSSL-M regularizes multiple view-specific affinity matrices into a shared affinity matrix based on reconstruction through a unified framework consisting of backward encoding networks and the self-expressive mapping. The shared affinity matrix is comprehensive and can flexibly encode complementary information from multiple view-specific affinity matrices. An enhanced structural consistency of affinity matrices from different views can be achieved and the intrinsic relationships among affinity matrices from multiple views can be effectively reflected in this manner. Technically, we formulate the proposed model as an optimization problem, which can be solved by an alternating optimization scheme. Experimental results over seven different benchmark datasets demonstrate that better clustering results can be obtained by our method compared with the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Yalan Qin and Hanzhou Wu and Xinpeng Zhang and Guorui Feng},
  doi          = {10.1109/TIP.2021.3128325},
  journal      = {IEEE Transactions on Image Processing},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised structured subspace learning for multi-view clustering},
  volume       = {31},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
