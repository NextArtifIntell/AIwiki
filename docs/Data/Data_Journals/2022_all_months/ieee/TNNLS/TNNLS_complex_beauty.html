<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---635">TNNLS - 635</h2>
<ul>
<li><details>
<summary>
(2022). Parameter-free consensus embedding learning for multiview
graph-based clustering. <em>TNNLS</em>, <em>33</em>(12), 7944–7950. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a consensus embedding from multiple views is the mainstream task in multiview graph-based clustering, in which the key problem is to handle the inconsistence among multiple views. In this article, we consider clustering effectiveness and practical applicability collectively, and propose a parameter-free model to alleviate the inconsistence of multiple views cleverly. To be specific, the proposed model considers the diversities of multiple views as two-layers. The first layer considers the inconsistence among different features of each view and the second layer considers linking the preembeddings of multiple views attentively. By this way, a consensus embedding can be learned via kernel method effectively and the whole learning procedure is parameter-free. To solve the optimization problem involved in the proposed model, we propose an alternative algorithm which is efficient and easy to implement in practice. In the experiments, we evaluate the proposed model on synthetic and real datasets and the experimental results demonstrate its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Danyang Wu and Feiping Nie and Xia Dong and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3087162},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7944-7950},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameter-free consensus embedding learning for multiview graph-based clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multivariate uncertainty in deep learning. <em>TNNLS</em>,
<em>33</em>(12), 7937–7943. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has the potential to dramatically impact navigation and tracking state estimation problems critical to autonomous vehicles and robotics. Measurement uncertainties in state estimation systems based on Kalman and other Bayes filters are typically assumed to be a fixed covariance matrix. This assumption is risky, particularly for “black box” deep learning models, in which uncertainty can vary dramatically and unexpectedly. Accurate quantification of multivariate uncertainty will allow for the full potential of deep learning to be used more safely and reliably in these applications. We show how to model multivariate uncertainty for regression problems with neural networks, incorporating both aleatoric and epistemic sources of heteroscedastic uncertainty. We train a deep uncertainty covariance matrix model in two ways: directly using a multivariate Gaussian density loss function and indirectly using end-to-end training through a Kalman filter. We experimentally show in a visual tracking problem the large impact that accurate multivariate uncertainty quantification can have on the Kalman filter performance for both in-domain and out-of-domain evaluation data. We additionally show, in a challenging visual odometry problem, how end-to-end filter training can allow uncertainty predictions to compensate for filter weaknesses.},
  archive      = {J_TNNLS},
  author       = {Rebecca L. Russell and Christopher Reale},
  doi          = {10.1109/TNNLS.2021.3086757},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7937-7943},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multivariate uncertainty in deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subarchitecture ensemble pruning in neural architecture
search. <em>TNNLS</em>, <em>33</em>(12), 7928–7936. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) is gaining more and more attention in recent years because of its flexibility and remarkable capability to reduce the burden of neural network design. To achieve better performance, however, the searching process usually costs massive computations that might not be affordable for researchers and practitioners. Although recent attempts have employed ensemble learning methods to mitigate the enormous computational cost, however, they neglect a key property of ensemble methods, namely diversity, which leads to collecting more similar subarchitectures with potential redundancy in the final design. To tackle this problem, we propose a pruning method for NAS ensembles called “ subarchitecture ensemble pruning in neural architecture search (SAEP).” It targets to leverage diversity and to achieve subensemble architectures at a smaller size with comparable performance to ensemble architectures that are not pruned. Three possible solutions are proposed to decide which subarchitectures to prune during the searching process. Experimental results exhibit the effectiveness of the proposed method by largely reducing the number of subarchitectures without degrading the performance.},
  archive      = {J_TNNLS},
  author       = {Yijun Bian and Qingquan Song and Mengnan Du and Jun Yao and Huanhuan Chen and Xia Hu},
  doi          = {10.1109/TNNLS.2021.3085299},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7928-7936},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Subarchitecture ensemble pruning in neural architecture search},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-guided article embedding refinement for
session-based news recommendation. <em>TNNLS</em>, <em>33</em>(12),
7921–7927. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized news recommendation aims to recommend news articles to customers, by exploiting the personal preferences and short-term reading interest of users. A practical challenge in personalized news recommendations is the lack of logged user interactions. Recently, the session-based news recommendation has attracted increasing attention, which tries to recommend the next news article given previous articles in an active session. Current session-based news recommendation methods mainly extract latent embeddings from news articles and user-item interactions. However, many existing methods could not exploit the semantic-level structural information among news articles. And the feature learning process simply relies on the news articles in training data, which may not be sufficient to learn semantically rich embeddings. This brief presents a context-aware graph embedding (CAGE) approach for session-based news recommendation. It employs external knowledge graphs to improve the semantic-level representations of news articles. Moreover, graph neural networks are incorporated to further enhance the article embeddings. In addition, we consider the similarity among sessions and design attention neural networks to model the short-term user preferences. Extensive results on multiple news recommendation benchmark datasets show that CAGE performs better than some competitive baselines in most cases.},
  archive      = {J_TNNLS},
  author       = {Heng-Shiou Sheu and Zhixuan Chu and Daiqing Qi and Sheng Li},
  doi          = {10.1109/TNNLS.2021.3084958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7921-7927},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge-guided article embedding refinement for session-based news recommendation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new approach to descriptors generation for image retrieval
by analyzing activations of deep neural network layers. <em>TNNLS</em>,
<em>33</em>(12), 7913–7920. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we consider the problem of descriptors construction for the task of content-based image retrieval using deep neural networks. The idea of neural codes, based on fully connected layers’ activations, is extended by incorporating the information contained in convolutional layers. It is known that the total number of neurons in the convolutional part of the network is large and the majority of them have little influence on the final classification decision. Therefore, in this brief, we propose a novel algorithm that allows us to extract the most significant neuron activations and utilize this information to construct effective descriptors. The descriptors consisting of values taken from both the fully connected and convolutional layers perfectly represent the whole image content. The images retrieved using these descriptors match semantically very well to the query image, and also, they are similar in other secondary image characteristics, such as background, textures, or color distribution. These features of the proposed descriptors are verified experimentally based on the IMAGENET1M dataset using the VGG16 neural network. For comparison, we also test the proposed approach on the ResNet50 network.},
  archive      = {J_TNNLS},
  author       = {Paweł Staszewski and Maciej Jaworski and Jinde Cao and Leszek Rutkowski},
  doi          = {10.1109/TNNLS.2021.3084633},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7913-7920},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new approach to descriptors generation for image retrieval by analyzing activations of deep neural network layers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensusability of first-order multiagent systems under
distributed PID controller with time delay. <em>TNNLS</em>,
<em>33</em>(12), 7908–7912. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article analyzes the consensus of first-order multiagent systems under the network topology with a directed spanning tree. A distributed PID controller with time delay is designed. D-parameterization approach is used and the crossing set consisting of frequencies such that at least one characteristic root is on the imaginary axis is identified. It is proven that the rightward crossings of the characteristic roots are always guaranteed. The exact delay margin is then determined. Numerical simulation is proposed to demonstrate the theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Qian Ma and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2021.3084366},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7908-7912},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensusability of first-order multiagent systems under distributed PID controller with time delay},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topology and content co-alignment graph convolutional
learning. <em>TNNLS</em>, <em>33</em>(12), 7899–7907. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional graph neural networks (GNNs), graph convolutional learning is carried out through topology-driven recursive node content aggregation for network representation learning. In reality, network topology and node content each provide unique and important information, and they are not always consistent because of noise, irrelevance, or missing links between nodes. A pure topology-driven feature aggregation approach between unaligned neighborhoods may deteriorate learning from nodes with poor structure-content consistency, due to the propagation of incorrect messages over the whole network. Alternatively, in this brief, we advocate a co-alignment graph convolutional learning (CoGL) paradigm, by aligning topology and content networks to maximize consistency. Our theme is to enforce the learning from the topology network to be consistent with the content network while simultaneously optimizing the content network to comply with the topology for optimized representation learning. Given a network, CoGL first reconstructs a content network from node features then co-aligns the content network and the original network through a unified optimization goal with: 1) minimized content loss; 2) minimized classification loss; and 3) minimized adversarial loss. Experiments on six benchmarks demonstrate that CoGL achieves comparable and even better performance compared with existing state-of-the-art GNN models.},
  archive      = {J_TNNLS},
  author       = {Min Shi and Yufei Tang and Xingquan Zhu},
  doi          = {10.1109/TNNLS.2021.3084125},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7899-7907},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Topology and content co-alignment graph convolutional learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised learning in neural networks:
Feedback-network-free implementation and biological plausibility.
<em>TNNLS</em>, <em>33</em>(12), 7888–7898. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The well-known backpropagation learning algorithm is probably the most popular learning algorithm in artificial neural networks. It has been widely used in various applications of deep learning. The backpropagation algorithm requires a separate feedback network to back propagate errors. This feedback network must have the same topology and connection strengths (weights) as the feed-forward network. In this article, we propose a new learning algorithm that is mathematically equivalent to the backpropagation algorithm but does not require a feedback network. The elimination of the feedback network makes the implementation of the new algorithm much simpler. The elimination of the feedback network also significantly increases biological plausibility for biological neural networks to learn using the new algorithm by means of some retrograde regulatory mechanisms that may exist in neurons. This new algorithm also eliminates the need for two-phase adaptation (feed-forward phase and feedback phase). Hence, neurons can adapt asynchronously and concurrently in a way analogous to that of biological neurons.},
  archive      = {J_TNNLS},
  author       = {Feng Lin},
  doi          = {10.1109/TNNLS.2021.3089134},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7888-7898},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised learning in neural networks: Feedback-network-free implementation and biological plausibility},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding optimal paths using networks without
learning—unifying classical approaches. <em>TNNLS</em>, <em>33</em>(12),
7877–7887. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory or path planning is a fundamental issue in a wide variety of applications. In this article, we show that it is possible to solve path planning on a maze for multiple start point and endpoint highly efficiently with a novel configuration of multilayer networks that use only weighted pooling operations, for which no network training is needed. These networks create solutions, which are identical to those from classical algorithms such as breadth-first search (BFS), Dijkstra’s algorithm, or TD(0). Different from competing approaches, very large mazes containing almost one billion nodes with dense obstacle configuration and several thousand importance-weighted path endpoints can this way be solved quickly in a single pass on parallel hardware.},
  archive      = {J_TNNLS},
  author       = {Tomas Kulvicius and Sebastian Herzog and Minija Tamosiunaite and Florentin Wörgötter},
  doi          = {10.1109/TNNLS.2021.3089023},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7877-7887},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finding optimal paths using networks without Learning—Unifying classical approaches},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complex robotic manipulation via graph-based hindsight goal
generation. <em>TNNLS</em>, <em>33</em>(12), 7863–7876. (<a
href="https://doi.org/10.1109/TNNLS.2021.3088947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning algorithms, such as hindsight experience replay (HER) and hindsight goal generation (HGG), have been able to solve challenging robotic manipulation tasks in multigoal settings with sparse rewards. HER achieves its training success through hindsight replays of past experience with heuristic goals but underperforms in challenging tasks in which goals are difficult to explore. HGG enhances HER by selecting intermediate goals that are easy to achieve in the short term and promising to lead to target goals in the long term. This guided exploration makes HGG applicable to tasks in which target goals are far away from the object’s initial position. However, the vanilla HGG is not applicable to manipulation tasks with obstacles because the Euclidean metric used for HGG is not an accurate distance metric in such an environment. Although, with the guidance of a handcrafted distance grid, grid-based HGG can solve manipulation tasks with obstacles, a more feasible method that can solve such tasks automatically is still in demand. In this article, we propose graph-based hindsight goal generation (G-HGG), an extension of HGG selecting hindsight goals based on shortest distances in an obstacle-avoiding graph, which is a discrete representation of the environment. We evaluated G-HGG on four challenging manipulation tasks with obstacles, where significant enhancements in both sample efficiency and overall success rate are shown over HGG and HER. Videos can be viewed at https://videoviewsite.wixsite.com/ghgg .},
  archive      = {J_TNNLS},
  author       = {Zhenshan Bing and Matthias Brucker and Fabrice O. Morin and Rui Li and Xiaojie Su and Kai Huang and Alois Knoll},
  doi          = {10.1109/TNNLS.2021.3088947},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7863-7876},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Complex robotic manipulation via graph-based hindsight goal generation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic intermediate generation with deep reinforcement
learning for robust two-exposure image fusion. <em>TNNLS</em>,
<em>33</em>(12), 7853–7862. (<a
href="https://doi.org/10.1109/TNNLS.2021.3088907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing low dynamic range (LDR) for high dynamic range (HDR) images has gained a lot of attention, especially to achieve real-world application significance when the hardware resources are limited to capture images with different exposure times. However, existing HDR image generation by picking the best parts from each LDR image often yields unsatisfactory results due to either the lack of input images or well-exposed contents. To overcome this limitation, we model the HDR image generation process in two-exposure fusion as a deep reinforcement learning problem and learn an online compensating representation to fuse with LDR inputs for HDR image generation. Moreover, we build a two-exposure dataset with reference HDR images from a public multiexposure dataset that has not yet been normalized to train and evaluate the proposed model. By assessing the built dataset, we show that our reinforcement HDR image generation significantly outperforms other competing methods under different challenging scenarios, even with limited well-exposed contents. More experimental results on a no-reference multiexposure image dataset demonstrate the generality and effectiveness of the proposed model. To the best of our knowledge, this is the first work to use a reinforcement-learning-based framework for an online compensating representation in two-exposure image fusion.},
  archive      = {J_TNNLS},
  author       = {Jia-Li Yin and Bo-Hao Chen and Yan-Tsung Peng and Hau Hwang},
  doi          = {10.1109/TNNLS.2021.3088907},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7853-7862},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automatic intermediate generation with deep reinforcement learning for robust two-exposure image fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding neural networks and individual neuron
importance via information-ordered cumulative ablation. <em>TNNLS</em>,
<em>33</em>(12), 7842–7852. (<a
href="https://doi.org/10.1109/TNNLS.2021.3088685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate the use of three information-theoretic quantities—entropy, mutual information with the class variable, and a class selectivity measure based on Kullback–Leibler (KL) divergence—to understand and study the behavior of already trained fully connected feedforward neural networks (NNs). We analyze the connection between these information-theoretic quantities and classification performance on the test set by cumulatively ablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our results parallel those recently published by Morcos et al., indicating that class selectivity is not a good indicator for classification performance. However, looking at individual layers separately, both mutual information and class selectivity are positively correlated with classification performance, at least for networks with ReLU activation functions. We provide explanations for this phenomenon and conclude that it is ill-advised to compare the proposed information-theoretic quantities across layers. Furthermore, we show that cumulative ablation of neurons with ascending or descending information-theoretic quantities can be used to formulate hypotheses regarding the joint behavior of multiple neurons, such as redundancy and synergy, with comparably low computational cost. We also draw connections to the information bottleneck theory for NNs.},
  archive      = {J_TNNLS},
  author       = {Rana Ali Amjad and Kairen Liu and Bernhard C. Geiger},
  doi          = {10.1109/TNNLS.2021.3088685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7842-7852},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Understanding neural networks and individual neuron importance via information-ordered cumulative ablation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-independent structured pruning of neural networks via
coresets. <em>TNNLS</em>, <em>33</em>(12), 7829–7841. (<a
href="https://doi.org/10.1109/TNNLS.2021.3088587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model compression is crucial for the deployment of neural networks on devices with limited computational and memory resources. Many different methods show comparable accuracy of the compressed model and similar compression rates. However, the majority of the compression methods are based on heuristics and offer no worst case guarantees on the tradeoff between the compression rate and the approximation error for an arbitrarily new sample. We propose the first efficient structured pruning algorithm with a provable tradeoff between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, and it approximates the output of a layer of neurons/filters by a coreset of neurons/filters in the previous layer and discards the rest. We apply this framework in a layer-by-layer fashion from the bottom to the top. Unlike previous works, our coreset is data-independent, meaning that it provably guarantees the accuracy of the function for any input $x\in \mathbb {R}^{d}$ , including an adversarial one.},
  archive      = {J_TNNLS},
  author       = {Ben Mussay and Dan Feldman and Samson Zhou and Vladimir Braverman and Margarita Osadchy},
  doi          = {10.1109/TNNLS.2021.3088587},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7829-7841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-independent structured pruning of neural networks via coresets},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fixed-time projection neural network for solving
l₁-minimization problem. <em>TNNLS</em>, <em>33</em>(12), 7818–7828. (<a
href="https://doi.org/10.1109/TNNLS.2021.3088535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new projection neural network (PNN) for solving ${L_{\mathrm{1}}}$ -minimization problem is proposed, which is based on classic PNN and sliding mode control technique. Furthermore, the proposed network can be used to make sparse signal reconstruction and image reconstruction. First, a sign function is introduced into the PNN model to design fixed-time PNN (FPNN). Then, under the condition that the projection matrix satisfies the restricted isometry property (RIP), the stability and fixed-time convergence of the proposed FPNN are proved by the Lyapunov method. Finally, based on the experimental results of signal simulation and image reconstruction, the proposed FPNN shows the effectiveness and superiority compared with that of the existing PNNs.},
  archive      = {J_TNNLS},
  author       = {Xing He and Hongsong Wen and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3088535},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7818-7828},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fixed-time projection neural network for solving l₁-minimization problem},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient gradient support pursuit with less hard
thresholding for cardinality-constrained learning. <em>TNNLS</em>,
<em>33</em>(12), 7806–7817. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, stochastic hard thresholding (HT) optimization methods [e.g., stochastic variance reduced gradient hard thresholding (SVRGHT)] are becoming more attractive for solving large-scale sparsity/rank-constrained problems. However, they have much higher HT oracle complexities, especially for high-dimensional data or large-scale matrices. To address this issue and inspired by the well-known Gradient Support Pursuit (GraSP) method, this article proposes a new Relaxed Gradient Support Pursuit (RGraSP) framework. Unlike GraSP, RGraSP only requires to yield an approximation solution at each iteration. Based on the property of RGraSP, we also present an efficient stochastic variance reduction-gradient support pursuit algorithm and its fast version (called stochastic variance reduced gradient support pursuit (SVRGSP+). We prove that the gradient oracle complexity of both our algorithms is two times less than that of SVRGHT. In particular, their HT complexity is about $\kappa _{\widehat {s}}$ times less than that of SVRGHT, where $\kappa _{\widehat {s}}$ is the restricted condition number. Moreover, we prove that our algorithms enjoy fast linear convergence to an approximately global optimum, and also present an asynchronous parallel variant to deal with very high-dimensional and sparse data. Experimental results on both synthetic and real-world datasets show that our algorithms yield superior results than the state-of-the-art gradient HT methods.},
  archive      = {J_TNNLS},
  author       = {Fanhua Shang and Bingkun Wei and Hongying Liu and Yuanyuan Liu and Pan Zhou and Maoguo Gong},
  doi          = {10.1109/TNNLS.2021.3087805},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7806-7817},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient gradient support pursuit with less hard thresholding for cardinality-constrained learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer-based adaptive optimized control for stochastic
nonlinear systems with input and state constraints. <em>TNNLS</em>,
<em>33</em>(12), 7791–7805. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, an adaptive neural network (NN) optimized output-feedback control problem is studied for a class of stochastic nonlinear systems with unknown nonlinear dynamics, input saturation, and state constraints. A nonlinear state observer is designed to estimate the unmeasured states, and the NNs are used to approximate the unknown nonlinear functions. Under the framework of the backstepping technique, the virtual and actual optimal controllers are developed by employing the actor–critic architecture. Meanwhile, the tan-type Barrier optimal performance index functions are developed to prevent the nonlinear systems from the state constraints, and all the states are confined within the preselected compact sets all the time. It is worth mentioning that the proposed optimized control is clearly simple since the reinforcement learning (RL) algorithm is derived based on the negative gradient of a simple positive function. Furthermore, the proposed optimal control strategy ensures that all the signals in the closed-loop system are bounded. Finally, a practical simulation example is carried out to further illustrate the effectiveness of the proposed optimal control method.},
  archive      = {J_TNNLS},
  author       = {Yongming Li and Jiaxin Zhang and Wei Liu and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2021.3087796},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7791-7805},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based adaptive optimized control for stochastic nonlinear systems with input and state constraints},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). End-to-end hierarchical reinforcement learning with
integrated subgoal discovery. <em>TNNLS</em>, <em>33</em>(12),
7778–7790. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical reinforcement learning (HRL) is a promising approach to perform long-horizon goal-reaching tasks by decomposing the goals into subgoals. In a holistic HRL paradigm, an agent must autonomously discover such subgoals and also learn a hierarchy of policies that uses them to reach the goals. Recently introduced end-to-end HRL methods accomplish this by using the higher-level policy in the hierarchy to directly search the useful subgoals in a continuous subgoal space. However, learning such a policy may be challenging when the subgoal space is large. We propose integrated discovery of salient subgoals (LIDOSS), an end-to-end HRL method with an integrated subgoal discovery heuristic that reduces the search space of the higher-level policy, by explicitly focusing on the subgoals that have a greater probability of occurrence on various state-transition trajectories leading to the goal. We evaluate LIDOSS on a set of continuous control tasks in the MuJoCo domain against hierarchical actor critic (HAC), a state-of-the-art end-to-end HRL method. The results show that LIDOSS attains better goal achievement rates than HAC in most of the tasks.},
  archive      = {J_TNNLS},
  author       = {Shubham Pateria and Budhitama Subagdja and Ah-Hwee Tan and Chai Quek},
  doi          = {10.1109/TNNLS.2021.3087733},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7778-7790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {End-to-end hierarchical reinforcement learning with integrated subgoal discovery},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distillation-guided residual learning for binary
convolutional neural networks. <em>TNNLS</em>, <em>33</em>(12),
7765–7777. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to bridge the performance gap between binary convolutional neural network (BCNN) and floating-point CNN (FCNN). This performance gap is mainly caused by the inferior modeling capability and training strategy of BCNN, which leads to substantial residuals in intermediate feature maps between BCNN and FCNN. To minimize the performance gap, we enforce BCNN to produce similar intermediate feature maps with the ones of FCNN. This intuition leads to a more effective training strategy for BCNN, i.e., optimizing each binary convolutional block with blockwise distillation loss derived from FCNN. The goal of minimizing the residuals in intermediate feature maps also motivates us to update the binary convolutional block architecture to facilitate the optimization of blockwise distillation loss. Specifically, a lightweight shortcut branch is inserted into each binary convolutional block to complement residuals at each block. Benefited from its squeeze-and-interaction (SI) structure, this shortcut branch introduces a fraction of parameters, e.g., less than 10\% overheads, but effectively boosts the modeling capability of binary convolution blocks in BCNN. Extensive experiments on ImageNet demonstrate the superior performance of our method in both classification efficiency and accuracy, e.g., BCNN trained with our methods achieves the accuracy of 60.45\% on ImageNet, better than many state-of-the-art ones.},
  archive      = {J_TNNLS},
  author       = {Jianming Ye and Jingdong Wang and Shiliang Zhang},
  doi          = {10.1109/TNNLS.2021.3087731},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7765-7777},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distillation-guided residual learning for binary convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proximal online gradient is optimum for dynamic regret: A
general lower bound. <em>TNNLS</em>, <em>33</em>(12), 7755–7764. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online learning, the dynamic regret metric chooses the reference oracle that may change over time, while the typical (static) regret metric assumes the reference solution to be constant over the whole time horizon. The dynamic regret metric is particularly interesting for applications, such as online recommendation (since the customers’ preference always evolves over time). While the online gradient (OG) method has been shown to be optimal for the static regret metric, the optimal algorithm for the dynamic regret remains unknown. In this article, we show that proximal OG (a general version of OG) is optimum to the dynamic regret by showing that the proved lower bound matches the upper bound. It is highlighted that we provide a new and general lower bound of dynamic regret. It provides new understanding about the difficulty to follow the dynamics in the online setting.},
  archive      = {J_TNNLS},
  author       = {Yawei Zhao and Shuang Qiu and Kuan Li and Lailong Luo and Jianping Yin and Ji Liu},
  doi          = {10.1109/TNNLS.2021.3087579},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7755-7764},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Proximal online gradient is optimum for dynamic regret: A general lower bound},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rapid sensor fault diagnosis for a class of nonlinear
systems via deterministic learning. <em>TNNLS</em>, <em>33</em>(12),
7743–7754. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a rapid sensor fault diagnosis (SFD) method is presented for a class of nonlinear systems. First, by exploiting the linear adaptive observer technology and the deterministic learning method (DLM), an adaptive neural network (NN) observer is constructed to capture the information of the unknown sensor fault function. Second, when the NN input orbit is a period or recurrent one, the partial persistent excitation (PE) condition of the NNs can be guaranteed through the DLM. Based on the partial PE condition and the uniformly completely observable property of a linear time-varying system, the accurate state estimation and the sensor fault identification can be achieved by properly choosing the observer gain. Third, a bank of dynamical observers utilizing the experiential knowledge is constructed to achieve rapid SFD and data recovery. The attractions of the proposed approach are that accurate approximations of sensor faults can be achieved through the DLM, and the data that are destroyed by the sensor faults can be recovered by using the learning results. Simulation studies of a robot system are utilized to show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Tianrui Chen and Zejian Zhu and Cong Wang and ZhaoYang Dong},
  doi          = {10.1109/TNNLS.2021.3087533},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7743-7754},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rapid sensor fault diagnosis for a class of nonlinear systems via deterministic learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Data-driven formation control for unknown MIMO nonlinear
discrete-time multi-agent systems with sensor fault. <em>TNNLS</em>,
<em>33</em>(12), 7728–7742. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-driven distributed formation control algorithm is proposed for an unknown heterogeneous non-affine nonlinear discrete-time MIMO multi-agent system (MAS) with sensor fault. For the considered unknown MAS, the dynamic linearization technique in model-free adaptive control (MFAC) theory is used to transform the unknown MAS into an equivalent virtual dynamic linearization data model. Then using the virtual data model, the structure of the distributed model-free adaptive controller is constructed. For the incorrect signal measurements due to the sensor fault, the radial basis function neural network (RBFNN) is first trained for the MAS under the fault-free case, then using the outputs of the well-trained RBFNN and the actual outputs of MAS under sensor fault case, the estimation laws of the unknown fault and system parameters in the virtual data model are designed with only the measured input–output (I/O) data information. Finally, the boundedness of the formation error is analyzed by the contraction mapping method and mathematical induction method. The effectiveness of the proposed algorithm is illustrated by simulation examples.},
  archive      = {J_TNNLS},
  author       = {Shuangshuang Xiong and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2021.3087481},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7728-7742},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven formation control for unknown MIMO nonlinear discrete-time multi-agent systems with sensor fault},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic mirror descent on overparameterized nonlinear
models. <em>TNNLS</em>, <em>33</em>(12), 7717–7727. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern learning problems are highly overparameterized, i.e., have many more model parameters than the number of training data points. As a result, the training loss may have infinitely many global minima (parameter vectors that perfectly “interpolate” the training data). It is therefore imperative to understand which interpolating solutions we converge to, how they depend on the initialization and learning algorithm, and whether they yield different test errors. In this article, we study these questions for the family of stochastic mirror descent (SMD) algorithms, of which stochastic gradient descent (SGD) is a special case. Recently, it has been shown that for overparameterized linear models, SMD converges to the closest global minimum to the initialization point, where closeness is in terms of the Bregman divergence corresponding to the potential function of the mirror descent. With appropriate initialization, this yields convergence to the minimum-potential interpolating solution, a phenomenon referred to as implicit regularization. On the theory side, we show that for sufficiently-overparameterized nonlinear models, SMD with a (small enough) fixed step size converges to a global minimum that is “very close” (in Bregman divergence) to the minimum-potential interpolating solution, thus attaining approximate implicit regularization. On the empirical side, our experiments on the MNIST and CIFAR-10 datasets consistently confirm that the above phenomenon occurs in practical scenarios. They further indicate a clear difference in the generalization performances of different SMD algorithms: experiments on the CIFAR-10 dataset with different regularizers, $\ell _{1}$ to encourage sparsity, $\ell _{2}$ (SGD) to encourage small Euclidean norm, and $\ell _{\infty }$ to discourage large components, surprisingly show that the $\ell _{\infty }$ norm consistently yields better generalization performance than SGD, which in turn generalizes better than the $\ell _{1}$ norm.},
  archive      = {J_TNNLS},
  author       = {Navid Azizan and Sahin Lale and Babak Hassibi},
  doi          = {10.1109/TNNLS.2021.3087480},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7717-7727},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic mirror descent on overparameterized nonlinear models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental deep neural network learning using
classification confidence thresholding. <em>TNNLS</em>, <em>33</em>(12),
7706–7716. (<a
href="https://doi.org/10.1109/TNNLS.2021.3087104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most modern neural networks for classification fail to take into account the concept of the unknown. Trained neural networks are usually tested in an unrealistic scenario with only examples from a closed set of known classes. In an attempt to develop a more realistic model, the concept of working in an open set environment has been introduced. This in turn leads to the concept of incremental learning where a model with its own architecture and initial trained set of data can identify unknown classes during the testing phase and autonomously update itself if evidence of a new class is detected. Some problems that arise in incremental learning are inefficient use of resources to retrain the classifier repeatedly and the decrease of classification accuracy as multiple classes are added over time. This process of instantiating new classes is repeated as many times as necessary, accruing errors. To address these problems, this article proposes the classification confidence threshold (CT) approach to prime neural networks for incremental learning to keep accuracies high by limiting forgetting. A lean method is also used to reduce resources used in the retraining of the neural network. The proposed method is based on the idea that a network is able to incrementally learn a new class even when exposed to a limited number samples associated with the new class. This method can be applied to most existing neural networks with minimal changes to network architecture.},
  archive      = {J_TNNLS},
  author       = {Justin Leo and Jugal Kalita},
  doi          = {10.1109/TNNLS.2021.3087104},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7706-7716},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Incremental deep neural network learning using classification confidence thresholding},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust finite-time zeroing neural networks with fixed and
varying parameters for solving dynamic generalized lyapunov equation.
<em>TNNLS</em>, <em>33</em>(12), 7695–7705. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For solving dynamic generalized Lyapunov equation, two robust finite-time zeroing neural network (RFTZNN) models with stationary and nonstationary parameters are generated through the usage of an improved sign-bi-power (SBP) activation function (AF). Taking differential errors and model implementation errors into account, two corresponding perturbed RFTZNN models are derived to facilitate the analyses of robustness on the two RFTZNN models. Theoretical analysis gives the quantitatively estimated upper bounds for the convergence time (UBs-CT) of the two derived models, implying a superiority of the convergence that varying parameter RFTZNN (VP-RFTZNN) possesses over the fixed parameter RFTZNN (FP-RFTZNN). When the coefficient matrices and perturbation matrices are uniformly bounded, residual error of FP-RFTZNN is bounded, whereas that of VP-RFTZNN monotonically decreases at a super-exponential rate after a finite time, and eventually converges to 0. When these matrices are bounded but not uniform, residual error of FP-RFTZNN is no longer bounded, but that of VP-RFTZNN still converges. These superiorities of VP-RFTZNN are illustrated by abundant comparative experiments, and its application value is further proved by an application to robot.},
  archive      = {J_TNNLS},
  author       = {Qiuyue Zuo and Kenli Li and Lin Xiao and Keqin Li},
  doi          = {10.1109/TNNLS.2021.3086500},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7695-7705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust finite-time zeroing neural networks with fixed and varying parameters for solving dynamic generalized lyapunov equation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network-embedded stochastic nonlinear
state-space models and their applications to process monitoring.
<em>TNNLS</em>, <em>33</em>(12), 7682–7694. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process complexities are characterized by strong nonlinearities, dynamics, and uncertainties. Monitoring such a complex process requires a high-quality model describing the corresponding nonlinear dynamic behavior. The proposed model is constructed using deep neural networks (DNNs) to represent the state transition and observation generation, both of which constitute a stochastic nonlinear state-space model. A new bidirectional recurrent neural network (RNN), creating a connection of the hidden layer between a forward RNN and a backward RNN, is proposed to generate the filtering estimation and the smoothing estimation of process states which further generate observations with DNN-based process models. The smoothing estimator and the process model are first learned offline with all collected samples. Then the filtering estimator is fine-tuned by the learned smoother and process models to achieve real-time monitoring since the filter state is estimated based on the past and the current observations. Two indices are designed based on the learned model for monitoring the process anomaly. The proposed process monitoring model can deal with complex nonlinearities, process dynamics, and process uncertainties, all of which can be very challenging for the existing methods, such as kernel mapping and stacked auto-encoder. Two case studies validate that the effectiveness of the proposed method outperforms the other comparative methods by at least 10\% when using the averaged fault detection rate in the industrial experimental data.},
  archive      = {J_TNNLS},
  author       = {Kai Wang and Junghui Chen and Zhihuan Song and Yalin Wang and Chunhua Yang},
  doi          = {10.1109/TNNLS.2021.3086323},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7682-7694},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep neural network-embedded stochastic nonlinear state-space models and their applications to process monitoring},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Learning from a complementary-label source domain: Theory
and algorithms. <em>TNNLS</em>, <em>33</em>(12), 7667–7681. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unsupervised domain adaptation (UDA), a classifier for the target domain is trained with massive true-label data from the source domain and unlabeled data from the target domain. However, collecting true-label data in the source domain can be expensive and sometimes impractical. Compared to the true label (TL), a complementary label (CL) specifies a class that a pattern does not belong to, and hence, collecting CLs would be less laborious than collecting TLs. In this article, we propose a novel setting where the source domain is composed of complementary-label data, and a theoretical bound of this setting is provided. We consider two cases of this setting: one is that the source domain only contains complementary-label data [completely complementary UDA (CC-UDA)] and the other is that the source domain has plenty of complementary-label data and a small amount of true-label data [partly complementary UDA (PC-UDA)]. To this end, a c omplementary l abel advers aria l net work (CLARINET) is proposed to solve CC-UDA and PC-UDA problems. CLARINET maintains two deep networks simultaneously, with one focusing on classifying the complementary-label source data and the other taking care of the source-to-target distributional adaptation. Experiments show that CLARINET significantly outperforms a series of competent baselines on handwritten digit-recognition and object-recognition tasks.},
  archive      = {J_TNNLS},
  author       = {Yiyang Zhang and Feng Liu and Zhen Fang and Bo Yuan and Guangquan Zhang and Jie Lu},
  doi          = {10.1109/TNNLS.2021.3086093},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7667-7681},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning from a complementary-label source domain: Theory and algorithms},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward region-aware attention learning for scene graph
generation. <em>TNNLS</em>, <em>33</em>(12), 7655–7666. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGGen) is a challenging task due to a complex visual context of an image. Intuitively, the human visual system can volitionally focus on attended regions by salient stimuli associated with visual cues. For example, to infer the relationship between man and horse, the interaction between human leg and horseback can provide strong visual evidence to predict the predicate ride. Besides, the attended region face can also help to determine the object man. Till now, most of the existing works studied the SGGen by extracting coarse-grained bounding box features while understanding fine-grained visual regions received limited attention. To mitigate the drawback, this article proposes a region-aware attention learning method. The key idea is to explicitly construct the attention space to explore salient regions with the object and predicate inferences. First, we extract a set of regions in an image with the standard detection pipeline. Each region regresses to an object. Second, we propose the object-wise attention graph neural network (GNN), which incorporates attention modules into the graph structure to discover attended regions for object inference. Third, we build the predicate-wise co-attention GNN to jointly highlight subject’s and object’s attended regions for predicate inference. Particularly, each subject-object pair is connected with one of the latent predicates to construct one triplet. The proposed intra-triplet and inter-triplet learning mechanism can help discover the pair-wise attended regions to infer predicates. Extensive experiments on two popular benchmarks demonstrate the superiority of the proposed method. Additional ablation studies and visualization further validate its effectiveness.},
  archive      = {J_TNNLS},
  author       = {An-An Liu and Hongshuo Tian and Ning Xu and Weizhi Nie and Yongdong Zhang and Mohan Kankanhalli},
  doi          = {10.1109/TNNLS.2021.3086066},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7655-7666},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward region-aware attention learning for scene graph generation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Composite antidisturbance control for non-gaussian
stochastic systems via information-theoretic learning technique.
<em>TNNLS</em>, <em>33</em>(12), 7644–7654. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel composite hierarchical antidisturbance control (CHADC) algorithm aided by the information-theoretic learning (ITL) technique is developed for non-Gaussian stochastic systems subject to dynamic disturbances. The whole control process consists of some time-domain intervals called batches. Within each batch, a CHADC scheme is applied to the system, where a disturbance observer (DO) is employed to estimate the dynamic disturbance and a composite control strategy integrating feedforward compensation and feedback control is adopted. The information-theoretic measure (entropy or information potential) is employed to quantify the randomness of the controlled system, based on which the gain matrices of DO and feedback controller are updated between two adjacent batches. In this way, the mean-square stability is guaranteed within each batch, and the system performance is improved along with the progress of batches. The proposed algorithm has enhanced disturbance rejection ability and good applicability to non-Gaussian noise environment, which contributes to extending CHADC theory to the general stochastic case. Finally, simulation examples are included to verify the effectiveness of theoretical results.},
  archive      = {J_TNNLS},
  author       = {Bo Tian and Chenliang Wang and Lei Guo},
  doi          = {10.1109/TNNLS.2021.3086032},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7644-7654},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite antidisturbance control for non-gaussian stochastic systems via information-theoretic learning technique},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Achieving online regression performance of LSTMs with simple
RNNs. <em>TNNLS</em>, <em>33</em>(12), 7632–7643. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are widely used for online regression due to their ability to generalize nonlinear temporal dependencies. As an RNN model, long short-term memory networks (LSTMs) are commonly preferred in practice, as these networks are capable of learning long-term dependencies while avoiding the vanishing gradient problem. However, due to their large number of parameters, training LSTMs requires considerably longer training time compared to simple RNNs (SRNNs). In this article, we achieve the online regression performance of LSTMs with SRNNs efficiently. To this end, we introduce a first-order training algorithm with a linear time complexity in the number of parameters. We show that when SRNNs are trained with our algorithm, they provide very similar regression performance with the LSTMs in two to three times shorter training time. We provide strong theoretical analysis to support our experimental results by providing regret bounds on the convergence rate of our algorithm. Through an extensive set of experiments, we verify our theoretical work and demonstrate significant performance improvements of our algorithm with respect to LSTMs and the other state-of-the-art learning models.},
  archive      = {J_TNNLS},
  author       = {N. Mert Vural and Fatih Ilhan and Selim F. Yilmaz and Salih Ergüt and Suleyman Serdar Kozat},
  doi          = {10.1109/TNNLS.2021.3086029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7632-7643},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Achieving online regression performance of LSTMs with simple RNNs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tuning convolutional spiking neural network with
biologically plausible reward propagation. <em>TNNLS</em>,
<em>33</em>(12), 7621–7631. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) contain more biologically realistic structures and biologically inspired learning principles than those in standard artificial neural networks (ANNs). SNNs are considered the third generation of ANNs, powerful on the robust computation with a low computational cost. The neurons in SNNs are nondifferential, containing decayed historical states and generating event-based spikes after their states reaching the firing threshold. These dynamic characteristics of SNNs make it difficult to be directly trained with the standard backpropagation (BP), which is also considered not biologically plausible. In this article, a biologically plausible reward propagation (BRP) algorithm is proposed and applied to the SNN architecture with both spiking-convolution (with both 1-D and 2-D convolutional kernels) and full-connection layers. Unlike the standard BP that propagates error signals from postsynaptic to presynaptic neurons layer by layer, the BRP propagates target labels instead of errors directly from the output layer to all prehidden layers. This effort is more consistent with the top-down reward-guiding learning in cortical columns of the neocortex. Synaptic modifications with only local gradient differences are induced with pseudo-BP that might also be replaced with the spike-timing-dependent plasticity (STDP). The performance of the proposed BRP-SNN is further verified on the spatial (including MNIST and Cifar-10) and temporal (including TIDigits and DvsGesture) tasks, where the SNN using BRP has reached a similar accuracy compared to other state-of-the-art (SOTA) BP-based SNNs and saved 50\% more computational cost than ANNs. We think that the introduction of biologically plausible learning rules to the training procedure of biologically realistic SNNs will give us more hints and inspiration toward a better understanding of the biological system’s intelligent nature.},
  archive      = {J_TNNLS},
  author       = {Tielin Zhang and Shuncheng Jia and Xiang Cheng and Bo Xu},
  doi          = {10.1109/TNNLS.2021.3085966},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7621-7631},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tuning convolutional spiking neural network with biologically plausible reward propagation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DNB: A joint learning framework for deep bayesian
nonparametric clustering. <em>TNNLS</em>, <em>33</em>(12), 7610–7620.
(<a href="https://doi.org/10.1109/TNNLS.2021.3085891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms based on deep neural networks have been widely studied for image analysis. Most existing methods require partial knowledge of the true labels, namely, the number of clusters, which is usually not available in practice. In this article, we propose a Bayesian nonparametric framework, deep nonparametric Bayes (DNB), for jointly learning image clusters and deep representations in a doubly unsupervised manner. In doubly unsupervised learning, we are dealing with the problem of “unknown unknowns,” where we estimate not only the unknown image labels but also the unknown number of labels as well. The proposed algorithm alternates between generating a potentially unbounded number of clusters in the forward pass and learning the deep networks in the backward pass. With the help of the Dirichlet process mixtures, the proposed method is able to partition the latent representations space without specifying the number of clusters a priori. An important feature of this work is that all the estimation is realized with an end-to-end solution, which is very different from the methods that rely on post hoc analysis to select the number of clusters. Another key idea in this article is to provide a principled solution to the problem of “trivial solution” for deep clustering, which has not been much studied in the current literature. With extensive experiments on benchmark datasets, we show that our doubly unsupervised method achieves good clustering performance and outperforms many other unsupervised image clustering methods.},
  archive      = {J_TNNLS},
  author       = {Zeya Wang and Yang Ni and Baoyu Jing and Deqing Wang and Hao Zhang and Eric Xing},
  doi          = {10.1109/TNNLS.2021.3085891},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7610-7620},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DNB: A joint learning framework for deep bayesian nonparametric clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep probabilistic transfer learning framework for soft
sensor modeling with missing data. <em>TNNLS</em>, <em>33</em>(12),
7598–7609. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft sensors have been extensively developed and applied in the process industry. One of the main challenges of the data-driven soft sensors is the lack of labeled data and the need to absorb the knowledge from a related source operating condition to enhance the soft sensing performance on the target application. This article introduces deep transfer learning to soft sensor modeling and proposes a deep probabilistic transfer regression (DPTR) framework. In DPTR, a deep generative regression model is first developed to learn Gaussian latent feature representations and model the regression relationship under the stochastic gradient variational Bayes framework. Then, a probabilistic latent space transfer strategy is designed to reduce the discrepancy between the source and target latent features such that the knowledge from the source data can be explored and transferred to enhance the target soft sensor performance. Besides, considering the missing values in the process data in the target operating condition, the DPTR is further extended to handle the missing data problem utilizing the strong generation and reconstruction capability of the deep generative model. The effectiveness of the proposed method is validated through an industrial multiphase flow process.},
  archive      = {J_TNNLS},
  author       = {Zheng Chai and Chunhui Zhao and Biao Huang and Hongtian Chen},
  doi          = {10.1109/TNNLS.2021.3085869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7598-7609},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep probabilistic transfer learning framework for soft sensor modeling with missing data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust adaptive control for a small unmanned helicopter
using reinforcement learning. <em>TNNLS</em>, <em>33</em>(12),
7589–7597. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel adaptive controller for a small-size unmanned helicopter using the reinforcement learning (RL) control methodology. The helicopter is subject to system uncertainties and unknown external disturbances. The dynamic unmodeling uncertainties of the system are estimated online by the actor network, and the tracking performance function is optimized via the critic network. The estimation error of the actor-critic network and the external unknown disturbances are compensated via the nonlinear robust component based on the sliding mode control method. The stability of the closed-loop system and the asymptotic convergence of the attitude tracking error are proved via the Lyapunov-based stability analysis. Finally, real-time experiments are performed on a helicopter control testbed. The experimental results show that the proposed controller achieves good control performance.},
  archive      = {J_TNNLS},
  author       = {Bin Xian and Xu Zhang and Haonan Zhang and Xun Gu},
  doi          = {10.1109/TNNLS.2021.3085767},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7589-7597},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust adaptive control for a small unmanned helicopter using reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Position-aware participation-contributed temporal dynamic
model for group activity recognition. <em>TNNLS</em>, <em>33</em>(12),
7574–7588. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group activity recognition (GAR) aiming at understanding the behavior of a group of people in a video clip has received increasing attention recently. Nevertheless, most of the existing solutions ignore that not all the persons contribute to the group activity of the scene equally. That is to say, the contribution from different individual behaviors to group activity is different; meanwhile, the contribution from people with different spatial positions is also different. To this end, we propose a novel Position-aware Participation-Contributed Temporal Dynamic Model (P2CTDM), in which two types of the key actor are constructed and learned. Specifically, we focus on the behaviors of key actors, who maintain steady motions (long moving time, called long motions) or display remarkable motions (but closely related to other people and the group activity, called flash motions) at a certain moment. For capturing long motions, we rank individual motions according to their intensity measured by stacking optical flows. For capturing flash motions that are closely related to other people, we design a position-aware interaction module (PIM) that simultaneously considers the feature similarity and position information. Beyond that, for capturing flash motions that are highly related to the group activity, we also present an aggregation long short-term memory (Agg-LSTM) to fuse the outputs from PIM by time-varying trainable attention factors. Four widely used benchmarks are adopted to evaluate the performance of the proposed P2CTDM compared to the state of the art.},
  archive      = {J_TNNLS},
  author       = {Rui Yan and Xiangbo Shu and Chengcheng Yuan and Qi Tian and Jinhui Tang},
  doi          = {10.1109/TNNLS.2021.3085567},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7574-7588},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Position-aware participation-contributed temporal dynamic model for group activity recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A probabilistically quantized learning control framework for
networked linear systems. <em>TNNLS</em>, <em>33</em>(12), 7559–7573.
(<a href="https://doi.org/10.1109/TNNLS.2021.3085559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider quantized learning control for linear networked systems with additive channel noise. Our objective is to achieve high tracking performance while reducing the communication burden on the communication network. To address this problem, we propose an integrated framework consisting of two modules: a probabilistic quantizer and a learning scheme. The employed probabilistic quantizer is developed by employing a Bernoulli distribution driven by the quantization errors. Three learning control schemes are studied, namely, a constant gain, a decreasing gain sequence satisfying certain conditions, and an optimal gain sequence that is recursively generated based on a performance index. We show that the control with a constant gain can only ensure the input error sequence to converge to a bounded sphere in a mean-square sense, where the radius of this sphere is proportional to the constant gain. On the contrary, we show that the control that employs any of the two proposed gain sequences drives the input error to zero in the mean-square sense. In addition, we show that the convergence rate associated with the constant gain is exponential, whereas the rate associated with the proposed gain sequences is not faster than a specific exponential trend. Illustrative simulations are provided to demonstrate the convergence rate properties and steady-state tracking performance associated with each gain, and their robustness against modeling uncertainties.},
  archive      = {J_TNNLS},
  author       = {Dong Shen and Niu Huo and Samer S. Saab},
  doi          = {10.1109/TNNLS.2021.3085559},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7559-7573},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A probabilistically quantized learning control framework for networked linear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Delay-dependent switching approaches for stability analysis
of two additive time-varying delay neural networks. <em>TNNLS</em>,
<em>33</em>(12), 7545–7558. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article analyzes the exponentially stable problem of neural networks (NNs) with two additive time-varying delay components. Disparate from the previous solutions on this similar model, switching ideas, that divide the time-varying delay intervals and treat the small intervals as switching signals, are introduced to transfer the studied problem into a switching problem. Besides, delay-dependent switching adjustment indicators are proposed to construct a novel set of augmented multiple Lyapunov–Krasovskii functionals (LKFs) that not only satisfy the switching condition but also make the suitable delay-dependent integral items be in the each corresponding LKF based on each switching mode. Combined with some switching techniques, some less conservativeness stability criteria with different numbers of switching modes are obtained. In the end, two simulation examples are performed to demonstrate the effectiveness and efficiency of the presented methods comparing other available ones.},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Zhang and Degang Wang and Kaoru Ota and Mianxiong Dong and Hongxing Li},
  doi          = {10.1109/TNNLS.2021.3085555},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7545-7558},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Delay-dependent switching approaches for stability analysis of two additive time-varying delay neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonnegative consensus tracking of networked systems with
convergence rate optimization. <em>TNNLS</em>, <em>33</em>(12),
7534–7544. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the nonnegative consensus tracking problem for networked systems with a distributed static output-feedback (SOF) control protocol. The distributed SOF controller design for networked systems presents a more challenging issue compared with the distributed state-feedback controller design. The agents are described by multi-input multi-output (MIMO) positive dynamic systems which may contain uncertain parameters, and the interconnection among the followers is modeled using an undirected connected communication graph. By employing positive systems theory, a series of necessary and sufficient conditions governing the consensus of the nominal, as well as uncertain, networked positive systems, is developed. Semidefinite programming consensus design approaches are proposed for the convergence rate optimization of MIMO agents. In addition, by exploiting the positivity characteristic of the systems, a linear-programming-based design approach is also proposed for the convergence rate optimization of single-input multi-output (SIMO) agents. The proposed approaches and the corresponding theoretical results are validated by case studies.},
  archive      = {J_TNNLS},
  author       = {Jason J. R. Liu and James Lam and Bohao Zhu and Xiaomei Wang and Zhan Shu and Ka-Wai Kwok},
  doi          = {10.1109/TNNLS.2021.3085396},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7534-7544},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonnegative consensus tracking of networked systems with convergence rate optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning based optimal tracking control under
unmeasurable disturbances with application to HVAC systems.
<em>TNNLS</em>, <em>33</em>(12), 7523–7533. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the design of an optimal controller for solving tracking problems subject to unmeasurable disturbances and unknown system dynamics using reinforcement learning (RL). Many existing RL control methods take disturbance into account by directly measuring it and manipulating it for exploration during the learning process, thereby preventing any disturbance induced bias in the control estimates. However, in most practical scenarios, disturbance is neither measurable nor manipulable. The main contribution of this article is the introduction of a combination of a bias compensation mechanism and the integral action in the Q-learning framework to remove the need to measure or manipulate the disturbance, while preventing disturbance induced bias in the optimal control estimates. A bias compensated Q-learning scheme is presented that learns the disturbance induced bias terms separately from the optimal control parameters and ensures the convergence of the control parameters to the optimal solution even in the presence of unmeasurable disturbances. Both state feedback and output feedback algorithms are developed based on policy iteration (PI) and value iteration (VI) that guarantee the convergence of the tracking error to zero. The feasibility of the design is validated on a practical optimal control application of a heating, ventilating, and air conditioning (HVAC) zone controller.},
  archive      = {J_TNNLS},
  author       = {Syed Ali Asad Rizvi and Amanda J. Pertzborn and Zongli Lin},
  doi          = {10.1109/TNNLS.2021.3085358},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7523-7533},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning based optimal tracking control under unmeasurable disturbances with application to HVAC systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prescribed finite-time adaptive neural tracking control for
nonlinear state-constrained systems: Barrier function approach.
<em>TNNLS</em>, <em>33</em>(12), 7513–7522. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this article is to present a novel backstepping-based adaptive neural tracking control design procedure for nonlinear systems with time-varying state constraints. The designed adaptive neural tracking controller is expected to have the following characters: under its action: 1) the designed virtual control signals meet the constraints on the corresponding virtual control states in order to realize the backstepping design ideal and 2) the output tracking error tends to a sufficiently small neighborhood of the origin with the prescribed finite time and accuracy level. By combining the barrier Lyapunov function approach with the adaptive neural backstepping technique, a novel adaptive neural tracking controller is proposed. It is shown that the constructed controller makes sure that the output tracking error converges to a small neighborhood of the origin with the prespecified tracking accuracy and settling time. Finally, the proposed control scheme is further tested by simulation examples.},
  archive      = {J_TNNLS},
  author       = {Xu Yuan and Bing Chen and Chong Lin},
  doi          = {10.1109/TNNLS.2021.3085324},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7513-7522},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prescribed finite-time adaptive neural tracking control for nonlinear state-constrained systems: Barrier function approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diagonal recurrent neural network-based hysteresis modeling.
<em>TNNLS</em>, <em>33</em>(12), 7502–7512. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Preisach model and the neural networks are two of the most popular strategies to model hysteresis. In this article, we first mathematically prove that the rate-independent Preisach model is actually a diagonal recurrent neural network (dRNN) with the binary step activation function. For the first time, the hysteresis nature and conditions of the classical dRNN with the tanh activation function are mathematically discovered and investigated, instead of using the common black-box approach and its variants. It is shown that the dRNN neuron is a versatile rate-dependent hysteresis system under specific conditions. The dRNN composed of those neurons can be used for modeling the rate-dependent hysteresis and it can approximate the Preisach model with arbitrary precision with specific parameters for rate-independent hysteresis modeling. Experiments show that the classical dRNN models both kinds of hysteresis more accurately and efficiently than the Preisach model.},
  archive      = {J_TNNLS},
  author       = {Guangzeng Chen and Guangke Chen and Yunjiang Lou},
  doi          = {10.1109/TNNLS.2021.3085321},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7502-7512},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Diagonal recurrent neural network-based hysteresis modeling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Centralized and collective neurodynamic optimization
approaches for sparse signal reconstruction via l₁-minimization.
<em>TNNLS</em>, <em>33</em>(12), 7488–7501. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops several centralized and collective neurodynamic approaches for sparse signal reconstruction by solving the $L_{1}$ -minimization problem. First, two centralized neurodynamic approaches are designed based on the augmented Lagrange method and the Lagrange method with derivative feedback and projection operator. Then, the optimality and global convergence of them are derived. In addition, considering that the collective neurodynamic approaches have the function of information protection and distributed information processing, first, under mild conditions, we transform the $L_{1}$ -minimization problem into two network optimization problems. Later, two collective neurodynamic approaches based on the above centralized neurodynamic approaches and multiagent consensus theory are proposed to address the obtained network optimization problems. As far as we know, this is the first attempt to use the collective neurodynamic approaches to deal with the $L_{1}$ -minimization problem in a distributed manner. Finally, several comparative experiments on sparse signal and image reconstruction demonstrate that our proposed centralized and collective neurodynamic approaches are efficient and effective.},
  archive      = {J_TNNLS},
  author       = {You Zhao and Xiaofeng Liao and Xing He and Rongqiang Tang},
  doi          = {10.1109/TNNLS.2021.3085314},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7488-7501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Centralized and collective neurodynamic optimization approaches for sparse signal reconstruction via l₁-minimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time and fixed-time synchronization of
quaternion-valued neural networks with/without mixed delays: An improved
one-norm method. <em>TNNLS</em>, <em>33</em>(12), 7475–7487. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the finite-time synchronization (FTSYN) of a class of quaternion-valued neural networks (QVNNs) with discrete and distributed time delays is studied. Furthermore, the FTSYN and fixed-time synchronization (FIXSYN) of the QVNNs without time delay are investigated. Different from the existing results, which used decomposition techniques, by introducing an improved one-norm, we use a direct analytical method to study the synchronization problems. Incidentally, several properties of one-norm of the quaternion are analyzed, and then, three effective controllers are proposed to synchronize the drive and response QVNNs within a finite time or fixed time. Moreover, efficient criteria are proposed to guarantee that the synchronization of QVNNs with or without mixed time delays can be realized within a finite and fixed time interval, respectively. In addition, the settling times are reckoned. Compared with the existing work, our advantages are mainly reflected in the simpler Lyapunov analytical process and more general activation function. Finally, the validity and practicability of the conclusions are illustrated via four numerical examples.},
  archive      = {J_TNNLS},
  author       = {Tao Peng and Jianlong Qiu and Jianquan Lu and Zhengwen Tu and Jinde Cao},
  doi          = {10.1109/TNNLS.2021.3085253},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7475-7487},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time and fixed-time synchronization of quaternion-valued neural networks With/Without mixed delays: An improved one-norm method},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Image clustering using an augmented generative adversarial
network and information maximization. <em>TNNLS</em>, <em>33</em>(12),
7461–7474. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image clustering has recently attracted significant attention due to the increased availability of unlabeled datasets. The efficiency of traditional clustering algorithms heavily depends on the distance functions used and the dimensionality of the features. Therefore, performance degradation is often observed when tackling either unprocessed images or high-dimensional features extracted from processed images. To deal with these challenges, we propose a deep clustering framework consisting of a modified generative adversarial network (GAN) and an auxiliary classifier. The modification employs Sobel operations prior to the discriminator of the GAN to enhance the separability of the learned features. The discriminator is then leveraged to generate representations as to the input to an auxiliary classifier. An objective function is utilized to train the auxiliary classifier by maximizing the mutual information between the representations obtained via the discriminator model and the same representations perturbed via adversarial training. We further improve the robustness of the auxiliary classifier by introducing a penalty term into the objective function. This minimizes the divergence across multiple transformed representations generated by the discriminator model with a low dropout rate. The auxiliary classifier is implemented with a group of multiple cluster-heads, where a tolerance hyper-parameter is used to tackle imbalanced data. Our results indicate that the proposed method achieves competitive results compared with state-of-the-art clustering methods on a wide range of benchmark datasets including CIFAR-10, CIFAR-100/20, and STL10.},
  archive      = {J_TNNLS},
  author       = {Foivos Ntelemis and Yaochu Jin and Spencer A. Thomas},
  doi          = {10.1109/TNNLS.2021.3085125},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7461-7474},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Image clustering using an augmented generative adversarial network and information maximization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A reinforcement learning approach to price cloud resources
with provable convergence guarantees. <em>TNNLS</em>, <em>33</em>(12),
7448–7460. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to generate more revenues is crucial to cloud providers. Evidences from the Amazon cloud system indicate that “dynamic pricing” would be more profitable than “static pricing.” The challenges are: How to set the price in real-time so to maximize revenues? How to estimate the price dependent demand so to optimize the pricing decision? We first design a discrete-time based dynamic pricing scheme and formulate a Markov decision process to characterize the evolving dynamics of the price-dependent demand. We formulate a revenue maximization framework to determine the optimal price and theoretically characterize the “structure” of the optimal revenue and optimal price. We apply the $Q$ -learning to infer the optimal price from historical transaction data and derive sufficient conditions on the model to guarantee its convergence to the optimal price, but it converges slowly. To speed up the convergence, we incorporate the structure of the optimal revenue that we obtained earlier, leading to the VpQ-learning ( $Q$ -learning with value projection) algorithm. We derive sufficient conditions, under which the VpQ-learning algorithm converges to the optimal policy. Experiments on a real-world dataset show that the VpQ-learning algorithm outperforms a variety of baselines, i.e., improves the revenue by as high as 50\% over the $Q$ -learning, speedy $Q$ -learning, and adaptive real-time dynamic programming (ARTDP), and by as high as 20\% over the fixed pricing scheme.},
  archive      = {J_TNNLS},
  author       = {Hong Xie and John C. S. Lui},
  doi          = {10.1109/TNNLS.2021.3085088},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7448-7460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A reinforcement learning approach to price cloud resources with provable convergence guarantees},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic event-triggered state estimation for markov jump
neural networks with partially unknown probabilities. <em>TNNLS</em>,
<em>33</em>(12), 7438–7447. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the investigation of finite-time dissipative state estimation for Markov jump neural networks. First, in view of the subsistent phenomenon that the state estimator cannot capture the system modes synchronously, the hidden Markov model with partly unknown probabilities is introduced in this article to describe such asynchronization constraint. For the upper limit of network bandwidth and computing resources, a novel dynamic event-triggered transmission mechanism, whose threshold parameter is constructed as an adjustable diagonal matrix, is set between the estimator and the original system to avoid data collision and save energy. Then, with the assistance of Lyapunov techniques, an event-based asynchronous state estimator is designed to ensure that the resulting system is finite-time bounded with a prescribed dissipation performance index. Ultimately, the effectiveness of the proposed estimator design approach combining with a dynamic event-triggered transmission mechanism is demonstrated by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Jie Tao and Zehui Xiao and Zeyu Li and Jun Wu and Renquan Lu and Peng Shi and Xiaofeng Wang},
  doi          = {10.1109/TNNLS.2021.3085001},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7438-7447},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic event-triggered state estimation for markov jump neural networks with partially unknown probabilities},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiview subspace dual clustering. <em>TNNLS</em>,
<em>33</em>(12), 7425–7437. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A single clustering refers to the partitioning of data such that the similar data are assigned into the same group, whereas the dissimilar data are separated into different groups. Recently, multiview clustering has received significant attention in recent years. However, most existing works tackle the single-clustering scenario, which only use single clustering to partition the data. In practice, nevertheless, the real-world data are complex and can be clustered in multiple ways depending on different interpretations of the data. Unlike these methods, in this article, we apply dual clustering to multiview subspace clustering. We propose a multiview dual-clustering method to simultaneously explore consensus representation and dual-clustering structure in a unified framework. First, multiview features are integrated into a latent embedding representation through a multiview learning process. Second, the dual-clustering segmentation is incorporated into the subspace clustering framework. Finally, the learned dual representations are assigned to the corresponding clusterings. The proposed approach is efficiently solved using an alternating optimization scheme. Extensive experiments demonstrate the superiority of our method on real-world multiview dual- and single-clustering datasets.},
  archive      = {J_TNNLS},
  author       = {Shirui Luo and Xiaochun Cao},
  doi          = {10.1109/TNNLS.2021.3084976},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7425-7437},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview subspace dual clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered adaptive NN tracking control for MIMO
nonlinear discrete-time systems. <em>TNNLS</em>, <em>33</em>(12),
7414–7424. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on the design of a novel event-based adaptive neural network (NN) control algorithm for a class of multiple-input–multiple-output (MIMO) nonlinear discrete-time systems. A controller is designed through a novel recursive design procedure, under which the dependence on virtual controls is avoided and only system states are needed. The numbers of the event-triggered conditions and parameters updated online in each subsystem reduce to only one, which largely reduces the computation burden and simplifies the algorithm realization. In this case, radial basis function NNs (RBFNNs) are employed to approximate the control input. The semiglobal uniformly ultimate boundedness (SGUUB) of all the signals in the closed-loop system is guaranteed by the Lyapunov difference approach. The effectiveness of the proposed algorithm is validated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Wenqi Xu and Xiaoping Liu and Huanqing Wang and Yucheng Zhou},
  doi          = {10.1109/TNNLS.2021.3084965},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7414-7424},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive NN tracking control for MIMO nonlinear discrete-time systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal network embedding for link prediction via VAE joint
attention mechanism. <em>TNNLS</em>, <em>33</em>(12), 7400–7413. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning or embedding aims to project the network into a low-dimensional space that can be devoted to different network tasks. Temporal networks are an important type of network whose topological structure changes over time. Compared with methods on static networks, temporal network embedding (TNE) methods are facing three challenges: 1) it cannot describe the temporal dependence across network snapshots; 2) the node embedding in the latent space fails to indicate changes in the network topology; and 3) it cannot avoid a lot of redundant computation via parameter inheritance on a series of snapshots. To overcome these problems, we propose a novel TNE method named temporal network embedding method based on the VAE framework (TVAE), which is based on a variational autoencoder (VAE) to capture the evolution of temporal networks for link prediction. It not only generates low-dimensional embedding vectors for nodes but also preserves the dynamic nonlinear features of temporal networks. Through the combination of a self-attention mechanism and recurrent neural networks, TVAE can update node representations and keep the temporal dependence of vectors over time. We utilize parameter inheritance to keep the new embedding close to the previous one, rather than explicitly using regularization, and thus, it is effective for large-scale networks. We evaluate our model and several baselines on synthetic data sets and real-world networks. The experimental results demonstrate that TVAE has superior performance and lower time cost compared with the baselines.},
  archive      = {J_TNNLS},
  author       = {Pengfei Jiao and Xuan Guo and Xin Jing and Dongxiao He and Huaming Wu and Shirui Pan and Maoguo Gong and Wenjun Wang},
  doi          = {10.1109/TNNLS.2021.3084957},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7400-7413},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal network embedding for link prediction via VAE joint attention mechanism},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic stability of markovian neural networks with
generally hybrid transition rates. <em>TNNLS</em>, <em>33</em>(12),
7390–7399. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the problem of the stability for Markovian neural networks (MNNs) with time delay. The transition rate is considered to be generally hybrid, which treats those existing ones as its special cases. The introduced generally hybrid transition rates (GHTRs) make these systems more general and practical. Apropos of the GHTRs, a double-boundary approach rather than the traditional estimation method is introduced to make full use of the error information in GHTRs. In order to fully capture system information, a parameter-type-delay-dependent-matrix (PTDDM) approach is proposed, in which the PTDDM approach removes some zero components on slack matrices in previous works. Thus, the PTDDM approach can fully link the relationship among time delay and state-related vectors. Based on these ingredients, a novel stochastic stability condition is proposed for MNNs with GHTRs. A numerical example is illustrated to demonstrate the effectiveness of the proposed approaches.},
  archive      = {J_TNNLS},
  author       = {Yufeng Tian and Zhanshan Wang},
  doi          = {10.1109/TNNLS.2021.3084925},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7390-7399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stochastic stability of markovian neural networks with generally hybrid transition rates},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Net learning. <em>TNNLS</em>, <em>33</em>(12), 7380–7389.
(<a href="https://doi.org/10.1109/TNNLS.2021.3084902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks, which generalize deep learning to graph-structured data, have achieved significant improvements in numerous graph-related tasks. Petri nets (PNs), on the other hand, are mainly used for the modeling and analysis of various event-driven systems from the perspective of prior knowledge, mechanisms, and tasks. Compared with graph data, net data can simulate the dynamic behavioral features of systems and are more suitable for representing real-world problems. However, the problem of large-scale data analysis has been puzzling the PN field for decades, and thus, limited its universal applicability. In this article, a framework of net learning (NL) is proposed. NL contains the advantages of PN modeling and analysis with the advantages of graph learning computation. Then, two kinds of NL algorithms are designed for performance analysis of stochastic PNs, and more specifically, the hidden feature information of the PN is obtained by mapping net information to the low-dimensional feature space. Experiments demonstrate the effectiveness of the proposed model and algorithms on the performance analysis of stochastic PNs.},
  archive      = {J_TNNLS},
  author       = {Junli Wang and Hongda Qi and Mingjian Guang and Chaobo Zhang and Chungang Yan and Changjun Jiang},
  doi          = {10.1109/TNNLS.2021.3084902},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7380-7389},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Net learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). An efficient sharing grouped convolution via bayesian
learning. <em>TNNLS</em>, <em>33</em>(12), 7367–7379. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with traditional convolutions, grouped convolutional neural networks are promising for both model performance and network parameters. However, existing models with the grouped convolution still have parameter redundancy. In this article, concerning the grouped convolution, we propose a sharing grouped convolution structure to reduce parameters. To efficiently eliminate parameter redundancy and improve model performance, we propose a Bayesian sharing framework to transfer the vanilla grouped convolution to be the sharing structure. Intragroup correlation and intergroup importance are introduced into the prior of the parameters. We handle the Maximum Type II likelihood estimation problem of the intragroup correlation and intergroup importance by a group LASSO-type algorithm. The prior mean of the sharing kernels is iteratively updated. Extensive experiments are conducted to demonstrate that on different grouped convolutional neural networks, the proposed sharing grouped convolution structure with the Bayesian sharing framework can reduce parameters and improve prediction accuracy. The proposed sharing framework can reduce parameters up to 64.17\%. For ResNeXt-50 with the sharing grouped convolution on ImageNet dataset, network parameters can be reduced by 96.875\% in all grouped convolutional layers, and accuracies are improved to 78.86\% and 94.54\% for top-1 and top-5, respectively.},
  archive      = {J_TNNLS},
  author       = {Tinghuan Chen and Bin Duan and Qi Sun and Meng Zhang and Guoqing Li and Hao Geng and Qianru Zhang and Bei Yu},
  doi          = {10.1109/TNNLS.2021.3084900},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7367-7379},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient sharing grouped convolution via bayesian learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network pruning using adaptive exemplar filters.
<em>TNNLS</em>, <em>33</em>(12), 7357–7366. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular network pruning algorithms reduce redundant information by optimizing hand-crafted models, and may cause suboptimal performance and long time in selecting filters. We innovatively introduce adaptive exemplar filters to simplify the algorithm design, resulting in an automatic and efficient pruning approach called EPruner. Inspired by the face recognition community, we use a message-passing algorithm Affinity Propagation on the weight matrices to obtain an adaptive number of exemplars, which then act as the preserved filters. EPruner breaks the dependence on the training data in determining the “important” filters and allows the CPU implementation in seconds, an order of magnitude faster than GPU-based SOTAs. Moreover, we show that the weights of exemplars provide a better initialization for the fine-tuning. On VGGNet-16, EPruner achieves a 76.34\%-FLOPs reduction by removing 88.80\% parameters, with 0.06\% accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a 65.12\%-FLOPs reduction by removing 64.18\% parameters, with only 0.71\% top-5 accuracy loss on ILSVRC-2012. Our code is available at https://github.com/lmbxmu/EPruner .},
  archive      = {J_TNNLS},
  author       = {Mingbao Lin and Rongrong Ji and Shaojie Li and Yan Wang and Yongjian Wu and Feiyue Huang and Qixiang Ye},
  doi          = {10.1109/TNNLS.2021.3084856},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7357-7366},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Network pruning using adaptive exemplar filters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IBLF-based adaptive neural control of state-constrained
uncertain stochastic nonlinear systems. <em>TNNLS</em>, <em>33</em>(12),
7345–7356. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the adaptive neural backstepping control approaches are designed for uncertain stochastic nonlinear systems with full-state constraints. According to the symmetry of constraint boundary, two cases of controlled systems subject to symmetric and asymmetric constraints are studied, respectively. Then, corresponding adaptive neural controllers are developed by virtue of backstepping design procedure and the learning ability of radial basis function neural network (RBFNN). It is worth mentioning that the integral Barrier Lyapunov function (IBLF), as an effective tool, is first applied to solve the above constraint problems. As a result, the state constraints are avoided from being transformed into error constraints via the proposed schemes. In addition, based on Lyapunov stability analysis, it is demonstrated that the errors can converge to a small neighborhood of zero, the full states do not exceed the given constraint bounds, and all signals in the closed-loop systems are semiglobally uniformly ultimately bounded (SGUUB) in probability. Finally, the numerical simulation results are provided to exhibit the effectiveness of the proposed control approaches.},
  archive      = {J_TNNLS},
  author       = {Tingting Gao and Tieshan Li and Yan-Jun Liu and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2021.3084820},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7345-7356},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IBLF-based adaptive neural control of state-constrained uncertain stochastic nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learned gradient compression for distributed deep learning.
<em>TNNLS</em>, <em>33</em>(12), 7330–7344. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks on large datasets containing high-dimensional data requires a large amount of computation. A solution to this problem is data-parallel distributed training, where a model is replicated into several computational nodes that have access to different chunks of the data. This approach, however, entails high communication rates and latency because of the computed gradients that need to be shared among nodes at every iteration. The problem becomes more pronounced in the case that there is wireless communication between the nodes (i.e., due to the limited network bandwidth). To address this problem, various compression methods have been proposed, including sparsification, quantization, and entropy encoding of the gradients. Existing methods leverage the intra-node information redundancy, that is, they compress gradients at each node independently. In contrast, we advocate that the gradients across the nodes are correlated and propose methods to leverage this inter-node redundancy to improve compression efficiency. Depending on the node communication protocol (parameter server or ring-allreduce), we propose two instances for the gradient compression that we coin Learned Gradient Compression (LGC). Our methods exploit an autoencoder (i.e., trained during the first stages of the distributed training) to capture the common information that exists in the gradients of the distributed nodes. To constrain the nodes’ computational complexity, the autoencoder is realized with a lightweight neural network. We have tested our LGC methods on the image classification and semantic segmentation tasks using different convolutional neural networks (CNNs) [ResNet50, ResNet101, and pyramid scene parsing network (PSPNet)] and multiple datasets (ImageNet, Cifar10, and CamVid). The ResNet101 model trained for image classification on Cifar10 achieved significant compression rate reductions with the accuracy of 93.57\%, which is lower than the baseline distributed training with uncompressed gradients only by 0.18\%. The rate of the model is reduced by $8095\times $ and $8\times $ compared with the baseline and the state-of-the-art deep gradient compression (DGC) method, respectively.},
  archive      = {J_TNNLS},
  author       = {Lusine Abrahamyan and Yiming Chen and Giannis Bekoulis and Nikos Deligiannis},
  doi          = {10.1109/TNNLS.2021.3084806},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7330-7344},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learned gradient compression for distributed deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inverse-based approach to explaining and visualizing
convolutional neural networks. <em>TNNLS</em>, <em>33</em>(12),
7318–7329. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new method for understanding and visualizing convolutional neural networks (CNNs). Most existing approaches to this problem focus on a global score and evaluate the pixelwise contribution of inputs to the score. The analysis of CNNs for multilabeled outputs or regression has not yet been considered in the literature, despite their success on image classification tasks with well-defined global scores. To address this problem, we propose a new inverse-based approach that computes the inverse of a feedforward pass to identify activations of interest in lower layers. We developed a layerwise inverse procedure based on two observations: 1) inverse results should have consistent internal activations to the original forward pass and 2) a small amount of activation in inverse results is desirable for human interpretability. Experimental results show that the proposed method allows us to analyze CNNs for classification and regression in the same framework. We demonstrated that our method successfully finds attributions in the inputs for image classification with comparable performance to state-of-the-art methods. To visualize the tradeoff between various methods, we developed a novel plot that shows the tradeoff between the amount of activations and the rate of class reidentification. In the case of regression, our method showed that conventional CNNs for single image super-resolution overlook a portion of frequency bands that may result in performance degradation.},
  archive      = {J_TNNLS},
  author       = {Hyuk Jin Kwon and Hyung Il Koo and Jae Woong Soh and Nam Ik Cho},
  doi          = {10.1109/TNNLS.2021.3084757},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7318-7329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inverse-based approach to explaining and visualizing convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative dual-adversarial network with spectral fidelity
and spatial enhancement for hyperspectral pansharpening. <em>TNNLS</em>,
<em>33</em>(12), 7303–7317. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) pansharpening is of great importance in improving the spatial resolution of HS images for remote sensing tasks. HS image comprises abundant spectral contents, whereas panchromatic (PAN) image provides spatial information. HS pansharpening constitutes the possibility for providing the pansharpened image with both high spatial and spectral resolution. This article develops a specific pansharpening framework based on a generative dual-adversarial network (called PS-GDANet). Specifically, the pansharpening problem is formulated as a dual task that can be solved by a generative adversarial network (GAN) with two discriminators. The spatial discriminator forces the intensity component of the pansharpened image to be as consistent as possible with the PAN image, and the spectral discriminator helps to preserve spectral information of the original HS image. Instead of designing a deep network, PS-GDANet extends GANs to two discriminators and provides a high-resolution pansharpened image in a fraction of iterations. The experimental results demonstrate that PS-GDANet outperforms several widely accepted state-of-the-art pansharpening methods in terms of qualitative and quantitative assessment.},
  archive      = {J_TNNLS},
  author       = {Wenqian Dong and Shaoxiong Hou and Song Xiao and Jiahui Qu and Qian Du and Yunsong Li},
  doi          = {10.1109/TNNLS.2021.3084745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7303-7317},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generative dual-adversarial network with spectral fidelity and spatial enhancement for hyperspectral pansharpening},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a finitely activated terminal RNN approach to
time-variant problem solving. <em>TNNLS</em>, <em>33</em>(12),
7289–7302. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns with terminal recurrent neural network (RNN) models for time-variant computing, featuring finite-valued activation functions (AFs), and finite-time convergence of error variables. Terminal RNNs stand for specific models that admit terminal attractors, and the dynamics of each neuron retains finite-time convergence. The might-existing imperfection in solving time-variant problems, through theoretically examining the asymptotically convergent RNNs, is pointed out for which the finite-time-convergent models are most desirable. The existing AFs are summarized, and it is found that there is a lack of the AFs that take only finite values. A finitely valued terminal RNN, among others, is taken into account, which involves only basic algebraic operations and taking roots. The proposed terminal RNN model is used to solve the time-variant problems undertaken, including the time-variant quadratic programming and motion planning of redundant manipulators. The numerical results are presented to demonstrate effectiveness of the proposed neural network, by which the convergence rate is comparable with that of the existing power-rate RNN.},
  archive      = {J_TNNLS},
  author       = {Mingxuan Sun and Yu Zhang and Yuxin Wu and Xiongxiong He},
  doi          = {10.1109/TNNLS.2021.3084740},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7289-7302},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On a finitely activated terminal RNN approach to time-variant problem solving},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistent meta-regularization for better meta-knowledge in
few-shot learning. <em>TNNLS</em>, <em>33</em>(12), 7277–7288. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, meta-learning provides a powerful paradigm to deal with the few-shot learning problem. However, existing meta-learning approaches ignore the prior fact that good meta-knowledge should alleviate the data inconsistency between training and test data, caused by the extremely limited data, in each few-shot learning task. Moreover, legitimately utilizing the prior understanding of meta-knowledge can lead us to design an efficient method to improve the meta-learning model. Under this circumstance, we consider the data inconsistency from the distribution perspective, making it convenient to bring in the prior fact, and propose a new consistent meta-regularization (Con-MetaReg) to help the meta-learning model learn how to reduce the data-distribution discrepancy between the training and test data. In this way, the ability of meta-knowledge on keeping the training and test data consistent is enhanced, and the performance of the meta-learning model can be further improved. The extensive analyses and experiments demonstrate that our method can indeed improve the performances of different meta-learning models in few-shot regression, classification, and fine-grained classification.},
  archive      = {J_TNNLS},
  author       = {Pinzhuo Tian and Wenbin Li and Yang Gao},
  doi          = {10.1109/TNNLS.2021.3084733},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7277-7288},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consistent meta-regularization for better meta-knowledge in few-shot learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pattern formation in a reaction-diffusion BAM neural network
with time delay: (k1, k2) mode hopf-zero bifurcation case.
<em>TNNLS</em>, <em>33</em>(12), 7266–7276. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the joint effects of connection weight and time delay on pattern formation for a delayed reaction-diffusion BAM neural network (RDBAMNN) with Neumann boundary conditions by using the $({k_{1}},{k_{2}})$ mode Hopf-zero bifurcation. First, the conditions for ${k_{1}}$ mode zero bifurcation are obtained by choosing connection weight as the bifurcation parameter. It is found that the connection weight has a great impact on the properties of steady state. With connection weight increasing, the homogeneous steady state becomes inhomogeneous, which means that the connection weight can affect the spatial stability of steady state. Then, the specified conditions for the ${k_{2}}$ mode Hopf bifurcation and the $({k_{1}},{k_{2}})$ mode Hopf-zero bifurcation are established. By using the center manifold, the third-order normal form of the Hopf-zero bifurcation is obtained. Through the analysis of the normal form, the bifurcation diagrams on two parameters’ planes (connection weight and time delay) are obtained, which contains six areas. Some interesting spatial patterns are found in these areas: a homogeneous periodic solution, a homogeneous steady state, two inhomogeneous steady state, and two inhomogeneous periodic solutions.},
  archive      = {J_TNNLS},
  author       = {Tao Dong and Weilai Xiang and Tingwen Huang and Huaqing Li},
  doi          = {10.1109/TNNLS.2021.3084693},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7266-7276},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pattern formation in a reaction-diffusion BAM neural network with time delay: (k1, k2) mode hopf-zero bifurcation case},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hyperspectral image super-resolution via deep spatiospectral
attention convolutional neural networks. <em>TNNLS</em>,
<em>33</em>(12), 7251–7265. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) are of crucial importance in order to better understand features from a large number of spectral channels. Restricted by its inner imaging mechanism, the spatial resolution is often limited for HSIs. To alleviate this issue, in this work, we propose a simple and efficient architecture of deep convolutional neural networks to fuse a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution HSI (HR-HSI). The network is designed to preserve both spatial and spectral information thanks to a new architecture based on: 1) the use of the LR-HSI at the HR-MSI’s scale to get an output with satisfied spectral preservation and 2) the application of the attention and pixelShuffle modules to extract information, aiming to output high-quality spatial details. Finally, a plain mean squared error loss function is used to measure the performance during the training. Extensive experiments demonstrate that the proposed network architecture achieves the best performance (both qualitatively and quantitatively) compared with recent state-of-the-art HSI super-resolution approaches. Moreover, other significant advantages can be pointed out by the use of the proposed approach, such as a better network generalization ability, a limited computational burden, and the robustness with respect to the number of training samples. Please find the source code and pretrained models from https://liangjiandeng.github.io/Projects_Res/HSRnet_2021tnnls.html .},
  archive      = {J_TNNLS},
  author       = {Jin-Fan Hu and Ting-Zhu Huang and Liang-Jian Deng and Tai-Xiang Jiang and Gemine Vivone and Jocelyn Chanussot},
  doi          = {10.1109/TNNLS.2021.3084682},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7251-7265},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hyperspectral image super-resolution via deep spatiospectral attention convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SeReNe: Sensitivity-based regularization of neurons for
structured sparsity in neural networks. <em>TNNLS</em>, <em>33</em>(12),
7237–7250. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks include millions of learnable parameters, making their deployment over resource-constrained devices problematic. Sensitivity-based regularization of neurons (SeReNe) is a method for learning sparse topologies with a structure, exploiting neural sensitivity as a regularizer. We define the sensitivity of a neuron as the variation of the network output with respect to the variation of the activity of the neuron. The lower the sensitivity of a neuron, the less the network output is perturbed if the neuron output changes. By including the neuron sensitivity in the cost function as a regularization term, we are able to prune neurons with low sensitivity. As entire neurons are pruned rather than single parameters, practical network footprint reduction becomes possible. Our experimental results on multiple network architectures and datasets yield competitive compression ratios with respect to state-of-the-art references.},
  archive      = {J_TNNLS},
  author       = {Enzo Tartaglione and Andrea Bragagnolo and Francesco Odierna and Attilio Fiandrotti and Marco Grangetto},
  doi          = {10.1109/TNNLS.2021.3084527},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7237-7250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SeReNe: Sensitivity-based regularization of neurons for structured sparsity in neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Easy2Hard: Learning to solve the intractables from a
synthetic dataset for structure-preserving image smoothing.
<em>TNNLS</em>, <em>33</em>(12), 7223–7236. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image smoothing is a prerequisite for many computer vision and graphics applications. In this article, we raise an intriguing question whether a dataset that semantically describes meaningful structures and unimportant details can facilitate a deep learning model to smooth complex natural images. To answer it, we generate ground-truth labels from easy samples by candidate generation and a screening test and synthesize hard samples in structure-preserving smoothing by blending intricate and multifarious details with the labels. To take full advantage of this dataset, we present a joint edge detection and structure-preserving image smoothing neural network (JESS-Net). Moreover, we propose the distinctive total variation loss as prior knowledge to narrow the gap between synthetic and real data. Experiments on different datasets and real images show clear improvements of our method over the state of the arts in terms of both the image cleanness and structure-preserving ability. Code and dataset are available at https://github.com/YidFeng/Easy2Hard .},
  archive      = {J_TNNLS},
  author       = {Yidan Feng and Sen Deng and Xuefeng Yan and Xin Yang and Mingqiang Wei and Ligang Liu},
  doi          = {10.1109/TNNLS.2021.3084473},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7223-7236},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Easy2Hard: Learning to solve the intractables from a synthetic dataset for structure-preserving image smoothing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum-inspired support vector machine. <em>TNNLS</em>,
<em>33</em>(12), 7210–7222. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is a particularly powerful and flexible supervised learning model that analyzes data for both classification and regression, whose usual algorithm complexity scales polynomially with the dimension of data space and the number of data points. To tackle the big data challenge, a quantum SVM algorithm was proposed, which is claimed to achieve exponential speedup for least squares SVM (LS-SVM). Here, inspired by the quantum SVM algorithm, we present a quantum-inspired classical algorithm for LS-SVM. In our approach, an improved fast sampling technique, namely indirect sampling, is proposed for sampling the kernel matrix and classifying. We first consider the LS-SVM with a linear kernel, and then discuss the generalization of our method to nonlinear kernels. Theoretical analysis shows our algorithm can make classification with arbitrary success probability in logarithmic runtime of both the dimension of data space and the number of data points for low rank, low condition number, and high dimensional data matrix, matching the runtime of the quantum SVM.},
  archive      = {J_TNNLS},
  author       = {Chen Ding and Tian-Yi Bao and He-Liang Huang},
  doi          = {10.1109/TNNLS.2021.3084467},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7210-7222},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quantum-inspired support vector machine},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic net nonparallel hyperplane support vector machine
and its geometrical rationality. <em>TNNLS</em>, <em>33</em>(12),
7199–7209. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin support vector machine (TWSVM), which constructs two nonparallel classifying hyperplanes, is widely applied to various fields. However, TWSVM solves two quadratic programming problems (QPPs) separately such that the final classifiers lack consistency and enough prediction accuracy. Moreover, by reason of only considering the 1-norm penalty for slack variables, TWSVM is not well defined in the geometrical view. In this article, we propose a novel elastic net nonparallel hyperplane support vector machine (ENNHSVM), which adopts elastic net penalty for slack variables and constructs two nonparallel separating hyperplanes simultaneously. We further discuss the properties of ENNHSVM theoretically and derive the violation tolerance upper bound to better demonstrate the relative violations of training samples in the same class. In particular, we design a safe screening rule for ENNHSVM to speed up the calculations. We finally compare the performance of ENNHSVM on both synthetic datasets and benchmark datasets with the Lagrangian SVM, the twin parametric-margin SVM, the elastic net SVM, the TWSVM, and the nonparallel hyperplane SVM.},
  archive      = {J_TNNLS},
  author       = {Kai Qi and Hu Yang},
  doi          = {10.1109/TNNLS.2021.3084404},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7199-7209},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Elastic net nonparallel hyperplane support vector machine and its geometrical rationality},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum margin multi-dimensional classification.
<em>TNNLS</em>, <em>33</em>(12), 7185–7198. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional classification (MDC) assumes heterogeneous class spaces for each example, where class variables from different class spaces characterize semantics of the example along different dimensions. The heterogeneity of class spaces leads to incomparability of the modeling outputs from different class spaces, which is the major difficulty in designing MDC approaches. In this article, we make a first attempt toward adapting maximum margin techniques for MDC problem and a novel approach named M3MDC is proposed. Specifically, M3MDC maximizes the margins between each pair of class labels with respect to individual class variable while modeling relationship across class variables (as well as class labels within individual class variable) via covariance regularization. The resulting formulation admits convex objective function with nonlinear constraints, which can be solved via alternating optimization with quadratic programming (QP) or closed-form solution in either alternating step. Comparative studies on the most comprehensive real-world MDC datasets to date are conducted and it is shown that M3MDC achieves highly competitive performance against state-of-the-art MDC approaches.},
  archive      = {J_TNNLS},
  author       = {Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1109/TNNLS.2021.3084373},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7185-7198},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum margin multi-dimensional classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural message passing with hierarchical layer
aggregation and neighbor normalization. <em>TNNLS</em>, <em>33</em>(12),
7172–7184. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a unified framework for graph neural networks, message passing-based neural network (MPNN) has attracted a lot of research interest and has been shown successfully in a number of domains in recent years. However, because of over-smoothing and vanishing gradients, deep MPNNs are still difficult to train. To alleviate these issues, we first introduce a deep hierarchical layer aggregation (DHLA) strategy, which utilizes a block-based layer aggregation to aggregate representations from different layers and transfers the output of the previous block to the subsequent block, so that deeper MPNNs can be easily trained. Additionally, to stabilize the training process, we also develop a novel normalization strategy, neighbor normalization (NeighborNorm), which normalizes the neighbor of each node to further address the training issue in deep MPNNs. Our analysis reveals that NeighborNorm can smooth the gradient of the loss function, i.e., adding NeighborNorm makes the optimization landscape much easier to navigate. Experimental results on two typical graph pattern-recognition tasks, including node classification and graph classification, demonstrate the necessity and effectiveness of the proposed strategies for graph message-passing neural networks.},
  archive      = {J_TNNLS},
  author       = {Xiaolong Fan and Maoguo Gong and Zedong Tang and Yue Wu},
  doi          = {10.1109/TNNLS.2021.3084319},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7172-7184},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep neural message passing with hierarchical layer aggregation and neighbor normalization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Observer-based output feedback event-triggered adaptive
control for linear multiagent systems under switching topologies.
<em>TNNLS</em>, <em>33</em>(12), 7161–7171. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consensus problem of general linear multiagent systems (MASs) is studied under switching topologies by using observer-based event-triggered control method in this article. On the basis of the output information of agents, two kinds of novel event-triggered adaptive control schemes are designed to achieve the leaderless and leader-follower consensus problems, which do not need to utilize the global information of the communication networks. Finally, two simulation examples are introduced to show that the consensus error converges to zero and Zeno behavior is eliminated in MASs. Compared with the existing output feedback control research, one of the significant advantages of our methods is that the controller protocols and triggering mechanisms do not rely on any global information, are independent of the network scale, and are fully distributed ways.},
  archive      = {J_TNNLS},
  author       = {Juan Zhang and Huaguang Zhang and Kun Zhang and Yuliang Cai},
  doi          = {10.1109/TNNLS.2021.3084317},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7161-7171},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based output feedback event-triggered adaptive control for linear multiagent systems under switching topologies},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global stabilization for a class of stochastic nonlinear
time-delay systems with unknown measurement drifts and its application.
<em>TNNLS</em>, <em>33</em>(12), 7153–7160. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the control problem for a class of stochastic nonlinear time-delay systems with uncertain output functions. Under the appropriate assumptions, a stabilization controller is explicitly constructed by applying the adding a power integrator method. Then, using the Lyapunov–Krasovskii functionals to address time-delay, it is proven that the designed controller can guarantee the closed-loop system to be globally asymptotically stable (GAS) in probability. Finally, two simulations show that the control strategy is effective and can be applied to the actual system.},
  archive      = {J_TNNLS},
  author       = {Qingtan Meng and Qian Ma},
  doi          = {10.1109/TNNLS.2021.3084295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7153-7160},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global stabilization for a class of stochastic nonlinear time-delay systems with unknown measurement drifts and its application},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Part-based semantic transform for few-shot semantic
segmentation. <em>TNNLS</em>, <em>33</em>(12), 7141–7152. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation remains an open problem for the lack of an effective method to handle the semantic misalignment between objects. In this article, we propose part-based semantic transform (PST) and target at aligning object semantics in support images with those in query images by semantic decomposition-and-match. The semantic decomposition process is implemented with prototype mixture models (PMMs), which use an expectation–maximization (EM) algorithm to decompose object semantics into multiple prototypes corresponding to object parts. The semantic match between prototypes is performed with a min-cost flow module, which encourages correct correspondence while depressing mismatches between object parts. With semantic decomposition-and-match, PST enforces the network’s tolerance to objects’ appearance and/or pose variation and facilities channelwise and spatial semantic activation of objects in query images. Extensive experiments on Pascal VOC and MS-COCO datasets show that PST significantly improves upon state-of-the-arts. In particular, on MS-COCO, it improves the performance of five-shot semantic segmentation by up to 7.79\% with a moderate cost of inference speed and model size. Code for PST is released at https://github.com/Yang-Bob/PST .},
  archive      = {J_TNNLS},
  author       = {Boyu Yang and Fang Wan and Chang Liu and Bohao Li and Xiangyang Ji and Qixiang Ye},
  doi          = {10.1109/TNNLS.2021.3084252},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7141-7152},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Part-based semantic transform for few-shot semantic segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neuromorphic context-dependent learning framework with
fault-tolerant spike routing. <em>TNNLS</em>, <em>33</em>(12),
7126–7140. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic computing is a promising technology that realizes computation based on event-based spiking neural networks (SNNs). However, fault-tolerant on-chip learning remains a challenge in neuromorphic systems. This study presents the first scalable neuromorphic fault-tolerant context-dependent learning (FCL) hardware framework. We show how this system can learn associations between stimulation and response in two context-dependent learning tasks from experimental neuroscience, despite possible faults in the hardware nodes. Furthermore, we demonstrate how our novel fault-tolerant neuromorphic spike routing scheme can avoid multiple fault nodes successfully and can enhance the maximum throughput of the neuromorphic network by 0.9\%–16.1\% in comparison with previous studies. By utilizing the real-time computational capabilities and multiple-fault-tolerant property of the proposed system, the neuronal mechanisms underlying the spiking activities of neuromorphic networks can be readily explored. In addition, the proposed system can be applied in real-time learning and decision-making applications, brain–machine integration, and the investigation of brain cognition during learning.},
  archive      = {J_TNNLS},
  author       = {Shuangming Yang and Jiang Wang and Bin Deng and Mostafa Rahimi Azghadi and Bernabe Linares-Barranco},
  doi          = {10.1109/TNNLS.2021.3084250},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7126-7140},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuromorphic context-dependent learning framework with fault-tolerant spike routing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel time-series memory auto-encoder with sequentially
updated reconstructions for remaining useful life prediction.
<em>TNNLS</em>, <em>33</em>(12), 7114–7125. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the significant tasks in remaining useful life (RUL) prediction is to find a good health indicator (HI) that can effectively represent the degradation process of a system. However, it is difficult for traditional data-driven methods to construct accurate HIs due to their incomprehensive consideration of temporal dependencies within the monitoring data, especially for aeroengines working under nonstationary operating conditions (OCs). Aiming at this problem, this article develops a novel unsupervised deep neural network, the so-called times series memory auto-encoder with sequentially updated reconstructions (SUR-TSMAE) to improve the accuracy of extracted HIs, which directly takes the multidimensional time series as input to simultaneously achieve feature extraction from both feature-dimension and time-dimension. Further, to make full use of the temporal dependencies, a novel long-short time memory with sequentially updated reconstructions (SUR-LSTM), which uses the errors not only from the current memory cell but also from subsequent memory cells to update the output layer’s weight of the current memory cell, is developed to act as the reconstructed layer in the SUR-TSMAE. The use of SUR-LSTM can help the SUR-TSMAE rapidly reconstruct the input time series with higher precision. Experimental results on a public dataset demonstrate the outstanding performance of SUR-TSMAE in comparison with some existing methods.},
  archive      = {J_TNNLS},
  author       = {Song Fu and Shisheng Zhong and Lin Lin and Minghang Zhao},
  doi          = {10.1109/TNNLS.2021.3084249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7114-7125},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel time-series memory auto-encoder with sequentially updated reconstructions for remaining useful life prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based topological mapping and navigation with
self-organizing neural networks. <em>TNNLS</em>, <em>33</em>(12),
7101–7113. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial mapping and navigation are critical cognitive functions of autonomous agents, enabling one to learn an internal representation of an environment and move through space with real-time sensory inputs, such as visual observations. Existing models for vision-based mapping and navigation, however, suffer from memory requirements that increase linearly with exploration duration and indirect path following behaviors. This article presents ${e}$ -TM, a self-organizing neural network-based framework for incremental topological mapping and navigation. ${e}$ -TM models the exploration trajectories explicitly as episodic memory, wherein salient landmarks are sequentially extracted as “events” from streaming observations. A memory consolidation procedure then performs a playback mechanism and transfers the embedded knowledge of the environmental layout into spatial memory, encoding topological relations between landmarks. Fusion adaptive resonance theory (ART) networks, as the building block of the two memory modules, can generalize multiple input patterns into memory templates and, therefore, provide a compact spatial representation and support the discovery of novel shortcuts through inferences. For navigation, ${e}$ -TM applies a transfer learning paradigm to integrate human demonstrations into a pretrained locomotion network for smoother movements. Experimental results based on VizDoom, a simulated 3-D environment, have shown that, compared to semiparametric topological memory (SPTM), a state-of-the-art model, ${e}$ -TM reduces the time costs of navigation significantly while learning much sparser topological graphs.},
  archive      = {J_TNNLS},
  author       = {Yue Hu and Budhitama Subagdja and Ah-Hwee Tan and Quanjun Yin},
  doi          = {10.1109/TNNLS.2021.3084212},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7101-7113},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Vision-based topological mapping and navigation with self-organizing neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Filter sketch for network pruning. <em>TNNLS</em>,
<em>33</em>(12), 7091–7100. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel network pruning approach by information preserving of pretrained network weights (filters). Network pruning with the information preserving is formulated as a matrix sketch problem, which is efficiently solved by the off-the-shelf frequent direction method. Our approach, referred to as FilterSketch, encodes the second-order information of pretrained weights, which enables the representation capacity of pruned networks to be recovered with a simple fine-tuning procedure. FilterSketch requires neither training from scratch nor data-driven iterative optimization, leading to a several-orders-of-magnitude reduction of time cost in the optimization of pruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3\% of floating-point operations (FLOPs) and prunes 59.9\% of network parameters with negligible accuracy cost for ResNet-110. On ILSVRC-2012, it reduces 45.5\% of FLOPs and removes 43.0\% of parameters with only 0.69\% accuracy drop for ResNet-50. Our code and pruned models can be found at https://github.com/lmbxmu/FilterSketch .},
  archive      = {J_TNNLS},
  author       = {Mingbao Lin and Liujuan Cao and Shaojie Li and Qixiang Ye and Yonghong Tian and Jianzhuang Liu and Qi Tian and Rongrong Ji},
  doi          = {10.1109/TNNLS.2021.3084206},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7091-7100},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Filter sketch for network pruning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial caching training: Unsupervised inductive network
representation learning on large-scale graphs. <em>TNNLS</em>,
<em>33</em>(12), 7079–7090. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning (NRL) has far-reaching effects on data mining research, showing its importance in many real-world applications. NRL, also known as network embedding, aims at preserving graph structures in a low-dimensional space. These learned representations can be used for subsequent machine learning tasks, such as vertex classification, link prediction, and data visualization. Recently, graph convolutional network (GCN)-based models, e.g., GraphSAGE, have drawn a lot of attention for their success in inductive NRL. When conducting unsupervised learning on large-scale graphs, some of these models employ negative sampling (NS) for optimization, which encourages a target vertex to be close to its neighbors while being far from its negative samples. However, NS draws negative vertices through a random pattern or based on the degrees of vertices. Thus, the generated samples could be either highly relevant or completely unrelated to the target vertex. Moreover, as the training goes, the gradient of NS objective calculated with the inner product of the unrelated negative samples and the target vertex may become zero, which will lead to learning inferior representations. To address these problems, we propose an adversarial training method tailored for unsupervised inductive NRL on large networks. For efficiently keeping track of high-quality negative samples, we design a caching scheme with sampling and updating strategies that has a wide exploration of vertex proximity while considering training costs. Besides, the proposed method is adaptive to various existing GCN-based models without significantly complicating their optimization process. Extensive experiments show that our proposed method can achieve better performance compared with the state-of-the-art models.},
  archive      = {J_TNNLS},
  author       = {Junyang Chen and Zhiguo Gong and Wei Wang and Cong Wang and Zhenghua Xu and Jianming Lv and Xueliang Li and Kaishun Wu and Weiwen Liu},
  doi          = {10.1109/TNNLS.2021.3084195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7079-7090},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial caching training: Unsupervised inductive network representation learning on large-scale graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel graph-based trajectory predictor with pseudo-oracle.
<em>TNNLS</em>, <em>33</em>(12), 7064–7078. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction in dynamic scenes remains a challenging and critical problem in numerous applications, such as self-driving cars and socially aware robots. Challenges concentrate on capturing pedestrians’ motion patterns and social interactions, as well as handling the future uncertainties. Recent studies focus on modeling pedestrians’ motion patterns with recurrent neural networks, capturing social interactions with pooling- or graph-based methods, and handling future uncertainties by using the random Gaussian noise as the latent variable. However, they do not integrate specific obstacle avoidance experiences (OAEs) that may improve prediction performance. For example, pedestrians’ future trajectories are always influenced by others in front. Here, we propose the Graph-based Trajectory Predictor with Pseudo-Oracle (GTPPO), an encoder–decoder-based method conditioned on pedestrians’ future behaviors. Pedestrians’ motion patterns are encoded with a long short-term memory unit, which introduces temporal attention to highlight specific time steps. Their interactions are captured by a graph-based attention mechanism, which draws OAE into the data-driven learning process of graph attention. Future uncertainties are handled by generating multimodal outputs with an informative latent variable. Such a variable is generated by a novel pseudo-oracle predictor, which minimizes the knowledge gap between historical and ground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY, and Stanford Drone datasets, and the results demonstrate state-of-the-art performance. Besides, the qualitative evaluations show successful cases of handling sudden motion changes in the future. Such findings indicate that GTPPO can peek into the future.},
  archive      = {J_TNNLS},
  author       = {Biao Yang and Guocheng Yan and Pin Wang and Ching-Yao Chan and Xiang Song and Yang Chen},
  doi          = {10.1109/TNNLS.2021.3084143},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7064-7078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel graph-based trajectory predictor with pseudo-oracle},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synchronization rather than finite-time synchronization
results of fractional-order multi-weighted complex networks.
<em>TNNLS</em>, <em>33</em>(12), 7052–7063. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the synchronization of fractional-order multi-weighted complex networks (FMWCNs) with order $\alpha \in (0,1)$ . A useful fractional-order inequality ${}_{t_{0}}^{C} D_{t}^{\alpha } V(x(t))\leq -\mu V(x(t))$ is extended to a more general form ${}_{t_{0}}^{C} D_{t}^{\alpha } V(x(t))\leq -\mu V^{\gamma }(x(t)),\gamma \in (0,1]$ , which plays a pivotal role in studies of synchronization for FMWCNs. However, the inequality ${}_{t_{0}}^{C} D_{t}^{\alpha } V(x(t))\leq -\mu V^{\gamma }(x(t)),\gamma \in (0,1)$ has been applied to achieve the finite-time synchronization for fractional-order systems in the absence of rigorous mathematical proofs. Based on reduction to absurdity in this article, we prove that it cannot be used to obtain finite-time synchronization results under bounded nonzero initial value conditions. Moreover, by using feedback control strategy and Lyapunov direct approach, some sufficient conditions are presented in the forms of linear matrix inequalities (LMIs) to ensure the synchronization for FMWCNs in the sense of a widely accepted definition of synchronization. Meanwhile, these proposed sufficient results cannot guarantee the finite-time synchronization of FMWCNs. Finally, two chaotic systems are given to verify the feasibility of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiangqian Yao and Yu Liu and Zhijun Zhang and Weiwei Wan},
  doi          = {10.1109/TNNLS.2021.3083886},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7052-7063},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization rather than finite-time synchronization results of fractional-order multi-weighted complex networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On information plane analyses of neural network
classifiers—a review. <em>TNNLS</em>, <em>33</em>(12), 7039–7051. (<a
href="https://doi.org/10.1109/TNNLS.2021.3089037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review the current literature concerned with information plane (IP) analyses of neural network (NN) classifiers. While the underlying information bottleneck theory and the claim that information-theoretic compression is causally linked to generalization are plausible, empirical evidence was found to be both supporting and conflicting. We review this evidence together with a detailed analysis of how the respective information quantities were estimated. Our survey suggests that compression visualized in IPs is not necessarily information-theoretic but is rather often compatible with geometric compression of the latent representations. This insight gives the IP a renewed justification. Aside from this, we shed light on the problem of estimating mutual information in deterministic NNs and its consequences. Specifically, we argue that, even in feedforward NNs, the data processing inequality needs not to hold for estimates of mutual information. Similarly, while a fitting phase, in which the mutual information is between the latent representation and the target increases, is necessary (but not sufficient) for good classification performance, depending on the specifics of mutual information estimation, such a fitting phase needs to not be visible in the IP.},
  archive      = {J_TNNLS},
  author       = {Bernhard C. Geiger},
  doi          = {10.1109/TNNLS.2021.3089037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7039-7051},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On information plane analyses of neural network Classifiers—A review},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of modulation classification using deep learning:
Signal representation and data preprocessing. <em>TNNLS</em>,
<em>33</em>(12), 7020–7038. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modulation classification is one of the key tasks for communications systems monitoring, management, and control for addressing technical issues, including spectrum awareness, adaptive transmissions, and interference avoidance. Recently, deep learning (DL)-based modulation classification has attracted significant attention due to its superiority in feature extraction and classification accuracy. In DL-based modulation classification, one major challenge is to preprocess a received signal and represent it in a proper format before feeding the signal into deep neural networks. This article provides a comprehensive survey of the state-of-the-art DL-based modulation classification algorithms, especially the techniques of signal representation and data preprocessing utilized in these algorithms. Since a received signal can be represented by either features, images, sequences, or a combination of them, existing algorithms of DL-based modulation classification can be categorized into four groups and are reviewed accordingly in this article. Furthermore, the advantages as well as disadvantages of each signal representation method are summarized and discussed.},
  archive      = {J_TNNLS},
  author       = {Shengliang Peng and Shujun Sun and Yu-Dong Yao},
  doi          = {10.1109/TNNLS.2021.3085433},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {7020-7038},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of modulation classification using deep learning: Signal representation and data preprocessing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of convolutional neural networks: Analysis,
applications, and prospects. <em>TNNLS</em>, <em>33</em>(12), 6999–7019.
(<a href="https://doi.org/10.1109/TNNLS.2021.3084827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A convolutional neural network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention from both industry and academia in the past few years. The existing reviews mainly focus on CNN’s applications in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide some novel ideas and prospects in this fast-growing field. Besides, not only 2-D convolution but also 1-D and multidimensional ones are involved. First, this review introduces the history of CNN. Second, we provide an overview of various convolutions. Third, some classic and advanced CNN models are introduced; especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for functions and hyperparameter selection. Fifth, the applications of 1-D, 2-D, and multidimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed as guidelines for future work.},
  archive      = {J_TNNLS},
  author       = {Zewen Li and Fan Liu and Wenjie Yang and Shouheng Peng and Jun Zhou},
  doi          = {10.1109/TNNLS.2021.3084827},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {12},
  pages        = {6999-7019},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of convolutional neural networks: Analysis, applications, and prospects},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-prototypical learning for domain-agnostic few-shot
recognition. <em>TNNLS</em>, <em>33</em>(11), 6990–6996. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to classify novel images based on a few labeled samples with the help of meta-knowledge. Most previous works address this problem based on the hypothesis that the training set and testing set are from the same domain, which is not realistic for some real-world applications. Thus, we extend FSL to domain-agnostic few-shot recognition, where the domain of the testing task is unknown. In domain-agnostic few-shot recognition, the model is optimized on data from one domain and evaluated on tasks from different domains. Previous methods for FSL mostly focus on learning general features or adapting to few-shot tasks effectively. They suffer from inappropriate features or complex adaptation in domain-agnostic few-shot recognition. In this brief, we propose meta-prototypical learning to address this problem. In particular, a meta-encoder is optimized to learn the general features. Different from the traditional prototypical learning, the meta encoder can effectively adapt to few-shot tasks from different domains by the traces of the few labeled examples. Experiments on many datasets demonstrate that meta-prototypical learning performs competitively on traditional few-shot tasks, and on few-shot tasks from different domains, meta-prototypical learning outperforms related methods.},
  archive      = {J_TNNLS},
  author       = {Rui-Qi Wang and Xu-Yao Zhang and Cheng-Lin Liu},
  doi          = {10.1109/TNNLS.2021.3083650},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6990-6996},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta-prototypical learning for domain-agnostic few-shot recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Frequency principle in broad learning system.
<em>TNNLS</em>, <em>33</em>(11), 6983–6989. (<a
href="https://doi.org/10.1109/TNNLS.2021.3081568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved breakthrough improvement in various application fields. Nevertheless, they usually suffer from a time-consuming training process because of the complicated structures of neural networks with a huge number of parameters. As an alternative, a fast and efficient discriminative broad learning system (BLS) is proposed, which takes the advantages of flat structure and incremental learning. The BLS has achieved outstanding performance in classification and regression problems. However, the previous studies ignored the reason why the BLS can generalize well. In this article, we focus on the interpretation from the viewpoint of the frequency domain. We discover the existence of the frequency principle in BLS, i.e., the BLS preferentially captures low-frequency components quickly and then fits the high frequencies during the incremental process of adding feature nodes and enhancement nodes. The frequency principle may be of great inspiration for expanding the application of BLS.},
  archive      = {J_TNNLS},
  author       = {Guang-Yong Chen and Min Gan and C. L. Philip Chen and Hong-Tao Zhu and Long Chen},
  doi          = {10.1109/TNNLS.2021.3081568},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6983-6989},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Frequency principle in broad learning system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Breaking neural reasoning architectures with metamorphic
relation-based adversarial examples. <em>TNNLS</em>, <em>33</em>(11),
6976–6982. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to read, reason, and infer lies at the heart of neural reasoning architectures. After all, the ability to perform logical reasoning over language remains a coveted goal of Artificial Intelligence. To this end, models such as the Turing-complete differentiable neural computer (DNC) boast of real logical reasoning capabilities, along with the ability to reason beyond simple surface-level matching. In this brief, we propose the first probe into DNC’s logical reasoning capabilities with a focus on text-based question answering (QA). More concretely, we propose a conceptually simple but effective adversarial attack based on metamorphic relations. Our proposed adversarial attack reduces DNCs’ state-of-the-art accuracy from 100\% to 1.5\% in the worst case, exposing weaknesses and susceptibilities in modern neural reasoning architectures. We further empirically explore possibilities to defend against such attacks and demonstrate the utility of our adversarial framework as a simple scalable method to improve model adversarial robustness.},
  archive      = {J_TNNLS},
  author       = {Alvin Chan and Lei Ma and Felix Juefei-Xu and Yew-Soon Ong and Xiaofei Xie and Minhui Xue and Yang Liu},
  doi          = {10.1109/TNNLS.2021.3072166},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6976-6982},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Breaking neural reasoning architectures with metamorphic relation-based adversarial examples},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning binary hash codes based on adaptable label
representations. <em>TNNLS</em>, <em>33</em>(11), 6961–6975. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of supervised hashing is to construct hash mappings from collections of images and semantic annotations such that semantically relevant images are embedded nearby in the learned binary hash representations. Existing deep supervised hashing approaches that employ classification frameworks with a classification training objective for learning hash codes often encode class labels as one-hot or multi-hot vectors. We argue that such label encodings do not well reflect semantic relations among classes and instead, effective class label representations ought to be learned from data, which could provide more discriminative signals for hashing. In this article, we introduce Adaptive Labeling Deep Hashing (AdaLabelHash) that learns binary hash codes based on learnable class label representations. We treat the class labels as the vertices of a $K$ -dimensional hypercube, which are trainable variables and adapted together with network weights during the backward network training procedure. The label representations, referred to as codewords, are the target outputs of hash mapping learning. In the label space, semantically relevant images are then expressed by the codewords that are nearby regarding Hamming distances, yielding compact and discriminative binary hash representations. Furthermore, we find that the learned label representations well reflect semantic relations. Our approach is easy to realize and can simultaneously construct both the label representations and the compact binary embeddings. Quantitative and qualitative evaluations on several popular benchmarks validate the superiority of AdaLabelHash in learning effective binary codes for image search.},
  archive      = {J_TNNLS},
  author       = {Huei-Fang Yang and Cheng-Hao Tu and Chu-Song Chen},
  doi          = {10.1109/TNNLS.2021.3095399},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6961-6975},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning binary hash codes based on adaptable label representations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A distributed lyapunov-based redesign approach for
heterogeneous uncertain agents with cooperation–competition
interactions. <em>TNNLS</em>, <em>33</em>(11), 6946–6960. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A swarming behavior problem is investigated in this article for heterogeneous uncertain agents with cooperation–competition interactions. In such a problem, the agents are described by second-order continuous systems with different intrinsic nonlinear terms, which satisfies the “linearity-in-parameters” condition, and the agents’ models are coupled together through a distributed protocol containing the information of competitive neighbors. Then, for four different types of cooperation–competition networks, a distributed Lyapunov-based redesign approach is proposed for the heterogeneous uncertain agents, where the distributed controller and the estimation laws of unknown parameters are obtained. Under their joint actions, the heterogeneous uncertain multiagent system can achieve distributed stabilization for structurally unbalanced networks and output bipartite consensus for structurally balanced networks. In particular, the concept of coherent networks is proposed for structurally unbalanced directed networks, which is beneficial to the design of distributed controllers. Finally, four illustrative examples are given to show the effectiveness of the designed distributed controller.},
  archive      = {J_TNNLS},
  author       = {Hong-Xiang Hu and Changyun Wen and Guanghui Wen},
  doi          = {10.1109/TNNLS.2021.3084142},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6946-6960},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A distributed lyapunov-based redesign approach for heterogeneous uncertain agents with Cooperation–Competition interactions},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Tracking by joint local and global search: A target-aware
attention-based approach. <em>TNNLS</em>, <em>33</em>(11), 6931–6945.
(<a href="https://doi.org/10.1109/TNNLS.2021.3083933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking-by-detection is a very popular framework for single-object tracking that attempts to search the target object within a local search window for each frame. Although such a local search mechanism works well on simple videos, however, it makes the trackers sensitive to extremely challenging scenarios, such as heavy occlusion and fast motion. In this article, we propose a novel and general target-aware attention mechanism (termed TANet) and integrate it with a tracking-by-detection framework to conduct joint local and global search for robust tracking. Specifically, we extract the features of the target object patch and continuous video frames; then, we concatenate and feed them into a decoder network to generate target-aware global attention maps. More importantly, we resort to adversarial training for better attention prediction. The appearance and motion discriminator networks are designed to ensure its consistency in spatial and temporal views. In the tracking procedure, we integrate target-aware attention with multiple trackers by exploring candidate search regions for robust tracking. Extensive experiments on both short- and long-term tracking benchmark datasets all validated the effectiveness of our algorithm.},
  archive      = {J_TNNLS},
  author       = {Xiao Wang and Jin Tang and Bin Luo and Yaowei Wang and Yonghong Tian and Feng Wu},
  doi          = {10.1109/TNNLS.2021.3083933},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6931-6945},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tracking by joint local and global search: A target-aware attention-based approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilayer sparsity-based tensor decomposition for low-rank
tensor completion. <em>TNNLS</em>, <em>33</em>(11), 6916–6930. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for tensor completion (TC) have limited ability for characterizing low-rank (LR) structures. To depict the complex hierarchical knowledge with implicit sparsity attributes hidden in a tensor, we propose a new multilayer sparsity-based tensor decomposition (MLSTD) for the low-rank tensor completion (LRTC). The method encodes the structured sparsity of a tensor by the multiple-layer representation. Specifically, we use the CANDECOMP/PARAFAC (CP) model to decompose a tensor into an ensemble of the sum of rank-1 tensors, and the number of rank-1 components is easily interpreted as the first-layer sparsity measure. Presumably, the factor matrices are smooth since local piecewise property exists in within-mode correlation. In subspace, the local smoothness can be regarded as the second-layer sparsity. To describe the refined structures of factor/subspace sparsity, we introduce a new sparsity insight of subspace smoothness: a self-adaptive low-rank matrix factorization (LRMF) scheme, called the third-layer sparsity. By the progressive description of the sparsity structure, we formulate an MLSTD model and embed it into the LRTC problem. Then, an effective alternating direction method of multipliers (ADMM) algorithm is designed for the MLSTD minimization problem. Various experiments in RGB images, hyperspectral images (HSIs), and videos substantiate that the proposed LRTC methods are superior to state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jize Xue and Yongqiang Zhao and Shaoguang Huang and Wenzhi Liao and Jonathan Cheung-Wai Chan and Seong G. Kong},
  doi          = {10.1109/TNNLS.2021.3083931},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6916-6930},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilayer sparsity-based tensor decomposition for low-rank tensor completion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-event-triggered output control of neural networks
with mixed delays. <em>TNNLS</em>, <em>33</em>(11), 6905–6915. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of memory-event-triggered $H_{\infty }$ output feedback control for neural networks with mixed delays (discrete and distributed delays). The probability density of the communication delay among neurons is modeled as the kernel of the distributed delay. To reduce network communication burden, a novel memory-event-triggered scheme (METS) using the historical system output is introduced to choose which data should be sent to the controller. Based on a constructed Lyapunov–Krasovskii functional (LKF) with the distributed delay kernel and a generalized integral inequality, new sufficient conditions are formed by linear matrix inequalities (LMIs) for designing an event-triggered $H_{\infty }$ controller. Finally, experiments based on a computer and a real wireless network are executed to confirm the validity of the developed method.},
  archive      = {J_TNNLS},
  author       = {Shen Yan and Zhou Gu and Sing Kiong Nguang},
  doi          = {10.1109/TNNLS.2021.3083898},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6905-6915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory-event-triggered output control of neural networks with mixed delays},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Adaptive finite-time command filtered controller design for
nonlinear systems with output constraints and input nonlinearities.
<em>TNNLS</em>, <em>33</em>(11), 6893–6904. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses a finite-time tracking control issue for a class of nonlinear systems with asymmetric time-varying output constraints and input nonlinearities. To guarantee the finite-time convergence of tracking errors, a novel finite-time command filtered backstepping approach is presented by using the command filtered backstepping technique, finite-time theory, and barrier Lyapunov functions. The newly proposed method can not only reduce the complexity of computation of the conventional backstepping control and compensate filtered errors caused by dynamic surface control but also can ensure that the output variables are restricted in compact bounding sets. Moreover, the proposed controller is applied to robot manipulator systems, which guarantees the practical boundedness of all the signals in the closed-loop system. Finally, the effectiveness and practicability of the developed control strategy are validated by a simulation example.},
  archive      = {J_TNNLS},
  author       = {Kun Wang and Xiaoping Liu and Yuanwei Jing},
  doi          = {10.1109/TNNLS.2021.3083800},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6893-6904},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive finite-time command filtered controller design for nonlinear systems with output constraints and input nonlinearities},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised feature selection via orthogonal basis
clustering and local structure preserving. <em>TNNLS</em>,
<em>33</em>(11), 6881–6892. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the “curse of dimensionality” issue, how to discard redundant features and select informative features in high-dimensional data has become a critical problem, hence there are many research studies dedicated to solving this problem. Unsupervised feature selection technique, which does not require any prior category information to conduct with, has gained a prominent place in preprocessing high-dimensional data among all feature selection techniques, and it has been applied to many neural networks and learning systems related applications, e.g., pattern classification. In this article, we propose an efficient method for unsupervised feature selection via orthogonal basis clustering and reliable local structure preserving, which is referred to as OCLSP briefly. Our OCLSP method consists of an orthogonal basis clustering together with an adaptive graph regularization, which realizes the functionality of simultaneously achieving excellent cluster separation and preserving the local information of data. Besides, we exploit an efficient alternative optimization algorithm to solve the challenging optimization problem of our proposed OCLSP method, and we perform a theoretical analysis of its computational complexity and convergence. Eventually, we conduct comprehensive experiments on nine real-world datasets to test the validity of our proposed OCLSP method, and the experimental results demonstrate that our proposed OCLSP method outperforms many state-of-the-art unsupervised feature selection methods in terms of clustering accuracy and normalized mutual information, which indicates that our proposed OCLSP method has a strong ability in identifying more important features.},
  archive      = {J_TNNLS},
  author       = {Xiaochang Lin and Jiewen Guan and Bilian Chen and Yifeng Zeng},
  doi          = {10.1109/TNNLS.2021.3083763},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6881-6892},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised feature selection via orthogonal basis clustering and local structure preserving},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developmental network-2: The autonomous generation of
optimal internal-representation hierarchy. <em>TNNLS</em>,
<em>33</em>(11), 6867–6880. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very challenging for machine learning methods to reach the goal of general-purpose learning since there are so many complicated situations in different tasks. The learning methods need to generate flexible internal representations for all scenarios met before. The hierarchical internal representation is considered as an efficient way to build such flexible representations. By hierarchy, we mean important local features in the input can be combined to form higher level features with more context. In this work, we analyze how our proposed general-purpose learning framework—the developmental network-2 (DN-2)—autonomously generates internal hierarchy with new mechanisms. Specifically, DN-2 incrementally allocates neuronal resources to different levels of representation during learning instead of handcrafting static boundaries among different levels of representation. We present the mathematical proof to demonstrate that optimal properties in terms of maximum likelihood (ML) are established under the conditions of limited learning experience and resources. The phoneme recognition and real-world visual navigation experiments that are of different modalities and include many different situations are designed to investigate general-purpose learning capability of DN-2. The experimental results show that DN-2 successfully learns different tasks. The formed internal hierarchical representations focus on important features, and the invariant abstract arise from optimal internal representations. We believe that DN-2 is in the right way toward fully autonomous learning.},
  archive      = {J_TNNLS},
  author       = {Xiang Wu and Zejia Zheng and Juyang Weng},
  doi          = {10.1109/TNNLS.2021.3083759},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6867-6880},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Developmental network-2: The autonomous generation of optimal internal-representation hierarchy},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble support vector recurrent neural network for brain
signal detection. <em>TNNLS</em>, <em>33</em>(11), 6856–6866. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain–computer interface (BCI) P300 speller analyzes the P300 signals from the brain to achieve direct communication between humans and machines, which can assist patients with severe disabilities to control external machines or robots to complete expected tasks. Therefore, the classification method of P300 signals plays an important role in the development of BCI systems and technologies. In this article, a novel ensemble support vector recurrent neural network (E-SVRNN) framework is proposed and developed to acquire more accurate and efficient electroencephalogram (EEG) signal classification results. First, we construct a support vector machine (SVM) to formulate EEG signals recognizing model. Second, the SVM formulation is transformed into a standard convex quadratic programming (QP) problem. Third, the convex QP problem is solved by combining a varying parameter recurrent neural network (VPRNN) with a penalty function. Experimental results on BCI competition II and BCI competition III datasets demonstrate that the proposed E-SVRNN framework can achieve accuracy rates as high as 100\% and 99\%, respectively. In addition, the results of comparison experiments verify that the proposed E-SVRNN possesses the best recognition accuracy and information transfer rate (ITR) compared with most of the state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhijun Zhang and Guangqiang Chen and Song Yang},
  doi          = {10.1109/TNNLS.2021.3083710},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6856-6866},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ensemble support vector recurrent neural network for brain signal detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised adaptive embedding for dimensionality
reduction. <em>TNNLS</em>, <em>33</em>(11), 6844–6855. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data are highly correlative and redundant, making it difficult to explore and analyze. Amount of unsupervised dimensionality reduction (DR) methods has been proposed, in which constructing a neighborhood graph is the primary step of DR methods. However, there exist two problems: 1) the construction of graph is usually separate from the selection of projection direction and 2) the original data are inevitably noisy. In this article, we propose an unsupervised adaptive embedding (UAE) method for DR to solve these challenges, which is a linear graph-embedding method. First, an adaptive allocation method of neighbors is proposed to construct the affinity graph. Second, the construction of affinity graph and calculation of projection matrix are integrated together. It considers the local relationship between samples and global characteristic of high-dimensional data, in which the cleaned data matrix is originally proposed to remove noise in subspace. The relationship between our method and local preserving projections (LPPs) is also explored. Finally, an alternative iteration optimization algorithm is derived to solve our model, the convergence and computational complexity of which are also analyzed. Comprehensive experiments on synthetic and benchmark datasets illustrate the superiority of our method.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Fangyuan Xie and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3083695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6844-6855},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised adaptive embedding for dimensionality reduction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communication-censored distributed stochastic gradient
descent. <em>TNNLS</em>, <em>33</em>(11), 6831–6843. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a communication-efficient algorithm to solve the stochastic optimization problem defined over a distributed network, aiming at reducing the burdensome communication in applications, such as distributed machine learning. Different from the existing works based on quantization and sparsification, we introduce a communication-censoring technique to reduce the transmissions of variables, which leads to our communication-censored distributed stochastic gradient descent (CSGD) algorithm. Specifically, in CSGD, the latest minibatch stochastic gradient at a worker will be transmitted to the server if and only if it is sufficiently informative. When the latest gradient is not available, the stale one will be reused at the server. To implement this communication-censoring strategy, the batch size is increasing in order to alleviate the effect of stochastic gradient noise. Theoretically, CSGD enjoys the same order of convergence rate as that of SGD but effectively reduces communication. Numerical experiments demonstrate the sizable communication saving of CSGD.},
  archive      = {J_TNNLS},
  author       = {Weiyu Li and Zhaoxian Wu and Tianyi Chen and Liping Li and Qing Ling},
  doi          = {10.1109/TNNLS.2021.3083655},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6831-6843},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Communication-censored distributed stochastic gradient descent},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A privacy-preserving semisupervised algorithm under maximum
correntropy criterion. <em>TNNLS</em>, <em>33</em>(11), 6817–6830. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing semisupervised learning approaches generally focus on the single-agent (centralized) setting, and hence, there is the risk of privacy leakage during joint data processing. At the same time, using the mean square error criterion in such approaches does not allow one to efficiently deal with problems involving non-Gaussian distribution. Thus, in this article, we present a novel privacy-preserving semisupervised algorithm under the maximum correntropy criterion (MCC). The proposed algorithm allows us to share data among different entities while effectively mitigating the risk of privacy leaks. In addition, under MCC, our proposed approach works well for data with non-Gaussian distribution noise. Our experiments on three different learning tasks demonstrate that our method distinctively outperforms the related algorithms in common regression learning scenarios.},
  archive      = {J_TNNLS},
  author       = {Ling Zuo and Yinghan Xu and Chi Cheng and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TNNLS.2021.3083535},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6817-6830},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A privacy-preserving semisupervised algorithm under maximum correntropy criterion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model-driven deep unfolding method for JPEG artifacts
removal. <em>TNNLS</em>, <em>33</em>(11), 6802–6816. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have achieved notable progress in removing blocking artifacts caused by lossy JPEG compression on images. However, most deep learning-based methods handle this task by designing black-box network architectures to directly learn the relationships between the compressed images and their clean versions. These network architectures are always lack of sufficient interpretability, which limits their further improvements in deblocking performance. To address this issue, in this article, we propose a model-driven deep unfolding method for JPEG artifacts removal, with interpretable network structures. First, we build a maximum posterior (MAP) model for deblocking using convolutional dictionary learning and design an iterative optimization algorithm using proximal operators. Second, we unfold this iterative algorithm into a learnable deep network structure, where each module corresponds to a specific operation of the iterative algorithm. In this way, our network inherits the benefits of both the powerful model ability of data-driven deep learning method and the interpretability of traditional model-driven method. By training the proposed network in an end-to-end manner, all learnable modules can be automatically explored to well characterize the representations of both JPEG artifacts and image content. Experiments on synthetic and real-world datasets show that our method is able to generate competitive or even better deblocking results, compared with state-of-the-art methods both quantitatively and qualitatively.},
  archive      = {J_TNNLS},
  author       = {Xueyang Fu and Menglu Wang and Xiangyong Cao and Xinghao Ding and Zheng-Jun Zha},
  doi          = {10.1109/TNNLS.2021.3083504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6802-6816},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A model-driven deep unfolding method for JPEG artifacts removal},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fault diagnosis of hydraulic systems based on deep learning
model with multirate data samples. <em>TNNLS</em>, <em>33</em>(11),
6789–6801. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hydraulic systems are a class of typical complex nonlinear systems, which have been widely used in manufacturing, metallurgy, energy, and other industries. Nowadays, the intelligent fault diagnosis problem of hydraulic systems has received increasing attention for it can increase operational safety and reliability, reduce maintenance cost, and improve productivity. However, because of the high nonlinear and strong fault concealment, the fault diagnosis of hydraulic systems is still a challenging task. Besides, the data samples collected from the hydraulic system are always in different sampling rates, and the coupling relationship between the components brings difficulties to accurate data acquisition. To solve the above issues, a deep learning model with multirate data samples is proposed in this article, which can extract features from the multirate sampling data automatically without expertise, thus it is more suitable in the industrial situation. Experiment results demonstrate that the proposed method achieves high diagnostic and fault pattern recognition accuracy even when the imbalance degree of sample data is as large as 1:100. Moreover, the proposed method can increase about 10\% diagnosis accuracy when compared with some state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Keke Huang and Shujie Wu and Fanbiao Li and Chunhua Yang and Weihua Gui},
  doi          = {10.1109/TNNLS.2021.3083401},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6789-6801},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fault diagnosis of hydraulic systems based on deep learning model with multirate data samples},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Top-k partial label machine. <em>TNNLS</em>,
<em>33</em>(11), 6775–6788. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with ambiguities in partial label learning (PLL), the existing PLL methods implement disambiguations, by either identifying the ground-truth label or averaging the candidate labels. However, these methods can be easily misled by the false-positive labels in the candidate label set. We find that these ambiguities often originate from the noise caused by highly correlated or overlapping candidate labels, which leads to the difficulty in identifying the ground-truth label on the first attempt. To give the trained models more tolerance, we first propose the top- $k$ partial loss and convex top- $k$ partial hinge loss. Based on the losses, we present a novel top- $k$ partial label machine (TPLM) for partial label classification. An efficient optimization algorithm is proposed based on accelerated proximal stochastic dual coordinate ascent (Prox-SDCA) and linear programming (LP). Moreover, we present a theoretical analysis of the generalization error for TPLM. Comprehensive experiments on both controlled UCI datasets and real-world partial label datasets demonstrate that the proposed method is superior to the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Xiuwen Gong and Dong Yuan and Wei Bao},
  doi          = {10.1109/TNNLS.2021.3083397},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6775-6788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Top-k partial label machine},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-/fixed-time stability of nonautonomous functional
differential inclusion: Lyapunov approach involving indefinite
derivative. <em>TNNLS</em>, <em>33</em>(11), 6763–6774. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a type of nonautonomous delayed differential equation (DDE) with discontinuity. Under the framework of the Filippov state solution, the finite-time stability (FNTS)/fixed-time stability (FXTS) problems of nonautonomous functional differential inclusion (FDI) are studied via the generalized Lyapunov functional method. The generalized Lyapunov functional used in this article is allowed to obtain an indefinite time derivative almost anywhere (a.a.) along the system’s state solutions. Nevertheless, the classic Lyapunov functional requires that its time derivative retains seminegative/negative definiteness anywhere. As a result, novel FNTS and FXTS criteria of the trivial state solution for FDI are established. Moreover, the settling time (ST) of FNTS/FXTS is provided. Then, the developed Lyapunov functional approach is applied to realize the finite-/fixed-time stabilization control of delayed neuron networks (DNNs) possessing discontinuous activation and ball motion models. The proposed method and results concerning FNTS/FXTS are of great significance in neural network (NN)/mechanical control engineering applications.},
  archive      = {J_TNNLS},
  author       = {Zuowei Cai and Lihong Huang and Zengyun Wang},
  doi          = {10.1109/TNNLS.2021.3083396},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6763-6774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-/Fixed-time stability of nonautonomous functional differential inclusion: Lyapunov approach involving indefinite derivative},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero-shot learning via structure-aligned generative
adversarial network. <em>TNNLS</em>, <em>33</em>(11), 6749–6762. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a structure-aligned generative adversarial network framework to improve zero-shot learning (ZSL) by mitigating the semantic gap, domain shift, and hubness problem. The proposed framework contains two parts, i.e., a generative adversarial network with a softmax classifier part, and a structure-aligned part. In the first part, the generative adversarial network aims at generating pseudovisual features through the guiding generator and discriminator play the minimax two-player game together. At the same time, the softmax classifier is committed to increasing the interclass distance and reducing intraclass distance. Then, the harmful effect of domain shift and hubness problems can be mitigated. In another part, we introduce a structure-aligned module where the structural consistency between visual space and semantic space is learned. By aligning the structure between visual space and semantic space, the semantic gap between them can be bridged. The performance of classification is improved when the structure-aligned visual-semantic embedding space is transferred to the unseen classes. Our framework reformulates the ZSL as a standard fully supervised classification task using the pseudovisual features of unseen classes. Extensive experiments conducted on five benchmark data sets demonstrate that the proposed framework significantly outperforms state-of-the-art methods in both conventional and generalized settings.},
  archive      = {J_TNNLS},
  author       = {Chenwei Tang and Zhenan He and Yunxia Li and Jiancheng Lv},
  doi          = {10.1109/TNNLS.2021.3083367},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6749-6762},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Zero-shot learning via structure-aligned generative adversarial network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-training enhanced: Network embedding and overlapping
community detection with adversarial learning. <em>TNNLS</em>,
<em>33</em>(11), 6737–6748. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding (NE) aims to encode the relations of vertices into a low-dimensional space. After NE, we can obtain the learned vectors of vertices that preserve the proximity of network structures for subsequent applications, e.g., vertex classification and link prediction. In existing NE models, they usually exploit the skip-gram with a negative sampling method to optimize their objective functions. Generally, this method learns the vertex representation only from the local connectivity of vertices (i.e., neighbors). However, there is a larger scope of vertex connectivity in real-world scenarios: a vertex may have multifaceted aspects and should belong to overlapping communities. Taking a social network as the overlapping example, a user may subscribe to the channels of politics, economy, and sports simultaneously, but the politics share more common attributes with the economy and less with the sports. In this article, we propose an adversarial learning approach (ACNE) for modeling overlapping communities of vertices. Specifically, we map the association between communities and vertices into an embedding space. Moreover, we take further research on enhancing our ACNE with the following two operations. First, in the initialization stage, we adopt a walking strategy with perception to obtain paths containing more possible boundary vertices to improve overlapping community detection. Then, after representation learning with ACNE, we use soft community assignments from a simple classifier as supervision to update the weights of ACNE. This self-training mechanism referred to as ACNE-ST can help ACNE to achieve better performance. Experimental results demonstrate that the proposed methods, including ACNE and ACNE-ST, can outperform the state-of-the-art models on the subsequent tasks of vertex classification and overlapping community detection.},
  archive      = {J_TNNLS},
  author       = {Junyang Chen and Zhiguo Gong and Jiqian Mo and Wei Wang and Wei Wang and Cong Wang and Xiao Dong and Weiwen Liu and Kaishun Wu},
  doi          = {10.1109/TNNLS.2021.3083318},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6737-6748},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-training enhanced: Network embedding and overlapping community detection with adversarial learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep rating and review neural network for item
recommendation. <em>TNNLS</em>, <em>33</em>(11), 6726–6736. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the sparsity issue, many recommender systems have been proposed to consider the review text as the auxiliary information to improve the recommendation quality. Despite success, they only use the ratings as the ground truth for error backpropagation. However, the rating information can only indicate the users’ overall preference for the items, while the review text contains rich information about the users’ preferences and the attributes of the items. In real life, reviews with the same rating may have completely opposite semantic information. If only the ratings are used for error backpropagation, the latent factors of these reviews will tend to be consistent, resulting in the loss of a large amount of review information. In this article, we propose a novel deep model termed deep rating and review neural network (DRRNN) for recommendation. Specifically, compared with the existing models that adopt the review text as the auxiliary information, DRRNN additionally considers both the target rating and target review of the given user–item pair as ground truth for error backpropagation in the training stage. Therefore, we can keep more semantic information of the reviews while making rating predictions. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed DRRNN model in terms of rating prediction.},
  archive      = {J_TNNLS},
  author       = {Wu-Dong Xi and Ling Huang and Chang-Dong Wang and Yin-Yu Zheng and Jian-Huang Lai},
  doi          = {10.1109/TNNLS.2021.3083264},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6726-6736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep rating and review neural network for item recommendation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global graph attention embedding network for relation
prediction in knowledge graphs. <em>TNNLS</em>, <em>33</em>(11),
6712–6725. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incompleteness of knowledge graphs triggers considerable research interest in relation prediction. As the key to predicting relations among entities, many efforts have been devoted to learning the embeddings of entities and relations by incorporating a variety of neighbors’ information which includes not only the information from direct outgoing and incoming neighbors but also the ones from the indirect neighbors on the multihop paths. However, previous models usually consider entity paths of limited length or ignore sequential information of the paths. Either simplification will make the model lack a global understanding of knowledge graphs and may result in the loss of important and indispensable information. In this article, we propose a novel global graph attention embedding network (GGAE) for relation prediction by combining global information from both direct neighbors and multihop neighbors. Concretely, given a knowledge graph, we first introduce the path construction algorithms to obtain meaningful paths, then design path modeling methods to capture the potential long-distance sequential information in the multihop paths, final propose an entity graph attention and a relation graph attention mechanisms to obtain entity embeddings and relation embeddings. Moreover, an entity graph attention mechanism is proposed to calculate the entity embeddings by aggregating direct incoming and outgoing neighbors from: 1) an original knowledge graph with the original entity and relation embeddings and 2) a new knowledge graph constructed by the paths whose embeddings are updated by path modeling methods. for each relation, we construct a new graph with related entities and present a relation graph attention to learn the features. Therefore, our model can encapsulate the information from different distance neighbors, and enable the embeddings of entities and relations to better capture all-sided semantic information. The experimental results on benchmark datasets verify the superiority of our model over the state-of-the-art ones.},
  archive      = {J_TNNLS},
  author       = {Qian Li and Daling Wang and Shi Feng and Cheng Niu and Yifei Zhang},
  doi          = {10.1109/TNNLS.2021.3083259},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6712-6725},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global graph attention embedding network for relation prediction in knowledge graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local anomaly detection for multivariate time series by
temporal dependency based on poisson model. <em>TNNLS</em>,
<em>33</em>(11), 6701–6711. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series data are invasive in different domains, ranging from data center supervision and e-commerce data to financial transactions. This kind of data presents an important challenge for anomaly detection due to the temporal dependency aspect of its observations. In this article, we investigate the problem of unsupervised local anomaly detection in multivariate time series data from temporal modeling and residual analysis perspectives. The residual analysis has been shown to be effective in classical anomaly detection problems. However, it is a nontrivial task in multivariate time series as the temporal dependency between the time series observations complicates the residual modeling process. Methodologically, we propose a unified learning framework to characterize the residuals and their coherence with the temporal aspect of the whole multivariate time series. Experiments on real-world datasets are provided showing the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Seif-Eddine Benkabou and Khalid Benabdeslem and Vivien Kraus and Kilian Bourhis and Bruno Canitia},
  doi          = {10.1109/TNNLS.2021.3083183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6701-6711},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local anomaly detection for multivariate time series by temporal dependency based on poisson model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Time-/event-triggered adaptive neural asymptotic tracking
control for nonlinear systems with full-state constraints and
application to a single-link robot. <em>TNNLS</em>, <em>33</em>(11),
6690–6700. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes the time-/event-triggered adaptive neural control strategies for the asymptotic tracking problem of a class of uncertain nonlinear systems with full-state constraints. First, we design a time-triggered strategy. The effect caused by the residuals of the estimation via radial basis function (RBF) neural networks (NNs), and the reasonable upper bounds on the first derivative of the reference signal and the derivative of each virtual control, can be eliminated by designing appropriate adaptive laws and utilizing the basic properties of RBF NNs. Moreover, the construction of the barrier Lyapunov functions (BLFs) in this work ensures the compliance of the full-state constraints and also holds the asymptotic output tracking performance. Then, based on the time-triggered strategy, we further design a relative threshold event-triggered strategy. The proposed event-triggered adaptive neural controller can solve the main control objective of this work, that is: 1) the full-state constraint requirements of the system are not violated and 2) the output signal asymptotically tracks the reference signal. Compared with the traditional method, the event-triggered strategy can improve the utilization of communication channels and resources and has greater practical significance. Finally, an example of single-link robot under the proposed two strategies illustrates the validity of the constructed controllers.},
  archive      = {J_TNNLS},
  author       = {Jiaming Zhang and Ben Niu and Ding Wang and Huanqing Wang and Ping Zhao and Guangdeng Zong},
  doi          = {10.1109/TNNLS.2021.3082994},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6690-6700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Time-/Event-triggered adaptive neural asymptotic tracking control for nonlinear systems with full-state constraints and application to a single-link robot},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive neural network control for a class of
fractional-order nonstrict-feedback nonlinear systems with full-state
constraints and input saturation. <em>TNNLS</em>, <em>33</em>(11),
6677–6689. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses an adaptive neural network (NN) constraint control scheme for a class of fractional-order uncertain nonlinear nonstrict-feedback systems with full-state constraints and input saturation. The radial basis function (RBF) NNs are used to deal with the algebraic loop problem from the nonstrict-feedback formation based on the approximation structure. In order to overcome the problem of input saturation nonlinearity, a smooth nonaffine function is applied to approach the saturation function. To arrest the violation of full-state constraints, the barrier Lyapunov function (BLF) is introduced in each step of the backstepping procedure. By using the fractional-order Lyapunov stability theory and the given conditions, it proves that all the states remain in their constraint bounds, the tracking error converges to a bounded compact set containing the origin, and all signals in the closed-loop system are ensured to be bounded. Finally, the effectiveness of the proposed control scheme is verified by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Changhui Wang and Limin Cui and Mei Liang and Jialin Li and Yantao Wang},
  doi          = {10.1109/TNNLS.2021.3082984},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6677-6689},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network control for a class of fractional-order nonstrict-feedback nonlinear systems with full-state constraints and input saturation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance analysis and applications of finite-time ZNN
models with constant/fuzzy parameters for TVQPEI. <em>TNNLS</em>,
<em>33</em>(11), 6665–6676. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on extensive applications of the time-variant quadratic programming with equality and inequality constraints (TVQPEI) problem and the effectiveness of the zeroing neural network (ZNN) to address time-variant problems, this article proposes a novel finite-time ZNN (FT-ZNN) model with a combined activation function, aimed at providing a superior efficient neurodynamic method to solve the TVQPEI problem. The remarkable properties of the FT-ZNN model are faster finite-time convergence and preferable robustness, which are analyzed in detail, where in the case of the robustness discussion, two kinds of noises (i.e., bounded constant noise and bounded time-variant noise) are taken into account. Moreover, the proposed several theorems all compute the convergent time of the nondisturbed FT-ZNN model and the disturbed FT-ZNN model approaching to the upper bound of residual error. Besides, to enhance the performance of the FT-ZNN model, a fuzzy finite-time ZNN (FFT-ZNN), which possesses a fuzzy parameter, is further presented for solving the TVQPEI problem. A simulative example about the FT-ZNN and FFT-ZNN models solving the TVQPEI problem is given, and the experimental results expectably conform to the theoretical analysis. In addition, the designed FT-ZNN model is effectually applied to the repetitive motion of the three-link redundant robot and image fusion to show its potential practical value.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Lei Jia and Yaonan Wang and Jianhua Dai and Qing Liao and Quanxin Zhu},
  doi          = {10.1109/TNNLS.2021.3082950},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6665-6676},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Performance analysis and applications of finite-time ZNN models with Constant/Fuzzy parameters for TVQPEI},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transductive relation-propagation with decoupling training
for few-shot learning. <em>TNNLS</em>, <em>33</em>(11), 6652–6664. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning, aiming to learn novel concepts from one or a few labeled examples, is an interesting and very challenging problem with many practical advantages. Existing few-shot methods usually utilize data of the same classes to train the feature embedding module and in a row, which is unable to learn adapting to new tasks. Besides, traditional few-shot models fail to take advantage of the valuable relations of the support-query pairs, leading to performance degradation. In this article, we propose a transductive relation-propagation graph neural network (GNN) with a decoupling training strategy (TRPN-D) to explicitly model and propagate such relations across support-query pairs, and empower the few-shot module the ability of transferring past knowledge to new tasks via the decoupling training. Our few-shot module, namely TRPN, treats the relation of each support-query pair as a graph node, named relational node, and resorts to the known relations between support samples, including both intraclass commonality and interclass uniqueness. Through relation propagation, the model could generate the discriminative relation embeddings for support-query pairs. To the best of our knowledge, this is the first work that decouples the training of the embedding network and the few-shot graph module with different tasks, which might offer a new way to solve the few-shot learning problem. Extensive experiments conducted on several benchmark datasets demonstrate that our method can significantly outperform a variety of state-of-the-art few-shot learning methods.},
  archive      = {J_TNNLS},
  author       = {Yuqing Ma and Shihao Bai and Wei Liu and Shuo Wang and Yue Yu and Xiao Bai and Xianglong Liu and Meng Wang},
  doi          = {10.1109/TNNLS.2021.3082928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6652-6664},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transductive relation-propagation with decoupling training for few-shot learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Complementary memtransistor-based multilayer neural
networks for online supervised learning through
(anti-)spike-timing-dependent plasticity. <em>TNNLS</em>,
<em>33</em>(11), 6640–6651. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a complete hardware-based architecture of multilayer neural networks (MNNs), including electronic synapses, neurons, and periphery circuitry to implement supervised learning (SL) algorithm of extended remote supervised method (ReSuMe). In this system, complementary (a pair of n- and p-type) memtransistors (C-MTs) are used as an electrical synapse. By applying the learning rule of spike-timing-dependent plasticity (STDP) to the memtransistor connecting presynaptic neuron to the output one whereas the contrary anti-STDP rule to the other memtransistor connecting presynaptic neuron to the teacher one, extended ReSuMe with multiple layers is realized without the usage of those complicated supervising modules in previous approaches. In this way, both the C-MT-based chip area and power consumption of the learning circuit for weight updating operation are drastically decreased comparing with the conventional single memtransistor (S-MT)-based designs. Two typical benchmarks, the linearly nonseparable benchmark XOR problem and Mixed National Institute of Standards and Technology database (MNIST) recognition have been successfully tackled using the proposed MNN system while impact of the nonideal factors of realistic devices has been evaluated.},
  archive      = {J_TNNLS},
  author       = {Yue Zhou and Nuo Xu and Bin Gao and Fuwei Zhuge and Zijian Tang and Xinchen Deng and Yi Li and Yuhui He and Xiangshui Miao},
  doi          = {10.1109/TNNLS.2021.3082911},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6640-6651},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Complementary memtransistor-based multilayer neural networks for online supervised learning through (Anti-)Spike-timing-dependent plasticity},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based neural architecture search for person
re-identification. <em>TNNLS</em>, <em>33</em>(11), 6627–6639. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed significant progress of person reidentification (reID) driven by expert-designed deep neural network architectures. Despite the remarkable success, such architectures often suffer from high model complexity and time-consuming pretraining process, as well as the mismatches between the image classification-driven backbones and the reID task. To address these issues, we introduce neural architecture search (NAS) into automatically designing person reID backbones, i.e., reID-NAS, which is achieved via automatically searching attention-based network architectures from scratch. Different from traditional NAS approaches that originated for image classification, we design a reID-based search space as well as a search objective to fit NAS for the reID tasks. In terms of the search space, reID-NAS includes a lightweight attention module to precisely locate arbitrary pedestrian bounding boxes, which is automatically added as attention to the reID architectures. In terms of the search objective, reID-NAS introduces a new retrieval objective to search and train reID architectures from scratch. Finally, we propose a hybrid optimization strategy to improve the search stability in reID-NAS. In our experiments, we validate the effectiveness of different parts in reID-NAS, and show that the architecture searched by reID-NAS achieves a new state of the art, with one order of magnitude fewer parameters on three-person reID datasets. As a concomitant benefit, the reliance on the pretraining process is vastly reduced by reID-NAS, which facilitates one to directly search and train a lightweight reID model from scratch.},
  archive      = {J_TNNLS},
  author       = {Qinqin Zhou and Bineng Zhong and Xin Liu and Rongrong Ji},
  doi          = {10.1109/TNNLS.2021.3082701},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6627-6639},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-based neural architecture search for person re-identification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mining non-redundant co-location patterns. <em>TNNLS</em>,
<em>33</em>(11), 6613–6626. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-location pattern mining refers to discovering neighboring relationships of spatial features distributed in geographic space. With the rapid growth of spatial datasets, the usefulness of co-location patterns is strongly limited by the large number of discovered patterns containing multiple redundancies. To address this problem, in this article, we propose a novel approach for discovering the super participation index-closed (SPI-closed) co-location patterns which are a newly proposed lossless condensed representation of co-location patterns by considering distributions of the spatial instances. In the proposed approach, first, a linear-time method is designed to generate complete and correct neighboring cliques using extended neighboring relationships. Based on these cliques, a hash structure is then constructed to store the distributions of the co-location patterns in a condensed way. Finally, using this hash structure, the SPI-closed co-location patterns (SCPs) are efficiently discovered even if the prevalence threshold is changed, while similar approaches have to restart their mining processes. To confirm the efficiency of the proposed method, we compared its performance with similar approaches in the literature on multiple real and synthetic spatial datasets. The experiments confirm that our new approach is more efficient, effective, and flexible than similar approaches.},
  archive      = {J_TNNLS},
  author       = {Xuguang Bao and Jinjie Lu and Tianlong Gu and Liang Chang and Zhoubo Xu and Lizhen Wang},
  doi          = {10.1109/TNNLS.2021.3082628},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6613-6626},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mining non-redundant co-location patterns},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Restricted minimum error entropy criterion for robust
classification. <em>TNNLS</em>, <em>33</em>(11), 6599–6612. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum error entropy (MEE) criterion is a powerful approach for non-Gaussian signal processing and robust machine learning. However, the instantiation of MEE on robust classification is a rather vacancy in the literature. The original MEE purely focuses on minimizing Renyi’s quadratic entropy of the prediction errors, which could exhibit inferior capability in noisy classification tasks. To this end, we analyze the optimal error distribution with adverse outliers and introduce a specific codebook for restriction, which optimizes the error distribution toward the optimal case. Half-quadratic-based optimization and convergence analysis of the proposed learning criterion, called restricted MEE (RMEE), are provided. The experimental results considering logistic regression and extreme learning machine on synthetic data and UCI datasets, respectively, are presented to demonstrate the superior robustness of RMEE. Furthermore, we evaluate RMEE on a noisy electroencephalogram dataset, so as to strengthen its practical impact.},
  archive      = {J_TNNLS},
  author       = {Yuanhao Li and Badong Chen and Natsue Yoshimura and Yasuharu Koike},
  doi          = {10.1109/TNNLS.2021.3082571},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6599-6612},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Restricted minimum error entropy criterion for robust classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributional soft actor-critic: Off-policy reinforcement
learning for addressing value estimation errors. <em>TNNLS</em>,
<em>33</em>(11), 6584–6598. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reinforcement learning (RL), function approximation errors are known to easily lead to the $Q$ -value overestimations, thus greatly reducing policy performance. This article presents a distributional soft actor–critic (DSAC) algorithm, which is an off-policy RL method for continuous control setting, to improve the policy performance by mitigating $Q$ -value overestimations. We first discover in theory that learning a distribution function of state–action returns can effectively mitigate $Q$ -value overestimations because it is capable of adaptively adjusting the update step size of the $Q$ -value function. Then, a distributional soft policy iteration (DSPI) framework is developed by embedding the return distribution function into maximum entropy RL. Finally, we present a deep off-policy actor–critic variant of DSPI, called DSAC, which directly learns a continuous return distribution by keeping the variance of the state–action returns within a reasonable range to address exploding and vanishing gradient problems. We evaluate DSAC on the suite of MuJoCo continuous control tasks, achieving the state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Jingliang Duan and Yang Guan and Shengbo Eben Li and Yangang Ren and Qi Sun and Bo Cheng},
  doi          = {10.1109/TNNLS.2021.3082568},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6584-6598},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistability of switched neural networks with gaussian
activation functions under state-dependent switching. <em>TNNLS</em>,
<em>33</em>(11), 6569–6583. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents theoretical results on the multistability of switched neural networks with Gaussian activation functions under state-dependent switching. It is shown herein that the number and location of the equilibrium points of the switched neural networks can be characterized by making use of the geometrical properties of Gaussian functions and local linearization based on the Brouwer fixed-point theorem. Four sets of sufficient conditions are derived to ascertain the existence of $7^{p_{1}}5^{p_{2}}3^{p_{3}}$ equilibrium points, and $4^{p_{1}}3^{p_{2}}2^{p_{3}}$ of them are locally stable, wherein $p_{1}$ , $p_{2}$ , and $p_{3}$ are nonnegative integers satisfying $0\leq p_{1}+p_{2}+p_{3}\leq n$ and $n$ is the number of neurons. It implies that there exist up to $7^{n}$ equilibria, and up to $4^{n}$ of them are locally stable when $p_{1}=n$ . It also implies that properly selecting $p_{1}$ , $p_{2}$ , and $p_{3}$ can engender a desirable number of stable equilibria. Two numerical examples are elaborated to substantiate the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Zhenyuan Guo and Shiqin Ou and Jun Wang},
  doi          = {10.1109/TNNLS.2021.3082560},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6569-6583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistability of switched neural networks with gaussian activation functions under state-dependent switching},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from crowds with multiple noisy label distribution
propagation. <em>TNNLS</em>, <em>33</em>(11), 6558–6568. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing services provide a fast, efficient, and cost-effective way to obtain large labeled data for supervised learning. Unfortunately, the quality of crowdsourced labels cannot satisfy the standards of practical applications. Ground-truth inference, simply called label integration, designs proper aggregation methods to infer the unknown true label of each instance (sample) from the multiple noisy label set provided by ordinary crowd labelers (workers). However, nearly all existing label integration methods focus solely on the multiple noisy label set per individual instance while totally ignoring the intercorrelation among multiple noisy label sets of different instances. To solve this problem, a multiple noisy label distribution propagation (MNLDP) method is proposed in this article. MNLDP at first estimates the multiple noisy label distribution of each instance from its multiple noisy label set and then propagates its multiple noisy label distribution to its nearest neighbors. Consequently, each instance absorbs a fraction of the multiple noisy label distributions from its nearest neighbors and yet simultaneously maintains a fraction of its own original multiple noisy label distribution. Empirical studies on a collection of an artificial dataset, six simulated UCI datasets, and three real-world crowdsourced datasets show that MNLDP outperforms all other existing state-of-the-art label integration methods in terms of the integration accuracy and classification accuracy.},
  archive      = {J_TNNLS},
  author       = {Liangxiao Jiang and Hao Zhang and Fangna Tao and Chaoqun Li},
  doi          = {10.1109/TNNLS.2021.3082496},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6558-6568},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning from crowds with multiple noisy label distribution propagation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Command-filtered robust adaptive NN control with the
prescribed performance for the 3-d trajectory tracking of underactuated
AUVs. <em>TNNLS</em>, <em>33</em>(11), 6545–6557. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel robust adaptive neural network (NN) control scheme with prescribed performance is developed for the 3-D trajectory tracking of underactuated autonomous underwater vehicles (AUVs) with uncertain dynamics and unknown disturbances using new prescribed performance functions, an additional term, the radial basis function (RBF) NN, and the command-filtered backstepping approach. Different from the traditional prescribed performance functions, the new prescribed performance functions are innovatively proposed such that the time desired for the trajectory tracking errors of AUVs to reach and stay within the prescribed error tolerance band can be preset exactly and flexibly. The additional term with the Nussbaum function is designed to deal with the underactuation problem of AUVs. By means of RBF NN, the uncertain item lumped by the uncertain dynamics of AUVs and unknown disturbances is eventually transformed into a linearly parametric form with only a single unknown parameter. The developed control scheme ensures that all signals in the AUV 3-D trajectory tracking closed-loop control system are bounded. Simulation results with comparisons show the validity and the superiority of our developed control scheme.},
  archive      = {J_TNNLS},
  author       = {Jian Li and Jialu Du and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3082407},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6545-6557},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Command-filtered robust adaptive NN control with the prescribed performance for the 3-D trajectory tracking of underactuated AUVs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What and where: Learn to plug adapters via NAS for
multidomain learning. <em>TNNLS</em>, <em>33</em>(11), 6532–6544. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important and challenging problem, multidomain learning (MDL) typically seeks a set of effective lightweight domain-specific adapter modules plugged into a common domain-agnostic network. Usually, existing ways of adapter plugging and structure design are handcrafted and fixed for all domains before model learning, resulting in learning inflexibility and computational intensiveness. With this motivation, we propose to learn a data-driven adapter plugging strategy with neural architecture search (NAS), which automatically determines where to plug for those adapter modules. Furthermore, we propose an NAS-adapter module for adapter structure design in an NAS-driven learning scheme, which automatically discovers effective adapter module structures for different domains. Experimental results demonstrate the effectiveness of our MDL model against existing approaches under the conditions of comparable performance.},
  archive      = {J_TNNLS},
  author       = {Hanbin Zhao and Hao Zeng and Xin Qin and Yongjian Fu and Hui Wang and Bourahla Omar and Xi Li},
  doi          = {10.1109/TNNLS.2021.3082316},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6532-6544},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {What and where: Learn to plug adapters via NAS for multidomain learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Endmember-guided unmixing network (EGU-net): A general deep
learning framework for self-supervised hyperspectral unmixing.
<em>TNNLS</em>, <em>33</em>(11), 6518–6531. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, enormous efforts have been made to improve the performance of linear or nonlinear mixing models for hyperspectral unmixing (HU), yet their ability to simultaneously generalize various spectral variabilities (SVs) and extract physically meaningful endmembers still remains limited due to the poor ability in data fitting and reconstruction and the sensitivity to various SVs. Inspired by the powerful learning ability of deep learning (DL), we attempt to develop a general DL approach for HU, by fully considering the properties of endmembers extracted from the hyperspectral imagery, called endmember-guided unmixing network (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a two-stream Siamese deep network, which learns an additional network from the pure or nearly pure endmembers to correct the weights of another unmixing network by sharing network parameters and adding spectrally meaningful constraints (e.g., nonnegativity and sum-to-one) toward a more accurate and interpretable unmixing solution. Furthermore, the resulting general framework is not only limited to pixelwise spectral unmixing but also applicable to spatial information modeling with convolutional operators for spatial–spectral unmixing. Experimental results conducted on three different datasets with the ground truth of abundance maps corresponding to each material demonstrate the effectiveness and superiority of the EGU-Net over state-of-the-art unmixing algorithms. The codes will be available from the website: https://github.com/danfenghong/IEEE_TNNLS_EGU-Net .},
  archive      = {J_TNNLS},
  author       = {Danfeng Hong and Lianru Gao and Jing Yao and Naoto Yokoya and Jocelyn Chanussot and Uta Heiden and Bing Zhang},
  doi          = {10.1109/TNNLS.2021.3082289},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6518-6531},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Endmember-guided unmixing network (EGU-net): A general deep learning framework for self-supervised hyperspectral unmixing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised discriminative learning with spectral
constrained generative adversarial network for hyperspectral anomaly
detection. <em>TNNLS</em>, <em>33</em>(11), 6504–6517. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection (AD) using hyperspectral images (HSIs) is of great interest for deep space exploration and Earth observations. This article proposes a weakly supervised discriminative learning with a spectral constrained generative adversarial network (GAN) for hyperspectral anomaly detection (HAD), called weaklyAD. It can enhance the discrimination between anomaly and background with background homogenization and anomaly saliency in cases where anomalous samples are limited and sensitive to the background. A novel probability-based category thresholding is first proposed to label coarse samples in preparation for weakly supervised learning. Subsequently, a discriminative reconstruction model is learned by the proposed network in a weakly supervised fashion. The proposed network has an end-to-end architecture, which not only includes an encoder, a decoder, a latent layer discriminator, and a spectral discriminator competitively but also contains a novel Kullback–Leibler (KL) divergence-based orthogonal projection divergence (OPD) spectral constraint. Finally, the well-learned network is used to reconstruct HSIs captured by the same sensor. Our work paves a new weakly supervised way for HAD, which intends to match the performance of supervised methods without the prerequisite of manually labeled data. Assessments and generalization experiments over real HSIs demonstrate the unique promise of such a proposed approach.},
  archive      = {J_TNNLS},
  author       = {Tao Jiang and Weiying Xie and Yunsong Li and Jie Lei and Qian Du},
  doi          = {10.1109/TNNLS.2021.3082158},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6504-6517},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly supervised discriminative learning with spectral constrained generative adversarial network for hyperspectral anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IffDetector: Inference-aware feature filtering for object
detection. <em>TNNLS</em>, <em>33</em>(11), 6494–6503. (<a
href="https://doi.org/10.1109/TNNLS.2021.3081864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern convolutional neural network (CNN)-based object detectors focus on feature configuration during training but often ignore feature optimization during inference. In this article, we propose a new feature optimization approach to enhance features and suppress background noise in both the training and inference stages. We introduce a generic inference-aware feature filtering (IFF) module that can be easily combined with existing detectors, resulting in our iffDetector. Unlike conventional open-loop feature calculation approaches without feedback, the proposed IFF module performs the closed-loop feature optimization by leveraging high-level semantics to enhance the convolutional features. By applying the Fourier transform to analyze our detector, we prove that the IFF module acts as a negative feedback that can theoretically guarantee the stability of the feature learning. IFF can be fused with CNN-based object detectors in a plug-and-play manner with little computational cost overhead. Experiments on the PASCAL VOC and MS COCO datasets demonstrate that our iffDetector consistently outperforms state-of-the-art methods with significant margins.},
  archive      = {J_TNNLS},
  author       = {Mingyuan Mao and Yuxin Tian and Baochang Zhang and Qixiang Ye and Wanquan Liu and David Doermann},
  doi          = {10.1109/TNNLS.2021.3081864},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6494-6503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IffDetector: Inference-aware feature filtering for object detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rich embedding features for one-shot semantic segmentation.
<em>TNNLS</em>, <em>33</em>(11), 6484–6493. (<a
href="https://doi.org/10.1109/TNNLS.2021.3081693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot semantic segmentation poses the challenging task of segmenting object regions from unseen categories with only one annotated example as guidance. Thus, how to effectively construct robust feature representations from the guidance image is crucial to the success of one-shot semantic segmentation. To this end, we propose in this article a simple, yet effective approach named rich embedding features (REFs). Given a reference image accompanied with its annotated mask, our REF constructs rich embedding features of the support object from three perspectives: 1) global embedding to capture the general characteristics; 2) peak embedding to capture the most discriminative information; 3) adaptive embedding to capture the internal long-range dependencies. By combining these informative features, we can easily harvest sufficient and rich guidance even from a single reference image. In addition to REF, we further propose a simple depth-priority context module to obtain useful contextual cues from the query image. This successfully raises the performance of one-shot semantic segmentation to a new level. We conduct experiments on pattern analysis, statical modeling and computational learning (Pascal) visual object classes (VOC) 2012 and common object in context (COCO) to demonstrate the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Xiaolin Zhang and Yunchao Wei and Zhao Li and Chenggang Yan and Yi Yang},
  doi          = {10.1109/TNNLS.2021.3081693},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6484-6493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rich embedding features for one-shot semantic segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global mittag–leffler stability of the delayed
fractional-coupled reaction-diffusion system on networks without strong
connectedness. <em>TNNLS</em>, <em>33</em>(11), 6473–6483. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we mainly consider the existence of solutions and global Mittag–Leffler stability of delayed fractional-order coupled reaction-diffusion neural networks without strong connectedness. Using the Leary–Schauder’s fixed point theorem and the Lyapunov method, some criteria for the existence of solutions and global Mittag–Leffler stability are given. Finally, the correctness of the theory is verified by a numerical example.},
  archive      = {J_TNNLS},
  author       = {Yue Cao and Yonggui Kao and Ju H. Park and Haibo Bao},
  doi          = {10.1109/TNNLS.2021.3080830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6473-6483},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global Mittag–Leffler stability of the delayed fractional-coupled reaction-diffusion system on networks without strong connectedness},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orientation-preserving rewards’ balancing in reinforcement
learning. <em>TNNLS</em>, <em>33</em>(11), 6458–6472. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auxiliary rewards are widely used in complex reinforcement learning tasks. However, previous work can hardly avoid the interference of auxiliary rewards on pursuing the main rewards, which leads to the destruction of the optimal policy. Thus, it is challenging but essential to balance the main and auxiliary rewards. In this article, we explicitly formulate the problem of rewards’ balancing as searching for a Pareto optimal solution, with the overall objective of preserving the policy’s optimization orientation for the main rewards (i.e., the policy driven by the balanced rewards is consistent with the policy driven by the main rewards). To this end, we propose a variant Pareto and show that it can effectively guide the policy search toward more main rewards. Furthermore, we establish an iterative learning framework for rewards’ balancing and theoretically analyze its convergence and time complexity. Experiments in both discrete (grid word) and continuous (Doom) environments demonstrated that our algorithm can effectively balance rewards, and achieve remarkable performance compared with those RLs with heuristically designed rewards. In the ViZDoom platform, our algorithm can learn expert-level policies.},
  archive      = {J_TNNLS},
  author       = {Jinsheng Ren and Shangqi Guo and Feng Chen},
  doi          = {10.1109/TNNLS.2021.3080521},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6458-6472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Orientation-preserving rewards’ balancing in reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DLnet with training task conversion stream for precise
semantic segmentation in actual traffic scene. <em>TNNLS</em>,
<em>33</em>(11), 6443–6457. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many successful semantic segmentation models trained on certain datasets experience a performance gap when they are applied to the actual scene images, expressing weak robustness of these models in the actual scene. The training task conversion (TTC) and domain adaption field have been originally proposed to solve the performance gap problem. Unfortunately, many existing models for TTC and domain adaptation have defects, and even if the TTC is completed, the performance is far from the original task model. Thus, how to maintain excellent performance while completing TTC is the main challenge. In order to address this challenge, a deep learning model named DLnet is proposed for TTC from the existing image dataset-based training task to the actual scene image-based training task. The proposed network, named the DLnet, contains three main innovations. The proposed network is verified by experiments. The experimental results show that the proposed DLnet not only can achieve state-of-the-art quantitative performance on four popular datasets but also can obtain outstanding qualitative performance in four actual urban scenes, which demonstrates the robustness and performance of the proposed DLnet. In addition, although the proposed DLnet cannot achieve outstanding performance in real time, it can still achieve a moderate performance in real time, which is within an acceptable range.},
  archive      = {J_TNNLS},
  author       = {Yingfeng Cai and Lei Dai and Hai Wang and Long Chen and Yicheng Li},
  doi          = {10.1109/TNNLS.2021.3080261},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6443-6457},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DLnet with training task conversion stream for precise semantic segmentation in actual traffic scene},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fully natural gradient scheme for improving inference of
the heterogeneous multioutput gaussian process model. <em>TNNLS</em>,
<em>33</em>(11), 6429–6442. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent novel extension of multioutput Gaussian processes (GPs) handles heterogeneous outputs, assuming that each output has its own likelihood function. It uses a vector-valued GP prior to jointly model all likelihoods’ parameters as latent functions drawn from a GP with a linear model of coregionalization (LMC) covariance. By means of an inducing points’ framework, the model is able to obtain tractable variational bounds amenable to stochastic variational inference (SVI). Nonetheless, the strong conditioning between the variational parameters and the hyperparameters burdens the adaptive gradient optimization methods used in the original approach. To overcome this issue, we borrow ideas from variational optimization introducing an exploratory distribution over the hyperparameters, allowing inference together with the posterior’s variational parameters through a fully natural gradient (NG) optimization scheme. Furthermore, in this work, we introduce an extension of the heterogeneous multioutput model, where its latent functions are drawn from convolution processes. We show that our optimization scheme can achieve better local optima solutions with higher test performance rates than adaptive gradient methods for both the LMC and the convolution process model. We also show how to make the convolutional model scalable by means of SVI and how to optimize it through a fully NG scheme. We compare the performance of the different methods over the toy and real databases.},
  archive      = {J_TNNLS},
  author       = {Juan-José Giraldo and Mauricio A. Álvarez},
  doi          = {10.1109/TNNLS.2021.3080238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6429-6442},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fully natural gradient scheme for improving inference of the heterogeneous multioutput gaussian process model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus of stochastic dynamical multiagent systems in
directed networks via PI protocols. <em>TNNLS</em>, <em>33</em>(11),
6417–6428. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of swarm intelligence, the consensus of multiagent systems (MASs) has attracted substantial attention due to its broad range of applications in the practical world. Inspired by the considerable gap between control theory and engineering practices, this article is aimed at addressing the mean square consensus problems for stochastic dynamical nonlinear MASs in directed networks by designing proportional-integral (PI) protocols. In light of the general algebraic connectivity, consensus underlying PI protocols for a directed strongly connected network is investigated, and due to the $M$ -matrix approaches, consensus with PI protocols for a directed network containing a spanning tree is studied. By constructing appropriate Lyapunov functions, combining with the stochastic analysis technique and LaSalle’s invariant principles, some sufficient conditions are derived under which the stochastic dynamical MASs realize consensus in mean square. Numerical simulations are finally presented to illustrate the validity of the main results.},
  archive      = {J_TNNLS},
  author       = {Haibo Gu and Kexin Liu and Jinhu Lü and Zhang Ren},
  doi          = {10.1109/TNNLS.2021.3080122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6417-6428},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus of stochastic dynamical multiagent systems in directed networks via PI protocols},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DPFL-nets: Deep pyramid feature learning networks for
multiscale change detection. <em>TNNLS</em>, <em>33</em>(11), 6402–6416.
(<a href="https://doi.org/10.1109/TNNLS.2021.3079627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complementary properties of different types of sensors, change detection between heterogeneous images receives increasing attention from researchers. However, change detection cannot be handled by directly comparing two heterogeneous images since they demonstrate different image appearances and statistics. In this article, we propose a deep pyramid feature learning network (DPFL-Net) for change detection, especially between heterogeneous images. DPFL-Net can learn a series of hierarchical features in an unsupervised fashion, containing both spatial details and multiscale contextual information. The learned pyramid features from two input images make unchanged pixels matched exactly and changed ones dissimilar and after transformed into the same space for each scale successively. We further propose fusion blocks to aggregate multiscale difference images (DIs), generating an enhanced DI with strong separability. Based on the enhanced DI, unchanged areas are predicted and used to train DPFL-Net in the next iteration. In this article, pyramid features and unchanged areas are updated alternately, leading to an unsupervised change detection method. In the feature transformation process, local consistency is introduced to constrain the learned pyramid features, modeling the correlations between the neighboring pixels and reducing the false alarms. Experimental results demonstrate that the proposed approach achieves superior or at least comparable results to the existing state-of-the-art change detection methods in both homogeneous and heterogeneous cases.},
  archive      = {J_TNNLS},
  author       = {Meijuan Yang and Licheng Jiao and Fang Liu and Biao Hou and Shuyuan Yang and Meng Jian},
  doi          = {10.1109/TNNLS.2021.3079627},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6402-6416},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DPFL-nets: Deep pyramid feature learning networks for multiscale change detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based meta-reinforcement learning for tracking
control of AUV with time-varying dynamics. <em>TNNLS</em>,
<em>33</em>(11), 6388–6401. (<a
href="https://doi.org/10.1109/TNNLS.2021.3079148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is a promising technique for designing a model-free controller by interacting with the environment. Several researchers have applied RL to autonomous underwater vehicles (AUVs) for motion control, such as trajectory tracking. However, the existing RL-based controller usually assumes that the unknown AUV dynamics keep invariant during the operation period, limiting its further application in the complex underwater environment. In this article, a novel meta-RL-based control scheme is proposed for trajectory tracking control of AUV in the presence of unknown and time-varying dynamics. To this end, we divide the tracking task for AUV with time-varying dynamics into multiple specific tasks with fixed time-varying dynamics, to which we apply meta-RL for training to distill the general control policy. The obtained control policy can transfer to the testing phase with high adaptability. Inspired by the line-of-sight (LOS) tracking rule, we formulate each specific task as a Markov decision process (MDP) with a well-designed state and reward function. Furthermore, a novel policy network with an attention module is proposed to extract the hidden information of AUV dynamics. The simulation environment with time-varying dynamics is established, and the simulation results reveal the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Peng Jiang and Shiji Song and Gao Huang},
  doi          = {10.1109/TNNLS.2021.3079148},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6388-6401},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-based meta-reinforcement learning for tracking control of AUV with time-varying dynamics},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward deep adaptive hinging hyperplanes. <em>TNNLS</em>,
<em>33</em>(11), 6373–6387. (<a
href="https://doi.org/10.1109/TNNLS.2021.3079113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive hinging hyperplane (AHH) model is a popular piecewise linear representation with a generalized tree structure and has been successfully applied in dynamic system identification. In this article, we aim to construct the deep AHH (DAHH) model to extend and generalize the networking of AHH model for high-dimensional problems. The network structure of DAHH is determined through a forward growth, in which the activity ratio is introduced to select effective neurons and no connecting weights are involved between the layers. Then, all neurons in the DAHH network can be flexibly connected to the output in a skip-layer format, and only the corresponding weights are the parameters to optimize. With such a network framework, the backpropagation algorithm can be implemented in DAHH to efficiently tackle large-scale problems and the gradient vanishing problem is not encountered in the training of DAHH. In fact, the optimization problem of DAHH can maintain convexity with convex loss in the output layer, which brings natural advantages in optimization. Different from the existing neural networks, DAHH is easier to interpret, where neurons are connected sparsely and analysis of variance (ANOVA) decomposition can be applied, facilitating to revealing the interactions between variables. A theoretical analysis toward universal approximation ability and explicit domain partitions are also derived. Numerical experiments verify the effectiveness of the proposed DAHH.},
  archive      = {J_TNNLS},
  author       = {Qinghua Tao and Jun Xu and Zhen Li and Na Xie and Shuning Wang and Xiaoli Li and Johan A. K. Suykens},
  doi          = {10.1109/TNNLS.2021.3079113},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6373-6387},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward deep adaptive hinging hyperplanes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Drill the cork of information bottleneck by inputting the
most important data. <em>TNNLS</em>, <em>33</em>(11), 6360–6372. (<a
href="https://doi.org/10.1109/TNNLS.2021.3079112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has become the most powerful machine learning tool in the last decade. However, how to efficiently train deep neural networks remains to be thoroughly solved. The widely used minibatch stochastic gradient descent (SGD) still needs to be accelerated. As a promising tool to better understand the learning dynamic of minibatch SGD, the information bottleneck (IB) theory claims that the optimization process consists of an initial fitting phase and the following compression phase. Based on this principle, we further study typicality sampling, an efficient data selection method, and propose a new explanation of how it helps accelerate the training process of the deep networks. We show that the fitting phase depicted in the IB theory will be boosted with a high signal-to-noise ratio of gradient approximation if the typicality sampling is appropriately adopted. Furthermore, this finding also implies that the prior information of the training set is critical to the optimization process, and the better use of the most important data can help the information flow through the bottleneck faster. Both theoretical analysis and experimental results on synthetic and real-world datasets demonstrate our conclusions.},
  archive      = {J_TNNLS},
  author       = {Xinyu Peng and Jiawei Zhang and Fei-Yue Wang and Li Li},
  doi          = {10.1109/TNNLS.2021.3079112},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6360-6372},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Drill the cork of information bottleneck by inputting the most important data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved inference for imputation-based semisupervised
learning under misspecified setting. <em>TNNLS</em>, <em>33</em>(11),
6346–6359. (<a
href="https://doi.org/10.1109/TNNLS.2021.3077312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semisupervised learning (SSL) has been extensively studied in related literature. Despite its success, many existing learning algorithms for semisupervised problems require specific distributional assumptions, such as “cluster assumption” and “low-density assumption,” and thus, it is often hard to verify them in practice. We are interested in quantifying the effect of SSL based on kernel methods under a misspecified setting. The misspecified setting means that the target function is not contained in a hypothesis space under which some specific learning algorithm works. Practically, this assumption is mild and standard for various kernel-based approaches. Under this misspecified setting, this article makes an attempt to provide a theoretical justification on when and how the unlabeled data can be exploited to improve inference of a learning task. Our theoretical justification is indicated from the viewpoint of the asymptotic variance of our proposed two-step estimation. It is shown that the proposed pointwise nonparametric estimator has a smaller asymptotic variance than the supervised estimator using the labeled data alone. Several simulated experiments are implemented to support our theoretical results.},
  archive      = {J_TNNLS},
  author       = {Shaogao Lv and Linsen Wei and Qian Zhang and Bin Liu and Zenglin Xu},
  doi          = {10.1109/TNNLS.2021.3077312},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6346-6359},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved inference for imputation-based semisupervised learning under misspecified setting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Qutrit-inspired fully self-supervised shallow quantum
learning network for brain tumor segmentation. <em>TNNLS</em>,
<em>33</em>(11), 6331–6345. (<a
href="https://doi.org/10.1109/TNNLS.2021.3077188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical self-supervised networks suffer from convergence problems and reduced segmentation accuracy due to forceful termination. Qubits or bilevel quantum bits often describe quantum neural network models. In this article, a novel self-supervised shallow learning network model exploiting the sophisticated three-level qutrit-inspired quantum information system, referred to as quantum fully self-supervised neural network (QFS-Net), is presented for automated segmentation of brain magnetic resonance (MR) images. The QFS-Net model comprises a trinity of a layered structure of qutrits interconnected through parametric Hadamard gates using an eight-connected second-order neighborhood-based topology. The nonlinear transformation of the qutrit states allows the underlying quantum neural network model to encode the quantum states, thereby enabling a faster self-organized counterpropagation of these states between the layers without supervision. The suggested QFS-Net model is tailored and extensively validated on the Cancer Imaging Archive (TCIA) dataset collected from the Nature repository. The experimental results are also compared with state-of-the-art supervised (U-Net and URes-Net architectures) and the self-supervised QIS-Net model and its classical counterpart. Results shed promising segmented outcomes in detecting tumors in terms of dice similarity and accuracy with minimum human intervention and computational resources. The proposed QFS-Net is also investigated on natural gray-scale images from the Berkeley segmentation dataset and yields promising outcomes in segmentation, thereby demonstrating the robustness of the QFS-Net model.},
  archive      = {J_TNNLS},
  author       = {Debanjan Konar and Siddhartha Bhattacharyya and Bijaya K. Panigrahi and Elizabeth C. Behrman},
  doi          = {10.1109/TNNLS.2021.3077188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6331-6345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Qutrit-inspired fully self-supervised shallow quantum learning network for brain tumor segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discrete-time signatures and randomness in reservoir
computing. <em>TNNLS</em>, <em>33</em>(11), 6321–6330. (<a
href="https://doi.org/10.1109/TNNLS.2021.3076777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new explanation of the geometric nature of the reservoir computing (RC) phenomenon is presented. RC is understood in the literature as the possibility of approximating input–output systems with randomly chosen recurrent neural systems and a trained linear readout layer. Light is shed on this phenomenon by constructing what is called strongly universal reservoir systems as random projections of a family of state-space systems that generate Volterra series expansions. This procedure yields a state-affine reservoir system with randomly generated coefficients in a dimension that is logarithmically reduced with respect to the original system. This reservoir system is able to approximate any element in the fading memory filters class just by training a different linear readout for each different filter. Explicit expressions for the probability distributions needed in the generation of the projected reservoir system are stated, and bounds for the committed approximation error are provided.},
  archive      = {J_TNNLS},
  author       = {Christa Cuchiero and Lukas Gonon and Lyudmila Grigoryeva and Juan-Pablo Ortega and Josef Teichmann},
  doi          = {10.1109/TNNLS.2021.3076777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6321-6330},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discrete-time signatures and randomness in reservoir computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). FDDH: Fast discriminative discrete hashing for large-scale
cross-modal retrieval. <em>TNNLS</em>, <em>33</em>(11), 6306–6320. (<a
href="https://doi.org/10.1109/TNNLS.2021.3076684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing, favored for its effectiveness and efficiency, has received wide attention to facilitating efficient retrieval across different modalities. Nevertheless, most existing methods do not sufficiently exploit the discriminative power of semantic information when learning the hash codes while often involving time-consuming training procedure for handling the large-scale dataset. To tackle these issues, we formulate the learning of similarity-preserving hash codes in terms of orthogonally rotating the semantic data, so as to minimize the quantization loss of mapping such data to hamming space and propose an efficient fast discriminative discrete hashing (FDDH) approach for large-scale cross-modal retrieval. More specifically, FDDH introduces an orthogonal basis to regress the targeted hash codes of training examples to their corresponding semantic labels and utilizes the $\varepsilon $ -dragging technique to provide provable large semantic margins. Accordingly, the discriminative power of semantic information can be explicitly captured and maximized. Moreover, an orthogonal transformation scheme is further proposed to map the nonlinear embedding data into the semantic subspace, which can well guarantee the semantic consistency between the data feature and its semantic representation. Consequently, an efficient closed-form solution is derived for discriminative hash code learning, which is very computationally efficient. In addition, an effective and stable online learning strategy is presented for optimizing modality-specific projection functions, featuring adaptivity to different training sizes and streaming data. The proposed FDDH approach theoretically approximates the bi-Lipschitz continuity, runs sufficiently fast, and also significantly improves the retrieval performance over the state-of-the-art methods. The source code is released at https://github.com/starxliu/FDDH .},
  archive      = {J_TNNLS},
  author       = {Xin Liu and Xingzhi Wang and Yiu-Ming Cheung},
  doi          = {10.1109/TNNLS.2021.3076684},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6306-6320},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FDDH: Fast discriminative discrete hashing for large-scale cross-modal retrieval},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating an adaptive and robust walking pattern for a
prosthetic ankle–foot by utilizing a nonlinear autoregressive network
with exogenous inputs. <em>TNNLS</em>, <em>33</em>(11), 6297–6305. (<a
href="https://doi.org/10.1109/TNNLS.2021.3076060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges in developing powered lower limb prostheses is emulating the behavior of an intact lower limb with different walking speeds over diverse terrains. Numerous studies have been conducted on control algorithms in the field of rehabilitation robotics to achieve this overarching goal. Recent studies on powered prostheses have frequently used a hierarchical control scheme consisting of three control levels. Most control structures have at least one element of discrete transition properties that requires numerous sensors to improve classification accuracy, consequently increasing computational load and costs. In this study, we proposed a user-independent and free-mode method for eliminating the need to switch among different controllers. We constructed a database by using four OPAL wearable devices (Mobility Lab, APDM Inc., USA) for seven able-bodied subjects. We recorded the gait of each subject at three ambulation speeds during ground-level walking to train a nonlinear autoregressive network with an exogenous input recurrent neural network (NARX RNN) to estimate foot orientation (angular position) in the sagittal plane using shank angular velocity as external input. The trained NARX RNN estimated the foot orientation of all the subjects at different walking speeds over flat terrain with an average root-mean-square error (RMSE) of 2.1° ± 1.7°. The minimum correlation between the estimated and measured values was 86\%. Moreover, a t-test showed that the error was normally distributed with a high certainty level (0.88 minimum $p$ -value).},
  archive      = {J_TNNLS},
  author       = {Hamza Al Kouzbary and Mouaz Al Kouzbary and Lai Kuan Tham and Jingjing Liu and Hanie Nadia Shasmin and Noor Azuan Abu Osman},
  doi          = {10.1109/TNNLS.2021.3076060},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6297-6305},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generating an adaptive and robust walking pattern for a prosthetic Ankle–Foot by utilizing a nonlinear autoregressive network with exogenous inputs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resource-aware distributed differential evolution for
training expensive neural-network-based controller in power electronic
circuit. <em>TNNLS</em>, <em>33</em>(11), 6286–6296. (<a
href="https://doi.org/10.1109/TNNLS.2021.3075205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural-network (NN)-based control method is a new emerging promising technique for controller design in a power electronic circuit (PEC). However, the optimization of NN-based controllers (NNCs) has significant challenges in two aspects. The first challenge is that the search space of the NNC optimization problem is such complex that the global optimization ability of the existing algorithms still needs to be improved. The second challenge is that the training process of the NNC parameters is very computationally expensive and requires a long execution time. Thus, in this article, we develop a powerful evolutionary computation-based algorithm to find a high-quality solution and reduce computational time. First, the differential evolution (DE) algorithm is adopted because it is a powerful global optimizer in solving a complex optimization problem. This can help to overcome the premature convergence in local optima to train the NNC parameters well. Second, to reduce the computational time, the DE is extended to distribute DE (DDE) by dispatching all the individuals to different distributed computing resources for parallel computing. Moreover, a resource-aware strategy (RAS) is designed to further efficiently utilize the resources by adaptively dispatching individuals to resources according to the real-time performance of the resources, which can simultaneously concern the computing ability and load state of each resource. Experimental results show that, compared with some other typical evolutionary algorithms, the proposed algorithm can get significantly better solutions within a shorter computational time.},
  archive      = {J_TNNLS},
  author       = {Xiao-Fang Liu and Zhi-Hui Zhan and Jun Zhang},
  doi          = {10.1109/TNNLS.2021.3075205},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6286-6296},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Resource-aware distributed differential evolution for training expensive neural-network-based controller in power electronic circuit},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning with noisy labels via self-reweighting from class
centroids. <em>TNNLS</em>, <em>33</em>(11), 6275–6285. (<a
href="https://doi.org/10.1109/TNNLS.2021.3073248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep neural networks have been proved effective in many applications, they are data hungry, and training deep models often requires laboriously labeled data. However, when labeled data contain erroneous labels, they often lead to model performance degradation. A common solution is to assign each sample with a dynamic weight during optimization, and the weight is adjusted in accordance with the loss. However, those weights are usually unreliable since they are measured by the losses of corrupted labels. Thus, this scheme might impede the discriminative ability of neural networks trained on noisy data. To address this issue, we propose a novel reweighting method, dubbed self-reweighting from class centroids (SRCC), by assigning sample weights based on the similarities between the samples and our online learned class centroids. Since we exploit statistical class centers in the image feature space to reweight data samples in learning, our method is robust to noise caused by corrupted labels. In addition, even after reweighting the noisy data, the decision boundaries might still suffer distortions. Thus, we leverage mixed inputs that are generated by linearly interpolating two random images and their labels to further regularize the boundaries. We employ the learned class centroids to evaluate the confidence of our generated mixed data via measuring feature similarities. During the network optimization, the class centroids are updated as more discriminative feature representations of original images are learned. In doing so, SRCC will generate more robust weighting coefficients for noisy and mixed data and facilitates our feature representation learning in return. Extensive experiments on both the synthetic and real image recognition tasks demonstrate that our method SRCC outperforms the state of the art on learning with noisy data.},
  archive      = {J_TNNLS},
  author       = {Fan Ma and Yu Wu and Xin Yu and Yi Yang},
  doi          = {10.1109/TNNLS.2021.3073248},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6275-6285},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning with noisy labels via self-reweighting from class centroids},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial entropy optimization for unsupervised domain
adaptation. <em>TNNLS</em>, <em>33</em>(11), 6263–6274. (<a
href="https://doi.org/10.1109/TNNLS.2021.3073119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is proposed to deal with the challenging problem where the probability distribution of the training source is different from the testing target. Recently, adversarial learning has become the dominating technique for domain adaptation. Usually, adversarial domain adaptation methods simultaneously train a feature learner and a domain discriminator to learn domain-invariant features. Accordingly, how to effectively train the domain-adversarial model to learn domain-invariant features becomes a challenge in the community. To this end, we propose in this article a novel domain adaptation scheme named adversarial entropy optimization (AEO) to address the challenge. Specifically, we minimize the entropy when samples are from the independent distributions of source domain or target domain to improve the discriminability of the model. At the same time, we maximize the entropy when features are from the combined distribution of source domain and target domain so that the domain discriminator can be confused and the transferability of representations can be promoted. This minimax regime is well matched with the core idea of adversarial learning, empowering our model with transferability as well as discriminability for domain adaptation tasks. Also, AEO is flexible and compatible with different deep networks and domain adaptation frameworks. Experiments on five data sets show that our method can achieve state-of-the-art performance across diverse domain adaptation tasks.},
  archive      = {J_TNNLS},
  author       = {Ao Ma and Jingjing Li and Ke Lu and Lei Zhu and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2021.3073119},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6263-6274},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial entropy optimization for unsupervised domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LIAF-net: Leaky integrate and analog fire network for
lightweight and efficient spatiotemporal information processing.
<em>TNNLS</em>, <em>33</em>(11), 6249–6262. (<a
href="https://doi.org/10.1109/TNNLS.2021.3073016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) based on the leaky integrate and fire (LIF) model have been applied to energy-efficient temporal and spatiotemporal processing tasks. Due to the bioplausible neuronal dynamics and simplicity, LIF-SNN benefits from event-driven processing, however, usually face the embarrassment of reduced performance. This may because, in LIF-SNN, the neurons transmit information via spikes. To address this issue, in this work, we propose a leaky integrate and analog fire (LIAF) neuron model so that analog values can be transmitted among neurons, and a deep network termed LIAF-Net is built on it for efficient spatiotemporal processing. In the temporal domain, LIAF follows the traditional LIF dynamics to maintain its temporal processing capability. In the spatial domain, LIAF is able to integrate spatial information through convolutional integration or fully connected integration. As a spatiotemporal layer, LIAF can also be used with traditional artificial neural network (ANN) layers jointly. In addition, the built network can be trained with backpropagation through time (BPTT) directly, which avoids the performance loss caused by ANN to SNN conversion. Experiment results indicate that LIAF-Net achieves comparable performance to the gated recurrent unit (GRU) and long short-term memory (LSTM) on bAbI question answering (QA) tasks and achieves state-of-the-art performance on spatiotemporal dynamic vision sensor (DVS) data sets, including MNIST-DVS, CIFAR10-DVS, and DVS128 Gesture, with much less number of synaptic weights and computational overhead compared with traditional networks built by LSTM, GRU, convolutional LSTM (ConvLSTM), or 3-D convolution (Conv3D). Compared with traditional LIF-SNN, LIAF-Net also shows dramatic accuracy gain on all these experiments. In conclusion, LIAF-Net provides a framework combining the advantages of both ANNs and SNNs for lightweight and efficient spatiotemporal information processing.},
  archive      = {J_TNNLS},
  author       = {Zhenzhi Wu and Hehui Zhang and Yihan Lin and Guoqi Li and Meng Wang and Ye Tang},
  doi          = {10.1109/TNNLS.2021.3073016},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6249-6262},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LIAF-net: Leaky integrate and analog fire network for lightweight and efficient spatiotemporal information processing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting operation importance for differentiable neural
architecture search. <em>TNNLS</em>, <em>33</em>(11), 6235–6248. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, differentiable neural architecture search (NAS) methods have made significant progress in reducing the computational costs of NASs. Existing methods search for the best architecture by choosing candidate operations with higher architecture weights. However, architecture weights cannot accurately reflect the importance of each operation, that is, the operation with the highest weight might not be related to the best performance. To circumvent this deficiency, we propose a novel indicator that can fully represent the operation importance and, thus, serve as an effective metric to guide the model search. Based on this indicator, we further develop a NAS scheme for “exploiting operation importance for effective NAS” (EoiNAS). More precisely, we propose a high-order Markov chain-based strategy to slim the search space to further improve search efficiency and accuracy. To evaluate the effectiveness of the proposed EoiNAS, we applied our method to two tasks: image classification and semantic segmentation. Extensive experiments on both tasks provided strong evidence that our method is capable of discovering high-performance architectures while guaranteeing the requisite efficiency during searching.},
  archive      = {J_TNNLS},
  author       = {Yuan Zhou and Xukai Xie and Sun-Yuan Kung},
  doi          = {10.1109/TNNLS.2021.3072950},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6235-6248},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Exploiting operation importance for differentiable neural architecture search},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive neural network control of a flexible spacecraft
subject to input nonlinearity and asymmetric output constraint.
<em>TNNLS</em>, <em>33</em>(11), 6226–6234. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the vibration reducing and angle tracking problems of a flexible unmanned spacecraft system subject to input nonlinearity, asymmetric output constraint, and system parameter uncertainties. Using the backstepping technique, a boundary control scheme is designed to suppress the vibration and regulate the angle of the spacecraft. A modified asymmetric barrier Lyapunov function is utilized to ensure that the output constraint is never transgressed. Considering the system robustness, neural networks are used to handle the system parameter uncertainties and compensate for the effect of input nonlinearity. With the proposed adaptive neural network control law, the stability of the closed-loop system is proved based on the Lyapunov analysis, and numerical simulations are carried out to show the validity of the developed control scheme.},
  archive      = {J_TNNLS},
  author       = {Yu Liu and Xiongbin Chen and Yilin Wu and He Cai and Hiroshi Yokoi},
  doi          = {10.1109/TNNLS.2021.3072907},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6226-6234},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network control of a flexible spacecraft subject to input nonlinearity and asymmetric output constraint},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep cellular recurrent network for efficient analysis of
time-series data with spatial information. <em>TNNLS</em>,
<em>33</em>(11), 6215–6225. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient processing of large-scale time-series data is an intricate problem in machine learning. Conventional sensor signal processing pipelines with hand-engineered feature extraction often involve huge computational costs with high dimensional data. Deep recurrent neural networks have shown promise in automated feature learning for improved time-series processing. However, generic deep recurrent models grow in scale and depth with the increased complexity of the data. This is particularly challenging in presence of high dimensional data with temporal and spatial characteristics. Consequently, this work proposes a novel deep cellular recurrent neural network (DCRNN) architecture to efficiently process complex multidimensional time-series data with spatial information. The cellular recurrent architecture in the proposed model allows for location-aware synchronous processing of time-series data from spatially distributed sensor signal sources. Extensive trainable parameter sharing due to cellularity in the proposed architecture ensures efficiency in the use of recurrent processing units with high-dimensional inputs. This study also investigates the versatility of the proposed DCRNN model for the classification of multiclass time-series data from different application domains. Consequently, the proposed DCRNN architecture is evaluated using two time-series data sets: a multichannel scalp electroencephalogram (EEG) data set for seizure detection, and a machine fault detection data set obtained in-house. The results suggest that the proposed architecture achieves state-of-the-art performance while utilizing substantially less trainable parameters when compared to comparable methods in the literature.},
  archive      = {J_TNNLS},
  author       = {Lasitha S. Vidyaratne and Mahbubul Alam and Alexander M. Glandon and Anna Shabalina and Chris Tennant and Khan M. Iftekharuddin},
  doi          = {10.1109/TNNLS.2021.3072885},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6215-6225},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep cellular recurrent network for efficient analysis of time-series data with spatial information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Neural adaptive self-triggered control for uncertain
nonlinear systems with input hysteresis. <em>TNNLS</em>,
<em>33</em>(11), 6206–6214. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of neural adaptive self-triggered tracking control for uncertain nonlinear systems with input hysteresis is considered. Combining radial basis function neural networks (RBFNNs) and adaptive backstepping technique, an adaptive self-triggered tracking control approach is developed, where the next trigger instant is determined by the current information. Compared with the event-triggered control mechanism, its biggest advantage is that it does not need to continuously monitor the trigger condition of the system, which is convenient for physical realization. By the proposed controller, the hysteresis’s effect can be compensated effectively and the tracking error can be bounded by an explicit function of design parameters. Simultaneously, all other signals in the closed-loop system can be remaining bounded. Finally, two examples are presented to verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jianhui Wang and Hongkang Zhang and Kemao Ma and Zhi Liu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3072784},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6206-6214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural adaptive self-triggered control for uncertain nonlinear systems with input hysteresis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the sufficient condition for solving the gap-filling
problem using deep convolutional neural networks. <em>TNNLS</em>,
<em>33</em>(11), 6194–6205. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) are routinely used for image segmentation of biomedical data sets to obtain quantitative measurements of cellular structures like tissues. These cellular structures often contain gaps in their boundaries, leading to poor segmentation performance when using DCNNs like the U-Net. The gaps can usually be corrected by post-hoc computer vision (CV) steps, which are specific to the data set and require a disproportionate amount of work. As DCNNs are Universal Function Approximators, it is conceivable that the corrections should be obsolete by selecting the appropriate architecture for the DCNN. In this article, we present a novel theoretical framework for the gap-filling problem in DCNNs that allows the selection of architecture to circumvent the CV steps. Combining information-theoretic measures of the data set with a fundamental property of DCNNs, the size of their receptive field, allows us to formulate statements about the solvability of the gap-filling problem independent of the specifics of model training. In particular, we obtain mathematical proof showing that the maximum proficiency of filling a gap by a DCNN is achieved if its receptive field is larger than the gap length. We then demonstrate the consequence of this result using numerical experiments on a synthetic and real data set and compare the gap-filling ability of the ubiquitous U-Net architecture with variable depths. Our code is available at https://github.com/ai-biology/dcnn-gap-filling .},
  archive      = {J_TNNLS},
  author       = {Felix Peppert and Max Von Kleist and Christof Schütte and Vikram Sunkara},
  doi          = {10.1109/TNNLS.2021.3072746},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6194-6205},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the sufficient condition for solving the gap-filling problem using deep convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven dynamic multiobjective optimal control: An
aspiration-satisfying reinforcement learning approach. <em>TNNLS</em>,
<em>33</em>(11), 6183–6193. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an iterative data-driven algorithm for solving dynamic multiobjective (MO) optimal control problems arising in control of nonlinear continuous-time systems. It is first shown that the Hamiltonian functional corresponding to each objective can be leveraged to compare the performance of admissible policies. Hamiltonian inequalities are then used for which their satisfaction guarantees satisfying the objectives’ aspirations. Relaxed Hamilton–Jacobi–Bellman (HJB) equations in terms of HJB inequalities are then solved in a dynamic constrained MO framework to find Pareto optimal solutions. Relation to satisficing (good enough) decision-making framework is shown. A sum-of-square (SOS)-based iterative algorithm is developed to solve the formulated aspiration-satisfying MO optimization. To obviate the requirement of complete knowledge of the system dynamics, a data-driven satisficing reinforcement learning approach is proposed to solve the SOS optimization problem in real time using only the information of the system trajectories measured during a time interval without having full knowledge of the system dynamics. Finally, two simulation examples are utilized to verify the analytical results of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Majid Mazouchi and Yongliang Yang and Hamidreza Modares},
  doi          = {10.1109/TNNLS.2021.3072571},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6183-6193},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven dynamic multiobjective optimal control: An aspiration-satisfying reinforcement learning approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time robust intelligent control of strict-feedback
nonlinear systems with flight dynamics application. <em>TNNLS</em>,
<em>33</em>(11), 6173–6182. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tracking control is investigated for a class of uncertain strict-feedback systems with robust design and learning systems. Using the switching mechanism, the states will be driven back by the robust design when they run out of the region of adaptive control. The adaptive design is working to achieve precise adaptation and higher tracking precision in the neural working domain, while the finite-time robust design is developed to make the system stable outside. To achieve good tracking performance, the novel prediction error-based adaptive law is constructed by considering the estimation performance. Furthermore, the output constraint is achieved by imbedding the barrier Lyapunov function-based design. The finite-time convergence and the uniformly ultimate boundedness of the system signal can be guaranteed. Simulation studies show that the proposed approach presents robustness and adaptation to system uncertainty.},
  archive      = {J_TNNLS},
  author       = {Bin Xu and Xia Wang and Yingxin Shou and Peng Shi and Zhongke Shi},
  doi          = {10.1109/TNNLS.2021.3072552},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6173-6182},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time robust intelligent control of strict-feedback nonlinear systems with flight dynamics application},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparative study of deep neural network-aided canonical
correlation analysis-based process monitoring and fault detection
methods. <em>TNNLS</em>, <em>33</em>(11), 6158–6172. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate analysis is an important kind of method in process monitoring and fault detection, in which the canonical correlation analysis (CCA) makes use of the correlation change between two groups of variables to distinguish the system status and has been greatly studied and applied. For the monitoring of nonlinear dynamic systems, the deep neural network-aided CCA (DNN-CCA) has received much attention recently, but it lacks a general definition and comparative study of different network structures. Therefore, this article first introduces four deep neural network (DNN) models that are suitable to combine with CCA, and the general form of DNN-CCA is given in detail. Then, the experimental comparison of these methods is conducted through three cases, so as to analyze the characteristics and distinctions of CCA aided by each DNN model. Finally, some suggestions on method selection are summarized, and the existed open issues in the current DNN-CCA form and future directions are discussed.},
  archive      = {J_TNNLS},
  author       = {Zhiwen Chen and Ketian Liang and Steven X. Ding and Chao Yang and Tao Peng and Xiaofeng Yuan},
  doi          = {10.1109/TNNLS.2021.3072491},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6158-6172},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A comparative study of deep neural network-aided canonical correlation analysis-based process monitoring and fault detection methods},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluster synchronization of coupled neural networks with lévy
noise via event-triggered pinning control. <em>TNNLS</em>,
<em>33</em>(11), 6144–6157. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster synchronization means that all multiagents are divided into different clusters according to the equations or roles of nodes in a complex network, and by designing an appropriate algorithm, each cluster can achieve synchronization to a certain value or an isolated node. However, the synchronization values between different clusters are different. With a feedback controller based on the calculation of the control input value and a trigger condition leading to the updating instants, this article introduces the trigger mechanism and designs a new data sampling strategy to achieve cluster synchronization of the coupled neural networks (CNNs), which reduces the number of updates of the controller, thereby reducing unnecessary waste of limited resources. In addition, an example proposes a synchronization algorithm and gives iterative procedures to calculate the trigger instants and prove the validity of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Wuneng Zhou and Yuqing Sun and Xin Zhang and Peng Shi},
  doi          = {10.1109/TNNLS.2021.3072475},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6144-6157},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cluster synchronization of coupled neural networks with lévy noise via event-triggered pinning control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight deep neural network for joint learning of
underwater object detection and color conversion. <em>TNNLS</em>,
<em>33</em>(11), 6129–6143. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image processing has been shown to exhibit significant potential for exploring underwater environments. It has been applied to a wide variety of fields, such as underwater terrain scanning and autonomous underwater vehicles (AUVs)-driven applications, such as image-based underwater object detection. However, underwater images often suffer from degeneration due to attenuation, color distortion, and noise from artificial lighting sources as well as the effects of possibly low-end optical imaging devices. Thus, object detection performance would be degraded accordingly. To tackle this problem, in this article, a lightweight deep underwater object detection network is proposed. The key is to present a deep model for jointly learning color conversion and object detection for underwater images. The image color conversion module aims at transforming color images to the corresponding grayscale images to solve the problem of underwater color absorption to enhance the object detection performance with lower computational complexity. The presented experimental results with our implementation on the Raspberry pi platform have justified the effectiveness of the proposed lightweight jointly learning model for underwater object detection compared with the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Chia-Hung Yeh and Chu-Han Lin and Li-Wei Kang and Chih-Hsiang Huang and Min-Hui Lin and Chuan-Yu Chang and Chua-Chin Wang},
  doi          = {10.1109/TNNLS.2021.3072414},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6129-6143},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lightweight deep neural network for joint learning of underwater object detection and color conversion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AutoMER: Spatiotemporal neural architecture search for
microexpression recognition. <em>TNNLS</em>, <em>33</em>(11), 6116–6128.
(<a href="https://doi.org/10.1109/TNNLS.2021.3072290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial microexpressions offer useful insights into subtle human emotions. This unpremeditated emotional leakage exhibits the true emotions of a person. However, the minute temporal changes in the video sequences are very difficult to model for accurate classification. In this article, we propose a novel spatiotemporal architecture search algorithm, AutoMER for microexpression recognition (MER). Our main contribution is a new parallelogram design-based search space for efficient architecture search. We introduce a spatiotemporal feature module named 3-D singleton convolution for cell-level analysis. Furthermore, we present four such candidate operators and two 3-D dilated convolution operators to encode the raw video sequences in an end-to-end manner. To the best of our knowledge, this is the first attempt to discover 3-D convolutional neural network (CNN) architectures with a network-level search for MER. The searched models using the proposed AutoMER algorithm are evaluated over five microexpression data sets: CASME-I, SMIC, CASME-II, CAS(ME) $\hat{2}$ , and SAMM. The proposed generated models quantitatively outperform the existing state-of-the-art approaches. The AutoMER is further validated with different configurations, such as downsampling rate factor, multiscale singleton 3-D convolution, parallelogram, and multiscale kernels. Overall, five ablation experiments were conducted to analyze the operational insights of the proposed AutoMER.},
  archive      = {J_TNNLS},
  author       = {Monu Verma and M. Satish Kumar Reddy and Yashwanth Reddy Meedimale and Murari Mandal and Santosh Kumar Vipparthi},
  doi          = {10.1109/TNNLS.2021.3072290},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6116-6128},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {AutoMER: Spatiotemporal neural architecture search for microexpression recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy-preserving asynchronous vertical federated learning
algorithms for multiparty collaborative learning. <em>TNNLS</em>,
<em>33</em>(11), 6103–6115. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The privacy-preserving federated learning for vertically partitioned (VP) data has shown promising results as the solution of the emerging multiparty joint modeling application, in which the data holders (such as government branches, private finance, and e-business companies) collaborate throughout the learning process rather than relying on a trusted third party to hold data. However, most of the existing federated learning algorithms for VP data are limited to synchronous computation. To improve the efficiency when the unbalanced computation/communication resources are common among the parties in the federated learning system, it is essential to develop asynchronous training algorithms for VP data while keeping the data privacy. In this article, we propose an asynchronous federated stochastic gradient descent (AFSGD-VP) algorithm and its two variance reduction variants, including stochastic variance reduced gradient (SVRG) and SAGA on the VP data. Moreover, we provide the convergence analyses of AFSGD-VP and its SVRG and SAGA variants under the condition of strong convexity and without any restrictions of staleness. We also discuss their model privacy, data privacy, computational complexities, and communication costs. To the best of our knowledge, AFSGD-VP and its SVRG and SAGA variants are the first asynchronous federated learning algorithms for VP data with theoretical guarantees. Extensive experimental results on a variety of VP datasets not only verify the theoretical results of AFSGD-VP and its SVRG and SAGA variants but also show that our algorithms have much higher efficiency than the corresponding synchronous algorithms.},
  archive      = {J_TNNLS},
  author       = {Bin Gu and An Xu and Zhouyuan Huo and Cheng Deng and Heng Huang},
  doi          = {10.1109/TNNLS.2021.3072238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6103-6115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Privacy-preserving asynchronous vertical federated learning algorithms for multiparty collaborative learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dirichlet process mixture of generalized inverted dirichlet
distributions for positive vector data with extended variational
inference. <em>TNNLS</em>, <em>33</em>(11), 6089–6102. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian nonparametric approach for estimation of a Dirichlet process (DP) mixture of generalized inverted Dirichlet distributions [i.e., an infinite generalized inverted Dirichlet mixture model (InGIDMM)] has been proposed. The generalized inverted Dirichlet distribution has been proven to be efficient in modeling the vectors that contain only positive elements. Under the classical variational inference (VI) framework, the key challenge in the Bayesian estimation of InGIDMM is that the expectation of the joint distribution of data and variables cannot be explicitly calculated. Therefore, numerical methods are usually applied to simulate the optimal posterior distributions. With the recently proposed extended VI (EVI) framework, we introduce lower bound approximations to the original variational objective function in the VI framework such that an analytically tractable solution can be derived. Hence, the problem in numerical simulation has been overcome. By applying the DP mixture technique, an InGIDMM can automatically determine the number of mixture components from the observed data. Moreover, the DP mixture model with an infinite number of mixture components also avoids the problems of underfitting and overfitting. The performance of the proposed approach is demonstrated with both synthesized data and real-life data applications.},
  archive      = {J_TNNLS},
  author       = {Zhanyu Ma and Yuping Lai and Jiyang Xie and Deyu Meng and W. Bastiaan Kleijn and Jun Guo and Jingyi Yu},
  doi          = {10.1109/TNNLS.2021.3072209},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6089-6102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dirichlet process mixture of generalized inverted dirichlet distributions for positive vector data with extended variational inference},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Custom hardware architectures for deep learning on portable
devices: A review. <em>TNNLS</em>, <em>33</em>(11), 6068–6088. (<a
href="https://doi.org/10.1109/TNNLS.2021.3082304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The staggering innovations and emergence of numerous deep learning (DL) applications have forced researchers to reconsider hardware architecture to accommodate fast and efficient application-specific computations. Applications, such as object detection, image recognition, speech translation, as well as music synthesis and image generation, can be performed with high accuracy at the expense of substantial computational resources using DL. Furthermore, the desire to adopt Industry 4.0 and smart technologies within the Internet of Things infrastructure has initiated several studies to enable on-chip DL capabilities for resource-constrained devices. Specialized DL processors reduce dependence on cloud servers, improve privacy, lessen latency, and mitigate bandwidth congestion. As we reach the limits of shrinking transistors, researchers are exploring various application-specific hardware architectures to meet the performance and efficiency requirements for DL tasks. Over the past few years, several software optimizations and hardware innovations have been proposed to efficiently perform these computations. In this article, we review several DL accelerators, as well as technologies with emerging devices, to highlight their architectural features in application-specific integrated circuit (IC) and field-programmable gate array (FPGA) platforms. Finally, the design considerations for DL hardware in portable applications have been discussed, along with some deductions about the future trends and potential research directions to innovate DL accelerator architectures further. By compiling this review, we expect to help aspiring researchers widen their knowledge in custom hardware architectures for DL.},
  archive      = {J_TNNLS},
  author       = {Kh Shahriya Zaman and Mamun Bin Ibne Reaz and Sawal Hamid Md Ali and Ahmad Ashrif A Bakar and Muhammad Enamul Hoque Chowdhury},
  doi          = {10.1109/TNNLS.2021.3082304},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6068-6088},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Custom hardware architectures for deep learning on portable devices: A review},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vehicle detection from UAV imagery with deep learning: A
review. <em>TNNLS</em>, <em>33</em>(11), 6047–6067. (<a
href="https://doi.org/10.1109/TNNLS.2021.3080276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection from unmanned aerial vehicle (UAV) imagery is one of the most important tasks in a large number of computer vision-based applications. This crucial task needed to be done with high accuracy and speed. However, it is a very challenging task due to many characteristics related to the aerial images and the used hardware, such as different vehicle sizes, orientations, types, density, limited datasets, and inference speed. In recent years, many classical and deep-learning-based methods have been proposed in the literature to address these problems. Handed engineering- and shallow learning-based techniques suffer from poor accuracy and generalization to other complex cases. Deep-learning-based vehicle detection algorithms achieved better results due to their powerful learning ability. In this article, we provide a review on vehicle detection from UAV imagery using deep learning techniques. We start by presenting the different types of deep learning architectures, such as convolutional neural networks, recurrent neural networks, autoencoders, generative adversarial networks, and their contribution to improve the vehicle detection task. Then, we focus on investigating the different vehicle detection methods, datasets, and the encountered challenges all along with the suggested solutions. Finally, we summarize and compare the techniques used to improve vehicle detection from UAV-based images, which could be a useful aid to researchers and developers to select the most adequate method for their needs.},
  archive      = {J_TNNLS},
  author       = {Abdelmalek Bouguettaya and Hafed Zarzour and Ahmed Kechida and Amine Mohammed Taberkit},
  doi          = {10.1109/TNNLS.2021.3080276},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {11},
  pages        = {6047-6067},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Vehicle detection from UAV imagery with deep learning: A review},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast rates of gaussian empirical gain maximization with
heavy-tailed noise. <em>TNNLS</em>, <em>33</em>(10), 6038–6043. (<a
href="https://doi.org/10.1109/TNNLS.2022.3171171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a regression setup, we study in this brief the performance of Gaussian empirical gain maximization (EGM), which includes a broad variety of well-established robust estimation approaches. In particular, we conduct a refined learning theory analysis for Gaussian EGM, investigate its regression calibration properties, and develop improved convergence rates in the presence of heavy-tailed noise. To achieve these purposes, we first introduce a new weak moment condition that could accommodate the cases where the noise distribution may be heavy-tailed. Based on the moment condition, we then develop a novel comparison theorem that can be used to characterize the regression calibration properties of Gaussian EGM. It also plays an essential role in deriving improved convergence rates. Therefore, the present study broadens our theoretical understanding of Gaussian EGM.},
  archive      = {J_TNNLS},
  author       = {Shouyou Huang and Yunlong Feng and Qiang Wu},
  doi          = {10.1109/TNNLS.2022.3171171},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6038-6043},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast rates of gaussian empirical gain maximization with heavy-tailed noise},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered adaptive control of uncertain nonlinear
systems with composite condition. <em>TNNLS</em>, <em>33</em>(10),
6030–6037. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concentrates on the event-based collaborative design for strict-feedback systems with uncertain nonlinearities. The controller is designed based on neural network (NN) weights adaptive law. The controller and NN weights adaptive law are only updated at the triggering instants determined by a novel composite triggering threshold. Considering the conservativeness of event condition, the state-model error is integrated into constructing the composite condition and NN weights adaptive law. In the context of the proposed mechanism, the requirements of system information and the allowable range of event-triggering error are relaxed. The number of triggering instants is greatly reduced without deteriorating the system performance. Moreover, the stability of the closed-loop is proved by the Lyapunov method following time-interval and sampling instants. Simulation results show the effectiveness of the scheme proposed in this article.},
  archive      = {J_TNNLS},
  author       = {Xinglan Liu and Bin Xu and Yingxin Shou and Quan-Yong Fan and Yingxue Chen},
  doi          = {10.1109/TNNLS.2021.3072107},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6030-6037},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive control of uncertain nonlinear systems with composite condition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural embedding singular value decomposition for
collaborative filtering. <em>TNNLS</em>, <em>33</em>(10), 6021–6029. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Singular value decomposition (SVD) is one of the most effective algorithms in recommender systems (RSs). Due to the iterative nature of SVD algorithms, one big challenge is initialization that has a major impact on the convergence and performance of RSs. Unfortunately, existing SVD algorithms in the literature typically initialize the user and item features in a random manner; thus, data information is not fully utilized. This work addresses the challenge of developing an efficient initialization method for SVD algorithms. We propose a general neural embedding initialization framework, where a low-complexity probabilistic autoencoder neural network initializes the features of user and item. This framework supports explicit and implicit feedback data sets. The design details of our proposed framework are elaborated and discussed. Experimental results show that RSs based on our proposed initialization framework outperform the state-of-the-art methods in rating prediction. Moreover, regarding item ranking, our proposed framework shows an improvement of at least 2.20\% ~5.74\% than existing SVD algorithms and other matrix factorization methods in the literature.},
  archive      = {J_TNNLS},
  author       = {Tianlin Huang and Rujie Zhao and Lvqing Bi and Defu Zhang and Chao Lu},
  doi          = {10.1109/TNNLS.2021.3070853},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6021-6029},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural embedding singular value decomposition for collaborative filtering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully decoupled neural network learning using delayed
gradients. <em>TNNLS</em>, <em>33</em>(10), 6013–6020. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training neural networks with backpropagation (BP) requires a sequential passing of activations and gradients. This has been recognized as the lockings (i.e., the forward, backward, and update lockings) among modules (each module contains a stack of layers) inherited from the BP. In this brief, we propose a fully decoupled training scheme using delayed gradients (FDG) to break all these lockings. The FDG splits a neural network into multiple modules and trains them independently and asynchronously using different workers (e.g., GPUs). We also introduce a gradient shrinking process to reduce the stale gradient effect caused by the delayed gradients. Our theoretical proofs show that the FDG can converge to critical points under certain conditions. Experiments are conducted by training deep convolutional neural networks to perform classification tasks on several benchmark data sets. These experiments show comparable or better results of our approach compared with the state-of-the-art methods in terms of generalization and acceleration. We also show that the FDG is able to train various networks, including extremely deep ones (e.g., ResNet-1202), in a decoupled fashion.},
  archive      = {J_TNNLS},
  author       = {Huiping Zhuang and Yi Wang and Qinglai Liu and Zhiping Lin},
  doi          = {10.1109/TNNLS.2021.3069883},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6013-6020},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully decoupled neural network learning using delayed gradients},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synchronization of chaotic neural networks: Average-delay
impulsive control. <em>TNNLS</em>, <em>33</em>(10), 6007–6012. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the brief, delayed impulsive control is investigated for the synchronization of chaotic neural networks. In order to overcome the difficulty that the delays in impulsive control input can be flexible, we utilize the concept of average impulsive delay (AID). To be specific, we relax the restriction on the upper/lower bound of such delays, which is not well addressed in most existing results. Then, by using the methods of average impulsive interval (AII) and AID, we establish a Lyapunov-based relaxed condition for the synchronization of chaotic neural networks. It is shown that the time delay in impulsive control input may bring a synchronizing effect to the chaos synchronization. Furthermore, we use the method of linear matrix inequality (LMI) for designing average-delay impulsive control, in which the delays satisfy the AID condition. Finally, an illustrative example is given to show the validity of the derived results.},
  archive      = {J_TNNLS},
  author       = {Bangxin Jiang and Jungang Lou and Jianquan Lu and Kaibo Shi},
  doi          = {10.1109/TNNLS.2021.3069830},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {6007-6012},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of chaotic neural networks: Average-delay impulsive control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring effective connectivity networks from fMRI time
series with a temporal entropy-score. <em>TNNLS</em>, <em>33</em>(10),
5993–6006. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring brain-effective connectivity networks from neuroimaging data has become a very hot topic in neuroinformatics and bioinformatics. In recent years, the search methods based on Bayesian network score have been greatly developed and become an emerging method for inferring effective connectivity. However, the previous score functions ignore the temporal information from functional magnetic resonance imaging (fMRI) series data and may not be able to determine all orientations in some cases. In this article, we propose a novel score function for inferring effective connectivity from fMRI data based on the conditional entropy and transfer entropy (TE) between brain regions. The new score employs the TE to capture the temporal information and can effectively infer connection directions between brain regions. Experimental results on both simulated and real-world data demonstrate the efficacy of our proposed score function.},
  archive      = {J_TNNLS},
  author       = {Jinduo Liu and Junzhong Ji and Guangxu Xun and Aidong Zhang},
  doi          = {10.1109/TNNLS.2021.3072149},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5993-6006},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Inferring effective connectivity networks from fMRI time series with a temporal entropy-score},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IVS-caffe—hardware-oriented neural network model
development. <em>TNNLS</em>, <em>33</em>(10), 5978–5992. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a hardware-oriented neural network development tool, called Intelligent Vision System Lab (IVS)-Caffe. IVS-Caffe can simulate the hardware behavior of convolution neural network inference calculation. It can quantize weights, input, and output features of convolutional neural network (CNN) and simulate the behavior of multipliers and accumulators calculation to achieve the bit-accurate result. Furthermore, it can test the accuracy of the chosen CNN hardware accelerator. Besides, this article proposes an algorithm to solve the deviation of gradient backpropagation in the bit-accurate quantized multipliers and accumulators. This allows the training of a bit-accurate model and further increases the accuracy of the CNN model at user-designed bit width. The proposed tool takes Faster region based CNN (R-CNN) + Matthew D. Zeiler and Rob Fergus (ZF)-Net, Single Shot MultiBox Detector (SSD) + VGG, SSD + MobileNet, and Tiny you only look once (YOLO) v2 as the experimental models. These models include both one-stage object detection and two-stage object detection models, and base networks include the convolution layer, the fully connected layer, and the modern advanced layers, such as the inception module and depthwise separable convolution. In these experiments, direct quantization of layer-I/O fixed-point models to bit-accurate models will have a 2\% mean average precision (mAP) drop of accuracy in the constraint that all layers’ accumulators and multipliers are quantized to less or equal to 14 and 12 bit, respectively. After retraining of these quantized models with the proposed IVS-Caffe, we can achieve less than 1\% mAP drop in accuracy in the constraint that all layers’ accumulators and multipliers are quantized to less or equal to 14 and 11 bit, respectively. With the proposed IVS-Caffe, we can analyze the accuracy of the target model when it is running at hardware accelerators with different bit widths, which is beneficial to fine-tune the target model or customize the hardware accelerators with lower power consumption. Code is available at https://github.com/apple35932003/IVS-Caffe .},
  archive      = {J_TNNLS},
  author       = {Chia-Chi Tsai and Jiun-In Guo},
  doi          = {10.1109/TNNLS.2021.3072145},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5978-5992},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IVS-Caffe—Hardware-oriented neural network model development},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-efficient class-incremental learning for image
classification. <em>TNNLS</em>, <em>33</em>(10), 5966–5977. (<a
href="https://doi.org/10.1109/TNNLS.2021.3072041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the memory-resource-limited constraints, class-incremental learning (CIL) usually suffers from the “catastrophic forgetting” problem when updating the joint classification model on the arrival of newly added classes. To cope with the forgetting problem, many CIL methods transfer the knowledge of old classes by preserving some exemplar samples into the size-constrained memory buffer. To utilize the memory buffer more efficiently, we propose to keep more auxiliary low-fidelity exemplar samples, rather than the original real-high-fidelity exemplar samples. Such a memory-efficient exemplar preserving scheme makes the old-class knowledge transfer more effective. However, the low-fidelity exemplar samples are often distributed in a different domain away from that of the original exemplar samples, that is, a domain shift. To alleviate this problem, we propose a duplet learning scheme that seeks to construct domain-compatible feature extractors and classifiers, which greatly narrows down the above domain gap. As a result, these low-fidelity auxiliary exemplar samples have the ability to moderately replace the original exemplar samples with a lower memory cost. In addition, we present a robust classifier adaptation scheme, which further refines the biased classifier (learned with the samples containing distillation label knowledge about old classes) with the help of the samples of pure true class labels. Experimental results demonstrate the effectiveness of this work against the state-of-the-art approaches. We will release the code, baselines, and training statistics for all models to facilitate future research.},
  archive      = {J_TNNLS},
  author       = {Hanbin Zhao and Hui Wang and Yongjian Fu and Fei Wu and Xi Li},
  doi          = {10.1109/TNNLS.2021.3072041},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5966-5977},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory-efficient class-incremental learning for image classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noise robust face hallucination based on smooth correntropy
representation. <em>TNNLS</em>, <em>33</em>(10), 5953–5965. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face hallucination technologies have been widely developed during the past decades, among which the sparse manifold learning (SML)-based approaches have become the popular ones and achieved promising performance. However, these SML methods always failed in handling noisy images due to the least-square regression (LSR) they used for error approximation. To this end, we propose, in this article, a smooth correntropy representation (SCR) model for noisy face hallucination. In SCR, the correntropy regularization and smooth constraint are combined into one unified framework to improve the resolution of noisy face images. Specifically, we introduce the correntropy induced metric (CIM) rather than the LSR to regularize the encoding errors, which admits the proposed method robust to noise with uncertain distributions. Besides, the fused LASSO penalty is added into the feature space to ensure similar training samples holding similar representation coefficients. This encourages the SCR not only robust to noise but also can well exploit the inherent typological structure of patch manifold, resulting in more accurate representations in noise environment. Comparison experiments against several state-of-the-art methods demonstrate the superiority of SCR in super-resolving noisy low-resolution (LR) face images.},
  archive      = {J_TNNLS},
  author       = {Licheng Liu and Qiying Feng and C. L. Philip Chen and Yaonan Wang},
  doi          = {10.1109/TNNLS.2021.3071982},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5953-5965},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noise robust face hallucination based on smooth correntropy representation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal coding in spiking neural networks with alpha
synaptic function: Learning with backpropagation. <em>TNNLS</em>,
<em>33</em>(10), 5939–5952. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artificial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual spikes. In classification tasks, the output of the network is indicated by the first neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically plausible synaptic transfer function. In addition, we use trainable pulses that provide bias, add flexibility during training, and exploit the decayed part of the synaptic function. We show that such networks can be successfully trained on multiple data sets encoded in time, including MNIST. Our model outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. The spiking network spontaneously discovers two operating modes, mirroring the accuracy-speed tradeoff observed in human decision-making: a highly accurate but slow regime, and a fast but slightly lower accuracy regime. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks toward energy-efficient, state-based biologically inspired neural architectures. We provide open-source code for the model.},
  archive      = {J_TNNLS},
  author       = {Iulia-Maria Comşa and Krzysztof Potempa and Luca Versari and Thomas Fischbacher and Andrea Gesmundo and Jyrki Alakuijala},
  doi          = {10.1109/TNNLS.2021.3071976},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5939-5952},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal coding in spiking neural networks with alpha synaptic function: Learning with backpropagation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neuro-evolutionary direct policy search for multiobjective
optimal control. <em>TNNLS</em>, <em>33</em>(10), 5926–5938. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct policy search (DPS) is emerging as one of the most effective and widely applied reinforcement learning (RL) methods to design optimal control policies for multiobjective Markov decision processes (MOMDPs). Traditionally, DPS defines the control policy within a preselected functional class and searches its optimal parameterization with respect to a given set of objectives. The functional class should be tailored to the problem at hand and its selection is crucial, as it determines the search space within which solutions can be found. In MOMDPs problems, a different objective tradeoff determines a different fitness landscape, requiring a tradeoff-dynamic functional class selection. Yet, in state-of-the-art applications, the policy class is generally selected a priori and kept constant across the multidimensional objective space. In this work, we present a novel policy search routine called neuro-evolutionary multiobjective DPS (NEMODPS), which extends the DPS problem formulation to conjunctively search the policy functional class and its parameterization in a hyperspace containing policy architectures and coefficients. NEMODPS begins with a population of minimally structured approximating networks and progressively builds more sophisticated architectures by topological and parametrical mutation and crossover, and selection of the fittest individuals concerning multiple objectives. We tested NEMODPS for the problem of designing the control policy of a multipurpose water system. Numerical results show that the tradeoff-dynamic structural and parametrical policy search of NEMODPS is consistent across multiple runs, and outperforms the solutions designed via traditional DPS with predefined policy topologies.},
  archive      = {J_TNNLS},
  author       = {Marta Zaniolo and Matteo Giuliani and Andrea Castelletti},
  doi          = {10.1109/TNNLS.2021.3071960},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5926-5938},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuro-evolutionary direct policy search for multiobjective optimal control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Graph-based bayesian optimization for large-scale
objective-based experimental design. <em>TNNLS</em>, <em>33</em>(10),
5913–5925. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design is an inseparable part of most scientific and engineering tasks, including real and simulation-based experimental design processes and parameter/hyperparameter tuning/optimization. Several model-based experimental design techniques have been developed for design in domains with partial available knowledge about the underlying process. This article focuses on a powerful class of model-based experimental design called the mean objective cost of uncertainty (MOCU). The MOCU-based techniques are objective-based, meaning that they take the main objective of the process into account during the experimental design process. However, the lack of scalability of MOCU-based techniques prevents their application to most practical problems, including large discrete or combinatorial spaces. To achieve a scalable objective-based experimental design, this article proposes a graph-based MOCU-based Bayesian optimization framework. The correlations among samples in the large design space are accounted for using a graph-based Gaussian process, and an efficient closed-form sequential selection is achieved through the well-known expected improvement policy. The proposed framework’s performance is assessed through the structural intervention in gene regulatory networks, aiming to make the network away from the states associated with cancer.},
  archive      = {J_TNNLS},
  author       = {Mahdi Imani and Seyede Fatemeh Ghoreishi},
  doi          = {10.1109/TNNLS.2021.3071958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5913-5925},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph-based bayesian optimization for large-scale objective-based experimental design},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning with label proportions by incorporating unmarked
data. <em>TNNLS</em>, <em>33</em>(10), 5898–5912. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with label proportions (LLP) deals with the problem that the training data are provided as bags, where the label proportions of training bags rather than the labels of individual training instances are accessible. Existing LLP studies assume that the label proportions of all training bags are accessible. However, in many applications, it is time-consuming to mark all training bags with label proportions, which leads to the problem of learning with both marked and unmarked bags, namely, semisupervised LLP (SLLP). In this work, we propose semisupervised proportional support vector machine (SS- $\propto $ SVM), which extends the proportional SVM ( $\propto $ SVM) model to its semisupervised version. To the best of our knowledge, SS- $\propto $ SVM is the first attempt to cope with the SLLP problem. Two realizations, alter-SS- $\propto $ SVM and conv-SS- $\propto $ SVM, which are based on alternating optimization and convex relaxation, respectively, are developed to solve the proposed SS- $\propto $ SVM model. Moreover, we design a cutting plane (CP) method to optimize conv-SS- $\propto $ SVM with a guaranteed convergence rate and present a fast accelerated proximal gradient method to solve the multiple kernel learning subproblem in conv-SS- $\propto $ SVM efficiently. Empirical experiments not only justify the superiority of SS- $\propto $ SVM over its supervised counterpart in classification accuracy but also demonstrate the high competitive computational efficiency of the CP optimization of conv-SS- $\propto $ SVM.},
  archive      = {J_TNNLS},
  author       = {Jing Chai and Ivor W. Tsang},
  doi          = {10.1109/TNNLS.2021.3071924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5898-5912},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning with label proportions by incorporating unmarked data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explicit metric-based multiconcept multi-instance learning
with triplet and superbag. <em>TNNLS</em>, <em>33</em>(10), 5888–5897.
(<a href="https://doi.org/10.1109/TNNLS.2021.3071814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-instance learning (MIL) has garnered considerable attention in recent years due to its favorable performance in various scenarios. Nonetheless, most previous studies have implicitly expressed the correlation between instances and bags. Moreover, the importance of negative instances has been largely overlooked. Hence, we seek to present an explicit and intuitively understandable method that can compensate for these deficiencies. In this article, we creatively introduce a metric-based multiconcept MIL approach based on two aspects. First, the triplet-based bag embedding method identifies instance categories and builds attention weights for every instance explicitly. Accordingly, bag embedding is accomplished under the limitation of weak supervision. Second, the developed instance correlation metric approach in the superbag considers the multiconcept issue to boost the model generalization performance. We have designed a rich variety of experiments to demonstrate the performance of our algorithm. The artificial data experiment reveals the interpretability of the proposed network. The results of the comparison experiment confirm that our method shows favorable performance in multiple tasks. Finally, we illustrate the motivation of the presented method by the ablation experiments.},
  archive      = {J_TNNLS},
  author       = {Ziqiu Chi and Zhe Wang and Wenli Du},
  doi          = {10.1109/TNNLS.2021.3071814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5888-5897},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explicit metric-based multiconcept multi-instance learning with triplet and superbag},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning control of robotic knee with
human-in-the-loop by flexible policy iteration. <em>TNNLS</em>,
<em>33</em>(10), 5873–5887. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are motivated by the real challenges presented in a human–robot system to develop new designs that are efficient at data level and with performance guarantees, such as stability and optimality at system level. Existing approximate/adaptive dynamic programming (ADP) results that consider system performance theoretically are not readily providing practically useful learning control algorithms for this problem, and reinforcement learning (RL) algorithms that address the issue of data efficiency usually do not have performance guarantees for the controlled system. This study fills these important voids by introducing innovative features to the policy iteration algorithm. We introduce flexible policy iteration (FPI), which can flexibly and organically integrate experience replay and supplemental values from prior experience into the RL controller. We show system-level performances, including convergence of the approximate value function, (sub)optimality of the solution, and stability of the system. We demonstrate the effectiveness of the FPI via realistic simulations of the human–robot system. It is noted that the problem we face in this study may be difficult to address by design methods based on classical control theory as it is nearly impossible to obtain a customized mathematical model of a human–robot system either online or offline. The results we have obtained also indicate the great potential of RL control to solving realistic and challenging problems with high-dimensional control inputs.},
  archive      = {J_TNNLS},
  author       = {Xiang Gao and Jennie Si and Yue Wen and Minhan Li and He Huang},
  doi          = {10.1109/TNNLS.2021.3071727},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5873-5887},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning control of robotic knee with human-in-the-loop by flexible policy iteration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive data structure regularized multiclass
discriminative feature selection. <em>TNNLS</em>, <em>33</em>(10),
5859–5872. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS), which aims to identify the most informative subset of input features, is an important approach to dimensionality reduction. In this article, a novel FS framework is proposed for both unsupervised and semisupervised scenarios. To make efficient use of data distribution to evaluate features, the framework combines data structure learning (as referred to as data distribution modeling) and FS in a unified formulation such that the data structure learning improves the results of FS and vice versa. Moreover, two types of data structures, namely the soft and hard data structures, are learned and used in the proposed FS framework. The soft data structure refers to the pairwise weights among data samples, and the hard data structure refers to the estimated labels obtained from clustering or semisupervised classification. Both of these data structures are naturally formulated as regularization terms in the proposed framework. In the optimization process, the soft and hard data structures are learned from data represented by the selected features, and then, the most informative features are reselected by referring to the data structures. In this way, the framework uses the interactions between data structure learning and FS to select the most discriminative and informative features. Following the proposed framework, a new semisupervised FS (SSFS) method is derived and studied in depth. Experiments on real-world data sets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Mingyu Fan and Xiaoqin Zhang and Jie Hu and Nannan Gu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3071603},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5859-5872},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive data structure regularized multiclass discriminative feature selection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Whitening-net: A generalized network to diagnose the faults
among different machines and conditions. <em>TNNLS</em>,
<em>33</em>(10), 5845–5858. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent bearing diagnostic methods are developing rapidly, but they are difficult to implement due to the lack of real industrial data. A feasible way to deal with this problem is to train a network through laboratory data to mine the causality of bearing faults. This means that the constructed network can handle domain deviations caused by the change of machines, working conditions, noise, and so on which is, however, not a simple task. In response to this problem, a new domain generalization framework—Whitening-Net—was proposed in this article. This framework first defined the homologous compound domain signal as the data basis. Subsequently, the causal loss was proposed to impose regularization constraints on the network, which enhances the network’s ability to mine causality. To avoid domain-specific information from interfering with causal mining, a whitening structure was proposed to whiten the domain, prompting the network to pay more attention to the causality of the signal rather than the domain noise. The results of diagnosis and interpretation proved the ability of Whitening-Net in mining causal mechanisms, which shows that the proposed network can generalize to different machines, even if the tested working conditions and bearing types are completely different from the training domains.},
  archive      = {J_TNNLS},
  author       = {Jie Li and Yu Wang and Yanyang Zi and Zhijie Zhang},
  doi          = {10.1109/TNNLS.2021.3071564},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5845-5858},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Whitening-net: A generalized network to diagnose the faults among different machines and conditions},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized event-driven constrained control using
adaptive critic designs. <em>TNNLS</em>, <em>33</em>(10), 5830–5844. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the decentralized event-driven control problem of nonlinear dynamical systems with mismatched interconnections and asymmetric input constraints. To begin with, by introducing a discounted cost function for each auxiliary subsystem, we transform the decentralized event-driven constrained control problem into a group of nonlinear $H_{2}$ -constrained optimal control problems. Then, we develop the event-driven Hamilton–Jacobi–Bellman equations (ED-HJBEs), which arise in the nonlinear $H_{2}$ -constrained optimal control problems. Meanwhile, we demonstrate that all the solutions of the ED-HJBEs together keep the overall system stable in the sense of uniform ultimate boundedness (UUB). To solve the ED-HJBEs, we build a critic-only architecture under the framework of adaptive critic designs. The architecture only employs critic neural networks and updates their weight vectors via the gradient descent method. After that, based on the Lyapunov approach, we prove that the UUB stability of all signals in the closed-loop auxiliary subsystems is assured. Finally, simulations of an illustrated nonlinear interconnected plant are provided to validate the present designs.},
  archive      = {J_TNNLS},
  author       = {Xiong Yang and Yuanheng Zhu and Na Dong and Qinglai Wei},
  doi          = {10.1109/TNNLS.2021.3071548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5830-5844},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decentralized event-driven constrained control using adaptive critic designs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised domain adaptation for aspect extraction
via multilevel interaction transfer. <em>TNNLS</em>, <em>33</em>(10),
5818–5829. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained aspect term extraction is an essential subtask in aspect-based opinion analysis. It aims to identify the aspect terms (also known as opinion targets) of a product or service in each sentence. To learn a good aspect extraction model, an expensive annotation process is usually involved to acquire sufficient token-level labels for each domain, which is not realistic. To address this limitation, some previous works propose domain adaptation strategies to transfer knowledge from a sufficiently labeled source domain to unlabeled target domains. However, due to both the difficulty of fine-grained prediction problems and the large domain gap between different domains, the performance is still far from satisfactory. In this work, we conduct a pioneer study on leveraging sentence-level aspect category labels that can be usually available in commercial services, such as review sites or social media to promote token-level transfer for extraction purpose. Specifically, the aspect category information can be used to construct pivot knowledge for transfer with the assumption that the interactions between the sentence-level aspect category and the token-level aspect terms are invariant across domains. To this end, we propose a novel multilevel reconstruction mechanism that aligns both the fine- and coarse-grained information in multiple levels of abstractions. Comprehensive experiments over several benchmark data sets clearly demonstrate that our approach can fully utilize the sentence-level aspect category labels to improve cross-domain aspect term extraction with a large performance gain.},
  archive      = {J_TNNLS},
  author       = {Tao Liang and Wenya Wang and Fengmao Lv},
  doi          = {10.1109/TNNLS.2021.3071474},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5818-5829},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weakly supervised domain adaptation for aspect extraction via multilevel interaction transfer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stability and synchronization of nonautonomous
reaction–diffusion neural networks with general time-varying delays.
<em>TNNLS</em>, <em>33</em>(10), 5804–5817. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the stability and synchronization of nonautonomous reaction–diffusion neural networks with general time-varying delays. Compared with the existing works concerning reaction–diffusion neural networks, the main innovation of this article is that the network coefficients are time-varying, and the delays are general (which means that fewer constraints are posed on delays; for example, the commonly used conditions of differentiability and boundedness are no longer needed). By Green’s formula and some analytical techniques, some easily checkable criteria on stability and synchronization for the underlying neural networks are established. These obtained results not only improve some existing ones but also contain some novel results that have not yet been reported. The effectiveness and superiorities of the established criteria are verified by three numerical examples.},
  archive      = {J_TNNLS},
  author       = {Hao Zhang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3071404},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5804-5817},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability and synchronization of nonautonomous Reaction–Diffusion neural networks with general time-varying delays},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep mixture generative autoencoders. <em>TNNLS</em>,
<em>33</em>(10), 5789–5803. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs) are one of the most popular unsupervised generative models that rely on learning latent representations of data. In this article, we extend the classical concept of Gaussian mixtures into the deep variational framework by proposing a mixture of VAEs (MVAE). Each component in the MVAE model is implemented by a variational encoder and has an associated subdecoder. The separation between the latent spaces modeled by different encoders is enforced using the $d$ -variable Hilbert–Schmidt independence criterion (dHSIC). Each component would capture different data variational features. We also propose a mechanism for finding the appropriate number of VAE components for a given task, leading to an optimal architecture. The differentiable categorical Gumbel-softmax distribution is used in order to generate dropout masking parameters within the end-to-end backpropagation training framework. Extensive experiments show that the proposed MVAE model can learn a rich latent data representation and is able to discover additional underlying data representation factors.},
  archive      = {J_TNNLS},
  author       = {Fei Ye and Adrian G. Bors},
  doi          = {10.1109/TNNLS.2021.3071401},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5789-5803},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep mixture generative autoencoders},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An l1-and-l2-norm-oriented latent factor model for
recommender systems. <em>TNNLS</em>, <em>33</em>(10), 5775–5788. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recommender system (RS) is highly efficient in filtering people’s desired information from high-dimensional and sparse (HiDS) data. To date, a latent factor (LF)-based approach becomes highly popular when implementing a RS. However, current LF models mostly adopt single distance-oriented Loss like an $L_{2}$ norm-oriented one, which ignores target data’s characteristics described by other metrics like an $L_{1}$ norm-oriented one. To investigate this issue, this article proposes an $L_{1}$ -and- $L_{2}$ -norm-oriented LF ( $\text{L}^{3}\text{F}$ ) model. It adopts twofold ideas: 1) aggregating $L_{1}$ norm’s robustness and $L_{2}$ norm’s stability to form its Loss and 2) adaptively adjusting weights of $L_{1}$ and $L_{2}$ norms in its Loss. By doing so, it achieves fine aggregation effects with $L_{1}$ norm-oriented Loss ’s robustness and $L_{2}$ norm-oriented Loss ’s stability to precisely describe HiDS data with outliers. Experimental results on nine HiDS datasets generated by real systems show that an $\text{L}^{3}\text{F}$ model significantly outperforms state-of-the-art models in prediction accuracy for missing data of an HiDS dataset. Its computational efficiency is also comparable with the most efficient LF models. Hence, it has good potential for addressing HiDS data from real applications.},
  archive      = {J_TNNLS},
  author       = {Di Wu and Mingsheng Shang and Xin Luo and Zidong Wang},
  doi          = {10.1109/TNNLS.2021.3071392},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5775-5788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An l1-and-l2-norm-oriented latent factor model for recommender systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). On the rates of convergence from surrogate risk minimizers
to the bayes optimal classifier. <em>TNNLS</em>, <em>33</em>(10),
5766–5774. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classification, the use of 0–1 loss is preferable since the minimizer of 0–1 risk leads to the Bayes optimal classifier. However, due to the nonconvexity of 0–1 loss, this optimization problem is NP-hard. Therefore, many convex surrogate loss functions have been adopted. Previous works have shown that if a Bayes-risk consistent loss function is used as a surrogate, the minimizer of the empirical surrogate risk can achieve the Bayes optimal classifier as the sample size tends to infinity. Nevertheless, the comparison of convergence rates of minimizers of different empirical surrogate risks to the Bayes optimal classifier has rarely been studied. Which characterization of the surrogate loss determines its convergence rate to the Bayes optimal classifier? Can we modify the loss function to achieve a faster convergence rate? In this article, we study the convergence rates of empirical surrogate minimizers to the Bayes optimal classifier. Specifically, we introduce the notions of consistency intensity and conductivity to characterize a surrogate loss function and exploit this notion to obtain the rate of convergence from an empirical surrogate risk minimizer to the Bayes optimal classifier, enabling fair comparisons of the excess risks of different surrogate risk minimizers. The main result of this article has practical implications including: 1) showing that hinge loss (SVM) is superior to logistic loss (Logistic regression) and exponential loss (Adaboost) in the sense that its empirical minimizer converges faster to the Bayes optimal classifier and 2) guiding the design of new loss functions to speed up the convergence rate to the Bayes optimal classifier with a data-dependent loss correction method inspired by our theorems.},
  archive      = {J_TNNLS},
  author       = {Jingwei Zhang and Tongliang Liu and Dacheng Tao},
  doi          = {10.1109/TNNLS.2021.3071370},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5766-5774},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the rates of convergence from surrogate risk minimizers to the bayes optimal classifier},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep feature aggregation framework driven by graph
convolutional network for scene classification in remote sensing.
<em>TNNLS</em>, <em>33</em>(10), 5751–5765. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene classification of high spatial resolution (HSR) images can provide data support for many practical applications, such as land planning and utilization, and it has been a crucial research topic in the remote sensing (RS) community. Recently, deep learning methods driven by massive data show the impressive ability of feature learning in the field of HSR scene classification, especially convolutional neural networks (CNNs). Although traditional CNNs achieve good classification results, it is difficult for them to effectively capture potential context relationships. The graphs have powerful capacity to represent the relevance of data, and graph-based deep learning methods can spontaneously learn intrinsic attributes contained in RS images. Inspired by the abovementioned facts, we develop a deep feature aggregation framework driven by graph convolutional network (DFAGCN) for the HSR scene classification. First, the off-the-shelf CNN pretrained on ImageNet is employed to obtain multilayer features. Second, a graph convolutional network-based model is introduced to effectively reveal patch-to-patch correlations of convolutional feature maps, and more refined features can be harvested. Finally, a weighted concatenation method is adopted to integrate multiple features (i.e., multilayer convolutional features and fully connected features) by introducing three weighting coefficients, and then a linear classifier is employed to predict semantic classes of query images. Experimental results performed on the UCM, AID, RSSCN7, and NWPU-RESISC45 data sets demonstrate that the proposed DFAGCN framework obtains more competitive performance than some state-of-the-art methods of scene classification in terms of OAs.},
  archive      = {J_TNNLS},
  author       = {Kejie Xu and Hong Huang and Peifang Deng and Yuan Li},
  doi          = {10.1109/TNNLS.2021.3071369},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5751-5765},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep feature aggregation framework driven by graph convolutional network for scene classification in remote sensing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-based prediction of network controllability
robustness. <em>TNNLS</em>, <em>33</em>(10), 5739–5750. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network controllability robustness (CR) reflects how well a networked system can maintain its controllability against destructive attacks. Its measure is quantified by a sequence of values that record the remaining controllability of the network after a sequence of node-removal or edge-removal attacks. Traditionally, the CR is determined by attack simulations, which is computationally time-consuming or even infeasible. In this article, an improved method for predicting the network CR is developed based on machine learning using a group of convolutional neural networks (CNNs). In this scheme, a number of training data generated by simulations are used to train the group of CNNs for classification and prediction, respectively. Extensive experimental studies are carried out, which demonstrate that 1) the proposed method predicts more precisely than the classical single-CNN predictor; 2) the proposed CNN-based predictor provides a better predictive measure than the traditional spectral measures and network heterogeneity.},
  archive      = {J_TNNLS},
  author       = {Yang Lou and Yaodong He and Lin Wang and Kim Fung Tsang and Guanrong Chen},
  doi          = {10.1109/TNNLS.2021.3071367},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5739-5750},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge-based prediction of network controllability robustness},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning gaussian–bernoulli RBMs using difference of convex
functions optimization. <em>TNNLS</em>, <em>33</em>(10), 5728–5738. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian–Bernoulli restricted Boltzmann machine (GB-RBM) is a useful generative model that captures meaningful features from the given $n$ -dimensional continuous data. The difficulties associated with learning GB-RBM are reported extensively in earlier studies. They indicate that the training of the GB-RBM using the current standard algorithms, namely contrastive divergence (CD) and persistent contrastive divergence (PCD), needs a carefully chosen small learning rate to avoid divergence which, in turn, results in slow learning. In this work, we alleviate such difficulties by showing that the negative log-likelihood for a GB-RBM can be expressed as a difference of convex functions if we keep the variance of the conditional distribution of visible units (given hidden unit states) and the biases of the visible units, constant. Using this, we propose a stochastic difference of convex (DC) functions programming (S-DCP) algorithm for learning the GB-RBM. We present extensive empirical studies on several benchmark data sets to validate the performance of this S-DCP algorithm. It is seen that S-DCP is better than the CD and PCD algorithms in terms of speed of learning and the quality of the generative model learned.},
  archive      = {J_TNNLS},
  author       = {Vidyadhar Upadhya and P. S. Sastry},
  doi          = {10.1109/TNNLS.2021.3071358},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5728-5738},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning Gaussian–Bernoulli RBMs using difference of convex functions optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spherical formation tracking control of nonlinear
second-order agents with adaptive neural flow estimate. <em>TNNLS</em>,
<em>33</em>(10), 5716–5727. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the spherical formation tracking control problem of nonlinear second-order vehicles moving in flowfields under both undirected networks and directed, strongly connected networks. Different from the previous adaptive estimate of the time-invariant parameters of flowfields, the flowfields under our consideration are spatial and absolutely unknown dynamics. Adaptive neural networks (ANNs) with the novel cooperative adaptive algorithms are proposed to approximate the flowfield acting on the channel of each vehicle’s velocity (i.e., the mismatched flowfield) and the flowfield pushing the acceleration (i.e., the matched flowfield), respectively. For the purpose of avoiding the complex derivation derived from backstepping, the novel first-order filters are generated by dynamic surface based on barrier functions and relative positions of neighbors. The proposed control algorithms and adaptive upgrade law are fully distributed without using any global information of the graph. The uniform boundedness is analyzed in the Lyapunov sense. Simulation results are given to verify the theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Yang-Yang Chen and Rong Huang and Yanteng Ge and Ya Zhang},
  doi          = {10.1109/TNNLS.2021.3071317},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5716-5727},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spherical formation tracking control of nonlinear second-order agents with adaptive neural flow estimate},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction with unpredictable feature evolution.
<em>TNNLS</em>, <em>33</em>(10), 5706–5715. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with feature evolution studies the scenario where the features of the data streams can evolve, i.e., old features vanish and new features emerge. Its goal is to keep the model always performing well even when the features happen to evolve. To tackle this problem, canonical methods assume that the old features will vanish simultaneously and the new features themselves will emerge simultaneously as well. They also assume that there is an overlapping period where old and new features both exist when the feature space starts to change. However, in reality, the feature evolution could be unpredictable, which means that the features can vanish or emerge arbitrarily, causing the overlapping period incomplete. In this article, we propose a novel paradigm: prediction with unpredictable feature evolution (PUFE) where the feature evolution is unpredictable. To address this problem, we fill the incomplete overlapping period and formulate it as a new matrix completion problem. We give a theoretical bound on the least number of observed entries to make the overlapping period intact. With this intact overlapping period, we leverage an ensemble method to take the advantage of both the old and new feature spaces without manually deciding which base models should be incorporated. Theoretical and experimental results validate that our method can always follow the best base models and, thus, realize the goal of learning with feature evolution.},
  archive      = {J_TNNLS},
  author       = {Bo-Jian Hou and Lijun Zhang and Zhi-Hua Zhou},
  doi          = {10.1109/TNNLS.2021.3071311},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5706-5715},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prediction with unpredictable feature evolution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven designs of fault detection systems via neural
network-aided learning. <em>TNNLS</em>, <em>33</em>(10), 5694–5705. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the aid of neural networks, this article develops two data-driven designs of fault detection (FD) for dynamic systems. The first neural network is constructed for generating residual signals in the so-called finite impulse response (FIR) filter-based form, and the second one is designed for recursively generating residual signals. By theoretical analysis, we show that two proposed neural networks via self-organizing learning can find their optimal architectures, respectively, corresponding to FIR filter and recursive observer for FD purposes. Additional contributions of this study lie in that we establish bridges that link model- and neural-network-based methods for detecting faults in dynamic systems. An experiment on a three-tank system is adopted to illustrate the effectiveness of two proposed neural network-aided FD algorithms.},
  archive      = {J_TNNLS},
  author       = {Hongtian Chen and Zheng Chai and Oguzhan Dogru and Bin Jiang and Biao Huang},
  doi          = {10.1109/TNNLS.2021.3071292},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5694-5705},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven designs of fault detection systems via neural network-aided learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A decoder-free variational deep embedding for unsupervised
clustering. <em>TNNLS</em>, <em>33</em>(10), 5681–5693. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep clustering frameworks, autoencoder (AE)- or variational AE-based clustering approaches are the most popular and competitive ones that encourage the model to obtain suitable representations and avoid the tendency for degenerate solutions simultaneously. However, for the clustering task, the decoder for reconstructing the original input is usually useless when the model is finished training. The encoder–decoder architecture limits the depth of the encoder so that the learning capacity is reduced severely. In this article, we propose a decoder-free variational deep embedding for unsupervised clustering (DFVC). It is well known that minimizing reconstruction error amounts to maximizing a lower bound on the mutual information (MI) between the input and its representation. That provides a theoretical guarantee for us to discard the bloated decoder. Inspired by contrastive self-supervised learning, we can directly calculate or estimate the MI of the continuous variables. Specifically, we investigate unsupervised representation learning by simultaneously considering the MI estimation of continuous representations and the MI computation of categorical representations. By introducing the data augmentation technique, we incorporate the original input, the augmented input, and their high-level representations into the MI estimation framework to learn more discriminative representations. Instead of matching to a simple standard normal distribution adversarially, we use end-to-end learning to constrain the latent space to be cluster-friendly by applying the Gaussian mixture distribution as the prior. Extensive experiments on challenging data sets show that our model achieves higher performance over a wide range of state-of-the-art clustering approaches.},
  archive      = {J_TNNLS},
  author       = {Qiang Ji and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TNNLS.2021.3071275},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5681-5693},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A decoder-free variational deep embedding for unsupervised clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning deep context-sensitive decomposition for low-light
image enhancement. <em>TNNLS</em>, <em>33</em>(10), 5666–5680. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the quality of low-light (LOL) images plays a very important role in many image processing and multimedia applications. In recent years, a variety of deep learning techniques have been developed to address this challenging task. A typical framework is to simultaneously estimate the illumination and reflectance, but they disregard the scene-level contextual information encapsulated in feature spaces, causing many unfavorable outcomes, e.g., details loss, color unsaturation, and artifacts. To address these issues, we develop a new context-sensitive decomposition network (CSDNet) architecture to exploit the scene-level contextual dependencies on spatial scales. More concretely, we build a two-stream estimation mechanism including reflectance and illumination estimation network. We design a novel context-sensitive decomposition connection to bridge the two-stream mechanism by incorporating the physical principle. The spatially varying illumination guidance is further constructed for achieving the edge-aware smoothness property of the illumination component. According to different training patterns, we construct CSDNet (paired supervision) and context-sensitive decomposition generative adversarial network (CSDGAN) (unpaired supervision) to fully evaluate our designed architecture. We test our method on seven testing benchmarks [including massachusetts institute of technology (MIT)-Adobe FiveK, LOL, ExDark, and naturalness preserved enhancement (NPE)] to conduct plenty of analytical and evaluated experiments. Thanks to our designed context-sensitive decomposition connection, we successfully realized excellent enhanced results (with sufficient details, vivid colors, and few noises), which fully indicates our superiority against existing state-of-the-art approaches. Finally, considering the practical needs for high efficiency, we develop a lightweight CSDNet (named LiteCSDNet) by reducing the number of channels. Furthermore, by sharing an encoder for these two components, we obtain a more lightweight version (SLiteCSDNet for short). SLiteCSDNet just contains 0.0301M parameters but achieves the almost same performance as CSDNet. Code is available at https://github.com/KarelZhang/CSDNet-CSDGAN .},
  archive      = {J_TNNLS},
  author       = {Long Ma and Risheng Liu and Jiaao Zhang and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TNNLS.2021.3071245},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5666-5680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning deep context-sensitive decomposition for low-light image enhancement},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified analysis on the global dissipativity and stability
of fractional-order multidimension-valued memristive neural networks
with time delay. <em>TNNLS</em>, <em>33</em>(10), 5656–5665. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unified criteria are analyzed on the global dissipativity and stability for the delayed fractional-order systems of multidimension-valued memristive neural networks (FSMVMNNs) in this article. First, based on the comprehensive knowledge about multidimensional algebra, fractional derivatives, and nonsmooth analysis, we establish the unified model for the studied FSMVMNNs in order to propose a more uniform method to analyze the dynamic behaviors of multidimensional neural networks. Then, by mainly applying the Lyapunov method, employing several new lemmas, and solving some mathematical difficulties, without any separation, we acquire the unified and concise criteria. The derived criteria have many advantages in a smaller calculation, lower conservatism, more diversity, and higher flexibility. Finally, we provide two numerical examples to express the availability and improvements of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jianying Xiao and Shouming Zhong and Shiping Wen},
  doi          = {10.1109/TNNLS.2021.3071183},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5656-5665},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unified analysis on the global dissipativity and stability of fractional-order multidimension-valued memristive neural networks with time delay},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain adaptation preconceived hashing for unconstrained
visual retrieval. <em>TNNLS</em>, <em>33</em>(10), 5641–5655. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to hash has been widely applied for image retrieval due to the low storage and high retrieval efficiency. Existing hashing methods assume that the distributions of the retrieval pool (i.e., the data sets being retrieved) and the query data are similar, which, however, cannot truly reflect the real-world condition due to the unconstrained visual cues, such as illumination, pose, background, and so on. Due to the large distribution gap between the retrieval pool and the query set, the performances of traditional hashing methods are seriously degraded. Therefore, we first propose a new efficient but transferable hashing model for unconstrained cross-domain visual retrieval, in which the retrieval pool and the query sample are drawn from different but semantic relevant domains. Specifically, we propose a simple yet effective unsupervised hashing method, domain adaptation preconceived hashing (DAPH), toward learning domain-invariant hashing representation. Three merits of DAPH are observed: 1) to the best of our knowledge, we first propose unconstrained visual retrieval by introducing DA into hashing for learning transferable hashing codes; 2) a domain-invariant feature transformation with marginal discrepancy distance minimization and feature reconstruction constraint is learned, such that the hashing code is not only domain adaptive but content preserved; and 3) a DA preconceived quantization loss is proposed, which further guarantees the discrimination of the learned hashing code for sample retrieval. Extensive experiments on various benchmark data sets verify that our DAPH outperforms many state-of-the-art hashing methods toward unconstrained (unrestricted) instance retrieval in both single- and cross-domain scenarios.},
  archive      = {J_TNNLS},
  author       = {Fuxiang Huang and Lei Zhang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2021.3071127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5641-5655},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain adaptation preconceived hashing for unconstrained visual retrieval},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Class-imbalanced deep learning via a class-balanced
ensemble. <em>TNNLS</em>, <em>33</em>(10), 5626–5640. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a prevalent phenomenon in various real-world applications and it presents significant challenges to model learning, including deep learning. In this work, we embed ensemble learning into the deep convolutional neural networks (CNNs) to tackle the class-imbalanced learning problem. An ensemble of auxiliary classifiers branching out from various hidden layers of a CNN is trained together with the CNN in an end-to-end manner. To that end, we designed a new loss function that can rectify the bias toward the majority classes by forcing the CNN’s hidden layers and its associated auxiliary classifiers to focus on the samples that have been misclassified by previous layers, thus enabling subsequent layers to develop diverse behavior and fix the errors of previous layers in a batch-wise manner. A unique feature of the new method is that the ensemble of auxiliary classifiers can work together with the main CNN to form a more powerful combined classifier, or can be removed after finished training the CNN and thus only acting the role of assisting class imbalance learning of the CNN to enhance the neural network’s capability in dealing with class-imbalanced data. Comprehensive experiments are conducted on four benchmark data sets of increasing complexity (CIFAR-10, CIFAR-100, iNaturalist, and CelebA) and the results demonstrate significant performance improvements over the state-of-the-art deep imbalance learning methods.},
  archive      = {J_TNNLS},
  author       = {Zhi Chen and Jiang Duan and Li Kang and Guoping Qiu},
  doi          = {10.1109/TNNLS.2021.3071122},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5626-5640},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Class-imbalanced deep learning via a class-balanced ensemble},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Heterogeneous face interpretable disentangled representation
for joint face recognition and synthesis. <em>TNNLS</em>,
<em>33</em>(10), 5611–5625. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous faces are acquired with different sensors, which are closer to real-world scenarios and play an important role in the biometric security field. However, heterogeneous face analysis is still a challenging problem due to the large discrepancy between different modalities. Recent works either focus on designing a novel loss function or network architecture to directly extract modality-invariant features or synthesizing the same modality faces initially to decrease the modality gap. Yet, the former always lacks explicit interpretability, and the latter strategy inherently brings in synthesis bias. In this article, we explore to learn the plain interpretable representation for complex heterogeneous faces and simultaneously perform face recognition and synthesis tasks. We propose the heterogeneous face interpretable disentangled representation (HFIDR) that could explicitly interpret dimensions of face representation rather than simple mapping. Benefited from the interpretable structure, we further could extract latent identity information for cross-modality recognition and convert the modality factor to synthesize cross-modality faces. Moreover, we propose a multimodality heterogeneous face interpretable disentangled representation (M-HFIDR) to extend the basic approach suitable for the multimodality face recognition and synthesis. To evaluate the ability of generalization, we construct a novel large-scale face sketch data set. Experimental results on multiple heterogeneous face databases demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Decheng Liu and Xinbo Gao and Chunlei Peng and Nannan Wang and Jie Li},
  doi          = {10.1109/TNNLS.2021.3071119},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5611-5625},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Heterogeneous face interpretable disentangled representation for joint face recognition and synthesis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale multiagent system tracking control using mean
field games. <em>TNNLS</em>, <em>33</em>(10), 5602–5610. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the tracking control problem with a large-scale group of agents. Unlike traditional control techniques used in multiagent systems (MASs), a new type of intelligent design is needed to handle the intractable “Curse of Dimensionality” caused by the extremely large number of agents. To address this challenge, the mean field game (MFG) theory has been embedded into reinforcement learning to advance intelligent tracking control with large-scale MAS. Specifically, MFG-based control can calculate the optimal strategy based on one unified fix-dimension probability density function (pdf) instead of high-dimensional large-scale MAS information collected from individual agents. Moreover, the approximate dynamic programming technique is adopted to generate a new type of MFG-based algorithm. Each agent has three neural networks (NNs) to approximate the solution of the mean field type control. In addition to the algorithm development, the performance of the NNs is also analyzed using the Lyapunov method. Finally, the linear and nonlinear tracking control simulations are given to evaluate the algorithm’s performance.},
  archive      = {J_TNNLS},
  author       = {Zejian Zhou and Hao Xu},
  doi          = {10.1109/TNNLS.2021.3071109},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5602-5610},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large-scale multiagent system tracking control using mean field games},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum a posteriori approximation of hidden markov models
for proportional sequential data modeling with simultaneous feature
selection. <em>TNNLS</em>, <em>33</em>(10), 5590–5601. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the pillar generative machine learning approaches in time series data study and analysis is the hidden Markov model (HMM). Early research focused on the speech recognition application of the model with later expansion into numerous fields, including video classification, action recognition, and text translation. The recently developed generalized Dirichlet HMMs have proven efficient in proportional sequential data modeling. As such, we focus on investigating a maximum a posteriori (MAP) framework for the inference of its parameters. The proposed approach differs from the widely deployed Baum–Welch through the placement of priors that regularizes the estimation process. A feature selection paradigm is also integrated simultaneously in the algorithm. For validation, we apply our proposed approach in the classification of dynamic textures and the recognition of infrared actions.},
  archive      = {J_TNNLS},
  author       = {Samr Ali and Nizar Bouguila},
  doi          = {10.1109/TNNLS.2021.3071083},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5590-5601},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum a posteriori approximation of hidden markov models for proportional sequential data modeling with simultaneous feature selection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus-based cooperative algorithms for training over
distributed data sets using stochastic gradients. <em>TNNLS</em>,
<em>33</em>(10), 5579–5589. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, distributed algorithms are proposed for training a group of neural networks with private data sets. Stochastic gradients are utilized in order to eliminate the requirement for true gradients. To obtain a universal model of the distributed neural networks trained using local data sets only, consensus tools are introduced to derive the model toward the optimum. Most of the existing works employ diminishing learning rates, which are often slow and impracticable for online learning, while constant learning rates are studied in some recent works, but the principle for choosing the rates is not well established. In this article, constant learning rates are adopted to empower the proposed algorithms with tracking ability. Under mild conditions, the convergence of the proposed algorithms is established by exploring the error dynamics of the connected agents, which provides an upper bound for selecting the constant learning rates. Performances of the proposed algorithms are analyzed with and without gradient noises, in the sense of mean square error (MSE). It is proved that the MSE converges with bounded errors determined by the gradient noises, and the MSE converges to zero if the gradient noises are absent. Simulation results are provided to validate the effectiveness of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhongguo Li and Bo Liu and Zhengtao Ding},
  doi          = {10.1109/TNNLS.2021.3071058},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5579-5589},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus-based cooperative algorithms for training over distributed data sets using stochastic gradients},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). A novel formulation of trace ratio linear discriminant
analysis. <em>TNNLS</em>, <em>33</em>(10), 5568–5578. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linear discriminant analysis (LDA) method needs to be transformed into another form to acquire an approximate closed-form solution, which could lead to the error between the approximate solution and the true value. Furthermore, the sensitivity of dimensionality reduction (DR) methods to subspace dimensionality cannot be eliminated. In this article, a new formulation of trace ratio LDA (TRLDA) is proposed, which has an optimal solution of LDA. When solving the projection matrix, the TRLDA method given by us is transformed into a quadratic problem with regard to the Stiefel manifold. In addition, we propose a new trace difference problem named optimal dimensionality linear discriminant analysis (ODLDA) to determine the optimal subspace dimension. The nonmonotonicity of ODLDA guarantees the existence of optimal subspace dimensionality. Both the two approaches have achieved efficient DR on several data sets.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Lin Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3071030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5568-5578},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel formulation of trace ratio linear discriminant analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multipixel anomaly detection with unknown patterns for
hyperspectral imagery. <em>TNNLS</em>, <em>33</em>(10), 5557–5567. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, anomaly detection is considered for hyperspectral imagery in the Gaussian background with an unknown covariance matrix. The anomaly to be detected occupies multiple pixels with an unknown pattern. Two adaptive detectors are proposed based on the generalized likelihood ratio test design procedure and ad hoc modification of it. Surprisingly, it turns out that the two proposed detectors are equivalent. Analytical expressions are derived for the probability of false alarm of the proposed detector, which exhibits a constant false alarm rate against the noise covariance matrix. Numerical examples using simulated data reveal how some system parameters (e.g., the background data size and pixel number) affect the performance of the proposed detector. Experiments are conducted on five real hyperspectral data sets, demonstrating that the proposed detector achieves better detection performance than its counterparts.},
  archive      = {J_TNNLS},
  author       = {Jun Liu and Zengfu Hou and Wei Li and Ran Tao and Danilo Orlando and Hongbin Li},
  doi          = {10.1109/TNNLS.2021.3071026},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5557-5567},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multipixel anomaly detection with unknown patterns for hyperspectral imagery},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved results on fixed-/preassigned-time synchronization
for memristive complex-valued neural networks. <em>TNNLS</em>,
<em>33</em>(10), 5542–5556. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns the problems of synchronization in a fixed time or prespecified time for memristive complex-valued neural networks (MCVNNs), in which the state variables, activation functions, rates of neuron self-inhibition, neural connection memristive weights, and external inputs are all assumed to be complex-valued. First, the more comprehensive fixed-time stability theorem and more accurate estimations on settling time (ST) are systematically established by using the comparison principle. Second, by introducing different norms of complex numbers instead of decomposing the complex-valued system into real and imaginary parts, we successfully design several simpler discontinuous controllers to acquire much improved fixed-time synchronization (FXTS) results. Third, based on similar mathematical derivations, the preassigned-time synchronization (PATS) conditions are explored by newly developed new control strategies, in which ST can be prespecified and is independent of initial values and any parameters of neural networks and controllers. Finally, numerical simulations are provided to illustrate the effectiveness and superiority of the improved synchronization methodology.},
  archive      = {J_TNNLS},
  author       = {Qintao Gan and Liangchen Li and Jing Yang and Yan Qin and Mingqiang Meng},
  doi          = {10.1109/TNNLS.2021.3070966},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5542-5556},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved results on fixed-/Preassigned-time synchronization for memristive complex-valued neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and analysis of data-driven learning control: An
optimization-based approach. <em>TNNLS</em>, <em>33</em>(10), 5527–5541.
(<a href="https://doi.org/10.1109/TNNLS.2021.3070920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to perform perfect tracking tasks based on measurement data is desirable in the controller design of systems operating repetitively. This motivates this article to seek an optimization-based design and analysis approach for data-driven learning control systems by focusing on iterative learning control (ILC) of repetitive systems with unknown nonlinear time-varying dynamics. It is shown that perfect output tracking can be realized with updating inputs, where no explicit model knowledge but only measured input–output data are leveraged. In particular, adaptive updating strategies are proposed to obtain parameter estimations of nonlinearities. A double-dynamics analysis approach is applied to establish ILC convergence, together with boundedness of input, output, and estimated parameters, which benefits from employing properties of nonnegative matrices. Simulations are implemented to verify the validity of our optimization-based adaptive ILC.},
  archive      = {J_TNNLS},
  author       = {Deyuan Meng and Jingyao Zhang},
  doi          = {10.1109/TNNLS.2021.3070920},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5527-5541},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and analysis of data-driven learning control: An optimization-based approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A high-efficient hybrid physics-informed neural networks
based on convolutional neural network. <em>TNNLS</em>, <em>33</em>(10),
5514–5526. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a hybrid physics-informed neural network (hybrid PINN) for partial differential equations (PDEs). We borrow the idea from the convolutional neural network (CNN) and finite volume methods. Unlike the physics-informed neural network (PINN) and its variations, the method proposed in this article uses an approximation of the differential operator to solve the PDEs instead of automatic differentiation (AD). The approximation is given by a local fitting method, which is the main contribution of this article. As a result, our method has been proved to have a convergent rate. This will also avoid the issue that the neural network gives a bad prediction, which sometimes happened in PINN. To the author’s best knowledge, this is the first work that the machine learning PDE’s solver has a convergent rate, such as in numerical methods. The numerical experiments verify the correctness and efficiency of our algorithm. We also show that our method can be applied in inverse problems and surface PDEs, although without proof.},
  archive      = {J_TNNLS},
  author       = {Zhiwei Fang},
  doi          = {10.1109/TNNLS.2021.3070878},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5514-5526},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A high-efficient hybrid physics-informed neural networks based on convolutional neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based distributed resilient fault-tolerant control
method for heterogeneous MASs under unknown leader dynamic.
<em>TNNLS</em>, <em>33</em>(10), 5504–5513. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the distributed fault-tolerant resilient consensus problem for heterogeneous multiagent systems (MASs) under both physical failures and network denial-of-service (DoS) attacks. Different from the existing consensus results, the dynamic model of the leader is unknown for all followers in this article. To learn this unknown dynamic model under the influence of DoS attacks, a distributed resilient learning algorithm is proposed by using the idea of data-driven. Based on the learned dynamic model of the leader, a distributed resilient estimator is designed for each agent to estimate the states of the leader. Then, a new adaptive fault-tolerant resilient controller is designed to resist the effect of physical failures and network DoS attacks. Moreover, it is shown that the consensus can be achieved with the proposed learning-based fault-tolerant resilient control method. Finally, a simulation example is provided to show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Chao Deng and Xiao-Zheng Jin and Wei-Wei Che and Hai Wang},
  doi          = {10.1109/TNNLS.2021.3070869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5504-5513},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning-based distributed resilient fault-tolerant control method for heterogeneous MASs under unknown leader dynamic},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive observation-based efficient reinforcement learning
for uncertain systems. <em>TNNLS</em>, <em>33</em>(10), 5492–5503. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops an adaptive observation-based efficient reinforcement learning (RL) approach for systems with uncertain drift dynamics. A novel concurrent learning adaptive extended observer (CL-AEO) is first designed to jointly estimate the system state and parameter. This observer has a two-time-scale structure and does not require any additional numerical techniques to calculate the state derivative information. The idea of concurrent learning (CL) is leveraged to use the recorded data, which leads to a relaxed verifiable excitation condition for the convergence of parameter estimation. Based on the estimated state and parameter provided by the CL-AEO, a simulation of experience-based RL scheme is developed to online approximate the optimal control policy. Rigorous theoretical analysis is given to show that the practical convergence of the system state to the origin and the developed policy to the ideal optimal policy can be achieved without the persistence of excitation (PE) condition. Finally, the effectiveness and superiority of the developed methodology are demonstrated via comparative simulations.},
  archive      = {J_TNNLS},
  author       = {Maopeng Ran and Lihua Xie},
  doi          = {10.1109/TNNLS.2021.3070852},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5492-5503},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive observation-based efficient reinforcement learning for uncertain systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Degradation alignment in remaining useful life prediction
using deep cycle-consistent learning. <em>TNNLS</em>, <em>33</em>(10),
5480–5491. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the benefits of reduced maintenance cost and increased operational safety, effective prognostic methods have always been highly demanded in real industries. In the recent years, intelligent data-driven remaining useful life (RUL) prediction approaches have been successfully developed and achieved promising performance. However, the existing methods mostly set hard RUL labels on the training data and pay less attention to the degradation pattern variations of different entities. This article proposes a deep learning-based RUL prediction method. The cycle-consistent learning scheme is proposed to achieve a new representation space, where the data of different entities in similar degradation levels can be well aligned. A first predicting time determination approach is further proposed, which facilitates the following degradation percentage estimation and RUL prediction tasks. The experimental results on a popular degradation data set suggest that the proposed method offers a novel perspective on data-driven prognostic studies and a promising tool for RUL estimations.},
  archive      = {J_TNNLS},
  author       = {Xiang Li and Wei Zhang and Hui Ma and Zhong Luo and Xu Li},
  doi          = {10.1109/TNNLS.2021.3070840},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5480-5491},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Degradation alignment in remaining useful life prediction using deep cycle-consistent learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A separation-based methodology to consensus tracking of
switched high-order nonlinear multiagent systems. <em>TNNLS</em>,
<em>33</em>(10), 5467–5479. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates a reduced-complexity adaptive methodology to consensus tracking for a team of uncertain high-order nonlinear systems with switched (possibly asynchronous) dynamics. It is well known that high-order nonlinear systems are intrinsically challenging as feedback linearization and backstepping methods successfully developed for low-order systems fail to work. Even the adding-one-power-integrator methodology, well explored for the single-agent high-order case, presents some complexity issues and is unsuited for distributed control. At the core of the proposed distributed methodology is a newly proposed definition for separable functions: this definition allows the formulation of a separation-based lemma to handle the high-order terms with reduced complexity in the control design. Complexity is reduced in a twofold sense: the control gain of each virtual control law does not have to be incorporated in the next virtual control law iteratively, thus leading to a simpler expression of the control laws; the power of the virtual and actual control laws increases only proportionally (rather than exponentially) with the order of the systems, dramatically reducing high-gain issues.},
  archive      = {J_TNNLS},
  author       = {Maolong Lv and Wenwu Yu and Jinde Cao and Simone Baldi},
  doi          = {10.1109/TNNLS.2021.3070824},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5467-5479},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A separation-based methodology to consensus tracking of switched high-order nonlinear multiagent systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuation multiple instance learning for weakly and fully
supervised object detection. <em>TNNLS</em>, <em>33</em>(10), 5452–5466.
(<a href="https://doi.org/10.1109/TNNLS.2021.3070801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) is a challenging task that requires simultaneously learning object detectors and estimating object locations under the supervision of image category labels. Many WSOD methods that adopt multiple instance learning (MIL) have nonconvex objective functions and, therefore, are prone to get stuck in local minima (falsely localize object parts) while missing full object extent during training. In this article, we introduce classical continuation optimization into MIL, thereby creating continuation MIL (C-MIL) with the aim to alleviate the nonconvexity problem in a systematic way. To fulfill this purpose, we partition instances into class-related and spatially related subsets and approximate MIL’s objective function with a series of smoothed objective functions defined within the subsets. We further propose a parametric strategy to implement continuation smooth functions, which enables C-MIL to be applied to instance selection tasks in a uniform manner. Optimizing smoothed loss functions prevents the training procedure from falling prematurely into local minima and facilities learning full object extent. Extensive experiments demonstrate the superiority of CMIL over conventional MIL methods. As a general instance selection method, C-MIL is also applied to supervised object detection to optimize anchors/features, improving the detection performance with a significant margin.},
  archive      = {J_TNNLS},
  author       = {Qixiang Ye and Fang Wan and Chang Liu and Qingming Huang and Xiangyang Ji},
  doi          = {10.1109/TNNLS.2021.3070801},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5452-5466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continuation multiple instance learning for weakly and fully supervised object detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous state and unknown input estimation for complex
networks with redundant channels under dynamic event-triggered
mechanisms. <em>TNNLS</em>, <em>33</em>(10), 5441–5451. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the simultaneous state and unknown input estimation problem for a class of discrete time-varying complex networks (CNs) under redundant channels and dynamic event-triggered mechanisms (ETMs). The redundant channels, modeled by an array of mutually independent Bernoulli distributed stochastic variables, are exploited to enhance transmission reliability. For energy-saving purposes, a dynamic event-triggered transmission scheme is enforced to ensure that every sensor node sends its measurement to the corresponding estimator only when a certain condition holds. The primary objective of the investigation carried out is to construct a recursive estimator for both the state and the unknown input such that certain upper bounds on the estimation error covariances are first guaranteed and then minimized at each time instant in the presence of dynamic event-triggered strategies and redundant channels. By solving two series of recursive difference equations, the desired estimator gains are computed. Finally, an illustrative example is presented to show the usefulness of the developed estimator design method.},
  archive      = {J_TNNLS},
  author       = {Qi Li and Zidong Wang and Jun Hu and Weiguo Sheng},
  doi          = {10.1109/TNNLS.2021.3070797},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5441-5451},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simultaneous state and unknown input estimation for complex networks with redundant channels under dynamic event-triggered mechanisms},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning-based 2-d frequency estimation of multiple
sinusoidals. <em>TNNLS</em>, <em>33</em>(10), 5429–5440. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency estimation of 2-D multicomponent sinusoidal signals is a fundamental issue in the statistical signal processing community that arises in various disciplines. In this article, we extend the DeepFreq model by modifying its network architecture and apply it to 2-D signals. We name the proposed framework 2-D ResFreq. Compared with the original DeepFreq framework, the 2-D convolutional implementation of the matched filtering module facilitates the transformation from time-domain signals to frequency-domain signals and reduces the number of network parameters. The additional upsampling layer and stacked residual blocks are designed to perform superresolution. Moreover, we introduce frequency amplitude information into the optimization function to improve the amplitude accuracy. After training, the signals in the test set are forward-mapped to 2-D accurate and high-resolution frequency representations. Frequency and amplitude estimation are achieved by measuring the locations and strengths of the spectral peaks. We conduct numerical experiments to demonstrate the superior performance of the proposed architecture in terms of its superresolution capability and estimation accuracy.},
  archive      = {J_TNNLS},
  author       = {Pingping Pan and Yunjian Zhang and Zhenmiao Deng and Wei Qi},
  doi          = {10.1109/TNNLS.2021.3070707},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5429-5440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning-based 2-D frequency estimation of multiple sinusoidals},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive NN finite-time resilient control for nonlinear
time-delay systems with unknown false data injection and actuator
faults. <em>TNNLS</em>, <em>33</em>(10), 5416–5428. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers neural network (NN)-based adaptive finite-time resilient control problem for a class of nonlinear time-delay systems with unknown fault data injection attacks and actuator faults. In the procedure of recursive design, a coordinate transformation and a modified fractional-order command-filtered (FOCF) backstepping technique are incorporated to handle the unknown false data injection attacks and overcome the issue of “explosion of complexity” caused by repeatedly taking derivatives for virtual control laws. The theoretical analysis proves that the developed resilient controller can guarantee the finite-time stability of the closed-loop system (CLS) and the stabilization errors converge to an adjustable neighborhood of zero. The foremost contributions of this work include: 1) by means of a modified FOCF technique, the adaptive resilient control problem of more general nonlinear time-delay systems with unknown cyberattacks and actuator faults is first considered; 2) different from most of the existing results, the commonly used assumptions on the sign of attack weight and prior knowledge of actuator faults are fully removed in this article. Finally, two simulation examples are given to demonstrate the effectiveness of the developed control scheme.},
  archive      = {J_TNNLS},
  author       = {Shuai Song and Ju H. Park and Baoyong Zhang and Xiaona Song},
  doi          = {10.1109/TNNLS.2021.3070623},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5416-5428},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN finite-time resilient control for nonlinear time-delay systems with unknown false data injection and actuator faults},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint learning of neural transfer and architecture
adaptation for image recognition. <em>TNNLS</em>, <em>33</em>(10),
5401–5415. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art visual recognition systems usually rely on the following pipeline: 1) pretraining a neural network on a large-scale data set (e.g., ImageNet) and 2) finetuning the network weights on a smaller, task-specific data set. Such a pipeline assumes that the sole weight adaptation is able to transfer the network capability from one domain to another domain based on a strong assumption that a fixed architecture is appropriate for all domains. However, each domain with a distinct recognition target may need different levels/paths of feature hierarchy, where some neurons may become redundant, and some others are reactivated to form new network structures. In this work, we prove that dynamically adapting network architectures tailored for each domain task along with weight finetuning benefits in both efficiency and effectiveness, compared to the existing image recognition pipeline that only tunes the weights regardless of the architecture. Our method can be easily generalized to an unsupervised paradigm by replacing supernet training with self-supervised learning in the source domain tasks and performing linear evaluation in the downstream tasks. This further improves the search efficiency of our method. Moreover, we also provide principled and empirical analysis to explain why our approach works by investigating the ineffectiveness of existing neural architecture search. We find that preserving the joint distribution of the network architecture and weights is of importance. This analysis not only benefits image recognition but also provides insights for crafting neural networks. Experiments on five representative image recognition tasks, such as person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation, demonstrate the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Guangrun Wang and Liang Lin and Rongcong Chen and Guangcong Wang and Jiqi Zhang},
  doi          = {10.1109/TNNLS.2021.3070605},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5401-5415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint learning of neural transfer and architecture adaptation for image recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonblind image deblurring via deep learning in complex
field. <em>TNNLS</em>, <em>33</em>(10), 5387–5400. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonblind image deblurring is about recovering the latent clear image from a blurry one generated by a known blur kernel, which is an often-seen yet challenging inverse problem in imaging. Its key is how to robustly suppress noise magnification during the inversion process. Recent approaches made a breakthrough by exploiting convolutional neural network (CNN)-based denoising priors in the image domain or the gradient domain, which allows using a CNN for noise suppression. The performance of these approaches is highly dependent on the effectiveness of the denoising CNN in removing magnified noise whose distribution is unknown and varies at different iterations of the deblurring process for different images. In this article, we introduce a CNN-based image prior defined in the Gabor domain. The prior not only utilizes the optimal space-frequency resolution and strong orientation selectivity of the Gabor transform but also enables using complex-valued (CV) representations in intermediate processing for better denoising. A CV CNN is developed to exploit the benefits of the CV representations, with better generalization to handle unknown noises over the real-valued ones. Combining our Gabor-domain CV CNN-based prior with an unrolling scheme, we propose a deep-learning-based approach to nonblind image deblurring. Extensive experiments have demonstrated the superior performance of the proposed approach over the state-of-the-art ones.},
  archive      = {J_TNNLS},
  author       = {Yuhui Quan and Peikang Lin and Yong Xu and Yuesong Nan and Hui Ji},
  doi          = {10.1109/TNNLS.2021.3070596},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5387-5400},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Nonblind image deblurring via deep learning in complex field},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiagent meta-reinforcement learning for adaptive
multipath routing optimization. <em>TNNLS</em>, <em>33</em>(10),
5374–5386. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the routing problem of packet networks through multiagent reinforcement learning (RL), which is a very challenging topic in distributed and autonomous networked systems. In specific, the routing problem is modeled as a networked multiagent partially observable Markov decision process (MDP). Since the MDP of a network node is not only affected by its neighboring nodes’ policies but also the network traffic demand, it becomes a multitask learning problem. Inspired by recent success of RL and metalearning, we propose two novel model-free multiagent RL algorithms, named multiagent proximal policy optimization (MAPPO) and multiagent metaproximal policy optimization (meta-MAPPO), to optimize the network performances under fixed and time-varying traffic demand, respectively. A practicable distributed implementation framework is designed based on the separability of exploration and exploitation in training MAPPO. Compared with the existing routing optimization policies, our simulation results demonstrate the excellent performances of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Long Chen and Bin Hu and Zhi-Hong Guan and Lian Zhao and Xuemin Shen},
  doi          = {10.1109/TNNLS.2021.3070584},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5374-5386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiagent meta-reinforcement learning for adaptive multipath routing optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical multiagent reinforcement learning for
allocating guaranteed display ads. <em>TNNLS</em>, <em>33</em>(10),
5361–5373. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the problem of guaranteed display ads (GDAs) allocation, which requires proactively allocate display ads to different impressions to fulfill their impression demands indicated in the contracts. Existing methods for this problem either assume the impressions that are static or solely consider a specific ad’s benefits. Thus, it is hard to generalize to the industrial production scenario where the impressions are dynamical and large-scale, and the overall allocation optimality of all the considered GDAs is required. To bridge this gap, we formulate this problem as a sequential decision-making problem in the scope of multiagent reinforcement learning (MARL), by assigning an allocation agent to each ad and coordinating all the agents for allocating GDAs. The inputs are the states (e.g., the demands of the ad and the remaining time steps for displaying the ads) of each ad and the impressions at different time steps, and the outputs are the display ratios of each ad for each impression. Specifically, we propose a novel hierarchical MARL (HMARL) method that creates hierarchies over the agent policies to handle a large number of ads and the dynamics of impressions. HMARL contains: 1) a manager policy to navigate the agent to choose an appropriate subpolicy and 2) a set of subpolicies that let the agents perform diverse conditioning on their states. Extensive experiments on three real-world data sets from the Tencent advertising platform with tens of millions of records demonstrate significant improvements of HMARL over state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Lu Wang and Lei Han and Xinru Chen and Chengchang Li and Junzhou Huang and Weinan Zhang and Wei Zhang and Xiaofeng He and Dijun Luo},
  doi          = {10.1109/TNNLS.2021.3070484},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5361-5373},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical multiagent reinforcement learning for allocating guaranteed display ads},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face sketch synthesis using regularized broad learning
system. <em>TNNLS</em>, <em>33</em>(10), 5346–5360. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two main categories of face sketch synthesis: data- and model-driven. The data-driven method synthesizes sketches from training photograph–sketch patches at the cost of detail loss. The model-driven method can preserve more details, but the mapping from photographs to sketches is a time-consuming training process, especially when the deep structures require to be refined. We propose a face sketch synthesis method via regularized broad learning system (RBLS). The broad learning-based system directly transforms photographs into sketches with rich details preserved. Also, the incremental learning scheme of broad learning system (BLS) ensures that our method easily increases feature mappings and remodels the network without retraining when the extracted feature mapping nodes are not sufficient. Besides, a Bayesian estimation-based regularization is introduced with the BLS to aid further feature selection and improve the generalization ability and robustness. Various experiments on the CUHK student data set and Aleix Robert (AR) data set demonstrated the effectiveness and efficiency of our RBLS method. Unlike existing methods, our method synthesizes high-quality face sketches much efficiently and greatly reduces computational complexity both in the training and test processes.},
  archive      = {J_TNNLS},
  author       = {Ping Li and Bin Sheng and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3070463},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5346-5360},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Face sketch synthesis using regularized broad learning system},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discriminative multi-view dynamic image fusion for
cross-view 3-d action recognition. <em>TNNLS</em>, <em>33</em>(10),
5332–5345. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dramatic imaging viewpoint variation is the critical challenge toward action recognition for depth video. To address this, one feasible way is to enhance view-tolerance of visual feature, while still maintaining strong discriminative capacity. Multi-view dynamic image (MVDI) is the most recently proposed 3-D action representation manner that is able to compactly encode human motion information and 3-D visual clue well. However, it is still view-sensitive. To leverage its performance, a discriminative MVDI fusion method is proposed by us via multi-instance learning (MIL). Specifically, the dynamic images (DIs) from different observation viewpoints are regarded as the instances for 3-D action characterization. After being encoded using Fisher vector (FV), they are then aggregated by sum-pooling to yield the representative 3-D action signature. Our insight is that viewpoint aggregation helps to enhance view-tolerance. And, FV can map the raw DI feature to the higher dimensional feature space to promote the discriminative power. Meanwhile, a discriminative viewpoint instance discovery method is also proposed to discard the viewpoint instances unfavorable for action characterization. The wide-range experiments on five data sets demonstrate that our proposition can significantly enhance the performance of cross-view 3-D action recognition. And, it is also applicable to cross-view 3-D object recognition. The source code is available at https://github.com/3huo/ActionView .},
  archive      = {J_TNNLS},
  author       = {Yancheng Wang and Yang Xiao and Junyi Lu and Bo Tan and Zhiguo Cao and Zhenjun Zhang and Joey Tianyi Zhou},
  doi          = {10.1109/TNNLS.2021.3070179},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5332-5345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Discriminative multi-view dynamic image fusion for cross-view 3-D action recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uniform stability of complex-valued neural networks of
fractional order with linear impulses and fixed time delays.
<em>TNNLS</em>, <em>33</em>(10), 5321–5331. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a generation of the real-valued neural network (RVNN), complex-valued neural network (CVNN) is based on the complex-valued (CV) parameters and variables. The fractional-order (FO) CVNN with linear impulses and fixed time delays is discussed. By using the sign function, the Banach fixed point theorem, and two classes of activation functions, some criteria of uniform stability for the solution and existence and uniqueness for equilibrium solution are derived. Finally, three experimental simulations are presented to illustrate the correctness and effectiveness of the obtained results.},
  archive      = {J_TNNLS},
  author       = {Hui Li and Yonggui Kao and Haibo Bao and Yangquan Chen},
  doi          = {10.1109/TNNLS.2021.3070136},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5321-5331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Uniform stability of complex-valued neural networks of fractional order with linear impulses and fixed time delays},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prototype-based multisource domain adaptation.
<em>TNNLS</em>, <em>33</em>(10), 5308–5320. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to transfer knowledge from labeled source domain to unlabeled target domain. Recently, multisource domain adaptation (MDA) has begun to attract attention. Its performance should go beyond simply mixing all source domains together for knowledge transfer. In this article, we propose a novel prototype-based method for MDA. Specifically, for solving the problem that the target domain has no label, we use the prototype to transfer the semantic category information from source domains to target domain. First, a feature extraction network is applied to both source and target domains to obtain the extracted features from which the domain-invariant features and domain-specific features will be disentangled. Then, based on these two kinds of features, the named inherent class prototypes and domain prototypes are estimated, respectively. Then a prototype mapping to the extracted feature space is learned in the feature reconstruction process. Thus, the class prototypes for all source and target domains can be constructed in the extracted feature space based on the previous domain prototypes and inherent class prototypes. By forcing the extracted features are close to the corresponding class prototypes for all domains, the feature extraction network is progressively adjusted. In the end, the inherent class prototypes are used as a classifier in the target domain. Our contribution is that through the inherent class prototypes and domain prototypes, the semantic category information from source domains is transformed into the target domain by constructing the corresponding class prototypes. In our method, all source and target domains are aligned twice at the feature level for better domain-invariant features and more closer features to the class prototypes, respectively. Several experiments on public data sets also prove the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Lihua Zhou and Mao Ye and Dan Zhang and Ce Zhu and Luping Ji},
  doi          = {10.1109/TNNLS.2021.3070085},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5308-5320},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prototype-based multisource domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-source contribution learning for domain adaptation.
<em>TNNLS</em>, <em>33</em>(10), 5293–5307. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning becomes an attractive technology to tackle a task from a target domain by leveraging previously acquired knowledge from a similar domain (source domain). Many existing transfer learning methods focus on learning one discriminator with single-source domain. Sometimes, knowledge from single-source domain might not be enough for predicting the target task. Thus, multiple source domains carrying richer transferable information are considered to complete the target task. Although there are some previous studies dealing with multi-source domain adaptation, these methods commonly combine source predictions by averaging source performances. Different source domains contain different transferable information; they may contribute differently to a target domain compared with each other. Hence, the source contribution should be taken into account when predicting a target task. In this article, we propose a novel multi-source contribution learning method for domain adaptation (MSCLDA). As proposed, the similarities and diversities of domains are learned simultaneously by extracting multi-view features. One view represents common features (similarities) among all domains. Other views represent different characteristics (diversities) in a target domain; each characteristic is expressed by features extracted in a source domain. Then multi-level distribution matching is employed to improve the transferability of latent features, aiming to reduce misclassification of boundary samples by maximizing discrepancy between different classes and minimizing discrepancy between the same classes. Concurrently, when completing a target task by combining source predictions, instead of averaging source predictions or weighting sources using normalized similarities, the original weights learned by normalizing similarities between source and target domains are adjusted using pseudo target labels to increase the disparities of weight values, which is desired to improve the performance of the final target predictor if the predictions of sources exist significant difference. Experiments on real-world visual data sets demonstrate the superiorities of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Keqiuyin Li and Jie Lu and Hua Zuo and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2021.3069982},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5293-5307},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-source contribution learning for domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EDropout: Energy-based dropout and pruning of deep neural
networks. <em>TNNLS</em>, <em>33</em>(10), 5279–5292. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dropout is a well-known regularization method by sampling a sub-network from a larger deep neural network and training different sub-networks on different subsets of the data. Inspired by the dropout concept, we propose EDropout as an energy-based framework for pruning neural networks in classification tasks. In this approach, a set of binary pruning state vectors (population) represents a set of corresponding sub-networks from an arbitrary original neural network. An energy loss function assigns a scalar energy loss value to each pruning state. The energy-based model (EBM) stochastically evolves the population to find states with lower energy loss. The best pruning state is then selected and applied to the original network. Similar to dropout, the kept weights are updated using backpropagation in a probabilistic model. The EBM again searches for better pruning states and the cycle continuous. This procedure is a switching between the energy model, which manages the pruning states, and the probabilistic model, which updates the kept weights, in each iteration. The population can dynamically converge to a pruning state. This can be interpreted as dropout leading to pruning the network. From an implementation perspective, unlike most of the pruning methods, EDropout can prune neural networks without manually modifying the network architecture code. We have evaluated the proposed method on different flavors of ResNets, AlexNet, $l_{1}$ pruning, ThinNet, ChannelNet, and SqueezeNet on the Kuzushiji, Fashion, CIFAR-10, CIFAR-100, Flowers, and ImageNet data sets, and compared the pruning rate and classification performance of the models. The networks trained with EDropout on average achieved a pruning rate of more than 50\% of the trainable parameters with approximately &lt; 5\% and &lt; 1\% drop of Top-1 and Top-5 classification accuracy, respectively.},
  archive      = {J_TNNLS},
  author       = {Hojjat Salehinejad and Shahrokh Valaee},
  doi          = {10.1109/TNNLS.2021.3069970},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5279-5292},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {EDropout: Energy-based dropout and pruning of deep neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time synchronization of markovian coupled neural
networks with delays via intermittent quantized control: Linear
programming approach. <em>TNNLS</em>, <em>33</em>(10), 5268–5278. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is devoted to investigating finite-time synchronization (FTS) for coupled neural networks (CNNs) with time-varying delays and Markovian jumping topologies by using an intermittent quantized controller. Due to the intermittent property, it is very hard to surmount the effects of time delays and ascertain the settling time. A new lemma with novel finite-time stability inequality is developed first. Then, by constructing a new Lyapunov functional and utilizing linear programming (LP) method, several sufficient conditions are obtained to assure that the Markovian CNNs achieve synchronization with an isolated node in a settling time that relies on the initial values of considered systems, the width of control and rest intervals, and the time delays. The control gains are designed by solving the LP. Moreover, an optimal algorithm is given to enhance the accuracy in estimating the settling time. Finally, a numerical example is provided to show the merits and correctness of the theoretical analysis.},
  archive      = {J_TNNLS},
  author       = {Rongqiang Tang and Housheng Su and Yi Zou and Xinsong Yang},
  doi          = {10.1109/TNNLS.2021.3069926},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5268-5278},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization of markovian coupled neural networks with delays via intermittent quantized control: Linear programming approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General bitwidth assignment for efficient deep convolutional
neural network quantization. <em>TNNLS</em>, <em>33</em>(10), 5253–5267.
(<a href="https://doi.org/10.1109/TNNLS.2021.3069886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model quantization is essential to deploy deep convolutional neural networks (DCNNs) on resource-constrained devices. In this article, we propose a general bitwidth assignment algorithm based on theoretical analysis for efficient layerwise weight and activation quantization of DCNNs. The proposed algorithm develops a prediction model to explicitly estimate the loss of classification accuracy led by weight quantization with a geometrical approach. Consequently, dynamic programming is adopted to achieve optimal bitwidth assignment on weights based on the estimated error. Furthermore, we optimize bitwidth assignment for activations by considering the signal-to-quantization-noise ratio (SQNR) between weight and activation quantization. The proposed algorithm is general to reveal the tradeoff between classification accuracy and model size for various network architectures. Extensive experiments demonstrate the efficacy of the proposed bitwidth assignment algorithm and the error rate prediction model. Furthermore, the proposed algorithm is shown to be well extended to object detection.},
  archive      = {J_TNNLS},
  author       = {Wen Fei and Wenrui Dai and Chenglin Li and Junni Zou and Hongkai Xiong},
  doi          = {10.1109/TNNLS.2021.3069886},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5253-5267},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {General bitwidth assignment for efficient deep convolutional neural network quantization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered adaptive neural network sensor failure
compensation for switched interconnected nonlinear systems with unknown
control coefficients. <em>TNNLS</em>, <em>33</em>(10), 5241–5252. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a decentralized adaptive neural network (NN) event-triggered sensor failure compensation control issue is investigated for nonlinear switched large-scale systems. Due to the presence of unknown control coefficients, output interactions, sensor faults, and arbitrary switchings, previous works cannot solve the investigated issue. First, to estimate unmeasured states, a novel observer is designed. Then, NNs are utilized for identifying both interconnected terms and unstructured uncertainties. A novel fault compensation mechanism is proposed to circumvent the obstacle caused by sensor faults, and a Nussbaum-type function is introduced to tackle unknown control coefficients. A novel switching threshold strategy is developed to balance communication constraints and system performance. Based on the common Lyapunov function (CLF) method, an event-triggered decentralized control scheme is proposed to guarantee that all closed-loop signals are bounded even if sensors undergo failures. It is shown that the Zeno behavior is avoided. Finally, simulation results are presented to show the validity of the proposed strategy.},
  archive      = {J_TNNLS},
  author       = {Jing Zhang and Zhengrong Xiang},
  doi          = {10.1109/TNNLS.2021.3069817},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5241-5252},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered adaptive neural network sensor failure compensation for switched interconnected nonlinear systems with unknown control coefficients},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning-based cooperative optimal output
regulation via distributed adaptive internal model. <em>TNNLS</em>,
<em>33</em>(10), 5229–5240. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a data-driven distributed control method is proposed to solve the cooperative optimal output regulation problem of leader–follower multiagent systems. Different from traditional studies on cooperative output regulation, a distributed adaptive internal model is originally developed, which includes a distributed internal model and a distributed observer to estimate the leader’s dynamics. Without relying on the dynamics of multiagent systems, we have proposed two reinforcement learning algorithms, policy iteration and value iteration, to learn the optimal controller through online input and state data, and estimated values of the leader’s state. By combining these methods, we have established a basis for connecting data-distributed control methods with adaptive dynamic programming approaches in general since these are the theoretical foundation from which they are built.},
  archive      = {J_TNNLS},
  author       = {Weinan Gao and Mohammed Mynuddin and Donald C. Wunsch and Zhong-Ping Jiang},
  doi          = {10.1109/TNNLS.2021.3069728},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5229-5240},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning-based cooperative optimal output regulation via distributed adaptive internal model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spike-timing-dependent plasticity with activation-dependent
scaling for receptive fields development. <em>TNNLS</em>,
<em>33</em>(10), 5215–5228. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike-timing-dependent plasticity (STDP) is one of the most popular and deeply biologically motivated forms of unsupervised Hebbian-type learning. In this article, we propose a variant of STDP extended by an additional activation-dependent scale factor. The consequent learning rule is an efficient algorithm, which is simple to implement and applicable to spiking neural networks (SNNs). It is demonstrated that the proposed plasticity mechanism combined with competitive learning can serve as an effective mechanism for the unsupervised development of receptive fields (RFs). Furthermore, the relationship between synaptic scaling and lateral inhibition is explored in the context of the successful development of RFs. Specifically, we demonstrate that maintaining a high level of synaptic scaling followed by its rapid increase is crucial for the development of neuronal mechanisms of selectivity. The strength of the proposed solution is assessed in classification tasks performed on the Modified National Institute of Standards and Technology (MNIST) data set with an accuracy level of 94.65\% (a single network) and 95.17\% (a network committee)—comparable to the state-of-the-art results of single-layer SNN architectures trained in an unsupervised manner. Furthermore, the training process leads to sparse data representation and the developed RFs have the potential to serve as local feature detectors in multilayered spiking networks. We also prove theoretically that when applied to linear Poisson neurons, our rule conserves total synaptic strength, guaranteeing the convergence of the learning process.},
  archive      = {J_TNNLS},
  author       = {Marcin Białas and Jacek Mańdziuk},
  doi          = {10.1109/TNNLS.2021.3069683},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5215-5228},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spike-timing-dependent plasticity with activation-dependent scaling for receptive fields development},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective collaborative representation learning for
multilabel text categorization. <em>TNNLS</em>, <em>33</em>(10),
5200–5214. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the booming of deep learning, massive attention has been paid to developing neural models for multilabel text categorization (MLTC). Most of the works concentrate on disclosing word–label relationship, while less attention is taken in exploiting global clues, particularly with the relationship of document–label. To address this limitation, we propose an effective collaborative representation learning (CRL) model in this article. CRL consists of a factorization component for generating shallow representations of documents and a neural component for deep text-encoding and classification. We have developed strategies for jointly training those two components, including an alternating-least-squares-based approach for factorizing the pointwise mutual information (PMI) matrix of label–document and multitask learning (MTL) strategy for the neural component. According to the experimental results on six data sets, CRL can explicitly take advantage of the relationship of document–label and achieve competitive classification performance in comparison with some state-of-the-art deep methods.},
  archive      = {J_TNNLS},
  author       = {Hao Wu and Shaowei Qin and Rencan Nie and Jinde Cao and Sergey Gorbachev},
  doi          = {10.1109/TNNLS.2021.3069647},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5200-5214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Effective collaborative representation learning for multilabel text categorization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training-free deep generative networks for compressed
sensing of neural action potentials. <em>TNNLS</em>, <em>33</em>(10),
5190–5199. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption is an important issue for resource-constrained wireless neural recording applications with limited data bandwidth. Compressed sensing (CS) is a promising framework for addressing this challenge because it can compress data in an energy-efficient way. Recent work has shown that deep neural networks (DNNs) can serve as valuable models for CS of neural action potentials (APs). However, these models typically require impractically large datasets and computational resources for training, and they do not easily generalize to novel circumstances. Here, we propose a new CS framework, termed APGen, for the reconstruction of APs in a training-free manner. It consists of a deep generative network and an analysis sparse regularizer. We validate our method on two in vivo datasets. Even without any training, APGen outperformed model-based and data-driven methods in terms of reconstruction accuracy, computational efficiency, and robustness to AP overlap and misalignment. The computational efficiency of APGen and its ability to perform without training make it an ideal candidate for long-term, resource-constrained, and large-scale wireless neural recording. It may also promote the development of real-time, naturalistic brain–computer interfaces.},
  archive      = {J_TNNLS},
  author       = {Biao Sun and Chaoxu Mu and Zexu Wu and Xinshan Zhu},
  doi          = {10.1109/TNNLS.2021.3069436},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5190-5199},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Training-free deep generative networks for compressed sensing of neural action potentials},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiview subspace clustering via co-training robust data
representation. <em>TNNLS</em>, <em>33</em>(10), 5177–5189. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking the assumption that data samples are able to be reconstructed with the dictionary formed by themselves, recent multiview subspace clustering (MSC) algorithms aim to find a consensus reconstruction matrix via exploring complementary information across multiple views. Most of them directly operate on the original data observations without preprocessing, while others operate on the corresponding kernel matrices. However, they both ignore that the collected features may be designed arbitrarily and hard guaranteed to be independent and nonoverlapping. As a result, original data observations and kernel matrices would contain a large number of redundant details. To address this issue, we propose an MSC algorithm that groups samples and removes data redundancy concurrently. In specific, eigendecomposition is employed to obtain the robust data representation of low redundancy for later clustering. By utilizing the two processes into a unified model, clustering results will guide eigendecomposition to generate more discriminative data representation, which, as feedback, helps obtain better clustering results. In addition, an alternate and convergent algorithm is designed to solve the optimization problem. Extensive experiments are conducted on eight benchmarks, and the proposed algorithm outperforms comparative ones in recent literature by a large margin, verifying its superiority. At the same time, its effectiveness, computational efficiency, and robustness to noise are validated experimentally.},
  archive      = {J_TNNLS},
  author       = {Jiyuan Liu and Xinwang Liu and Yuexiang Yang and Xifeng Guo and Marius Kloft and Liangzhong He},
  doi          = {10.1109/TNNLS.2021.3069424},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5177-5189},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview subspace clustering via co-training robust data representation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised learning for nonsequential data: A canonical
polyadic decomposition approach. <em>TNNLS</em>, <em>33</em>(10),
5162–5176. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient modeling of feature interactions underpins supervised learning for nonsequential tasks, characterized by a lack of inherent ordering of features (variables). The brute force approach of learning a parameter for each interaction of every order comes at an exponential computational and memory cost (curse of dimensionality). To alleviate this issue, it has been proposed to implicitly represent the model parameters as a tensor, the order of which is equal to the number of features; for efficiency, it can be further factorized into a compact tensor train (TT) format. However, both TT and other tensor networks (TNs), such as tensor ring and hierarchical Tucker, are sensitive to the ordering of their indices (and hence to the features). To establish the desired invariance to feature ordering, we propose to represent the weight tensor through the canonical polyadic (CP) decomposition (CPD) and introduce the associated inference and learning algorithms, including suitable regularization and initialization schemes. It is demonstrated that the proposed CP-based predictor significantly outperforms other TN-based predictors on sparse data while exhibiting comparable performance on dense nonsequential tasks. Furthermore, for enhanced expressiveness, we generalize the framework to allow feature mapping to arbitrarily high-dimensional feature vectors. In conjunction with feature vector normalization, this is shown to yield dramatic improvements in performance for dense nonsequential tasks, matching models such as fully connected neural networks.},
  archive      = {J_TNNLS},
  author       = {Alexandros Haliassos and Kriton Konstantinidis and Danilo P. Mandic},
  doi          = {10.1109/TNNLS.2021.3069399},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5162-5176},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised learning for nonsequential data: A canonical polyadic decomposition approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multigraph transformer for free-hand sketch recognition.
<em>TNNLS</em>, <em>33</em>(10), 5150–5161. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning meaningful representations of free-hand sketches remains a challenging task given the signal sparsity and the high-level abstraction of sketches. Existing techniques have focused on exploiting either the static nature of sketches with convolutional neural networks (CNNs) or the temporal sequential property with recurrent neural networks (RNNs). In this work, we propose a new representation of sketches as multiple sparsely connected graphs. We design a novel graph neural network (GNN), the multigraph transformer (MGT), for learning representations of sketches from multiple graphs, which simultaneously capture global and local geometric stroke structures as well as temporal information. We report extensive numerical experiments on a sketch recognition task to demonstrate the performance of the proposed approach. Particularly, MGT applied on 414k sketches from Google QuickDraw: 1) achieves a small recognition gap to the CNN-based performance upper bound (72.80\% versus 74.22\%) and infers faster than the CNN competitors and 2) outperforms all RNN-based models by a significant margin. To the best of our knowledge, this is the first work proposing to represent sketches as graphs and apply GNNs for sketch recognition. Code and trained models are available at https://github.com/PengBoXiangShang/multigraph_transformer .},
  archive      = {J_TNNLS},
  author       = {Peng Xu and Chaitanya K. Joshi and Xavier Bresson},
  doi          = {10.1109/TNNLS.2021.3069230},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5150-5161},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multigraph transformer for free-hand sketch recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Two-stage bayesian optimization for scalable inference in
state-space models. <em>TNNLS</em>, <em>33</em>(10), 5138–5149. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-space models (SSMs) are a rich class of dynamical models with a wide range of applications in economics, healthcare, computational biology, robotics, and more. Proper analysis, control, learning, and decision-making in dynamical systems modeled by SSMs depend on the accuracy of the inferred/learned model. Most of the existing inference techniques for SSMs are capable of dealing with very small systems, unable to be applied to most of the large-scale practical problems. Toward this, this article introduces a two-stage Bayesian optimization (BO) framework for scalable and efficient inference in SSMs. The proposed framework maps the original large parameter space to a reduced space, containing a small linear combination of the original space. This reduced space, which captures the most variability in the inference function (e.g., log likelihood or log a posteriori), is obtained by eigenvalue decomposition of the covariance of gradients of the inference function approximated by a particle filtering scheme. Then, an exponential reduction in the search space of parameters during the inference process is achieved through the proposed two-stage BO policy, where the solution of the first-stage BO policy in the reduced space specifies the search space of the second-stage BO in the original space. The proposed framework’s accuracy and speed are demonstrated through several experiments, including real metagenomics data from a gut microbial community.},
  archive      = {J_TNNLS},
  author       = {Mahdi Imani and Seyede Fatemeh Ghoreishi},
  doi          = {10.1109/TNNLS.2021.3069172},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5138-5149},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-stage bayesian optimization for scalable inference in state-space models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural time-aware sequential recommendation by jointly
modeling preference dynamics and explicit feature couplings.
<em>TNNLS</em>, <em>33</em>(10), 5125–5137. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommendation, both stationary and dynamic user preferences on items are embedded in the interactions between users and items (e.g., rating or clicking) within their contexts. Sequential recommender systems (SRSs) need to jointly involve such context-aware user–item interactions in terms of the couplings between the user and item features and sequential user actions on items over time. However, such joint modeling is non-trivial and significantly challenges the existing work on preference modeling, which either only models user–item interactions by latent factorization models but ignores user preference dynamics or only captures sequential user action patterns without involving user/item features and context factors and their coupling and influence on user actions. We propose a neural time-aware recommendation network (TARN) with a temporal context to jointly model 1) stationary user preferences by a feature interaction network and 2) user preference dynamics by a tailored convolutional network. The feature interaction network factorizes the pairwise couplings between non-zero features of users, items, and temporal context by the inner product of their feature embeddings while alleviating data sparsity issues. In the convolutional network, we introduce a convolutional layer with multiple filter widths to capture multi-fold sequential patterns, where an attentive average pooling (AAP) obtains significant and large-span feature combinations. To learn the preference dynamics, a novel temporal action embedding represents user actions by incorporating the embeddings of items and temporal context as the inputs of the convolutional network. The experiments on typical public data sets demonstrate that TARN outperforms state-of-the-art methods and show the necessity and contribution of involving time-aware preference dynamics and explicit user/item feature couplings in modeling and interpreting evolving user preferences.},
  archive      = {J_TNNLS},
  author       = {Qi Zhang and Longbing Cao and Chongyang Shi and Zhendong Niu},
  doi          = {10.1109/TNNLS.2021.3069058},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5125-5137},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural time-aware sequential recommendation by jointly modeling preference dynamics and explicit feature couplings},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Hash bit selection via collaborative neurodynamic
optimization with discrete hopfield networks. <em>TNNLS</em>,
<em>33</em>(10), 5116–5124. (<a
href="https://doi.org/10.1109/TNNLS.2021.3068500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hash bit selection (HBS) aims to find the most discriminative and informative hash bits from a hash pool generated by using different hashing algorithms. It is usually formulated as a binary quadratic programming problem with an information-theoretic objective function and a string-length constraint. In this article, it is equivalently reformulated in the form of a quadratic unconstrained binary optimization problem by augmenting the objective function with a penalty function. The reformulated problem is solved via collaborative neurodynamic optimization (CNO) with a population of classic discrete Hopfield networks. The two most important hyperparameters of the CNO approach are determined based on Monte Carlo test results. Experimental results on three benchmark data sets are elaborated to substantiate the superiority of the collaborative neurodynamic approach to several existing methods for HBS.},
  archive      = {J_TNNLS},
  author       = {Xinqi Li and Jun Wang and Sam Kwong},
  doi          = {10.1109/TNNLS.2021.3068500},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5116-5124},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hash bit selection via collaborative neurodynamic optimization with discrete hopfield networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of deep learning on CPUs: Opportunities and
co-optimizations. <em>TNNLS</em>, <em>33</em>(10), 5095–5115. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CPU is a powerful, pervasive, and indispensable platform for running deep learning (DL) workloads in systems ranging from mobile to extreme-end servers. In this article, we present a survey of techniques for optimizing DL applications on CPUs. We include the methods proposed for both inference and training and those offered in the context of mobile, desktop/server, and distributed systems. We identify the areas of strength and weaknesses of CPUs in the field of DL. This article will interest practitioners and researchers in the area of artificial intelligence, computer architecture, mobile systems, and parallel computing.},
  archive      = {J_TNNLS},
  author       = {Sparsh Mittal and Poonam Rajput and Sreenivas Subramoney},
  doi          = {10.1109/TNNLS.2021.3071762},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {10},
  pages        = {5095-5115},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of deep learning on CPUs: Opportunities and co-optimizations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative learning control for output tracking of nonlinear
systems with unavailable state information. <em>TNNLS</em>,
<em>33</em>(9), 5085–5092. (<a
href="https://doi.org/10.1109/TNNLS.2021.3062633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a novel design framework of adaptive iterative learning control (ILC) approach for a class of uncertain nonlinear systems. By using the closed-loop reference model that can be viewed as an observer, the proposed adaptive ILC approach can be adapted to deal with the output tracking problem of nonlinear systems with unavailable system states. In the systems considered, two classes of uncertainties are taken into account, including parametric input disturbances and input distribution uncertainties. To facilitate the controller design and convergence analysis, the composite energy function (CEF) methodology is employed. The design framework in this brief is novel and widely applicable, which extends the CEF-based ILC approach to output tracking control of nonlinear systems without requiring full knowledge of state information and complicated observer design process. To show the effectiveness of the proposed design framework and control algorithms, two numerical examples are illustrated.},
  archive      = {J_TNNLS},
  author       = {Xuefang Li and Dong Shen and Beichen Ding},
  doi          = {10.1109/TNNLS.2021.3062633},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5085-5092},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative learning control for output tracking of nonlinear systems with unavailable state information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum cryptanalysis on a multivariate cryptosystem based
on clipped hopfield neural network. <em>TNNLS</em>, <em>33</em>(9),
5080–5084. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shor’s quantum algorithm and other efficient quantum algorithms can break many public-key cryptographic schemes in polynomial time on a quantum computer. In response, researchers proposed postquantum cryptography to resist quantum computers. The multivariate cryptosystem (MVC) is one of a few options of postquantum cryptography. It is based on the NP-hardness of the computational problem to solve nonlinear equations over a finite field. Recently, Wang et al. (2018) proposed a MVC based on extended clipped hopfield neural networks (eCHNN). Its main security assumption is backed by the discrete logarithm (DL) problem over Matrics. In this brief, we present quantum cryptanalysis of Wang et al. ’s eCHNN-based MVC. We first show that Shor’s quantum algorithm can be modified to solve the DL problem over Matrics. Then we show that Wang et al. ’s construction of eCHNN-based MVC is not secure against quantum computers; this against the original intention of that multivariate cryptography is one of a few options of postquantum cryptography.},
  archive      = {J_TNNLS},
  author       = {Songsong Dai},
  doi          = {10.1109/TNNLS.2021.3059434},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5080-5084},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quantum cryptanalysis on a multivariate cryptosystem based on clipped hopfield neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semisupervised feature selection via generalized
uncorrelated constraint and manifold embedding. <em>TNNLS</em>,
<em>33</em>(9), 5070–5079. (<a
href="https://doi.org/10.1109/TNNLS.2021.3069038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge regression is frequently utilized by both supervised learning and semisupervised learning. However, the results cannot obtain the closed-form solution and perform manifold structure when ridge regression is directly applied to semisupervised learning. To address this issue, we propose a novel semisupervised feature selection method under generalized uncorrelated constraint, namely SFS. The generalized uncorrelated constraint equips the framework with the elegant closed-form solution and is introduced to the ridge regression with embedding the manifold structure. The manifold structure and closed-form solution can better save data’s topology information compared to the deep network with gradient descent. Furthermore, the full rank constraint of the projection matrix also avoids the occurrence of excessive row sparsity. The scale factor of the constraint that can be adaptively obtained also provides the subspace constraint more flexibility. Experimental results on data sets validate the superiority of our method to the state-of-the-art semisupervised feature selection methods.},
  archive      = {J_TNNLS},
  author       = {Xuelong Li and Yunxing Zhang and Rui Zhang},
  doi          = {10.1109/TNNLS.2021.3069038},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5070-5079},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised feature selection via generalized uncorrelated constraint and manifold embedding},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning improvement heuristics for solving routing
problems. <em>TNNLS</em>, <em>33</em>(9), 5057–5069. (<a
href="https://doi.org/10.1109/TNNLS.2021.3068828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies in using deep learning (DL) to solve routing problems focus on construction heuristics, whose solutions are still far from optimality. Improvement heuristics have great potential to narrow this gap by iteratively refining a solution. However, classic improvement heuristics are all guided by handcrafted rules that may limit their performance. In this article, we propose a deep reinforcement learning framework to learn the improvement heuristics for routing problems. We design a self-attention-based deep architecture as the policy network to guide the selection of the next solution. We apply our method to two important routing problems, i.e., the traveling salesman problem (TSP) and the capacitated vehicle routing problem (CVRP). Experiments show that our method outperforms state-of-the-art DL-based approaches. The learned policies are more effective than the traditional handcrafted ones and can be further enhanced by simple diversifying strategies. Moreover, the policies generalize well to different problem sizes, initial solutions, and even real-world data set.},
  archive      = {J_TNNLS},
  author       = {Yaoxin Wu and Wen Song and Zhiguang Cao and Jie Zhang and Andrew Lim},
  doi          = {10.1109/TNNLS.2021.3068828},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5057-5069},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning improvement heuristics for solving routing problems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time synchronization of reaction-diffusion inertial
memristive neural networks via gain-scheduled pinning control.
<em>TNNLS</em>, <em>33</em>(9), 5045–5056. (<a
href="https://doi.org/10.1109/TNNLS.2021.3068734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the considered reaction-diffusion inertial memristive neural networks (IMNNs), this article proposes a novel gain-scheduled generalized pinning control scheme, where three pinning control strategies are involved and $2^{n}$ controller gains can be scheduled for different system parameters. Moreover, a time delay is considered in the controller to make it has a memory function. With the designed controller, drive-and-response systems can be synchronized within a finite-time interval. Note that the final finite-time synchronization criterion is obtained in the forms of linear matrix inequalities (LMIs) by introducing a memristor-dependent sign function into the controller and constructing a new Lyapunov–Krasovskii functional (LKF). Furthermore, by utilizing some improved integral inequality methods, the conservatism of the main results can be greatly reduced. Finally, three numerical examples are provided to illustrate the feasibility, superiority, and practicability of this article.},
  archive      = {J_TNNLS},
  author       = {Xiaona Song and Jingtao Man and Ju H. Park and Shuai Song},
  doi          = {10.1109/TNNLS.2021.3068734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5045-5056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization of reaction-diffusion inertial memristive neural networks via gain-scheduled pinning control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IPool—information-based pooling in hierarchical graph neural
networks. <em>TNNLS</em>, <em>33</em>(9), 5032–5044. (<a
href="https://doi.org/10.1109/TNNLS.2021.3067441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of data science, the analysis of network or graph data has become a very timely research problem. A variety of recent works have been proposed to generalize neural networks to graphs, either from a spectral graph theory or a spatial perspective. The majority of these works, however, focus on adapting the convolution operator to graph representation. At the same time, the pooling operator also plays an important role in distilling multiscale and hierarchical representations, but it has been mostly overlooked so far. In this article, we propose a parameter-free pooling operator, called iPool, that permits to retain the most informative features in arbitrary graphs. With the argument that informative nodes dominantly characterize graph signals, we propose a criterion to evaluate the amount of information of each node given its neighbors and theoretically demonstrate its relationship to neighborhood conditional entropy. This new criterion determines how nodes are selected and coarsened graphs are constructed in the pooling layer. The resulting hierarchical structure yields an effective isomorphism-invariant representation of networked data on arbitrary topologies. The proposed strategy achieves superior or competitive performance in graph classification on a collection of public graph benchmark data sets and superpixel-induced image graph data sets.},
  archive      = {J_TNNLS},
  author       = {Xing Gao and Wenrui Dai and Chenglin Li and Hongkai Xiong and Pascal Frossard},
  doi          = {10.1109/TNNLS.2021.3067441},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5032-5044},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IPool—Information-based pooling in hierarchical graph neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGBT tracking via noise-robust cross-modal ranking.
<em>TNNLS</em>, <em>33</em>(9), 5019–5031. (<a
href="https://doi.org/10.1109/TNNLS.2021.3067107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing RGBT tracking methods usually localize a target object with a bounding box, in which the trackers are often affected by the inclusion of background clutter. To address this issue, this article presents a novel algorithm, called noise-robust cross-modal ranking, to suppress background effects in target bounding boxes for RGBT tracking. In particular, we handle the noise interference in cross-modal fusion and seed labels from the following two aspects. First, the soft cross-modality consistency is proposed to allow the sparse inconsistency in fusing different modalities, aiming to take both collaboration and heterogeneity of different modalities into account for more effective fusion. Second, the optimal seed learning is designed to handle label noises of ranking seeds caused by some problems, such as irregular object shape and occlusion. In addition, to deploy the complementarity and maintain the structural information of different features within each modality, we perform an individual ranking for each feature and employ a cross-feature consistency to pursue their collaboration. A unified optimization framework with an efficient convergence speed is developed to solve the proposed model. Extensive experiments demonstrate the effectiveness and efficiency of the proposed approach comparing with state-of-the-art tracking methods on GTOT and RGBT234 benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Chenglong Li and Zhiqiang Xiang and Jin Tang and Bin Luo and Futian Wang},
  doi          = {10.1109/TNNLS.2021.3067107},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5019-5031},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RGBT tracking via noise-robust cross-modal ranking},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BNAS: Efficient neural architecture search using broad
scalable architecture. <em>TNNLS</em>, <em>33</em>(9), 5004–5018. (<a
href="https://doi.org/10.1109/TNNLS.2021.3067028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient neural architecture search (ENAS) achieves novel efficiency for learning architecture with high-performance via parameter sharing and reinforcement learning (RL). In the phase of architecture search, ENAS employs deep scalable architecture as search space whose training process consumes most of the search cost. Moreover, time-consuming model training is proportional to the depth of deep scalable architecture. Through experiments using ENAS on CIFAR-10, we find that layer reduction of scalable architecture is an effective way to accelerate the search process of ENAS but suffers from a prohibitive performance drop in the phase of architecture estimation. In this article, we propose a broad neural architecture search (BNAS) where we elaborately design broad scalable architecture dubbed broad convolutional neural network (BCNN) to solve the above issue. On the one hand, the proposed broad scalable architecture has fast training speed due to its shallow topology. Moreover, we also adopt RL and parameter sharing used in ENAS as the optimization strategy of BNAS. Hence, the proposed approach can achieve higher search efficiency. On the other hand, the broad scalable architecture extracts multi-scale features and enhancement representations, and feeds them into global average pooling (GAP) layer to yield more reasonable and comprehensive representations. Therefore, the performance of broad scalable architecture can be promised. In particular, we also develop two variants for BNAS that modify the topology of BCNN. In order to verify the effectiveness of BNAS, several experiments are performed and experimental results show that 1) BNAS delivers 0.19 days which is $2.37\times $ less expensive than ENAS who ranks the best in RL-based NAS approaches; 2) compared with small-size (0.5 million parameters) and medium-size (1.1 million parameters) models, the architecture learned by BNAS obtains state-of-the-art performance (3.58\% and 3.24\% test error) on CIFAR-10; and 3) the learned architecture achieves 25.3\% top-1 error on ImageNet just using 3.9 million parameters.},
  archive      = {J_TNNLS},
  author       = {Zixiang Ding and Yaran Chen and Nannan Li and Dongbin Zhao and Zhiquan Sun and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2021.3067028},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {5004-5018},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BNAS: Efficient neural architecture search using broad scalable architecture},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semisupervised semantic segmentation by improving prediction
confidence. <em>TNNLS</em>, <em>33</em>(9), 4991–5003. (<a
href="https://doi.org/10.1109/TNNLS.2021.3066850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the recent image segmentation methods have tried to achieve the utmost segmentation results using large-scale pixel-level annotated data sets. However, obtaining these pixel-level annotated training data is usually tedious and expensive. In this work, we address the task of semisupervised semantic segmentation, which reduces the need for large numbers of pixel-level annotated images. We propose a method for semisupervised semantic segmentation by improving the confidence of the predicted class probability map via two parts. First, we build an adversarial framework that regards the segmentation network as the generator and uses a fully convolutional network as the discriminator. The adversarial learning makes the prediction class probability closer to 1. Second, the information entropy of the predicted class probability map is computed to represent the unpredictability of the segmentation prediction. Then, we infer the label-error map of the segmentation prediction and minimize the uncertainty on misclassified regions for unlabeled images. In contrast to existing semisupervised and weakly supervised semantic segmentation methods, the proposed method results in more confident predictions by focusing on the misclassified regions, especially the boundary regions. Our experimental results on the PASCAL VOC 2012 and PASCAL-CONTEXT data sets show that the proposed method achieves competitive segmentation performance.},
  archive      = {J_TNNLS},
  author       = {Huaian Chen and Yi Jin and Guoqiang Jin and Changan Zhu and Enhong Chen},
  doi          = {10.1109/TNNLS.2021.3066850},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4991-5003},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised semantic segmentation by improving prediction confidence},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jointly heterogeneous palmprint discriminant feature
learning. <em>TNNLS</em>, <em>33</em>(9), 4979–4990. (<a
href="https://doi.org/10.1109/TNNLS.2021.3066381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous palmprint recognition has attracted considerable research attention in recent years because it has the potential to greatly improve the recognition performance for personal authentication. In this article, we propose a simultaneous heterogeneous palmprint feature learning and encoding method for heterogeneous palmprint recognition. Unlike existing hand-crafted palmprint descriptors that usually extract features from raw pixels and require strong prior knowledge to design them, the proposed method automatically learns the discriminant binary codes from the informative direction convolution difference vectors of palmprint images. Differing from most heterogeneous palmprint descriptors that individually extract palmprint features from each modality, our method jointly learns the discriminant features from heterogeneous palmprint images so that the specific discriminant properties of different modalities can be better exploited. Furthermore, we present a general heterogeneous palmprint discriminative feature learning model to make the proposed method suitable for multiple heterogeneous palmprint recognition. Experimental results on the widely used PolyU multispectral palmprint database clearly demonstrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Lunke Fei and Bob Zhang and Yong Xu and Chunwei Tian and Imad Rida and David Zhang},
  doi          = {10.1109/TNNLS.2021.3066381},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4979-4990},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Jointly heterogeneous palmprint discriminant feature learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LU-optimality conditions in optimization problems with
mechanical work objective functionals. <em>TNNLS</em>, <em>33</em>(9),
4971–4978. (<a
href="https://doi.org/10.1109/TNNLS.2021.3066196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce interval-valued Kuhn–Tucker (KT)-pseudoinvex optimization problems governed by interval-valued path-independent curvilinear integral objective functionals. Concretely, it is proven that an interval-valued KT-pseudoinvex variational control problem is described such that every KT point is an LU-optimal solution. In addition, the main results are highlighted by two illustrative applications describing the controlled behavior of an artificial neural system.},
  archive      = {J_TNNLS},
  author       = {Savin Treanţă},
  doi          = {10.1109/TNNLS.2021.3066196},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4971-4978},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {LU-optimality conditions in optimization problems with mechanical work objective functionals},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep network quantization via error compensation.
<em>TNNLS</em>, <em>33</em>(9), 4960–4970. (<a
href="https://doi.org/10.1109/TNNLS.2021.3064293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For portable devices with limited resources, it is often difficult to deploy deep networks due to the prohibitive computational overhead. Numerous approaches have been proposed to quantize weights and/or activations to speed up the inference. Loss-aware quantization has been proposed to directly formulate the impact of weight quantization on the model’s final loss. However, we discover that, under certain circumstances, such a method may not converge and end up oscillating. To tackle this issue, we introduce a novel loss-aware quantization algorithm to efficiently compress deep networks with low bit-width model weights. We provide a more accurate estimation of gradients by leveraging the Taylor expansion to compensate for the quantization error, which leads to better convergence behavior. Our theoretical analysis indicates that the gradient mismatch issue can be fixed by the newly introduced quantization error compensation term. Experimental results for both linear models and convolutional networks verify the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Hanyu Peng and Jiaxiang Wu and Zhiwei Zhang and Shifeng Chen and Hai-Tao Zhang},
  doi          = {10.1109/TNNLS.2021.3064293},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4960-4970},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep network quantization via error compensation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tensorizing GAN with high-order pooling for alzheimer’s
disease assessment. <em>TNNLS</em>, <em>33</em>(9), 4945–4959. (<a
href="https://doi.org/10.1109/TNNLS.2021.3063516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is of great significance to apply deep learning for the early diagnosis of Alzheimer’s disease (AD). In this work, a novel tensorizing GAN with high-order pooling is proposed to assess mild cognitive impairment (MCI) and AD. By tensorizing a three-player cooperative game-based framework, the proposed model can benefit from the structural information of the brain. By incorporating the high-order pooling scheme into the classifier, the proposed model can make full use of the second-order statistics of holistic magnetic resonance imaging (MRI). To the best of our knowledge, the proposed Tensor-train, High-order pooling and Semisupervised learning-based GAN (THS-GAN) is the first work to deal with classification on MR images for AD diagnosis. Extensive experimental results on Alzheimer’s disease neuroimaging initiative (ADNI) data set are reported to demonstrate that the proposed THS-GAN achieves superior performance compared with existing methods, and to show that both tensor-train and high-order pooling can enhance classification performance. The visualization of generated samples also shows that the proposed model can generate plausible samples for semisupervised learning purpose.},
  archive      = {J_TNNLS},
  author       = {Wen Yu and Baiying Lei and Michael K. Ng and Albert C. Cheung and Yanyan Shen and Shuqiang Wang},
  doi          = {10.1109/TNNLS.2021.3063516},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4945-4959},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Tensorizing GAN with high-order pooling for alzheimer’s disease assessment},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-structured DNN weight pruning—is it beneficial in any
platform? <em>TNNLS</em>, <em>33</em>(9), 4930–4944. (<a
href="https://doi.org/10.1109/TNNLS.2021.3063265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large deep neural network (DNN) models pose the key challenge to energy efficiency due to the significantly higher energy consumption of off-chip DRAM accesses than arithmetic or SRAM operations. It motivates the intensive research on model compression with two main approaches. Weight pruning leverages the redundancy in the number of weights and can be performed in a non-structured, which has higher flexibility and pruning rate but incurs index accesses due to irregular weights, or structured manner, which preserves the full matrix structure with a lower pruning rate. Weight quantization leverages the redundancy in the number of bits in weights. Compared to pruning, quantization is much more hardware-friendly and has become a “must-do” step for FPGA and ASIC implementations. Thus, any evaluation of the effectiveness of pruning should be on top of quantization. The key open question is, with quantization, what kind of pruning (non-structured versus structured) is most beneficial? This question is fundamental because the answer will determine the design aspects that we should really focus on to avoid the diminishing return of certain optimizations. This article provides a definitive answer to the question for the first time. First, we build ADMM-NN-S by extending and enhancing ADMM-NN, a recently proposed joint weight pruning and quantization framework, with the algorithmic supports for structured pruning, dynamic ADMM regulation, and masked mapping and retraining. Second, we develop a methodology for fair and fundamental comparison of non-structured and structured pruning in terms of both storage and computation efficiency. Our results show that ADMM-NN-S consistently outperforms the prior art: 1) it achieves $348\times $ , $36\times $ , and $8\times $ overall weight pruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero accuracy loss and 2) we demonstrate the first fully binarized (for all layers) DNNs can be lossless in accuracy in many cases. These results provide a strong baseline and credibility of our study. Based on the proposed comparison framework, with the same accuracy and quantization, the results show that non-structured pruning is not competitive in terms of both storage and computation efficiency. Thus, we conclude that structured pruning has a greater potential compared to non-structured pruning. We encourage the community to focus on studying the DNN inference acceleration with structured sparsity.},
  archive      = {J_TNNLS},
  author       = {Xiaolong Ma and Sheng Lin and Shaokai Ye and Zhezhi He and Linfeng Zhang and Geng Yuan and Sia Huat Tan and Zhengang Li and Deliang Fan and Xuehai Qian and Xue Lin and Kaisheng Ma and Yanzhi Wang},
  doi          = {10.1109/TNNLS.2021.3063265},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4930-4944},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Non-structured DNN weight Pruning—Is it beneficial in any platform?},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DeepKeyGen: A deep learning-based stream cipher generator
for medical image encryption and decryption. <em>TNNLS</em>,
<em>33</em>(9), 4915–4929. (<a
href="https://doi.org/10.1109/TNNLS.2021.3062754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for medical image encryption is increasingly pronounced, for example, to safeguard the privacy of the patients’ medical imaging data. In this article, a novel deep learning-based key generation network (DeepKeyGen) is proposed as a stream cipher generator to generate the private key, which can then be used for encrypting and decrypting of medical images. In DeepKeyGen, the generative adversarial network (GAN) is adopted as the learning network to generate the private key. Furthermore, the transformation domain (that represents the “style” of the private key to be generated) is designed to guide the learning network to realize the private key generation process. The goal of DeepKeyGen is to learn the mapping relationship of how to transfer the initial image to the private key. We evaluate DeepKeyGen using three data sets, namely, the Montgomery County chest X-ray data set, the Ultrasonic Brachial Plexus data set, and the BraTS18 data set. The evaluation findings and security analysis show that the proposed key generation network can achieve a high-level security in generating the private key.},
  archive      = {J_TNNLS},
  author       = {Yi Ding and Fuyuan Tan and Zhen Qin and Mingsheng Cao and Kim-Kwang Raymond Choo and Zhiguang Qin},
  doi          = {10.1109/TNNLS.2021.3062754},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4915-4929},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DeepKeyGen: A deep learning-based stream cipher generator for medical image encryption and decryption},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatiotemporal memories for missing samples reconstruction.
<em>TNNLS</em>, <em>33</em>(9), 4900–4914. (<a
href="https://doi.org/10.1109/TNNLS.2021.3062463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a systematic theory to reconstruct missing samples in a time series using a spatiotemporal memory based on artificial neural networks. The Markov order of the input process is learned and subsequently used for learning temporal correlations from data difference sequences. We enforce the Lipschitz continuity criterion in our algorithm, leading to a regularized optimization framework for learning. The performance of the algorithm is analyzed using both theory and simulations. The efficacy of the technique is tested on synthetic and real life data sets. Our technique is analytic and uses nonlinear feedback within an optimization setup. Simulation results show that the algorithm presented in this article significantly outperforms the state-of-the-art algorithms for missing samples reconstruction with the same data set and similar training conditions.},
  archive      = {J_TNNLS},
  author       = {Prayag Gowgi and Amrutha Machireddy and Shayan Srinivasa Garani},
  doi          = {10.1109/TNNLS.2021.3062463},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4900-4914},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatiotemporal memories for missing samples reconstruction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deeply supervised convolutional neural network for
pavement crack detection with multiscale feature fusion. <em>TNNLS</em>,
<em>33</em>(9), 4890–4899. (<a
href="https://doi.org/10.1109/TNNLS.2021.3062070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic crack detection is vital for efficient and economical road maintenance. With the explosive development of convolutional neural networks (CNNs), recent crack detection methods are mostly based on CNNs. In this article, we propose a deeply supervised convolutional neural network for crack detection via a novel multiscale convolutional feature fusion module. Within this multiscale feature fusion module, the high-level features are introduced directly into the low-level features at different convolutional stages. Besides, deep supervision provides integrated direct supervision for convolutional feature fusion, which is helpful to improve model convergency and final performance of crack detection. Multiscale convolutional features learned at different convolution stages are fused together to robustly represent cracks, whose geometric structures are complicated and hardly captured by single-scale features. To demonstrate its superiority and generalizability, we evaluate the proposed network on three public crack data sets, respectively. Sufficient experimental results demonstrate that our method outperforms other state-of-the-art crack detection, edge detection, and image segmentation methods in terms of F1-score and mean IU.},
  archive      = {J_TNNLS},
  author       = {Zhong Qu and Chong Cao and Ling Liu and Dong-Yang Zhou},
  doi          = {10.1109/TNNLS.2021.3062070},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4890-4899},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deeply supervised convolutional neural network for pavement crack detection with multiscale feature fusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A segment-based drift adaptation method for data streams.
<em>TNNLS</em>, <em>33</em>(9), 4876–4889. (<a
href="https://doi.org/10.1109/TNNLS.2021.3062062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In concept drift adaptation, we aim to design a blind or an informed strategy to update our best predictor for future data at each time point. However, existing informed drift adaptation methods need to wait for an entire batch of data to detect drift and then update the predictor (if drift is detected), which causes adaptation delay. To overcome the adaptation delay, we propose a sequentially updated statistic, called drift-gradient to quantify the increase of distributional discrepancy when every new instance arrives. Based on drift-gradient, a segment-based drift adaptation (SEGA) method is developed to online update our best predictor. Drift-gradient is defined on a segment in the training set. It can precisely quantify the increase of distributional discrepancy between the old segment and the newest segment when only one new instance is available at each time point. A lower value of drift-gradient on the old segment represents that the distribution of the new instance is closer to the distribution of the old segment. Based on the drift-gradient, SEGA retrains our best predictors with the segments that have the minimum drift-gradient when every new instance arrives. SEGA has been validated by extensive experiments on both synthetic and real-world, classification and regression data streams. The experimental results show that SEGA outperforms competitive blind and informed drift adaptation methods.},
  archive      = {J_TNNLS},
  author       = {Yiliao Song and Jie Lu and Anjin Liu and Haiyan Lu and Guangquan Zhang},
  doi          = {10.1109/TNNLS.2021.3062062},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4876-4889},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A segment-based drift adaptation method for data streams},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A gradient-guided evolutionary approach to training deep
neural networks. <em>TNNLS</em>, <em>33</em>(9), 4861–4875. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been widely recognized that the efficient training of neural networks (NNs) is crucial to classification performance. While a series of gradient-based approaches have been extensively developed, they are criticized for the ease of trapping into local optima and sensitivity to hyperparameters. Due to the high robustness and wide applicability, evolutionary algorithms (EAs) have been regarded as a promising alternative for training NNs in recent years. However, EAs suffer from the curse of dimensionality and are inefficient in training deep NNs (DNNs). By inheriting the advantages of both the gradient-based approaches and EAs, this article proposes a gradient-guided evolutionary approach to train DNNs. The proposed approach suggests a novel genetic operator to optimize the weights in the search space, where the search direction is determined by the gradient of weights. Moreover, the network sparsity is considered in the proposed approach, which highly reduces the network complexity and alleviates overfitting. Experimental results on single-layer NNs, deep-layer NNs, recurrent NNs, and convolutional NNs (CNNs) demonstrate the effectiveness of the proposed approach. In short, this work not only introduces a novel approach for training DNNs but also enhances the performance of EAs in solving large-scale optimization problems.},
  archive      = {J_TNNLS},
  author       = {Shangshang Yang and Ye Tian and Cheng He and Xingyi Zhang and Kay Chen Tan and Yaochu Jin},
  doi          = {10.1109/TNNLS.2021.3061630},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4861-4875},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A gradient-guided evolutionary approach to training deep neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic, recurrent, fuzzy neural network for
processing noisy time-series data. <em>TNNLS</em>, <em>33</em>(9),
4851–4860. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapidly increasing volumes of data and the need for big data analytics have emphasized the need for algorithms that can accommodate incomplete or noisy data. The concept of recurrency is an important aspect of signal processing, providing greater robustness and accuracy in many situations, such as biological signal processing. Probabilistic fuzzy neural networks (PFNN) have shown potential in dealing with uncertainties associated with both stochastic and nonstochastic noise simultaneously. Previous research work on this topic has addressed either the fuzzy-neural aspects or alternatively the probabilistic aspects, but currently a probabilistic fuzzy neural algorithm with recurrent feedback does not exist. In this article, a PFNN with a recurrent probabilistic generation module (designated PFNN-R) is proposed to enhance and extend the ability of the PFNN to accommodate noisy data. A back-propagation-based mechanism, which is used to shape the distribution of the probabilistic density function of the fuzzy membership, is also developed. The objective of the work was to develop an approach that provides an enhanced capability to accommodate various types of noisy data. We apply the algorithm to a number of benchmark problems and demonstrate through simulation results that the proposed technique incorporating recurrency advances the ability of PFNNs to model time-series data with high intensity, random noise.},
  archive      = {J_TNNLS},
  author       = {Yong Li and Richard Gault and T. Martin McGinnity},
  doi          = {10.1109/TNNLS.2021.3061432},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4851-4860},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic, recurrent, fuzzy neural network for processing noisy time-series data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online optimal adaptive control of partially uncertain
nonlinear discrete-time systems using multilayer neural networks.
<em>TNNLS</em>, <em>33</em>(9), 4840–4850. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article intends to address an online optimal adaptive regulation of nonlinear discrete-time systems in affine form and with partially uncertain dynamics using a multilayer neural network (MNN). The actor–critic framework estimates both the optimal control input and value function. Instantaneous control input error and temporal difference are used to tune the weights of the critic and actor networks, respectively. The selection of the basis functions and their derivatives are not required in the proposed approach. The state vector, critic, and actor NN weights are proven to be bounded using the Lyapunov method. Our approach can be extended to neural networks with an arbitrary number of hidden layers. We have demonstrated our approach via a simulation example.},
  archive      = {J_TNNLS},
  author       = {Rohollah Moghadam and Pappa Natarajan and Sarangapani Jagannathan},
  doi          = {10.1109/TNNLS.2021.3061414},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4840-4850},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online optimal adaptive control of partially uncertain nonlinear discrete-time systems using multilayer neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multinetwork collaborative feature learning for
semisupervised person reidentification. <em>TNNLS</em>, <em>33</em>(9),
4826–4839. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person reidentification (Re-ID) aims at matching images of the same identity captured from the disjoint camera views, which remains a very challenging problem due to the large cross-view appearance variations. In practice, the mainstream methods usually learn a discriminative feature representation using a deep neural network, which needs a large number of labeled samples in the training process. In this article, we design a simple yet effective multinetwork collaborative feature learning (MCFL) framework to alleviate the data annotation requirement for person Re-ID, which can confidently estimate the pseudolabels of unlabeled sample pairs and consistently learn the discriminative features of input images. To keep the precision of pseudolabels, we further build a novel self-paced collaborative regularizer to extensively exchange the weight information of unlabeled sample pairs between different networks. Once the pseudolabels are correctly estimated, we take the corresponding sample pairs into the training process, which is beneficial to learn more discriminative features for person Re-ID. Extensive experimental results on the Market1501, DukeMTMC, and CUHK03 data sets have shown that our method outperforms most of the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Sanping Zhou and Jinjun Wang and Jun Shu and Deyu Meng and Le Wang and Nanning Zheng},
  doi          = {10.1109/TNNLS.2021.3061164},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4826-4839},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multinetwork collaborative feature learning for semisupervised person reidentification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Edge-based decentralized adaptive pinning synchronization of
complex networks under link attacks. <em>TNNLS</em>, <em>33</em>(9),
4815–4825. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the pinning synchronization problem with edge-based decentralized adaptive schemes under link attacks. The link attacks considered here are a class of malicious attacks to break links between neighboring nodes in complex networks. In such an insecure network environment, two kinds of edge-based decentralized adaptive update strategies (synchronous and asynchronous) on coupling strengths and gains are designed to realize the security synchronization of complex networks. Moreover, by virtue of the edge pinning technique, the corresponding secure synchronization problem is considered under the case where only a small fraction of coupling strengths and gains is updated. These designed adaptive strategies do not require any global information, and therefore, the obtained results in this article are developed in a fully decentralized framework. Finally, a numerical example is provided to verify the availability of the achieved theoretical outcomes.},
  archive      = {J_TNNLS},
  author       = {Dan Liu and Dan Ye},
  doi          = {10.1109/TNNLS.2021.3061137},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4815-4825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Edge-based decentralized adaptive pinning synchronization of complex networks under link attacks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Memory attention networks for skeleton-based action
recognition. <em>TNNLS</em>, <em>33</em>(9), 4800–4814. (<a
href="https://doi.org/10.1109/TNNLS.2021.3061115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has been extensively studied, but it remains an unsolved problem because of the complex variations of skeleton joints in 3-D spatiotemporal space. To handle this issue, we propose a newly temporal-then-spatial recalibration method named memory attention networks (MANs) and deploy MANs using the temporal attention recalibration module (TARM) and spatiotemporal convolution module (STCM). In the TARM, a novel temporal attention mechanism is built based on residual learning to recalibrate frames of skeleton data temporally. In the STCM, the recalibrated sequence is transformed or encoded as the input of CNNs to further model the spatiotemporal information of skeleton sequence. Based on MANs, a new collaborative memory fusion module (CMFM) is proposed to further improve the efficiency, leading to the collaborative MANs (C-MANs), trained with two streams of base MANs. TARM, STCM, and CMFM form a single network seamlessly and enable the whole network to be trained in an end-to-end fashion. Comparing with the state-of-the-art methods, MANs and C-MANs improve the performance significantly and achieve the best results on six data sets for action recognition. The source code has been made publicly available at https://github.com/memory-attention-networks .},
  archive      = {J_TNNLS},
  author       = {Ce Li and Chunyu Xie and Baochang Zhang and Jungong Han and Xiantong Zhen and Jie Chen},
  doi          = {10.1109/TNNLS.2021.3061115},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4800-4814},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory attention networks for skeleton-based action recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Item relationship graph neural networks for e-commerce.
<em>TNNLS</em>, <em>33</em>(9), 4785–4799. (<a
href="https://doi.org/10.1109/TNNLS.2021.3060872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a modern e-commerce recommender system, it is important to understand the relationships among products. Recognizing product relationships—such as complements or substitutes—accurately is an essential task for generating better recommendation results, as well as improving explainability in recommendation. Products and their associated relationships naturally form a product graph, yet existing efforts do not fully exploit the product graph’s topological structure. They usually only consider the information from directly connected products. In fact, the connectivity of products a few hops away also contains rich semantics and could be utilized for improved relationship prediction. In this work, we formulate the problem as a multilabel link prediction task and propose a novel graph neural network-based framework, item relationship graph neural network (IRGNN), for discovering multiple complex relationships simultaneously. We incorporate multihop relationships of products by recursively updating node embeddings using the messages from their neighbors. An edge relational network is designed to effectively capture relational information between products. Extensive experiments are conducted on real-world product data, validating the effectiveness of IRGNN, especially on large and sparse product graphs.},
  archive      = {J_TNNLS},
  author       = {Weiwen Liu and Yin Zhang and Jianling Wang and Yun He and James Caverlee and Patrick P. K. Chan and Daniel S. Yeung and Pheng-Ann Heng},
  doi          = {10.1109/TNNLS.2021.3060872},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4785-4799},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Item relationship graph neural networks for E-commerce},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shuffle GAN with autoencoder: A deep learning approach to
separate moving and stationary targets in SAR imagery. <em>TNNLS</em>,
<em>33</em>(9), 4770–4784. (<a
href="https://doi.org/10.1109/TNNLS.2021.3060747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) has been widely applied in both civilian and military fields because it provides high-resolution images of the ground target regardless of weather conditions, day or night. In SAR imaging, the separation of moving and stationary targets is of great significance as it is capable of removing the ambiguity stemming from inevitable moving targets in stationary scene imaging and suppressing clutter in moving target imaging. The newly emerged generative adversarial networks (GANs) have great performance in many other signal processing areas; however, they have not been introduced to radar imaging tasks. In this work, we propose a novel shuffle GAN with autoencoder separation method to separate the moving and stationary targets in SAR imagery. The proposed algorithm is based on the independence of well-focused stationary targets and blurred moving targets for creating adversarial constraints. Note that the algorithm operates in a totally unsupervised fashion without requiring a sample set that contains mixed and separated SAR images. Experiments are carried out on synthetic and real SAR data to validate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Wei Pu},
  doi          = {10.1109/TNNLS.2021.3060747},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4770-4784},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Shuffle GAN with autoencoder: A deep learning approach to separate moving and stationary targets in SAR imagery},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Feature-level attention-guided multitask CNN for fault
diagnosis and working conditions identification of rolling bearing.
<em>TNNLS</em>, <em>33</em>(9), 4757–4769. (<a
href="https://doi.org/10.1109/TNNLS.2021.3060494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and real-time fault diagnosis (FD) and working conditions identification (WCI) are the key to ensuring the safe operation of mechanical systems. We observe that there is a close correlation between the fault condition and the working condition in the vibration signal. Most of the intelligent FD methods only learn some features from the vibration signals and then use them to identify fault categories. They ignore the impact of working conditions on the bearing system, and such a single-task learning method cannot learn the complementary information contained in multiple related tasks. Therefore, this article is devoted to mining richer and complementary globally shared features from vibration signals to complete the FD and WCI of rolling bearings at the same time. To this end, we propose a novel multitask attention convolutional neural network (MTA-CNN) that can automatically give feature-level attention to specific tasks. The MTA-CNN consists of a global feature shared network (GFS-network) for learning globally shared features and $K$ task-specific networks with feature-level attention module (FLA-module). This architecture allows the FLA-module to automatically learn the features of specific tasks from globally shared features, thereby sharing information among different tasks. We evaluated our method on the wheelset bearing data set and motor bearing data set. The results show that our method has a better performance than the state-of-the-art deep learning methods and strongly prove that our multitask learning mechanism can improve the results of each task.},
  archive      = {J_TNNLS},
  author       = {Huan Wang and Zhiliang Liu and Dandan Peng and Mei Yang and Yong Qin},
  doi          = {10.1109/TNNLS.2021.3060494},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4757-4769},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature-level attention-guided multitask CNN for fault diagnosis and working conditions identification of rolling bearing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An integrated reinforcement learning and centralized
programming approach for online taxi dispatching. <em>TNNLS</em>,
<em>33</em>(9), 4742–4756. (<a
href="https://doi.org/10.1109/TNNLS.2021.3060187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing the supply and demand for ride-sourcing companies is a challenging issue, especially with real-time requests and stochastic traffic conditions of large-scale congested road networks. To tackle this challenge, this article proposes a robust and scalable approach that integrates reinforcement learning (RL) and a centralized programming (CP) structure to promote real-time taxi operations. Both real-time order matching decisions and vehicle relocation decisions at the microscopic network scale are integrated within a Markov decision process framework. The RL component learns the decomposed state-value function, which represents the taxi drivers’ experience, the off-line historical demand pattern, and the traffic network congestion. The CP component plans nonmyopic decisions for drivers collectively under the prescribed system constraints to explicitly realize cooperation. Furthermore, to circumvent sparse reward and sample imbalance problems over the microscopic road network, this article proposed a temporal-difference learning algorithm with prioritized gradient descent and adaptive exploration techniques. A simulator is built and trained with the Manhattan road network and New York City yellow taxi data to simulate the real-time vehicle dispatching environment. Both centralized and decentralized taxi dispatching policies are examined with the simulator. This case study shows that the proposed approach can further improve taxi drivers’ profits while reducing customers’ waiting times compared to several existing vehicle dispatching algorithms.},
  archive      = {J_TNNLS},
  author       = {Enming Liang and Kexin Wen and William H. K. Lam and Agachai Sumalee and Renxin Zhong},
  doi          = {10.1109/TNNLS.2021.3060187},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4742-4756},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An integrated reinforcement learning and centralized programming approach for online taxi dispatching},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical reinforcement learning with universal policies
for multistep robotic manipulation. <em>TNNLS</em>, <em>33</em>(9),
4727–4741. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistep tasks, such as block stacking or parts (dis)assembly, are complex for autonomous robotic manipulation. A robotic system for such tasks would need to hierarchically combine motion control at a lower level and symbolic planning at a higher level. Recently, reinforcement learning (RL)-based methods have been shown to handle robotic motion control with better flexibility and generalizability. However, these methods have limited capability to handle such complex tasks involving planning and control with many intermediate steps over a long time horizon. First, current RL systems cannot achieve varied outcomes by planning over intermediate steps (e.g., stacking blocks in different orders). Second, the exploration efficiency of learning multistep tasks is low, especially when rewards are sparse. To address these limitations, we develop a unified hierarchical reinforcement learning framework, named Universal Option Framework (UOF), to enable the agent to learn varied outcomes in multistep tasks. To improve learning efficiency, we train both symbolic planning and kinematic control policies in parallel, aided by two proposed techniques: 1) an auto-adjusting exploration strategy (AAES) at the low level to stabilize the parallel training, and 2) abstract demonstrations at the high level to accelerate convergence. To evaluate its performance, we performed experiments on various multistep block-stacking tasks with blocks of different shapes and combinations and with different degrees of freedom for robot control. The results demonstrate that our method can accomplish multistep manipulation tasks more efficiently and stably, and with significantly less memory consumption.},
  archive      = {J_TNNLS},
  author       = {Xintong Yang and Ze Ji and Jing Wu and Yu-Kun Lai and Changyun Wei and Guoliang Liu and Rossitza Setchi},
  doi          = {10.1109/TNNLS.2021.3059912},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4727-4741},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical reinforcement learning with universal policies for multistep robotic manipulation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive transition probability matrix learning for
multiview spectral clustering. <em>TNNLS</em>, <em>33</em>(9),
4712–4726. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering as an important unsupervised method has been gathering a great deal of attention. However, most multiview clustering methods exploit the self-representation property to capture the relationship among data, resulting in high computation cost in calculating the self-representation coefficients. In addition, they usually employ different regularizers to learn the representation tensor or matrix from which a transition probability matrix is constructed in a separate step, such as the one proposed by Wu et al. . Thus, an optimal transition probability matrix cannot be guaranteed. To solve these issues, we propose a unified model for multiview spectral clustering by directly learning an adaptive transition probability matrix (MCA 2 M), rather than an individual representation matrix of each view. Different from the one proposed by Wu et al. , MCA 2 M utilizes the one-step strategy to directly learn the transition probability matrix under the robust principal component analysis framework. Unlike existing methods using the absolute symmetrization operation to guarantee the nonnegativity and symmetry of the affinity matrix, the transition probability matrix learned from MCA 2 M is nonnegative and symmetric without any postprocessing. An alternating optimization algorithm is designed based on the efficient alternating direction method of multipliers. Extensive experiments on several real-world databases demonstrate that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Yongyong Chen and Xiaolin Xiao and Zhongyun Hua and Yicong Zhou},
  doi          = {10.1109/TNNLS.2021.3059874},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4712-4726},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive transition probability matrix learning for multiview spectral clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). State estimation for probabilistic boolean networks via
outputs observation. <em>TNNLS</em>, <em>33</em>(9), 4699–4711. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the state estimation for probabilistic Boolean networks via observing output sequences. Detectability describes the ability of an observer to uniquely estimate system states. By defining the probability of an observed output sequence, a new concept called detectability measure is proposed. The detectability measure is defined as the limit of the sum of probabilities of all detectable output sequences when the length of output sequences goes to infinity, and it can be regarded as a quantitative assessment of state estimation. A stochastic state estimator is designed by defining a corresponding nondeterministic stochastic finite automaton, which combines the information of state estimation and probability of output sequences. The proposed concept of detectability measure further performs the quantitative analysis on detectability. Furthermore, by defining a Markov chain, the calculation of detectability measure is converted to the calculation of the sum of probabilities of certain specific states in Markov chain. Finally, numerical examples are given to illustrate the obtained theoretical results.},
  archive      = {J_TNNLS},
  author       = {Jie Zhong and Zongxi Yu and Yuanyuan Li and Jianquan Lu},
  doi          = {10.1109/TNNLS.2021.3059795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4699-4711},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {State estimation for probabilistic boolean networks via outputs observation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive bipartite fixed-time time-varying output
formation-containment tracking of heterogeneous linear multiagent
systems. <em>TNNLS</em>, <em>33</em>(9), 4688–4698. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the bipartite fixed-time time-varying output formation-containment tracking issue for heterogeneous linear multiagent systems with multiple leaders. Both cooperative communication and antagonistic communication between neighbor agents are taken into account. First, the bipartite fixed-time compensator is put forward to estimate the convex hull of leaders’ states. Different from the existing techniques, the proposed compensator has the following three highlights: 1) it is continuous without involving the sign function, and thus, the chattering phenomenon can be avoided; 2) its estimation can be achieved within a fixed time; and 3) the communication between neighbors can not only be cooperative but also be antagonistic. Note that the proposed compensator is dependent on the global information of network topology. To deal with this issue, the fully distributed adaptive bipartite fixed-time compensator is further proposed. It can estimate not only the convex hull of leaders’ states but also the leaders’ system matrices. Based on the proposed compensators, the distributed controllers are then developed such that the bipartite time-varying output formation-containment tracking can be achieved within a fixed time. Finally, two examples are given to illustrate the feasibility of the main theoretical findings.},
  archive      = {J_TNNLS},
  author       = {Yuliang Cai and Huaguang Zhang and Yingchun Wang and Zhiyun Gao and Qiang He},
  doi          = {10.1109/TNNLS.2021.3059763},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4688-4698},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive bipartite fixed-time time-varying output formation-containment tracking of heterogeneous linear multiagent systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible body partition-based adversarial learning for
visible infrared person re-identification. <em>TNNLS</em>,
<em>33</em>(9), 4676–4687. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) aims to retrieve images of the same person across disjoint camera views. Most Re-ID studies focus on pedestrian images captured by visible cameras, without considering the infrared images obtained in the dark scenarios. Person retrieval between visible and infrared modalities is of great significance to public security. Current methods usually train a model to extract global feature descriptors and obtain discriminative representations for visible infrared person Re-ID (VI-REID). Nevertheless, they ignore the detailed information of heterogeneous pedestrian images, which affects the performance of Re-ID. In this article, we propose a flexible body partition (FBP) model-based adversarial learning method (FBP-AL) for VI-REID. To learn more fine-grained information, FBP model is exploited to automatically distinguish part representations according to the feature maps of pedestrian images. Specially, we design a modality classifier and introduce adversarial learning which attempts to discriminate features between visible and infrared modality. Adaptive weighting-based representation learning and threefold triplet loss-based metric learning compete with modality classification to obtain more effective modality-sharable features, thus shrinking the cross-modality gap and enhancing the feature discriminability. Extensive experimental results on two cross-modality person Re-ID data sets, i.e., SYSU-MM01 and RegDB, exhibit the superiority of the proposed method compared with the state-of-the-art solutions.},
  archive      = {J_TNNLS},
  author       = {Ziyu Wei and Xi Yang and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2021.3059713},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4676-4687},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flexible body partition-based adversarial learning for visible infrared person re-identification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale affine matrix rank minimization with a novel
nonconvex regularizer. <em>TNNLS</em>, <em>33</em>(9), 4661–4675. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank minimization aims to recover a matrix of minimum rank subject to linear system constraint. It can be found in various data analysis and machine learning areas, such as recommender systems, video denoising, and signal processing. Nuclear norm minimization is a dominating approach to handle it. However, such a method ignores the difference among singular values of target matrix. To address this issue, nonconvex low-rank regularizers have been widely used. Unfortunately, existing methods suffer from different drawbacks, such as inefficiency and inaccuracy. To alleviate such problems, this article proposes a flexible model with a novel nonconvex regularizer. Such a model not only promotes low rankness but also can be solved much faster and more accurate. With it, the original low-rank problem can be equivalently transformed into the resulting optimization problem under the rank restricted isometry property (rank-RIP) condition. Subsequently, Nesterov’s rule and inexact proximal strategies are adopted to achieve a novel algorithm highly efficient in solving this problem at a convergence rate of $O(1/K)$ , with $K$ being the iterate count. Besides, the asymptotic convergence rate is also analyzed rigorously by adopting the Kurdyka- ojasiewicz (KL) inequality. Furthermore, we apply the proposed optimization model to typical low-rank problems, including matrix completion, robust principal component analysis (RPCA), and tensor completion. Exhaustively empirical studies regarding data analysis tasks, i.e., synthetic data analysis, image recovery, personalized recommendation, and background subtraction, indicate that the proposed model outperforms state-of-the-art models in both accuracy and efficiency.},
  archive      = {J_TNNLS},
  author       = {Zhi Wang and Yu Liu and Xin Luo and Jianjun Wang and Chao Gao and Dezhong Peng and Wu Chen},
  doi          = {10.1109/TNNLS.2021.3059711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4661-4675},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Large-scale affine matrix rank minimization with a novel nonconvex regularizer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rollback ensemble with multiple local minima in fine-tuning
deep learning networks. <em>TNNLS</em>, <em>33</em>(9), 4648–4660. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval is a challenging problem that requires learning generalized features enough to identify untrained classes, even with very few classwise training samples. In this article, to obtain generalized features further in learning retrieval data sets, we propose a novel fine-tuning method of pretrained deep networks. In the retrieval task, we discovered a phenomenon in which the loss reduction in fine-tuning deep networks is stagnated, even while weights are largely updated. To escape from the stagnated state, we propose a new fine-tuning strategy to roll back some of the weights to the pretrained values. The rollback scheme is observed to drive the learning path to a gentle basin that provides more generalized features than a sharp basin. In addition, we propose a multihead ensemble structure to create synergy among multiple local minima obtained by our rollback scheme. Experimental results show that the proposed learning method significantly improves generalization performance, achieving state-of-the-art performance on the Inshop and SOP data sets.},
  archive      = {J_TNNLS},
  author       = {Youngmin Ro and Jongwon Choi and Byeongho Heo and Jin Young Choi},
  doi          = {10.1109/TNNLS.2021.3059669},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4648-4660},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rollback ensemble with multiple local minima in fine-tuning deep learning networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary shallowing deep neural networks at block
levels. <em>TNNLS</em>, <em>33</em>(9), 4635–4647. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have been demonstrated to be trainable even with hundreds of layers, which exhibit remarkable improvement on expressive power and provide significant performance gains in a variety of tasks. However, the prohibitive computational cost has become a severe challenge for deploying them on resource-constrained platforms. Meanwhile, widely adopted deep neural network architectures, for example, ResNets or DenseNets, are manually crafted on benchmark datasets, which hamper their generalization ability to other domains. To cope with these issues, we propose an evolutionary algorithm-based method for shallowing deep neural networks (DNNs) at block levels, which is termed as ESNB. Different from existing studies, ESNB utilizes the ensemble view of block-wise DNNs and employs the multiobjective optimization paradigm to reduce the number of blocks while avoiding performance degradation. It automatically discovers shallower network architectures by pruning less informative blocks, and employs knowledge distillation to recover the performance. Moreover, a novel prior knowledge incorporation strategy is proposed to improve the exploration ability of the evolutionary search process, and a correctness-aware knowledge distillation strategy is designed for better knowledge transferring. Experimental results show that the proposed method can effectively accelerate the inference of DNNs while achieving superior performance when compared with the state-of-the-art competing methods.},
  archive      = {J_TNNLS},
  author       = {Yao Zhou and Gary G. Yen and Zhang Yi},
  doi          = {10.1109/TNNLS.2021.3059529},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4635-4647},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Evolutionary shallowing deep neural networks at block levels},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying visible parts via pose estimation for occluded
person re-identification. <em>TNNLS</em>, <em>33</em>(9), 4624–4634. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the occlusion problem in person re-identification (re-id), which is one of the main challenges in real-world person retrieval scenarios. Previous methods on the occluded re-id problem usually assume that only the probes are occluded, thereby removing occlusions by manually cropping. However, this may not always hold in practice. This article relaxes this assumption and investigates a more general occlusion problem, where both the probe and gallery images could be occluded. The key to this challenging problem is depressing the noise information by identifying bodies and occlusions. We propose to incorporate the pose information into the re-id framework, which benefits the model in three aspects. First, it provides the location of the body. We then design a Pose-Masked Feature Branch to make our model focus on the body region only and filter those noise features brought by occlusions. Second, the estimated pose reveals which body parts are visible, giving us a hint to construct more informative person features. We propose a Pose-Embedded Feature Branch to adaptively re-calibrate channel-wise feature responses based on the visible body parts. Third, in testing, the estimated pose indicates which regions are informative and reliable for both probe and gallery images. Then we explicitly split the extracted spatial feature into parts. Only part features from those commonly visible parts are utilized in the retrieval. To better evaluate the performances of the occluded re-id, we also propose a large-scale data set for the occluded re-id with more than 35 000 images, namely Occluded-DukeMTMC. Extensive experiments show our approach surpasses previous methods on the occluded, partial, and non-occluded re-id data sets.},
  archive      = {J_TNNLS},
  author       = {Jiaxu Miao and Yu Wu and Yi Yang},
  doi          = {10.1109/TNNLS.2021.3059515},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4624-4634},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Identifying visible parts via pose estimation for occluded person re-identification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subspace clustering via structured sparse relation
representation. <em>TNNLS</em>, <em>33</em>(9), 4610–4623. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the corruptions or noises that existed in real-world data sets, the affinity graphs constructed by the classical spectral clustering-based subspace clustering algorithms may not be able to reveal the intrinsic subspace structures of data sets faithfully. In this article, we reconsidered the data reconstruction problem in spectral clustering-based algorithms and proposed the idea of “relation reconstruction.” We pointed out that a data sample could be represented by the neighborhood relation computed between its neighbors and itself. The neighborhood relation could indicate the true membership of its corresponding original data sample to the subspaces of a data set. We also claimed that a data sample’s neighborhood relation could be reconstructed by the neighborhood relations of other data samples; then, we suggested a much different way to define affinity graphs consequently. Based on these propositions, a sparse relation representation (SRR) method was proposed for solving subspace clustering problems. Moreover, by introducing the local structure information of original data sets into SRR, an extension of SRR, namely structured sparse relation representation (SSRR) was presented. We gave an optimization algorithm for solving SRR and SSRR problems and analyzed its computation burden and convergence. Finally, plentiful experiments conducted on different types of databases showed the superiorities of SRR and SSRR.},
  archive      = {J_TNNLS},
  author       = {Lai Wei and Fenfen Ji and Hao Liu and Rigui Zhou and Changming Zhu and Xiafen Zhang},
  doi          = {10.1109/TNNLS.2021.3059511},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4610-4623},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Subspace clustering via structured sparse relation representation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Input-to-state representation in linear reservoirs dynamics.
<em>TNNLS</em>, <em>33</em>(9), 4598–4609. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir computing is a popular approach to design recurrent neural networks, due to its training simplicity and approximation performance. The recurrent part of these networks is not trained (e.g., via gradient descent), making them appealing for analytical studies by a large community of researchers with backgrounds spanning from dynamical systems to neuroscience. However, even in the simple linear case, the working principle of these networks is not fully understood and their design is usually driven by heuristics. A novel analysis of the dynamics of such networks is proposed, which allows the investigator to express the state evolution using the controllability matrix. Such a matrix encodes salient characteristics of the network dynamics; in particular, its rank represents an input-independent measure of the memory capacity of the network. Using the proposed approach, it is possible to compare different reservoir architectures and explain why a cyclic topology achieves favorable results as verified by practitioners.},
  archive      = {J_TNNLS},
  author       = {Pietro Verzelli and Cesare Alippi and Lorenzo Livi and Peter Tiňo},
  doi          = {10.1109/TNNLS.2021.3059389},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4598-4609},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Input-to-state representation in linear reservoirs dynamics},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Template-free try-on image synthesis via semantic-guided
optimization. <em>TNNLS</em>, <em>33</em>(9), 4584–4597. (<a
href="https://doi.org/10.1109/TNNLS.2021.3058379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The virtual try-on task is so attractive that it has drawn considerable attention in the field of computer vision. However, presenting the 3-D physical characteristic (e.g., pleat and shadow) based on a 2-D image is very challenging. Although there have been several previous studies on 2-D-based virtual try-on work, most: 1) required user-specified target poses that are not user-friendly and may not be the best for the target clothing and 2) failed to address some problematic cases, including facial details, clothing wrinkles, and body occlusions. To address these two challenges, in this article, we propose an innovative template-free try-on image synthesis (TF-TIS) network. The TF-TIS first synthesizes the target pose according to the user-specified in-shop clothing. Afterward, given an in-shop clothing image, a user image, and a synthesized pose, we propose a novel model for synthesizing a human try-on image with the target clothing in the best fitting pose. The qualitative and quantitative experiments both indicate that the proposed TF-TIS outperforms the state-of-the-art methods, especially for difficult cases.},
  archive      = {J_TNNLS},
  author       = {Chien-Lung Chou and Chieh-Yun Chen and Chia-Wei Hsieh and Hong-Han Shuai and Jiaying Liu and Wen-Huang Cheng},
  doi          = {10.1109/TNNLS.2021.3058379},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4584-4597},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Template-free try-on image synthesis via semantic-guided optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus of positive networked systems on directed graphs.
<em>TNNLS</em>, <em>33</em>(9), 4575–4583. (<a
href="https://doi.org/10.1109/TNNLS.2021.3058184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the distributed consensus problem for identical continuous-time positive linear systems with state-feedback control. Existing works of such a problem mainly focus on the case where the networked communication topologies are of either undirected and incomplete graphs or strongly connected directed graphs. On the other hand, in this work, the communication topologies of the networked system are described by directed graphs each containing a spanning tree, which is a more general and new scenario due to the interplay between the eigenvalues of the Laplacian matrix and the controller gains. Specifically, the problem involves complex eigenvalues, the Hurwitzness of complex matrices, and positivity constraints, which make analysis difficult in the Laplacian matrix. First, a necessary and sufficient condition for the consensus analysis of directed networked systems with positivity constraints is given, by using positive systems theory and graph theory. Unlike the general Riccati design methods that involve solving an algebraic Riccati equation (ARE), a condition represented by an algebraic Riccati inequality (ARI) is obtained for the existence of a solution. Subsequently, an equivalent condition, which corresponds to the consensus design condition, is derived, and a semidefinite programming algorithm is developed. It is shown that, when a protocol is solved by the algorithm for the networked system on a specific communication graph, there exists a set of graphs such that the positive consensus problem can be solved as well.},
  archive      = {J_TNNLS},
  author       = {Jason J. R. Liu and Ka-Wai Kwok and Yukang Cui and Jun Shen and James Lam},
  doi          = {10.1109/TNNLS.2021.3058184},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4575-4583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consensus of positive networked systems on directed graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature selection boosted by unselected features.
<em>TNNLS</em>, <em>33</em>(9), 4562–4574. (<a
href="https://doi.org/10.1109/TNNLS.2021.3058172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to select strongly relevant features and discard the rest. Recently, embedded feature selection methods, which incorporate feature weights learning into the training process of a classifier, have attracted much attention. However, traditional embedded methods merely focus on the combinatorial optimality of all selected features. They sometimes select the weakly relevant features with satisfactory combination abilities and leave out some strongly relevant features, thereby degrading the generalization performance. To address this issue, we propose a novel embedded framework for feature selection, termed feature selection boosted by unselected features (FSBUF). Specifically, we introduce an extra classifier for unselected features into the traditional embedded model and jointly learn the feature weights to maximize the classification loss of unselected features. As a result, the extra classifier recycles the unselected strongly relevant features to replace the weakly relevant features in the selected feature subset. Our final objective can be formulated as a minimax optimization problem, and we design an effective gradient-based algorithm to solve it. Furthermore, we theoretically prove that the proposed FSBUF is able to improve the generalization ability of traditional embedded feature selection methods. Extensive experiments on synthetic and real-world data sets exhibit the comprehensibility and superior performance of FSBUF.},
  archive      = {J_TNNLS},
  author       = {Wei Zheng and Shuo Chen and Zhenyong Fu and Fa Zhu and Hui Yan and Jian Yang},
  doi          = {10.1109/TNNLS.2021.3058172},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4562-4574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature selection boosted by unselected features},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural networks enhanced optimal admittance control of
robot–environment interaction using reinforcement learning.
<em>TNNLS</em>, <em>33</em>(9), 4551–4561. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an adaptive admittance control scheme is developed for robots to interact with time-varying environments. Admittance control is adopted to achieve a compliant physical robot–environment interaction, and the uncertain environment with time-varying dynamics is defined as a linear system. A critic learning method is used to obtain the desired admittance parameters based on the cost function composed of interaction force and trajectory tracking without the knowledge of the environmental dynamics. To deal with dynamic uncertainties in the control system, a neural-network (NN)-based adaptive controller with a dynamic learning framework is developed to guarantee the trajectory tracking performance. Experiments are conducted and the results have verified the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Guangzhu Peng and C. L. Philip Chen and Chenguang Yang},
  doi          = {10.1109/TNNLS.2021.3057958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4551-4561},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural networks enhanced optimal admittance control of Robot–Environment interaction using reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning for fashion style generation. <em>TNNLS</em>,
<em>33</em>(9), 4538–4550. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we work on generating fashion style images with deep neural network algorithms. Given a garment image, and single or multiple style images (e.g., flower, blue and white porcelain), it is a challenge to generate a synthesized clothing image with single or mix-and-match styles due to the need to preserve global clothing contents with coverable styles, to achieve high fidelity of local details, and to conform different styles with specific areas. To address this challenge, we propose a fashion style generator (FashionG) framework for the single-style generation and a spatially constrained FashionG (SC-FashionG) framework for mix-and-match style generation. Both FashionG and SC-FashionG are end-to-end feedforward neural networks that consist of a generator for image transformation and a discriminator for preserving content and style globally and locally. Specifically, a global-based loss is calculated based on full images, which can preserve the global clothing form and design. A patch-based loss is calculated based on image patches, which can preserve detailed local style patterns. We develop an alternating patch-global optimization methodology to minimize these losses. Compared with FashionG, SC-FashionG employs an additional spatial constraint to ensure that each style is blended only onto a specific area of the clothing image. Extensive experiments demonstrate the effectiveness of both single-style and mix-and-match style generations.},
  archive      = {J_TNNLS},
  author       = {Shuhui Jiang and Jun Li and Yun Fu},
  doi          = {10.1109/TNNLS.2021.3057892},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4538-4550},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep learning for fashion style generation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local stability of wasserstein GANs with abstract gradient
penalty. <em>TNNLS</em>, <em>33</em>(9), 4527–4537. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence of generative adversarial networks (GANs) has been studied substantially in various aspects to achieve successful generative tasks. Ever since it is first proposed, the idea has achieved many theoretical improvements by injecting an instance noise, choosing different divergences, penalizing the discriminator, and so on. In essence, these efforts are to approximate a real-world measure with an idle measure through a learning procedure. In this article, we provide an analysis of GANs in the most general setting to reveal what, in essence, should be satisfied to achieve successful convergence. This work is not trivial since handling a converging sequence of an abstract measure requires a lot more sophisticated concepts. In doing so, we find an interesting fact that the discriminator can be penalized in a more general setting than what has been implemented. Furthermore, our experiment results substantiate our theoretical argument on various generative tasks.},
  archive      = {J_TNNLS},
  author       = {Cheolhyeong Kim and Seungtae Park and Hyung Ju Hwang},
  doi          = {10.1109/TNNLS.2021.3057885},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4527-4537},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local stability of wasserstein GANs with abstract gradient penalty},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multistability and stabilization of fractional-order
competitive neural networks with unbounded time-varying delays.
<em>TNNLS</em>, <em>33</em>(9), 4515–4526. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the multistability and stabilization of fractional-order competitive neural networks (FOCNNs) with unbounded time-varying delays. By utilizing the monotone operator, several sufficient conditions of the coexistence of equilibrium points (EPs) are obtained for FOCNNs with concave–convex activation functions. And then, the multiple $\mu $ -stability of delayed FOCNNs is derived by the analytical method. Meanwhile, several comparisons with existing work are shown, which implies that the derived results cover the inverse-power stability and Mittag–Leffler stability as special cases. Moreover, the criteria on the stabilization of FOCNNs with uncertainty are established by designing a controller. Compared with the results of fractional-order neural networks, the obtained results in this article enrich and improve the previous results. Finally, three numerical examples are provided to show the effectiveness of the presented results.},
  archive      = {J_TNNLS},
  author       = {Fanghai Zhang and Zhigang Zeng},
  doi          = {10.1109/TNNLS.2021.3057861},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4515-4526},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multistability and stabilization of fractional-order competitive neural networks with unbounded time-varying delays},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical semantic graph reasoning for train component
detection. <em>TNNLS</em>, <em>33</em>(9), 4502–4514. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based approaches have achieved superior performance on object detection applications. However, object detection for industrial scenarios, where the objects may also have some structures and the structured patterns are normally presented in a hierarchical way, is not well investigated yet. In this work, we propose a novel deep learning-based method, hierarchical graphical reasoning (HGR), which utilizes the hierarchical structures of trains for train component detection. HGR contains multiple graphical reasoning branches, each of which is utilized to conduct graphical reasoning for one cluster of train components based on their sizes. In each branch, the visual appearances and structures of train components are considered jointly with our proposed novel densely connected dual-gated recurrent units (Dense-DGRUs). To the best of our knowledge, HGR is the first kind of framework that explores hierarchical structures among objects for object detection. We have collected a data set of 1130 images captured from moving trains, in which 17 334 train components are manually annotated with bounding boxes. Based on this data set, we carry out extensive experiments that have demonstrated our proposed HGR outperforms the existing state-of-the-art baselines significantly. The data set and the source code can be downloaded online at https://github.com/ChengZY/HGR .},
  archive      = {J_TNNLS},
  author       = {Cen Chen and Kenli Li and Xiaofeng Zou and Zhongyao Cheng and Wei Wei and Qi Tian and Zeng Zeng},
  doi          = {10.1109/TNNLS.2021.3057792},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4502-4514},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical semantic graph reasoning for train component detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pinning impulsive synchronization of stochastic delayed
neural networks via uniformly stable function. <em>TNNLS</em>,
<em>33</em>(9), 4491–4501. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the synchronization of stochastic delayed neural networks under pinning impulsive control, where a small fraction of nodes are selected as the pinned nodes at each impulsive moment. By proposing a uniformly stable function as a new tool, some novel mean square decay results are presented to analyze the error system obtained from the leader and the considered neural networks. For the divergent error system without impulsive effects, the impulsive gains of pinning impulsive controller can admit destabilizing impulse and the number of destabilizing impulse may be infinite. However, if the error system without impulsive effects is convergent, to achieve the synchronization of the stochastic neural networks, the growth exponent of the product of impulsive gains can not exceed some positive constant. It is shown that the obtained results increase the flexibility of the impulsive gains compared with the existing results. Finally, a numerical example is given to illustrate the practicality of synchronization criteria.},
  archive      = {J_TNNLS},
  author       = {Lijun Pan and Qiang Song and Jinde Cao and Minvydas Ragulskis},
  doi          = {10.1109/TNNLS.2021.3057490},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4491-4501},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pinning impulsive synchronization of stochastic delayed neural networks via uniformly stable function},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prescribed performance fault-tolerant control for uncertain
nonlinear MIMO system using actor–critic learning structure.
<em>TNNLS</em>, <em>33</em>(9), 4479–4490. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the prescribed performance fault-tolerant control problem for a class of uncertain nonlinear multi-input and multioutput systems. A learning-based fault-tolerant controller is proposed to achieve the asymptotic stability, without requiring a priori knowledge of the system dynamics. To deal with the prescribed performance, a new error transformation function is introduced to convert the constrained error dynamics into an equivalent unconstrained one. Under the actor–critic learning structure, a continuous-time long-term performance index is presented to evaluate the current control behavior. Then, a critic network is used to approximate the designed performance index and provide a reinforcement signal to the action network. Based on the robust integral of the sign of error feedback control method, an action network-based controller is developed. It is shown by the Lyapunov approach that the tracking error can converge to zero asymptotically with the prescribed performance guaranteed. Simulation results are provided to validate the feasibility and effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Xuerao Wang and Qingling Wang and Changyin Sun},
  doi          = {10.1109/TNNLS.2021.3057482},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4479-4490},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prescribed performance fault-tolerant control for uncertain nonlinear MIMO system using Actor–Critic learning structure},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A two-stream continual learning system with variational
domain-agnostic feature replay. <em>TNNLS</em>, <em>33</em>(9),
4466–4478. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning in nonstationary environments is one of the biggest challenges in machine learning. Nonstationarity can be caused by either task drift, i.e., the drift in the conditional distribution of labels given the input data, or the domain drift, i.e., the drift in the marginal distribution of the input data. This article aims to tackle this challenge with a modularized two-stream continual learning (CL) system, where the model is required to learn new tasks from a support stream and adapted to new domains in the query stream while maintaining previously learned knowledge. To deal with both drifts within and across the two streams, we propose a variational domain-agnostic feature replay-based approach that decouples the system into three modules: an inference module that filters the input data from the two streams into domain-agnostic representations, a generative module that facilitates the high-level knowledge transfer, and a solver module that applies the filtered and transferable knowledge to solve the queries. We demonstrate the effectiveness of our proposed approach in addressing the two fundamental scenarios and complex scenarios in two-stream CL.},
  archive      = {J_TNNLS},
  author       = {Qicheng Lao and Xiang Jiang and Mohammad Havaei and Yoshua Bengio},
  doi          = {10.1109/TNNLS.2021.3057453},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4466-4478},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A two-stream continual learning system with variational domain-agnostic feature replay},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid structural sparsification error model for image
restoration. <em>TNNLS</em>, <em>33</em>(9), 4451–4465. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works on structural sparse representation (SSR), which exploit image nonlocal self-similarity (NSS) prior by grouping similar patches for processing, have demonstrated promising performance in various image restoration applications. However, conventional SSR-based image restoration methods directly fit the dictionaries or transforms to the internal (corrupted) image data. The trained internal models inevitably suffer from overfitting to data corruption, thus generating the degraded restoration results. In this article, we propose a novel hybrid structural sparsification error (HSSE) model for image restoration, which jointly exploits image NSS prior using both the internal and external image data that provide complementary information. Furthermore, we propose a general image restoration scheme based on the HSSE model, and an alternating minimization algorithm for a range of image restoration applications, including image inpainting, image compressive sensing and image deblocking. Extensive experiments are conducted to demonstrate that the proposed HSSE-based scheme outperforms many popular or state-of-the-art image restoration methods in terms of both objective metrics and visual perception.},
  archive      = {J_TNNLS},
  author       = {Zhiyuan Zha and Bihan Wen and Xin Yuan and Jiantao Zhou and Ce Zhu and Alex Chichung Kot},
  doi          = {10.1109/TNNLS.2021.3057439},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4451-4465},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid structural sparsification error model for image restoration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive learning and sampled-control for nonlinear game
systems using dynamic event-triggering strategy. <em>TNNLS</em>,
<em>33</em>(9), 4437–4450. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static event-triggering-based control problems have been investigated when implementing adaptive dynamic programming algorithms. The related triggering rules are only current state-dependent without considering previous values. This motivates our improvements. This article aims to provide an explicit formulation for dynamic event-triggering that guarantees asymptotic stability of the event-sampled nonzero-sum differential game system and desirable approximation of critic neural networks. This article first deduces the static triggering rule by processing the coupling terms of Hamilton–Jacobi equations, and then, Zeno-free behavior is realized by devising an exponential term. Subsequently, a novel dynamic-triggering rule is devised into the adaptive learning stage by defining a dynamic variable, which is mathematically characterized by a first-order filter. Moreover, mathematical proofs illustrate the system stability and the weight convergence. Theoretical analysis reveals the characteristics of dynamic rule and its relations with the static rules. Finally, a numerical example is presented to substantiate the established claims. The comparative simulation results confirm that both static and dynamic strategies can reduce the communication that arises in the control loops, while the latter undertakes less communication burden due to fewer triggered events.},
  archive      = {J_TNNLS},
  author       = {Chaoxu Mu and Ke Wang and Zhen Ni},
  doi          = {10.1109/TNNLS.2021.3057438},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4437-4450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive learning and sampled-control for nonlinear game systems using dynamic event-triggering strategy},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local critic training for model-parallel learning of deep
neural networks. <em>TNNLS</em>, <em>33</em>(9), 4424–4436. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel model-parallel learning method, called local critic training , which trains neural networks using additional modules called local critic networks . The main network is divided into several layer groups, and each layer group is updated through error gradients estimated by the corresponding local critic network. We show that the proposed approach successfully decouples the update process of the layer groups for both convolutional neural networks (CNNs) and recurrent neural networks (RNNs). In addition, we demonstrate that the proposed method is guaranteed to converge to a critical point. We also show that trained networks by the proposed method can be used for structural optimization. Experimental results show that our method achieves satisfactory performance, reduces training time greatly, and decreases memory consumption per machine. Code is available at https://github.com/hjdw2/Local-critic-training .},
  archive      = {J_TNNLS},
  author       = {Hojung Lee and Cho-Jui Hsieh and Jong-Seok Lee},
  doi          = {10.1109/TNNLS.2021.3057380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4424-4436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Local critic training for model-parallel learning of deep neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint label inference and discriminant embedding.
<em>TNNLS</em>, <em>33</em>(9), 4413–4423. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based learning in semisupervised models provides an effective tool for modeling big data sets in high-dimensional spaces. It has been useful for propagating a small set of initial labels to a large set of unlabeled data. Thus, it meets the requirements of many emerging applications. However, in real-world applications, the scarcity of labeled data can negatively affect the performance of the semisupervised method. In this article, we present a new framework for semisupervised learning called joint label inference and discriminant embedding for soft label inference and linear feature extraction. The proposed criterion and its associated optimization algorithm take advantage of both labeled and unlabeled data samples in order to estimate the discriminant transformation. This type of criterion should allow learning more discriminant semisupervised models. Nine public image data sets are used in the experiments and method comparisons. These experimental results show that the performance of the proposed method is superior to that of many advanced semisupervised graph-based algorithms.},
  archive      = {J_TNNLS},
  author       = {Fadi Dornaika and Abdullah Baradaaji and Youssof El Traboulsi},
  doi          = {10.1109/TNNLS.2021.3057270},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4413-4423},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint label inference and discriminant embedding},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CerebelluMorphic: Large-scale neuromorphic model and
architecture for supervised motor learning. <em>TNNLS</em>,
<em>33</em>(9), 4398–4412. (<a
href="https://doi.org/10.1109/TNNLS.2021.3057070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cerebellum plays a vital role in motor learning and control with supervised learning capability, while neuromorphic engineering devises diverse approaches to high-performance computation inspired by biological neural systems. This article presents a large-scale cerebellar network model for supervised learning, as well as a cerebellum-inspired neuromorphic architecture to map the cerebellar anatomical structure into the large-scale model. Our multinucleus model and its underpinning architecture contain approximately 3.5 million neurons, upscaling state-of-the-art neuromorphic designs by over 34 times. Besides, the proposed model and architecture incorporate 3411k granule cells, introducing a 284 times increase compared to a previous study including only 12k cells. This large scaling induces more biologically plausible cerebellar divergence/convergence ratios, which results in better mimicking biology. In order to verify the functionality of our proposed model and demonstrate its strong biomimicry, a reconfigurable neuromorphic system is used, on which our developed architecture is realized to replicate cerebellar dynamics during the optokinetic response. In addition, our neuromorphic architecture is used to analyze the dynamical synchronization within the Purkinje cells, revealing the effects of firing rates of mossy fibers on the resonance dynamics of Purkinje cells. Our experiments show that real-time operation can be realized, with a system throughput of up to 4.70 times larger than previous works with high synaptic event rate. These results suggest that the proposed work provides both a theoretical basis and a neuromorphic engineering perspective for brain-inspired computing and the further exploration of cerebellar learning.},
  archive      = {J_TNNLS},
  author       = {Shuangming Yang and Jiang Wang and Nan Zhang and Bin Deng and Yanwei Pang and Mostafa Rahimi Azghadi},
  doi          = {10.1109/TNNLS.2021.3057070},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4398-4412},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CerebelluMorphic: Large-scale neuromorphic model and architecture for supervised motor learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Faster stochastic quasi-newton methods. <em>TNNLS</em>,
<em>33</em>(9), 4388–4397. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization methods have become a class of popular optimization tools in machine learning. Especially, stochastic gradient descent (SGD) has been widely used for machine learning problems, such as training neural networks, due to low per-iteration computational complexity. In fact, the Newton or quasi-newton (QN) methods leveraging the second-order information are able to achieve a better solution than the first-order methods. Thus, stochastic QN (SQN) methods have been developed to achieve a better solution efficiently than the stochastic first-order methods by utilizing approximate second-order information. However, the existing SQN methods still do not reach the best known stochastic first-order oracle (SFO) complexity. To fill this gap, we propose a novel faster stochastic QN method (SpiderSQN) based on the variance reduced technique of SIPDER. We prove that our SpiderSQN method reaches the best known SFO complexity of $\mathcal {O}(n+n^{1/2}\epsilon ^{-2})$ in the finite-sum setting to obtain an $\epsilon $ -first-order stationary point. To further improve its practical performance, we incorporate SpiderSQN with different momentum schemes. Moreover, the proposed algorithms are generalized to the online setting, and the corresponding SFO complexity of $\mathcal {O}(\epsilon ^{-3})$ is developed, which also matches the existing best result. Extensive experiments on benchmark data sets demonstrate that our new algorithms outperform state-of-the-art approaches for nonconvex optimization.},
  archive      = {J_TNNLS},
  author       = {Qingsong Zhang and Feihu Huang and Cheng Deng and Heng Huang},
  doi          = {10.1109/TNNLS.2021.3056947},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4388-4397},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Faster stochastic quasi-newton methods},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regular polytope networks. <em>TNNLS</em>, <em>33</em>(9),
4373–4387. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are widely used as a model for classification in a large variety of tasks. Typically, a learnable transformation (i.e., the classifier) is placed at the end of such models returning a value for each class used for classification. This transformation plays an important role in determining how the generated features change during the learning process. In this work, we argue that this transformation not only can be fixed (i.e., set as nontrainable) with no loss of accuracy and with a reduction in memory usage, but it can also be used to learn stationary and maximally separated embeddings. We show that the stationarity of the embedding and its maximal separated representation can be theoretically justified by setting the weights of the fixed classifier to values taken from the coordinate vertices of the three regular polytopes available in $\mathbb {R}^{d}$ , namely, the $d$ -Simplex, the $d$ -Cube, and the $d$ -Orthoplex. These regular polytopes have the maximal amount of symmetry that can be exploited to generate stationary features angularly centered around their corresponding fixed weights. Our approach improves and broadens the concept of a fixed classifier, recently proposed by Hoffer et al. , to a larger class of fixed classifier models. Experimental results confirm the theoretical analysis, the generalization capability, the faster convergence, and the improved performance of the proposed method. Code will be publicly available.},
  archive      = {J_TNNLS},
  author       = {Federico Pernici and Matteo Bruni and Claudio Baecchi and Alberto Del Bimbo},
  doi          = {10.1109/TNNLS.2021.3056762},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4373-4387},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Regular polytope networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perturbation of spike timing benefits neural network
performance on similarity search. <em>TNNLS</em>, <em>33</em>(9),
4361–4372. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perturbation has a positive effect, as it contributes to the stability of neural systems through adaptation and robustness. For example, deep reinforcement learning generally engages in exploratory behavior by injecting noise into the action space and network parameters. It can consistently increase the agent’s exploration ability and lead to richer sets of behaviors. Evolutionary strategies also apply parameter perturbations, which makes network architecture robust and diverse. Our main concern is whether the notion of synaptic perturbation introduced in a spiking neural network (SNN) is biologically relevant or if novel frameworks and components are desired to account for the perturbation properties of artificial neural systems. In this work, we first review part of the locality-sensitive hashing (LSH) of similarity search, the FLY algorithm, as recently published in Science , and propose an improved architecture, time-shifted spiking LSH (TS-SLSH), with the consideration of temporal perturbations of the firing moments of spike pulses. Experiment results show promising performance of the proposed method and demonstrate its generality to various spiking neuron models. Therefore, we expect temporal perturbation to play an active role in SNN performance.},
  archive      = {J_TNNLS},
  author       = {Ziru Wang and Jiawen Liu and Yongqiang Ma and Badong Chen and Nanning Zheng and Pengju Ren},
  doi          = {10.1109/TNNLS.2021.3056694},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4361-4372},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Perturbation of spike timing benefits neural network performance on similarity search},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stability of sliding mode ILC design for a class of
nonlinear systems with unknown control input delay. <em>TNNLS</em>,
<em>33</em>(9), 4346–4360. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studied the stability and convergence of a robust iterative learning control (ILC) design for a class of nonlinear systems with unknown control input delay. First, the iterative integral sliding mode (IISM) design was proposed, which comprised iterative actions. The iterative action made the convergence of the tracking error under the ideal sliding mode. Then, a suitable iterative update law was provided for the IISM-based robust ILC controller. The controller had the capability of both minimizing the steady tracking error and suppressing the unrepeatable disturbance. Using the controller, the closed-loop system stability was analyzed, and the stability conditions were given. Consequently, the sliding mode convergence in the iteration domain was proved by a composite energy function (CEF). In addition, by analyzing the influence of affection on the tracking error, several measures were taken to solve the chattering problem of the sliding mode control. Finally, a one-link robotic manipulator and a vertical three-tank system were used to verify the control design. The application simulations validated the performance of the proposed sliding mode iterative learning control (SMILC) design, which achieved the stability of the nonlinear system and overcame the control input time delay.},
  archive      = {J_TNNLS},
  author       = {Xiaoyu Zhang and Richard W. Longman},
  doi          = {10.1109/TNNLS.2021.3056680},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4346-4360},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stability of sliding mode ILC design for a class of nonlinear systems with unknown control input delay},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-emotion-enhanced convolutional LSTM for sentiment
analysis. <em>TNNLS</em>, <em>33</em>(9), 4332–4345. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) neural networks and attention mechanism have been widely used in sentiment representation learning and detection of texts. However, most of the existing deep learning models for text sentiment analysis ignore emotion’s modulation effect on sentiment feature extraction, and the attention mechanisms of these deep neural network architectures are based on word- or sentence-level abstractions. Ignoring higher level abstractions may pose a negative effect on learning text sentiment features and further degrade sentiment classification performance. To address this issue, in this article, a novel model named AEC-LSTM is proposed for text sentiment detection, which aims to improve the LSTM network by integrating emotional intelligence (EI) and attention mechanism. Specifically, an emotion-enhanced LSTM, named ELSTM, is first devised by utilizing EI to improve the feature learning ability of LSTM networks, which accomplishes its emotion modulation of learning system via the proposed emotion modulator and emotion estimator. In order to better capture various structure patterns in text sequence, ELSTM is further integrated with other operations, including convolution, pooling, and concatenation. Then, topic-level attention mechanism is proposed to adaptively adjust the weight of text hidden representation. With the introduction of EI and attention mechanism, sentiment representation and classification can be more effectively achieved by utilizing sentiment semantic information hidden in text topic and context. Experiments on real-world data sets show that our approach can improve sentiment classification performance effectively and outperform state-of-the-art deep learning-based methods significantly.},
  archive      = {J_TNNLS},
  author       = {Faliang Huang and Xuelong Li and Changan Yuan and Shichao Zhang and Jilian Zhang and Shaojie Qiao},
  doi          = {10.1109/TNNLS.2021.3056664},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4332-4345},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attention-emotion-enhanced convolutional LSTM for sentiment analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer-based NN control for nonlinear systems with
full-state constraints and external disturbances. <em>TNNLS</em>,
<em>33</em>(9), 4322–4331. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For full-state constrained nonlinear systems with input saturation, this article studies the output-feedback tracking control under the condition that the states and external disturbances are both unmeasurable. A novel composite observer consisting of state observer and disturbance observer is designed to deal with the unmeasurable states and disturbances simultaneously. Distinct from the related literature, an auxiliary system with approximate coordinate transformation is used to attenuate the effects generated by input saturation. Then, using radial basis function neural networks (RBF NNs) and the barrier Lyapunov function (BLF), an opportune backstepping design procedure is given with employing the dynamic surface control (DSC) to avoid the problem of “explosion of complexity.” Based on the given design procedure, an output-feedback controller is constructed and guarantees all the signals in the closed-loop system are semiglobally uniformly ultimately bounded. It is shown that the tracking error is regulated by the saturated input error and design parameters without the violation of the state constraints. Finally, a simulation example of a robot arm is given to demonstrate the effectiveness of the proposed controller.},
  archive      = {J_TNNLS},
  author       = {Huifang Min and Shengyuan Xu and Shumin Fei and Xin Yu},
  doi          = {10.1109/TNNLS.2021.3056524},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4322-4331},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based NN control for nonlinear systems with full-state constraints and external disturbances},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiview multi-instance multilabel active learning.
<em>TNNLS</em>, <em>33</em>(9), 4311–4321. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview multi-instance multilabel learning (M3L) is a framework for modeling complex objects. In this framework, each object (or bag) contains one or more instances, is represented with different feature views, and simultaneously annotated with a set of nonexclusive semantic labels. Given the multiplicity of the studied objects, traditional M3L methods generally demand a large number of labeled bags to train a predictive model to annotate bags (or instances) with semantic labels. However, annotating sufficient bags is very expensive and often impractical. In this article, we present an active learning-based M3L approach (M3AL) to reduce the labeling costs of bags and to improve the performance as much as possible. M3AL first adapts the multiview self-representation learning to evacuate the shared and individual information of bags and to learn the shared/individual similarities between bags across/within views. Next, to avoid scrutinizing all the possible labels, M3AL introduces a new query strategy that leverages the shared and individual information, and the diverse instance distribution of bags across views, to select the most informative bag-label pair for the query. Experimental studies on benchmark data sets show that M3AL can significantly reduce the query costs while achieving a better performance than other related competitive methods at the same cost.},
  archive      = {J_TNNLS},
  author       = {Guoxian Yu and Yuying Xing and Jun Wang and Carlotta Domeniconi and Xiangliang Zhang},
  doi          = {10.1109/TNNLS.2021.3056436},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4311-4321},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiview multi-instance multilabel active learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Maximum joint probability with multiple representations for
clustering. <em>TNNLS</em>, <em>33</em>(9), 4300–4310. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical generative models in unsupervised learning intend to maximize $p(X)$ . In practice, samples may have multiple representations caused by various transformations, measurements, and so on. Therefore, it is crucial to integrate information from different representations, and lots of models have been developed. However, most of them fail to incorporate the prior information about data distribution $p(X)$ to distinguish representations. In this article, we propose a novel clustering framework that attempts to maximize the joint probability of data and parameters. Under this framework, the prior distribution can be employed to measure the rationality of diverse representations. $K$ -means is a special case of the proposed framework. Meanwhile, a specific clustering model considering both multiple kernels and multiple views is derived to verify the validity of the designed framework and model.},
  archive      = {J_TNNLS},
  author       = {Rui Zhang and Hongyuan Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3056420},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4300-4310},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Maximum joint probability with multiple representations for clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stigmergic independent reinforcement learning for multiagent
collaboration. <em>TNNLS</em>, <em>33</em>(9), 4285–4299. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid evolution of wireless mobile devices, there emerges an increased need to design effective collaboration mechanisms between intelligent agents to gradually approach the final collective objective by continuously learning from the environment based on their individual observations. In this regard, independent reinforcement learning (IRL) is often deployed in multiagent collaboration to alleviate the problem of a nonstationary learning environment. However, behavioral strategies of intelligent agents in IRL can be formulated only upon their local individual observations of the global environment, and appropriate communication mechanisms must be introduced to reduce their behavioral localities. In this article, we address the problem of communication between intelligent agents in IRL by jointly adopting mechanisms with two different scales. For the large scale, we introduce the stigmergy mechanism as an indirect communication bridge between independent learning agents, and carefully design a mathematical method to indicate the impact of digital pheromone. For the small scale, we propose a conflict-avoidance mechanism between adjacent agents by implementing an additionally embedded neural network to provide more opportunities for participants with higher action priorities. In addition, we present a federal training method to effectively optimize the neural network of each agent in a decentralized manner. Finally, we establish a simulation scenario in which a number of mobile agents in a certain area move automatically to form a specified target shape. Extensive simulations demonstrate the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Xing Xu and Rongpeng Li and Zhifeng Zhao and Honggang Zhang},
  doi          = {10.1109/TNNLS.2021.3056418},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4285-4299},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stigmergic independent reinforcement learning for multiagent collaboration},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from architectural redundancy: Enhanced deep
supervision in deep multipath encoder–decoder networks. <em>TNNLS</em>,
<em>33</em>(9), 4271–4284. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep encoder–decoders are the model of choice for pixel-level estimation due to their redundant deep architectures. Yet they still suffer from the vanishing supervision information issue that affects convergence because of their overly deep architectures. In this work, we propose and theoretically derive an enhanced deep supervision (EDS) method which improves on conventional deep supervision (DS) by incorporating variance minimization into the optimization. A new structure variance loss is introduced to build a bridge between deep encoder–decoders and variance minimization, and provides a new way to minimize the variance by forcing different intermediate decoding outputs (paths) to reach an agreement. We also design a focal weighting strategy to effectively combine multiple losses in a scale-balanced way, so that the supervision information is sufficiently enforced throughout the encoder–decoders. To evaluate the proposed method on the pixel-level estimation task, a novel multipath residual encoder is proposed and extensive experiments are conducted on four challenging density estimation and crowd counting benchmarks. The experimental results demonstrate the superiority of our EDS over other paradigms, and improved estimation performance is reported using our deeply supervised encoder–decoder.},
  archive      = {J_TNNLS},
  author       = {Ying Luo and Jinhu Lü and Xiaolong Jiang and Baochang Zhang},
  doi          = {10.1109/TNNLS.2021.3056384},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4271-4284},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning from architectural redundancy: Enhanced deep supervision in deep multipath Encoder–Decoder networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Commonality autoencoder: Learning common features for
change detection from heterogeneous images. <em>TNNLS</em>,
<em>33</em>(9), 4257–4270. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection based on heterogeneous images, such as optical images and synthetic aperture radar images, is a challenging problem because of their huge appearance differences. To combat this problem, we propose an unsupervised change detection method that contains only a convolutional autoencoder (CAE) for feature extraction and the commonality autoencoder for commonalities exploration. The CAE can eliminate a large part of redundancies in two heterogeneous images and obtain more consistent feature representations. The proposed commonality autoencoder has the ability to discover common features of ground objects between two heterogeneous images by transforming one heterogeneous image representation into another. The unchanged regions with the same ground objects share much more common features than the changed regions. Therefore, the number of common features can indicate changed regions and unchanged regions, and then a difference map can be calculated. At last, the change detection result is generated by applying a segmentation algorithm to the difference map. In our method, the network parameters of the commonality autoencoder are learned by the relevance of unchanged regions instead of the labels. Our experimental results on five real data sets demonstrate the promising performance of the proposed framework compared with several existing approaches.},
  archive      = {J_TNNLS},
  author       = {Yue Wu and Jiaheng Li and Yongzhe Yuan and A. K. Qin and Qi-Guang Miao and Mao-Guo Gong},
  doi          = {10.1109/TNNLS.2021.3056238},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4257-4270},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Commonality autoencoder: Learning common features for change detection from heterogeneous images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Overcoming long-term catastrophic forgetting through
adversarial neural pruning and synaptic consolidation. <em>TNNLS</em>,
<em>33</em>(9), 4243–4256. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling a neural network to sequentially learn multiple tasks is of great significance for expanding the applicability of neural networks in real-world applications. However, artificial neural networks face the well-known problem of catastrophic forgetting. What is worse, the degradation of previously learned skills becomes more severe as the task sequence increases, known as the long-term catastrophic forgetting. It is due to two facts: first, as the model learns more tasks, the intersection of the low-error parameter subspace satisfying for these tasks becomes smaller or even does not exist; second, when the model learns a new task, the cumulative error keeps increasing as the model tries to protect the parameter configuration of previous tasks from interference. Inspired by the memory consolidation mechanism in mammalian brains with synaptic plasticity, we propose a confrontation mechanism in which Adversarial Neural Pruning and synaptic Consolidation (ANPyC) is used to overcome the long-term catastrophic forgetting issue. The neural pruning acts as long-term depression to prune task-irrelevant parameters, while the novel synaptic consolidation acts as long-term potentiation to strengthen task-relevant parameters. During the training, this confrontation achieves a balance in that only crucial parameters remain, and non-significant parameters are freed to learn subsequent tasks. ANPyC avoids forgetting important information and makes the model efficient to learn a large number of tasks. Specifically, the neural pruning iteratively relaxes the current task’s parameter conditions to expand the common parameter subspace of the task; the synaptic consolidation strategy, which consists of a structure-aware parameter-importance measurement and an element-wise parameter updating strategy, decreases the cumulative error when learning new tasks. Our approach encourages the synapse to be sparse and polarized, which enables long-term learning and memory. ANPyC exhibits effectiveness and generalization on both image classification and generation tasks with multiple layer perceptron, convolutional neural networks, and generative adversarial networks, and variational autoencoder. The full source code is available at https://github.com/GeoX-Lab/ANPyC .},
  archive      = {J_TNNLS},
  author       = {Jian Peng and Bo Tang and Hao Jiang and Zhuo Li and Yinjie Lei and Tao Lin and Haifeng Li},
  doi          = {10.1109/TNNLS.2021.3056201},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4243-4256},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Overcoming long-term catastrophic forgetting through adversarial neural pruning and synaptic consolidation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted error entropy-based information theoretic learning
for robust subspace representation. <em>TNNLS</em>, <em>33</em>(9),
4228–4242. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most of the existing representation learning frameworks, the noise contaminating the data points is often assumed to be independent and identically distributed ( i.i.d. ), where the Gaussian distribution is often imposed. This assumption, though greatly simplifies the resulting representation problems, may not hold in many practical scenarios. For example, the noise in face representation is usually attributable to local variation, random occlusion, and unconstrained illumination, which is essentially structural, and hence, does not satisfy the i.i.d. property or the Gaussianity. In this article, we devise a generic noise model, referred to as independent and piecewise identically distributed ( i.p.i.d. ) model for robust presentation learning, where the statistical behavior of the underlying noise is characterized using a union of distributions. We demonstrate that our proposed i.p.i.d. model can better describe the complex noise encountered in practical scenarios and accommodate the traditional i.i.d. one as a special case. Assisted by the proposed noise model, we then develop a new information-theoretic learning framework for robust subspace representation through a novel minimum weighted error entropy criterion. Thanks to the superior modeling capability of the i.p.i.d. model, our proposed learning method achieves superior robustness against various types of noise. When applying our scheme to the subspace clustering and image recognition problems, we observe significant performance gains over the existing approaches.},
  archive      = {J_TNNLS},
  author       = {Yuanman Li and Jiantao Zhou and Jinyu Tian and Xianwei Zheng and Yuan Yan Tang},
  doi          = {10.1109/TNNLS.2021.3056188},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4228-4242},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weighted error entropy-based information theoretic learning for robust subspace representation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral response function-guided deep optimization-driven
network for spectral super-resolution. <em>TNNLS</em>, <em>33</em>(9),
4213–4227. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) are crucial for many research works. Spectral super-resolution (SSR) is a method used to obtain high-spatial-resolution (HR) HSIs from HR multispectral images. Traditional SSR methods include model-driven algorithms and deep learning. By unfolding a variational method, this article proposes an optimization-driven convolutional neural network (CNN) with a deep spatial–spectral prior, resulting in physically interpretable networks. Unlike the fully data-driven CNN, auxiliary spectral response function (SRF) is utilized to guide CNNs to group the bands with spectral relevance. In addition, the channel attention module (CAM) and the reformulated spectral angle mapper loss function are applied to achieve an effective reconstruction model. Finally, experiments on two types of data sets, including natural and remote sensing images, demonstrate the spectral enhancement effect of the proposed method, and also, the classification results on the remote sensing data set verified the validity of the information enhanced by the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jiang He and Jie Li and Qiangqiang Yuan and Huanfeng Shen and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2021.3056181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4213-4227},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spectral response function-guided deep optimization-driven network for spectral super-resolution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast self-supervised clustering with anchor graph.
<em>TNNLS</em>, <em>33</em>(9), 4199–4212. (<a
href="https://doi.org/10.1109/TNNLS.2021.3056080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefit from avoiding the utilization of labeled samples, which are usually insufficient in the real world, unsupervised learning has been regarded as a speedy and powerful strategy on clustering tasks. However, clustering directly from primal data sets leads to high computational cost, which limits its application on large-scale and high-dimensional problems. Recently, anchor-based theories are proposed to partly mitigate this problem and field naturally sparse affinity matrix, while it is still a challenge to get excellent performance along with high efficiency. To dispose of this issue, we first presented a fast semisupervised framework (FSSF) combined with a balanced $K$ -means-based hierarchical $K$ -means (BKHK) method and the bipartite graph theory. Thereafter, we proposed a fast self-supervised clustering method involved in this crucial semisupervised framework, in which all labels are inferred from a constructed bipartite graph with exactly $k$ connected components. The proposed method remarkably accelerates the general semisupervised learning through the anchor and consists of four significant parts: 1) obtaining the anchor set as interim through BKHK algorithm; 2) constructing the bipartite graph; 3) solving the self-supervised problem to construct a typical probability model with FSSF; and 4) selecting the most representative points regarding anchors from BKHK as an interim and conducting label propagation. The experimental results on toy examples and benchmark data sets have demonstrated that the proposed method outperforms other approaches.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Zhenyu Ma and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3056080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4199-4212},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast self-supervised clustering with anchor graph},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Certifiable robustness to adversarial state uncertainty in
deep reinforcement learning. <em>TNNLS</em>, <em>33</em>(9), 4184–4198.
(<a href="https://doi.org/10.1109/TNNLS.2021.3056046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network-based systems are now state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a deep Q-network (DQN) policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios, a classic control task, and Atari Pong. This article extends our prior work with new performance guarantees, extensions to other reinforcement learning algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.},
  archive      = {J_TNNLS},
  author       = {Michael Everett and Björn Lütjens and Jonathan P. How},
  doi          = {10.1109/TNNLS.2021.3056046},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4184-4198},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Certifiable robustness to adversarial state uncertainty in deep reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decision-tree-initialized dendritic neuron model for fast
and accurate data classification. <em>TNNLS</em>, <em>33</em>(9),
4173–4183. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a decision tree (DT)-based method for initializing a dendritic neuron model (DNM). Neural networks become larger and larger, thus consuming more and more computing resources. This calls for a strong need to prune neurons that do not contribute much to their network’s output. Pruning those with low contribution may lead to a loss of accuracy of DNM. Our proposed method is novel because 1) it can reduce the number of dendrites in DNM while improving training efficiency without affecting accuracy and 2) it can select proper initialization weight and threshold of neurons. The Adam algorithm is used to train DNM after its initialization with our proposed DT-based method. To verify its effectiveness, we apply it to seven benchmark datasets. The results show that decision-tree-initialized DNM is significantly better than the original DNM, k-nearest neighbor, support vector machine, back-propagation neural network, and DT classification methods. It exhibits the lowest model complexity and highest training speed without losing any accuracy. The interactions among attributes can also be observed in its dendritic neurons.},
  archive      = {J_TNNLS},
  author       = {Xudong Luo and Xiaohao Wen and MengChu Zhou and Abdullah Abusorrah and Lukui Huang},
  doi          = {10.1109/TNNLS.2021.3055991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4173-4183},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Decision-tree-initialized dendritic neuron model for fast and accurate data classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). H∞ state estimation for BAM neural networks with binary mode
switching and distributed leakage delays under periodic scheduling
protocol. <em>TNNLS</em>, <em>33</em>(9), 4160–4172. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the $H_{\infty }$ state estimation problem for a class of bidirectional associative memory (BAM) neural networks with binary mode switching, where the distributed delays are included in the leakage terms. A couple of stochastic variables taking values of 1 or 0 are introduced to characterize the switching behavior between the redundant models of the BAM neural network, and a general type of neuron activation function (i.e., the sector-bounded nonlinearity) is considered. In order to prevent the data transmissions from collisions, a periodic scheduling protocol (i.e., round-robin protocol) is adopted to orchestrate the transmission order of sensors. The purpose of this work is to develop a full-order estimator such that the error dynamics of the state estimation is exponentially mean-square stable and the $H_{\infty }$ performance requirement of the output estimation error is also achieved. Sufficient conditions are established to ensure the existence of the required estimator by constructing a mode-dependent Lyapunov-Krasovskii functional. Then, the desired estimator parameters are obtained by solving a set of matrix inequalities. Finally, a numerical example is provided to show the effectiveness of the proposed estimator design method.},
  archive      = {J_TNNLS},
  author       = {Fuad E. Alsaadi and Zidong Wang and Yuqiang Luo and Njud S. Alharbi and Fawaz W. Alsaade},
  doi          = {10.1109/TNNLS.2021.3055942},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4160-4172},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {H∞ state estimation for BAM neural networks with binary mode switching and distributed leakage delays under periodic scheduling protocol},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the distribution of successor states in boolean threshold
networks. <em>TNNLS</em>, <em>33</em>(9), 4147–4159. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the distribution of successor states in Boolean networks (BNs). The state vector ${\mathbf{y}}$ is called a successor of ${\mathbf{x}}$ if ${\mathbf{y}}= \textbf {F}({\mathbf{x}})$ holds, where ${\mathbf{x}}, {\mathbf{y}}\in {0,1}^{n}$ are state vectors and $\textbf {F}$ is an ordered set of Boolean functions describing the state transitions. This problem is motivated by analyzing how information propagates via hidden layers in Boolean threshold networks (discrete model of neural networks) and is kept or lost during time evolution in BNs. In this article, we measure the distribution via entropy and study how entropy changes via the transition from ${\mathbf{x}}$ to ${\mathbf{y}}$ , assuming that ${\mathbf{x}}$ is given uniformly at random. We focus on BNs consisting of exclusive OR (XOR) functions, canalyzing functions, and threshold functions. As a main result, we show that there exists a BN consisting of $d$ -ary XOR functions, which preserves the entropy if $d$ is odd and $n &amp;gt; d$ , whereas there does not exist such a BN if $d$ is even. We also show that there exists a specific BN consisting of $d$ -ary threshold functions, which preserves the entropy if $n \mod d = 0$ . Furthermore, we theoretically analyze the upper and lower bounds of the entropy for BNs consisting of canalyzing functions and perform computational experiments using BN models of real biological networks.},
  archive      = {J_TNNLS},
  author       = {Sini Guo and Pengyu Liu and Wai-Ki Ching and Tatsuya Akutsu},
  doi          = {10.1109/TNNLS.2021.3055841},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {9},
  pages        = {4147-4159},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {On the distribution of successor states in boolean threshold networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online reinforcement learning control by direct heuristic
dynamic programming: From time-driven to event-driven. <em>TNNLS</em>,
<em>33</em>(8), 4139–4144. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, time-driven learning refers to the machine learning method that updates parameters in a prediction model continuously as new data arrives. Among existing approximate dynamic programming (ADP) and reinforcement learning (RL) algorithms, the direct heuristic dynamic programming (dHDP) has been shown an effective tool as demonstrated in solving several complex learning control problems. It continuously updates the control policy and the critic as system states continuously evolve. It is therefore desirable to prevent the time-driven dHDP from updating due to insignificant system event such as noise. Toward this goal, we propose a new event-driven dHDP. By constructing a Lyapunov function candidate, we prove the uniformly ultimately boundedness (UUB) of the system states and the weights in the critic and the control policy networks. Consequently, we show the approximate control and cost-to-go function approaching Bellman optimality within a finite bound. We also illustrate how the event-driven dHDP algorithm works in comparison to the original time-driven dHDP.},
  archive      = {J_TNNLS},
  author       = {Qingtao Zhao and Jennie Si and Jian Sun},
  doi          = {10.1109/TNNLS.2021.3053037},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4139-4144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online reinforcement learning control by direct heuristic dynamic programming: From time-driven to event-driven},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixed-time synchronization of competitive neural networks
with multiple time scales. <em>TNNLS</em>, <em>33</em>(8), 4133–4138.
(<a href="https://doi.org/10.1109/TNNLS.2021.3052868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, we investigate the fixed-time synchronization of competitive neural networks with multiple time scales. These neural networks play an important role in visual processing, pattern recognition, neural computing, and so on. Our main contribution is the design of a novel synchronizing controller, which does not depend on the ratio between the fast and slow time scales. This feature makes the controller easy to implement since it is designed through well-posed algebraic conditions (i.e., even when the ratio between the time scales goes to 0, the controller gain is well defined and does not go to infinity). Last but not least, the closed-loop dynamics is characterized by a high convergence speed with a settling time which is upper bounded, and the bound is independent of the initial conditions. A numerical simulation illustrates our results and emphasizes their effectiveness.},
  archive      = {J_TNNLS},
  author       = {Wu Yang and Yan-Wu Wang and Irinel-Constantin Morǎrescu and Xiao-Kang Liu and Yuehua Huang},
  doi          = {10.1109/TNNLS.2021.3052868},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4133-4138},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fixed-time synchronization of competitive neural networks with multiple time scales},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Scalable inverse reinforcement learning through
multifidelity bayesian optimization. <em>TNNLS</em>, <em>33</em>(8),
4125–4132. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data in many practical problems are acquired according to decisions or actions made by users or experts to achieve specific goals. For instance, policies in the mind of biologists during the intervention process in genomics and metagenomics are often reflected in available data in these domains, or data in cyber–physical systems are often acquired according to actions/decisions made by experts/engineers for purposes, such as control or stabilization. Quantification of experts’ policies through available data, which is also known as reward function learning, has been discussed extensively in the literature in the context of inverse reinforcement learning (IRL). However, most of the available techniques come short to deal with practical problems due to the following main reasons: 1) lack of scalability: arising from incapability or poor performance of existing techniques in dealing with large systems and 2) lack of reliability: coming from the incapability of the existing techniques to properly learn the optimal reward function during the learning process. Toward this, in this brief, we propose a multifidelity Bayesian optimization (MFBO) framework that significantly scales the learning process of a wide range of existing IRL techniques. The proposed framework enables the incorporation of multiple approximators and efficiently takes their uncertainty and computational costs into account to balance exploration and exploitation during the learning process. The proposed framework’s high performance is demonstrated through genomics, metagenomics, and sets of random simulated problems.},
  archive      = {J_TNNLS},
  author       = {Mahdi Imani and Seyede Fatemeh Ghoreishi},
  doi          = {10.1109/TNNLS.2021.3051012},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4125-4132},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable inverse reinforcement learning through multifidelity bayesian optimization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Adversarial binary mutual learning for semi-supervised deep
hashing. <em>TNNLS</em>, <em>33</em>(8), 4110–4124. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is a popular search algorithm for its compact binary representation and efficient Hamming distance calculation. Benefited from the advance of deep learning, deep hashing methods have achieved promising performance. However, those methods usually learn with expensive labeled data but fail to utilize unlabeled data. Furthermore, the traditional pairwise loss used by those methods cannot explicitly force similar/dissimilar pairs to small/large distances. Both weaknesses limit existing methods’ performance. To solve the first problem, we propose a novel semi-supervised deep hashing model named adversarial binary mutual learning (ABML). Specifically, our ABML consists of a generative model $G_{H}$ and a discriminative model $D_{H}$ , where $D_{H}$ learns labeled data in a supervised way and $G_{H}$ learns unlabeled data by synthesizing real images. We adopt an adversarial learning (AL) strategy to transfer the knowledge of unlabeled data to $D_{H}$ by making $G_{H}$ and $D_{H}$ mutually learn from each other. To solve the second problem, we propose a novel Weibull cross-entropy loss (WCE) by using the Weibull distribution, which can distinguish tiny differences of distances and explicitly force similar/dissimilar distances as small/large as possible. Thus, the learned features are more discriminative. Finally, by incorporating ABML with WCE loss, our model can acquire more semantic and discriminative features. Extensive experiments on four common data sets (CIFAR-10, large database of handwritten digits (MNIST), ImageNet-10, and NUS-WIDE) and a large-scale data set ImageNet demonstrate that our approach successfully overcomes the two difficulties above and significantly outperforms state-of-the-art hashing methods.},
  archive      = {J_TNNLS},
  author       = {Guan’An Wang and Qinghao Hu and Yang Yang and Jian Cheng and Zeng-Guang Hou},
  doi          = {10.1109/TNNLS.2021.3055834},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4110-4124},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial binary mutual learning for semi-supervised deep hashing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spiking neural network regularization with fixed and
adaptive drop-keep probabilities. <em>TNNLS</em>, <em>33</em>(8),
4096–4109. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dropout and DropConnect are two techniques to facilitate the regularization of neural network models, having achieved the state-of-the-art results in several benchmarks. In this paper, to improve the generalization capability of spiking neural networks (SNNs), the two drop techniques are first applied to the state-of-the-art SpikeProp learning algorithm resulting in two improved learning algorithms called SPDO (SpikeProp with Dropout) and SPDC (SpikeProp with DropConnect). In view that a higher membrane potential of a biological neuron implies a higher probability of neural activation, three adaptive drop algorithms, SpikeProp with Adaptive Dropout (SPADO), SpikeProp with Adaptive DropConnect (SPADC), and SpikeProp with Group Adaptive Drop (SPGAD), are proposed by adaptively adjusting the keep probability for training SNNs. A convergence theorem for SPDC is proven under the assumptions of the bounded norm of connection weights and a finite number of equilibria. In addition, the five proposed algorithms are carried out in a collaborative neurodynamic optimization framework to improve the learning performance of SNNs. The experimental results on the four benchmark data sets demonstrate that the three adaptive algorithms converge faster than SpikeProp, SPDO, and SPDC, and the generalization errors of the five proposed algorithms are significantly smaller than that of SpikeProp. Furthermore, the experimental results also show that the five algorithms based on collaborative neurodynamic optimization can be improved in terms of several measures.},
  archive      = {J_TNNLS},
  author       = {Junhong Zhao and Jie Yang and Jun Wang and Wei Wu},
  doi          = {10.1109/TNNLS.2021.3055825},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4096-4109},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spiking neural network regularization with fixed and adaptive drop-keep probabilities},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional ordinal regression forest for image ordinal
estimation. <em>TNNLS</em>, <em>33</em>(8), 4084–4095. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image ordinal estimation is to predict the ordinal label of a given image, which can be categorized as an ordinal regression (OR) problem. Recent methods formulate an OR problem as a series of binary classification problems. Such methods cannot ensure that the global ordinal relationship is preserved since the relationships among different binary classifiers are neglected. We propose a novel OR approach, termed convolutional OR forest (CORF), for image ordinal estimation, which can integrate OR and differentiable decision trees with a convolutional neural network for obtaining precise and stable global ordinal relationships. The advantages of the proposed CORF are twofold. First, instead of learning a series of binary classifiers independently , the proposed method aims at learning an ordinal distribution for OR by optimizing those binary classifiers simultaneously . Second, the differentiable decision trees in the proposed CORF can be trained together with the ordinal distribution in an end-to-end manner. The effectiveness of the proposed CORF is verified on two image ordinal estimation tasks, i.e., facial age estimation and image esthetic assessment, showing significant improvements and better stability over the state-of-the-art OR methods.},
  archive      = {J_TNNLS},
  author       = {Haiping Zhu and Hongming Shan and Yuheng Zhang and Lingfu Che and Xiaoyang Xu and Junping Zhang and Jianbo Shi and Fei-Yue Wang},
  doi          = {10.1109/TNNLS.2021.3055816},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4084-4095},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional ordinal regression forest for image ordinal estimation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPGA-based high-throughput CNN hardware accelerator with
high computing resource utilization ratio. <em>TNNLS</em>,
<em>33</em>(8), 4069–4083. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field-programmable gate array (FPGA)-based CNN hardware accelerator adopting single-computing-engine (CE) architecture or multi-CE architecture has attracted great attention in recent years. The actual throughput of the accelerator is also getting higher and higher but is still far below the theoretical throughput due to the inefficient computing resource mapping mechanism and data supply problem, and so on. To solve these problems, a novel composite hardware CNN accelerator architecture is proposed in this article. To perform the convolution layer (CL) efficiently, a novel multiCE architecture based on a row-level pipelined streaming strategy is proposed. For each CE, an optimized mapping mechanism is proposed to improve its computing resource utilization ratio and an efficient data system with continuous data supply is designed to avoid the idle state of the CE. Besides, to relieve the off-chip bandwidth stress, a weight data allocation strategy is proposed. To perform the fully connected layer (FCL), a single-CE architecture based on a batch-based computing method is proposed. Based on these design methods and strategies, visual geometry group network-16 (VGG-16) and ResNet-101 are both implemented on the XC7VX980T FPGA platform. The VGG-16 accelerator consumed 3395 multipliers and got the throughput of 1 TOPS at 150 MHz, that is, about 98.15\% of the theoretical throughput ( $2 \times 3395 \times 150$ MOPS). Similarly, the ResNet-101 accelerator achieved 600 GOPS at 100 MHz, about 96.12\% of the theoretical throughput ( $2 \times 3121 \times 100$ MOPS).},
  archive      = {J_TNNLS},
  author       = {Wenjin Huang and Huangtao Wu and Qingkun Chen and Conghui Luo and Shihao Zeng and Tianrui Li and Yihua Huang},
  doi          = {10.1109/TNNLS.2021.3055814},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4069-4083},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FPGA-based high-throughput CNN hardware accelerator with high computing resource utilization ratio},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task weakly-supervised attention network for dementia
status estimation with structural MRI. <em>TNNLS</em>, <em>33</em>(8),
4056–4068. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of clinical scores (of neuropsychological tests) based on noninvasive structural magnetic resonance imaging (MRI) helps understand the pathological stage of dementia (e.g., Alzheimer’s disease (AD)) and forecast its progression. Existing machine/deep learning approaches typically preselect dementia-sensitive brain locations for MRI feature extraction and model construction, potentially leading to undesired heterogeneity between different stages and degraded prediction performance. Besides, these methods usually rely on prior anatomical knowledge (e.g., brain atlas) and time-consuming nonlinear registration for the preselection of brain locations, thereby ignoring individual-specific structural changes during dementia progression because all subjects share the same preselected brain regions. In this article, we propose a multi-task weakly-supervised attention network (MWAN) for the joint regression of multiple clinical scores from baseline MRI scans. Three sequential components are included in MWAN: 1) a backbone fully convolutional network for extracting MRI features; 2) a weakly supervised dementia attention block for automatically identifying subject-specific discriminative brain locations; and 3) an attention-aware multitask regression block for jointly predicting multiple clinical scores. The proposed MWAN is an end-to-end and fully trainable deep learning model in which dementia-aware holistic feature learning and multitask regression model construction are integrated into a unified framework. Our MWAN method was evaluated on two public AD data sets for estimating clinical scores of mini-mental state examination (MMSE), clinical dementia rating sum of boxes (CDRSB), and AD assessment scale cognitive subscale (ADAS-Cog). Quantitative experimental results demonstrate that our method produces superior regression performance compared with state-of-the-art methods. Importantly, qualitative results indicate that the dementia-sensitive brain locations automatically identified by our MWAN method well retain individual specificities and are biologically meaningful.},
  archive      = {J_TNNLS},
  author       = {Chunfeng Lian and Mingxia Liu and Li Wang and Dinggang Shen},
  doi          = {10.1109/TNNLS.2021.3055772},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4056-4068},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-task weakly-supervised attention network for dementia status estimation with structural MRI},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal tracking control of nonlinear multiagent systems
using internal reinforce q-learning. <em>TNNLS</em>, <em>33</em>(8),
4043–4055. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel reinforcement learning (RL) method is developed to solve the optimal tracking control problem of unknown nonlinear multiagent systems (MASs). Different from the representative RL-based optimal control algorithms, an internal reinforce Q-learning (IrQ-L) method is proposed, in which an internal reinforce reward (IRR) function is introduced for each agent to improve its capability of receiving more long-term information from the local environment. In the IrQL designs, a Q-function is defined on the basis of IRR function and an iterative IrQL algorithm is developed to learn optimally distributed control scheme, followed by the rigorous convergence and stability analysis. Furthermore, a distributed online learning framework, namely, reinforce-critic-actor neural networks, is established in the implementation of the proposed approach, which is aimed at estimating the IRR function, the Q-function, and the optimal control scheme, respectively. The implemented procedure is designed in a data-driven way without needing knowledge of the system dynamics. Finally, simulations and comparison results with the classical method are given to demonstrate the effectiveness of the proposed tracking control method.},
  archive      = {J_TNNLS},
  author       = {Zhinan Peng and Rui Luo and Jiangping Hu and Kaibo Shi and Sing Kiong Nguang and Bijoy Kumar Ghosh},
  doi          = {10.1109/TNNLS.2021.3055761},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4043-4055},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal tracking control of nonlinear multiagent systems using internal reinforce Q-learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global negative correlation learning: A unified framework
for global optimization of ensemble models. <em>TNNLS</em>,
<em>33</em>(8), 4031–4042. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensembles are a widely implemented approach in the machine learning community and their success is traditionally attributed to the diversity within the ensemble. Most of these approaches foster diversity in the ensemble by data sampling or by modifying the structure of the constituent models. Despite this, there is a family of ensemble models in which diversity is explicitly promoted in the error function of the individuals. The negative correlation learning (NCL) ensemble framework is probably the most well-known algorithm within this group of methods. This article analyzes NCL and reveals that the framework actually minimizes the combination of errors of the individuals of the ensemble instead of minimizing the residuals of the final ensemble. We propose a novel ensemble framework, named global negative correlation learning (GNCL), which focuses on the optimization of the global ensemble instead of the individual fitness of its components. An analytical solution for the parameters of base regressors based on the NCL framework and the global error function proposed is also provided under the assumption of fixed basis functions (although the general framework could also be instantiated for neural networks with nonfixed basis functions). The proposed ensemble framework is evaluated by extensive experiments with regression and classification data sets. Comparisons with other state-of-the-art ensemble methods confirm that GNCL yields the best overall performance.},
  archive      = {J_TNNLS},
  author       = {Carlos Perales-González and Francisco Fernández-Navarro and Mariano Carbonero-Ruz and Javier Pérez-Rodríguez},
  doi          = {10.1109/TNNLS.2021.3055734},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4031-4042},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global negative correlation learning: A unified framework for global optimization of ensemble models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Massive-scale aerial photo categorization by
cross-resolution visual perception enhancement. <em>TNNLS</em>,
<em>33</em>(8), 4017–4030. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorizing aerial photographs with varied weather/lighting conditions and sophisticated geomorphic factors is a key module in autonomous navigation, environmental evaluation, and so on. Previous image recognizers cannot fulfill this task due to three challenges: 1) localizing visually/semantically salient regions within each aerial photograph in a weakly annotated context due to the unaffordable human resources required for pixel-level annotation; 2) aerial photographs are generally with multiple informative attributes (e.g., clarity and reflectivity), and we have to encode them for better aerial photograph modeling; and 3) designing a cross-domain knowledge transferal module to enhance aerial photograph perception since multiresolution aerial photographs are taken asynchronistically and are mutually complementary. To handle the above problems, we propose to optimize aerial photograph’s feature learning by leveraging the low-resolution spatial composition to enhance the deep learning of perceptual features with a high resolution. More specifically, we first extract many BING-based object patches (Cheng et al. , 2014) from each aerial photograph. A weakly supervised ranking algorithm selects a few semantically salient ones by seamlessly incorporating multiple aerial photograph attributes. Toward an interpretable aerial photograph recognizer indicative to human visual perception, we construct a gaze shifting path (GSP) by linking the top-ranking object patches and, subsequently, derive the deep GSP feature. Finally, a cross-domain multilabel SVM is formulated to categorize each aerial photograph. It leverages the global feature from low-resolution counterparts to optimize the deep GSP feature from a high-resolution aerial photograph. Comparative results on our compiled million-scale aerial photograph set have demonstrated the competitiveness of our approach. Besides, the eye-tracking experiment has shown that our ranking-based GSPs are over 92\% consistent with the real human gaze shifting sequences.},
  archive      = {J_TNNLS},
  author       = {Luming Zhang and Xiaoqin Zhang and Mingliang Xu and Ling Shao},
  doi          = {10.1109/TNNLS.2021.3055548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4017-4030},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Massive-scale aerial photo categorization by cross-resolution visual perception enhancement},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifelong incremental reinforcement learning with online
bayesian inference. <em>TNNLS</em>, <em>33</em>(8), 4003–4016. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central capability of a long-lived reinforcement learning (RL) agent is to incrementally adapt its behavior as its environment changes and to incrementally build upon previous experiences to facilitate future learning in real-world scenarios. In this article, we propose lifelong incremental reinforcement learning (LLIRL), a new incremental algorithm for efficient lifelong adaptation to dynamic environments. We develop and maintain a library that contains an infinite mixture of parameterized environment models, which is equivalent to clustering environment parameters in a latent space. The prior distribution over the mixture is formulated as a Chinese restaurant process (CRP), which incrementally instantiates new environment models without any external information to signal environmental changes in advance. During lifelong learning, we employ the expectation–maximization (EM) algorithm with online Bayesian inference to update the mixture in a fully incremental manner. In EM, the E-step involves estimating the posterior expectation of environment-to-cluster assignments, whereas the M-step updates the environment parameters for future learning. This method allows for all environment models to be adapted as necessary, with new models instantiated for environmental changes and old models retrieved when previously seen environments are encountered again. Simulation experiments demonstrate that LLIRL outperforms relevant existing methods and enables effective incremental adaptation to various dynamic environments for lifelong learning.},
  archive      = {J_TNNLS},
  author       = {Zhi Wang and Chunlin Chen and Daoyi Dong},
  doi          = {10.1109/TNNLS.2021.3055499},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {4003-4016},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Lifelong incremental reinforcement learning with online bayesian inference},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward the optimal design and FPGA implementation of spiking
neural networks. <em>TNNLS</em>, <em>33</em>(8), 3988–4002. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a biologically plausible spiking neural network (SNN) largely depends on the model parameters and neural dynamics. This article proposes a parameter optimization scheme for improving the performance of a biologically plausible SNN and a parallel on-field-programmable gate array (FPGA) online learning neuromorphic platform for the digital implementation based on two numerical methods, namely, the Euler and third-order Runge–Kutta (RK3) methods. The optimization scheme explores the impact of biological time constants on information transmission in the SNN and improves the convergence rate of the SNN on digit recognition with a suitable choice of the time constants. The parallel digital implementation leads to a significant speedup over software simulation on a general-purpose CPU. The parallel implementation with the Euler method enables around $180\times $ ( $20\times $ ) training (inference) speedup over a Pytorch-based SNN simulation on CPU. Moreover, compared with previous work, our parallel implementation shows more than $300\times $ ( $240\times $ ) improvement on speed and $180\times $ ( $250\times $ ) reduction in energy consumption for training (inference). In addition, due to the high-order accuracy, the RK3 method is demonstrated to gain $2\times $ training speedup over the Euler method, which makes it suitable for online training in real-time applications.},
  archive      = {J_TNNLS},
  author       = {Wenzhe Guo and Hasan Erdem Yantır and Mohammed E. Fouda and Ahmed M. Eltawil and Khaled Nabil Salama},
  doi          = {10.1109/TNNLS.2021.3055421},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3988-4002},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward the optimal design and FPGA implementation of spiking neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward full-stack acceleration of deep convolutional neural
networks on FPGAs. <em>TNNLS</em>, <em>33</em>(8), 3974–3987. (<a
href="https://doi.org/10.1109/TNNLS.2021.3055240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the huge success and rapid development of convolutional neural networks (CNNs), there is a growing demand for hardware accelerators that accommodate a variety of CNNs to improve their inference latency and energy efficiency, in order to enable their deployment in real-time applications. Among popular platforms, field-programmable gate arrays (FPGAs) have been widely adopted for CNN acceleration because of their capability to provide superior energy efficiency and low-latency processing, while supporting high reconfigurability, making them favorable for accelerating rapidly evolving CNN algorithms. This article introduces a highly customized streaming hardware architecture that focuses on improving the compute efficiency for streaming applications by providing full-stack acceleration of CNNs on FPGAs. The proposed accelerator maps most computational functions, that is, convolutional and deconvolutional layers into a singular unified module, and implements the residual and concatenative connections between the functions with high efficiency, to support the inference of mainstream CNNs with different topologies. This architecture is further optimized through exploiting different levels of parallelism, layer fusion, and fully leveraging digital signal processing blocks (DSPs). The proposed accelerator has been implemented on Intel’s Arria 10 GX1150 hardware and evaluated with a wide range of benchmark models. The results demonstrate a high performance of over 1.3 TOP/s of throughput, up to 97\% of compute [multiply-accumulate (MAC)] efficiency, which outperforms the state-of-the-art FPGA accelerators.},
  archive      = {J_TNNLS},
  author       = {Shuanglong Liu and Hongxiang Fan and Martin Ferianc and Xinyu Niu and Huifeng Shi and Wayne Luk},
  doi          = {10.1109/TNNLS.2021.3055240},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3974-3987},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward full-stack acceleration of deep convolutional neural networks on FPGAs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning knowledge graph embedding with heterogeneous
relation attention networks. <em>TNNLS</em>, <em>33</em>(8), 3961–3973.
(<a href="https://doi.org/10.1109/TNNLS.2021.3055147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Zhifei Li and Hai Liu and Zhaoli Zhang and Tingting Liu and Neal N. Xiong},
  doi          = {10.1109/TNNLS.2021.3055147},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3961-3973},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning knowledge graph embedding with heterogeneous relation attention networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Margin distribution analysis. <em>TNNLS</em>,
<em>33</em>(8), 3948–3960. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Margin is an important concept in machine learning; theoretical analyses further reveal that the distribution of margin plays a more critical role than the minimum margin in generalization power. Recently, several approaches have achieved performance breakthroughs by optimizing the margin distribution, but their computational cost, which is usually higher than before, still hinders them to be widely applied. In this article, we propose margin distribution analysis (MDA), which optimizes the margin distribution more simply by maximizing the margin mean and minimizing the margin variance simultaneously. MDA is efficient and resistive to class-imbalance naturally, since its objective distinguishes the margin means of different classes and can be broken up into two linear equations. In practice, it can also cooperate with other frameworks such as reweight-minimization when facing complex circumstances with noise and outliers. Empirical studies validate the superiority of MDA in real-world data sets, and demonstrate that simple approaches can also perform competitively by optimizing margin distribution.},
  archive      = {J_TNNLS},
  author       = {Jun Wang and Zhi-Hua Zhou},
  doi          = {10.1109/TNNLS.2021.3054979},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3948-3960},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Margin distribution analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time synchronization of complex-valued
memristive-based neural networks via hybrid control. <em>TNNLS</em>,
<em>33</em>(8), 3938–3947. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The finite-time synchronization problem is investigated for the master–slave complex-valued memristive neural networks in this article. A novel Lyapunov-function based finite-time stability criterion with impulsive effects is proposed and utilized to design the decentralized finite-time synchronization controller. Not only the settling time but also the attractive domain with respect to the impulsive gain and average impulsive interval, as well as initial values is derived according to the sufficient synchronization condition. Two examples are outlined to illustrate the validity of our hybrid control strategy.},
  archive      = {J_TNNLS},
  author       = {Tianhu Yu and Jinde Cao and Leszek Rutkowski and Yi-Ping Luo},
  doi          = {10.1109/TNNLS.2021.3054967},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3938-3947},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization of complex-valued memristive-based neural networks via hybrid control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional neural network for behavioral modeling and
predistortion of wideband power amplifiers. <em>TNNLS</em>,
<em>33</em>(8), 3923–3937. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power amplifier (PA) models, such as the neural network (NN) models and the multilayer NN models, have problems with high complexity. In this article, we first propose a novel behavior model for wideband PAs, using a real-valued time-delay convolutional NN (RVTDCNN). The input data of the model is sorted and arranged as a graph composed of the in-phase and quadrature ( $I/Q$ ) components and envelope-dependent terms of current and past signals. Then, we created a predesigned filter using the convolutional layer to extract the basis functions required for the PA forward or reverse modeling. Finally, the generated rich basis functions are input into a simple, fully connected layer to build the model. Due to the weight sharing characteristics of the convolutional model’s structure, the strong memory effect does not lead to a significant increase in the complexity of the model. Meanwhile, the extraction effect of the predesigned filter also reduces the training complexity of the model. The experimental results show that the performance of the RVTDCNN model is almost the same as the NN models and the multilayer NN models. Meanwhile, compared with the abovementioned models, the coefficient number and computational complexity of the RVTDCNN model are significantly reduced. This advantage is noticeable when the memory effects of the PA are increased by using wider signal bandwidths.},
  archive      = {J_TNNLS},
  author       = {Xin Hu and Zhijun Liu and Xiaofei Yu and Yulong Zhao and Wenhua Chen and Biao Hu and Xuekun Du and Xiang Li and Mohamed Helaoui and Weidong Wang and Fadhel M. Ghannouchi},
  doi          = {10.1109/TNNLS.2021.3054867},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3923-3937},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Convolutional neural network for behavioral modeling and predistortion of wideband power amplifiers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Remote state estimation of nonlinear systems over fading
channels via recurrent neural networks. <em>TNNLS</em>, <em>33</em>(8),
3908–3922. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the remote state estimation for nonlinear dynamic systems with known linear dynamics and unknown nonlinear perturbations. The nonlinear dynamic plant is monitored by multiple distributed sensors over a random access wireless network with shared common radio channel. We focus on the communication strategy and remote state estimation algorithm design so as to achieve a remote state estimation stability subject to unknown nonlinearities in plant and various wireless impairments, such as multisensor interference, wireless fading, and additive channel noise. By exploiting the additive properties of the physical wireless channels, we propose a novel information fusion over-the-air mechanism to address the signal collision and interference among the sensors. Utilizing the partial knowledge on the linear dynamics of the plant, we also propose a novel recurrent neural network (RNN)-based remote state estimator aided by a virtual state estimation mean-square-error (MSE) process. We further propose a novel online training algorithm such that the RNN at the remote estimator can effectively learn the unknown plant nonlinearities. Using the Lyapunov drift analysis approach, we establish closed-form sufficient requirements on the communication resources needed to achieve almost sure stability of both state estimation and RNN online training in high signal-to-noise ratio (SNR) regime. As a result, our proposed scheme is asymptomatic optimal for large SNR in the sense that both the plant state and the unknown plant nonlinearities can be perfectly recovered at the remote estimator. The proposed scheme is also compared with various baselines and we show that significant performance gains can be achieved.},
  archive      = {J_TNNLS},
  author       = {Songfu Cai and Vincent K. N. Lau},
  doi          = {10.1109/TNNLS.2021.3054826},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3908-3922},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Remote state estimation of nonlinear systems over fading channels via recurrent neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-manifold optimization for multi-view subspace
clustering. <em>TNNLS</em>, <em>33</em>(8), 3895–3907. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The meaningful patterns embedded in high-dimensional multi-view data sets typically tend to have a much more compact representation that often lies close to a low-dimensional manifold. Identification of hidden structures in such data mainly depends on the proper modeling of the geometry of low-dimensional manifolds. In this regard, this article presents a manifold optimization-based integrative clustering algorithm for multi-view data. To identify consensus clusters, the algorithm constructs a joint graph Laplacian that contains denoised cluster information of the individual views. It optimizes a joint clustering objective while reducing the disagreement between the cluster structures conveyed by the joint and individual views. The optimization is performed alternatively over $k$ -means and Stiefel manifolds. The Stiefel manifold helps to model the nonlinearities and differential clusters within the individual views, whereas $k$ -means manifold tries to elucidate the best-fit joint cluster structure of the data. A gradient-based movement is performed separately on the manifold of each view so that individual nonlinearity is preserved while looking for shared cluster information. The convergence of the proposed algorithm is established over the manifold and asymptotic convergence bound is obtained to quantify theoretically how fast the sequence of iterates generated by the algorithm converges to an optimal solution. The integrative clustering on benchmark and multi-omics cancer data sets demonstrates that the proposed algorithm outperforms state-of-the-art multi-view clustering approaches.},
  archive      = {J_TNNLS},
  author       = {Aparajita Khan and Pradipta Maji},
  doi          = {10.1109/TNNLS.2021.3054789},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3895-3907},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-manifold optimization for multi-view subspace clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Directional deep embedding and appearance learning for fast
video object segmentation. <em>TNNLS</em>, <em>33</em>(8), 3884–3894.
(<a href="https://doi.org/10.1109/TNNLS.2021.3054769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent semisupervised video object segmentation (VOS) methods rely on fine-tuning deep convolutional neural networks online using the given mask of the first frame or predicted masks of subsequent frames. However, the online fine-tuning process is usually time-consuming, limiting the practical use of such methods. We propose a directional deep embedding and appearance learning (DDEAL) method, which is free of the online fine-tuning process, for fast VOS. First, a global directional matching module (GDMM), which can be efficiently implemented by parallel convolutional operations, is proposed to learn a semantic pixel-wise embedding as an internal guidance. Second, an effective directional appearance model-based statistics is proposed to represent the target and background on a spherical embedding space for VOS. Equipped with the GDMM and the directional appearance model learning module, DDEAL learns static cues from the labeled first frame and dynamically updates cues of the subsequent frames for object segmentation. Our method exhibits the state-of-the-art VOS performance without using online fine-tuning. Specifically, it achieves a ${\mathcal{ J}}$ &amp; ${\mathcal{ F}}$ mean score of 74.8\% on DAVIS 2017 data set and an overall score ${\mathcal{ G}}$ of 71.3\% on the large-scale YouTube-VOS data set, while retaining a speed of 25 fps with a single NVIDIA TITAN Xp GPU. Furthermore, our faster version runs 31 fps with only a little accuracy loss.},
  archive      = {J_TNNLS},
  author       = {Yingjie Yin and De Xu and Xingang Wang and Lei Zhang},
  doi          = {10.1109/TNNLS.2021.3054769},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3884-3894},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Directional deep embedding and appearance learning for fast video object segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-based optimal consensus control for multiagent systems
with policy gradient reinforcement learning. <em>TNNLS</em>,
<em>33</em>(8), 3872–3883. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the optimally distributed consensus control problem for discrete-time multiagent systems with completely unknown dynamics and computational ability differences. The problem can be viewed as solving nonzero-sum games with distributed reinforcement learning (RL), and each agent is a player in these games. First, to guarantee the real-time performance of learning algorithms, a data-based distributed control algorithm is proposed for multiagent systems using offline system interaction data sets. By utilizing the interactive data produced during the run of a real-time system, the proposed algorithm improves system performance based on distributed policy gradient RL. The convergence and stability are guaranteed based on functional analysis and the Lyapunov method. Second, to address asynchronous learning caused by computational ability differences in multiagent systems, the proposed algorithm is extended to an asynchronous version in which executing policy improvement or not of each agent is independent of its neighbors. Furthermore, an actor-critic structure, which contains two neural networks, is developed to implement the proposed algorithm in synchronous and asynchronous cases. Based on the method of weighted residuals, the convergence and optimality of the neural networks are guaranteed by proving the approximation errors converge to zero. Finally, simulations are conducted to show the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Xindi Yang and Hao Zhang and Zhuping Wang},
  doi          = {10.1109/TNNLS.2021.3054685},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3872-3883},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-based optimal consensus control for multiagent systems with policy gradient reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concept drift-tolerant transfer learning in dynamic
environments. <em>TNNLS</em>, <em>33</em>(8), 3857–3871. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing transfer learning methods that focus on problems in stationary environments are not usually applicable to dynamic environments, where concept drift may occur. To the best of our knowledge, the concept drift-tolerant transfer learning (CDTL), whose major challenge is the need to adapt the target model and knowledge of source domains to the changing environments, has yet to be well explored in the literature. This article, therefore, proposes a hybrid ensemble approach to deal with the CDTL problem provided that data in the target domain are generated in a streaming chunk-by-chunk manner from nonstationary environments. At each time step, a class-wise weighted ensemble is presented to adapt the model of target domains to new environments. It assigns a weight vector for each classifier generated from the previous data chunks to allow each class of the current data leveraging historical knowledge independently. Then, a domain-wise weighted ensemble is introduced to combine the source and target models to select useful knowledge of each domain. The source models are updated with the source instances performed by the proposed adaptive weighted CORrelation ALignment (AW-CORAL). AW-CORAL iteratively minimizes domain discrepancy meanwhile decreases the effect of unrelated source instances. In this way, positive knowledge of source domains can be potentially promoted while negative knowledge is reduced. Empirical studies on synthetic and real benchmark data sets demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Cuie Yang and Yiu-Ming Cheung and Jinliang Ding and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2021.3054665},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3857-3871},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Concept drift-tolerant transfer learning in dynamic environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel sparse graph-regularized singular value
decomposition model and its application to genomic data analysis.
<em>TNNLS</em>, <em>33</em>(8), 3842–3856. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the gene coexpression pattern is a central challenge for high-dimensional gene expression analysis. Recently, sparse singular value decomposition (SVD) has been used to achieve this goal. However, this model ignores the structural information between variables (e.g., a gene network). The typical graph-regularized penalty can be used to incorporate such prior graph information to achieve more accurate discovery and better interpretability. However, the existing approach fails to consider the opposite effect of variables with negative correlations. In this article, we propose a novel sparse graph-regularized SVD model with absolute operator (AGSVD) for high-dimensional gene expression pattern discovery. The key of AGSVD is to impose a novel graph-regularized penalty ( $| \boldsymbol {u}|^{T} \boldsymbol {L}| \boldsymbol {u}|$ ). However, such a penalty is a nonconvex and nonsmooth function, so it brings new challenges to model solving. We show that the nonconvex problem can be efficiently handled in a convex fashion by adopting an alternating optimization strategy. The simulation results on synthetic data show that our method is more effective than the existing SVD-based ones. In addition, the results on several real gene expression data sets show that the proposed methods can discover more biologically interpretable expression patterns by incorporating the prior gene network.},
  archive      = {J_TNNLS},
  author       = {Wenwen Min and Xiang Wan and Tsung-Hui Chang and Shihua Zhang},
  doi          = {10.1109/TNNLS.2021.3054635},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3842-3856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel sparse graph-regularized singular value decomposition model and its application to genomic data analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampled-data synchronization of stochastic markovian jump
neural networks with time-varying delay. <em>TNNLS</em>, <em>33</em>(8),
3829–3841. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, sampled-data synchronization problem for stochastic Markovian jump neural networks (SMJNNs) with time-varying delay under aperiodic sampled-data control is considered. By constructing mode-dependent one-sided loop-based Lyapunov functional and mode-dependent two-sided loop-based Lyapunov functional and using the Itô formula, two different stochastic stability criteria are proposed for error SMJNNs with aperiodic sampled data. The slave system can be guaranteed to synchronize with the master system based on the proposed stochastic stability conditions. Furthermore, two corresponding mode-dependent aperiodic sampled-data controllers design methods are presented for error SMJNNs based on these two different stochastic stability criteria, respectively. Finally, two numerical simulation examples are provided to illustrate that the design method of aperiodic sampled-data controller given in this article can effectively stabilize unstable SMJNNs. It is also shown that the mode-dependent two-sided looped-functional method gives less conservative results than the mode-dependent one-sided looped-functional method.},
  archive      = {J_TNNLS},
  author       = {Guoliang Chen and Jianwei Xia and Ju H. Park and Hao Shen and Guangming Zhuang},
  doi          = {10.1109/TNNLS.2021.3054615},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3829-3841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sampled-data synchronization of stochastic markovian jump neural networks with time-varying delay},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion planning and adaptive neural tracking control of an
uncertain two-link rigid–flexible manipulator with vibration amplitude
constraint. <em>TNNLS</em>, <em>33</em>(8), 3814–3828. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with an uncertain two-link rigid–flexible manipulator with vibration amplitude constraint, intending to achieve its position control via motion planning and adaptive tracking approach. In motion planning, the motion trajectories for the two links of the manipulator are planned based on virtual damping and online trajectories correction techniques. The planned trajectories can not only guarantee that the two links can reach their desired angles, but also have the ability to suppress vibration, which can be adjusted to meet the vibration amplitude constraint by limiting the parameters of the planned trajectories. Then, the adaptive tracking controller is designed using the radial basis function neural network and the sliding mode control technique. The developed controller makes the two links of the manipulator track the planned trajectories under the uncertainties including unmodeled dynamics, parameter perturbations, and persistent external disturbances acting on the joint motors. The simulation results verify the effectiveness of the proposed control strategy and also demonstrate the superior performance of the motion planning and the tracking controller.},
  archive      = {J_TNNLS},
  author       = {Qingxin Meng and Xuzhi Lai and Ze Yan and Chun-Yi Su and Min Wu},
  doi          = {10.1109/TNNLS.2021.3054611},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3814-3828},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Motion planning and adaptive neural tracking control of an uncertain two-link Rigid–Flexible manipulator with vibration amplitude constraint},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-based design of finite-time adaptive control of
uncertain nonlinear systems. <em>TNNLS</em>, <em>33</em>(8), 3804–3813.
(<a href="https://doi.org/10.1109/TNNLS.2021.3054579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finite-time adaptive tracking control against event-trigger error is investigated in this article for a type of uncertain nonlinear systems. By fusing the techniques of command filter backstepping technical and event-triggered control (ETC), an adaptive event-triggered design method is proposed to construct the controller, under which the effect of event-triggered error can be compensated completely. Moreover, the proposed controller can increase robustness against uncertainties and event error in the backstepping design framework. In particular, we establish the finite-time convergence condition under which the tracking error asymptotically converges to zero in finite time with the aid of a scaling function. Detailed and rigorous stability proofs are given by making use of the improved finite time stability criterion. Two simulation examples are provided to exhibit the validity of the designed adaptive ETC approach.},
  archive      = {J_TNNLS},
  author       = {Yuan-Xin Li and Zhongsheng Hou and Wei-Wei Che and Zheng-Guang Wu},
  doi          = {10.1109/TNNLS.2021.3054579},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3804-3813},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-based design of finite-time adaptive control of uncertain nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A plug-in method for representation factorization in
connectionist models. <em>TNNLS</em>, <em>33</em>(8), 3792–3803. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on decomposing latent representations in generative adversarial networks or learned feature representations in deep autoencoders into semantically controllable factors in a semisupervised manner, without modifying the original trained models. Particularly, we propose factors’ decomposer-entangler network (FDEN) that learns to decompose a latent representation into mutually independent factors. Given a latent representation, the proposed framework draws a set of interpretable factors, each aligned to independent factors of variations by minimizing their total correlation in an information-theoretic means. As a plug-in method, we have applied our proposed FDEN to the existing networks of adversarially learned inference and pioneer network and performed computer vision tasks of image-to-image translation in semantic ways, e.g., changing styles, while keeping the identity of a subject, and object classification in a few-shot learning scheme. We have also validated the effectiveness of the proposed method with various ablation studies in the qualitative, quantitative, and statistical examination.},
  archive      = {J_TNNLS},
  author       = {Jee Seok Yoon and Myung-Cheol Roh and Heung-Il Suk},
  doi          = {10.1109/TNNLS.2021.3054480},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3792-3803},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A plug-in method for representation factorization in connectionist models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beneficial perturbation network for designing general
adaptive artificial intelligence systems. <em>TNNLS</em>,
<em>33</em>(8), 3778–3791. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is the gold standard of adaptive learning. It not only can learn and benefit from experience, but also can adapt to new situations. In contrast, deep neural networks only learn one sophisticated but fixed mapping from inputs to outputs. This limits their applicability to more dynamic situations, where the input to output mapping may change with different contexts. A salient example is continual learning-learning new independent tasks sequentially without forgetting previous tasks. Continual learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby a previously learned mapping of an old task is erased when learning new mappings for new tasks. Herein, we propose a new biologically plausible type of deep neural network with extra, out-of-network, task-dependent biasing units to accommodate these dynamic situations. This allows, for the first time, a single network to learn potentially unlimited parallel input to output mappings, and to switch on the fly between them at runtime. Biasing units are programed by leveraging beneficial perturbations (opposite to well-known adversarial perturbations) for each task. Beneficial perturbations for a given task bias the network toward that task, essentially switching the network into a different mode to process that task. This largely eliminates catastrophic interference between tasks. Our approach is memory-efficient and parameter-efficient, can accommodate many tasks, and achieves the state-of-the-art performance across different tasks and domains.},
  archive      = {J_TNNLS},
  author       = {Shixian Wen and Amanda Rios and Yunhao Ge and Laurent Itti},
  doi          = {10.1109/TNNLS.2021.3054423},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3778-3791},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Beneficial perturbation network for designing general adaptive artificial intelligence systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CRL: Collaborative representation learning by coordinating
topic modeling and network embeddings. <em>TNNLS</em>, <em>33</em>(8),
3765–3777. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning (NRL) has shown its effectiveness in many tasks, such as vertex classification, link prediction, and community detection. In many applications, vertices of social networks contain textual information, e.g., citation networks, which form a text corpus and can be applied to the typical representation learning methods. The global context in the text corpus can be utilized by topic models to discover the topic structures of vertices. Nevertheless, most existing NRL approaches focus on learning representations from the local neighbors of vertices and ignore the global structure of the associated textual information in networks. In this article, we propose a unified model based on matrix factorization (MF), named collaborative representation learning (CRL), which: 1) considers complementary global and local information simultaneously and 2) models topics and learns network embeddings collaboratively. Moreover, we incorporate the Fletcher–Reeves (FR) MF, a conjugate gradient method, to optimize the embedding matrices in an alternative mode. We call this parameter learning method as AFR in our work that can achieve convergence after a few numbers of iterations. Also, by evaluating CRL on topic coherence and vertex classification using several real-world data sets, our experimental study shows that this collaborative model not only can improve the performance of topic discovery over the baseline topic models but also can learn better network representations than the state-of-the-art context-aware NRL models.},
  archive      = {J_TNNLS},
  author       = {Junyang Chen and Zhiguo Gong and Wei Wang and Weiwen Liu and Xiao Dong},
  doi          = {10.1109/TNNLS.2021.3054422},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3765-3777},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CRL: Collaborative representation learning by coordinating topic modeling and network embeddings},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variational inference and learning of piecewise linear
dynamical systems. <em>TNNLS</em>, <em>33</em>(8), 3753–3764. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the temporal behavior of data is of primordial importance in many scientific and engineering fields. Baseline methods assume that both the dynamic and observation equations follow linear-Gaussian models. However, there are many real-world processes that cannot be characterized by a single linear behavior. Alternatively, it is possible to consider a piecewise linear model which, combined with a switching mechanism, is well suited when several modes of behavior are needed. Nevertheless, switching dynamical systems are intractable because their computational complexity increases exponentially with time. In this article, we propose a variational approximation of piecewise linear dynamical systems. We provide full details of the derivation of two variational expectation-maximization algorithms: a filter and a smoother. We show that the model parameters can be split into two sets: static and dynamic parameters, and that the former parameters can be estimated offline together with the number of linear modes, or the number of states of the switching variable. We apply the proposed method to the head-pose tracking, and we thoroughly compare our algorithms with several state of the art trackers.},
  archive      = {J_TNNLS},
  author       = {Xavier Alameda-Pineda and Vincent Drouard and Radu Patrice Horaud},
  doi          = {10.1109/TNNLS.2021.3054407},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3753-3764},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Variational inference and learning of piecewise linear dynamical systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Target tracking control of a biomimetic underwater vehicle
through deep reinforcement learning. <em>TNNLS</em>, <em>33</em>(8),
3741–3752. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the underwater target tracking control problem of a biomimetic underwater vehicle (BUV) is addressed. Since it is difficult to build an effective mathematic model of a BUV due to the uncertainty of hydrodynamics, target tracking control is converted into the Markov decision process and is further achieved via deep reinforcement learning. The system state and reward function of underwater target tracking control are described. Based on the actor–critic reinforcement learning framework, the deep deterministic policy gradient actor–critic algorithm with supervision controller is proposed. The training tricks, including prioritized experience replay, actor network indirect supervision training, target network updating with different periods, and expansion of exploration space by applying random noise, are presented. Indirect supervision training is designed to address the issues of low stability and slow convergence of reinforcement learning in the continuous state and action space. Comparative simulations are performed to show the effectiveness of the training tricks. Finally, the proposed actor–critic reinforcement learning algorithm with supervision controller is applied to the physical BUV. Swimming pool experiments of underwater object tracking of the BUV are conducted in multiple scenarios to verify the effectiveness and robustness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Yu Wang and Chong Tang and Shuo Wang and Long Cheng and Rui Wang and Min Tan and Zengguang Hou},
  doi          = {10.1109/TNNLS.2021.3054402},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3741-3752},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Target tracking control of a biomimetic underwater vehicle through deep reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surrogate-assisted particle swarm optimization for evolving
variable-length transferable blocks for image classification.
<em>TNNLS</em>, <em>33</em>(8), 3727–3740. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have demonstrated promising performance on image classification tasks, but the manual design process becomes more and more complex due to the fast depth growth and the increasingly complex topologies of CNNs. As a result, neural architecture search (NAS) has emerged to automatically design CNNs that outperform handcrafted counterparts. However, the computational cost is immense, e.g., 22400 GPU-days and 2000 GPU-days for two outstanding NAS works named NAS and NASNet, respectively, which motivates this work. A new effective and efficient surrogate-assisted particle swarm optimization (PSO) algorithm is proposed to automatically evolve CNNs. This is achieved by proposing a novel surrogate model, a new method of creating a surrogate data set, and a new encoding strategy to encode variable-length blocks of CNNs, all of which are integrated into a PSO algorithm to form the proposed method. The proposed method shows its effectiveness by achieving the competitive error rates of 3.49\% on the CIFAR-10 data set, 18.49\% on the CIFAR-100 data set, and 1.82\% on the SVHN data set. The CNN blocks are efficiently learned by the proposed method from CIFAR-10 within 3 GPU-days due to the acceleration achieved by the surrogate model and the surrogate data set to avoid the training of 80.1\% of CNN blocks represented by the particles. Without any further search, the evolved blocks from CIFAR-10 can be successfully transferred to CIFAR-100, SVHN, and ImageNet, which exhibits the transferability of the block learned by the proposed method.},
  archive      = {J_TNNLS},
  author       = {Bin Wang and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TNNLS.2021.3054400},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3727-3740},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Surrogate-assisted particle swarm optimization for evolving variable-length transferable blocks for image classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transductive semisupervised deep hashing. <em>TNNLS</em>,
<em>33</em>(8), 3713–3726. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing methods have shown their superiority to traditional ones. However, they usually require a large amount of labeled training data for achieving high retrieval accuracies. We propose a novel transductive semisupervised deep hashing (TSSDH) method which is effective to train deep convolutional neural network (DCNN) models with both labeled and unlabeled training samples. TSSDH method consists of the following four main ingredients. First, we extend the traditional transductive learning (TL) principle to make it applicable to DCNN-based deep hashing. Second, we introduce confidence levels for unlabeled samples to reduce adverse effects from uncertain samples. Third, we employ a Gaussian likelihood loss for hash code learning to sufficiently penalize large Hamming distances for similar sample pairs. Fourth, we design the large-margin feature (LMF) regularization to make the learned features satisfy that the distances of similar sample pairs are minimized and the distances of dissimilar sample pairs are larger than a predefined margin. Comprehensive experiments show that the TSSDH method can produce superior image retrieval accuracies compared to the representative semisupervised deep hashing methods under the same number of labeled training samples.},
  archive      = {J_TNNLS},
  author       = {Weiwei Shi and Yihong Gong and Badong Chen and Xinhong Hei},
  doi          = {10.1109/TNNLS.2021.3054386},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3713-3726},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transductive semisupervised deep hashing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic learning from adaptive neural control for
discrete-time strict-feedback systems. <em>TNNLS</em>, <em>33</em>(8),
3700–3712. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article first investigates the issue on dynamic learning from adaptive neural network (NN) control of discrete-time strict-feedback nonlinear systems. To verify the exponential convergence of estimated NN weights, an extended stability result is presented for a class of discrete-time linear time-varying systems with time delays. Subsequently, by combining the $n$ -step-ahead predictor technology and backstepping, an adaptive NN controller is constructed, which integrates the novel weight updating laws with time delays and without the $\sigma $ modification. After ensuring the convergence of system output to a recurrent reference signal, the radial basis function (RBF) NN is verified to satisfy the partial persistent excitation condition. By the combination of the extended stability result, the estimated NN weights can be verified to exponentially converge to their ideal values. The convergent weight sequences are comprehensively represented and stored by constructing some elegant learning rules with some novel sequences and the mod function. The stored knowledge is used again to develop a neural learning control scheme. Compared with the traditional adaptive NN control, the proposed scheme can not only accomplish the same or similar tracking tasks but also greatly improve the transient control performance and alleviate the online computation. Finally, the validity of the presented scheme is illustrated by numerical and practical examples.},
  archive      = {J_TNNLS},
  author       = {Min Wang and Haotian Shi and Cong Wang and Jun Fu},
  doi          = {10.1109/TNNLS.2021.3054378},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3700-3712},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic learning from adaptive neural control for discrete-time strict-feedback systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Networked multiagent systems: Antagonistic interaction,
constraint, and its application. <em>TNNLS</em>, <em>33</em>(8),
3690–3699. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the consensus problem in the framework of networked multiagent systems with constraint where there exists antagonistic information. A major difficulty is how to characterize the communication among the interacting agents in the presence of antagonistic information without resorting to the signed graph theory, which plays a central role in the Altafini model. It is shown that the proposed control protocol enables us to solve the consensus problem in a node-based viewpoint where both cooperative and antagonistic interactions coexist. Moreover, the proposed setup is further extended to the case of input saturation, leading to the semiglobal consensus. In addition, the consensus region associated with antagonistic information among participating individuals is also elaborated. Finally, the deduced theoretical results are applied to the task distribution problem via unmanned ground vehicles.},
  archive      = {J_TNNLS},
  author       = {Wentao Zhang and Zhiqiang Zuo and Yijing Wang},
  doi          = {10.1109/TNNLS.2021.3054128},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3690-3699},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Networked multiagent systems: Antagonistic interaction, constraint, and its application},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal scale combination selection integrating three-way
decision with hasse diagram. <em>TNNLS</em>, <em>33</em>(8), 3675–3689.
(<a href="https://doi.org/10.1109/TNNLS.2021.3054063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale decision system (MDS) is an effective tool to describe hierarchical data in machine learning. Optimal scale combination (OSC) selection and attribute reduction are two key issues related to knowledge discovery in MDSs. However, searching for all OSCs may result in a combinatorial explosion, and the existing approaches typically incur excessive time consumption. In this study, searching for all OSCs is considered as an optimization problem with the scale space as the search space. Accordingly, a sequential three-way decision model of the scale space is established to reduce the search space by integrating three-way decision with the Hasse diagram. First, a novel scale combination is proposed to perform scale selection and attribute reduction simultaneously, and then an extended stepwise optimal scale selection (ESOSS) method is introduced to quickly search for a single local OSC on a subset of the scale space. Second, based on the obtained local OSCs, a sequential three-way decision model of the scale space is established to divide the search space into three pair-wise disjoint regions, namely the positive, negative, and boundary regions. The boundary region is regarded as a new search space, and it can be proved that a local OSC on the boundary region is also a global OSC. Therefore, all OSCs of a given MDS can be obtained by searching for the local OSCs on the boundary regions in a step-by-step manner. Finally, according to the properties of the Hasse diagram, a formula for calculating the maximal elements of a given boundary region is provided to alleviate space complexity. Accordingly, an efficient OSC selection algorithm is proposed to improve the efficiency of searching for all OSCs by reducing the search space. The experimental results demonstrate that the proposed method can significantly reduce computational time.},
  archive      = {J_TNNLS},
  author       = {Qinghua Zhang and Yunlong Cheng and Fan Zhao and Guoyin Wang and Shuyin Xia},
  doi          = {10.1109/TNNLS.2021.3054063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3675-3689},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal scale combination selection integrating three-way decision with hasse diagram},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiperspective progressive structure adaptation for JPEG
steganography detection across domains. <em>TNNLS</em>, <em>33</em>(8),
3660–3674. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of steganography detection is to identify whether the multimedia data contain hidden information. Although many detection algorithms have been presented to solve tasks with inconsistent distributions between the source and target domains, effectively exploiting transferable correlation information across domains remains challenging. As a solution, we present a novel multiperspective progressive structure adaptation (MPSA) scheme based on active progressive learning (APL) for JPEG steganography detection across domains. First, the source and target data originating from unprocessed steganalysis features are clustered together to explore the structures in different domains, where the intradomain and interdomain structures can be captured to provide adequate information for cross-domain steganography detection. Second, the structure vectors containing the global and local modalities are exploited to reduce nonlinear distribution discrepancy based on APL in the latent representation space. In this way, the signal-to-noise ratio (SNR) of a weak stego signal can be improved by selecting suitable objects and adjusting the learning sequence. Third, the structure adaptation across multiple domains is achieved by the constraints for iterative optimization to promote the discrimination and transferability of structure knowledge. In addition, a unified framework for single-source domain adaptation (SSDA) and multiple-source domain adaptation (MSDA) in mismatched steganalysis can enhance the model’s capability to avoid a potential negative transfer. Extensive experiments on various benchmark cross-domain steganography detection tasks show the superiority of the proposed approach over the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Ju Jia and Meng Luo and Jinshuo Liu and Weixiang Ren and Lina Wang},
  doi          = {10.1109/TNNLS.2021.3054045},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3660-3674},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiperspective progressive structure adaptation for JPEG steganography detection across domains},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relaxed block-diagonal dictionary pair learning with
locality constraint for image recognition. <em>TNNLS</em>,
<em>33</em>(8), 3645–3659. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel structured analysis–synthesis dictionary pair learning method for efficient representation and image classification, referred to as relaxed block-diagonal dictionary pair learning with a locality constraint (RBD-DPL). RBD-DPL aims to learn relaxed block-diagonal representations of the input data to enhance the discriminability of both analysis and synthesis dictionaries by dynamically optimizing the block-diagonal components of representation, while the off-block-diagonal counterparts are set to zero. In this way, the learned synthesis subdictionary is allowed to be more flexible in reconstructing the samples from the same class, and the analysis dictionary effectively transforms the original samples into a relaxed coefficient subspace, which is closely associated with the label information. Besides, we incorporate a locality-constraint term as a complement of the relaxation learning to enhance the locality of the analytical encoding so that the learned representation exhibits high intraclass similarity. A linear classifier is trained in the learned relaxed representation space for consistent classification. RBD-DPL is computationally efficient because it avoids both the use of class-specific complementary data matrices to learn discriminative analysis dictionary, as well as the time-consuming $l_{1}/l_{0}$ -norm sparse reconstruction process. The experimental results demonstrate that our RBD-DPL achieves at least comparable or better recognition performance than the state-of-the-art algorithms. Moreover, both the training and testing time are significantly reduced, which verifies the efficiency of our method. The MATLAB code of the proposed RBD-DPL is available at https://github.com/chenzhe207/RBD-DPL .},
  archive      = {J_TNNLS},
  author       = {Zhe Chen and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1109/TNNLS.2021.3053941},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3645-3659},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Relaxed block-diagonal dictionary pair learning with locality constraint for image recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Fast unsupervised projection for large-scale data.
<em>TNNLS</em>, <em>33</em>(8), 3634–3644. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) technique has been frequently used to alleviate information redundancy and reduce computational complexity. Traditional DR methods generally are inability to deal with nonlinear data and have high computational complexity. To cope with the problems, we propose a fast unsupervised projection (FUP) method. The simplified graph of FUP is constructed by samples and representative points, where the number of the representative points selected through iterative optimization is less than that of samples. By generating the presented graph, it is proved that large-scale data can be projected faster in numerous scenarios. Thereafter, the orthogonality FUP (OFUP) method is proposed to ensure the orthogonality of projection matrix. Specifically, the OFUP method is proved to be equivalent to PCA upon certain parameter setting. Experimental results on benchmark data sets show the effectiveness in retaining the essential information.},
  archive      = {J_TNNLS},
  author       = {Jingyu Wang and Lin Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TNNLS.2021.3053840},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3634-3644},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast unsupervised projection for large-scale data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Periodic event-triggered synchronization for discrete-time
complex dynamical networks. <em>TNNLS</em>, <em>33</em>(8), 3622–3633.
(<a href="https://doi.org/10.1109/TNNLS.2021.3053652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the periodic event-triggered synchronization of discrete-time complex dynamical networks (CDNs). First, a discrete-time version of periodic event-triggered mechanism (ETM) is proposed, under which the sensors sample the signals in a periodic manner. But whether the sampling signals are transmitted to controllers or not is determined by a predefined periodic ETM. Compared with the common ETMs in the field of discrete-time systems, the proposed method avoids monitoring the measurements point-to-point and enlarges the lower bound of the inter-event intervals. As a result, it is beneficial to save both the energy and communication resources. Second, the “discontinuous” Lyapunov functionals are constructed to deal with the sawtooth constraint of sampling signals. The functionals can be viewed as the discrete-time extension for those discontinuous ones in continuous-time fields. Third, sufficient conditions for the ultimately bounded synchronization are derived for the discrete-time CDNs with or without considering communication delays, respectively. A calculation method for simultaneously designing the triggering parameter and control gains is developed such that the estimation of error level is accurate as much as possible. Finally, the simulation examples are presented to show the effectiveness and improvements of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Sanbo Ding and Zhanshan Wang and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2021.3053652},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3622-3633},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Periodic event-triggered synchronization for discrete-time complex dynamical networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing attention for sequence modeling via reinforcement
learning. <em>TNNLS</em>, <em>33</em>(8), 3612–3621. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention has been shown highly effective for modeling sequences, capturing the more informative parts in learning a deep representation. However, recent studies show that the attention values do not always coincide with intuition in tasks, such as machine translation and sentiment classification. In this study, we consider using deep reinforcement learning to automatically optimize attention distribution during the minimization of end task training losses. With more sufficient environment states, iterative actions are taken to adjust attention weights so that more informative words receive more attention automatically. Results on different tasks and different attention networks demonstrate that our model is of great effectiveness in improving the end task performances, yielding more reasonable attention distribution. The more in-depth analysis further reveals that our retrofitting method can help to bring explainability for baseline attention.},
  archive      = {J_TNNLS},
  author       = {Hao Fei and Yue Zhang and Yafeng Ren and Donghong Ji},
  doi          = {10.1109/TNNLS.2021.3053633},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3612-3621},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimizing attention for sequence modeling via reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neighborhood geometric structure-preserving variational
autoencoder for smooth and bounded data sources. <em>TNNLS</em>,
<em>33</em>(8), 3598–3611. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data sources, such as human poses, lie on low-dimensional manifolds that are smooth and bounded. Learning low-dimensional representations for such data is an important problem. One typical solution is to utilize encoder–decoder networks. However, due to the lack of effective regularization in latent space, the learned representations usually do not preserve the essential data relations. For example, adjacent video frames in a sequence may be encoded into very different zones across the latent space with holes in between. This is problematic for many tasks such as denoising because slightly perturbed data have the risk of being encoded into very different latent variables, leaving output unpredictable. To resolve this problem, we first propose a neighborhood geometric structure-preserving variational autoencoder (SP-VAE), which not only maximizes the evidence lower bound but also encourages latent variables to preserve their structures as in ambient space. Then, we learn a set of small surfaces to approximately bound the learned manifold to deal with holes in latent space. We extensively validate the properties of our approach by reconstruction, denoising, and random image generation experiments on a number of data sources, including synthetic Swiss roll, human pose sequences, and facial expression images. The experimental results show that our approach learns more smooth manifolds than the baselines. We also apply our approach to the tasks of human pose refinement and facial expression image interpolation where it gets better results than the baselines.},
  archive      = {J_TNNLS},
  author       = {Xingyu Chen and Chunyu Wang and Xuguang Lan and Nanning Zheng and Wenjun Zeng},
  doi          = {10.1109/TNNLS.2021.3053591},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3598-3611},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neighborhood geometric structure-preserving variational autoencoder for smooth and bounded data sources},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving EEG decoding via clustering-based multitask
feature learning. <em>TNNLS</em>, <em>33</em>(8), 3587–3597. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate electroencephalogram (EEG) pattern decoding for specific mental tasks is one of the key steps for the development of brain–computer interface (BCI), which is quite challenging due to the considerably low signal-to-noise ratio of EEG collected at the brain scalp. Machine learning provides a promising technique to optimize EEG patterns toward better decoding accuracy. However, existing algorithms do not effectively explore the underlying data structure capturing the true EEG sample distribution and, hence, can only yield a suboptimal decoding accuracy. To uncover the intrinsic distribution structure of EEG data, we propose a clustering-based multitask feature learning algorithm for improved EEG pattern decoding. Specifically, we perform affinity propagation-based clustering to explore the subclasses (i.e., clusters) in each of the original classes and then assign each subclass a unique label based on a one-versus-all encoding strategy. With the encoded label matrix, we devise a novel multitask learning algorithm by exploiting the subclass relationship to jointly optimize the EEG pattern features from the uncovered subclasses. We then train a linear support vector machine with the optimized features for EEG pattern decoding. Extensive experimental studies are conducted on three EEG data sets to validate the effectiveness of our algorithm in comparison with other state-of-the-art approaches. The improved experimental results demonstrate the outstanding superiority of our algorithm, suggesting its prominent performance for EEG pattern decoding in BCI applications.},
  archive      = {J_TNNLS},
  author       = {Yu Zhang and Tao Zhou and Wei Wu and Hua Xie and Hongru Zhu and Guoxu Zhou and Andrzej Cichocki},
  doi          = {10.1109/TNNLS.2021.3053576},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3587-3597},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving EEG decoding via clustering-based multitask feature learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abnormal event detection and localization via adversarial
event prediction. <em>TNNLS</em>, <em>33</em>(8), 3572–3586. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present adversarial event prediction (AEP), a novel approach to detecting abnormal events through an event prediction setting. Given normal event samples, AEP derives the prediction model, which can discover the correlation between the present and future of events in the training step. In obtaining the prediction model, we propose adversarial learning for the past and future of events. The proposed adversarial learning enforces AEP to learn the representation for predicting future events and restricts the representation learning for the past of events. By exploiting the proposed adversarial learning, AEP can produce the discriminative model to detect an anomaly of events without complementary information, such as optical flow and explicit abnormal event samples in the training step. We demonstrate the efficiency of AEP for detecting anomalies of events using the UCSD-Ped, CUHK Avenue, Subway, and UCF-Crime data sets. Experiments include the performance analysis depending on hyperparameter settings and the comparison with existing state-of-the-art methods. The experimental results show that the proposed adversarial learning can assist in deriving a better model for normal events on AEP, and AEP trained by the proposed adversarial learning can surpass the existing state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Jongmin Yu and Younkwan Lee and Kin Choong Yow and Moongu Jeon and Witold Pedrycz},
  doi          = {10.1109/TNNLS.2021.3053563},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3572-3586},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Abnormal event detection and localization via adversarial event prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical-bayesian-based sparse stochastic configuration
networks for construction of prediction intervals. <em>TNNLS</em>,
<em>33</em>(8), 3560–3571. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the architecture complexity and ill-posed problems of neural networks when dealing with high-dimensional data, this article presents a Bayesian-learning-based sparse stochastic configuration network (SCN) (BSSCN). The BSSCN inherits the basic idea of training an SCN in the Bayesian framework but replaces the common Gaussian distribution with a Laplace one as the prior distribution of the output weights of SCN. Meanwhile, a lower bound of the Laplace sparse prior distribution using a two-level hierarchical prior is adopted based on which an approximate Gaussian posterior with sparse property is obtained. It leads to the facilitation of training the BSSCN, and the analytical solution for output weights of BSSCN can be obtained. Furthermore, the hyperparameter estimation process is derived by maximizing the corresponding lower bound of the marginal likelihood function based on the expectation-maximization algorithm. In addition, considering the uncertainties caused by both noises in the real-world data and model mismatch, a bootstrap ensemble strategy using BSSCN is designed to construct the prediction intervals (PIs) of the target variables. The experimental results on three benchmark data sets and two real-world high-dimensional data sets demonstrate the effectiveness of the proposed method in terms of both prediction accuracy and quality of the constructed PIs.},
  archive      = {J_TNNLS},
  author       = {Jun Lu and Jinliang Ding and Changxin Liu and Tianyou Chai},
  doi          = {10.1109/TNNLS.2021.3053306},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3560-3571},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical-bayesian-based sparse stochastic configuration networks for construction of prediction intervals},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RGB-d point cloud registration based on salient object
detection. <em>TNNLS</em>, <em>33</em>(8), 3547–3559. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust algorithm for aligning rigid, noisy, and partially overlapping red green blue-depth (RGB-D) point clouds. To address the problems of data degradation and uneven distribution, we offer three strategies to increase the robustness of the iterative closest point (ICP) algorithm. First, we introduce a salient object detection (SOD) method to extract a set of points with significant structural variation in the foreground, which can avoid the unbalanced proportion of foreground and background point sets leading to the local registration. Second, registration algorithms that rely only on structural information for alignment cannot establish the correct correspondences when faced with the point set with no significant change in structure. Therefore, a bidirectional color distance (BCD) is designed to build precise correspondence with bidirectional search and color guidance. Third, the maximum correntropy criterion (MCC) and trimmed strategy are introduced into our algorithm to handle with noise and outliers. We experimentally validate that our algorithm is more robust than previous algorithms on simulated and real-world scene data in most scenarios and achieve a satisfying 3-D reconstruction of indoor scenes.},
  archive      = {J_TNNLS},
  author       = {Teng Wan and Shaoyi Du and Wenting Cui and Runzhao Yao and Yuyan Ge and Ce Li and Yue Gao and Nanning Zheng},
  doi          = {10.1109/TNNLS.2021.3053274},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3547-3559},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RGB-D point cloud registration based on salient object detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A convex model for support vector distance metric learning.
<em>TNNLS</em>, <em>33</em>(8), 3533–3546. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning (DML) aims to learn a distance metric to process the data distribution. However, most of the existing methods are $k$ NN DML methods and employ the $k$ NN model to classify the test instances. The drawback of $k$ NN DML is that all training instances need to be accessed and stored to classify the test instances, and the classification performance is influenced by the setting of the nearest neighbor number $k$ . To solve these problems, there are several DML methods that employ the SVM model to classify the test instances. However, all of them are nonconvex and the convex support vector DML method has not been explicitly proposed. In this article, we propose a convex model for support vector DML (CSV-DML), which is capable of replacing the $k$ NN model of DML with the SVM model. To make CSV-DML can use the most kernel functions of the existing SVM methods, a nonlinear mapping is used to map the original instances into a feature space. Since the explicit form of nonlinear mapped instances is unknown, the original instances are further transformed into the kernel form, which can be calculated explicitly. CSV-DML is constructed to work directly on the kernel-transformed instances. Specifically, we learn a specific Mahalanobis distance metric from the kernel-transformed training instances and train a DML-based separating hyperplane based on it. An iterated approach is formulated to optimize CSV-DML, which is based on generalized block coordinate descent and can converge to the global optimum. In CSV-DML, since the dimension of kernel-transformed instances is only related to the number of original training instances, we develop a novel parameter reduction scheme for reducing the feature dimension. Extensive experiments show that the proposed CSV-DML method outperforms the previous methods.},
  archive      = {J_TNNLS},
  author       = {Yibang Ruan and Yanshan Xiao and Zhifeng Hao and Bo Liu},
  doi          = {10.1109/TNNLS.2021.3053266},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3533-3546},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A convex model for support vector distance metric learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Link prediction based on stochastic information diffusion.
<em>TNNLS</em>, <em>33</em>(8), 3522–3532. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction (LP) in networks aims at determining future interactions among elements; it is a critical machine-learning tool in different domains, ranging from genomics to social networks to marketing, especially in e-commerce recommender systems. Although many LP techniques have been developed in the prior art, most of them consider only static structures of the underlying networks, rarely incorporating the network’s information flow. Exploiting the impact of dynamic streams, such as information diffusion, is still an open research topic for LP. Information diffusion allows nodes to receive information beyond their social circles, which, in turn, can influence the creation of new links. In this work, we analyze the LP effects through two diffusion approaches, susceptible-infected-recovered and independent cascade. As a result, we propose the progressive-diffusion (PD) method for LP based on nodes’ propagation dynamics. The proposed model leverages a stochastic discrete-time rumor model centered on each node’s propagation dynamics. It presents low-memory and low-processing footprints and is amenable to parallel and distributed processing implementation. Finally, we also introduce an evaluation metric for LP methods considering both the information diffusion capacity and the LP accuracy. Experimental results on a series of benchmarks attest to the proposed method’s effectiveness compared with the prior art in both criteria.},
  archive      = {J_TNNLS},
  author       = {Didier A. Vega-Oliveros and Liang Zhao and Anderson Rocha and Lilian Berton},
  doi          = {10.1109/TNNLS.2021.3053263},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3522-3532},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Link prediction based on stochastic information diffusion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparative convolutional dynamic multi-attention
recommendation model. <em>TNNLS</em>, <em>33</em>(8), 3510–3521. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, an attention mechanism has been used to help recommender systems grasp user interests more accurately. It focuses on their pivotal interests from a psychology perspective. However, most current studies based on it only focus on part of user interests; they have not mined user preferences thoroughly. To address the above problem, we propose a novel recommendation model: comparative convolutional dynamic multi-attention (CCDMA). This model provides a more accurate approach to represent user and item features and uses multi-attention-based convolutional neural networks to extract user and item latent feature vectors dynamically. The multi-attention mechanism considers both self-attention and cross-attention. Self-attention refers to the internal attention within users and items; cross-attention is the mutual attention between users and items. Moreover, we propose an optimized comparative learning framework that can mine the ternary relationships between one user and a pair of items, focusing on their relative relationship and the internal link between a pair of items. Extensive experiments on several real-world data sets show that the CCDMA model significantly outperforms state-of-the-art baselines in terms of different evaluation metrics.},
  archive      = {J_TNNLS},
  author       = {Juan Ni and Zhenhua Huang and Chang Yu and Dongdong Lv and Cheng Wang},
  doi          = {10.1109/TNNLS.2021.3053245},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3510-3521},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comparative convolutional dynamic multi-attention recommendation model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial learning of disentangled and generalizable
representations of visual attributes. <em>TNNLS</em>, <em>33</em>(8),
3498–3509. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a multitude of methods for image-to-image translation have demonstrated impressive results on problems, such as multidomain or multiattribute transfer. The vast majority of such works leverages the strengths of adversarial learning and deep convolutional autoencoders to achieve realistic results by well-capturing the target data distribution. Nevertheless, the most prominent representatives of this class of methods do not facilitate semantic structure in the latent space and usually rely on binary domain labels for test-time transfer. This leads to rigid models, unable to capture the variance of each domain label. In this light, we propose a novel adversarial learning method that: 1) facilitates the emergence of latent structure by semantically disentangling sources of variation and 2) encourages learning generalizable, continuous, and transferable latent codes that enable flexible attribute mixing. This is achieved by introducing a novel loss function that encourages representations to result in uniformly distributed class posteriors for disentangled attributes. In tandem with an algorithm for inducing generalizable properties, the resulting representations can be utilized for a variety of tasks such as intensity-preserving multiattribute image translation and synthesis, without requiring labeled test data. We demonstrate the merits of the proposed method by a set of qualitative and quantitative experiments on popular databases such as MultiPIE, RaFD, and BU-3DFE, where our method outperforms other state-of-the-art methods in tasks such as intensity-preserving multiattribute transfer and synthesis.},
  archive      = {J_TNNLS},
  author       = {James Oldfield and Yannis Panagakis and Mihalis A. Nicolaou},
  doi          = {10.1109/TNNLS.2021.3053205},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3498-3509},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial learning of disentangled and generalizable representations of visual attributes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven adaptive consensus learning from network
topologies. <em>TNNLS</em>, <em>33</em>(8), 3487–3497. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of consensus learning from network topologies is studied for strongly connected nonlinear nonaffine multiagent systems (MASs). A linear spatial dynamic relationship (LSDR) is built at first to formulate the dynamic I/O relationship between an agent and all the other agents that are communicated through the networked topology. The LSDR consists of a linear parametric uncertain term and a residual nonlinear uncertain term. Utilizing the LSDR, a data-driven adaptive learning consensus protocol (DDALCP) is proposed to learn from both time dynamics of agent itself and spatial dynamics of the whole MAS. The parametric uncertainty and nonlinear uncertainty are estimated through an estimator and an observer respectively to improve robustness. The proposed DDALCP has a strong learning ability to improve the consensus performance because time dynamics and network topology information are both considered. The proposed consensus learning method is data-driven and has no dependence on the system model. The theoretical results are demonstrated by simulations.},
  archive      = {J_TNNLS},
  author       = {Ronghu Chi and Yu Hui and Biao Huang and Zhongsheng Hou and Xuhui Bu},
  doi          = {10.1109/TNNLS.2021.3053186},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3487-3497},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven adaptive consensus learning from network topologies},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive NN-based consensus for a class of nonlinear
multiagent systems with actuator faults and faulty networks.
<em>TNNLS</em>, <em>33</em>(8), 3474–3486. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of fault-tolerant consensus control of a general nonlinear multiagent system subject to actuator faults and disturbed and faulty networks. By using neural network (NN) and adaptive control techniques, estimations of unknown state-dependent boundaries of nonlinear dynamics and actuator faults, which can reflect the worst impacts on the system, are first developed. A novel NN-based adaptive observer is designed for the observation of faulty transformation signals in networks. On the basis of the NN-based observer and adaptive control strategies, fault-tolerant consensus control schemes are designed to guarantee the bounded consensus of the closed-loop multiagent system with disturbed and faulty networks and actuator faults. The validity of the proposed adaptively distributed consensus control schemes is demonstrated by a multiagent system composed of five nonlinear forced pendulums.},
  archive      = {J_TNNLS},
  author       = {Xiaozheng Jin and Shaoyu Lü and Jiguo Yu},
  doi          = {10.1109/TNNLS.2021.3053112},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3474-3486},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive NN-based consensus for a class of nonlinear multiagent systems with actuator faults and faulty networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed group coordination of multiagent systems in
cloud computing systems using a model-free adaptive predictive control
strategy. <em>TNNLS</em>, <em>33</em>(8), 3461–3473. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the group coordinated control problem for distributed nonlinear multiagent systems (MASs) with unknown dynamics. Cloud computing systems are employed to divide agents into groups and establish networked distributed multigroup-agent systems (ND-MGASs). To achieve the coordination of all agents and actively compensate for communication network delays, a novel networked model-free adaptive predictive control (NMFAPC) strategy combining networked predictive control theory with model-free adaptive control method is proposed. In the NMFAPC strategy, each nonlinear agent is described as a time-varying data model, which only relies on the system measurement data for adaptive learning. To analyze the system performance, a simultaneous analysis method for stability and consensus of ND-MGASs is presented. Finally, the effectiveness and practicability of the proposed NMFAPC strategy are verified by numerical simulations and experimental examples. The achievement also provides a solution for the coordination of large-scale nonlinear MASs.},
  archive      = {J_TNNLS},
  author       = {Haoran Tan and Yaonan Wang and Min Wu and Zhiwu Huang and Zhiqiang Miao},
  doi          = {10.1109/TNNLS.2021.3053016},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3461-3473},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed group coordination of multiagent systems in cloud computing systems using a model-free adaptive predictive control strategy},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). GenDet: Meta learning to generate detectors from few shots.
<em>TNNLS</em>, <em>33</em>(8), 3448–3460. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has made enormous progress and has been widely used in many applications. However, it performs poorly when only limited training data is available for novel classes that the model has never seen before. Most existing approaches solve few-shot detection tasks implicitly without directly modeling the detectors for novel classes. In this article, we propose GenDet, a new meta-learning-based framework that can effectively generate object detectors for novel classes from few shots and, thus, conducts few-shot detection tasks explicitly. The detector generator is trained by numerous few-shot detection tasks sampled from base classes each with sufficient samples, and thus, it is expected to generalize well on novel classes. An adaptive pooling module is further introduced to suppress distracting samples and aggregate the detectors generated from multiple shots. Moreover, we propose to train a reference detector for each base class in the conventional way, with which to guide the training of the detector generator. The reference detectors and the detector generator can be trained simultaneously. Finally, the generated detectors of different classes are encouraged to be orthogonal to each other for better generalization. The proposed approach is extensively evaluated on the ImageNet, VOC, and COCO data sets under various few-shot detection settings, and it achieves new state-of-the-art results.},
  archive      = {J_TNNLS},
  author       = {Liyang Liu and Bochao Wang and Zhanghui Kuang and Jing-Hao Xue and Yimin Chen and Wenming Yang and Qingmin Liao and Wayne Zhang},
  doi          = {10.1109/TNNLS.2021.3053005},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3448-3460},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {GenDet: Meta learning to generate detectors from few shots},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theory-inspired deep network for instantaneous-frequency
extraction and subsignals recovery from discrete blind-source data.
<em>TNNLS</em>, <em>33</em>(8), 3437–3447. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mathematical and engineering literature on signal processing and time-series analysis, there are two opposite points of view concerning the extraction of time-varying frequencies (commonly called instantaneous frequencies, IFs). One is to consider the given signal as a composite signal consisting of a finite number of subsignals that are oscillating, and the goal is to decompose the signal into the sum of the (unknown) subsignals, followed by extracting the IF from each subsignal; the other is first to extract from the given signal, the IFs of the (unknown) subsignals, from which the subsignals that constitute the given signal are recovered. Let us call the first the “signal decomposition approach” and the second the “signal resolution approach.” For the “signal decomposition approach,” rigorous mathematical theories on function decomposition have been well developed in the mathematical literature, with the most relevant one, called “atomic decomposition” initiated by R. Coifman, with various extensions by others, notably by D. Donoho, with the goal of extracting the signal building blocks, but without concern of which building blocks constitute any of the subsignals, and consequently, the subsignals along with their IFs cannot be recovered. On the other hand, the most popular of the decomposition approach is the “empirical mode decomposition (EMD),” proposed by N. Huang et al. , with many variations by others. In contrast to atomic decomposition, all variations of EMD are ad hoc algorithms, without any rigorous mathematical theory. Unfortunately, all existing versions of EMD fail to resolve the inverse problem on the recovery of the subsignals that constitute the given composite signal, and consequently, extracting the IFs is not satisfactory. For example, EMD fails to extract even two IFs that are not far apart from each other. In contrast to the signal decomposition approach, the “signal resolution approach” has a very long history dated back to the Prony method, introduced by G. de Prony in 1795, for solving the inverse problem of time-invariant linear systems. On the other hand, for nonstationary signals, the synchrosqueezed wavelet transform (SST), proposed by I. Daubechies over a decade ago, with various extensions and variations by others, was introduced to resolving the inverse problem, by first extracting the IFs from some reference frequency, followed by recovering the subsignals. Unfortunately, the SST approximate IFs could not be separated when the target IFs are close to one another at certain time instants, and even if they could be separated, the approximation is usually not sufficiently accurate. For these reasons, some signal components could not be recovered, and those that could be recovered are usually inexact. More recently, we introduced and developed a more direct method, called signal separation operation (SSO), published in 2016, to accurately compute the IFs and to accurately recover all signal components even if some of the target IFs are close to each other. The main contributions of this article are twofold. First, the SSO method is extended from uniformly sampled data to arbitrarily sampled data. This method is localized as illustrated by a number of numerical examples, including components with different subsignal arrival and departure times. It also yields a short-term prediction of the digital components along with their IFs. Second, we present a novel theory-inspired implementation of our method as a deep neural network (DNN). We have proved that a major advantage of DNN over shallow networks is that DNN can take advantage of any inherent compositional structure in the target function, while shallow networks are necessarily blind to such structure. Therefore, DNN can avoid the so-called curse of dimensionality using what we have called the blessing of compositionality. However, the compositional structure of the target function is not uniquely defined, and the constituent functions are typically not known so that the networks still need to be trained end-to-end. In contrast, the DNN introduced in this article implements a mathematical procedure so that no training is required at all, and the compositional structure is evident from the procedure. We will disclose the extension of the SSO method in Sections II and III and explain the construction of the deep network in Section IV .},
  archive      = {J_TNNLS},
  author       = {Ningning Han and H. N. Mhaskar and Charles K. Chui},
  doi          = {10.1109/TNNLS.2021.3052966},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3437-3447},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Theory-inspired deep network for instantaneous-frequency extraction and subsignals recovery from discrete blind-source data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underexposed image correction via hybrid priors navigated
deep propagation. <em>TNNLS</em>, <em>33</em>(8), 3425–3436. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing visual quality for underexposed images is an extensively concerning task that plays an important role in various areas of multimedia and computer vision. Most existing methods often fail to generate high-quality results with appropriate luminance and abundant details. To address these issues, we develop a novel framework, integrating both knowledge from physical principles and implicit distributions from data to address underexposed image correction. More concretely, we propose a new perspective to formulate this task as an energy-inspired model with advanced hybrid priors. A propagation procedure navigated by the hybrid priors is well designed for simultaneously propagating the reflectance and illumination toward desired results. We conduct extensive experiments to verify the necessity of integrating both underlying principles (i.e., with knowledge) and distributions (i.e., from data) as navigated deep propagation. Plenty of experimental results of underexposed image correction demonstrate that our proposed method performs favorably against the state-of-the-art methods on both subjective and objective assessments. In addition, we execute the task of face detection to further verify the naturalness and practical value of underexposed image correction. What is more, we apply our method to solve single-image haze removal whose experimental results further demonstrate our superiorities.},
  archive      = {J_TNNLS},
  author       = {Risheng Liu and Long Ma and Yuxi Zhang and Xin Fan and Zhongxuan Luo},
  doi          = {10.1109/TNNLS.2021.3052903},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3425-3436},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Underexposed image correction via hybrid priors navigated deep propagation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving complex-valued time-varying linear matrix equations
via QR decomposition with applications to robotic motion tracking and on
angle-of-arrival localization. <em>TNNLS</em>, <em>33</em>(8),
3415–3424. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of solving linear equations is considered as one of the fundamental problems commonly encountered in science and engineering. In this article, the complex-valued time-varying linear matrix equation (CVTV-LME) problem is investigated. Then, by employing a complex-valued, time-varying QR (CVTVQR) decomposition, the zeroing neural network (ZNN) method, equivalent transformations, Kronecker product, and vectorization techniques, we propose and study a CVTVQR decomposition-based linear matrix equation (CVTVQR-LME) model. In addition to the usage of the QR decomposition, the further advantage of the CVTVQR-LME model is reflected in the fact that it can handle a linear system with square or rectangular coefficient matrix in both the matrix and vector cases. Its efficacy in solving the CVTV-LME problems have been tested in a variety of numerical simulations as well as in two applications, one in robotic motion tracking and the other in angle-of-arrival localization.},
  archive      = {J_TNNLS},
  author       = {Vasilios N. Katsikis and Spyridon D. Mourtas and Predrag S. Stanimirović and Yunong Zhang},
  doi          = {10.1109/TNNLS.2021.3052896},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3415-3424},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Solving complex-valued time-varying linear matrix equations via QR decomposition with applications to robotic motion tracking and on angle-of-arrival localization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Detachable second-order pooling: Toward high-performance
first-order networks. <em>TNNLS</em>, <em>33</em>(8), 3400–3414. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Second-order pooling has proved to be more effective than its first-order counterpart in visual classification tasks. However, second-order pooling suffers from the high demand for a computational resource, limiting its use in practical applications. In this work, we present a novel architecture, namely a detachable second-order pooling network, to leverage the advantage of second-order pooling by first-order networks while keeping the model complexity unchanged during inference. Specifically, we introduce second-order pooling at the end of a few auxiliary branches and plug them into different stages of a convolutional neural network. During the training stage, the auxiliary second-order pooling networks assist the backbone first-order network to learn more discriminative feature representations. When training is completed, all auxiliary branches can be removed, and only the backbone first-order network is used for inference. Experiments conducted on CIFAR-10, CIFAR-100, and ImageNet data sets clearly demonstrated the leading performance of our network, which achieves even higher accuracy than second-order networks but keeps the low inference complexity of first-order networks.},
  archive      = {J_TNNLS},
  author       = {Lida Li and Jiangtao Xie and Peihua Li and Lei Zhang},
  doi          = {10.1109/TNNLS.2021.3052829},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3400-3414},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Detachable second-order pooling: Toward high-performance first-order networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal encoding and multispike learning framework for
efficient recognition of visual patterns. <em>TNNLS</em>,
<em>33</em>(8), 3387–3399. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological systems under a parallel and spike-based computation endow individuals with abilities to have prompt and reliable responses to different stimuli. Spiking neural networks (SNNs) have thus been developed to emulate their efficiency and to explore principles of spike-based processing. However, the design of a biologically plausible and efficient SNN for image classification still remains as a challenging task. Previous efforts can be generally clustered into two major categories in terms of coding schemes being employed: rate and temporal. The rate-based schemes suffer inefficiency, whereas the temporal-based ones typically end with a relatively poor performance in accuracy. It is intriguing and important to develop an SNN with both efficiency and efficacy being considered. In this article, we focus on the temporal-based approaches in a way to advance their accuracy performance by a great margin while keeping the efficiency on the other hand. A new temporal-based framework integrated with the multispike learning is developed for efficient recognition of visual patterns. Different approaches of encoding and learning under our framework are evaluated with the MNIST and Fashion-MNIST data sets. Experimental results demonstrate the efficient and effective performance of our temporal-based approaches across a variety of conditions, improving accuracies to higher levels that are even comparable to rate-based ones but importantly with a lighter network structure and far less number of spikes. This article attempts to extend the advanced multispike learning to the challenging task of image recognition and bring state of the arts in temporal-based approaches to a novel level. The experimental results could be potentially favorable to low-power and high-speed requirements in the field of artificial intelligence and contribute to attract more efforts toward brain-like computing.},
  archive      = {J_TNNLS},
  author       = {Qiang Yu and Shiming Song and Chenxiang Ma and Jianguo Wei and Shengyong Chen and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2021.3052804},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3387-3399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal encoding and multispike learning framework for efficient recognition of visual patterns},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Element-wise feature relation learning network for
cross-spectral image patch matching. <em>TNNLS</em>, <em>33</em>(8),
3372–3386. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the majority of successful matching approaches are based on convolutional neural networks, which focus on learning the invariant and discriminative features for individual image patches based on image content. However, the image patch matching task is essentially to predict the matching relationship of patch pairs, that is, matching (similar) or non-matching (dissimilar). Therefore, we consider that the feature relation (FR) learning is more important than individual feature learning for image patch matching problem. Motivated by this, we propose an element-wise FR learning network for image patch matching, which transforms the image patch matching task into an image relationship-based pattern classification problem and dramatically improves generalization performances on image matching. Meanwhile, the proposed element-wise learning methods encourage full interaction between feature information and can naturally learn FR. Moreover, we propose to aggregate FR from multilevels, which integrates the multiscale FR for more precise matching. Experimental results demonstrate that our proposal achieves superior performances on cross-spectral image patch matching and single spectral image patch matching, and good generalization on image patch retrieval.},
  archive      = {J_TNNLS},
  author       = {Dou Quan and Shuang Wang and Ning Huyan and Jocelyn Chanussot and Ruojing Wang and Xuefeng Liang and Biao Hou and Licheng Jiao},
  doi          = {10.1109/TNNLS.2021.3052756},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3372-3386},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Element-wise feature relation learning network for cross-spectral image patch matching},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parkinson’s disease classification and clinical score
regression via united embedding and sparse learning from longitudinal
data. <em>TNNLS</em>, <em>33</em>(8), 3357–3371. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is known as an irreversible neurodegenerative disease that mainly affects the patient’s motor system. Early classification and regression of PD are essential to slow down this degenerative process from its onset. In this article, a novel adaptive unsupervised feature selection approach is proposed by exploiting manifold learning from longitudinal multimodal data. Classification and clinical score prediction are performed jointly to facilitate early PD diagnosis. Specifically, the proposed approach performs united embedding and sparse regression, which can determine the similarity matrices and discriminative features adaptively. Meanwhile, we constrain the similarity matrix among subjects and exploit the ${l}_{\mathrm {2,p}}$ norm to conduct sparse adaptive control for obtaining the intrinsic information of the multimodal data structure. An effective iterative optimization algorithm is proposed to solve this problem. We perform abundant experiments on the Parkinson’s Progression Markers Initiative (PPMI) data set to verify the validity of the proposed approach. The results show that our approach boosts the performance on the classification and clinical score regression of longitudinal data and surpasses the state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Zhongwei Huang and Haijun Lei and Guoliang Chen and Alejandro F. Frangi and Yanwu Xu and Ahmed Elazab and Jing Qin and Baiying Lei},
  doi          = {10.1109/TNNLS.2021.3052652},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3357-3371},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parkinson’s disease classification and clinical score regression via united embedding and sparse learning from longitudinal data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imbalanced data classification via cooperative interaction
between classifier and generator. <em>TNNLS</em>, <em>33</em>(8),
3343–3356. (<a
href="https://doi.org/10.1109/TNNLS.2021.3052243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning classifiers with imbalanced data can be strongly biased toward the majority class. To address this issue, several methods have been proposed using generative adversarial networks (GANs). Existing GAN-based methods, however, do not effectively utilize the relationship between a classifier and a generator. This article proposes a novel three-player structure consisting of a discriminator, a generator, and a classifier, along with decision boundary regularization. Our method is distinctive in which the generator is trained in cooperation with the classifier to provide minority samples that gradually expand the minority decision region, improving performance for imbalanced data classification. The proposed method outperforms the existing methods on real data sets as well as synthetic imbalanced data sets.},
  archive      = {J_TNNLS},
  author       = {Hyun-Soo Choi and Dahuin Jung and Siwon Kim and Sungroh Yoon},
  doi          = {10.1109/TNNLS.2021.3052243},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3343-3356},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Imbalanced data classification via cooperative interaction between classifier and generator},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive neural network control for full-state constrained
robotic manipulator with actuator saturation and time-varying delays.
<em>TNNLS</em>, <em>33</em>(8), 3331–3342. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptive neural network (NN) control method for an ${n}$ -link constrained robotic manipulator. Driven by actual demands, manipulator and actuator dynamics, state and input constraints, and unknown time-varying delays are taken into account simultaneously. NNs are employed to approximate unknown nonlinearities. Time-varying barrier Lyapunov functions are utilized to cope with full-state constraints. By resorting to saturation function and Lyapunov–Krasovskii functionals, the effects of actuator saturation and time delays are eliminated. It is proved that all the closed-loop signals are semiglobally uniformly ultimately bounded, full-state constraints and actuator saturation are not violated, and error signals remain within compact sets around zero. Simulation studies are given to demonstrate the validity and advantages of this control scheme.},
  archive      = {J_TNNLS},
  author       = {Weiwei Sun and You Wu and Xinyu Lv},
  doi          = {10.1109/TNNLS.2021.3051946},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3331-3342},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural network control for full-state constrained robotic manipulator with actuator saturation and time-varying delays},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Deep attention-based imbalanced image classification.
<em>TNNLS</em>, <em>33</em>(8), 3320–3330. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a common problem in real-world image classification problems, some classes are with abundant data, and the other classes are not. In this case, the representations of classifiers are likely to be biased toward the majority classes and it is challenging to learn proper features, leading to unpromising performance. To eliminate this biased feature representation, many algorithm-level methods learn to pay more attention to the minority classes explicitly according to the prior knowledge of the data distribution. In this article, an attention-based approach called deep attention-based imbalanced image classification (DAIIC) is proposed to automatically pay more attention to the minority classes in a data-driven manner. In the proposed method, an attention network and a novel attention augmented logistic regression function are employed to encapsulate as many features, which belongs to the minority classes, as possible into the discriminative feature learning process by assigning the attention for different classes jointly in both the prediction and feature spaces. With the proposed object function, DAIIC can automatically learn the misclassification costs for different classes. Then, the learned misclassification costs can be used to guide the training process to learn more discriminative features using the designed attention networks. Furthermore, the proposed method is applicable to various types of networks and data sets. Experimental results on both single-label and multilabel imbalanced image classification data sets show that the proposed method has good generalizability and outperforms several state-of-the-art methods for imbalanced image classification.},
  archive      = {J_TNNLS},
  author       = {Lituan Wang and Lei Zhang and Xiaofeng Qi and Zhang Yi},
  doi          = {10.1109/TNNLS.2021.3051721},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3320-3330},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep attention-based imbalanced image classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Generalized nonconvex approach for low-tubal-rank tensor
recovery. <em>TNNLS</em>, <em>33</em>(8), 3305–3319. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tensor–tensor product-induced tensor nuclear norm (t-TNN) (Lu et al. , 2020) minimization for low-tubal-rank tensor recovery attracts broad attention recently. However, minimizing the t-TNN faces some drawbacks. For example, the obtained solution could be suboptimal to the original problem due to its loose approximation. In this article, we extract a unified nonconvex surrogate of the tensor tubal rank as a tighter regularizer, which involves many popular nonconvex penalty functions. An iterative reweighted t-TNN algorithm is proposed to solve the resulting generalized nonconvex tubal rank minimization for tensor recovery. It converges to a critical point globally with rigorous proofs based on the Kurdyka–Łojasiwicz property. Furthermore, we provide the theoretical guarantees for exact and robust recovery by developing the tensor null space property. Extensive experiments demonstrate that our approach markedly enhances recovery performance compared with several state-of-the-art convex and nonconvex methods.},
  archive      = {J_TNNLS},
  author       = {Hailin Wang and Feng Zhang and Jianjun Wang and Tingwen Huang and Jianwen Huang and Xinling Liu},
  doi          = {10.1109/TNNLS.2021.3051650},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3305-3319},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized nonconvex approach for low-tubal-rank tensor recovery},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DQC-ADMM: Decentralized dynamic ADMM with quantized and
censored communications. <em>TNNLS</em>, <em>33</em>(8), 3290–3304. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed learning and optimization, a network of multiple computing units coordinates to solve a large-scale problem. This article focuses on dynamic optimization over a decentralized network. We develop a communication-efficient algorithm based on the alternating direction method of multipliers (ADMM) with quantized and censored communications, termed DQC-ADMM. At each time of the algorithm, the nodes collaborate to minimize the summation of their time-varying, local objective functions. Through local iterative computation and communication, DQC-ADMM is able to track the time-varying optimal solution. Different from traditional approaches requiring transmissions of the exact local iterates among the neighbors at every time, we propose to quantize the transmitted information, as well as adopt a communication-censoring strategy for the sake of reducing the communication cost in the optimization process. To be specific, a node transmits the quantized version of the local information to its neighbors, if and only if the value sufficiently deviates from the one previously transmitted. We theoretically justify that the proposed DQC-ADMM is capable of tracking the time-varying optimal solution, subject to a bounded error caused by the quantized and censored communications, as well as the system dynamics. Through numerical experiments, we evaluate the tracking performance and communication savings of the proposed DQC-ADMM.},
  archive      = {J_TNNLS},
  author       = {Yaohua Liu and Gang Wu and Zhi Tian and Qing Ling},
  doi          = {10.1109/TNNLS.2021.3051638},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3290-3304},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DQC-ADMM: Decentralized dynamic ADMM with quantized and censored communications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DAMAD: Database, attack, and model agnostic adversarial
perturbation detector. <em>TNNLS</em>, <em>33</em>(8), 3277–3289. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial perturbations have demonstrated the vulnerabilities of deep learning algorithms to adversarial attacks. Existing adversary detection algorithms attempt to detect the singularities; however, they are in general, loss-function, database, or model dependent. To mitigate this limitation, we propose DAMAD —a generalized perturbation detection algorithm which is agnostic to model architecture, training data set, and loss function used during training. The proposed adversarial perturbation detection algorithm is based on the fusion of autoencoder embedding and statistical texture features extracted from convolutional neural networks. The performance of DAMAD is evaluated on the challenging scenarios of cross-database, cross-attack, and cross-architecture training and testing along with traditional evaluation of testing on the same database with known attack and model. Comparison with state-of-the-art perturbation detection algorithms showcase the effectiveness of the proposed algorithm on six databases: ImageNet, CIFAR-10, Multi-PIE, MEDS, point and shoot challenge (PaSC), and MNIST. Performance evaluation with nearly a quarter of a million adversarial and original images and comparison with recent algorithms show the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Akshay Agarwal and Gaurav Goswami and Mayank Vatsa and Richa Singh and Nalini K. Ratha},
  doi          = {10.1109/TNNLS.2021.3051529},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3277-3289},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DAMAD: Database, attack, and model agnostic adversarial perturbation detector},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Encoder-x: Solving unknown coefficients automatically in
polynomial fitting by using an autoencoder. <em>TNNLS</em>,
<em>33</em>(8), 3264–3276. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling, prediction, and recognition tasks depend on the proper representation of the objective curves and surfaces. Polynomial functions have been proved to be a powerful tool for representing curves and surfaces. Until now, various methods have been used for polynomial fitting. With a recent boom in neural networks, researchers have attempted to solve polynomial fitting by using this end-to-end model, which has a powerful fitting ability. However, the current neural network-based methods are poor in stability and slow in convergence speed. In this article, we develop a novel neural network-based method, called Encoder-X, for polynomial fitting, which can solve not only the explicit polynomial fitting but also the implicit polynomial fitting. The method regards polynomial coefficients as the feature value of raw data in a polynomial space expression and therefore polynomial fitting can be achieved by a special autoencoder. The entire model consists of an encoder defined by a neural network and a decoder defined by a polynomial mathematical expression. We input sampling points into an encoder to obtain polynomial coefficients and then input them into a decoder to output the predicted function value. The error between the predicted function value and the true function value can update parameters in the encoder. The results prove that this method is better than the compared methods in terms of stability, convergence, and accuracy. In addition, Encoder-X can be used for solving other mathematical modeling tasks.},
  archive      = {J_TNNLS},
  author       = {Guojun Wang and Weijun Li and Liping Zhang and Linjun Sun and Peng Chen and Lina Yu and Xin Ning},
  doi          = {10.1109/TNNLS.2021.3051430},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3264-3276},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Encoder-X: Solving unknown coefficients automatically in polynomial fitting by using an autoencoder},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid system based on dynamic selection for time series
forecasting. <em>TNNLS</em>, <em>33</em>(8), 3251–3263. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid systems, which combine statistical and machine learning (ML) techniques using residual (error forecasting) modeling, have been highlighted in the literature due to their accuracy and ability to forecast time series with different characteristics. In these architectures, a crucial task is the proper modeling of the residuals since they may present random fluctuations, complex nonlinear patterns, and heteroscedastic behavior. Hence, the selection, specification, and training of one ML model to forecast the residuals are costly and challenging tasks since issues, such as underfitting, overfitting, and misspecification, can lead to a system with low accuracy or even deteriorate the linear forecast of the time series. This article proposes a hybrid system, named dynamic residual forecasting (DReF), that employs a modified dynamic selection (DS) algorithm to decide: the most suitable ML model to forecast a pattern of the residual series and if it is a promising candidate to increase the accuracy of the time series forecast from the linear combination. Thus, the DReF aims to reduce the uncertainty of the ML model selection and avoid the deterioration of the time series forecast. Furthermore, the proposed system searches for the most suitable parameters of the DS algorithm for each data set. In this article, the proposed method uses a pool of five ML models widely adopted in the literature: multilayer perceptron, support vector regression, radial basis function, long short-term memory, and convolutional neural network. An experimental evaluation was conducted using ten well-known time series. The results show that the DReF obtains superior results for the majority of the data sets compared with single and hybrid models of the literature.},
  archive      = {J_TNNLS},
  author       = {João F. L. de Oliveira and Eraylson G. Silva and Paulo S. G. de Mattos Neto},
  doi          = {10.1109/TNNLS.2021.3051384},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3251-3263},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid system based on dynamic selection for time series forecasting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neuron linear transformation: Modeling the domain shift for
crowd counting. <em>TNNLS</em>, <em>33</em>(8), 3238–3250. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain crowd counting (CDCC) is a hot topic due to its importance in public safety. The purpose of CDCC is to alleviate the domain shift between the source and target domain. Recently, typical methods attempt to extract domain-invariant features via image translation and adversarial learning. When it comes to specific tasks, we find that the domain shifts are reflected in model parameters’ differences. To describe the domain gap directly at the parameter level, we propose a neuron linear transformation (NLT) method, exploiting domain factor and bias weights to learn the domain shift. Specifically, for a specific neuron of a source model, NLT exploits few labeled target data to learn domain shift parameters. Finally, the target neuron is generated via a linear transformation. Extensive experiments and analysis on six real-world data sets validate that NLT achieves top performance compared with other domain adaptation methods. An ablation study also shows that the NLT is robust and more effective than supervised and fine-tune training. Code is available at https://github.com/taohan10200/NLT .},
  archive      = {J_TNNLS},
  author       = {Qi Wang and Tao Han and Junyu Gao and Yuan Yuan},
  doi          = {10.1109/TNNLS.2021.3051371},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3238-3250},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuron linear transformation: Modeling the domain shift for crowd counting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boundary stabilization of stochastic delayed cohen–grossberg
neural networks with diffusion terms. <em>TNNLS</em>, <em>33</em>(8),
3227–3237. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers the boundary stabilization for stochastic delayed Cohen–Grossberg neural networks (SDCGNNs) with diffusion terms by the Lyapunov functional method. In the realization of NNs, sometimes time delays and diffusion phenomenon cannot be ignored, so Cohen–Grossberg NNs with time delays and diffusion terms are studied in this article. Moreover, different from the previously distributed control, the boundary control is used to stabilize the system, which can reduce the spatial cost of the controller and is easy to implement. Boundary controllers are presented for system with Neumann boundary and mixed boundary conditions, and criteria are derived such that the controlled system achieves mean-square exponential stabilization. Based on the criterion, the effects of diffusion matrix, coupling strength, coupling matrix, and time delays on exponentially stability are analyzed. In the process of analysis, two difficulties need to be addressed: 1) how to introduce boundary control into system analysis? and 2) how to analyze the influence of system parameters on stability? We deal with these problems by using Poincaré’s inequality and Schur’s complement lemma. Moreover, mean-square exponential synchronization of stochastic delayed Hopfield NNs with diffusion terms, as an application of the theoretical result, is considered under the boundary control. Examples are given to illustrate the effectiveness of the theoretical results.},
  archive      = {J_TNNLS},
  author       = {Xiao-Zhen Liu and Kai-Ning Wu and Xiaohua Ding and Weihai Zhang},
  doi          = {10.1109/TNNLS.2021.3051363},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3227-3237},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Boundary stabilization of stochastic delayed Cohen–Grossberg neural networks with diffusion terms},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synchronization of complex dynamical networks subject to
noisy sampling interval and packet loss. <em>TNNLS</em>, <em>33</em>(8),
3216–3226. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the sampled-data synchronization issue for a class of complex dynamical networks (CDNs) subject to noisy sampling intervals and successive packet losses. The sampling intervals are subject to noisy perturbations, and categorical distribution is used to characterize the sampling errors of noisy sampling intervals. By means of the input delay approach, the CDN under consideration is first converted into a delay system with delayed input subject to dual randomness and probability distribution characteristic. To verify the probability distribution characteristic of the delayed input, a novel characterization method is proposed, which is not the same as that of some existing literature. Based on this, a unified framework is then established. By recurring to the techniques of stochastic analysis, a probability-distribution-dependent controller is designed to guarantee the mean-square exponential synchronization of the error dynamical network. Subsequently, a special model is considered where only the lower and upper bounds of delayed input are utilized. Finally, to verify the analysis results and testify the effectiveness and superiority of the designed synchronization algorithm, a numerical example and an example using Chua’s circuit are given.},
  archive      = {J_TNNLS},
  author       = {Zhipei Hu and Hongru Ren and Peng Shi},
  doi          = {10.1109/TNNLS.2021.3051052},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3216-3226},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synchronization of complex dynamical networks subject to noisy sampling interval and packet loss},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New generation deep learning for video object detection: A
survey. <em>TNNLS</em>, <em>33</em>(8), 3195–3215. (<a
href="https://doi.org/10.1109/TNNLS.2021.3053249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection, a basic task in the computer vision field, is rapidly evolving and widely used. In recent years, deep learning methods have rapidly become widespread in the field of video object detection, achieving excellent results compared with those of traditional methods. However, the presence of duplicate information and abundant spatiotemporal information in video data poses a serious challenge to video object detection. Therefore, in recent years, many scholars have investigated deep learning detection algorithms in the context of video data and have achieved remarkable results. Considering the wide range of applications, a comprehensive review of the research related to video object detection is both a necessary and challenging task. This survey attempts to link and systematize the latest cutting-edge research on video object detection with the goal of classifying and analyzing video detection algorithms based on specific representative models. The differences and connections between video object detection and similar tasks are systematically demonstrated, and the evaluation metrics and video detection performance of nearly 40 models on two data sets are presented. Finally, the various applications and challenges facing video object detection are discussed.},
  archive      = {J_TNNLS},
  author       = {Licheng Jiao and Ruohan Zhang and Fang Liu and Shuyuan Yang and Biao Hou and Lingling Li and Xu Tang},
  doi          = {10.1109/TNNLS.2021.3053249},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {8},
  pages        = {3195-3215},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {New generation deep learning for video object detection: A survey},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DNN-kWTA with bounded random offset voltage drifts in
threshold logic units. <em>TNNLS</em>, <em>33</em>(7), 3184–3192. (<a
href="https://doi.org/10.1109/TNNLS.2021.3050493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dual neural network-based $k$ -winner-take-all (DNN- $k$ WTA) is an analog neural model that is used to identify the $k$ largest numbers from $n$ inputs. Since threshold logic units (TLUs) are key elements in the model, offset voltage drifts in TLUs may affect the operational correctness of a DNN- $k$ WTA network. Previous studies assume that drifts in TLUs follow some particular distributions. This brief considers that only the drift range, given by $[-\Delta, \Delta]$ , is available. We consider two drift cases: time-invariant and time-varying. For the time-invariant case, we show that the state of a DNN- $k$ WTA network converges. The sufficient condition to make a network with the correct operation is given. Furthermore, for uniformly distributed inputs, we prove that the probability that a DNN- $k$ WTA network operates properly is greater than $(1-2\Delta)^{n}$ . The aforementioned results are generalized for the time-varying case. In addition, for the time-invariant case, we derive a method to compute the exact convergence time for a given data set. For uniformly distributed inputs, we further derive the mean and variance of the convergence time. The convergence time results give us an idea about the operational speed of the DNN- $k$ WTA model. Finally, simulation experiments have been conducted to validate those theoretical results.},
  archive      = {J_TNNLS},
  author       = {Wenhao Lu and Chi-Sing Leung and John Sum and Yi Xiao},
  doi          = {10.1109/TNNLS.2021.3050493},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3184-3192},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DNN-kWTA with bounded random offset voltage drifts in threshold logic units},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequence learning in a single trial: A spiking neurons model
based on hippocampal circuitry. <em>TNNLS</em>, <em>33</em>(7),
3178–3183. (<a
href="https://doi.org/10.1109/TNNLS.2021.3049281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast with our everyday experience using brain circuits, it can take a prohibitively long time to train a computational system to produce the correct sequence of outputs in the presence of a series of inputs. This suggests that something important is missing in the way in which models are trying to reproduce basic cognitive functions. In this work, we introduce a new neuronal network architecture that is able to learn, in a single trial, an arbitrary long sequence of any known objects. The key point of the model is the explicit use of mechanisms and circuitry observed in the hippocampus, which allow the model to reach a level of efficiency and accuracy that, to the best of our knowledge, is not possible with abstract network implementations. By directly following the natural system’s layout and circuitry, this type of implementation has the additional advantage that the results can be more easily compared to the experimental data, allowing a deeper and more direct understanding of the mechanisms underlying cognitive functions and dysfunctions and opening the way to a new generation of learning architectures.},
  archive      = {J_TNNLS},
  author       = {Simone Coppolino and Giuseppe Giacopelli and Michele Migliore},
  doi          = {10.1109/TNNLS.2021.3049281},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3178-3183},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sequence learning in a single trial: A spiking neurons model based on hippocampal circuitry},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven-based event-triggered control for nonlinear CPSs
against jamming attacks. <em>TNNLS</em>, <em>33</em>(7), 3171–3177. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief considers the security control problem for nonlinear cyber–physical systems (CPSs) against jamming attacks. First, a novel event-based model-free adaptive control (MFAC) framework is established. Second, a multistep predictive compensation algorithm (PCA) is developed to make compensation for the lost data caused by jamming attacks, even consecutive attacks. Then, an event-triggering mechanism with the dead-zone operator is introduced in the adaptive controller, which can effectively save communication resources and reduce the calculation burden of the controller without affecting the control performance of systems. Moreover, the boundedness of the tracking error is ensured in the mean-square sense, and only the input/output (I/O) data are used in the whole design process. Finally, simulation comparisons are provided to show the effectiveness of our method.},
  archive      = {J_TNNLS},
  author       = {Yingchun Wang and Xiaojie Qiu and Huaguang Zhang and Xiangpeng Xie},
  doi          = {10.1109/TNNLS.2020.3047931},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3171-3177},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven-based event-triggered control for nonlinear CPSs against jamming attacks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rank consistency induced multiview subspace clustering via
low-rank matrix factorization. <em>TNNLS</em>, <em>33</em>(7),
3157–3170. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview subspace clustering has been demonstrated to achieve excellent performance in practice by exploiting multiview complementary information. One of the strategies used in most existing methods is to learn a shared self-expressiveness coefficient matrix for all the view data. Different from such a strategy, this article proposes a rank consistency induced multiview subspace clustering model to pursue a consistent low-rank structure among view-specific self-expressiveness coefficient matrices. To facilitate a practical model, we parameterize the low-rank structure on all self-expressiveness coefficient matrices through the tri-factorization along with orthogonal constraints. This specification ensures that self-expressiveness coefficient matrices of different views have the same rank to effectively promote structural consistency across multiviews. Such a model can learn a consistent subspace structure and fully exploit the complementary information from the view-specific self-expressiveness coefficient matrices, simultaneously. The proposed model is formulated as a nonconvex optimization problem. An efficient optimization algorithm with guaranteed convergence under mild conditions is proposed. Extensive experiments on several benchmark databases demonstrate the advantage of the proposed model over the state-of-the-art multiview clustering approaches.},
  archive      = {J_TNNLS},
  author       = {Jipeng Guo and Yanfeng Sun and Junbin Gao and Yongli Hu and Baocai Yin},
  doi          = {10.1109/TNNLS.2021.3071797},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3157-3170},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rank consistency induced multiview subspace clustering via low-rank matrix factorization},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Granger causality inference in EEG source connectivity
analysis: A state-space approach. <em>TNNLS</em>, <em>33</em>(7),
3146–3156. (<a
href="https://doi.org/10.1109/TNNLS.2021.3096642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of estimating brain effective connectivity from electroencephalogram (EEG) signals using a Granger causality (GC) characterized on state-space models, extended from the conventional vector autoregressive (VAR) process. The scheme involves two main steps: model estimation and model inference to estimate brain connectivity. The model estimation performs a subspace identification and active source selection based on group-norm regularized least-squares. The model inference relies on the concept of state-space GC that requires solving a Riccati equation for the covariance of estimation error. We verify the performance on simulated datasets that represent realistic human brain activities under several conditions, including percentages and location of active sources, and the number of EEG electrodes. Our model’s accuracy in estimating connectivity is compared with a two-stage approach using source reconstructions and a VAR-based Granger analysis. Our method achieved better performances than the two-stage approach under the assumptions that the true source dynamics are sparse and generated from state-space models. When the method was applied to a real EEG SSVEP dataset, the temporal lobe was found to be a mediating connection between the temporal and occipital areas, which agreed with findings in previous studies.},
  archive      = {J_TNNLS},
  author       = {Parinthorn Manomaisaowapak and Anawat Nartkulpat and Jitkomut Songsiri},
  doi          = {10.1109/TNNLS.2021.3096642},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3146-3156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Granger causality inference in EEG source connectivity analysis: A state-space approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Observer-based neuro-adaptive optimized control of
strict-feedback nonlinear systems with state constraints.
<em>TNNLS</em>, <em>33</em>(7), 3131–3145. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptive neural network (NN) output feedback optimized control design for a class of strict-feedback nonlinear systems that contain unknown internal dynamics and the states that are immeasurable and constrained within some predefined compact sets. NNs are used to approximate the unknown internal dynamics, and an adaptive NN state observer is developed to estimate the immeasurable states. By constructing a barrier type of optimal cost functions for subsystems and employing an observer and the actor-critic architecture, the virtual and actual optimal controllers are developed under the framework of backstepping technique. In addition to ensuring the boundedness of all closed-loop signals, the proposed strategy can also guarantee that system states are confined within some preselected compact sets all the time. This is achieved by means of barrier Lyapunov functions which have been successfully applied to various kinds of nonlinear systems such as strict-feedback and pure-feedback dynamics. Besides, our developed optimal controller requires less conditions on system dynamics than some existing approaches concerning optimal control. The effectiveness of the proposed optimal control approach is eventually validated by numerical as well as practical examples.},
  archive      = {J_TNNLS},
  author       = {Yongming Li and Yanjun Liu and Shaocheng Tong},
  doi          = {10.1109/TNNLS.2021.3051030},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3131-3145},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based neuro-adaptive optimized control of strict-feedback nonlinear systems with state constraints},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explicit duration recurrent networks. <em>TNNLS</em>,
<em>33</em>(7), 3120–3130. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) can be used to operate over sequences of vectors and have been successfully applied to a variety of problems. However, it is hard to use RNNs to model the variable dwell time of the hidden state underlying an input sequence. In this article, we interpret the typical RNNs, including original RNN, standard long short-term memory (LSTM), peephole LSTM, projected LSTM, and gated recurrent unit (GRU), using a slightly extended hidden Markov model (HMM). Based on this interpretation, we are motivated to propose a novel RNN, called explicit duration recurrent network (EDRN), analog to a hidden semi-Markov model (HSMM). It has a better performance than conventional LSTMs and can explicitly model any duration distribution function of the hidden state. The model parameters become interpretable and can be used to infer many other quantities that the conventional RNNs cannot obtain. Therefore, EDRN is expected to extend and enrich the applications of RNNs. The interpretation also suggests that the conventional RNNs, including LSTM and GRU, can be made small modifications to improve their performance without increasing the parameters of the networks.},
  archive      = {J_TNNLS},
  author       = {Shun-Zheng Yu},
  doi          = {10.1109/TNNLS.2021.3051019},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3120-3130},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Explicit duration recurrent networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer-based adaptive synchronization of multiagent
systems with unknown parameters under attacks. <em>TNNLS</em>,
<em>33</em>(7), 3109–3119. (<a
href="https://doi.org/10.1109/TNNLS.2021.3051017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the observer-based adaptive synchronization of multiagent systems (MASs) with unknown parameters under attacks. First, to estimate the state of agents, the observer for MAS is introduced. When disturbance, nonlinear function, and system model uncertainty are not considered, the nominal controller is proposed to achieve synchronization and state estimation. Then, in order to eliminate the effect of unknown parameters in the disturbance, nonlinear function, and system model uncertainty, the adaptive controller with switching term is introduced. However, the attack will lead to the destruction of the network topology so as the destruction of the nominal controller. By constructing an appropriate Lyapunov function, we analyze the effect caused by attacks, and the security control law is given to make sure the synchronization of the MASs under attacks. Finally, a numerical simulation is given to verify the validness of the obtained theorem.},
  archive      = {J_TNNLS},
  author       = {Shiping Wen and Xiaoze Ni and Huamin Wang and Song Zhu and Kaibo Shi and Tingwen Huang},
  doi          = {10.1109/TNNLS.2021.3051017},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3109-3119},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based adaptive synchronization of multiagent systems with unknown parameters under attacks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synaptic scaling—an artificial neural network regularization
inspired by nature. <em>TNNLS</em>, <em>33</em>(7), 3094–3108. (<a
href="https://doi.org/10.1109/TNNLS.2021.3050422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nature has always inspired the human spirit and scientists frequently developed new methods based on observations from nature. Recent advances in imaging and sensing technology allow fascinating insights into biological neural processes. With the objective of finding new strategies to enhance the learning capabilities of neural networks, we focus on a phenomenon that is closely related to learning tasks and neural stability in biological neural networks, called homeostatic plasticity. Among the theories that have been developed to describe homeostatic plasticity, synaptic scaling has been found to be the most mature and applicable. We systematically discuss previous studies on the synaptic scaling theory and how they could be applied to artificial neural networks. Therefore, we utilize information theory to analytically evaluate how mutual information is affected by synaptic scaling. Based on these analytic findings, we propose two flavors in which synaptic scaling can be applied in the training process of simple and complex, feedforward, and recurrent neural networks. We compare our approach with state-of-the-art regularization techniques on standard benchmarks. We found that the proposed method yields the lowest error in both regression and classification tasks compared to previous regularization approaches in our experiments across a wide range of network feedforward and recurrent topologies and data sets.},
  archive      = {J_TNNLS},
  author       = {Martin Hofmann and Patrick Mäder},
  doi          = {10.1109/TNNLS.2021.3050422},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3094-3108},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synaptic Scaling—An artificial neural network regularization inspired by nature},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient approximation of high-dimensional functions with
neural networks. <em>TNNLS</em>, <em>33</em>(7), 3079–3093. (<a
href="https://doi.org/10.1109/TNNLS.2021.3049719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a framework for showing that neural networks can overcome the curse of dimensionality in different high-dimensional approximation problems. Our approach is based on the notion of a catalog network, which is a generalization of a standard neural network in which the nonlinear activation functions can vary from layer to layer as long as they are chosen from a predefined catalog of functions. As such, catalog networks constitute a rich family of continuous functions. We show that under appropriate conditions on the catalog, catalog networks can efficiently be approximated with rectified linear unit-type networks and provide precise estimates on the number of parameters needed for a given approximation accuracy. As special cases of the general results, we obtain different classes of functions that can be approximated with recitifed linear unit networks without the curse of dimensionality.},
  archive      = {J_TNNLS},
  author       = {Patrick Cheridito and Arnulf Jentzen and Florian Rossmannek},
  doi          = {10.1109/TNNLS.2021.3049719},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3079-3093},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient approximation of high-dimensional functions with neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient sparse bayesian learning algorithm based on
gaussian-scale mixtures. <em>TNNLS</em>, <em>33</em>(7), 3065–3078. (<a
href="https://doi.org/10.1109/TNNLS.2020.3049056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Bayesian learning (SBL) is a popular machine learning approach with a superior generalization capability due to the sparsity of its adopted model. However, it entails a matrix inversion at each iteration, hindering its practical applications with large-scale data sets. To overcome this bottleneck, we propose an efficient SBL algorithm with $\mathcal {O}(n^{2})$ computational complexity per iteration based on a Gaussian-scale mixture prior model. By specifying two different hyperpriors, the proposed efficient SBL algorithm can meet two different requirements, such as high efficiency and high sparsity. A surrogate function is introduced herein to approximate the posterior density of model parameters and thereby to avoid matrix inversions. Using a data-dependent term, a joint cost function with separate penalty terms is reformulated in a joint space of model parameters and hyperparameters. The resulting nonconvex optimization problem is solved using a block coordinate descent method in a majorization–minimization framework. Finally, the results of extensive experiments for sparse signal recovery and sparse image reconstruction on benchmark problems are elaborated to substantiate the effectiveness and superiority of the proposed approach in terms of computational time and estimation error.},
  archive      = {J_TNNLS},
  author       = {Wei Zhou and Hai-Tao Zhang and Jun Wang},
  doi          = {10.1109/TNNLS.2020.3049056},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3065-3078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient sparse bayesian learning algorithm based on gaussian-scale mixtures},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MetaMixUp: Learning adaptive interpolation policy of MixUp
with metalearning. <em>TNNLS</em>, <em>33</em>(7), 3050–3064. (<a
href="https://doi.org/10.1109/TNNLS.2020.3049011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MixUp is an effective data augmentation method to regularize deep neural networks via random linear interpolations between pairs of samples and their labels. It plays an important role in model regularization, semisupervised learning (SSL), and domain adaption. However, despite its empirical success, its deficiency of randomly mixing samples has poorly been studied. Since deep networks are capable of memorizing the entire data set, the corrupted samples generated by vanilla MixUp with a badly chosen interpolation policy will degrade the performance of networks. To overcome overfitting to corrupted samples, inspired by metalearning (learning to learn), we propose a novel technique of learning to a mixup in this work, namely, MetaMixUp. Unlike the vanilla MixUp that samples interpolation policy from a predefined distribution, this article introduces a metalearning-based online optimization approach to dynamically learn the interpolation policy in a data-adaptive way (learning to learn better). The validation set performance via metalearning captures the noisy degree, which provides optimal directions for interpolation policy learning. Furthermore, we adapt our method for pseudolabel-based SSL along with a refined pseudolabeling strategy. In our experiments, our method achieves better performance than vanilla MixUp and its variants under SL configuration. In particular, extensive experiments show that our MetaMixUp adapted SSL greatly outperforms MixUp and many state-of-the-art methods on CIFAR-10 and SVHN benchmarks under the SSL configuration.},
  archive      = {J_TNNLS},
  author       = {Zhijun Mai and Guosheng Hu and Dexiong Chen and Fumin Shen and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2020.3049011},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3050-3064},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MetaMixUp: Learning adaptive interpolation policy of MixUp with metalearning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-spectral feature representation for motor imagery
classification using convolutional neural networks. <em>TNNLS</em>,
<em>33</em>(7), 3038–3049. (<a
href="https://doi.org/10.1109/TNNLS.2020.3048385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have recently been applied to electroencephalogram (EEG)-based brain–computer interfaces (BCIs). EEG is a noninvasive neuroimaging technique, which can be used to decode user intentions. Because the feature space of EEG data is highly dimensional and signal patterns are specific to the subject, appropriate methods for feature representation are required to enhance the decoding accuracy of the CNN model. Furthermore, neural changes exhibit high variability between sessions, subjects within a single session, and trials within a single subject, resulting in major issues during the modeling stage. In addition, there are many subject-dependent factors, such as frequency ranges, time intervals, and spatial locations at which the signal occurs, which prevent the derivation of a robust model that can achieve the parameterization of these factors for a wide range of subjects. However, previous studies did not attempt to preserve the multivariate structure and dependencies of the feature space. In this study, we propose a method to generate a spatiospectral feature representation that can preserve the multivariate information of EEG data. Specifically, 3-D feature maps were constructed by combining subject-optimized and subject-independent spectral filters and by stacking the filtered data into tensors. In addition, a layer-wise decomposition model was implemented using our 3-D-CNN framework to secure reliable classification results on a single-trial basis. The average accuracies of the proposed model were 87.15\% (±7.31), 75.85\% (±12.80), and 70.37\% (±17.09) for the BCI competition data sets IV_2a, IV_2b, and OpenBMI data, respectively. These results are better than those obtained by state-of-the-art techniques, and the decomposition model obtained the relevance scores for neurophysiologically plausible electrode channels and frequency domains, confirming the validity of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Ji-Seon Bang and Min-Ho Lee and Siamac Fazli and Cuntai Guan and Seong-Whan Lee},
  doi          = {10.1109/TNNLS.2020.3048385},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3038-3049},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spatio-spectral feature representation for motor imagery classification using convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel feature selection method for high-dimensional mixed
decision tables. <em>TNNLS</em>, <em>33</em>(7), 3024–3037. (<a
href="https://doi.org/10.1109/TNNLS.2020.3048080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction, also called feature selection, is one of the most important issues of rough set theory, which is regarded as a vital preprocessing step in pattern recognition, machine learning, and data mining. Nowadays, high-dimensional mixed and incomplete data sets are very common in real-world applications. Certainly, the selection of a promising feature subset from such data sets is a very interesting, but challenging problem. Almost all of the existing methods generated a cover on the space of objects to determine important features. However, some tolerance classes in the cover are useless for the computational process. Thus, this article introduces a new concept of stripped neighborhood covers to reduce unnecessary tolerance classes from the original cover. Based on the proposed stripped neighborhood cover, we define a new reduct in mixed and incomplete decision tables, and then design an efficient heuristic algorithm to find this reduct. For each loop in the main loop of the proposed algorithm, we use an error measure to select an optimal feature and put it into the selected feature subset. Besides, to deal more efficiently with high-dimensional data sets, we also determine redundant features after each loop and remove them from the candidate feature subset. For the purpose of verifying the performance of the proposed algorithm, we carry out experiments on data sets downloaded from public data sources to compare with existing state-of-the-art algorithms. Experimental results showed that our algorithm outperforms compared algorithms, especially in classification accuracy.},
  archive      = {J_TNNLS},
  author       = {Nguyen Ngoc Thuy and Sartra Wongthanavasu},
  doi          = {10.1109/TNNLS.2020.3048080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3024-3037},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel feature selection method for high-dimensional mixed decision tables},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PID controller-guided attention neural network learning for
fast and effective real photographs denoising. <em>TNNLS</em>,
<em>33</em>(7), 3010–3023. (<a
href="https://doi.org/10.1109/TNNLS.2020.3048031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real photograph denoising is extremely challenging in low-level computer vision since the noise is sophisticated and cannot be fully modeled by explicit distributions. Although deep-learning techniques have been actively explored for this issue and achieved convincing results, most of the networks may cause vanishing or exploding gradients, and usually entail more time and memory to obtain a remarkable performance. This article overcomes these challenges and presents a novel network, namely, PID controller guide attention neural network (PAN-Net), taking advantage of both the proportional-integral-derivative (PID) controller and attention neural network for real photograph denoising. First, a PID-attention network (PID-AN) is built to learn and exploit discriminative image features. Meanwhile, we devise a dynamic learning scheme by linking the neural network and control action, which significantly improves the robustness and adaptability of PID-AN. Second, we explore both the residual structure and share-source skip connections to stack the PID-ANs. Such a framework provides a flexible way to feature residual learning, enabling us to facilitate the network training and boost the denoising performance. Extensive experiments show that our PAN-Net achieves superior denoising results against the state-of-the-art in terms of image quality and efficiency.},
  archive      = {J_TNNLS},
  author       = {Ruijun Ma and Bob Zhang and Yicong Zhou and Zhengming Li and Fangyuan Lei},
  doi          = {10.1109/TNNLS.2020.3048031},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {3010-3023},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PID controller-guided attention neural network learning for fast and effective real photographs denoising},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dissipativity-based finite-time filtering for uncertain
semi-markovian jump random systems with multiple time delays and state
constraints. <em>TNNLS</em>, <em>33</em>(7), 2995–3009. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the issue of dissipativity-based finite-time multiple delay-dependent filtering for uncertain semi-Markovian jump random nonlinear systems with state constraints. There are multiple time-varying delays, nonlinear functions, and intermittent faults (IFs) in the systems. This is one of the few attempts for the issue studied in this article. First, a filter is designed for the uncertain semi-Markovian jump random nonlinear systems. An augmented system with regard to the resulting filtering error is acquired. Then, sufficient conditions of the augmented system are generated by the stochastic Lyapunov function. Finite-time boundedness (FTB) and input–output finite-time mean square stabilization (IO-FTMSS) are both realized. The effectiveness and feasibility of the method are rendered via three examples.},
  archive      = {J_TNNLS},
  author       = {Shaoxin Sun and Huaguang Zhang and Jian Han and Juan Zhang},
  doi          = {10.1109/TNNLS.2020.3047991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2995-3009},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dissipativity-based finite-time filtering for uncertain semi-markovian jump random systems with multiple time delays and state constraints},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active learning with multiple kernels. <em>TNNLS</em>,
<em>33</em>(7), 2980–2994. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multiple kernel learning (OMKL) has provided an attractive performance in nonlinear function learning tasks. Leveraging a random feature (RF) approximation, the major drawback of OMKL, known as the curse of dimensionality, has been recently alleviated. These advantages enable RF-based OMKL to be considered in practice. In this article, we introduce a new research problem, named stream-based active MKL (AMKL), in which a learner is allowed to label some selected data from an oracle according to a selection criterion. This is necessary for many real-world applications as acquiring a true label is costly or time consuming. We theoretically prove that the proposed AMKL achieves an optimal sublinear regret $\mathcal {O}(\sqrt {T})$ as in OMKL with little labeled data, implying that the proposed selection criterion indeed avoids unnecessary label requests. Furthermore, we present AMKL with an adaptive kernel selection (named AMKL-AKS) in which irrelevant kernels can be excluded from a kernel dictionary “on the fly.” This approach improves the efficiency of active learning and the accuracy of function learning. Via numerical tests with real data sets, we verify the superiority of AMKL-AKS, yielding a similar accuracy performance with OMKL counterpart using a fewer number of labeled data.},
  archive      = {J_TNNLS},
  author       = {Songnam Hong and Jeongmin Chae},
  doi          = {10.1109/TNNLS.2020.3047953},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2980-2994},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Active learning with multiple kernels},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural-network-based distributed asynchronous
event-triggered consensus tracking of a class of uncertain nonlinear
multi-agent systems. <em>TNNLS</em>, <em>33</em>(7), 2965–2979. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a neural-network-based adaptive asynchronous event-triggered design strategy for the distributed consensus tracking of uncertain lower triangular nonlinear multi-agent systems under a directed network. Compared with the existing event-triggered recursive consensus tracking designs using multiple neural networks for each follower and continuous communications among followers, the primary contribution of this study is the development of an asynchronous event-triggered consensus tracking methodology based on a single-neural network for each follower under event-driven intermittent communications among followers. To this end, a distributed event-triggered estimator using neighbors’ triggered output information is developed to estimate a leader signal. Subsequently, the estimated leader signal is used to design local trackers. Only a triggering law and a single-neural network are used to design the local tracking law of each follower, irrespective of unmatched unknown nonlinearities. The information of each follower and its neighbors is asynchronously and intermittently communicated through a directed network. Thus, the proposed asynchronous event-triggered tracking scheme can save communicational and computational resources. From the Lyapunov stability theorem, the stability of the entire closed-loop system is analyzed and the comparative simulation results demonstrate the effectiveness of the proposed control strategy.},
  archive      = {J_TNNLS},
  author       = {Yun Ho Choi and Sung Jin Yoo},
  doi          = {10.1109/TNNLS.2020.3047945},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2965-2979},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural-network-based distributed asynchronous event-triggered consensus tracking of a class of uncertain nonlinear multi-agent systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Broad learning with reinforcement learning signal feedback:
Theory and applications. <em>TNNLS</em>, <em>33</em>(7), 2952–2964. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broad learning systems (BLSs) have attracted considerable attention due to their powerful ability in efficient discriminative learning. In this article, a modified BLS with reinforcement learning signal feedback (BLRLF) is proposed as an efficient method for improving the performance of standard BLS. The main differences between our research and BLS are as follows. First, we add weight optimization after adding additional nodes or new training samples. Motivated by the weight iterative optimization in the convolution neural network (CNN), we use the output of the network as feedback while employing value iteration (VI)-based adaptive dynamic programming (ADP) to facilitate calculation of near-optimal increments of connection weights. Second, different from the homogeneous incremental algorithms in standard BLS, we integrate those broad expansion methods, and the heuristic search method is used to enable the proposed BLRLF to optimize the network structure autonomously. Although the training time is affected to a certain extent compared with BLS, the newly proposed BLRLF still retains a fast computational nature. Finally, the proposed BLRLF is evaluated using popular benchmarks from the UC Irvine Machine Learning Repository and many other challenging data sets. These results show that BLRLF outperforms many state-of-the-art deep learning algorithms and shallow networks proposed in recent years.},
  archive      = {J_TNNLS},
  author       = {Ruiqi Mao and Rongxin Cui and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3047941},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2952-2964},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Broad learning with reinforcement learning signal feedback: Theory and applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamically weighted balanced loss: Class imbalanced
learning and confidence calibration of deep neural networks.
<em>TNNLS</em>, <em>33</em>(7), 2940–2951. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced class distribution is an inherent problem in many real-world classification tasks where the minority class is the class of interest. Many conventional statistical and machine learning classification algorithms are subject to frequency bias, and learning discriminating boundaries between the minority and majority classes could be challenging. To address the class distribution imbalance in deep learning, we propose a class rebalancing strategy based on a class-balanced dynamically weighted loss function where weights are assigned based on the class frequency and predicted probability of ground-truth class. The ability of dynamic weighting scheme to self-adapt its weights depending on the prediction scores allows the model to adjust for instances with varying levels of difficulty resulting in gradient updates driven by hard minority class samples. We further show that the proposed loss function is classification calibrated. Experiments conducted on highly imbalanced data across different applications of cyber intrusion detection (CICIDS2017 data set) and medical imaging (ISIC2019 data set) show robust generalization. Theoretical results supported by superior empirical performance provide justification for the validity of the proposed dynamically weighted balanced (DWB) loss function.},
  archive      = {J_TNNLS},
  author       = {K. Ruwani M. Fernando and Chris P. Tsokos},
  doi          = {10.1109/TNNLS.2020.3047335},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2940-2951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamically weighted balanced loss: Class imbalanced learning and confidence calibration of deep neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer design for sampled-data systems via deterministic
learning. <em>TNNLS</em>, <em>33</em>(7), 2931–2939. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A unified approach is proposed to design sampled-data observers for a certain type of unknown nonlinear systems undergoing recurrent motions based on deterministic learning in this article. First, a discrete-time implementation of high-gain observer (HGO) is utilized to obtain state trajectory from sampled output measurements. By taking the recurrent estimated trajectory as inputs to a dynamical radial basis function network (RBFN), a partial persistent exciting (PE) condition is satisfied, and a locally accurate approximation of nonlinear dynamics can be realized along the estimated sampled-data trajectory. Second, an RBFN-based observer consisting of the obtained dynamics from the process of deterministic learning is designed. Without resorting to high gains, the RBFN-based observer is shown capable of achieving correct state observation. The novelty of this article lies in that, by incorporating deterministic learning with the discrete-time HGO, the nonlinear dynamics can be accurately approximated along the estimated trajectory, and such obtained knowledge can then be utilized to realize nonhigh-gain state estimation for the same or similar sampled-data systems. Simulation is performed to validate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Jingtao Hu and Weiming Wu and Bing Ji and Cong Wang},
  doi          = {10.1109/TNNLS.2020.3047226},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2931-2939},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer design for sampled-data systems via deterministic learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MCRF and mRD: Two classification methods based on a novel
multiclass label noise filtering learning framework. <em>TNNLS</em>,
<em>33</em>(7), 2916–2930. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitigating label noise is a crucial problem in classification. Noise filtering is an effective method of dealing with label noise which does not need to estimate the noise rate or rely on any loss function. However, most filtering methods focus mainly on binary classification, leaving the more difficult counterpart problem of multiclass classification relatively unexplored. To remedy this deficit, we present a definition for label noise in a multiclass setting and propose a general framework for a novel label noise filtering learning method for multiclass classification. Two examples of noise filtering methods for multiclass classification, multiclass complete random forest (mCRF) and multiclass relative density, are derived from their binary counterparts using our proposed framework. In addition, to optimize the NI_threshold hyperparameter in mCRF, we propose two new optimization methods: a new voting cross-validation method and an adaptive method that employs a 2-means clustering algorithm. Furthermore, we incorporate SMOTE into our label noise filtering learning framework to handle the ubiquitous problem of imbalanced data in multiclass classification. We report experiments on both synthetic data sets and UCI benchmarks to demonstrate our proposed methods are highly robust to label noise in comparison with state-of-the-art baselines. All code and data results are available at https://github.com/syxiaa/Multiclass-Label-Noise-Filtering-Learning .},
  archive      = {J_TNNLS},
  author       = {Shuyin Xia and Baiyun Chen and Guoyin Wang and Yong Zheng and Xinbo Gao and Elisabeth Giem and Zizhong Chen},
  doi          = {10.1109/TNNLS.2020.3047046},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2916-2930},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MCRF and mRD: Two classification methods based on a novel multiclass label noise filtering learning framework},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized zero-shot learning with multiple graph adaptive
generative networks. <em>TNNLS</em>, <em>33</em>(7), 2903–2915. (<a
href="https://doi.org/10.1109/TNNLS.2020.3046924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) for (generalized) zero-shot learning (ZSL) aim to generate unseen image features when conditioned on unseen class embeddings, each of which corresponds to one unique category. Most existing works on GANs for ZSL generate features by merely feeding the seen image feature/class embedding (combined with random Gaussian noise) pairs into the generator/discriminator for a two-player minimax game. However, the structure consistency of the distributions among the real/fake image features, which may shift the generated features away from their real distribution to some extent, is seldom considered. In this paper, to align the weights of the generator for better structure consistency between real/fake features, we propose a novel multigraph adaptive GAN (MGA-GAN). Specifically, a Wasserstein GAN equipped with a classification loss is trained to generate discriminative features with structure consistency. MGA-GAN leverages the multigraph similarity structures between sliced seen real/fake feature samples to assist in updating the generator weights in the local feature manifold. Moreover, correlation graphs for the whole real/fake features are adopted to guarantee structure correlation in the global feature manifold. Extensive evaluations on four benchmarks demonstrate well the superiority of MGA-GAN over its state-of-the-art counterparts.},
  archive      = {J_TNNLS},
  author       = {Guo-Sen Xie and Zheng Zhang and Guoshuai Liu and Fan Zhu and Li Liu and Ling Shao and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3046924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2903-2915},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized zero-shot learning with multiple graph adaptive generative networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer-based fixed-time neural control for a class of
nonlinear systems. <em>TNNLS</em>, <em>33</em>(7), 2892–2902. (<a
href="https://doi.org/10.1109/TNNLS.2020.3046865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with an issue of fixed time adaptive neural control for a class of uncertain nonlinear systems subject to hysteresis input and immeasurable states. The state observer and neural networks (NNs) are used to estimate the immeasurable states and approximate the unknown nonlinearities, respectively. On this foundation, an adaptive fixed time neural control strategy is developed. Technically, this control strategy is based on a novel fixed-time stability criterion. Different from the research on fixed-time control in the conventional literature, this article designs a new controller with two fractional exponential powers. In the light of the established stability criterion, the fixed-time stability of the systems is guaranteed under the proposed control scheme. Finally, a simulation study is carried out to test the performance of the developed control strategy.},
  archive      = {J_TNNLS},
  author       = {Yan Zhang and Fang Wang},
  doi          = {10.1109/TNNLS.2020.3046865},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2892-2902},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based fixed-time neural control for a class of nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid residual dilated LSTM and exponential smoothing
model for midterm electric load forecasting. <em>TNNLS</em>,
<em>33</em>(7), 2879–2891. (<a
href="https://doi.org/10.1109/TNNLS.2020.3046629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a hybrid and hierarchical deep learning model for midterm load forecasting. The model combines exponential smoothing (ETS), advanced long short-term memory (LSTM), and ensembling. ETS extracts dynamically the main components of each individual time series and enables the model to learn their representation. Multilayer LSTM is equipped with dilated recurrent skip connections and a spatial shortcut path from lower layers to allow the model to better capture long-term seasonal relationships and ensure more efficient training. A common learning procedure for LSTM and ETS, with a penalized pinball loss, leads to simultaneous optimization of data representation and forecasting performance. In addition, ensembling at three levels ensures a powerful regularization. A simulation study performed on the monthly electricity demand time series for 35 European countries confirmed the high performance of the proposed model and its competitiveness with classical models such as ARIMA and ETS as well as state-of-the-art models based on machine learning.},
  archive      = {J_TNNLS},
  author       = {Grzegorz Dudek and Paweł Pełka and Slawek Smyl},
  doi          = {10.1109/TNNLS.2020.3046629},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2879-2891},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid residual dilated LSTM and exponential smoothing model for midterm electric load forecasting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive optimal control for unknown constrained nonlinear
systems with a novel quasi-model network. <em>TNNLS</em>,
<em>33</em>(7), 2867–2878. (<a
href="https://doi.org/10.1109/TNNLS.2020.3046614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A policy-iteration-based algorithm is presented in this article for optimal control of unknown continuous-time nonlinear systems subject to bounded inputs by utilizing the adaptive dynamic programming (ADP). Three neural networks (NNs), called critic network, actor network, and quasi-model network, are utilized in the proposed algorithm to give approximations of the control law, the cost function, and the function constituted by partial derivatives of value functions with respect to states and unknown input gain dynamics, respectively. At each iteration, based on the least sum of squares method, the parameters of critic and quasi-model networks will be tuned simultaneously, which eliminates the necessity of separately learning the system model in advance. Then, the control law is improved by satisfying the necessary optimality condition. Then, the proposed algorithm’s optimality and convergence properties are exhibited. Finally, the simulation results demonstrate the availability of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Xiumei Han and Xudong Zhao and Hamid Reza Karimi and Ding Wang and Guangdeng Zong},
  doi          = {10.1109/TNNLS.2020.3046614},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2867-2878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive optimal control for unknown constrained nonlinear systems with a novel quasi-model network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Low-latency in situ image analytics with FPGA-based
quantized convolutional neural network. <em>TNNLS</em>, <em>33</em>(7),
2853–2866. (<a
href="https://doi.org/10.1109/TNNLS.2020.3046452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time in situ image analytics impose stringent latency requirements on intelligent neural network inference operations. While conventional software-based implementations on the graphic processing unit (GPU)-accelerated platforms are flexible and have achieved very high inference throughput, they are not suitable for latency-sensitive applications where real-time feedback is needed. Here, we demonstrate that high-performance reconfigurable computing platforms based on field-programmable gate array (FPGA) processing can successfully bridge the gap between low-level hardware processing and high-level intelligent image analytics algorithm deployment within a unified system. The proposed design performs inference operations on a stream of individual images as they are produced and has a deeply pipelined hardware design that allows all layers of a quantized convolutional neural network (QCNN) to compute concurrently with partial image inputs. Using the case of label-free classification of human peripheral blood mononuclear cell (PBMC) subtypes as a proof-of-concept illustration, our system achieves an ultralow classification latency of 34.2 $\mu \text{s}$ with over 95\% end-to-end accuracy by using a QCNN, while the cells are imaged at throughput exceeding 29 200 cells/s. Our QCNN design is modular and is readily adaptable to other QCNNs with different latency and resource requirements.},
  archive      = {J_TNNLS},
  author       = {Maolin Wang and Kelvin C. M. Lee and Bob M. F. Chung and Sharatchandra Varma Bogaraju and Ho-Cheung Ng and Justin S. J. Wong and Ho Cheung Shum and Kevin K. Tsia and Hayden Kwok-Hay So},
  doi          = {10.1109/TNNLS.2020.3046452},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2853-2866},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Low-latency in situ image analytics with FPGA-based quantized convolutional neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Agglomerative neural networks for multiview clustering.
<em>TNNLS</em>, <em>33</em>(7), 2842–2852. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional multiview clustering methods seek a view consensus through minimizing the pairwise discrepancy between the consensus and subviews. However, pairwise comparison cannot portray the interview relationship precisely if some of the subviews can be further agglomerated. To address the above challenge, we propose the agglomerative analysis to approximate the optimal consensus view, thereby describing the subview relationship within a view structure. We present an agglomerative neural network (ANN) based on constrained Laplacian rank to cluster multiview data directly without a dedicated postprocessing step (e.g., using $K$ -means). We further extend ANN with a learnable data space to handle data of complex scenarios. Our evaluations against several state-of-the-art multiview clustering approaches on four popular data sets show the promising view-consensus analysis ability of ANN. We further demonstrate ANN’s capability in analyzing complex view structures, extensibility through our case study and robustness and effectiveness of data-driven modifications.},
  archive      = {J_TNNLS},
  author       = {Zhe Liu and Yun Li and Lina Yao and Xianzhi Wang and Feiping Nie},
  doi          = {10.1109/TNNLS.2020.3045932},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2842-2852},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Agglomerative neural networks for multiview clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A study on truncated newton methods for linear
classification. <em>TNNLS</em>, <em>33</em>(7), 2828–2841. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truncated Newton (TN) methods have been a useful technique for large-scale optimization. Instead of obtaining the full Newton direction, a truncated method approximately solves the Newton equation with an inner conjugate gradient (CG) procedure (TNCG for the whole method). These methods have been employed to efficiently solve linear classification problems. However, even in this deeply studied field, various theoretical and numerical aspects were not completely explored. The first contribution of this work is to comprehensively study the global and local convergence when TNCG is applied to linear classification. Because of the lack of twice differentiability under some losses, many past works cannot be applied here. We prove various missing pieces of theory from scratch and clarify many proper references. The second contribution is to study the termination of the CG method. For the first time when TNCG is applied to linear classification, we show that the inner stopping condition strongly affects the convergence speed. We propose using a quadratic stopping criterion to achieve both robustness and efficiency. The third contribution is that of combining the study on inner stopping criteria with that of preconditioning. We discuss how convergence theory is affected by preconditioning and finally propose an effective preconditioned TNCG.},
  archive      = {J_TNNLS},
  author       = {Leonardo Galli and Chih-Jen Lin},
  doi          = {10.1109/TNNLS.2020.3045836},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2828-2841},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A study on truncated newton methods for linear classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal discovery in linear non-gaussian acyclic model with
multiple latent confounders. <em>TNNLS</em>, <em>33</em>(7), 2816–2827.
(<a href="https://doi.org/10.1109/TNNLS.2020.3045812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery from observational data is a fundamental problem in science. Though the linear non-Gaussian acyclic model (LiNGAM) has shown promising results in various applications, it still faces the following challenges in the data with multiple latent confounders: 1) how to detect the latent confounders and 2) how to uncover the causal relations among observed and latent variables. To address these two challenges, we propose a hybrid causal discovery method for the LiNGAM with multiple latent confounders (MLCLiNGAM). First, we utilize the constraint-based method to learn the causal skeleton. Second, we identify the causal directions, by conducting regression and independence tests on the adjacent pairs in the causal skeleton. Third, we detect the latent confounders with the help of the maximal clique patterns raised by the latent confounders and reconstruct the causal structure with latent variables. Theoretical results show the correctness and efficiency of the algorithms. We conduct extensive experiments on synthetic and real data, which illustrates the efficiency and effectiveness of the proposed algorithms.},
  archive      = {J_TNNLS},
  author       = {Wei Chen and Ruichu Cai and Kun Zhang and Zhifeng Hao},
  doi          = {10.1109/TNNLS.2020.3045812},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2816-2827},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Causal discovery in linear non-gaussian acyclic model with multiple latent confounders},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BiCoSS: Toward large-scale cognition brain with
multigranular neuromorphic architecture. <em>TNNLS</em>, <em>33</em>(7),
2801–2815. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The further exploration of the neural mechanisms underlying the biological activities of the human brain depends on the development of large-scale spiking neural networks (SNNs) with different categories at different levels, as well as the corresponding computing platforms. Neuromorphic engineering provides approaches to high-performance biologically plausible computational paradigms inspired by neural systems. In this article, we present a biological-inspired cognitive supercomputing system (BiCoSS) that integrates multiple granules (GRs) of SNNs to realize a hybrid compatible neuromorphic platform. A scalable hierarchical heterogeneous multicore architecture is presented, and a synergistic routing scheme for hybrid neural information is proposed. The BiCoSS system can accommodate different levels of GRs and biological plausibility of SNN models in an efficient and scalable manner. Over four million neurons can be realized on BiCoSS with a power efficiency of 2.8k larger than the GPU platform, and the average latency of BiCoSS is 3.62 and 2.49 times higher than conventional architectures of digital neuromorphic systems. For the verification, BiCoSS is used to replicate various biological cognitive activities, including motor learning, action selection, context-dependent learning, and movement disorders. Comprehensively considering the programmability, biological plausibility, learning capability, computational power, and scalability, BiCoSS is shown to outperform the alternative state-of-the-art works for large-scale SNN, while its real-time computational capability enables a wide range of potential applications.},
  archive      = {J_TNNLS},
  author       = {Shuangming Yang and Jiang Wang and Xinyu Hao and Huiyan Li and Xile Wei and Bin Deng and Kenneth A. Loparo},
  doi          = {10.1109/TNNLS.2020.3045492},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2801-2815},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BiCoSS: Toward large-scale cognition brain with multigranular neuromorphic architecture},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameterized luenberger-type h∞ state estimator for delayed
static neural networks. <em>TNNLS</em>, <em>33</em>(7), 2791–2800. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new Luenberger-type state estimator that has parameterized observer gains dependent on the activation function, to improve the $H_{\infty }$ state estimation performance of the static neural networks with time-varying delay. The nonlinearity of the activation function has a significant impact on stability analysis and robustness/performance. In the proposed state estimator, a parameter-dependent estimator gain is reconstructed by using the properties of the sector nonlinearity of the activation functions that are represented as linear combinations of weighting parameters. In the reformulated form, the constraints of the parameters for the activation function are considered in terms of linear matrix inequalities. Based on the Lyapunov–Krasovskii function and the improved reciprocally convex inequality, enhanced conditions for designing a new state estimator that guarantees $H_{\infty }$ performance are derived through a parameterization technique. The compared results with recent studies demonstrate the superiority and effectiveness of the presented method.},
  archive      = {J_TNNLS},
  author       = {Yongsik Jin and Wookyong Kwon and Sangmoon Lee},
  doi          = {10.1109/TNNLS.2020.3045146},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2791-2800},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Parameterized luenberger-type h∞ state estimator for delayed static neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning and adaptive optimal control for
continuous-time nonlinear systems: A value iteration approach.
<em>TNNLS</em>, <em>33</em>(7), 2781–2790. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the adaptive optimal control problem for continuous-time nonlinear systems described by differential equations. A key strategy is to exploit the value iteration (VI) method proposed initially by Bellman in 1957 as a fundamental tool to solve dynamic programming problems. However, previous VI methods are all exclusively devoted to the Markov decision processes and discrete-time dynamical systems. In this article, we aim to fill up the gap by developing a new continuous-time VI method that will be applied to address the adaptive or nonadaptive optimal control problems for continuous-time systems described by differential equations. Like the traditional VI, the continuous-time VI algorithm retains the nice feature that there is no need to assume the knowledge of an initial admissible control policy. As a direct application of the proposed VI method, a new class of adaptive optimal controllers is obtained for nonlinear systems with totally unknown dynamics. A learning-based control algorithm is proposed to show how to learn robust optimal controllers directly from real-time data. Finally, two examples are given to illustrate the efficacy of the proposed methodology.},
  archive      = {J_TNNLS},
  author       = {Tao Bian and Zhong-Ping Jiang},
  doi          = {10.1109/TNNLS.2020.3045087},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2781-2790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reinforcement learning and adaptive optimal control for continuous-time nonlinear systems: A value iteration approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stabilizing training of generative adversarial nets via
langevin stein variational gradient descent. <em>TNNLS</em>,
<em>33</em>(7), 2768–2780. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs), which are famous for the capability of learning complex underlying data distribution, are, however, known to be tricky in the training process, which would probably result in mode collapse or performance deterioration. Current approaches of dealing with GANs’ issues almost utilize some practical training techniques for the purpose of regularization, which, on the other hand, undermines the convergence and theoretical soundness of GAN. In this article, we propose to stabilize GAN training via a novel particle-based variational inference—Langevin Stein variational gradient descent (LSVGD), which not only inherits the flexibility and efficiency of original SVGD but also aims to address its instability issues by incorporating an extra disturbance into the update dynamics. We further demonstrate that, by properly adjusting the noise variance, LSVGD simulates a Langevin process whose stationary distribution is exactly the target distribution. We also show that LSVGD dynamics has an implicit regularization, which is able to enhance particles’ spread-out and diversity. Finally, we present an efficient way of applying particle-based variational inference on a general GAN training procedure no matter what loss function is adopted. Experimental results on one synthetic data set and three popular benchmark data sets—Cifar-10, Tiny-ImageNet, and CelebA—validate that LSVGD can remarkably improve the performance and stability of various GAN models.},
  archive      = {J_TNNLS},
  author       = {Dong Wang and Xiaoqian Qin and Fengyi Song and Li Cheng},
  doi          = {10.1109/TNNLS.2020.3045082},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2768-2780},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stabilizing training of generative adversarial nets via langevin stein variational gradient descent},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge-routed visual question reasoning: Challenges for
deep representation embedding. <em>TNNLS</em>, <em>33</em>(7),
2758–2767. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though beneficial for encouraging the visual question answering (VQA) models to discover the underlying knowledge by exploiting the input–output correlation beyond image and text contexts, the existing knowledge VQA data sets are mostly annotated in a crowdsource way, e.g., collecting questions and external reasons from different users via the Internet. In addition to the challenge of knowledge reasoning, how to deal with the annotator bias also remains unsolved, which often leads to superficial overfitted correlations between questions and answers. To address this issue, we propose a novel data set named knowledge-routed visual question reasoning for VQA model evaluation. Considering that a desirable VQA model should correctly perceive the image context, understand the question, and incorporate its learned knowledge, our proposed data set aims to cut off the shortcut learning exploited by the current deep embedding models and push the research boundary of the knowledge-based visual question reasoning. Specifically, we generate the question–answer pair based on both the visual genome scene graph and an external knowledge base with controlled programs to disentangle the knowledge from other biases. The programs can select one or two triplets from the scene graph or knowledge base to push multistep reasoning, avoid answer ambiguity, and balance the answer distribution. In contrast to the existing VQA data sets, we further imply the following two major constraints on the programs to incorporate knowledge reasoning. First, multiple knowledge triplets can be related to the question, but only one knowledge relates to the image object. This can enforce the VQA model to correctly perceive the image instead of guessing the knowledge based on the given question solely. Second, all questions are based on different knowledge, but the candidate answers are the same for both the training and test sets. We make the testing knowledge unused during training to evaluate whether a model can understand question words and handle unseen combinations. Extensive experiments with various baselines and state-of-the-art VQA models are conducted to demonstrate that there still exists a big gap between the model with and without groundtruth supporting triplets when given the embedded knowledge base. This reveals the weakness of the current deep embedding models on the knowledge reasoning problem.},
  archive      = {J_TNNLS},
  author       = {Qingxing Cao and Bailin Li and Xiaodan Liang and Keze Wang and Liang Lin},
  doi          = {10.1109/TNNLS.2020.3045034},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2758-2767},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge-routed visual question reasoning: Challenges for deep representation embedding},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The heidelberg spiking data sets for the systematic
evaluation of spiking neural networks. <em>TNNLS</em>, <em>33</em>(7),
2744–2757. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks are the basis of versatile and power-efficient information processing in the brain. Although we currently lack a detailed understanding of how these networks compute, recently developed optimization techniques allow us to instantiate increasingly complex functional spiking neural networks in-silico. These methods hold the promise to build more efficient non-von-Neumann computing hardware and will offer new vistas in the quest of unraveling brain circuit function. To accelerate the development of such methods, objective ways to compare their performance are indispensable. Presently, however, there are no widely accepted means for comparing the computational performance of spiking neural networks. To address this issue, we introduce two spike-based classification data sets, broadly applicable to benchmark both software and neuromorphic hardware implementations of spiking neural networks. To accomplish this, we developed a general audio-to-spiking conversion procedure inspired by neurophysiology. Furthermore, we applied this conversion to an existing and a novel speech data set. The latter is the free, high-fidelity, and word-level aligned Heidelberg digit data set that we created specifically for this study. By training a range of conventional and spiking classifiers, we show that leveraging spike timing information within these data sets is essential for good classification accuracy. These results serve as the first reference for future performance comparisons of spiking neural networks.},
  archive      = {J_TNNLS},
  author       = {Benjamin Cramer and Yannik Stradmann and Johannes Schemmel and Friedemann Zenke},
  doi          = {10.1109/TNNLS.2020.3044364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {7},
  pages        = {2744-2757},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {The heidelberg spiking data sets for the systematic evaluation of spiking neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reachable set estimation of delayed markovian jump neural
networks based on an improved reciprocally convex inequality.
<em>TNNLS</em>, <em>33</em>(6), 2737–2742. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief investigates the reachable set estimation problem of the delayed Markovian jump neural networks (NNs) with bounded disturbances. First, an improved reciprocally convex inequality is proposed, which contains some existing ones as its special cases. Second, an augmented Lyapunov–Krasovskii functional (LKF) tailored for delayed Markovian jump NNs is proposed. Thirdly, based on the proposed reciprocally convex inequality and the augmented LKF, an accurate ellipsoidal description of the reachable set for delayed Markovian jump NNs is obtained. Finally, simulation results are given to illustrate the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Guoqiang Tan and Zhanshan Wang},
  doi          = {10.1109/TNNLS.2020.3045599},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2737-2742},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reachable set estimation of delayed markovian jump neural networks based on an improved reciprocally convex inequality},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vertebrae labeling via end-to-end integral regression
localization and multi-label classification network. <em>TNNLS</em>,
<em>33</em>(6), 2726–2736. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification and localization of the vertebrae in CT scans is a critical and standard pre-processing step for clinical spinal diagnosis and treatment. Existing methods are mainly based on the integration of multiple neural networks, and most of them use heatmaps to locate the vertebrae’s centroid. However, the process of obtaining vertebrae’s centroid coordinates using heatmaps is non-differentiable, so it is impossible to train the network to label the vertebrae directly. Therefore, for end-to-end differential training of vertebrae coordinates on CT scans, a robust and accurate automatic vertebral labeling algorithm is proposed in this study. First, a novel end-to-end integral regression localization and multi-label classification network is developed, which can capture multi-scale features and also utilize the residual module and skip connection to fuse the multi-level features. Second, to solve the problem that the process of finding coordinates is non-differentiable and the spatial structure of location being destroyed, an integral regression module is used in the localization network. It combines the advantages of heatmaps representation and direct regression coordinates to achieve end-to-end training and can be compatible with any key point detection methods of medical images based on heatmaps. Finally, multi-label classification of vertebrae is carried out to improve the identification rate, which uses bidirectional long short-term memory (Bi-LSTM) online to enhance the learning of long contextual information of vertebrae. The proposed method is evaluated on a challenging data set, and the results are significantly better than state-of-the-art methods (identification rate is 91.1\% and the mean localization error is 2.2 mm). The method is evaluated on a new CT data set, and the results show that our method has good generalization.},
  archive      = {J_TNNLS},
  author       = {Chunli Qin and Ji Zhou and Demin Yao and Han Zhuang and Hui Wang and Shiyao Chen and Yonghong Shi and Zhijian Song},
  doi          = {10.1109/TNNLS.2020.3045601},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2726-2736},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Vertebrae labeling via end-to-end integral regression localization and multi-label classification network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Information processing capacity of a single-node reservoir
computer: An experimental evaluation. <em>TNNLS</em>, <em>33</em>(6),
2714–2725. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical dynamical systems are able to process information in a nontrivial manner. The machine learning paradigm of reservoir computing (RC) provides a suitable framework for information processing in (analog) dynamical systems. The potential of dynamical systems for RC can be quantitatively characterized by the information processing capacity (IPC) measure. Here, we evaluate the IPC measure of a reservoir computer based on a single-analog nonlinear node coupled with delay. We link the extracted IPC measures to the dynamical regime of the reservoir, reporting an experimentally measured nonlinear memory of up to seventh order. In addition, we find a nonhomogeneous distribution of the linear and nonlinear contributions to the IPC as a function of the system operating conditions. Finally, we unveil the role of noise in the IPC of the analog implementation by performing ad hoc numerical simulations. In this manner, we identify the so-called edge of stability as being the most promising operating condition of the experimental implementation for RC purposes in terms of computational power and noise robustness. Similarly, a strong input drive is shown to have beneficial properties, albeit with a reduced memory depth.},
  archive      = {J_TNNLS},
  author       = {Benedikt Vettelschoss and André Röhm and Miguel C. Soriano},
  doi          = {10.1109/TNNLS.2021.3116709},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2714-2725},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Information processing capacity of a single-node reservoir computer: An experimental evaluation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cellular automata can reduce memory requirements of
collective-state computing. <em>TNNLS</em>, <em>33</em>(6), 2701–2713.
(<a href="https://doi.org/10.1109/TNNLS.2021.3119543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various nonclassical approaches of distributed information processing, such as neural networks, reservoir computing (RC), vector symbolic architectures (VSAs), and others, employ the principle of collective-state computing. In this type of computing, the variables relevant in computation are superimposed into a single high-dimensional state vector, the collective state. The variable encoding uses a fixed set of random patterns, which has to be stored and kept available during the computation. In this article, we show that an elementary cellular automaton with rule 90 (CA90) enables the space–time tradeoff for collective-state computing models that use random dense binary representations, i.e., memory requirements can be traded off with computation running CA90. We investigate the randomization behavior of CA90, in particular, the relation between the length of the randomization period and the size of the grid, and how CA90 preserves similarity in the presence of the initialization noise. Based on these analyses, we discuss how to optimize a collective-state computing model, in which CA90 expands representations on the fly from short seed patterns—rather than storing the full set of random patterns. The CA90 expansion is applied and tested in concrete scenarios using RC and VSAs. Our experimental results show that collective-state computing with CA90 expansion performs similarly compared to traditional collective-state models, in which random patterns are generated initially by a pseudorandom number generator and then stored in a large memory.},
  archive      = {J_TNNLS},
  author       = {Denis Kleyko and Edward Paxon Frady and Friedrich T. Sommer},
  doi          = {10.1109/TNNLS.2021.3119543},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2701-2713},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cellular automata can reduce memory requirements of collective-state computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural schrödinger equation: Physical law as deep neural
network. <em>TNNLS</em>, <em>33</em>(6), 2686–2700. (<a
href="https://doi.org/10.1109/TNNLS.2021.3120472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show a new family of neural networks based on the Schrödinger equation (SE-NET). In this analogy, the trainable weights of the neural networks correspond to the physical quantities of the Schrödinger equation. These physical quantities can be trained using the complex-valued adjoint method. Since the propagation of the SE-NET can be described by the evolution of physical systems, its outputs can be computed by using a physical solver. The trained network is transferable to actual optical systems. As a demonstration, we implemented the SE-NET with the Crank–Nicolson finite difference method on Pytorch. From the results of numerical simulations, we found that the performance of the SE-NET becomes better when the SE-NET becomes wider and deeper. However, the training of the SE-NET was unstable due to gradient explosions when SE-NET becomes deeper. Therefore, we also introduced phase-only training, which only updates the phase of the potential field (refractive index) in the Schrödinger equation. This enables stable training even for the deep SE-NET model because the unitarity of the system is kept under the training. In addition, the SE-NET enables a joint optimization of physical structures and digital neural networks. As a demonstration, we performed a numerical demonstration of end-to-end machine learning (ML) with an optical frontend toward a compact spectrometer. Our results extend the application field of ML to hybrid physical-digital optimizations.},
  archive      = {J_TNNLS},
  author       = {Mitsumasa Nakajima and Kenji Tanaka and Toshikazu Hashimoto},
  doi          = {10.1109/TNNLS.2021.3120472},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2686-2700},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neural schrödinger equation: Physical law as deep neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neuromorphic time-multiplexed reservoir computing with
on-the-fly weight generation for edge devices. <em>TNNLS</em>,
<em>33</em>(6), 2676–2685. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain has evolved to perform complex and computationally expensive cognitive tasks, such as audio–visual perception and object detection, with ease. For instance, the brain can recognize speech in different dialects and perform other cognitive tasks, such as attention, memory, and motor control, with just 20 W of power consumption. Taking inspiration from neural systems, we propose a low-power neuromorphic hardware architecture to perform classification on temporal data at the edge. The proposed architecture uses a neuromorphic cochlea model for feature extraction and reservoir computing (RC) framework as a classifier. In the proposed hardware architecture, the RC framework is modified for on-the-fly generation of reservoir connectivity, along with binary feedforward and reservoir weights. Also, a large reservoir is split into multiple small reservoirs for efficient use of hardware resources. These modifications reduce the computational and memory resources required, thereby resulting in a lower power budget. The proposed classifier is validated for speech and human activity recognition (HAR) tasks. We have prototyped our hardware architecture using Intel’s cyclone-10 low-power series field-programmable gate array (FPGA), consuming only 4790 logic elements (LEs) and 34.9-kB memory, making it a perfect candidate for edge computing applications. Moreover, we have implemented a complete system for speech recognition with the feature extraction block (cochlea model) and the proposed classifier, utilizing 15 532 LEs and 38.4-kB memory. By using the proposed idea of multiple small reservoirs along with on-the-fly generation of reservoir binary weights, our architecture can reduce the power consumption and memory requirement by order of magnitude compared to existing FPGA models for speech recognition tasks with similar complexity.},
  archive      = {J_TNNLS},
  author       = {Sarthak Gupta and Satrajit Chakraborty and Chetan Singh Thakur},
  doi          = {10.1109/TNNLS.2021.3085165},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2676-2685},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Neuromorphic time-multiplexed reservoir computing with on-the-fly weight generation for edge devices},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-performance reservoir computing with fluctuations in
linear networks. <em>TNNLS</em>, <em>33</em>(6), 2664–2675. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir computing has emerged as a powerful machine learning paradigm for harvesting nontrivial information processing out of disordered physical systems driven by sequential inputs. To this end, the system observables must become nonlinear functions of the input history. We show that encoding the input to quantum or classical fluctuations of a network of interacting harmonic oscillators can lead to a high performance comparable to that of a standard echo state network in several nonlinear benchmark tasks. This equivalence in performance holds even with a linear Hamiltonian and a readout linear in the system observables. Furthermore, we find that the performance of the network of harmonic oscillators in nonlinear tasks is robust to errors both in input and reservoir observables caused by external noise. For any reservoir computing system with a linear readout, the magnitude of trained weights can either amplify or suppress noise added to reservoir observables. We use this general result to explain why the oscillators are robust to noise and why having precise control over reservoir memory is important for noise robustness in general. Our results pave the way toward reservoir computing harnessing fluctuations in disordered linear systems.},
  archive      = {J_TNNLS},
  author       = {Johannes Nokkala and Rodrigo Martínez-Peña and Roberta Zambrini and Miguel C. Soriano},
  doi          = {10.1109/TNNLS.2021.3105695},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2664-2675},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {High-performance reservoir computing with fluctuations in linear networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical-task reservoir for online semantic analysis
from continuous speech. <em>TNNLS</em>, <em>33</em>(6), 2654–2663. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel architecture called hierarchical-task reservoir (HTR) suitable for real-time applications for which different levels of abstraction are available. We apply it to semantic role labeling (SRL) based on continuous speech recognition. Taking inspiration from the brain, this demonstrates the hierarchies of representations from perceptive to integrative areas, and we consider a hierarchy of four subtasks with increasing levels of abstraction (phone, word, part-of-speech (POS), and semantic role tags). These tasks are progressively learned by the layers of the HTR architecture. Interestingly, quantitative and qualitative results show that the hierarchical-task approach provides an advantage to improve the prediction. In particular, the qualitative results show that a shallow or a hierarchical reservoir, considered as baselines, does not produce estimations as good as the HTR model would. Moreover, we show that it is possible to further improve the accuracy of the model by designing skip connections and by considering word embedding (WE) in the internal representations. Overall, the HTR outperformed the other state-of-the-art reservoir-based approaches and it resulted in extremely efficient with respect to typical recurrent neural networks (RNNs) in deep learning (DL) [e.g., long short term memory (LSTMs)]. The HTR architecture is proposed as a step toward the modeling of online and hierarchical processes at work in the brain during language comprehension.},
  archive      = {J_TNNLS},
  author       = {Luca Pedrelli and Xavier Hinaut},
  doi          = {10.1109/TNNLS.2021.3095140},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2654-2663},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical-task reservoir for online semantic analysis from continuous speech},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiresolution reservoir graph neural network.
<em>TNNLS</em>, <em>33</em>(6), 2642–2653. (<a
href="https://doi.org/10.1109/TNNLS.2021.3090503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks are receiving increasing attention as state-of-the-art methods to process graph-structured data. However, similar to other neural networks, they tend to suffer from a high computational cost to perform training. Reservoir computing (RC) is an effective way to define neural networks that are very efficient to train, often obtaining comparable predictive performance with respect to the fully trained counterparts. Different proposals of reservoir graph neural networks have been proposed in the literature. However, their predictive performances are still slightly below the ones of fully trained graph neural networks on many benchmark datasets, arguably because of the oversmoothing problem that arises when iterating over the graph structure in the reservoir computation. In this work, we aim to reduce this gap defining a multiresolution reservoir graph neural network (MRGNN) inspired by graph spectral filtering. Instead of iterating on the nonlinearity in the reservoir and using a shallow readout function, we aim to generate an explicit $k$ -hop unsupervised graph representation amenable for further, possibly nonlinear, processing. Experiments on several datasets from various application areas show that our approach is extremely fast and it achieves in most of the cases comparable or even higher results with respect to state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Luca Pasa and Nicolò Navarin and Alessandro Sperduti},
  doi          = {10.1109/TNNLS.2021.3090503},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2642-2653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiresolution reservoir graph neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time and adaptive reservoir computing with application
to profile prediction in fusion plasma. <em>TNNLS</em>, <em>33</em>(6),
2630–2641. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nuclear fusion is a promising alternative to address the problem of sustainable energy production. The tokamak is an approach to fusion based on magnetic plasma confinement, constituting a complex physical system with many control challenges. We study the characteristics and optimization of reservoir computing (RC) for real-time and adaptive prediction of plasma profiles in the DIII-D tokamak. Our experiments demonstrate that RC achieves comparable results to state-of-the-art (deep) convolutional neural networks (CNNs) and long short-term memory (LSTM) models, with a significantly easier and faster training procedure. This efficient approach allows for fast and frequent adaptation of the model to new situations, such as changing plasma conditions or different fusion devices.},
  archive      = {J_TNNLS},
  author       = {Azarakhsh Jalalvand and Joseph Abbate and Rory Conlin and Geert Verdoolaege and Egemen Kolemen},
  doi          = {10.1109/TNNLS.2021.3085504},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2630-2641},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Real-time and adaptive reservoir computing with application to profile prediction in fusion plasma},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Echo state networks for practical nonlinear model predictive
control of unknown dynamic systems. <em>TNNLS</em>, <em>33</em>(6),
2615–2629. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear model predictive control (NMPC) of industrial processes is changeling in part because the model of the plant may not be completely known but also for being computationally demanding. This work proposes an extremely efficient reservoir computing (RC)-based control framework that speeds up the NMPC of processes. In this framework, while an echo state network (ESN) serves as the dynamic RC-based system model of a process, the practical nonlinear model predictive controller (PNMPC) simplifies NMPC by splitting the forced and the free responses of the trained ESN, yielding the so-called ESN-PNMPC architecture. While the free response is generated by the forward simulation of the ESN model, the forced response is obtained by a fast and recursive calculation of the input–output sensitivities from the ESN. The efficiency not only results from the fast training inherited by RC but also from a computationally cheap control action given by the aforementioned novel recursive formulation and the computation in the reduced dimension space of input and output signals. The resulting architecture, equipped with a correction filter, is robust to unforeseen disturbances. The potential of the ESN-PNMPC is shown by application to the control of the four-tank system and an oil production platform, outperforming the predictive approach with a long-short term memory (LSTM) model, two standard linear control algorithms, and approximate predictive control.},
  archive      = {J_TNNLS},
  author       = {Jean Panaioti Jordanou and Eric Aislan Antonelo and Eduardo Camponogara},
  doi          = {10.1109/TNNLS.2021.3136357},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2615-2629},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Echo state networks for practical nonlinear model predictive control of unknown dynamic systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive practical nonlinear model predictive control for
echo state network models. <em>TNNLS</em>, <em>33</em>(6), 2605–2614.
(<a href="https://doi.org/10.1109/TNNLS.2021.3109821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an adaptive practical nonlinear model predictive (NMPC) control algorithm which uses an echo state network (ESN) estimated online as a process model. In the proposed control algorithm, the ESN readout parameters are estimated online using a recursive least-squares method that considers an adaptive directional forgetting factor. The ESN model is used to obtain online a nonlinear prediction of the system free response, and a linearized version of the neural model is obtained at each sampling time to get a local approximation of the system step response, which is used to build the dynamic matrix of the system. The proposed controller was evaluated in a benchmark conical tank level control problem, and the results were compared with three baseline controllers. The proposed approach achieved similar results as the ones obtained by its nonadaptive baseline version in a scenario with the process operating with the nominal parameters, and outperformed all baseline algorithms in a scenario with process parameter changes. Additionally, the computational time required by the proposed algorithm was one-tenth of that required by the baseline NMPC, which shows that the proposed algorithm is suitable to implement state-of-the-art adaptive NMPC in a computationally affordable manner.},
  archive      = {J_TNNLS},
  author       = {Bernardo Barancelli Schwedersky and Rodolfo César Costa Flesch and Samuel Bahu Rovea},
  doi          = {10.1109/TNNLS.2021.3109821},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2605-2614},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive practical nonlinear model predictive control for echo state network models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An echo state network imparts a curve fitting.
<em>TNNLS</em>, <em>33</em>(6), 2596–2604. (<a
href="https://doi.org/10.1109/TNNLS.2021.3099091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are successfully employed in processing information from temporal data. Approaches to training such networks are varied and reservoir computing-based attainments, such as the echo state network (ESN), provide great ease in training. Akin to many machine learning algorithms rendering an interpolation function or fitting a curve, we observe that a driven system, such as an RNN, renders a continuous curve fitting if and only if it satisfies the echo state property. The domain of the learned curve is an abstract space of the left-infinite sequence of inputs and the codomain is the space of readout values. When the input originates from discrete-time dynamical systems, we find theoretical conditions under which a topological conjugacy between the input and reservoir dynamics can exist and present some numerical results relating the linearity in the reservoir to the forecasting abilities of the ESNs.},
  archive      = {J_TNNLS},
  author       = {G. Manjunath},
  doi          = {10.1109/TNNLS.2021.3099091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2596-2604},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An echo state network imparts a curve fitting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistency hierarchy of reservoir computers.
<em>TNNLS</em>, <em>33</em>(6), 2586–2595. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the propagation and distribution of information-carrying signals injected in dynamical systems serving as reservoir computers. Through different combinations of repeated input signals, a multivariate correlation analysis reveals measures known as the consistency spectrum and consistency capacity. These are high-dimensional portraits of the nonlinear functional dependence between input and reservoir state. For multiple inputs, a hierarchy of capacities characterizes the interference of signals from each source. For an individual input, the time-resolved capacities form a profile of the reservoir’s nonlinear fading memory. We illustrate this methodology for a range of echo state networks.},
  archive      = {J_TNNLS},
  author       = {Thomas Jüngling and Thomas Lymburn and Michael Small},
  doi          = {10.1109/TNNLS.2021.3119548},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2586-2595},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Consistency hierarchy of reservoir computers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reservoir memory machines as neural computers.
<em>TNNLS</em>, <em>33</em>(6), 2575–2585. (<a
href="https://doi.org/10.1109/TNNLS.2021.3094139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable neural computers (DNCs) extend artificial neural networks with an explicit memory without interference, thus enabling the model to perform classic computation tasks, such as graph traversal. However, such models are difficult to train, requiring long training times and large datasets. In this work, we achieve some of the computational capabilities of DNCs with a model that can be trained very efficiently, namely, an echo state network with an explicit memory without interference. This extension enables echo state networks to recognize all regular languages, including those that contractive echo state networks provably cannot recognize. Furthermore, we demonstrate experimentally that our model performs comparably to its fully trained deep version on several typical benchmark tasks for DNCs.},
  archive      = {J_TNNLS},
  author       = {Benjamin Paaßen and Alexander Schulz and Terrence C. Stewart and Barbara Hammer},
  doi          = {10.1109/TNNLS.2021.3094139},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2575-2585},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Reservoir memory machines as neural computers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guest editorial special issue on new frontiers in extremely
efficient reservoir computing. <em>TNNLS</em>, <em>33</em>(6),
2571–2574. (<a
href="https://doi.org/10.1109/TNNLS.2022.3172586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the penetration of artificial intelligence (AI) technology into industrial applications, not only computational effectiveness but also computational efficiency in machine learning (ML) methods has been increasingly demanded. Reservoir computing (RC) is an ML framework leveraging a dynamic reservoir for a nonlinear transformation of sequential inputs and a readout for mapping the reservoir state to a desired output. Since only the readout is trained with a simple learning algorithm, RC has attracted much attention as a promising approach to enhance compatibility between high computational performance and low learning cost. In addition, recent studies on physical reservoirs implemented with various physical substrates have boosted the potential of RC in the development of effective and efficient AI hardware. Therefore, it is time to further explore the new frontiers in extremely efficient RC.},
  archive      = {J_TNNLS},
  author       = {Gouhei Tanaka and Claudio Gallicchio and Alessio Micheli and Juan-Pablo Ortega and Akira Hirose},
  doi          = {10.1109/TNNLS.2022.3172586},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2571-2574},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Guest editorial special issue on new frontiers in extremely efficient reservoir computing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel deep class-imbalanced semisupervised model for wind
turbine blade icing detection. <em>TNNLS</em>, <em>33</em>(6),
2558–2570. (<a
href="https://doi.org/10.1109/TNNLS.2021.3102514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind energy is of great importance for future energy development. In order to fully exploit wind energy, wind farms are often located at high latitudes, a practice that is accompanied by a high risk of icing. Traditional blade icing detection methods are usually based on manual inspection or external sensors/tools, but these techniques are limited by human expertise and additional costs. Model-based methods are highly dependent on prior domain knowledge and prone to misinterpretation. Data-driven approaches can offer promising solutions but require a massive amount of labeled training data, which are not generally available. In addition, the data collected for icing detection tend to be imbalanced because, most of the time, wind turbines operate under normal conditions. To address these challenges, this article presents a novel deep class-imbalanced semisupervised (DCISS) model for estimating blade icing conditions. DCISS integrates class-imbalanced and semisupervised learning (SSL) using a prototypical network that can rebalance features and measure the similarities between labeled and unlabeled samples. In addition, a channel calibration attention module is proposed to improve the ability to extract features from raw data. The proposed model has been evaluated using the blade icing datasets of three wind turbines. Compared to the classical anomaly detection and state-of-the-art SSL algorithms, DCISS shows significant advantages in terms of accuracy. Compared to five different class-imbalanced loss functions, the proposed DCISS is competitive. The generalization and practicability of the proposed model are further verified in the use case of online estimation.},
  archive      = {J_TNNLS},
  author       = {Xu Cheng and Fan Shi and Xiufeng Liu and Meng Zhao and Shengyong Chen},
  doi          = {10.1109/TNNLS.2021.3102514},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2558-2570},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A novel deep class-imbalanced semisupervised model for wind turbine blade icing detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep graph learning for anomalous citation detection.
<em>TNNLS</em>, <em>33</em>(6), 2543–2557. (<a
href="https://doi.org/10.1109/TNNLS.2022.3145092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is one of the most active research areas in various critical domains, such as healthcare, fintech, and public security. However, little attention has been paid to scholarly data, that is, anomaly detection in a citation network. Citation is considered as one of the most crucial metrics to evaluate the impact of scientific research, which may be gamed in multiple ways. Therefore, anomaly detection in citation networks is of significant importance to identify manipulation and inflation of citations. To address this open issue, we propose a novel deep graph learning model, namely graph learning for anomaly detection (GLAD), to identify anomalies in citation networks. GLAD incorporates text semantic mining to network representation learning by adding both node attributes and link attributes via graph neural networks (GNNs). It exploits not only the relevance of citation contents, but also hidden relationships between papers. Within the GLAD framework, we propose an algorithm called Citation PUrpose (CPU) to discover the purpose of citation based on citation context. The performance of GLAD is validated through a simulated anomalous citation dataset. Experimental results demonstrate the effectiveness of GLAD on the anomalous citation detection task.},
  archive      = {J_TNNLS},
  author       = {Jiaying Liu and Feng Xia and Xu Feng and Jing Ren and Huan Liu},
  doi          = {10.1109/TNNLS.2022.3145092},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2543-2557},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep graph learning for anomalous citation detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint stance and rumor detection in hierarchical
heterogeneous graph. <em>TNNLS</em>, <em>33</em>(6), 2530–2542. (<a
href="https://doi.org/10.1109/TNNLS.2021.3114027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large volumes of false or unverified information (e.g., fake news and rumors) appear frequently in emerging social media, which are often discussed on a large scale and widely disseminated, causing bad consequences. Many studies on rumor detection indicate that the stance distribution of posts is closely related to the rumor veracity. However, these two tasks are generally considered separately or just using a shared encoder/layer via multitask learning, without exploring the more profound correlation between them. In particular, the performance of existing methods relies heavily on the quality of hand-crafted features and the quantity of labeled data, which is not conducive to early rumor detection and few-shot detection. In this article, we construct a hierarchical heterogeneous graph by associating posts containing the same high-frequency words to facilitate the feature cross-topic propagation and jointly formulate stance and rumor detection as multistage classification tasks. To realize the updating of node embeddings jointly driven by stance and rumor detection, we propose a multigraph neural network framework, which can more flexibly capture the attribute and structure information of the context. Experiments on real datasets collected from Twitter and Reddit show that our method outperforms state-of-the-art by a large margin on both stance and rumor detection. And the experimental results also show that our method has better interpretability and requires less labeled data.},
  archive      = {J_TNNLS},
  author       = {Chen Li and Hao Peng and Jianxin Li and Lichao Sun and Lingjuan Lyu and Lihong Wang and Philip S. Yu and Lifang He},
  doi          = {10.1109/TNNLS.2021.3114027},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2530-2542},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Joint stance and rumor detection in hierarchical heterogeneous graph},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning fast and slow: Propedeutica for real-time malware
detection. <em>TNNLS</em>, <em>33</em>(6), 2518–2529. (<a
href="https://doi.org/10.1109/TNNLS.2021.3121248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing malware detectors on safety-critical devices have difficulties in runtime detection due to the performance overhead. In this article, we introduce P ropedeutica , a framework for efficient and effective real-time malware detection, leveraging the best of conventional machine learning (ML) and deep learning (DL) techniques. In P ropedeutica , all software start executions are considered as benign and monitored by a conventional ML classifier for fast detection. If the software receives a borderline classification from the ML detector (e.g., the software is 50\% likely to be benign and 50\% likely to be malicious), the software will be transferred to a more accurate, yet performance demanding DL detector. To address spatial–temporal dynamics and software execution heterogeneity, we introduce a novel DL architecture (D eep M alware ) for P ropedeutica with multistream inputs. We evaluated P ropedeutica with 9115 malware samples and 1338 benign software from various categories for the Windows OS. With a borderline interval of [30\%, 70\%], P ropedeutica achieves an accuracy of 94.34\% and a false-positive rate of 8.75\%, with 41.45\% of the samples moved for D eep M alware analysis. Even using only CPU, P ropedeutica can detect malware within less than 0.1 s.},
  archive      = {J_TNNLS},
  author       = {Ruimin Sun and Xiaoyong Yuan and Pan He and Qile Zhu and Aokun Chen and André Grégio and Daniela Oliveira and Xiaolin Li},
  doi          = {10.1109/TNNLS.2021.3121248},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2518-2529},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning fast and slow: Propedeutica for real-time malware detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An evaluation of anomaly detection and diagnosis in
multivariate time series. <em>TNNLS</em>, <em>33</em>(6), 2508–2517. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several techniques for multivariate time series anomaly detection have been proposed recently, but a systematic comparison on a common set of datasets and metrics is lacking. This article presents a systematic and comprehensive evaluation of unsupervised and semisupervised deep-learning-based methods for anomaly detection and diagnosis on multivariate time series data from cyberphysical systems. Unlike previous works, we vary the model and post-processing of model errors, i.e., the scoring functions independently of each other, through a grid of ten models and four scoring functions, comparing these variants to state-of-the-art methods. In time-series anomaly detection, detecting anomalous events is more important than detecting individual anomalous time points. Through experiments, we find that the existing evaluation metrics either do not take events into account or cannot distinguish between a good detector and trivial detectors, such as a random or an all-positive detector. We propose a new metric to overcome these drawbacks, namely, the composite F-score (Fc 1 ), for evaluating time-series anomaly detection. Our study highlights that dynamic scoring functions work much better than static ones for multivariate time series anomaly detection, and the choice of scoring functions often matters more than the choice of the underlying model. We also find that a simple, channel-wise model—the univariate fully connected auto-encoder, with the dynamic Gaussian scoring function emerges as a winning candidate for both anomaly detection and diagnosis, beating state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Astha Garg and Wenyu Zhang and Jules Samaran and Ramasamy Savitha and Chuan-Sheng Foo},
  doi          = {10.1109/TNNLS.2021.3105827},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2508-2517},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An evaluation of anomaly detection and diagnosis in multivariate time series},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of anomaly detectors: Context matters.
<em>TNNLS</em>, <em>33</em>(6), 2494–2507. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models are challenging the classical methods in the field of anomaly detection nowadays. Every newly published method provides evidence of outperforming its predecessors, sometimes with contradictory results. The objective of this article is twofold: to compare anomaly detection methods of various paradigms with a focus on deep generative models and identification of sources of variability that can yield different results. The methods were compared on popular tabular and image datasets. We identified that the main sources of variability are the experimental conditions: 1) the type of dataset (tabular or image) and the nature of anomalies (statistical or semantic) and 2) strategy of selection of hyperparameters, especially the number of available anomalies in the validation set. Methods perform differently in different contexts, i.e., under a different combination of experimental conditions together with computational time. This explains the variability of the previous results and highlights the importance of careful specification of the context in the publication of a new method. All our code and results are available for download.},
  archive      = {J_TNNLS},
  author       = {Vít Škvára and Jan Francå and Matěj Zorek and Tomáš Pevný and Václav Šmídl},
  doi          = {10.1109/TNNLS.2021.3116269},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2494-2507},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Comparison of anomaly detectors: Context matters},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Center-aware adversarial autoencoder for anomaly detection.
<em>TNNLS</em>, <em>33</em>(6), 2480–2493. (<a
href="https://doi.org/10.1109/TNNLS.2021.3122179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection based on subspace learning has attracted much attention, in which the compactness of subspace is commonly considered as the core concern. Most related studies directly optimize the distance from the subspace representation to the fixed center, and the influence of the anomaly level of each normal sample is not considered to adjust the normal concentrated areas. In such cases, it is difficult to isolate the normal areas from the anomaly ones by making the subspace compact. To this end, we propose a center-aware adversarial autoencoder (CA-AAE) method, which detects anomaly samples by acquiring more compact and discriminative subspace representations. To fully exploit the subspace information to improve the compactness, anomaly-level description and feature learning are novelly integrated herein by dividing the output space of the encoder into presubspace and postsubspace. In presubspace, the toward-center prior distribution is imposed by the adversarial learning mechanism, and the anomaly level of normal samples can be described from a probabilistic perspective. In postsubspace, a novel center-aware strategy is established to enhance the compactness of the postsubspace, which achieves adaptive adjustment of the normal areas by constructing a weighted center based on the anomaly level. Then, a flexible anomaly score function is constructed in the testing stage, in which both the toward-center loss and the reconstruction loss are combined to balance the information in the learned subspace and the original space. Compared to other related methods, the proposed CA-AAE shows the effectiveness and advantages in numerical experiments.},
  archive      = {J_TNNLS},
  author       = {Daoming Li and Qinghua Tao and Jiahao Liu and Huangang Wang},
  doi          = {10.1109/TNNLS.2021.3122179},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2480-2493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Center-aware adversarial autoencoder for anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attract–repel encoder: Learning anomaly representation away
from landmarks. <em>TNNLS</em>, <em>33</em>(6), 2466–2479. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection (AD) has attracted great interest in the data mining community. With the development of deep learning, various deep autoencoders have been used and modified to solve AD problems due to their efficient data coding and reconstruction mechanisms. However, such methods still suffer challenges when solving some practical AD tasks. On the one hand, an AD dataset may contain diverse normal patterns rather than a universal pattern. Specifically, the normal data usually distribute in multiple clusters; meanwhile, the exact number of clusters is hard to know in practice. On the other hand, most existing autoencoder-based methods focus on encoding normal features but have not considered exploring the characteristics of abnormal data. To tackle these challenges, this article proposes a novel autoencoder-based AD model, the attract–repel encoder (ARE). ARE selects some landmarks in the encoding space to represent the diverse normal patterns. Besides, ARE can adaptively update the landmarks and their quantity during training. Then this article proposes the attract–repel loss (AR loss) function to train ARE. AR loss attracts normal samples to landmarks and repels anomalies away from landmarks so that it can learn both normal and abnormal features. Finally, ARE computes a sample’s anomaly score by summing up its reconstruction error and its distance to the landmarks. Moreover, ARE can be trained either semisupervised or unsupervised. This article presents comprehensive experiments to evaluate the effectiveness of our approach.},
  archive      = {J_TNNLS},
  author       = {Jiachen Zhao and Fang Deng and Yongling Li and Jie Chen},
  doi          = {10.1109/TNNLS.2021.3105400},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2466-2479},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Attract–Repel encoder: Learning anomaly representation away from landmarks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feature encoding with autoencoders for weakly supervised
anomaly detection. <em>TNNLS</em>, <em>33</em>(6), 2454–2465. (<a
href="https://doi.org/10.1109/TNNLS.2021.3086137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised anomaly detection aims at learning an anomaly detector from a limited amount of labeled data and abundant unlabeled data. Recent works build deep neural networks for anomaly detection by discriminatively mapping the normal samples and abnormal samples to different regions in the feature space or fitting different distributions. However, due to the limited number of annotated anomaly samples, directly training networks with the discriminative loss may not be sufficient. To overcome this issue, this article proposes a novel strategy to transform the input data into a more meaningful representation that could be used for anomaly detection. Specifically, we leverage an autoencoder to encode the input data and utilize three factors, hidden representation, reconstruction residual vector, and reconstruction error, as the new representation for the input data. This representation amounts to encode a test sample with its projection on the training data manifold, its direction to its projection, and its distance to its projection. In addition to this encoding, we also propose a novel network architecture to seamlessly incorporate those three factors. From our extensive experiments, the benefits of the proposed strategy are clearly demonstrated by its superior performance over the competitive methods. Code is available at: https://github.com/yj-zhou/Feature_Encoding_with_AutoEncoders_for_Weakly-supervised_Anomaly_Detection .},
  archive      = {J_TNNLS},
  author       = {Yingjie Zhou and Xucheng Song and Yanru Zhang and Fanxing Liu and Ce Zhu and Lingqiao Liu},
  doi          = {10.1109/TNNLS.2021.3086137},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2454-2465},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Feature encoding with autoencoders for weakly supervised anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semisupervised training of deep generative models for
high-dimensional anomaly detection. <em>TNNLS</em>, <em>33</em>(6),
2444–2453. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal behaviors in industrial systems may be early warnings on critical events that may cause severe damages to facilities and security. Thus, it is important to detect abnormal behaviors accurately and timely. However, the anomaly detection problem is hard to solve in practice, mainly due to the rareness and the expensive cost to get the labels of the anomalies. Deep generative models parameterized by neural networks have achieved state-of-the-art performance in practice for many unsupervised and semisupervised learning tasks. We present a new deep generative model, Latent Enhanced regression/classification Deep Generative Model (LEDGM), for the anomaly detection problem with multidimensional data. Instead of using two-stage decoupled models, we adopt an end-to-end learning paradigm. Instead of conditioning the latent on the class label, LEDGM conditions the label prediction on the learned latent so that the optimization goal is more in favor of better anomaly detection than better reconstruction that the previously proposed deep generative models have been trained for. Experimental results on several synthetic and real-world small- and large-scale datasets demonstrate that LEDGM can achieve improved anomaly detection performance on multidimensional data with very sparse labels. The results also suggest that both labeled anomalies and labeled normal are valuable for semisupervised learning. Generally, our results show that better performance can be achieved with more labeled data. The ablation experiments show that both the original input and the learned latent provide meaningful information for LEDGM to achieve high performance.},
  archive      = {J_TNNLS},
  author       = {Qin Xie and Peng Zhang and Boseon Yu and Jaesik Choi},
  doi          = {10.1109/TNNLS.2021.3095150},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2444-2453},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised training of deep generative models for high-dimensional anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to classify with incremental new class.
<em>TNNLS</em>, <em>33</em>(6), 2429–2443. (<a
href="https://doi.org/10.1109/TNNLS.2021.3104882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New class detection and effective model expansion are of great importance in incremental data mining. In open incremental data environments, data often come with novel classes, e.g., the emergence of new classes in image classification or new topics in opinion monitoring, and is denoted as class-incremental learning (C-IL) in literature. There are two main challenges in C-IL: how to conduct novelty detection and how to update the model with few novel class instances. Most previous methods pay much attention to the former challenge while ignoring the problem of efficiently updating models. To solve this problem, we propose a novel framework to handle the incremental new class, named learning to classify with incremental new class (LC-INC), which can process these two challenges automatically in one unified framework. In detail, LC-INC utilizes a novel structure network to consider the prototype information between class centers of known classes and newly incoming instances, which can dynamically combine the prediction information with structure information to detect novel class instances efficiently. On the other hand, the proposed structure network can also act as a meta-network, which can learn to expand the model much faster and more efficiently with inadequate novel class instances. Experiments on synthetic and real-world datasets successfully validate the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Da-Wei Zhou and Yang Yang and De-Chuan Zhan},
  doi          = {10.1109/TNNLS.2021.3104882},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2429-2443},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning to classify with incremental new class},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph convolutional adversarial networks for spatiotemporal
anomaly detection. <em>TNNLS</em>, <em>33</em>(6), 2416–2428. (<a
href="https://doi.org/10.1109/TNNLS.2021.3136171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic anomalies, such as traffic accidents and unexpected crowd gathering, may endanger public safety if not handled timely. Detecting traffic anomalies in their early stage can benefit citizens’ quality of life and city planning. However, traffic anomaly detection faces two main challenges. First, it is challenging to model traffic dynamics due to the complex spatiotemporal characteristics of traffic data. Second, the criteria of traffic anomalies may vary with locations and times. In this article, we propose a spatiotemporal graph convolutional adversarial network (STGAN) to address these above challenges. More specifically, we devise a spatiotemporal generator to predict the normal traffic dynamics and a spatiotemporal discriminator to determine whether an input sequence is real or not. There are high correlations between neighboring data points in the spatial and temporal dimensions. Therefore, we propose a recent module and leverage graph convolutional gated recurrent unit (GCGRU) to help the generator and discriminator learn the spatiotemporal features of traffic dynamics and traffic anomalies, respectively. After adversarial training, the generator and discriminator can be used as detectors independently, where the generator models the normal traffic dynamics patterns and the discriminator provides detection criteria varying with spatiotemporal features. We then design a novel anomaly score combining the abilities of two detectors, which considers the misleading of unpredictable traffic dynamics to the discriminator. We evaluate our method on two real-world datasets from New York City and California. The experimental results show that the proposed method detects various traffic anomalies effectively and outperforms the state-of-the-art methods. Furthermore, the devised anomaly score achieves more robust detection performances than the general score.},
  archive      = {J_TNNLS},
  author       = {Leyan Deng and Defu Lian and Zhenya Huang and Enhong Chen},
  doi          = {10.1109/TNNLS.2021.3136171},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2416-2428},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Graph convolutional adversarial networks for spatiotemporal anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-domain graph anomaly detection. <em>TNNLS</em>,
<em>33</em>(6), 2406–2415. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection on attributed graphs has received increasing research attention lately due to the broad applications in various high-impact domains, such as cybersecurity, finance, and healthcare. Heretofore, most of the existing efforts are predominately performed in an unsupervised manner due to the expensive cost of acquiring anomaly labels, especially for newly formed domains. How to leverage the invaluable auxiliary information from a labeled attributed graph to facilitate the anomaly detection in the unlabeled attributed graph is seldom investigated. In this study, we aim to tackle the problem of cross-domain graph anomaly detection with domain adaptation. However, this task remains nontrivial mainly due to: 1) the data heterogeneity including both the topological structure and nodal attributes in an attributed graph and 2) the complexity of capturing both invariant and specific anomalies on the target domain graph. To tackle these challenges, we propose a novel framework COMMANDER for cross-domain anomaly detection on attributed graphs. Specifically, COMMANDER first compresses the two attributed graphs from different domains to low-dimensional space via a graph attentive encoder. In addition, we utilize a domain discriminator and an anomaly classifier to detect anomalies that appear across networks from different domains. In order to further detect the anomalies that merely appear in the target network, we develop an attribute decoder to provide additional signals for assessing node abnormality. Extensive experiments on various real-world cross-domain graph datasets demonstrate the efficacy of our approach.},
  archive      = {J_TNNLS},
  author       = {Kaize Ding and Kai Shu and Xuan Shan and Jundong Li and Huan Liu},
  doi          = {10.1109/TNNLS.2021.3110982},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2406-2415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cross-domain graph anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A synergistic approach for graph anomaly detection with
pattern mining and feature learning. <em>TNNLS</em>, <em>33</em>(6),
2393–2405. (<a
href="https://doi.org/10.1109/TNNLS.2021.3102609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalies on graph data has two types of methods. One is pattern mining that discovers strange structures globally such as quasi-cliques, bipartite cores, or dense blocks in the graph’s adjacency matrix. The other is feature learning that mainly uses graph neural networks (GNNs) to aggregate information from local neighborhood into node representations. However, there is a lack of study that utilizes both the global and local information for graph anomaly detection. In this article, we propose a synergistic approach that leverages pattern mining to inform the GNN algorithms on how to aggregate local information through connections to capture the global patterns. Specifically, it uses a GNN encoder to perform feature aggregation, and the pattern mining algorithms supervise the GNN training process through a novel loss function. We provide theoretical analysis on the effectiveness of the loss function, as well as empirical analysis on the proposed approach across a variety of GNN algorithms and pattern mining methods. Experiments on real-world data show that the synergistic approach performs significantly better than existing graph anomaly detection methods.},
  archive      = {J_TNNLS},
  author       = {Tong Zhao and Tianwen Jiang and Neil Shah and Meng Jiang},
  doi          = {10.1109/TNNLS.2021.3102609},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2393-2405},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A synergistic approach for graph anomaly detection with pattern mining and feature learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly detection on attributed networks via contrastive
self-supervised learning. <em>TNNLS</em>, <em>33</em>(6), 2378–2392. (<a
href="https://doi.org/10.1109/TNNLS.2021.3068344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection on attributed networks attracts considerable research interests due to wide applications of attributed networks in modeling a wide range of complex systems. Recently, the deep learning-based anomaly detection methods have shown promising results over shallow approaches, especially on networks with high-dimensional attributes and complex structures. However, existing approaches, which employ graph autoencoder as their backbone, do not fully exploit the rich information of the network, resulting in suboptimal performance. Furthermore, these methods do not directly target anomaly detection in their learning objective and fail to scale to large networks due to the full graph training mechanism. To overcome these limitations, in this article, we present a novel Co ntrastive self-supervised L earning framework for A nomaly detection on attributed networks ( CoLA for abbreviation). Our framework fully exploits the local information from network data by sampling a novel type of contrastive instance pair, which can capture the relationship between each node and its neighboring substructure in an unsupervised way. Meanwhile, a well-designed graph neural network (GNN)-based contrastive learning model is proposed to learn informative embedding from high-dimensional attributes and local structure and measure the agreement of each instance pairs with its outputted scores. The multiround predicted scores by the contrastive learning model are further used to evaluate the abnormality of each node with statistical estimation. In this way, the learning model is trained by a specific anomaly detection-aware target. Furthermore, since the input of the GNN module is batches of instance pairs instead of the full network, our framework can adapt to large networks flexibly. Experimental results show that our proposed framework outperforms the state-of-the-art baseline methods on all seven benchmark data sets.},
  archive      = {J_TNNLS},
  author       = {Yixin Liu and Zhao Li and Shirui Pan and Chen Gong and Chuan Zhou and George Karypis},
  doi          = {10.1109/TNNLS.2021.3068344},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2378-2392},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anomaly detection on attributed networks via contrastive self-supervised learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated anomaly detection via curiosity-guided search and
self-imitation learning. <em>TNNLS</em>, <em>33</em>(6), 2365–2377. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is an important data mining task with numerous applications, such as intrusion detection, credit card fraud detection, and video surveillance. However, given a specific complicated task with complicated data, the process of building an effective deep learning-based system for anomaly detection still highly relies on human expertise and laboring trials. Also, while neural architecture search (NAS) has shown its promise in discovering effective deep architectures in various domains, such as image classification, object detection, and semantic segmentation, contemporary NAS methods are not suitable for anomaly detection due to the lack of intrinsic search space, unstable search process, and low sample efficiency. To bridge the gap, in this article, we propose AutoAD, an automated anomaly detection framework, which aims to search for an optimal neural network model within a predefined search space. Specifically, we first design a curiosity-guided search strategy to overcome the curse of local optimality. A controller, which acts as a search agent, is encouraged to take actions to maximize the information gain about the controller’s internal belief. We further introduce an experience replay mechanism based on self-imitation learning to improve the sample efficiency. Experimental results on various real-world benchmark datasets demonstrate that the deep model identified by AutoAD achieves the best performance, comparing with existing handcrafted models and traditional search methods.},
  archive      = {J_TNNLS},
  author       = {Yuening Li and Zhengzhang Chen and Daochen Zha and Kaixiong Zhou and Haifeng Jin and Haifeng Chen and Xia Hu},
  doi          = {10.1109/TNNLS.2021.3105636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2365-2377},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Automated anomaly detection via curiosity-guided search and self-imitation learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Entropic out-of-distribution detection: Seamless detection
of unknown examples. <em>TNNLS</em>, <em>33</em>(6), 2350–2364. (<a
href="https://doi.org/10.1109/TNNLS.2021.3112897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we argue that the unsatisfactory out-of-distribution (OOD) detection performance of neural networks is mainly due to the SoftMax loss anisotropy and propensity to produce low entropy probability distributions in disagreement with the principle of maximum entropy. On the one hand, current OOD detection approaches usually do not directly fix the SoftMax loss drawbacks, but rather build techniques to circumvent it. Unfortunately, those methods usually produce undesired side effects (e.g., classification accuracy drop, additional hyperparameters, slower inferences, and collecting extra data). On the other hand, we propose replacing SoftMax loss with a novel loss function that does not suffer from the mentioned weaknesses. The proposed IsoMax loss is isotropic (exclusively distance-based) and provides high entropy posterior probability distributions. Replacing the SoftMax loss by IsoMax loss requires no model or training changes. Additionally, the models trained with IsoMax loss produce as fast and energy-efficient inferences as those trained using SoftMax loss. Moreover, no classification accuracy drop is observed. The proposed method does not rely on outlier/background data, hyperparameter tuning, temperature calibration, feature extraction, metric learning, adversarial training, ensemble procedures, or generative models. Our experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in replacement that significantly improves neural networks’ OOD detection performance. Hence, it may be used as a baseline OOD detection approach to be combined with current or future OOD detection techniques to achieve even higher results.},
  archive      = {J_TNNLS},
  author       = {David Macêdo and Tsang Ing Ren and Cleber Zanchettin and Adriano L. I. Oliveira and Teresa Ludermir},
  doi          = {10.1109/TNNLS.2021.3112897},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2350-2364},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Entropic out-of-distribution detection: Seamless detection of unknown examples},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memorizing structure-texture correspondence for image
anomaly detection. <em>TNNLS</em>, <em>33</em>(6), 2335–2349. (<a
href="https://doi.org/10.1109/TNNLS.2021.3101403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on image anomaly detection by leveraging only normal images in the training phase. Most previous methods tackle anomaly detection by reconstructing the input images with an autoencoder (AE)-based model, and an underlying assumption is that the reconstruction errors for the normal images are small, and those for the abnormal images are large. However, these AE-based methods, sometimes, even reconstruct the anomalies well; consequently, they are less sensitive to anomalies. To conquer this issue, we propose to reconstruct the image by leveraging the structure-texture correspondence. Specifically, we observe that, usually, for normal images, the texture can be inferred from its corresponding structure (e.g., the blood vessels in the fundus image and the structured anatomy in optical coherence tomography image), while it is hard to infer the texture from a destroyed structure for the abnormal images. Therefore, a structure-texture correspondence memory (STCM) module is proposed to reconstruct image texture from its structure, where a memory mechanism is used to characterize the mapping from the normal structure to its corresponding normal texture. As the correspondence between destroyed structure and texture cannot be characterized by the memory, the abnormal images would have a larger reconstruction error, facilitating anomaly detection. In this work, we utilize two kinds of complementary structures (i.e., the semantic structure with human-labeled category information and the low-level structure with abundant details), which are extracted by two structure extractors. The reconstructions from the two kinds of structures are fused together by a learned attention weight to get the final reconstructed image. We further feed the reconstructed image into the two aforementioned structure extractors to extract structures. On the one hand, constraining the consistency between the structures extracted from the original input and that from the reconstructed image would regularize the network training; on the other hand, the error between the structures extracted from the original input and that from the reconstructed image can also be used as a supplement measurement to identify the anomaly. Extensive experiments validate the effectiveness of our method for image anomaly detection on both industrial inspection images and medical images.},
  archive      = {J_TNNLS},
  author       = {Kang Zhou and Jing Li and Yuting Xiao and Jianlong Yang and Jun Cheng and Wen Liu and Weixin Luo and Jiang Liu and Shenghua Gao},
  doi          = {10.1109/TNNLS.2021.3101403},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2335-2349},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memorizing structure-texture correspondence for image anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-augmented generative adversarial networks for anomaly
detection. <em>TNNLS</em>, <em>33</em>(6), 2324–2334. (<a
href="https://doi.org/10.1109/TNNLS.2021.3132928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a memory-augmented deep learning model for semisupervised anomaly detection (AD). While many traditional AD methods focus on modeling the distribution of normal data, additional constraints in the modeling process are needed to distinguish between normal and abnormal data. The proposed model, named memory augmented generative adversarial networks (MEMGAN), is coupled with external memory units through attentional operations. One property of MEMGAN in the latent space is such that encoded normal data are expected to reside in the convex hull of the memory units, while the abnormal ones are separated outside. This property makes the AD process of MEMGAN more robust and reliable. Experiments on AD datasets adapted from MVTec, MNIST, CIFAR10, and Arrhythmia demonstrate that MEMGAN notably improves over previous AD models. We also find that the decoded memory units in MEMGAN are more diverse and interpretable than those in previous memory-augmented models.},
  archive      = {J_TNNLS},
  author       = {Ziyi Yang and Teng Zhang and Iman Soltani Bozchalooi and Eric Darve},
  doi          = {10.1109/TNNLS.2021.3132928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2324-2334},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory-augmented generative adversarial networks for anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MOCCA: Multilayer one-class classification for anomaly
detection. <em>TNNLS</em>, <em>33</em>(6), 2313–2323. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies are ubiquitous in all scientific fields and can express an unexpected event due to incomplete knowledge about the data distribution or an unknown process that suddenly comes into play and distorts the observations. Usually, due to such events’ rarity, to train deep learning (DL) models on the anomaly detection (AD) task, scientists only rely on “normal” data, i.e., nonanomalous samples. Thus, letting the neural network infer the distribution beneath the input data. In such a context, we propose a novel framework, named multilayer one-class classification (MOCCA), to train and test DL models on the AD task. Specifically, we applied our approach to autoencoders. A key novelty in our work stems from the explicit optimization of the intermediate representations for the task at hand. Indeed, differently from commonly used approaches that consider a neural network as a single computational block, i.e., using the output of the last layer only, MOCCA explicitly leverages the multilayer structure of deep architectures. Each layer’s feature space is optimized for AD during training, while in the test phase, the deep representations extracted from the trained layers are combined to detect anomalies. With MOCCA, we split the training process into two steps. First, the autoencoder is trained on the reconstruction task only. Then, we only retain the encoder tasked with minimizing the $L_{2}$ distance between the output representation and a reference point, the anomaly-free training data centroid, at each considered layer. Subsequently, we combine the deep features extracted at the various trained layers of the encoder model to detect anomalies at inference time. To assess the performance of the models trained with MOCCA, we conduct extensive experiments on publicly available datasets, namely CIFAR10, MVTec AD, and ShanghaiTech. We show that our proposed method reaches comparable or superior performance to state-of-the-art approaches available in the literature. Finally, we provide a model analysis to give insights regarding the benefits of our training procedure.},
  archive      = {J_TNNLS},
  author       = {Fabio Valerio Massoli and Fabrizio Falchi and Alperen Kantarci and Şeymanur Akti and Hazim Kemal Ekenel and Giuseppe Amato},
  doi          = {10.1109/TNNLS.2021.3130074},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2313-2323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {MOCCA: Multilayer one-class classification for anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust unsupervised video anomaly detection by multipath
frame prediction. <em>TNNLS</em>, <em>33</em>(6), 2301–2312. (<a
href="https://doi.org/10.1109/TNNLS.2021.3083152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is commonly used in many applications, such as security surveillance, and is very challenging. A majority of recent video anomaly detection approaches utilize deep reconstruction models, but their performance is often suboptimal because of insufficient reconstruction error differences between normal and abnormal video frames in practice. Meanwhile, frame prediction-based anomaly detection methods have shown promising performance. In this article, we propose a novel and robust unsupervised video anomaly detection method by frame prediction with a proper design which is more in line with the characteristics of surveillance videos. The proposed method is equipped with a multipath ConvGRU-based frame prediction network that can better handle semantically informative objects and areas of different scales and capture spatial-temporal dependencies in normal videos. A noise tolerance loss is introduced during training to mitigate the interference caused by background noise. Extensive experiments have been conducted on the CUHK Avenue, ShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that our proposed method outperforms existing state-of-the-art approaches. Remarkably, our proposed method obtains the frame-level AUROC score of 88.3\% on the CUHK Avenue dataset.},
  archive      = {J_TNNLS},
  author       = {Xuanzhao Wang and Zhengping Che and Bo Jiang and Ning Xiao and Ke Yang and Jian Tang and Jieping Ye and Jingyu Wang and Qi Qi},
  doi          = {10.1109/TNNLS.2021.3083152},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2301-2312},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust unsupervised video anomaly detection by multipath frame prediction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SmithNet: Strictness on motion-texture coherence for anomaly
detection. <em>TNNLS</em>, <em>33</em>(6), 2287–2300. (<a
href="https://doi.org/10.1109/TNNLS.2021.3116212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a key functionality in various vision systems, such as surveillance and security. In this work, we present a convolutional neural network (CNN) that supports the detection of anomaly, which has not been defined when building the model, at frame level. Our CNN, named SmithNet, is structured to simultaneously learn commonly occurring textures and their corresponding motion. Its architecture is a combination of: 1) an encoder extracting motion-texture coherence from each video frame and 2) two decoders that separately reconstruct the input as well as predict its typical motion from the estimated coherence. We also introduce an encoding block, which is specifically designed for the task of anomaly detection. The optimization is performed on only data of normal events, and the network is expected to determine the ones that are unusual, i.e., have not been seen before. According to the experiments on eight benchmark datasets of different environments with various anomalous events, the performance of our network is competitive or outperforms current state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Trong-Nguyen Nguyen and Sébastien Roy and Jean Meunier},
  doi          = {10.1109/TNNLS.2021.3116212},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2287-2300},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SmithNet: Strictness on motion-texture coherence for anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial deep learning for anomaly detection.
<em>TNNLS</em>, <em>33</em>(6), 2282–2286. (<a
href="https://doi.org/10.1109/TNNLS.2022.3162123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nomaly detection aims at identifying data points which are rare or significantly different from the majority of data points. Many techniques are explored to build highly efficient and effective anomaly detection systems, but they are confronted with many difficulties when dealing with complex data, such as failing to capture intricate feature interactions or extract good feature representations. Deep-learning techniques have shown very promising performance in tackling different types of complex data in a broad range of tasks/problems, including anomaly detection. To address this new trend, we organized this Special Issue on Deep Learning for Anomaly Detection to cover the latest advancements of developing deep-learning techniques specially designed for anomaly detection. This editorial note provides an overview of the paper submissions to the Special Issue, and briefly introduces each of the accepted articles.},
  archive      = {J_TNNLS},
  author       = {Guansong Pang and Charu Aggarwal and Chunhua Shen and Nicu Sebe},
  doi          = {10.1109/TNNLS.2022.3162123},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {6},
  pages        = {2282-2286},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial deep learning for anomaly detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>33</em>(5), C3. (<a
href="https://doi.org/10.1109/TNNLS.2022.3167755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2022.3167755},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised feature selection with extended OLSDA via
embedding nonnegative manifold structure. <em>TNNLS</em>,
<em>33</em>(5), 2274–2280. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As to unsupervised learning, most discriminative information is encoded in the cluster labels. To obtain the pseudo labels, unsupervised feature selection methods usually utilize spectral clustering to generate them. Nonetheless, two related disadvantages exist accordingly: 1) the performance of feature selection highly depends on the constructed Laplacian matrix and 2) the pseudo labels are obtained with mixed signs, while the real ones should be nonnegative. To address this problem, a novel approach for unsupervised feature selection is proposed by extending orthogonal least square discriminant analysis (OLSDA) to the unsupervised case, such that nonnegative pseudo labels can be achieved. Additionally, an orthogonal constraint is imposed on the class indicator to hold the manifold structure. Furthermore, $\ell _{2,1}$ regularization is imposed to ensure that the projection matrix is row sparse for efficient feature selection and proved to be equivalent to $\ell _{2,0}$ regularization. Finally, extensive experiments on nine benchmark data sets are conducted to demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Rui Zhang and Hongyuan Zhang and Xuelong Li and Sheng Yang},
  doi          = {10.1109/TNNLS.2020.3045053},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2274-2280},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised feature selection with extended OLSDA via embedding nonnegative manifold structure},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StructADMM: Achieving ultrahigh efficiency in structured
pruning for DNNs. <em>TNNLS</em>, <em>33</em>(5), 2259–2273. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weight pruning methods of deep neural networks (DNNs) have been demonstrated to achieve a good model pruning rate without loss of accuracy, thereby alleviating the significant computation/storage requirements of large-scale DNNs. Structured weight pruning methods have been proposed to overcome the limitation of irregular network structure and demonstrated actual GPU acceleration. However, in prior work, the pruning rate (degree of sparsity) and GPU acceleration are limited (to less than 50\%) when accuracy needs to be maintained. In this work, we overcome these limitations by proposing a unified, systematic framework of structured weight pruning for DNNs. It is a framework that can be used to induce different types of structured sparsity, such as filterwise, channelwise, and shapewise sparsity, as well as nonstructured sparsity. The proposed framework incorporates stochastic gradient descent (SGD; or ADAM) with alternating direction method of multipliers (ADMM) and can be understood as a dynamic regularization method in which the regularization target is analytically updated in each iteration. Leveraging special characteristics of ADMM, we further propose a progressive, multistep weight pruning framework and a network purification and unused path removal procedure, in order to achieve higher pruning rate without accuracy loss. Without loss of accuracy on the AlexNet model, we achieve $2.58\times $ and $3.65\times $ average measured speedup on two GPUs, clearly outperforming the prior work. The average speedups reach $3.15\times $ and $8.52\times $ when allowing a moderate accuracy loss of 2\%. In this case, the model compression for convolutional layers is $15.0\times $ , corresponding to $11.93\times $ measured CPU speedup. As another example, for the ResNet-18 model on the CIFAR-10 data set, we achieve an unprecedented $54.2\times $ structured pruning rate on CONV layers. This is $32\times $ higher pruning rate compared with recent work and can further translate into $7.6\times $ inference time speedup on the Adreno 640 mobile GPU compared with the original, unpruned DNN model. We share our codes and models at the link http://bit.ly/2M0V7DO .},
  archive      = {J_TNNLS},
  author       = {Tianyun Zhang and Shaokai Ye and Xiaoyu Feng and Xiaolong Ma and Kaiqi Zhang and Zhengang Li and Jian Tang and Sijia Liu and Xue Lin and Yongpan Liu and Makan Fardad and Yanzhi Wang},
  doi          = {10.1109/TNNLS.2020.3045153},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2259-2273},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {StructADMM: Achieving ultrahigh efficiency in structured pruning for DNNs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neuromorphic CMOS circuit with self-repairing capability.
<em>TNNLS</em>, <em>33</em>(5), 2246–2258. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurophysiological observations confirm that the brain not only is able to detect the impaired synapses (in brain damage) but also it is relatively capable of repairing faulty synapses. It has been shown that retrograde signaling by astrocytes leads to the modulation of synaptic transmission and thus bidirectional collaboration of astrocyte with nearby neurons is an important aspect of self-repairing mechanism. Specifically, the retrograde signaling via astrocyte can increase the transmission probability of the healthy synapses linked to the neuron. Motivated by these findings, in the present research, a CMOS neuromorphic circuit with self-repairing capabilities is proposed based on astrocyte signaling. In this way, the computational model of self-repairing process is hired as a basis for designing a novel analog integrated circuit in the 180-nm CMOS technology. It is illustrated that the proposed analog circuit is able to successfully recompense the damaged synapses by appropriately modifying the voltage signals of the remaining healthy synapses in the wide range of frequency. The proposed circuit occupies 7500- $\mu \text{m}^{2}$ silicon area and its power consumption is about $65.4~\mu \text{W}$ . This neuromorphic fault-tolerant circuit can be considered as a key candidate for future silicon neuronal systems and implementation of neurorobotic and neuro-inspired circuits.},
  archive      = {J_TNNLS},
  author       = {Ehsan Rahiminejad and Fatemeh Azad and Adel Parvizi-Fard and Mahmood Amiri and Bernabé Linares-Barranco},
  doi          = {10.1109/TNNLS.2020.3045019},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2246-2258},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A neuromorphic CMOS circuit with self-repairing capability},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contrastive adversarial domain adaptation networks for
speaker recognition. <em>TNNLS</em>, <em>33</em>(5), 2236–2245. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to reduce the mismatch between the source and target domains. A domain adversarial network (DAN) has been recently proposed to incorporate adversarial learning into deep neural networks to create a domain-invariant space. However, DAN’s major drawback is that it is difficult to find the domain-invariant space by using a single feature extractor. In this article, we propose to split the feature extractor into two contrastive branches, with one branch delegating for the class-dependence in the latent space and another branch focusing on domain-invariance. The feature extractor achieves these contrastive goals by sharing the first and last hidden layers but possessing decoupled branches in the middle hidden layers. For encouraging the feature extractor to produce class-discriminative embedded features, the label predictor is adversarially trained to produce equal posterior probabilities across all of the outputs instead of producing one-hot outputs. We refer to the resulting domain adaptation network as “contrastive adversarial domain adaptation network (CADAN).” We evaluated the embedded features’ domain-invariance via a series of speaker identification experiments under both clean and noisy conditions. Results demonstrate that the embedded features produced by CADAN lead to a 33\% improvement in speaker identification accuracy compared with the conventional DAN.},
  archive      = {J_TNNLS},
  author       = {Longxin Li and Man-Wai Mak and Jen-Tzung Chien},
  doi          = {10.1109/TNNLS.2020.3044215},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2236-2245},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Contrastive adversarial domain adaptation networks for speaker recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An off-policy trust region policy optimization method with
monotonic improvement guarantee for deep reinforcement learning.
<em>TNNLS</em>, <em>33</em>(5), 2223–2235. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep reinforcement learning, off-policy data help reduce on-policy interaction with the environment, and the trust region policy optimization (TRPO) method is efficient to stabilize the policy optimization procedure. In this article, we propose an off-policy TRPO method, off-policy TRPO, which exploits both on- and off-policy data and guarantees the monotonic improvement of policies. A surrogate objective function is developed to use both on- and off-policy data and keep the monotonic improvement of policies. We then optimize this surrogate objective function by approximately solving a constrained optimization problem under arbitrary parameterization and finite samples. We conduct experiments on representative continuous control tasks from OpenAI Gym and MuJoCo. The results show that the proposed off-policy TRPO achieves better performance in the majority of continuous control tasks compared with other trust region policy-based methods using off-policy data.},
  archive      = {J_TNNLS},
  author       = {Wenjia Meng and Qian Zheng and Yue Shi and Gang Pan},
  doi          = {10.1109/TNNLS.2020.3044196},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2223-2235},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An off-policy trust region policy optimization method with monotonic improvement guarantee for deep reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SymNet: A simple symmetric positive definite manifold deep
learning method for image set classification. <em>TNNLS</em>,
<em>33</em>(5), 2208–2222. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By representing each image set as a nonsingular covariance matrix on the symmetric positive definite (SPD) manifold, visual classification with image sets has attracted much attention. Despite the success made so far, the issue of large within-class variability of representations still remains a key challenge. Recently, several SPD matrix learning methods have been proposed to assuage this problem by directly constructing an embedding mapping from the original SPD manifold to a lower dimensional one. The advantage of this type of approach is that it cannot only implement discriminative feature selection but also preserve the Riemannian geometrical structure of the original data manifold. Inspired by this fact, we propose a simple SPD manifold deep learning network (SymNet) for image set classification in this article. Specifically, we first design SPD matrix mapping layers to map the input SPD matrices into new ones with lower dimensionality. Then, rectifying layers are devised to activate the input matrices for the purpose of forming a valid SPD manifold, chiefly to inject nonlinearity for SPD matrix learning with two nonlinear functions. Afterward, we introduce pooling layers to further compress the input SPD matrices, and the log-map layer is finally exploited to embed the resulting SPD matrices into the tangent space via log-Euclidean Riemannian computing, such that the Euclidean learning applies. For SymNet, the (2-D) 2 principal component analysis (PCA) technique is utilized to learn the multistage connection weights without requiring complicated computations, thus making it be built and trained easier. On the tail of SymNet, the kernel discriminant analysis (KDA) algorithm is coupled with the output vectorized feature representations to perform discriminative subspace learning. Extensive experiments and comparisons with state-of-the-art methods on six typical visual classification tasks demonstrate the feasibility and validity of the proposed SymNet.},
  archive      = {J_TNNLS},
  author       = {Rui Wang and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1109/TNNLS.2020.3044176},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2208-2222},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SymNet: A simple symmetric positive definite manifold deep learning method for image set classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical representation learning in graph neural
networks with node decimation pooling. <em>TNNLS</em>, <em>33</em>(5),
2195–2207. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph neural networks (GNNs), pooling operators compute local summaries of input graphs to capture their global properties, and they are fundamental for building deep GNNs that learn hierarchical representations. In this work, we propose the Node Decimation Pooling (NDP), a pooling operator for GNNs that generates coarser graphs while preserving the overall graph topology. During training, the GNN learns new node representations and fits them to a pyramid of coarsened graphs, which is computed offline in a preprocessing stage. NDP consists of three steps. First, a node decimation procedure selects the nodes belonging to one side of the partition identified by a spectral algorithm that approximates the MAXCUT solution. Afterward, the selected nodes are connected with Kron reduction to form the coarsened graph. Finally, since the resulting graph is very dense, we apply a sparsification procedure that prunes the adjacency matrix of the coarsened graph to reduce the computational cost in the GNN. Notably, we show that it is possible to remove many edges without significantly altering the graph structure. Experimental results show that NDP is more efficient compared to state-of-the-art graph pooling operators while reaching, at the same time, competitive performance on a significant variety of graph classification tasks.},
  archive      = {J_TNNLS},
  author       = {Filippo Maria Bianchi and Daniele Grattarola and Lorenzo Livi and Cesare Alippi},
  doi          = {10.1109/TNNLS.2020.3044146},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2195-2207},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical representation learning in graph neural networks with node decimation pooling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust facial landmark detection by multiorder
multiconstraint deep networks. <em>TNNLS</em>, <em>33</em>(5),
2181–2194. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, heatmap regression has been widely explored in facial landmark detection and obtained remarkable performance. However, most of the existing heatmap regression-based facial landmark detection methods neglect to explore the high-order feature correlations, which is very important to learn more representative features and enhance shape constraints. Moreover, no explicit global shape constraints have been added to the final predicted landmarks, which leads to a reduction in accuracy. To address these issues, in this article, we propose a multiorder multiconstraint deep network (MMDN) for more powerful feature correlations and shape constraints’ learning. Especially, an implicit multiorder correlating geometry-aware (IMCG) model is proposed to introduce the multiorder spatial correlations and multiorder channel correlations for more discriminative representations. Furthermore, an explicit probability-based boundary-adaptive regression (EPBR) method is developed to enhance the global shape constraints and further search the semantically consistent landmarks in the predicted boundary for robust facial landmark detection. It is interesting to show that the proposed MMDN can generate more accurate boundary-adaptive landmark heatmaps and effectively enhance shape constraints to the predicted landmarks for faces with large pose variations and heavy occlusions. Experimental results on challenging benchmark data sets demonstrate the superiority of our MMDN over state-of-the-art facial landmark detection methods.},
  archive      = {J_TNNLS},
  author       = {Jun Wan and Zhihui Lai and Jing Li and Jie Zhou and Can Gao},
  doi          = {10.1109/TNNLS.2020.3044078},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2181-2194},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust facial landmark detection by multiorder multiconstraint deep networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Brain-inspired experience reinforcement model for bin
packing in varying environments. <em>TNNLS</em>, <em>33</em>(5),
2168–2180. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bin-packing problem (BPP) is a typical combinatorial optimization problem whose decision-making process is NP-hard. This article examines BPPs in varying environments, where random number and shape of items are to be packed in different instances. The objective is to find a unified model to derive optimal decision process that maximizes the utilization of bins. To this end, by mimicking the experience-based reasoning process of humans, this article proposes a novel brain-inspired experience reinforcement model, which takes advantage of both biological and engineering systems. By learning experience from similar situations, the model is adaptive, such as the human brain for sophisticated scenarios and varying environments. The proposed model mimics the functional coordination among brain regions by knowledge representation and knowledge extraction modules. The former one corresponds to the part of information processing and experience storage. The latter one includes two parts that can train reasoning strategies and improve the decision performance. The proposed model is applied to instances of random number and shape of items of BPP. The obtained results outperform the state-of-the-art methods for BPPs in varying environments.},
  archive      = {J_TNNLS},
  author       = {Linli Zhang and Dewei Li and Shuai Jia and Haibin Shao},
  doi          = {10.1109/TNNLS.2022.3144515},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2168-2180},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Brain-inspired experience reinforcement model for bin packing in varying environments},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An MVMD-CCA recognition algorithm in SSVEP-based BCI and its
application in robot control. <em>TNNLS</em>, <em>33</em>(5), 2159–2167.
(<a href="https://doi.org/10.1109/TNNLS.2021.3135696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel recognition algorithm for the steady-state visual evoked potentials (SSVEP)-based brain–computer interface (BCI) system. By combining the advantages of multivariate variational mode decomposition (MVMD) and canonical correlation analysis (CCA), an MVMD-CCA algorithm is investigated to improve the detection ability of SSVEP electroencephalogram (EEG) signals. In comparison with the classical filter bank canonical correlation analysis (FBCCA), the nonlinear and non-stationary EEG signals are decomposed into a fixed number of sub-bands by MVMD, which can enhance the effect of SSVEP-related sub-bands. The experimental results show that MVMD-CCA can effectively reduce the influence of noise and EEG artifacts and improve the performance of SSVEP-based BCI. The offline experiments show that the average accuracies of MVMD-CCA in the training dataset and testing dataset are improved by 3.08\% and 1.67\%, respectively. In the SSVEP-based online robotic manipulator grasping experiment, the recognition accuracies of the four subjects are 92.5\%, 93.33\%, 90.83\%, and 91.67\%, respectively.},
  archive      = {J_TNNLS},
  author       = {Kang Wang and Di-Hua Zhai and Yuhan Xiong and Leyun Hu and Yuanqing Xia},
  doi          = {10.1109/TNNLS.2021.3135696},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2159-2167},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An MVMD-CCA recognition algorithm in SSVEP-based BCI and its application in robot control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward cognitive navigation: Design and implementation of a
biologically inspired head direction cell network. <em>TNNLS</em>,
<em>33</em>(5), 2147–2158. (<a
href="https://doi.org/10.1109/TNNLS.2021.3128380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a vital cognitive function of animals, the navigation skill is first built on the accurate perception of the directional heading in the environment. Head direction cells (HDCs), found in the limbic system of animals, are proven to play an important role in identifying the directional heading allocentrically in the horizontal plane, independent of the animal’s location and the ambient conditions of the environment. However, practical HDC models that can be implemented in robotic applications are rarely investigated, especially those that are biologically plausible and yet applicable to the real world. In this article, we propose a computational HDC network that is consistent with several neurophysiological findings concerning biological HDCs and then implement it in robotic navigation tasks. The HDC network keeps a representation of the directional heading only relying on the angular velocity as an input. We examine the proposed HDC model in extensive simulations and real-world experiments and demonstrate its excellent performance in terms of accuracy and real-time capability.},
  archive      = {J_TNNLS},
  author       = {Zhenshan Bing and Amir EI Sewisy and Genghang Zhuang and Florian Walter and Fabrice O. Morin and Kai Huang and Alois Knoll},
  doi          = {10.1109/TNNLS.2021.3128380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2147-2158},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward cognitive navigation: Design and implementation of a biologically inspired head direction cell network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Target convergence analysis of cancer-inspired swarms for
early disease diagnosis and targeted collective therapy. <em>TNNLS</em>,
<em>33</em>(5), 2132–2146. (<a
href="https://doi.org/10.1109/TNNLS.2021.3130207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensing and perception is generally a challenging aspect of decision-making. In the nanoscale, however, these processes face further complications due to the physical limitations of devising the nanomachines with more limited perception, more noise, and fewer sensors. There is, hence, higher dependence on swarm sensing and perception of many nanomachines. Here, taking hardware and software bioinspiration, we propose Chemo-Mechanical Cancer-Inspired Swarm Perception (CMCISP) based on online nano fuzzy haptic feedback for early disease diagnosis and targeted therapy. Particularly, we use epithelial cancer cell’s scaffold as a carrier, its properties as a distributed perception mechanism, and its motility patterns as the swarm movements such as anti-durotaxis, blebbing, and chemotaxis. We implement the in-silico model of CMCISP using a hybrid computational framework of the cellular Potts model, swarm intelligence, and fuzzy decision-making. Furthermore, the target convergence of CMCISP is analytically proved using swarm control theory. Finally, several numerical experiments and validations for cancer target therapy, cellular stiffness measurement, anti-durotaxis movement, and robustness analysis are also conducted and compared with a mathematical chemotherapy model and authors’ previous works on targeted therapy. Results show improvements of up to 57.49\% in early cancer detection, 26.64\% in target convergence, and 68.01\% in increased normoxic cell density. The study also reveals the strategy’s robustness to environmental/sensory noise by applying six SNR levels of 0, 2, 5, 10, 30, and 50 dB, with an average diagnosis error of only 0.98\% and at most 2.51\%.},
  archive      = {J_TNNLS},
  author       = {Nasibeh Rady Raz and Mohammad-R. Akbarzadeh-T},
  doi          = {10.1109/TNNLS.2021.3130207},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2132-2146},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Target convergence analysis of cancer-inspired swarms for early disease diagnosis and targeted collective therapy},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memristor-based edge computing of blaze block for image
recognition. <em>TNNLS</em>, <em>33</em>(5), 2121–2131. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel edge computing system is proposed for image recognition via memristor-based blaze block circuit, which includes a memristive convolutional neural network (MCNN) layer, two single-memristive blaze blocks (SMBBs), four double-memristive blaze blocks (DMBBs), a global Avg-pooling (GAP) layer, and a memristive full connected (MFC) layer. SMBBs and DMBBs mainly utilize the depthwise separable convolution neural network (DwCNN) that can be implemented with a much smaller memristor crossbar (MC). In the backward propagation, we use batch normalization (BN) layers to accelerate the convergence. In the forward propagation, this circuit combines DwCNN layers/CNN layers with nonseparate BN layers, which means that the required number of operational amplifiers is cut by half as long as the greatly reduced power consumption. A diode is added after the rectified linear unit (ReLU) layer to limit the output of the circuit below the threshold voltage $V_{t}$ of the memristor; thus, the circuit is more stable. Experiments show that the proposed memristor-based circuit achieves an accuracy of 84.38\% on the CIFAR-10 data set with advantages in computing resources, calculation time, and power consumption. Experiments also show that, when the number of multistate conductance is 2 8 and the quantization bit of the data is 8, the circuit can achieve its best balance between power consumption and production cost.},
  archive      = {J_TNNLS},
  author       = {Huanhuan Ran and Shiping Wen and Qian Li and Yin Yang and Kaibo Shi and Yuming Feng and Pan Zhou and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.3045029},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2121-2131},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memristor-based edge computing of blaze block for image recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memristive circuit implementation of a self-repairing
network based on biological astrocytes in robot application.
<em>TNNLS</em>, <em>33</em>(5), 2106–2120. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large number of studies have shown that astrocytes can be combined with the presynaptic terminals and postsynaptic spines of neurons to constitute a triple synapse via an endocannabinoid retrograde messenger to achieve a self-repair ability in the human brain. Inspired by the biological self-repair mechanism of astrocytes, this work proposes a self-repairing neuron network circuit that utilizes a memristor to simulate changes in neurotransmitters when a set threshold is reached. The proposed circuit simulates an astrocyte-neuron network and comprises the following: 1) a single-astrocyte-neuron circuit module; 2) an astrocyte-neuron network circuit; 3) a module to detect malfunctions; and 4) a neuron PR (release probability of synaptic transmission) enhancement module. When faults occur in a synapse, the neuron module becomes silent or near silent because of the low PR of the synapses. The circuit can detect faults automatically. The damaged neuron can be repaired by enhancing the PR of other healthy neurons, analogous to the biological repair mechanism of astrocytes. This mechanism helps to repair the damaged circuit. A simulation of the circuit revealed the following: 1) as the number of neurons in the circuit increases, the self-repair ability strengthens and 2) as the number of damaged neurons in the astrocyte-neuron network increases, the self-repair ability weakens, and there is a significant degradation in the performance of the circuit. The self-repairing circuit was used for a robot, and it effectively improved the robots’ performance and reliability.},
  archive      = {J_TNNLS},
  author       = {Qinghui Hong and Hegan Chen and Jingru Sun and Chunhua Wang},
  doi          = {10.1109/TNNLS.2020.3041624},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2106-2120},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memristive circuit implementation of a self-repairing network based on biological astrocytes in robot application},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A brain-inspired approach for collision-free movement
planning in the small operational space. <em>TNNLS</em>, <em>33</em>(5),
2094–2105. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a small operational space, e.g., mesoscale or microscale, we need to control movements carefully because of fragile objects. This article proposes a novel structure based on spiking neural networks to imitate the joint function of multiple brain regions in visual guiding in the small operational space and offers two channels to achieve collision-free movements. For the state sensation, we simulate the primary visual cortex to directly extract features from multiple input images and the high-level visual cortex to obtain the object distance, which is indirectly measurable, in the Cartesian coordinates. Our approach emulates the prefrontal cortex from two aspects: multiple liquid state machines to predict distances of the next several steps based on the preceding trajectory and a block-based excitation-inhibition feedforward network to plan movements considering the target and prediction. Responding to “too close” states needs rich temporal information, and we leverage a cerebellar network for the subconscious reaction. From the viewpoint of the inner pathway, they also form two channels. One channel starts from state extraction to attraction movement planning, both in the camera coordinates, behaving visual-servo control. The other is the collision-avoidance channel, which calculates distances, predicts trajectories, and reacts to the repulsion, all in the Cartesian coordinates. We provide appropriate supervised signals for coarse training and apply reinforcement learning to modify synapses in accordance with reality. Simulation and experiment results validate the proposed method.},
  archive      = {J_TNNLS},
  author       = {Dengpeng Xing and Jiale Li and Tielin Zhang and Bo Xu},
  doi          = {10.1109/TNNLS.2021.3111051},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2094-2105},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A brain-inspired approach for collision-free movement planning in the small operational space},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). A multiobjective evolutionary nonlinear ensemble learning
with evolutionary feature selection for silicon prediction in blast
furnace. <em>TNNLS</em>, <em>33</em>(5), 2080–2093. (<a
href="https://doi.org/10.1109/TNNLS.2021.3059784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the blast furnace ironmaking process, accurate prediction of silicon content in molten iron is of great significance for maintaining stable furnace conditions, improving hot metal quality, and reducing energy consumption. However, most of the current research works employ linear correlation coefficient methods to select input features in modeling, which may not fully take the nonlinear and coupling relationships between features into account. Therefore, this article considers the input feature selection issue of silicon content prediction model from a new perspective and proposes a multiobjective evolutionary nonlinear ensemble learning model with evolutionary feature selection mechanism (MOENE-EFS), in which extreme learning machine is adopted as the base learner. MOENE-EFS takes the input feature scheme of each base learner as well as their network structure and parameters as decision variables and proposes a modified nondominated sorting differential evolution algorithm to optimize two conflicting objectives, i.e., accuracy and diversity of base learners, simultaneously. Through the optimization, a set of Pareto optimal base learners with high accuracy and strong diversity can be obtained. Moreover, different from the linear ensemble methods commonly used in classical evolutionary ensemble learning, this article proposes a nonlinear ensemble method to combine the obtained base learners based on differential evolution. Experimental results indicate that the two proposed strategies, i.e., evolutionary feature selection and nonlinear ensemble, are very effective in improving the accuracy and stability of the prediction model. MOENE-EFS also outperforms the other prediction models in both benchmark data and practical industrial data. Furthermore, analysis on the input features of all Pareto optimal base learners shows that the evolutionary feature selection is capable of selecting essential features and is consistent with human experience, which indicates it is a promising method to deal with the input feature selection issue in silicon content prediction.},
  archive      = {J_TNNLS},
  author       = {Xianpeng Wang and Tenghui Hu and Lixin Tang},
  doi          = {10.1109/TNNLS.2021.3059784},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2080-2093},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A multiobjective evolutionary nonlinear ensemble learning with evolutionary feature selection for silicon prediction in blast furnace},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recognizing missing electromyography signal by data split
reorganization strategy and weight-based multiple neural network voting
method. <em>TNNLS</em>, <em>33</em>(5), 2070–2079. (<a
href="https://doi.org/10.1109/TNNLS.2021.3105595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface electromyography (sEMG) signals have been applied widely in prosthetic hand controlling. In the sEMG signal acquisition, wireless devices bring convenience, but also introduce signal missing due to interference or failure during data transmission. The missing signal may only last for tens of milliseconds, but have a great impact on the recognition. Researchers have employed various methods to complete missing sEMG data, but the completed signal may not totally fit the origins, and more extra calculation time will be spent. When recognizing hand gestures by sEMG from few sensors, to recognize the slightly or not serious signal missing, this study proposed a data split reorganization (DSR) strategy and a weight-based multiple neural network voting (WMV) method. To validate the proposed methods, controllable missing sEMG signals are generated artificially. Three time domain features are extracted based on non-overlapping sliding windows. The DSR is employed to make full use of the features, and then the WMV is utilized to recognize them. Nine subjects participated in the experiments, and the results indicate that the accuracy of the proposed methods is higher. For 5\%, 10\%, and 15\% data missing ratios, the accuracy is 93.66\%, 92.55\%, and 91.19\%, respectively. The Wilcoxon signed-rank test also demonstrates that these results are significantly superior to the situations in which the proposed methods are not applied. In the future, we will optimize the proposed methods to recognize the seriously missing sEMG signal.},
  archive      = {J_TNNLS},
  author       = {Feng Duan and Yikang Yang},
  doi          = {10.1109/TNNLS.2021.3105595},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2070-2079},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recognizing missing electromyography signal by data split reorganization strategy and weight-based multiple neural network voting method},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training deep neural network for optimal power allocation in
islanded microgrid systems: A distributed learning-based approach.
<em>TNNLS</em>, <em>33</em>(5), 2057–2069. (<a
href="https://doi.org/10.1109/TNNLS.2021.3054778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, numerical optimization methods are used to solve distributed optimal power allocation (OPA) problems for islanded microgrid (MG) systems. Most of them are developed based on rigorous mathematical derivation. However, the complexity of such optimization algorithms inevitably creates a gap between theoretical analysis and real-time implementation. In order to bridge such a gap, in this article we provide a new distributed learning-based framework to solve the real-time OPA problem. Specifically, inspired by the human-thinking scheme, distributed deep neural networks (DNNs) together with a dynamic average consensus algorithm are first employed to obtain an approximate OPA solution in a distributed manner. Then a distributed balance generation and demand algorithm is designed to fine-tune it to obtain the final optimal feasible solution. In addition, it is theoretically proved that the proposed DNN can well approximate one existing OPA algorithm (Guo et al. 2018), where quantitative numbers of at most how many hidden layers and neurons are provided. Several experimental case studies show that our proposed distributed learning framework can achieve similar optimal results to those obtained by using typical existing distributed numerical optimization methods while it is superior in terms of simplicity and real-time capability.},
  archive      = {J_TNNLS},
  author       = {Fanghong Guo and Bowen Xu and Wen-An Zhang and Changyun Wen and Dan Zhang and Li Yu},
  doi          = {10.1109/TNNLS.2021.3054778},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2057-2069},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Training deep neural network for optimal power allocation in islanded microgrid systems: A distributed learning-based approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning with modulated hebbian plus
q-network architecture. <em>TNNLS</em>, <em>33</em>(5), 2045–2056. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a subclass of partially observable Markov decision process (POMDP) problems which we termed confounding POMDPs. In these types of POMDPs, temporal difference (TD)-based reinforcement learning (RL) algorithms struggle, as TD error cannot be easily derived from observations. We solve these types of problems using a new bio-inspired neural architecture that combines a modulated Hebbian network (MOHN) with deep Q-network (DQN), which we call modulated Hebbian plus Q-network architecture (MOHQA). The key idea is to use a Hebbian network with rarely correlated bio-inspired neural traces to bridge temporal delays between actions and rewards when confounding observations and sparse rewards result in inaccurate TD errors. In MOHQA, DQN learns low-level features and control, while the MOHN contributes to high-level decisions by associating rewards with past states and actions. Thus, the proposed architecture combines two modules with significantly different learning algorithms, a Hebbian associative network and a classical DQN pipeline, exploiting the advantages of both. Simulations on a set of POMDPs and on the Malmo environment show that the proposed algorithm improved DQN’s results and even outperformed control tests with advantage-actor critic (A2C), quantile regression DQN with long short-term memory (QRDQN + LSTM), Monte Carlo policy gradient (REINFORCE), and aggregated memory for reinforcement learning (AMRL) algorithms on most difficult POMDPs with confounding stimuli and sparse rewards.},
  archive      = {J_TNNLS},
  author       = {Pawel Ladosz and Eseoghene Ben-Iwhiwhu and Jeffery Dick and Nicholas Ketz and Soheil Kolouri and Jeffrey L. Krichmar and Praveen K. Pilly and Andrea Soltoggio},
  doi          = {10.1109/TNNLS.2021.3110281},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2045-2056},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep reinforcement learning with modulated hebbian plus Q-network architecture},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multisample online learning for probabilistic spiking neural
networks. <em>TNNLS</em>, <em>33</em>(5), 2034–2044. (<a
href="https://doi.org/10.1109/TNNLS.2022.3144296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) capture some of the efficiency of biological brains for inference and learning via the dynamic, online, and event-driven processing of binary time series. Most existing learning algorithms for SNNs are based on deterministic neuronal models, such as leaky integrate-and-fire, and rely on heuristic approximations of backpropagation through time that enforces constraints such as locality. In contrast, probabilistic SNN models can be trained directly via principled online, local, and update rules that have proven to be particularly effective for resource-constrained systems. This article investigates another advantage of probabilistic SNNs, namely, their capacity to generate independent outputs when queried over the same input. It is shown that the multiple generated output samples can be used during inference to robustify decisions and to quantify uncertainty—a feature that deterministic SNN models cannot provide. Furthermore, they can be leveraged for training in order to obtain more accurate statistical estimates of the log-loss training criterion and its gradient. Specifically, this article introduces an online learning rule based on generalized expectation–maximization (GEM) that follows a three-factor form with global learning signals and is referred to as GEM-SNN. Experimental results on structured output memorization and classification on a standard neuromorphic dataset demonstrate significant improvements in terms of log-likelihood, accuracy, and calibration when increasing the number of samples used for inference and training.},
  archive      = {J_TNNLS},
  author       = {Hyeryung Jang and Osvaldo Simeone},
  doi          = {10.1109/TNNLS.2022.3144296},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2034-2044},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multisample online learning for probabilistic spiking neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised estimation of monocular depth and VO in dynamic
environments via hybrid masks. <em>TNNLS</em>, <em>33</em>(5),
2023–2033. (<a
href="https://doi.org/10.1109/TNNLS.2021.3100895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods mymargin have achieved remarkable performance in 3-D sensing since they perceive environments in a biologically inspired manner. Nevertheless, the existing approaches trained by monocular sequences are still prone to fail in dynamic environments. In this work, we mitigate the negative influence of dynamic environments on the joint estimation of depth and visual odometry (VO) through hybrid masks. Since both the VO estimation and view reconstruction process in the joint estimation framework is vulnerable to dynamic environments, we propose the cover mask and the filter mask to alleviate the adverse effects, respectively. As the depth and VO estimation are tightly coupled during training, the improved VO estimation promotes depth estimation as well. Besides, a depth-pose consistency loss is proposed to overcome the scale inconsistency between different training samples of monocular sequences. Experimental results show that both our depth prediction and globally consistent VO estimation are state of the art when evaluated on the KITTI benchmark. We evaluate our depth prediction model on the Make3D dataset to prove the transferability of our method as well.},
  archive      = {J_TNNLS},
  author       = {Qiyu Sun and Yang Tang and Chongzhen Zhang and Chaoqiang Zhao and Feng Qian and Jürgen Kurths},
  doi          = {10.1109/TNNLS.2021.3100895},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2023-2033},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised estimation of monocular depth and VO in dynamic environments via hybrid masks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory recall: A simple neural network training framework
against catastrophic forgetting. <em>TNNLS</em>, <em>33</em>(5),
2010–2022. (<a
href="https://doi.org/10.1109/TNNLS.2021.3099700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely acknowledged that biological intelligence is capable of learning continually without forgetting previously learned skills. Unfortunately, it has been widely observed that many artificial intelligence techniques, especially (deep) neural network (NN)-based ones, suffer from catastrophic forgetting problem, which severely forgets previous tasks when learning a new one. How to train NNs without catastrophic forgetting, which is termed continual learning, is emerging as a frontier topic and attracting considerable research interest. Inspired by memory replay and synaptic consolidation mechanism in brain, in this article, we propose a novel and simple framework termed memory recall (MeRec) for continual learning with deep NNs. In particular, we first analyze the feature stability across tasks in NN and show that NN can yield task stable features in certain layers. Then, based on this observation, we use a memory module to keep the feature statistics (mean and std) for each learned task. Based on the memory and statistics, we show that a simple replay strategy with Gaussian distribution-based feature regeneration can recall and recover the knowledge from previous tasks. Together with the weight regularization, MeRec preserves weights learned from previous tasks. Based on this simple framework, MeRec achieved leading performance with extremely small memory budget (only two feature vectors for each class) for continual learning on CIFAR-10 and CIFAR-100 datasets, with at least 50\% accuracy drop reduction after several tasks compared to previous state-of-the-art approaches.},
  archive      = {J_TNNLS},
  author       = {Baosheng Zhang and Yuchen Guo and Yipeng Li and Yuwei He and Haoqian Wang and Qionghai Dai},
  doi          = {10.1109/TNNLS.2021.3099700},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {2010-2022},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Memory recall: A simple neural network training framework against catastrophic forgetting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How frequency injection locking can train oscillatory neural
networks to compute in phase. <em>TNNLS</em>, <em>33</em>(5), 1996–2009.
(<a href="https://doi.org/10.1109/TNNLS.2021.3107771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired computing employs devices and architectures that emulate biological functions for more adaptive and energy-efficient systems. Oscillatory neural networks (ONNs) are an alternative approach in emulating biological functions of the human brain and are suitable for solving large and complex associative problems. In this work, we investigate the dynamics of coupled oscillators to implement such ONNs. By harnessing the complex dynamics of coupled oscillatory systems, we forge a novel computation model—information is encoded in the phase of oscillations. Coupled interconnected oscillators can exhibit various behaviors due to the strength of the coupling. In this article, we present a novel method based on subharmonic injection locking (SHIL) for controlling the oscillatory states of coupled oscillators that allow them to lock in frequency with distinct phase differences. Circuit-level simulation results indicate SHIL effectiveness and its applicability to large-scale oscillatory networks for pattern recognition.},
  archive      = {J_TNNLS},
  author       = {Aida Todri-Sanial and Stefania Carapezzi and Corentin Delacour and Madeleine Abernot and Thierry Gil and Elisabetta Corti and Siegfried F. Karg and Juan Nüñez and Manuel Jiménèz and María J. Avedillo and Bernabé Linares-Barranco},
  doi          = {10.1109/TNNLS.2021.3107771},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1996-2009},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {How frequency injection locking can train oscillatory neural networks to compute in phase},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-driven intrinsic plasticity for spiking convolutional
neural networks. <em>TNNLS</em>, <em>33</em>(5), 1986–1995. (<a
href="https://doi.org/10.1109/TNNLS.2021.3084955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biologically discovered intrinsic plasticity (IP) learning rule, which changes the intrinsic excitability of an individual neuron by adaptively turning the firing threshold, has been shown to be crucial for efficient information processing. However, this learning rule needs extra time for updating operations at each step, causing extra energy consumption and reducing the computational efficiency. The event-driven or spike-based coding strategy of spiking neural networks (SNNs), i.e., neurons will only be active if driven by continuous spiking trains, employs all-or-none pulses (spikes) to transmit information, contributing to sparseness in neuron activations. In this article, we propose two event-driven IP learning rules, namely, input-driven and self-driven IP, based on basic IP learning. Input-driven means that IP updating occurs only when the neuron receives spiking inputs from its presynaptic neurons, whereas self-driven means that IP updating only occurs when the neuron generates a spike. A spiking convolutional neural network (SCNN) is developed based on the ANN2SNN conversion method, i.e., converting a well-trained rate-based artificial neural network to an SNN via directly mapping the connection weights. By comparing the computational performance of SCNNs with different IP rules on the recognition of MNIST, FashionMNIST, Cifar10, and SVHN datasets, we demonstrate that the two event-based IP rules can remarkably reduce IP updating operations, contributing to sparse computations and accelerating the recognition process. This work may give insights into the modeling of brain-inspired SNNs for low-power applications.},
  archive      = {J_TNNLS},
  author       = {Anguo Zhang and Xiumin Li and Yueming Gao and Yuzhen Niu},
  doi          = {10.1109/TNNLS.2021.3084955},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1986-1995},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-driven intrinsic plasticity for spiking convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Chain-structure echo state network with stochastic
optimization: Methodology and application. <em>TNNLS</em>,
<em>33</em>(5), 1974–1985. (<a
href="https://doi.org/10.1109/TNNLS.2021.3098866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a chain-structure echo state network (CESN) with stacked subnetwork modules is newly proposed as a new kind of deep recurrent neural network for multivariate time series prediction. Motivated by the philosophy of “divide and conquer,” the related input vectors are first divided into clusters, and the final output results of CESN are then integrated by successively learning the predicted values of each clustered variable. Network structure, mathematical model, training mechanism, and stability analysis are, respectively, studied for the proposed CESN. In the training stage, least-squares regression is first used to pretrain the output weights in a module-by-module way, and stochastic local search (SLS) is developed to fine-tune network weights toward global optima. The loss function of CESN can be effectively reduced by SLS. To avoid overfitting, the optimization process is stopped when the validation error starts to increase. Finally, SLS-CESN is evaluated in chaos prediction benchmarks and real applications. Four different examples are given to verify the effectiveness and robustness of CESN and SLS-CESN.},
  archive      = {J_TNNLS},
  author       = {Zhou Wu and Qian Li and Haijun Zhang},
  doi          = {10.1109/TNNLS.2021.3098866},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1974-1985},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Chain-structure echo state network with stochastic optimization: Methodology and application},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An event-based digital time difference encoder model
implementation for neuromorphic systems. <em>TNNLS</em>, <em>33</em>(5),
1959–1973. (<a
href="https://doi.org/10.1109/TNNLS.2021.3108047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic systems are a viable alternative to conventional systems for real-time tasks with constrained resources. Their low power consumption, compact hardware realization, and low-latency response characteristics are the key ingredients of such systems. Furthermore, the event-based signal processing approach can be exploited for reducing the computational load and avoiding data loss due to its inherently sparse representation of sensed data and adaptive sampling time. In event-based systems, the information is commonly coded by the number of spikes within a specific temporal window. However, the temporal information of event-based signals can be difficult to extract when using rate coding. In this work, we present a novel digital implementation of the model, called time difference encoder (TDE), for temporal encoding on event-based signals, which translates the time difference between two consecutive input events into a burst of output events. The number of output events along with the time between them encodes the temporal information. The proposed model has been implemented as a digital circuit with a configurable time constant, allowing it to be used in a wide range of sensing tasks that require the encoding of the time difference between events, such as optical flow-based obstacle avoidance, sound source localization, and gas source localization. This proposed bioinspired model offers an alternative to the Jeffress model for the interaural time difference estimation, which is validated in this work with a sound source lateralization proof-of-concept system. The model was simulated and implemented on a field-programmable gate array (FPGA), requiring 122 slice registers of hardware resources and less than 1 mW of power consumption.},
  archive      = {J_TNNLS},
  author       = {Daniel Gutierrez-Galan and Thorben Schoepe and Juan P. Dominguez-Morales and Angel Jimenez-Fernandez and Elisabetta Chicca and Alejandro Linares-Barranco},
  doi          = {10.1109/TNNLS.2021.3108047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1959-1973},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An event-based digital time difference encoder model implementation for neuromorphic systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rectified linear postsynaptic potential function for
backpropagation in deep spiking neural networks. <em>TNNLS</em>,
<em>33</em>(5), 1947–1958. (<a
href="https://doi.org/10.1109/TNNLS.2021.3110991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) use spatiotemporal spike patterns to represent and transmit information, which are not only biologically realistic but also suitable for ultralow-power event-driven neuromorphic implementation. Just like other deep learning techniques, deep SNNs (DeepSNNs) benefit from the deep architecture. However, the training of DeepSNNs is not straightforward because the well-studied error backpropagation (BP) algorithm is not directly applicable. In this article, we first establish an understanding as to why error BP does not work well in DeepSNNs. We then propose a simple yet efficient rectified linear postsynaptic potential function (ReL-PSP) for spiking neurons and a spike-timing-dependent BP (STDBP) learning algorithm for DeepSNNs where the timing of individual spikes is used to convey information (temporal coding), and learning (BP) is performed based on spike timing in an event-driven manner. We show that DeepSNNs trained with the proposed single spike time-based learning algorithm can achieve the state-of-the-art classification accuracy. Furthermore, by utilizing the trained model parameters obtained from the proposed STDBP learning algorithm, we demonstrate ultralow-power inference operations on a recently proposed neuromorphic inference accelerator. The experimental results also show that the neuromorphic hardware consumes 0.751 mW of the total power consumption and achieves a low latency of 47.71 ms to classify an image from the Modified National Institute of Standards and Technology (MNIST) dataset. Overall, this work investigates the contribution of spike timing dynamics for information encoding, synaptic plasticity, and decision-making, providing a new perspective to the design of future DeepSNNs and neuromorphic hardware.},
  archive      = {J_TNNLS},
  author       = {Malu Zhang and Jiadong Wang and Jibin Wu and Ammar Belatreche and Burin Amornpaisannon and Zhixuan Zhang and Venkata Pavan Kumar Miriyala and Hong Qu and Yansong Chua and Trevor E. Carlson and Haizhou Li},
  doi          = {10.1109/TNNLS.2021.3110991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1947-1958},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rectified linear postsynaptic potential function for backpropagation in deep spiking neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust transcoding sensory information with neural spikes.
<em>TNNLS</em>, <em>33</em>(5), 1935–1946. (<a
href="https://doi.org/10.1109/TNNLS.2021.3107449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural coding, including encoding and decoding, is one of the key problems in neuroscience for understanding how the brain uses neural signals to relate sensory perception and motor behaviors with neural systems. However, most of the existed studies only aim at dealing with the continuous signal of neural systems, while lacking a unique feature of biological neurons, termed spike, which is the fundamental information unit for neural computation as well as a building block for brain–machine interface. Aiming at these limitations, we propose a transcoding framework to encode multi-modal sensory information into neural spikes and then reconstruct stimuli from spikes. Sensory information can be compressed into 10\% in terms of neural spikes, yet re-extract 100\% of information by reconstruction. Our framework can not only feasibly and accurately reconstruct dynamical visual and auditory scenes, but also rebuild the stimulus patterns from functional magnetic resonance imaging (fMRI) brain activities. More importantly, it has a superb ability of noise immunity for various types of artificial noises and background signals. The proposed framework provides efficient ways to perform multimodal feature representation and reconstruction in a high-throughput fashion, with potential usage for efficient neuromorphic computing in a noisy environment.},
  archive      = {J_TNNLS},
  author       = {Qi Xu and Jiangrong Shen and Xuming Ran and Huajin Tang and Gang Pan and Jian K. Liu},
  doi          = {10.1109/TNNLS.2021.3107449},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1935-1946},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust transcoding sensory information with neural spikes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Triple-memory networks: A brain-inspired method for
continual learning. <em>TNNLS</em>, <em>33</em>(5), 1925–1934. (<a
href="https://doi.org/10.1109/TNNLS.2021.3111019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual acquisition of novel experience without interfering with previously learned knowledge, i.e., continual learning, is critical for artificial neural networks, while limited by catastrophic forgetting. A neural network adjusts its parameters when learning a new task but then fails to conduct the old tasks well. By contrast, the biological brain can effectively address catastrophic forgetting through consolidating memories as more specific or more generalized forms to complement each other, which is achieved in the interplay of the hippocampus and neocortex, mediated by the prefrontal cortex. Inspired by such a brain strategy, we propose a novel approach named triple-memory networks (TMNs) for continual learning. TMNs model the interplay of the three brain regions as a triple-network architecture of generative adversarial networks (GANs). The input information is encoded as specific representations of data distributions in a generator, or generalized knowledge of solving tasks in a discriminator and a classifier, with implementing appropriate brain-inspired algorithms to alleviate catastrophic forgetting in each module. Particularly, the generator replays generated data of the learned tasks to the discriminator and the classifier, both of which are implemented with a weight consolidation regularizer to complement the lost information in the generation process. TMNs achieve the state-of-the-art performance of generative memory replay on a variety of class-incremental learning benchmarks on MNIST, SVHN, CIFAR-10, and ImageNet-50.},
  archive      = {J_TNNLS},
  author       = {Liyuan Wang and Bo Lei and Qian Li and Hang Su and Jun Zhu and Yi Zhong},
  doi          = {10.1109/TNNLS.2021.3111019},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1925-1934},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Triple-memory networks: A brain-inspired method for continual learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired dynamic collective choice in large-population
systems: A robust mean-field game perspective. <em>TNNLS</em>,
<em>33</em>(5), 1914–1924. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the collective decision making in biological systems, such as honeybee swarm searching for a new colony, we study a dynamic collective choice problem for large-population systems with the purpose of realizing certain advantageous features observed in biology. This problem focuses on the situation where a large number of heterogeneous agents subject to adversarial disturbances move from initial positions toward one of the destinations in a finite time while trying to remain close to the average trajectory of all agents. To overcome the complexity of this problem resulting from the large population and the heterogeneity of agents, and also to enforce some specific choices by individuals, we formulate the problem under consideration as a robust mean-field game with non-convex and non-smooth cost functions. Through Nash equivalence principle, we first deal with a single-player $H_{\infty }$ tracking problem by taking the population behavior as a fixed trajectory, and then establish a mean-field system to estimate the population behavior. Optimal control strategies and worst disturbances, independent of the population size, are designed, which give a way to realize the collective decision-making behavior emerged in biological systems. We further prove that the designed strategies constitute $\epsilon _{N}$ -Nash equilibrium, where $\epsilon _{N}$ goes toward zero as the number of agents increases to infinity. The effectiveness of the proposed results are illustrated through two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Man Li and Jiahu Qin and Yaonan Wang and Yu Kang},
  doi          = {10.1109/TNNLS.2020.3027428},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1914-1924},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bio-inspired dynamic collective choice in large-population systems: A robust mean-field game perspective},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered ADP for nonzero-sum games of unknown
nonlinear systems. <em>TNNLS</em>, <em>33</em>(5), 1905–1913. (<a
href="https://doi.org/10.1109/TNNLS.2021.3071545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For nonzero-sum (NZS) games of nonlinear systems, reinforcement learning (RL) or adaptive dynamic programming (ADP) has shown its capability of approximating the desired index performance and the optimal input policy iteratively. In this article, an event-triggered ADP is proposed for NZS games of continuous-time nonlinear systems with completely unknown system dynamics. To achieve the Nash equilibrium solution approximately, the critic neural networks and actor neural networks are utilized to estimate the value functions and the control policies, respectively. Compared with the traditional time-triggered mechanism, the proposed algorithm updates the neural network weights as well as the inputs of players only when a state-based event-triggered condition is violated. It is shown that the system stability and the weights’ convergence are still guaranteed under mild assumptions, while occupation of communication and computation resources is considerably reduced. Meanwhile, the infamous Zeno behavior is excluded by proving the existence of a minimum inter-event time (MIET) to ensure the feasibility of the closed-loop event-triggered continuous-time system. Finally, a numerical example is simulated to illustrate the effectiveness of the proposed approach.},
  archive      = {J_TNNLS},
  author       = {Qingtao Zhao and Jian Sun and Gang Wang and Jie Chen},
  doi          = {10.1109/TNNLS.2021.3071545},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1905-1913},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered ADP for nonzero-sum games of unknown nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed adaptive fault-tolerant time-varying formation
control of unmanned airships with limited communication ranges against
input saturation for smart city observation. <em>TNNLS</em>,
<em>33</em>(5), 1891–1904. (<a
href="https://doi.org/10.1109/TNNLS.2021.3095431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the distributed fault-tolerant time-varying formation control problem for multiple unmanned airships (UAs) against limited communication ranges and input saturation to achieve the safe observation of a smart city. To address the strongly nonlinear functions caused by the time-varying formation flight with limited communication ranges and bias faults, intelligent adaptive learning mechanisms are proposed by incorporating fuzzy neural networks. Moreover, Nussbaum functions are introduced to handle the input saturation and loss-of-effectiveness faults. The distinct features of the proposed control scheme are that time-varying formation flight, actuator faults including bias and loss-of-effectiveness faults, limited communication ranges, and input saturation are simultaneously considered. It is proven by Lyapunov stability analysis that all UAs can achieve a safe formation flight for the smart city observation even in the presence of actuator faults. Hardware-in-the-loop experiments with open-source Pixhawk autopilots are conducted to show the effectiveness of the proposed control scheme.},
  archive      = {J_TNNLS},
  author       = {Ziquan Yu and Youmin Zhang and Bin Jiang and Chun-Yi Su and Jun Fu and Ying Jin and Tianyou Chai},
  doi          = {10.1109/TNNLS.2021.3095431},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1891-1904},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Distributed adaptive fault-tolerant time-varying formation control of unmanned airships with limited communication ranges against input saturation for smart city observation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic tracking control for uncertain MIMO systems: A
biologically inspired ESN approach. <em>TNNLS</em>, <em>33</em>(5),
1881–1890. (<a
href="https://doi.org/10.1109/TNNLS.2021.3091641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a biologically inspired echo state network (ESN)-based method is established for the asymptotic tracking control of a class of uncertain multi-input multi-output (MIMO) systems. By mimicking the characters of real biological systems, a diversified multiclustered echo state network (DMCESN) is proposed in this work and then it is applied to deal with the modeling uncertainties and coupling nonlinearities in the control systems. Different from the most existing neural network (NN)-based control methods that only ensure the uniform ultimate boundedness result, the proposed method can allow the tracking error to achieve asymptotic convergence through rigorous theoretical analysis. The effectiveness of the proposed method is also confirmed by numerical simulation by comparing with multilayer feedforward network-based control scheme and traditional ESN-based control, admitting better tracking performance of the proposed control.},
  archive      = {J_TNNLS},
  author       = {Qing Chen and Kai Zhao and Xiumin Li and Yujuan Wang},
  doi          = {10.1109/TNNLS.2021.3091641},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1881-1890},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymptotic tracking control for uncertain MIMO systems: A biologically inspired ESN approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-output selective ensemble identification of nonlinear
and nonstationary industrial processes. <em>TNNLS</em>, <em>33</em>(5),
1867–1880. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key characteristic of biological systems is the ability to update the memory by learning new knowledge and removing out-of-date knowledge so that intelligent decision can be made based on the relevant knowledge acquired in the memory. Inspired by this fundamental biological principle, this article proposes a multi-output selective ensemble regression (SER) for online identification of multi-output nonlinear time-varying industrial processes. Specifically, an adaptive local learning approach is developed to automatically identify and encode a newly emerging process state by fitting a local multi-output linear model based on the multi-output hypothesis testing. This growth strategy ensures a highly diverse and independent local model set. The online modeling is constructed as a multi-output SER predictor by optimizing the combining weights of the selected local multi-output models based on a probability metric. An effective pruning strategy is also developed to remove the unwanted out-of-date local multi-output linear models in order to achieve low online computational complexity without scarifying the prediction accuracy. A simulated two-output process and two real-world identification problems are used to demonstrate the effectiveness of the proposed multi-output SER over a range of benchmark schemes for real-time identification of multi-output nonlinear and nonstationary processes, in terms of both online identification accuracy and computational complexity.},
  archive      = {J_TNNLS},
  author       = {Tong Liu and Sheng Chen and Shan Liang and Shaojun Gan and Chris J. Harris},
  doi          = {10.1109/TNNLS.2020.3027701},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1867-1880},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multi-output selective ensemble identification of nonlinear and nonstationary industrial processes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Octopus-inspired microgripper for deformation-controlled
biological sample manipulation. <em>TNNLS</em>, <em>33</em>(5),
1857–1866. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predators in nature grip their prey in different ways, which give innovational ideas of gripping approaches in industrial applications. Octopus performs flexible gripping with the help of vacuum grippers, suction cups, which inspired a new type of microgripper for biological sample micromanipulation. The proposed gripper consists of a glass pipette and a pump driven by a step-motor. The step-motor is controlled with adaptive robust control to adjust the gripping pressure applied on the biological sample. A dynamic model is developed for the biological sample aiming for better deformation control performance. A visual detection algorithm is developed for data processing to identify the parameters in the dynamic model and the detection result of visual algorithm is also used as feedback of adaptive robust control, which diminishes the negative influence of parameter and model uncertainties. Zebrafish larva was used as the testing sample for experiment and the corresponding parameters were identified experimentally. The experimental results correlated well with the model predicted deformation curve and visual detection algorithm provided promising accuracy, which is less than $4~\boldsymbol{\mu }\text{m}$ . Adaptive robust control provides fast and accuracy response in point-to-point deformation testing, and the average responding time is less than 30 s and the average error is no larger than 1 pixel.},
  archive      = {J_TNNLS},
  author       = {Cheng Qian and Mingsi Tong and Xinghu Yu and Songlin Zhuang and Huijun Gao},
  doi          = {10.1109/TNNLS.2021.3070631},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1857-1866},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Octopus-inspired microgripper for deformation-controlled biological sample manipulation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spiking adaptive dynamic programming based on poisson
process for discrete-time nonlinear systems. <em>TNNLS</em>,
<em>33</em>(5), 1846–1856. (<a
href="https://doi.org/10.1109/TNNLS.2021.3085781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new iterative spiking adaptive dynamic programming (SADP) method based on the Poisson process is developed to solve optimal impulsive control problems. For a fixed time interval, combining the Poisson process and the maximum likelihood estimation (MLE), the three-tuple of state, spiking interval, and probability of Poisson distribution can be computed, and then, the iterative value functions and iterative control laws can be obtained. A property analysis method is developed to show that the value functions converge to optimal performance index function as the iterative index increases from zero to infinity. Finally, two simulation examples are given to verify the effectiveness of the developed algorithm.},
  archive      = {J_TNNLS},
  author       = {Qinglai Wei and Liyuan Han and Tielin Zhang},
  doi          = {10.1109/TNNLS.2021.3085781},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1846-1856},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Spiking adaptive dynamic programming based on poisson process for discrete-time nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous online adaptation of bioinspired adaptive
neuroendocrine control for autonomous walking robots. <em>TNNLS</em>,
<em>33</em>(5), 1833–1845. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Walking animals can continuously adapt their locomotion to deal with unpredictable changing environments. They can also take proactive steps to avoid colliding with an obstacle. In this study, we aim to realize such features for autonomous walking robots so that they can efficiently traverse complex terrains. To achieve this, we propose novel bioinspired adaptive neuroendocrine control. In contrast to conventional locomotion control methods, this approach does not require robot and environmental models, exteroceptive feedback, or multiple learning trials. It integrates three main modular neural mechanisms, relying only on proprioceptive feedback and short-term memory, namely: 1) neural central pattern generator (CPG)-based control; 2) an artificial hormone network (AHN); and 3) unsupervised input correlation-based learning (ICO). The neural CPG-based control creates insect-like gaits, while the AHN can continuously adapt robot joint movement individually with respect to the terrain during the stance phase using only the torque feedback. In parallel, the ICO generates short-term memory for proactive obstacle negotiation during the swing phase, allowing the posterior legs to step over the obstacle before hitting it. The control approach is evaluated on a bioinspired hexapod robot walking on complex unpredictable terrains (e.g., gravel, grass, and extreme random stepfield). The results show that the robot can successfully perform energy-efficient autonomous locomotion and online continuous adaptation with proactivity to overcome such terrains. Since our adaptive neural control approach does not require a robot model, it is general and can be applied to other bioinspired walking robots to achieve a similar adaptive, autonomous, and versatile function.},
  archive      = {J_TNNLS},
  author       = {Jettanan Homchanthanakul and Poramate Manoonpong},
  doi          = {10.1109/TNNLS.2021.3119127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1833-1845},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Continuous online adaptation of bioinspired adaptive neuroendocrine control for autonomous walking robots},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-motivated two-level event-triggered controller for
nonlinear systems. <em>TNNLS</em>, <em>33</em>(5), 1825–1832. (<a
href="https://doi.org/10.1109/TNNLS.2020.3047120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a biologically motivated two-level event-triggered mechanism is proposed to design a neuroadaptive controller with exponential convergence property. Specifically, an exponential adaptive neural network controller is designed, and a two-level event-triggered mechanism is developed for a class of nonlinear systems. The two-level event-triggered mechanism, which incorporates both static and dynamic event-triggered features, is motivated by the biological response to low- and high-speed changes in the environment. We also introduce a method in which time-varying control gain is used to achieve exponential convergence of the plant state. The effectiveness of the proposed control scheme is validated by numerical simulations. The minimal interevent time internal is lower bounded by a positive number, so no Zeno behavior occurs.},
  archive      = {J_TNNLS},
  author       = {Hui Gao and Le Yin},
  doi          = {10.1109/TNNLS.2020.3047120},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1825-1832},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Bio-motivated two-level event-triggered controller for nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial biologically learned/inspired methods for sensing,
control, and decision. <em>TNNLS</em>, <em>33</em>(5), 1820–1824. (<a
href="https://doi.org/10.1109/TNNLS.2022.3161003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Special Issue aims at collecting new ideas and contributions at the frontier of bridging the gap between biological and engineering systems. Contributions include a wide range of related research topics, from neural computing to adaptive control and cooperative control, from autonomous decision systems to mathematical and computational models, and from neuropsychology-based decision and control to engineering system sensing and control algorithms, as well as applications and case studies of biologically inspired systems. This editorial note provides a brief overview of the accepted articles.},
  archive      = {J_TNNLS},
  author       = {Yongduan Song and Jennie Si and Sonya Coleman and Dermot Kerr},
  doi          = {10.1109/TNNLS.2022.3161003},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {5},
  pages        = {1820-1824},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial biologically Learned/Inspired methods for sensing, control, and decision},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>33</em>(4), C3. (<a
href="https://doi.org/10.1109/TNNLS.2022.3160958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2022.3160958},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simple yet effective way for improving the performance of
GAN. <em>TNNLS</em>, <em>33</em>(4), 1811–1818. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In adversarial learning, the discriminator often fails to guide the generator successfully since it distinguishes between real and generated images using silly or nonrobust features. To alleviate this problem, this brief presents a simple but effective way that improves the performance of the generative adversarial network (GAN) without imposing the training overhead or modifying the network architectures of existing methods. The proposed method employs a novel cascading rejection (CR) module for discriminator, which extracts multiple nonoverlapped features in an iterative manner using the vector rejection operation. Since the extracted diverse features prevent the discriminator from concentrating on nonmeaningful features, the discriminator can guide the generator effectively to produce images that are more similar to the real images. In addition, since the proposed CR module requires only a few simple vector operations, it can be readily applied to existing frameworks with marginal training overheads. Quantitative evaluations on various data sets, including CIFAR-10, CelebA, CelebA-HQ, LSUN, and tiny-ImageNet, confirm that the proposed method significantly improves the performance of GAN and conditional GAN in terms of the Frechet inception distance (FID), indicating the diversity and visual appearance of the generated images.},
  archive      = {J_TNNLS},
  author       = {Yoon-Jae Yeo and Yong-Goo Shin and Seung Park and Sung-Jea Ko},
  doi          = {10.1109/TNNLS.2020.3045000},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1811-1818},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simple yet effective way for improving the performance of GAN},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking adaptive computing: Building a unified model
complexity-reduction framework with adversarial robustness.
<em>TNNLS</em>, <em>33</em>(4), 1803–1810. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive computing (AC) is a technique to dynamically select the layers to pass in a prespecified deep neural network (DNN) according to the input samples. In previous literature, AC was deemed as a standalone complexity-reduction skill. This brief studies AC through a different lens: we investigate how this strategy interacts with mainstream compression techniques in a unified complexity-reduction framework and whether its “input sample related” feature helps with the improvement of model robustness. Following this direction, we first propose a defensive accelerating branch (DAB) based on the AC strategy that can reduce the average computational cost and inference time of DNNs with higher accuracy compared with its counterparts. Then, the proposed DAB is jointly applied with the mainstream parameterwise compression skills, pruning and quantization, to build a unified complexity-reduction framework. Extensive experiments are conducted, and the results reveal quasi-orthogonality between the input-related and parameterwise complexity-reduction skills, which means that the proposed AC can be integrated into an off-the-shelf compressed model without hurting its accuracy. Besides, the robustness of the proposed compression framework is explored, and the experimental results demonstrate that DAB can be used as both the detector and the defensive tool when the model is under adversarial attacks. All these findings shed light on the great potential of DAB in building a unified complexity-reduction framework with both a high compression ratio and great adversarial robustness.},
  archive      = {J_TNNLS},
  author       = {Meiqi Wang and Liulu He and Jun Lin and Zhongfeng Wang},
  doi          = {10.1109/TNNLS.2020.3043329},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1803-1810},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Rethinking adaptive computing: Building a unified model complexity-reduction framework with adversarial robustness},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictor-based neural dynamic surface control for bipartite
tracking of a class of nonlinear multiagent systems. <em>TNNLS</em>,
<em>33</em>(4), 1791–1802. (<a
href="https://doi.org/10.1109/TNNLS.2020.3045026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with bipartite tracking for a class of nonlinear multiagent systems under a signed directed graph, where the followers are with unknown virtual control gains. In the predictor-based neural dynamic surface control (NDSC) framework, a bipartite tracking control strategy is proposed by the introduction of predictors and the minimal number of learning parameters (MNLPs) technology along with the graph theory. Different from the traditional NDSC, the predictor-based NDSC utilizes prediction errors to update the neural network for improving system transient performance. The MNLPs technology is employed to avoid the problem of “explosion of learning parameters”. It is proved that all closed-loop signals steered by the proposed control strategy are bounded, and the system achieves bipartite consensus. Simulation results verify the efficiency and effectiveness of the strategy.},
  archive      = {J_TNNLS},
  author       = {Yang Yang and Qidong Liu and Dong Yue and Qing-Long Han},
  doi          = {10.1109/TNNLS.2020.3045026},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1791-1802},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Predictor-based neural dynamic surface control for bipartite tracking of a class of nonlinear multiagent systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient memristor-based circuit implementation of
squeeze-and-excitation fully convolutional neural networks.
<em>TNNLS</em>, <em>33</em>(4), 1779–1790. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a surge of interest in applying memristors to hardware implementations of deep neural networks due to various desirable properties of the memristor, such as nonvolativity, multivalue, and nanosize. Most existing neural network circuit designs, however, are based on generic frameworks that are not optimized for memristors. Furthermore, to the best of our knowledge, there are no existing efficient memristor-based implementations of complex neural network operators, such as deconvolutions and squeeze-and-excitation (SE) blocks, which are critical for achieving high accuracy in common medical image analysis applications, such as semantic segmentation. This article proposes convolution-kernel first (CKF), an efficient scheme for designing memristor-based fully convolutional neural networks (FCNs). Compared with existing neural network circuits, CKF enables effective parameter pruning, which significantly reduces circuit power consumption. Furthermore, CKF includes the novel, memristor-optimized implementations of deconvolution layers and SE blocks. Simulation results on real medical image segmentation tasks confirm that CKF obtains up to 56.2\% reduction in terms of computations and 33.62-W reduction in terms of power consumption in the circuit after weight pruning while retaining high accuracy on the test set. Moreover, the pruning results can be applied directly to existing circuits without any modification for the corresponding system.},
  archive      = {J_TNNLS},
  author       = {Jiadong Chen and Yincheng Wu and Yin Yang and Shiping Wen and Kaibo Shi and Amine Bermak and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.3044047},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1779-1790},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An efficient memristor-based circuit implementation of squeeze-and-excitation fully convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online nash solution in networked multirobot formation using
stochastic near-optimal control under dynamic events. <em>TNNLS</em>,
<em>33</em>(4), 1765–1778. (<a
href="https://doi.org/10.1109/TNNLS.2020.3044039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an online stochastic dynamic event-based near-optimal controller for formation in the networked multirobot system. The system is prone to network uncertainties, such as packet loss and transmission delay, that introduce stochasticity in the system. The multirobot formation problem poses a nonzero-sum game scenario. The near-optimal control inputs/policies based on proposed event-based methodology attain a Nash equilibrium achieving the desired formation in the system. These policies are generated online only at events using actor–critic neural network architecture whose weights are updated too at the same instants. The approach ensures system stability by deriving the ultimate boundedness of estimation errors of actor–critic weights and the event-based closed-loop formation error. The efficacy of the proposed approach has been validated in real-time using three Pioneer P3-Dx mobile robots in a multirobot framework. The control update instants are minimized to as low as 20\% and 18\% for the two follower robots.},
  archive      = {J_TNNLS},
  author       = {Narendra Kumar Dhar and Anuj Nandanwar and Nishchal K. Verma and Laxmidhar Behera},
  doi          = {10.1109/TNNLS.2020.3044039},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1765-1778},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online nash solution in networked multirobot formation using stochastic near-optimal control under dynamic events},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global-guided selective context network for scene parsing.
<em>TNNLS</em>, <em>33</em>(4), 1752–1764. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on semantic segmentation are exploiting contextual information to address the problem of inconsistent parsing prediction in big objects and ignorance in small objects. However, they utilize multilevel contextual information equally across pixels, overlooking those different pixels may demand different levels of context. Motivated by the above-mentioned intuition, we propose a novel global-guided selective context network (GSCNet) to adaptively select contextual information for improving scene parsing. Specifically, we introduce two global-guided modules, called global-guided global module (GGM) and global-guided local module (GLM), to, respectively, select global context (GC) and local context (LC) for pixels. When given an input feature map, GGM jointly employs the input feature map and its globally pooled feature to learn its global contextual demand based on which per-pixel GC is selected. While GLM adopts low-level feature from the adjacent stage as LC and synthetically models the input feature map, its globally pooled feature and LC to generate local contextual demand, based on which per-pixel LC is selected. Furthermore, we combine these two modules as a selective context block and import such SCBs in different levels of the network to propagate contextual information in a coarse-to-fine manner. Finally, we conduct extensive experiments to verify the effectiveness of our proposed model and achieve state-of-the-art performance on four challenging scene parsing data sets, i.e., Cityscapes, ADE20K, PASCAL Context, and COCO Stuff. Especially, GSCNet-101 obtains 82.6\% on Cityscapes test set without using coarse data and 56.22\% on ADE20K test set.},
  archive      = {J_TNNLS},
  author       = {Jie Jiang and Jing Liu and Jun Fu and Xinxin Zhu and Zechao Li and Hanqing Lu},
  doi          = {10.1109/TNNLS.2020.3043808},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1752-1764},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Global-guided selective context network for scene parsing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subtraction gates: Another way to learn long-term
dependencies in recurrent neural networks. <em>TNNLS</em>,
<em>33</em>(4), 1740–1751. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) can remember temporal contextual information over various time steps. The well-known gradient vanishing/explosion problem restricts the ability of RNNs to learn long-term dependencies. The gate mechanism is a well-developed method for learning long-term dependencies in long short-term memory (LSTM) models and their variants. These models usually take the multiplication terms as gates to control the input and output of RNNs during forwarding computation and to ensure a constant error flow during training. In this article, we propose the use of subtraction terms as another type of gates to learn long-term dependencies. Specifically, the multiplication gates are replaced by subtraction gates, and the activations of RNNs input and output are directly controlled by subtracting the subtrahend terms. The error flows remain constant, as the linear identity connection is retained during training. The proposed subtraction gates have more flexible options of internal activation functions than the multiplication gates of LSTM. The experimental results using the proposed Subtraction RNN (SRNN) indicate comparable performances to LSTM and gated recurrent unit in the Embedded Reber Grammar, Penn Tree Bank, and Pixel-by-Pixel MNIST experiments. To achieve these results, the SRNN requires approximate three-quarters of the parameters used by LSTM. We also show that a hybrid model combining multiplication forget gates and subtraction gates could achieve good performance.},
  archive      = {J_TNNLS},
  author       = {Tao He and Hua Mao and Zhang Yi},
  doi          = {10.1109/TNNLS.2020.3043752},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1740-1751},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Subtraction gates: Another way to learn long-term dependencies in recurrent neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Model-free adaptive control for unknown MIMO nonaffine
nonlinear discrete-time systems with experimental validation.
<em>TNNLS</em>, <em>33</em>(4), 1727–1739. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a model-free adaptive control (MFAC) algorithm based on full form dynamic linearization (FFDL) data model is presented for a class of unknown multi-input multi-output (MIMO) nonaffine nonlinear discrete-time learning systems. A virtual equivalent data model in the input–output sense to the considered plant is established first by using the FFDL technology. Then, using the obtained data model, a data-driven MFAC algorithm is designed merely using the inputs and outputs data of the closed-loop learning system. The theoretical analysis of the monotonic convergence of the tracking error dynamics, the bounded-input bounded-output (BIBO) stability, and the internal stability of the closed-loop learning system is rigorously proved by the contraction mapping principle. The effectiveness of the proposed control algorithm is verified by a simulation and a quad-rotor aircraft experimental system.},
  archive      = {J_TNNLS},
  author       = {Shuangshuang Xiong and Zhongsheng Hou},
  doi          = {10.1109/TNNLS.2020.3043711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1727-1739},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free adaptive control for unknown MIMO nonaffine nonlinear discrete-time systems with experimental validation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constructing accurate and efficient deep spiking neural
networks with double-threshold and augmented schemes. <em>TNNLS</em>,
<em>33</em>(4), 1714–1726. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are considered as a potential candidate to overcome current challenges, such as the high-power consumption encountered by artificial neural networks (ANNs); however, there is still a gap between them with respect to the recognition accuracy on various tasks. A conversion strategy was, thus, introduced recently to bridge this gap by mapping a trained ANN to an SNN. However, it is still unclear that to what extent this obtained SNN can benefit both the accuracy advantage from ANN and high efficiency from the spike-based paradigm of computation. In this article, we propose two new conversion methods, namely TerMapping and AugMapping. The TerMapping is a straightforward extension of a typical threshold-balancing method with a double-threshold scheme, while the AugMapping additionally incorporates a new scheme of augmented spike that employs a spike coefficient to carry the number of typical all-or-nothing spikes occurring at a time step. We examine the performance of our methods based on the MNIST, Fashion-MNIST, and CIFAR10 data sets. The results show that the proposed double-threshold scheme can effectively improve the accuracies of the converted SNNs. More importantly, the proposed AugMapping is more advantageous for constructing accurate, fast, and efficient deep SNNs compared with other state-of-the-art approaches. Our study, therefore, provides new approaches for further integration of advanced techniques in ANNs to improve the performance of SNNs, which could be of great merit to applied developments with spike-based neuromorphic computing.},
  archive      = {J_TNNLS},
  author       = {Qiang Yu and Chenxiang Ma and Shiming Song and Gaoyan Zhang and Jianwu Dang and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.3043415},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1714-1726},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Constructing accurate and efficient deep spiking neural networks with double-threshold and augmented schemes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised feature selection with constrained ℓ₂,₀-norm
and optimized graph. <em>TNNLS</em>, <em>33</em>(4), 1702–1713. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel feature selection approach, named unsupervised feature selection with constrained $\ell _{2,0}$ -norm (row-sparsity constrained) and optimized graph (RSOGFS), which unifies feature selection and similarity matrix construction into a general framework instead of independently performing the two-stage process; thus, the similarity matrix preserving the local manifold structure of data can be determined adaptively. Unlike those sparse learning-based feature selection methods that can only solve the relaxation or approximation problems by introducing sparsity regularization term into the objective function, the proposed method directly tackles the original $\ell _{2,0}$ -norm constrained problem to achieve group feature selection. Two optimization strategies are provided to solve the original sparse constrained problem. The convergence and approximation guarantees for the new algorithms are rigorously proved, and the computational complexity and parameter determination are theoretically analyzed. Experimental results on real-world data sets show that the proposed method for solving a nonconvex problem is superior to the state of the arts for solving the relaxed or approximate convex problems.},
  archive      = {J_TNNLS},
  author       = {Feiping Nie and Xia Dong and Lai Tian and Rong Wang and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3043362},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1702-1713},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised feature selection with constrained ℓ₂,₀-norm and optimized graph},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integer echo state networks: Efficient reservoir computing
for digital hardware. <em>TNNLS</em>, <em>33</em>(4), 1688–1701. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approximation of echo state networks (ESNs) that can be efficiently implemented on digital hardware based on the mathematics of hyperdimensional computing. The reservoir of the proposed integer ESN (intESN) is a vector containing only $n$ -bits integers (where $n &amp;lt; 8$ is normally sufficient for a satisfactory performance). The recurrent matrix multiplication is replaced with an efficient cyclic shift operation. The proposed intESN approach is verified with typical tasks in reservoir computing: memorizing of a sequence of inputs, classifying time series, and learning dynamic processes. Such architecture results in dramatic improvements in memory footprint and computational efficiency, with minimal performance loss. The experiments on a field-programmable gate array confirm that the proposed intESN approach is much more energy efficient than the conventional ESN.},
  archive      = {J_TNNLS},
  author       = {Denis Kleyko and Edward Paxon Frady and Mansour Kheffache and Evgeny Osipov},
  doi          = {10.1109/TNNLS.2020.3043309},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1688-1701},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Integer echo state networks: Efficient reservoir computing for digital hardware},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transductive zero-shot hashing for multilabel image
retrieval. <em>TNNLS</em>, <em>33</em>(4), 1673–1687. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hash coding has been widely used in the approximate nearest neighbor search for large-scale image retrieval. Given semantic annotations such as class labels and pairwise similarities of the training data, hashing methods can learn and generate effective and compact binary codes. While some newly introduced images may contain undefined semantic labels, which we call unseen images, zero-shot hashing (ZSH) techniques have been studied for retrieval. However, existing ZSH methods mainly focus on the retrieval of single-label images and cannot handle multilabel ones. In this article, for the first time, a novel transductive ZSH method is proposed for multilabel unseen image retrieval. In order to predict the labels of the unseen/target data, a visual-semantic bridge is built via instance-concept coherence ranking on the seen/source data. Then, pairwise similarity loss and focal quantization loss are constructed for training a hashing model using both the seen/source and unseen/target data. Extensive evaluations on three popular multilabel data sets demonstrate that the proposed hashing method achieves significantly better results than the comparison methods.},
  archive      = {J_TNNLS},
  author       = {Qin Zou and Ling Cao and Zheng Zhang and Long Chen and Song Wang},
  doi          = {10.1109/TNNLS.2020.3043298},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1673-1687},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Transductive zero-shot hashing for multilabel image retrieval},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying the alignment of graph and features in deep
learning. <em>TNNLS</em>, <em>33</em>(4), 1663–1672. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the classification performance of graph convolutional networks (GCNs) is related to the alignment between features, graph, and ground truth, which we quantify using a subspace alignment measure (SAM) corresponding to the Frobenius norm of the matrix of pairwise chordal distances between three subspaces associated with features, graph, and ground truth. The proposed measure is based on the principal angles between subspaces and has both spectral and geometrical interpretations. We showcase the relationship between the SAM and the classification performance through the study of limiting cases of GCNs and systematic randomizations of both features and graph structure applied to a constructive example and several examples of citation networks of different origins. The analysis also reveals the relative importance of the graph and features for classification purposes.},
  archive      = {J_TNNLS},
  author       = {Yifan Qian and Paul Expert and Tom Rieu and Pietro Panzarasa and Mauricio Barahona},
  doi          = {10.1109/TNNLS.2020.3043196},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1663-1672},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Quantifying the alignment of graph and features in deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid recursive implementation of broad learning with
incremental features. <em>TNNLS</em>, <em>33</em>(4), 1650–1662. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The broad learning system (BLS) paradigm has recently emerged as a computationally efficient approach to supervised learning. Its efficiency arises from a learning mechanism based on the method of least-squares. However, the need for storing and inverting large matrices can put the efficiency of such mechanism at risk in big-data scenarios. In this work, we propose a new implementation of BLS in which the need for storing and inverting large matrices is avoided. The distinguishing features of the designed learning mechanism are as follows: 1) the training process can balance between efficient usage of memory and required iterations (hybrid recursive learning) and 2) retraining is avoided when the network is expanded (incremental learning). It is shown that, while the proposed framework is equivalent to the standard BLS in terms of trained network weights,much larger networks than the standard BLS can be smoothly trained by the proposed solution, projecting BLS toward the big-data frontier.},
  archive      = {J_TNNLS},
  author       = {Di Liu and Simone Baldi and Wenwu Yu and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3043110},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1650-1662},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A hybrid recursive implementation of broad learning with incremental features},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Angular deep supervised vector quantization for image
retrieval. <em>TNNLS</em>, <em>33</em>(4), 1638–1649. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the deep quantization methods adopt unsupervised approaches, and the quantization process usually occurs in the Euclidean space on top of the deep feature and its approximate value. When this approach is applied to the retrieval tasks, since the internal product space of the retrieval process is different from the Euclidean space of quantization, minimizing the quantization error (QE) does not necessarily lead to a good performance on the maximum inner product search (MIPS). To solve these problems, we treat Softmax classification as vector quantization (VQ) with angular decision boundaries and propose angular deep supervised VQ (ADSVQ) for image retrieval. Our approach can simultaneously learn the discriminative feature representation and the updatable codebook, both lying on a hypersphere. To reduce the QE between centroids and deep features, two regularization terms are proposed as supervision signals to encourage the intra-class compactness and inter-class balance, respectively. ADSVQ explicitly reformulates the asymmetric distance computation in MIPS to transform the image retrieval process into a two-stage classification process. Moreover, we discuss the extension of multiple-label cases from the perspective of quantization with binary classification. Extensive experiments demonstrate that the proposed ADSVQ has excellent performance on four well-known image data sets when compared with the state-of-the-art hashing methods.},
  archive      = {J_TNNLS},
  author       = {Chang Zhou and Lai Man Po and Weifeng Ou},
  doi          = {10.1109/TNNLS.2020.3043103},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1638-1649},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Angular deep supervised vector quantization for image retrieval},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Self-teaching video object segmentation. <em>TNNLS</em>,
<em>33</em>(4), 1623–1637. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation (VOS) is one of the most fundamental tasks for numerous sequent video applications. The crucial issue of online VOS is the drifting of segmenter when incrementally updated on continuous video frames under unconfident supervision constraints. In this work, we propose a self-teaching VOS (ST-VOS) method to make segmenter to learn online adaptation confidently as much as possible. In the segmenter learning at each time slice, the segment hypothesis and segmenter update are enclosed into a self-looping optimization circle such that they can be mutually improved for each other. To reduce error accumulation of the self-looping process, we specifically introduce a metalearning strategy to learn how to do this optimization within only a few iteration steps. To this end, the learning rates of segmenter are adaptively derived through metaoptimization in the channel space of convolutional kernels. Furthermore, to better launch the self-looping process, we calculate an initial mask map through part detectors and motion flow to well-establish a foundation for subsequent refinement, which could result in the robustness of the segmenter update. Extensive experiments demonstrate that this ST idea can boost the performance of baselines, and in the meantime, our ST-VOS achieves encouraging performance on the DAVIS16, Youtube-objects, DAVIS17, and SegTrackV2 data sets, where, in particular, the accuracy of 75.7\% in J-mean metric is obtained on the multi-instance DAVIS17 data set.},
  archive      = {J_TNNLS},
  author       = {Chuanwei Zhou and Chunyan Xu and Zhen Cui and Tong Zhang and Jian Yang},
  doi          = {10.1109/TNNLS.2020.3043099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1623-1637},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Self-teaching video object segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial attack on skeleton-based human action
recognition. <em>TNNLS</em>, <em>33</em>(4), 1609–1622. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models achieve impressive performance for skeleton-based human action recognition. Graph convolutional networks (GCNs) are particularly suitable for this task due to the graph-structured nature of skeleton data. However, the robustness of these models to adversarial attacks remains largely unexplored due to their complex spatiotemporal nature that must represent sparse and discrete skeleton joints. This work presents the first adversarial attack on skeleton-based action recognition with GCNs. The proposed targeted attack, termed constrained iterative attack for skeleton actions (CIASA), perturbs joint locations in an action sequence such that the resulting adversarial sequence preserves the temporal coherence, spatial integrity, and the anthropomorphic plausibility of the skeletons. CIASA achieves this feat by satisfying multiple physical constraints and employing spatial skeleton realignments for the perturbed skeletons along with regularization of the adversarial skeletons with generative networks. We also explore the possibility of semantically imperceptible localized attacks with CIASA and succeed in fooling the state-of-the-art skeleton action recognition models with high confidence. CIASA perturbations show high transferability in black-box settings. We also show that the perturbed skeleton sequences are able to induce adversarial behavior in the RGB videos created with computer graphics. A comprehensive evaluation with NTU and Kinetics data sets ascertains the effectiveness of CIASA for graph-based skeleton action recognition and reveals the imminent threat to the spatiotemporal deep learning tasks in general.},
  archive      = {J_TNNLS},
  author       = {Jian Liu and Naveed Akhtar and Ajmal Mian},
  doi          = {10.1109/TNNLS.2020.3043002},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1609-1622},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adversarial attack on skeleton-based human action recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven feedforward learning with force ripple
compensation for wafer stages: A variable-gain robust approach.
<em>TNNLS</em>, <em>33</em>(4), 1594–1608. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the increasing demand for denser integrated circuits, feedforward control plays an important role in the achievement of high servo performance of wafer stages. The preexisting feedforward control methods, however, are subject to either inflexibility to reference variations or poor robustness. In this article, these deficiencies are removed by a novel variable-gain iterative feedforward tuning (VGIFFT) method. The proposed VGIFFT method attains: 1) no involvement of any parametric model through data-driven estimation; 2) high performance regardless of reference variations through feedforward parameterization; and 3) especially high robustness against stochastic disturbance as well as against model uncertainty through a variable learning gain. What is more, the tradeoff in which preexisting methods are subject to between fast convergence and high robustness is broken through by VGIFFT. Experimental results validate the proposed method and confirm its effectiveness and enhanced performance.},
  archive      = {J_TNNLS},
  author       = {Fazhi Song and Yang Liu and Wen Jin and Jiubin Tan and Wei He},
  doi          = {10.1109/TNNLS.2020.3042975},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1594-1608},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Data-driven feedforward learning with force ripple compensation for wafer stages: A variable-gain robust approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semicentralized deep deterministic policy gradient in
cooperative StarCraft games. <em>TNNLS</em>, <em>33</em>(4), 1584–1593.
(<a href="https://doi.org/10.1109/TNNLS.2020.3042943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel semicentralized deep deterministic policy gradient (SCDDPG) algorithm for cooperative multiagent games. Specifically, we design a two-level actor-critic structure to help the agents with interactions and cooperation in the StarCraft combat. The local actor-critic structure is established for each kind of agents with partially observable information received from the environment. Then, the global actor-critic structure is built to provide the local design an overall view of the combat based on the limited centralized information, such as the health value. These two structures work together to generate the optimal control action for each agent and to achieve better cooperation in the games. Comparing with the fully centralized methods, this design can reduce the communication burden by only sending limited information to the global level during the learning process. Furthermore, the reward functions are also designed for both local and global structures based on the agents’ attributes to further improve the learning performance in the stochastic environment. The developed method has been demonstrated on several scenarios in a real-time strategy game, i.e., StarCraft. The simulation results show that the agents can effectively cooperate with their teammates and defeat the enemies in various StarCraft scenarios.},
  archive      = {J_TNNLS},
  author       = {Dong Xie and Xiangnan Zhong},
  doi          = {10.1109/TNNLS.2020.3042943},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1584-1593},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semicentralized deep deterministic policy gradient in cooperative StarCraft games},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Harvesting ambient RF for presence detection through deep
learning. <em>TNNLS</em>, <em>33</em>(4), 1571–1583. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the use of ambient radio frequency (RF) signals for human presence detection through deep learning. Using Wi-Fi signal as an example, we demonstrate that the channel state information (CSI) obtained at the receiver contains rich information about the propagation environment. Through judicious preprocessing of the estimated CSI followed by deep learning, reliable presence detection can be achieved. Several challenges in passive RF sensing are addressed. With presence detection, how to collect training data with human presence can have a significant impact on the performance. This is in contrast to activity detection when a specific motion pattern is of interest. A second challenge is that RF signals are complex-valued. Handling complex-valued input in deep learning requires careful data representation and network architecture design. Finally, human presence affects CSI variation along multiple dimensions; such variation, however, is often masked by system impediments, such as timing or frequency offset. Addressing these challenges, the proposed learning system uses preprocessing to preserve human motion-induced channel variation while insulating against other impairments. A convolutional neural network (CNN) properly trained with both magnitude and phase information is then designed to achieve reliable presence detection. Extensive experiments are conducted. Using off-the-shelf Wi-Fi devices, the proposed deep-learning-based RF sensing achieves near-perfect presence detection during multiple extended periods of test and exhibits superior performance compared with leading edge passive infrared sensors. A comparison with existing RF-based human presence detection also demonstrates its robustness in performance, especially when deployed in a completely new environment. The learning-based passive RF sensing thus provides a viable and promising alternative for presence or occupancy detection.},
  archive      = {J_TNNLS},
  author       = {Yang Liu and Tiexing Wang and Yuexin Jiang and Biao Chen},
  doi          = {10.1109/TNNLS.2020.3042908},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1571-1583},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Harvesting ambient RF for presence detection through deep learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully distributed adaptive NN-based consensus protocol for
nonlinear MASs: An attack-free approach. <em>TNNLS</em>, <em>33</em>(4),
1561–1570. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article works on the consensus problem of nonlinear multiagent systems (MASs) under directed graphs. Based on the local output information of neighboring agents, fully distributed adaptive attack-free protocols are designed, where speaking of attack-free protocol, we mean that the observer information transmission via communication channel is forbidden during the whole course. First, the fixed-time observer is introduced to estimate both the local state and the consensus error based on the local output and the relative output measurement among neighboring agents. Then, an observer-based protocol is generated by the consensus error estimation, where the adaptive gains are designed to estimate the unknown neural network constant weight matrix and the upper bound of the residual error vector. Furthermore, the fully distributed adaptive attack-free consensus protocol is proposed by introducing an extra adaptive gain to estimate the communication connectivity information. The proposed protocols are in essence attack-free since no observer information exchange among agents is undertaken during the whole process. Moreover, such a design structure takes the advantage of releasing communication burden.},
  archive      = {J_TNNLS},
  author       = {Yuezu Lv and Jialing Zhou and Guanghui Wen and Xinghuo Yu and Tingwen Huang},
  doi          = {10.1109/TNNLS.2020.3042821},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1561-1570},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fully distributed adaptive NN-based consensus protocol for nonlinear MASs: An attack-free approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast multiscale neighbor embedding. <em>TNNLS</em>,
<em>33</em>(4), 1546–1560. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction (DR) computes faithful low-dimensional (LD) representations of high-dimensional (HD) data. Outstanding performances are achieved by recent neighbor embedding (NE) algorithms such as $t$ -SNE, which mitigate the curse of dimensionality. The single-scale or multiscale nature of NE schemes drives the HD neighborhood preservation in the LD space (LDS). While single-scale methods focus on single-sized neighborhoods through the concept of perplexity, multiscale ones preserve neighborhoods in a broader range of sizes and account for the global HD organization to define the LDS. For both single-scale and multiscale methods, however, their time complexity in the number of samples is unaffordable for big data sets. Single-scale methods can be accelerated by relying on the inherent sparsity of the HD similarities they involve. On the other hand, the dense structure of the multiscale HD similarities prevents developing fast multiscale schemes in a similar way. This article addresses this difficulty by designing randomized accelerations of the multiscale methods. To account for all levels of interactions, the HD data are first subsampled at different scales, enabling to identify small and relevant neighbor sets for each data point thanks to vantage-point trees. Afterward, these sets are employed with a Barnes–Hut algorithm to cheaply evaluate the considered cost function and its gradient, enabling large-scale use of multiscale NE schemes. Extensive experiments demonstrate that the proposed accelerations are, statistically significantly, both faster than the original multiscale methods by orders of magnitude, and better preserving the HD neighborhoods than state-of-the-art single-scale schemes, leading to high-quality LD embeddings. Public codes are freely available at https://github.com/cdebodt .},
  archive      = {J_TNNLS},
  author       = {Cyril de Bodt and Dounia Mulders and Michel Verleysen and John Aldo Lee},
  doi          = {10.1109/TNNLS.2020.3042807},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1546-1560},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Fast multiscale neighbor embedding},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A variable-parameter noise-tolerant zeroing neural network
for time-variant matrix inversion with guaranteed robustness.
<em>TNNLS</em>, <em>33</em>(4), 1535–1545. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix inversion frequently occurs in the fields of science, engineering, and related fields. Numerous matrix inversion schemes are often based on the premise that the solution procedure is ideal and noise-free. However, external interference is generally ubiquitous and unavoidable in practice. Therefore, an integrated-enhanced zeroing neural network (IEZNN) model has been proposed to handle the time-variant matrix inversion issue interfered with by noise. However, the IEZNN model can only deal with small time-variant noise interference. With slightly larger noise interference, the IEZNN model may not converge to the theoretical solution exactly. Therefore, a variable-parameter noise-tolerant zeroing neural network (VPNTZNN) model is proposed to overcome shortcomings and improve the inadequacy. Moreover, the excellent convergence and robustness of the VPNTZNN model are rigorously analyzed and proven. Finally, compared with the original zeroing neural network (OZNN) model and the IEZNN model for matrix inversion, numerical simulations and a practical application reveal that the proposed VPNTZNN model has the best robust property under the same external noise interference.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Yongjun He and Jianhua Dai and Xinwang Liu and Bolin Liao and Haiyan Tan},
  doi          = {10.1109/TNNLS.2020.3042761},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1535-1545},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A variable-parameter noise-tolerant zeroing neural network for time-variant matrix inversion with guaranteed robustness},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free reinforcement learning by embedding an auxiliary
system for optimal control of nonlinear systems. <em>TNNLS</em>,
<em>33</em>(4), 1520–1534. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel integral reinforcement learning (IRL) algorithm is proposed to solve the optimal control problem for continuous-time nonlinear systems with unknown dynamics. The main challenging issue in learning is how to reject the oscillation caused by the externally added probing noise. This article challenges the issue by embedding an auxiliary trajectory that is designed as an exciting signal to learn the optimal solution. First, the auxiliary trajectory is used to decompose the state trajectory of the controlled system. Then, by using the decoupled trajectories, a model-free policy iteration (PI) algorithm is developed, where the policy evaluation step and the policy improvement step are alternated until convergence to the optimal solution. It is noted that an appropriate external input is introduced at the policy improvement step to eliminate the requirement of the input-to-state dynamics. Finally, the algorithm is implemented on the actor–critic structure. The output weights of the critic neural network (NN) and the actor NN are updated sequentially by the least-squares methods. The convergence of the algorithm and the stability of the closed-loop system are guaranteed. Two examples are given to show the effectiveness of the proposed algorithm.},
  archive      = {J_TNNLS},
  author       = {Zhenhui Xu and Tielong Shen and Daizhan Cheng},
  doi          = {10.1109/TNNLS.2020.3042589},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1520-1534},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free reinforcement learning by embedding an auxiliary system for optimal control of nonlinear systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mind the remainder: Taylor’s theorem view on recurrent
neural networks. <em>TNNLS</em>, <em>33</em>(4), 1507–1519. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) have gained tremendous popularity in almost every sequence modeling task. Despite the effort, these kinds of discrete unstructured data, such as texts, audio, and videos, are still difficult to be embedded in the feature space. Studies in improving the neural networks have accelerated since the introduction of more complex or deeper architectures. The improvements of previous methods are highly dependent on the model at the expense of huge computational sources. However, few of them pay attention to the algorithm. In this article, we bridge the Taylor series with the construction of RNN. Training RNN can be considered as a parameter estimate for the Taylor series. However, we found that there is a discrete term called the remainder in the finite Taylor series that cannot be optimized using gradient descent, which is part of the reason for the truncation error and the model falling into the local optimal solution. To address this, we propose a training algorithm that estimates the range of remainder and introduces the remainder obtained by sampling in this continuous space into the RNN to assist in optimizing the parameters. Notably, the performance of RNN can be improved without changing the RNN architecture in the testing phase. We demonstrate that our approach is able to achieve state-of-the-art performance in action recognition and cross-modal retrieval tasks.},
  archive      = {J_TNNLS},
  author       = {Xiang Guan and Yang Yang and Jingjing Li and Xing Xu and Heng Tao Shen},
  doi          = {10.1109/TNNLS.2020.3042537},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1507-1519},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mind the remainder: Taylor’s theorem view on recurrent neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerating monte carlo bayesian prediction via
approximating predictive uncertainty over the simplex. <em>TNNLS</em>,
<em>33</em>(4), 1492–1506. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the predictive uncertainty of a Bayesian learning model is critical in various decision-making problems, e.g., reinforcement learning, detecting the adversarial attack, self-driving car. As the model posterior is almost always intractable, most efforts were made on finding an accurate approximation to the true posterior. Even though a decent estimation of the model posterior is obtained, another approximation is required to compute the predictive distribution over the desired output. A common accurate solution is to use Monte Carlo (MC) integration. However, it needs to maintain a large number of samples, and evaluate the model repeatedly, and average multiple model outputs. In many real-world cases, this is computationally prohibitive. In this work, assuming that the exact posterior or a decent approximation is obtained, we propose a generic framework to approximate the output probability distribution induced by the model posterior with a parameterized model and in an amortized fashion. The aim is to approximate the predictive uncertainty of a specific Bayesian model, meanwhile alleviating the heavy workload of MC integration at testing time. The proposed method is universally applicable to Bayesian classification models that allow for posterior sampling. Theoretically, we show that the idea of amortization incurs no additional costs on approximation performance. Empirical results validate the strong practical performance of our approach.},
  archive      = {J_TNNLS},
  author       = {Yufei Cui and Wuguannan Yao and Qiao Li and Antoni B. Chan and Chun Jason Xue},
  doi          = {10.1109/TNNLS.2020.3042525},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1492-1506},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Accelerating monte carlo bayesian prediction via approximating predictive uncertainty over the simplex},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free reinforcement learning for fully cooperative
consensus problem of nonlinear multiagent systems. <em>TNNLS</em>,
<em>33</em>(4), 1482–1491. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an off-policy model-free algorithm based on reinforcement learning (RL) to optimize the fully cooperative (FC) consensus problem of nonlinear continuous-time multiagent systems (MASs). First, the optimal FC consensus problem is transformed into solving the coupled Hamilton–Jacobian–Bellman (HJB) equation. Then, we propose a policy iteration (PI)-based algorithm, which is further proved to be effective to solve the coupled HJB equation. To implement this scheme in a model-free way, a model-free Bellman equation is derived to find the optimal value function and the optimal control policy for each agent. Then, based on the least-squares approach, the tuning law for actor and critic weights is derived by employing actor and critic neural networks into the model-free Bellman equation to approximate the target policies and the value function. Finally, we propose an off-policy model-free integral RL (IRL) algorithm, which can be used to optimize the FC consensus problem of the whole system in real time by using measured data. The effectiveness of this proposed algorithm is verified by the simulation results.},
  archive      = {J_TNNLS},
  author       = {Hong Wang and Man Li},
  doi          = {10.1109/TNNLS.2020.3042508},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1482-1491},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free reinforcement learning for fully cooperative consensus problem of nonlinear multiagent systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representative task self-selection for flexible clustered
lifelong learning. <em>TNNLS</em>, <em>33</em>(4), 1467–1481. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the lifelong machine learning paradigm whose objective is to learn a sequence of tasks depending on previous experiences, e.g., knowledge library or deep network weights. However, the knowledge libraries or deep networks for most recent lifelong learning models are of prescribed size and can degenerate the performance for both learned tasks and coming ones when facing with a new task environment (cluster). To address this challenge, we propose a novel incremental clustered lifelong learning framework with two knowledge libraries: feature learning library and model knowledge library, called F lexible C lustered L ife l ong L earning (FCL 3 ). Specifically, the feature learning library modeled by an autoencoder architecture maintains a set of representation common across all the observed tasks, and the model knowledge library can be self-selected by identifying and adding new representative models (clusters). When a new task arrives, our FCL 3 model firstly transfers knowledge from these libraries to encode the new task, i.e., effectively and selectively soft-assigning this new task to multiple representative models over feature learning library. Then: 1) the new task with a higher outlier probability will be judged as a new representative, and used to redefine both feature learning library and representative models over time; or 2) the new task with lower outlier probability will only refine the feature learning library. For model optimization, we cast this lifelong learning problem as an alternating direction minimization problem as a new task comes. Finally, we evaluate the proposed framework by analyzing several multitask data sets, and the experimental results demonstrate that our FCL 3 model can achieve better performance than most lifelong learning frameworks, even batch clustered multitask learning models.},
  archive      = {J_TNNLS},
  author       = {Gan Sun and Yang Cong and Qianqian Wang and Bineng Zhong and Yun Fu},
  doi          = {10.1109/TNNLS.2020.3042500},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1467-1481},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Representative task self-selection for flexible clustered lifelong learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BayesFlow: Learning complex stochastic models with
invertible neural networks. <em>TNNLS</em>, <em>33</em>(4), 1452–1466.
(<a href="https://doi.org/10.1109/TNNLS.2020.3042395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the parameters of mathematical models is a common problem in almost all branches of science. However, this problem can prove notably difficult when processes and model descriptions become increasingly complex and an explicit likelihood function is not available. With this work, we propose a novel method for globally amortized Bayesian inference based on invertible neural networks that we call BayesFlow. The method uses simulations to learn a global estimator for the probabilistic mapping from observed data to underlying model parameters. A neural network pretrained in this way can then, without additional training or optimization, infer full posteriors on arbitrarily many real data sets involving the same model family. In addition, our method incorporates a summary network trained to embed the observed data into maximally informative summary statistics. Learning summary statistics from data makes the method applicable to modeling scenarios where standard inference techniques with handcrafted summary statistics fail. We demonstrate the utility of BayesFlow on challenging intractable models from population dynamics, epidemiology, cognitive science, and ecology. We argue that BayesFlow provides a general framework for building amortized Bayesian parameter estimation machines for any forward model from which data can be simulated.},
  archive      = {J_TNNLS},
  author       = {Stefan T. Radev and Ulf K. Mertens and Andreas Voss and Lynton Ardizzone and Ullrich Köthe},
  doi          = {10.1109/TNNLS.2020.3042395},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1452-1466},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {BayesFlow: Learning complex stochastic models with invertible neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modularizing deep learning via pairwise learning with
kernels. <em>TNNLS</em>, <em>33</em>(4), 1441–1451. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By redefining the conventional notions of layers, we present an alternative view on finitely wide, fully trainable deep neural networks as stacked linear models in feature spaces, leading to a kernel machine interpretation. Based on this construction, we then propose a provably optimal modular learning framework for classification that does not require between-module backpropagation. This modular approach brings new insights into the label requirement of deep learning (DL). It leverages only implicit pairwise labels (weak supervision) when learning the hidden modules. When training the output module, on the other hand, it requires full supervision but achieves high label efficiency, needing as few as ten randomly selected labeled examples (one from each class) to achieve 94.88\% accuracy on CIFAR-10 using a ResNet-18 backbone. Moreover, modular training enables fully modularized DL workflows, which then simplify the design and implementation of pipelines and improve the maintainability and reusability of models. To showcase the advantages of such a modularized workflow, we describe a simple yet reliable method for estimating reusability of pretrained modules as well as task transferability in a transfer learning setting. At practically no computation overhead, it precisely described the task space structure of 15 binary classification tasks from CIFAR-10.},
  archive      = {J_TNNLS},
  author       = {Shiyu Duan and Shujian Yu and José C. Príncipe},
  doi          = {10.1109/TNNLS.2020.3042346},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1441-1451},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Modularizing deep learning via pairwise learning with kernels},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiplayer stackelberg–nash game for nonlinear system via
value iteration-based integral reinforcement learning. <em>TNNLS</em>,
<em>33</em>(4), 1429–1440. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a multiplayer Stackelberg–Nash game (SNG) pertaining to a nonlinear dynamical system, including one leader and multiple followers. At the higher level, the leader makes its decision preferentially with consideration of the reaction functions of all followers, while, at the lower level, each of the followers reacts optimally to the leader’s strategy simultaneously by playing a Nash game. First, the optimal strategies for the leader and the followers are derived from down to the top, and these strategies are further shown to constitute the Stackelberg–Nash equilibrium points. Subsequently, to overcome the difficulty in calculating the equilibrium points analytically, we develop a novel two-level value iteration-based integral reinforcement learning (VI-IRL) algorithm that relies only upon partial information of system dynamics. We establish that the proposed method converges asymptotically to the equilibrium strategies under the weak coupling conditions. Moreover, we introduce effective termination criteria to guarantee the admissibility of the policy (strategy) profile obtained from a finite number of iterations of the proposed algorithm. In the implementation of our scheme, we employ neural networks (NNs) to approximate the value functions and invoke the least-squares methods to update the involved weights. Finally, the effectiveness of the developed algorithm is verified by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Man Li and Jiahu Qin and Nikolaos M. Freris and Daniel W. C. Ho},
  doi          = {10.1109/TNNLS.2020.3042331},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1429-1440},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multiplayer Stackelberg–Nash game for nonlinear system via value iteration-based integral reinforcement learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Looking closer at the scene: Multiscale representation
learning for remote sensing image scene classification. <em>TNNLS</em>,
<em>33</em>(4), 1414–1428. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image scene classification has attracted great attention because of its wide applications. Although convolutional neural network (CNN)-based methods for scene classification have achieved excellent results, the large-scale variation of the features and objects in remote sensing images limits the further improvement of the classification performance. To address this issue, we present multiscale representation for scene classification, which is realized by a global–local two-stream architecture. This architecture has two branches of the global stream and local stream, which can individually extract the global features and local features from the whole image and the most important area. In order to locate the most important area in the whole image using only image-level labels, a weakly supervised key area detection strategy of structured key area localization (SKAL) is specially designed to connect the above two streams. To verify the effectiveness of the proposed SKAL-based two-stream architecture, we conduct comparative experiments based on three widely used CNN models, including AlexNet, GoogleNet, and ResNet18, on four public remote sensing image scene classification data sets, and achieve the state-of-the-art results on all the four data sets. Our codes are provided in https://github.com/hw2hwei/SKAL .},
  archive      = {J_TNNLS},
  author       = {Qi Wang and Wei Huang and Zhitong Xiong and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3042276},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1414-1428},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Looking closer at the scene: Multiscale representation learning for remote sensing image scene classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and implementation of deep neural network-based
control for automatic parking maneuver process. <em>TNNLS</em>,
<em>33</em>(4), 1400–1413. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the design, test, and validation of a deep neural network (DNN)-based control scheme capable of predicting optimal motion commands for autonomous ground vehicles (AGVs) during the parking maneuver process. The proposed design utilizes a multilayer structure. In the first layer, a desensitized trajectory optimization method is iteratively performed to establish a set of time-optimal parking trajectories with the consideration of noise-perturbed initial configurations. Subsequently, by using the preplanned optimal parking trajectory data set, several DNNs are trained in order to learn the functional relationship between the system state-control actions in the second layer. To obtain further improvements regarding the DNN performances, a simple yet effective data aggregation approach is designed and applied. These trained DNNs are then utilized as the motion controllers to generate feedback actions in real time. Numerical results were executed to demonstrate the effectiveness and the real-time applicability of using the proposed control scheme to plan and steer the AGV parking maneuver. Experimental results were also provided to justify the algorithm performance in real-world implementations.},
  archive      = {J_TNNLS},
  author       = {Runqi Chai and Antonios Tsourdos and Al Savvaris and Senchun Chai and Yuanqing Xia and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3042120},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1400-1413},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Design and implementation of deep neural network-based control for automatic parking maneuver process},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamically generated hierarchical neural networks designed
with the aid of multiple support vector regressors and PNN architecture
with probabilistic selection. <em>TNNLS</em>, <em>33</em>(4), 1385–1399.
(<a href="https://doi.org/10.1109/TNNLS.2020.3041947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two issues on dynamically generated hierarchical neural networks such as the sort of basic neurons and how to compose a layer are considered in this article. On the first issue, a variant version of the least-square support vector regression (SVR) is chosen as a basic neuron. Support vector machine (SVM) is a representative classifier which usually shows good classification performance. Along with the SVMs, SVR was introduced to deal with the regression problem. Especially, least-square SVR has the advantages of high learning speed due to the substitution of the inequality constraints by the equality constraint in the formulation of the optimization problem. Based on the least-square SVR, the multiple least-square (MLS) SVR, which is a type of a linear combination of least-square SVRs with fuzzy clustering, is proposed to improve the modeling performance. In addition, a hierarchical neural network, where the MLS SVR is utilized as the generic node instead of the conventional polynomial, is developed. The key issues of hierarchical neural networks, which are generated dynamically layer by layer, are discussed on how to retain the diversity of the nodes located at the same layer according to the increase of the layer. In order to maintain the diversity of the nodes, various selection methods such as truncation selection and roulette wheel selection (RWS) to choose the nodes among candidate nodes are proposed. In addition, in order to reduce the computational overhead to determine all candidates which exhibit all compositions of the input variables, a new implementation method is proposed. From the viewpoint of the diversity of the selected nodes and the computational aspects, it is shown that the proposed method is preferred over the conventional design methodology.},
  archive      = {J_TNNLS},
  author       = {Seok-Beom Roh and Sung-Kwun Oh and Witold Pedrycz and Zunwei Fu},
  doi          = {10.1109/TNNLS.2020.3041947},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1385-1399},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamically generated hierarchical neural networks designed with the aid of multiple support vector regressors and PNN architecture with probabilistic selection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of end-to-end driving: Architectures and training
methods. <em>TNNLS</em>, <em>33</em>(4), 1364–1384. (<a
href="https://doi.org/10.1109/TNNLS.2020.3043505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving is of great interest to industry and academia alike. The use of machine learning approaches for autonomous driving has long been studied, but mostly in the context of perception. In this article, we take a deeper look on the so-called end-to-end approaches for autonomous driving, where the entire driving pipeline is replaced with a single neural network. We review the learning methods, input and output modalities, network architectures, and evaluation schemes in end-to-end driving literature. Interpretability and safety are discussed separately, as they remain challenging for this approach. Beyond providing a comprehensive overview of existing methods, we conclude the review with an architecture that combines the most promising elements of the end-to-end autonomous driving systems.},
  archive      = {J_TNNLS},
  author       = {Ardi Tampuu and Tambet Matiisen and Maksym Semikin and Dmytro Fishman and Naveed Muhammad},
  doi          = {10.1109/TNNLS.2020.3043505},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {4},
  pages        = {1364-1384},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey of end-to-end driving: Architectures and training methods},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>33</em>(3), C3. (<a
href="https://doi.org/10.1109/TNNLS.2022.3150941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2022.3150941},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Unsupervised feature selection via adaptive graph learning
and constraint. <em>TNNLS</em>, <em>33</em>(3), 1355–1362. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of graph-based feature selection methods relies heavily on the quality of the construction of the similarity matrix. However, most of the graphs on these methods are initially fixed, where few of them are constrained. Once the graph is determined, it will remain constant in the whole optimization process. In other words, in case that the graph constructed on the raw data is not appropriate, it will drag down the entire algorithm. Aiming to tackle this defect, a novel unsupervised feature selection via adaptive graph learning and constraint (EGCFS) is proposed to select the uncorrelated yet discriminative features by exploiting the embedded graph learning and constraint. The adaptive graph learning method incorporates the structure of the similarity matrix into the optimization process, which not only learns the graph structure adaptively but also obtains the closed-form solution of the graph coefficient. Special graph constraint is embedded with the feature selection process to connect nearer data points with larger probability. The idea of maximizing between-class scatter matrix and the adaptive graph structure is integrated into a uniform framework to obtain excellent structural performance. Moreover, the proposed embedded graph constraint not only performs with manifold structure but also validates the link between graph-based approach and $k$ -means from a unique perspective. Experiments on several benchmark data sets verify the effectiveness and superiority of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Rui Zhang and Yunxing Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3042330},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1355-1362},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised feature selection via adaptive graph learning and constraint},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved stability criteria for delayed neural networks
using a quadratic function negative-definiteness approach.
<em>TNNLS</em>, <em>33</em>(3), 1348–1354. (<a
href="https://doi.org/10.1109/TNNLS.2020.3042307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief is concerned with the stability of a neural network with a time-varying delay using the quadratic function negative-definiteness approach reported recently. A more general reciprocally convex combination inequality is taken to introduce some quadratic terms into the time derivative of a Lyapunov–Krasovskii (L–K) functional. As a result, the time derivative of the L–K functional is estimated by a novel quadratic function on the time-varying delay. Moreover, a simple way is introduced to calculate the coefficients of a quadratic function, which avoids tedious works by hand as done in some studies. The L–K functional approach is applied to derive a hierarchical type stability criterion for the delayed neural networks, which is of less conservatism in comparison with some existing results through two well-studied numerical examples.},
  archive      = {J_TNNLS},
  author       = {Jun Chen and Xian-Ming Zhang and Ju H. Park and Shengyuan Xu},
  doi          = {10.1109/TNNLS.2020.3042307},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1348-1354},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improved stability criteria for delayed neural networks using a quadratic function negative-definiteness approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmented generative networks: Data generation in the
uniform probability space. <em>TNNLS</em>, <em>33</em>(3), 1338–1347.
(<a href="https://doi.org/10.1109/TNNLS.2020.3042380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in generative networks have shown that it is possible to produce real-world-like data using deep neural networks. Some implicit probabilistic models that follow a stochastic procedure to directly generate data have been introduced to overcome the intractability of the posterior distribution. However, the ability to model data requires deep knowledge and understanding of its statistical dependence—which can be preserved and studied in appropriate latent spaces. In this article, we present a segmented generation process through linear and nonlinear manipulations in the same-dimensional latent space where data are projected to. Inspired by the known stochastic method to generate correlated data, we develop a segmented approach for the generation of dependent data, exploiting the concept of copula. The generation process is split into two frames: one embedding the covariance or copula information in the uniform probability space, and the other embedding the marginal distribution information in the sample domain. The proposed network structure, referred to as a segmented generative network (SGN), also provides an empirical method to sample directly from implicit copulas. To show its generality, we evaluate the presented approach in three application scenarios: a toy example, handwritten digits, and face image generation.},
  archive      = {J_TNNLS},
  author       = {Nunzio A. Letizia and Andrea M. Tonello},
  doi          = {10.1109/TNNLS.2020.3042380},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1338-1347},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Segmented generative networks: Data generation in the uniform probability space},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust stochastic gradient descent with student-t
distribution based first-order momentum. <em>TNNLS</em>, <em>33</em>(3),
1324–1337. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remarkable achievements by deep neural networks stand on the development of excellent stochastic gradient descent methods. Deep-learning-based machine learning algorithms, however, have to find patterns between observations and supervised signals, even though they may include some noise that hides the true relationship between them, more or less especially in the robotics domain. To perform well even with such noise, we expect them to be able to detect outliers and discard them when needed. We, therefore, propose a new stochastic gradient optimization method, whose robustness is directly built in the algorithm, using the robust student-t distribution as its core idea. We integrate our method to some of the latest stochastic gradient algorithms, and in particular, Adam, the popular optimizer, is modified through our method. The resultant algorithm, called t-Adam, along with the other stochastic gradient methods integrated with our core idea is shown to effectively outperform Adam and their original versions in terms of robustness against noise on diverse tasks, ranging from regression and classification to reinforcement learning problems.},
  archive      = {J_TNNLS},
  author       = {Wendyam Eric Lionel Ilboudo and Taisuke Kobayashi and Kenji Sugimoto},
  doi          = {10.1109/TNNLS.2020.3041755},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1324-1337},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Robust stochastic gradient descent with student-t distribution based first-order momentum},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single-image deraining via recurrent residual multiscale
networks. <em>TNNLS</em>, <em>33</em>(3), 1310–1323. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deraining approaches represent rain streaks with different rain layers and then separate the layers from the background image. However, because of the complexity of real-world rain, such as various densities, shapes, and directions of rain streaks, it is very difficult to decompose a rain image into clean background and rain layers. In this article, we develop a novel single-image deraining method based on residual multiscale pyramid to mitigate the difficulty of rain image decomposition. To be specific, we progressively remove rain streaks in a coarse-to-fine fashion, where heavy rain is first removed in coarse-resolution levels and then light rain is eliminated in fine-resolution levels. Furthermore, based on the observation that residuals between a restored image and its corresponding rain image give critical clues of rain streaks, we regard the residuals as an attention map to remove rains in the consecutive finer level image. To achieve a powerful yet compact deraining framework, we construct our network by recurrent layers and remove rain with the same network in different pyramid levels. In addition, we design a multiscale kernel selection network (MSKSN) to facilitate our single network to remove rain streaks at different levels. In this manner, we reduce 81&amp;#x0025; of the model parameters without decreasing deraining performance compared with our prior work. Extensive experimental results on widely used benchmarks show that our approach achieves superior deraining performance compared with the state of the art.},
  archive      = {J_TNNLS},
  author       = {Yupei Zheng and Xin Yu and Miaomiao Liu and Shunli Zhang},
  doi          = {10.1109/TNNLS.2020.3041752},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1310-1323},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Single-image deraining via recurrent residual multiscale networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A diversity framework for dealing with multiple types of
concept drift based on clustering in the model space. <em>TNNLS</em>,
<em>33</em>(3), 1299–1309. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream applications usually suffer from multiple types of concept drift. However, most existing approaches are only able to handle a subset of types of drift well, hindering predictive performance. We propose to use diversity as a framework to handle multiple types of drift. The motivation is that a diverse ensemble can not only contain models representing different concepts, which may be useful to handle recurring concepts, but also accelerate the adaptation to different types of concept drift. Our framework innovatively uses clustering in the model space to build a diverse ensemble and identify recurring concepts. The resulting diversity also accelerates adaptation to different types of drift where the new concept shares similarities with past concepts. Experiments with 20 synthetic and three real-world data streams containing different types of drift show that our diversity framework usually achieves similar or better prequential accuracy than existing approaches, especially when there are recurring concepts or when new concepts share similarities with past concepts.},
  archive      = {J_TNNLS},
  author       = {Chun Wai Chiu and Leandro L. Minku},
  doi          = {10.1109/TNNLS.2020.3041684},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1299-1309},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A diversity framework for dealing with multiple types of concept drift based on clustering in the model space},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed-precision kernel recursive least squares.
<em>TNNLS</em>, <em>33</em>(3), 1284–1298. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel recursive least squares (KRLS) is a widely used online machine learning algorithm for time series predictions. In this article, we present the mixed-precision KRLS, producing equivalent prediction accuracy to double-precision KRLS with a higher training throughput and a lower memory footprint. The mixed-precision KRLS applies single-precision arithmetic to the computation components being not only numerically resilient but also computationally intensive. Our mixed-precision KRLS demonstrates the 1.32, 1.15, 1.29, 1.09, and $1.08\times $ training throughput improvements using 24.95\%, 24.74\%, 24.89\%, 24.48\%, and 24.20\% less memory footprint without losing any prediction accuracy compared to double-precision KRLS for a 3-D nonlinear regression, a Lorenz chaotic time series, a Mackey-Glass chaotic time series, a sunspot number time series, and a sea surface temperature time series, respectively.},
  archive      = {J_TNNLS},
  author       = {JunKyu Lee and Dimitrios S. Nikolopoulos and Hans Vandierendonck},
  doi          = {10.1109/TNNLS.2020.3041677},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1284-1298},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mixed-precision kernel recursive least squares},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A local–global dual-stream network for building extraction
from very-high-resolution remote sensing images. <em>TNNLS</em>,
<em>33</em>(3), 1269–1283. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Buildings constitute one of the most important landscapes in remote sensing (RS) images and have been broadly analyzed in a wide range of applications from urban planning to other socioeconomic studies. As very-high-resolution (VHR) RS imagery becomes more accessible, the current building extraction methods are confronted with the challenges of the diverse appearances, various scales, and complicated structures of buildings in complex scenes. With the development of context-aware deep learning methods, it has been proven by numerous works that capturing contextual information can offer spatial relation cues for robust recognition and detection of the objects. In this article, we propose a novel local–global dual-stream network (DS-Net) that adaptively captures local and long-range information for the accurate mapping of building rooftops in VHR RS images. The local branch and the global branch of DS-Net work in a complementary manner to each other with different fields of view on the input image. Through a well-defined dual-stream architecture, DS-Net learns hierarchical representations for both the local and global branches, and a deep feature sharing strategy is further developed to enforce more collaborative integration of the two branches. Extensive experiments were carried out to verify the effectiveness of our model on three widely used VHR RS data sets: the Massachusetts buildings data set, the Inria Aerial Image Labeling data set, and the DeepGlobe Building Detection Challenge data set. Empirically, the proposed DS-Net achieves competitive or superior performance compared with the current state-of-the-art methods in terms of quantitative measures and visual evaluations.},
  archive      = {J_TNNLS},
  author       = {Hongyan Zhang and Yue Liao and Honghai Yang and Guangyi Yang and Liangpei Zhang},
  doi          = {10.1109/TNNLS.2020.3041646},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1269-1283},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A Local–Global dual-stream network for building extraction from very-high-resolution remote sensing images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locality-constrained discriminative matrix regression for
robust face identification. <em>TNNLS</em>, <em>33</em>(3), 1254–1268.
(<a href="https://doi.org/10.1109/TNNLS.2020.3041636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression-based methods have been widely applied in face identification, which attempts to approximately represent a query sample as a linear combination of all training samples. Recently, a matrix regression model based on nuclear norm has been proposed and shown strong robustness to structural noises. However, it may ignore two important issues: the label information and local relationship of data. In this article, a novel robust representation method called locality-constrained discriminative matrix regression (LDMR) is proposed, which takes label information and locality structure into account. Instead of focusing on the representation coefficients, LDMR directly imposes constraints on representation components by fully considering the label information, which has a closer connection to identification process. The locality structure characterized by subspace distances is used to learn class weights, and the correct class is forced to make more contribution to representation. Furthermore, the class weights are also incorporated into a competitive constraint on the representation components, which reduces the pairwise correlations between different classes and enhances the competitive relationships among all classes. An iterative optimization algorithm is presented to solve LDMR. Experiments on several benchmark data sets demonstrate that LDMR outperforms some state-of-the-art regression-based methods.},
  archive      = {J_TNNLS},
  author       = {Chao Zhang and Huaxiong Li and Yuhua Qian and Chunlin Chen and Xianzhong Zhou},
  doi          = {10.1109/TNNLS.2020.3041636},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1254-1268},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Locality-constrained discriminative matrix regression for robust face identification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditional random fields for multiview sequential data
modeling. <em>TNNLS</em>, <em>33</em>(3), 1242–1253. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multiview learning has been increasingly focused on machine learning. However, most existing multiview learning methods cannot directly deal with multiview sequential data, in which the inherent dynamical structure is often ignored. Especially, most traditional multiview machine learning methods assume that the items at different time slices within a sequence are independent of each other. In order to solve this problem, we propose a new multiview discriminant model based on conditional random fields (CRFs) to model multiview sequential data, called multiview CRF. It inherits the advantages of CRFs that build a relationship between items in each sequence. Moreover, by introducing specific features designed on the CRFs for multiview data, the multiview CRF not only considers the relationship among different views but also captures the correlation between the features from the same view. Particularly, some features can be reused or divided into different views to build an appropriate size of feature space. This helps to avoid underfitting problems caused by too small feature space or overfitting problems caused by too large feature space. In order to handle large-scale data, we use the stochastic gradient method to speed up our model. The experimental results on the text and video data illustrate the superiority of the proposed model.},
  archive      = {J_TNNLS},
  author       = {Shiliang Sun and Ziang Dong and Jing Zhao},
  doi          = {10.1109/TNNLS.2020.3041591},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1242-1253},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conditional random fields for multiview sequential data modeling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online minimax q network learning for two-player zero-sum
markov games. <em>TNNLS</em>, <em>33</em>(3), 1228–1241. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Nash equilibrium is an important concept in game theory. It describes the least exploitability of one player from any opponents. We combine game theory, dynamic programming, and recent deep reinforcement learning (DRL) techniques to online learn the Nash equilibrium policy for two-player zero-sum Markov games (TZMGs). The problem is first formulated as a Bellman minimax equation, and generalized policy iteration (GPI) provides a double-loop iterative way to find the equilibrium. Then, neural networks are introduced to approximate Q functions for large-scale problems. An online minimax Q network learning algorithm is proposed to train the network with observations. Experience replay, dueling network, and double Q-learning are applied to improve the learning process. The contributions are twofold: 1) DRL techniques are combined with GPI to find the TZMG Nash equilibrium for the first time and 2) the convergence of the online learning algorithm with a lookup table and experience replay is proven, whose proof is not only useful for TZMGs but also instructive for single-agent Markov decision problems. Experiments on different examples validate the effectiveness of the proposed algorithm on TZMG problems.},
  archive      = {J_TNNLS},
  author       = {Yuanheng Zhu and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2020.3041469},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1228-1241},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Online minimax q network learning for two-player zero-sum markov games},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent neural dynamics models for perturbed nonstationary
quadratic programs: A control-theoretical perspective. <em>TNNLS</em>,
<em>33</em>(3), 1216–1227. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent decades have witnessed a trend that control-theoretical techniques are widely leveraged in various areas, e.g., design and analysis of computational models. Computational methods can be modeled as a controller and searching the equilibrium point of a dynamical system is identical to solving an algebraic equation. Thus, absorbing mature technologies in control theory and integrating it with neural dynamics models can lead to new achievements. This work makes progress along this direction by applying control-theoretical techniques to construct new recurrent neural dynamics for manipulating a perturbed nonstationary quadratic program (QP) with time-varying parameters considered. Specifically, to break the limitations of existing continuous-time models in handling nonstationary problems, a discrete recurrent neural dynamics model is proposed to robustly deal with noise. This work shows how iterative computational methods for solving nonstationary QP can be revisited, designed, and analyzed in a control framework. A modified Newton iteration model and an improved gradient-based neural dynamics are established by referring to the superior structural technology of the presented recurrent neural dynamics, where the chief breakthrough is their excellent convergence and robustness over the traditional models. Numerical experiments are conducted to show the eminence of the proposed models in solving perturbed nonstationary QP.},
  archive      = {J_TNNLS},
  author       = {Yimeng Qi and Long Jin and Xin Luo and MengChu Zhou},
  doi          = {10.1109/TNNLS.2020.3041364},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1216-1227},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Recurrent neural dynamics models for perturbed nonstationary quadratic programs: A control-theoretical perspective},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symmetric nonnegative matrix factorization-based community
detection models and their convergence analysis. <em>TNNLS</em>,
<em>33</em>(3), 1203–1215. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is a popular yet thorny issue in social network analysis. A symmetric and nonnegative matrix factorization (SNMF) model based on a nonnegative multiplicative update (NMU) scheme is frequently adopted to address it. Current research mainly focuses on integrating additional information into it without considering the effects of a learning scheme. This study aims to implement highly accurate community detectors via the connections between an SNMF-based community detector&amp;#x2019;s detection accuracy and an NMU scheme&amp;#x2019;s scaling factor. The main idea is to adjust such scaling factor via a linear or nonlinear strategy, thereby innovatively implementing several scaling-factor-adjusted NMU schemes. They are applied to SNMF and graph-regularized SNMF models to achieve four novel SNMF-based community detectors. Theoretical studies indicate that with the proposed schemes and proper hyperparameter settings, each model can: 1) keep its loss function nonincreasing during its training process and 2) converge to a stationary point. Empirical studies on eight social networks show that they achieve significant accuracy gain in community detection over the state-of-the-art community detectors.},
  archive      = {J_TNNLS},
  author       = {Xin Luo and Zhigang Liu and Long Jin and Yue Zhou and Mengchu Zhou},
  doi          = {10.1109/TNNLS.2020.3041360},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1203-1215},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Symmetric nonnegative matrix factorization-based community detection models and their convergence analysis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal synchronization of unidirectionally coupled FO
chaotic electromechanical devices with the hierarchical neural network.
<em>TNNLS</em>, <em>33</em>(3), 1192–1202. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article solves the problem of optimal synchronization, which is important but challenging for coupled fractional-order (FO) chaotic electromechanical devices composed of mechanical and electrical oscillators and electromagnetic filed by using a hierarchical neural network structure. The synchronization model of the FO electromechanical devices with capacitive and resistive couplings is built, and the phase diagrams reveal that the dynamic properties are closely related to sets of physical parameters, coupling coefficients, and FOs. To force the slave system to move from its original orbits to the orbits of the master system, an optimal synchronization policy, which includes an adaptive neural feedforward policy and an optimal neural feedback policy, is proposed. The feedforward controller is developed in the framework of FO backstepping integrated with the hierarchical neural network to estimate unknown functions of dynamic system in which the mentioned network has the formula transformation and hierarchical form to reduce the numbers of weights and membership functions. Also, an adaptive dynamic programming (ADP) policy is proposed to address the zero-sum differential game issue in the optimal neural feedback controller in which the hierarchical neural network is designed to yield solutions of the constrained Hamilton–Jacobi–Isaacs (HJI) equation online. The presented scheme not only ensures uniform ultimate boundedness of closed-loop coupled FO chaotic electromechanical devices and realizes optimal synchronization but also achieves a minimum value of cost function. Simulation results further show the validity of the presented scheme.},
  archive      = {J_TNNLS},
  author       = {Shaohua Luo and Frank L. Lewis and Yongduan Song and Hassen M. Ouakad},
  doi          = {10.1109/TNNLS.2020.3041350},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1192-1202},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Optimal synchronization of unidirectionally coupled FO chaotic electromechanical devices with the hierarchical neural network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of backdoors in trained classifiers without access
to the training set. <em>TNNLS</em>, <em>33</em>(3), 1177–1191. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With wide deployment of deep neural network (DNN) classifiers, there is great potential for harm from adversarial learning attacks. Recently, a special type of data poisoning (DP) attack, known as a backdoor (or Trojan), was proposed. These attacks do not seek to degrade classification accuracy, but rather to have the classifier learn to classify to a target class $t^{\ast }$ whenever the backdoor pattern is present in a test example originally from a source class $s^{\ast }$ . Launching backdoor attacks does not require knowledge of the classifier or its training process—only the ability to poison the training set with exemplars containing a backdoor pattern (labeled with the target class). Defenses against backdoors can be deployed before/during training, post-training, or at test time. Here, we address post-training detection in DNN image classifiers, seldom considered in existing works, wherein the defender does not have access to the poisoned training set , but only to the trained classifier itself, as well as to clean (unpoisoned) examples from the classification domain. This scenario is of great interest because e.g., a classifier may be the basis of a phone app that will be shared with many users. Detection may thus reveal a widespread attack. We propose a purely unsupervised anomaly detection (AD) defense against imperceptible backdoor attacks that: 1) detects whether the trained DNN has been backdoor-attacked; 2) infers the source and target classes in a detected attack; 3) estimates the backdoor pattern itself. Our AD approach involves learning (via suitable cost function minimization) the minimum size/norm perturbation (putative backdoor) required to induce the classifier to misclassify (most) examples from class $s$ to class $t$ , for all $(s,t)$ pairs. Our hypothesis is that nonattacked pairs require large perturbations, while the attacked pair $(s^{\ast }, t^{\ast })$ requires much smaller ones. This is convincingly borne out experimentally. We identify a variety of plausible cost functions and devise a novel, robust hypothesis testing approach to perform detection inference. We test our approach, in comparison with the state-of-the-art methods, for several backdoor patterns, attack settings and mechanisms, and data sets and demonstrate its favorability. Our defense essentially requires setting a single hyperparameter (the detection threshold), which can e.g., be chosen to fix the system’s false positive rate.},
  archive      = {J_TNNLS},
  author       = {Zhen Xiang and David J. Miller and George Kesidis},
  doi          = {10.1109/TNNLS.2020.3041202},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1177-1191},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Detection of backdoors in trained classifiers without access to the training set},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ternary compression for communication-efficient federated
learning. <em>TNNLS</em>, <em>33</em>(3), 1162–1176. (<a
href="https://doi.org/10.1109/TNNLS.2020.3041185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning over massive data stored in different locations is essential in many real-world applications. However, sharing data is full of challenges due to the increasing demands of privacy and security with the growing use of smart mobile devices and Internet of thing (IoT) devices. Federated learning provides a potential solution to privacy-preserving and secure machine learning, by means of jointly training a global model without uploading data distributed on multiple devices to a central server. However, most existing work on federated learning adopts machine learning models with full-precision weights, and almost all these models contain a large number of redundant parameters that do not need to be transmitted to the server, consuming an excessive amount of communication costs. To address this issue, we propose a federated trained ternary quantization (FTTQ) algorithm, which optimizes the quantized networks on the clients through a self-learning quantization factor. Theoretical proofs of the convergence of quantization factors, unbiasedness of FTTQ, as well as a reduced weight divergence are given. On the basis of FTTQ, we propose a ternary federated averaging protocol (T-FedAvg) to reduce the upstream and downstream communication of federated learning systems. Empirical experiments are conducted to train widely used deep learning models on publicly available data sets, and our results demonstrate that the proposed T-FedAvg is effective in reducing communication costs and can even achieve slightly better performance on non-IID data in contrast to the canonical federated learning algorithms.},
  archive      = {J_TNNLS},
  author       = {Jinjin Xu and Wenli Du and Yaochu Jin and Wangli He and Ran Cheng},
  doi          = {10.1109/TNNLS.2020.3041185},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1162-1176},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Ternary compression for communication-efficient federated learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pairwise two-stream ConvNets for cross-domain action
recognition with small data. <em>TNNLS</em>, <em>33</em>(3), 1147–1161.
(<a href="https://doi.org/10.1109/TNNLS.2020.3041018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we target cross-domain action recognition (CDAR) in the video domain and propose a novel end-to-end pairwise two-stream ConvNets ( PTC ) algorithm for real-life conditions, in which only a few labeled samples are available. To cope with the limited training sample problem, we employ pairwise network architecture that can leverage training samples from a source domain and, thus, requires only a few labeled samples per category from the target domain. In particular, a frame self-attention mechanism and an adaptive weight scheme are embedded into the PTC network to adaptively combine the RGB and flow features. This design can effectively learn domain-invariant features for both the source and target domains. In addition, we propose a sphere boundary sample-selecting scheme that selects the training samples at the boundary of a class (in the feature space) to train the PTC model. In this way, a well-enhanced generalization capability can be achieved. To validate the effectiveness of our PTC model, we construct two CDAR data sets ( SDAI Action I and SDAI Action II ) that include indoor and outdoor environments; all actions and samples in these data sets were carefully collected from public action data sets. To the best of our knowledge, these are the first data sets specifically designed for the CDAR task. Extensive experiments were conducted on these two data sets. The results show that PTC outperforms state-of-the-art video action recognition methods in terms of both accuracy and training efficiency. It is noteworthy that when only two labeled training samples per category are used in the SDAI Action I data set, PTC achieves 21.9\% and 6.8\% improvement in accuracy over two-stream and temporal segment networks models, respectively. As an added contribution, the SDAI Action I and SDAI Action II data sets will be released to facilitate future research on the CDAR task.},
  archive      = {J_TNNLS},
  author       = {Zan Gao and Leming Guo and Tongwei Ren and An-An Liu and Zhi-Yong Cheng and Shengyong Chen},
  doi          = {10.1109/TNNLS.2020.3041018},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1147-1161},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Pairwise two-stream ConvNets for cross-domain action recognition with small data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synaptic learning with augmented spikes. <em>TNNLS</em>,
<em>33</em>(3), 1134–1146. (<a
href="https://doi.org/10.1109/TNNLS.2020.3040969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional neuron models use analog values for information representation and computation, while all-or-nothing spikes are employed in the spiking ones. With a more brain-like processing paradigm, spiking neurons are more promising for improvements in efficiency and computational capability. They extend the computation of traditional neurons with an additional dimension of time carried by all-or-nothing spikes. Could one benefit from both the accuracy of analog values and the time-processing capability of spikes? In this article, we introduce a concept of augmented spikes to carry complementary information with spike coefficients in addition to spike latencies. New augmented spiking neuron model and synaptic learning rules are proposed to process and learn patterns of augmented spikes. We provide systematic insights into the properties and characteristics of our methods, including classification of augmented spike patterns, learning capacity, construction of causality, feature detection, robustness, and applicability to practical tasks, such as acoustic and visual pattern recognition. Our augmented approaches show several advanced learning properties and reliably outperform the baseline ones that use typical all-or-nothing spikes. Our approaches significantly improve the accuracies of a temporal-based approach on sound and MNIST recognition tasks to 99.38\% and 97.90\%, respectively, highlighting the effectiveness and potential merits of our methods. More importantly, our augmented approaches are versatile and can be easily generalized to other spike-based systems, contributing to a potential development for them, including neuromorphic computing.},
  archive      = {J_TNNLS},
  author       = {Qiang Yu and Shiming Song and Chenxiang Ma and Linqiang Pan and Kay Chen Tan},
  doi          = {10.1109/TNNLS.2020.3040969},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1134-1146},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Synaptic learning with augmented spikes},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Learnable subspace clustering. <em>TNNLS</em>,
<em>33</em>(3), 1119–1133. (<a
href="https://doi.org/10.1109/TNNLS.2020.3040379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the large-scale subspace clustering (LS 2 C) problem with millions of data points. Many popular subspace clustering methods cannot directly handle the LS 2 C problem although they have been considered to be state-of-the-art methods for small-scale data points. A simple reason is that these methods often choose all data points as a large dictionary to build huge coding models, which results in high time and space complexity. In this article, we develop a learnable subspace clustering paradigm to efficiently solve the LS 2 C problem. The key concept is to learn a parametric function to partition the high-dimensional subspaces into their underlying low-dimensional subspaces instead of the computationally demanding classical coding models. Moreover, we propose a unified, robust, predictive coding machine (RPCM) to learn the parametric function, which can be solved by an alternating minimization algorithm. Besides, we provide a bounded contraction analysis of the parametric function. To the best of our knowledge, this article is the first work to efficiently cluster millions of data points among the subspace clustering methods. Experiments on million-scale data sets verify that our paradigm outperforms the related state-of-the-art methods in both efficiency and effectiveness.},
  archive      = {J_TNNLS},
  author       = {Jun Li and Hongfu Liu and Zhiqiang Tao and Handong Zhao and Yun Fu},
  doi          = {10.1109/TNNLS.2020.3040379},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1119-1133},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learnable subspace clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Momentum acceleration in the individual convergence of
nonsmooth convex optimization with constraints. <em>TNNLS</em>,
<em>33</em>(3), 1107–1118. (<a
href="https://doi.org/10.1109/TNNLS.2020.3040325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Momentum technique has recently emerged as an effective strategy in accelerating convergence of gradient descent (GD) methods and exhibits improved performance in deep learning as well as regularized learning. Typical momentum examples include Nesterov’s accelerated gradient (NAG) and heavy-ball (HB) methods. However, so far, almost all the acceleration analyses are only limited to NAG, and a few investigations about the acceleration of HB are reported. In this article, we address the convergence about the last iterate of HB in nonsmooth optimizations with constraints, which we name individual convergence. This question is significant in machine learning, where the constraints are required to impose on the learning structure and the individual output is needed to effectively guarantee this structure while keeping an optimal rate of convergence. Specifically, we prove that HB achieves an individual convergence rate of $O({1}/{\sqrt {t}})$ , where $t$ is the number of iterations. This indicates that both of the two momentum methods can accelerate the individual convergence of basic GD to be optimal. Even for the convergence of averaged iterates, our result avoids the disadvantages of the previous work in restricting the optimization problem to be unconstrained as well as limiting the performed number of iterations to be predefined. The novelty of convergence analysis presented in this article provides a clear understanding of how the HB momentum can accelerate the individual convergence and reveals more insights about the similarities and differences in getting the averaging and individual convergence rates. The derived optimal individual convergence is extended to regularized and stochastic settings, in which an individual solution can be produced by the projection-based operation. In contrast to the averaged output, the sparsity can be reduced remarkably without sacrificing the theoretical optimal rates. Several real experiments demonstrate the performance of HB momentum strategy.},
  archive      = {J_TNNLS},
  author       = {Wei Tao and Gao-Wei Wu and Qing Tao},
  doi          = {10.1109/TNNLS.2020.3040325},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1107-1118},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Momentum acceleration in the individual convergence of nonsmooth convex optimization with constraints},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A scalable algorithm for identifying multiple-sensor faults
using disentangled RNNs. <em>TNNLS</em>, <em>33</em>(3), 1093–1106. (<a
href="https://doi.org/10.1109/TNNLS.2020.3040224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of detecting and identifying sensor faults is critical for efficient, safe, regulatory-compliant, and sustainable operations of modern industrial processing systems. The increasing complexity of such systems brings, however, new challenges for sensor fault detection and sensor fault isolation (SFD-SFI). One of the key enablers for any SFD-SFI method is analytical redundancy, which is provided by an analytical model of sensor observations derived from first principles or identified from historical data. As defective sensors generate measurements that are inconsistent with their expected behavior as defined by the model, SFD amounts to the generation and monitoring of residuals between sensor observations and model predictions. In this article, we introduce a disentangled recurrent neural network (RNN) with the objective to cope with the smearing-out effect, i.e., where the propagation of a sensor fault to nonfaulty sensor results in large and misleading residuals. The introduction of a probabilistic model for the residual generation allows us to develop a novel procedure for the identification of the faulty sensors. The computational complexity of the proposed algorithm is linear in the number of sensors as opposed to the combinatorial nature of the SFI problem. Finally, we empirically verify the performance of the proposed SFD-SFI architecture using a real data set collected at a petrochemical plant.},
  archive      = {J_TNNLS},
  author       = {David Haldimann and Marco Guerriero and Yannick Maret and Nunzio Bonavita and Gregorio Ciarlo and Marta Sabbadin},
  doi          = {10.1109/TNNLS.2020.3040224},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1093-1106},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A scalable algorithm for identifying multiple-sensor faults using disentangled RNNs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly detection with bidirectional consistency in videos.
<em>TNNLS</em>, <em>33</em>(3), 1079–1092. (<a
href="https://doi.org/10.1109/TNNLS.2020.3039899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core component of most anomaly detectors is a self-supervised model, tasked with modeling patterns included in training samples and detecting unexpected patterns as the anomalies in testing samples. To cope with normal patterns, this model is typically trained with reconstruction constraints. However, the model has the risk of overfitting to training samples and being sensitive to hard normal patterns in the inference phase, which results in irregular responses at normal frames. To address this problem, we formulate anomaly detection as a mutual supervision problem. Due to collaborative training, the complementary information of mutual learning can alleviate the aforementioned problem. Based on this motivation, a SIamese generative network (SIGnet), including two subnetworks with the same architecture, is proposed to simultaneously model the patterns of the forward and backward frames. During training, in addition to traditional constraints on improving the reconstruction performance, a bidirectional consistency loss based on the forward and backward views is designed as the regularization term to improve the generalization ability of the model. Moreover, we introduce a consistency-based evaluation criterion to achieve stable scores at the normal frames, which will benefit detecting anomalies with fluctuant scores in the inference phase. The results on several challenging benchmark data sets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TNNLS},
  author       = {Zhiwen Fang and Jiafei Liang and Joey Tianyi Zhou and Yang Xiao and Feng Yang},
  doi          = {10.1109/TNNLS.2020.3039899},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1079-1092},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anomaly detection with bidirectional consistency in videos},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multitask attention network for lane detection and fitting.
<em>TNNLS</em>, <em>33</em>(3), 1066–1078. (<a
href="https://doi.org/10.1109/TNNLS.2020.3039675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many CNN-based segmentation methods have been applied in lane marking detection recently and gain excellent success for a strong ability in modeling semantic information. Although the accuracy of lane line prediction is getting better and better, lane markings’ localization ability is relatively weak, especially when the lane marking point is remote. Traditional lane detection methods usually utilize highly specialized handcrafted features and carefully designed postprocessing to detect the lanes. However, these methods are based on strong assumptions and, thus, are prone to scalability. In this work, we propose a novel multitask method that: 1) integrates the ability to model semantic information of CNN and the strong localization ability provided by handcrafted features and 2) predicts the position of vanishing line. A novel lane fitting method based on vanishing line prediction is also proposed for sharp curves and nonflat road in this article. By integrating segmentation, specialized handcrafted features, and fitting, the accuracy of location and the convergence speed of networks are improved. Extensive experimental results on four-lane marking detection data sets show that our method achieves state-of-the-art performance.},
  archive      = {J_TNNLS},
  author       = {Qi Wang and Tao Han and Zequn Qin and Junyu Gao and Xuelong Li},
  doi          = {10.1109/TNNLS.2020.3039675},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1066-1078},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask attention network for lane detection and fitting},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMGEA: A new ensemble adversarial attack powered by
long-term gradient memories. <em>TNNLS</em>, <em>33</em>(3), 1051–1065.
(<a href="https://doi.org/10.1109/TNNLS.2020.3039295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are vulnerable to adversarial attacks. More importantly, some adversarial examples crafted against an ensemble of source models transfer to other target models and, thus, pose a security threat to black-box applications (when attackers have no access to the target models). Current transfer-based ensemble attacks, however, only consider a limited number of source models to craft an adversarial example and, thus, obtain poor transferability. Besides, recent query-based black-box attacks, which require numerous queries to the target model, not only come under suspicion by the target model but also cause expensive query cost. In this article, we propose a novel transfer-based black-box attack, dubbed serial-minigroup-ensemble-attack (SMGEA). Concretely, SMGEA first divides a large number of pretrained white-box source models into several “minigroups.” For each minigroup, we design three new ensemble strategies to improve the intragroup transferability. Moreover, we propose a new algorithm that recursively accumulates the “long-term” gradient memories of the previous minigroup to the subsequent minigroup. This way, the learned adversarial information can be preserved, and the intergroup transferability can be improved. Experiments indicate that SMGEA not only achieves state-of-the-art black-box attack ability over several data sets but also deceives two online black-box saliency prediction systems in real world, i.e., DeepGaze-II ( https://deepgaze.bethgelab.org/ ) and SALICON ( http://salicon.net/demo/ ). Finally, we contribute a new code repository to promote research on adversarial attack and defense over ubiquitous pixel-to-pixel computer vision tasks. We share our code together with the pretrained substitute model zoo at https://github.com/CZHQuality/AAA-Pix2pix .},
  archive      = {J_TNNLS},
  author       = {Zhaohui Che and Ali Borji and Guangtao Zhai and Suiyi Ling and Jing Li and Xiongkuo Min and Guodong Guo and Patrick Le Callet},
  doi          = {10.1109/TNNLS.2020.3039295},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1051-1065},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SMGEA: A new ensemble adversarial attack powered by long-term gradient memories},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prior-based tensor approximation for anomaly detection in
hyperspectral imagery. <em>TNNLS</em>, <em>33</em>(3), 1037–1050. (<a
href="https://doi.org/10.1109/TNNLS.2020.3038659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to hyperspectral anomaly detection is to effectively distinguish anomalies from the background, especially in the case that background is complex and anomalies are weak. Hyperspectral imagery (HSI) as an image–spectrum merging cube data can be intrinsically represented as a third-order tensor that integrates spectral information and spatial information. In this article, a prior-based tensor approximation (PTA) is proposed for hyperspectral anomaly detection, in which HSI is decomposed into a background tensor and an anomaly tensor. In the background tensor, a low-rank prior is incorporated into spectral dimension by truncated nuclear norm regularization, and a piecewise-smooth prior on spatial dimension can be embedded by a linear total variation-norm regularization. For anomaly tensor, it is unfolded along spectral dimension coupled with spatial group sparse prior that can be represented by the ${l}_{2,1}$ -norm regularization. In the designed method, all the priors are integrated into a unified convex framework, and the anomalies can be finally determined by the anomaly tensor. Experimental results validated on several real hyperspectral data sets demonstrate that the proposed algorithm outperforms some state-of-the-art anomaly detection methods.},
  archive      = {J_TNNLS},
  author       = {Lu Li and Wei Li and Ying Qu and Chunhui Zhao and Ran Tao and Qian Du},
  doi          = {10.1109/TNNLS.2020.3038659},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1037-1050},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prior-based tensor approximation for anomaly detection in hyperspectral imagery},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manifold modeling in embedded space: An interpretable
alternative to deep image prior. <em>TNNLS</em>, <em>33</em>(3),
1022–1036. (<a
href="https://doi.org/10.1109/TNNLS.2020.3037923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep image prior (DIP), which uses a deep convolutional network (ConvNet) structure as an image prior, has attracted wide attention in computer vision and machine learning. DIP empirically shows the effectiveness of the ConvNet structures for various image restoration applications. However, why the DIP works so well is still unknown. In addition, the reason why the convolution operation is useful in image reconstruction, or image enhancement is not very clear. This study tackles this ambiguity of ConvNet/DIP by proposing an interpretable approach that divides the convolution into “delay embedding” and “transformation” (i.e., encoder–decoder). Our approach is a simple, but essential, image/tensor modeling method that is closely related to self-similarity. The proposed method is called manifold modeling in embedded space (MMES) since it is implemented using a denoising autoencoder in combination with a multiway delay-embedding transform. In spite of its simplicity, MMES can obtain quite similar results to DIP on image/tensor completion, super-resolution, deconvolution, and denoising. In addition, MMES is proven to be competitive with DIP, as shown in our experiments. These results can also facilitate interpretation/characterization of DIP from the perspective of a “low-dimensional patch-manifold prior.”},
  archive      = {J_TNNLS},
  author       = {Tatsuya Yokota and Hidekata Hontani and Qibin Zhao and Andrzej Cichocki},
  doi          = {10.1109/TNNLS.2020.3037923},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1022-1036},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Manifold modeling in embedded space: An interpretable alternative to deep image prior},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Composite-learning-based adaptive neural control for
dual-arm robots with relative motion. <em>TNNLS</em>, <em>33</em>(3),
1010–1021. (<a
href="https://doi.org/10.1109/TNNLS.2020.3037795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an adaptive control method for dual-arm robot systems to perform bimanual tasks under modeling uncertainties. Different from the traditional symmetric bimanual robot control, we study the dual-arm robot control with relative motions between robotic arms and a grasped object. The robot system is first divided into two subsystems: a settled manipulator system and a tool-used manipulator system. Then, a command filtered control technique is developed for trajectory tracking and contact force control. In addition, to deal with the inevitable dynamic uncertainties, a radial basis function neural network (RBFNN) is employed for the robot, with a novel composite learning law to update the NN weights. The composite learning is mainly based on an integration of the historic data of NN regression such that information of the estimate error can be utilized to improve the convergence. Moreover, a partial persistent excitation condition is employed to ensure estimation convergence. The stability analysis is performed by using the Lyapunov theorem. Numerical simulation results demonstrate the validity of the proposed control and learning algorithm.},
  archive      = {J_TNNLS},
  author       = {Yiming Jiang and Yaonan Wang and Zhiqiang Miao and Jing Na and Zhijia Zhao and Chenguang Yang},
  doi          = {10.1109/TNNLS.2020.3037795},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {1010-1021},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Composite-learning-based adaptive neural control for dual-arm robots with relative motion},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse ℓ1- and ℓ2-center classifiers. <em>TNNLS</em>,
<em>33</em>(3), 996–1009. (<a
href="https://doi.org/10.1109/TNNLS.2020.3036838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss two novel sparse versions of the classical nearest-centroid classifier. The proposed sparse classifiers are based on $\ell _{1}$ and $\ell _{2}$ distance criteria, respectively, and perform simultaneous feature selection and classification, by detecting the features that are most relevant for the classification purpose. We formally prove that the training of the proposed sparse models, with both distance criteria, can be performed exactly (i.e., the globally optimal set of features is selected) at a linear computational cost. Especially, the proposed sparse classifiers are trained in $O(mn)+O(m\log k)$ operations, where $n$ is the number of samples, $m$ is the total number of features, and $k \leq m$ is the number of features to be retained in the classifier. Furthermore, the complexity of testing and classifying a new sample is simply $O(k)$ for both methods. The proposed models can be employed either as stand-alone sparse classifiers or fast feature-selection techniques for prefiltering the features to be later fed to other types of classifiers (e.g., SVMs). The experimental results show that the proposed methods are competitive in accuracy with state-of-the-art feature selection and classification techniques while having a substantially lower computational cost.},
  archive      = {J_TNNLS},
  author       = {Giuseppe C. Calafiore and Giulia Fracastoro},
  doi          = {10.1109/TNNLS.2020.3036838},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {996-1009},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse ℓ1- and ℓ2-center classifiers},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multitask representation learning with multiview graph
convolutional networks. <em>TNNLS</em>, <em>33</em>(3), 983–995. (<a
href="https://doi.org/10.1109/TNNLS.2020.3036825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction and node classification are two important downstream tasks of network representation learning. Existing methods have achieved acceptable results but they perform these two tasks separately, which requires a lot of duplication of work and ignores the correlations between tasks. Besides, conventional models suffer from the identical treatment of information of multiple views, thus they fail to learn robust representation for downstream tasks. To this end, we tackle link prediction and node classification problems simultaneously via multitask multiview learning in this article. We first explain the feasibility and advantages of multitask multiview learning for these two tasks. Then we propose a novel model named MT-MVGCN to perform link prediction and node classification tasks simultaneously. More specifically, we design a multiview graph convolutional network to extract abundant information of multiple views in a network, which is shared by different tasks. We further apply two attention mechanisms: view the attention mechanism and task attention mechanism to make views and tasks adjust the view fusion process. Moreover, view reconstruction can be introduced as an auxiliary task to boost the performance of the proposed model. Experiments on real-world network data sets demonstrate that our model is efficient yet effective, and outperforms advanced baselines in these two tasks.},
  archive      = {J_TNNLS},
  author       = {Hong Huang and Yu Song and Yao Wu and Jia Shi and Xia Xie and Hai Jin},
  doi          = {10.1109/TNNLS.2020.3036825},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {983-995},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multitask representation learning with multiview graph convolutional networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic embedding projection-gated convolutional neural
networks for text classification. <em>TNNLS</em>, <em>33</em>(3),
973–982. (<a href="https://doi.org/10.1109/TNNLS.2020.3036192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is a fundamental and important area of natural language processing for assigning a text into at least one predefined tag or category according to its content. Most of the advanced systems are either too simple to get high accuracy or centered on using complex structures to capture the genuinely required category information, which requires long time to converge during their training stage. In order to address such challenging issues, we propose a dynamic embedding projection-gated convolutional neural network (DEP-CNN) for multi-class and multi-label text classification. Its dynamic embedding projection gate (DEPG) transforms and carries word information by using gating units and shortcut connections to control how much context information is incorporated into each specific position of a word-embedding matrix in a text. To our knowledge, we are the first to apply DEPG over a word-embedding matrix. The experimental results on four known benchmark datasets display that DEP-CNN outperforms its recent peers.},
  archive      = {J_TNNLS},
  author       = {Zhipeng Tan and Jing Chen and Qi Kang and Mengchu Zhou and Abdullah Abusorrah and Khaled Sedraoui},
  doi          = {10.1109/TNNLS.2020.3036192},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {973-982},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic embedding projection-gated convolutional neural networks for text classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Output feedback control of micromechanical gyroscopes using
neural networks and disturbance observer. <em>TNNLS</em>,
<em>33</em>(3), 962–972. (<a
href="https://doi.org/10.1109/TNNLS.2020.3030712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the output feedback control of micromechanical (MEMS) gyroscopes using neural networks (NNs) and disturbance observer (DOB). For the unmeasured system states, the state observer and the high gain observer are constructed. The adaptive NNs are investigated to approximate the nonlinear dynamics, including the known nominal terms and the system uncertainties caused by environmental fluctuations. For the time-varying disturbances, the DOB is utilized. The sliding mode control is employed to enhance the robustness. Through simulation verification, the output feedback control using NNs and DOB can adapt to the dynamics of MEMS gyroscope with unmeasured system speed, while an expected effective tracking performance is obtained in the presence of unknown system nonlinearities and external disturbances.},
  archive      = {J_TNNLS},
  author       = {Rui Zhang and Bin Xu and Peng Shi},
  doi          = {10.1109/TNNLS.2020.3030712},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {962-972},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Output feedback control of micromechanical gyroscopes using neural networks and disturbance observer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered output feedback synchronization of
master–slave neural networks under deception attacks. <em>TNNLS</em>,
<em>33</em>(3), 952–961. (<a
href="https://doi.org/10.1109/TNNLS.2020.3030638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of event-triggered synchronization of master–slave neural networks is investigated in this article. It is assumed that both communication channels from the sensor to controller and from controller to actuator are subject to stochastic deception attacks modeled by two independent Markov processes. Two discrete event-triggered mechanisms are introduced for both channels to reduce the number of data transmission through the communication channels. To comply with practical point of view, static output feedback is utilized. By employing the Lyapunov–Krasovskii functional method, some sufficient conditions on the synchronization of master–slave neural networks are derived in terms of linear matrix inequalities, which make it easy to design suitable output feedback controllers. Finally, a numerical example is presented to show the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Ali Kazemy and James Lam and Xian-Ming Zhang},
  doi          = {10.1109/TNNLS.2020.3030638},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {952-961},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Event-triggered output feedback synchronization of Master–Slave neural networks under deception attacks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SurReal: Complex-valued learning as principled
transformations on a scaling and rotation manifold. <em>TNNLS</em>,
<em>33</em>(3), 940–951. (<a
href="https://doi.org/10.1109/TNNLS.2020.3030565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex-valued data are ubiquitous in signal and image processing applications, and complex-valued representations in deep learning have appealing theoretical properties. While these aspects have long been recognized, complex-valued deep learning continues to lag far behind its real-valued counterpart. We propose a principled geometric approach to complex-valued deep learning. Complex-valued data could often be subject to arbitrary complex-valued scaling; as a result, real and imaginary components could covary. Instead of treating complex values as two independent channels of real values, we recognize their underlying geometry: we model the space of complex numbers as a product manifold of nonzero scaling and planar rotations. Arbitrary complex-valued scaling naturally becomes a group of transitive actions on this manifold. We propose to extend the property instead of the form of real-valued functions to the complex domain. We define convolution as the weighted Fréchet mean on the manifold that is equivariant to the group of scaling/rotation actions and define distance transform on the manifold that is invariant to the action group. The manifold perspective also allows us to define nonlinear activation functions, such as tangent ReLU and $G$ -transport, as well as residual connections on the manifold-valued data. We dub our model SurReal, as our experiments on MSTAR and RadioML deliver high performance with only a fractional size of real- and complex-valued baseline models.},
  archive      = {J_TNNLS},
  author       = {Rudrasis Chakraborty and Yifei Xing and Stella X. Yu},
  doi          = {10.1109/TNNLS.2020.3030565},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {940-951},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {SurReal: Complex-valued learning as principled transformations on a scaling and rotation manifold},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep echo state q-network (DEQN) and its application in
dynamic spectrum sharing for 5G and beyond. <em>TNNLS</em>,
<em>33</em>(3), 929–939. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) has been shown to be successful in many application domains. Combining recurrent neural networks (RNNs) and DRL further enables DRL to be applicable in non-Markovian environments by capturing temporal information. However, training of both DRL and RNNs is known to be challenging requiring a large amount of training data to achieve convergence. In many targeted applications, such as those used in the fifth-generation (5G) cellular communication, the environment is highly dynamic, while the available training data is very limited. Therefore, it is extremely important to develop DRL strategies that are capable of capturing the temporal correlation of the dynamic environment requiring limited training overhead. In this article, we introduce the deep echo state Q-network (DEQN) that can adapt to the highly dynamic environment in a short period of time with limited training data. We evaluate the performance of the introduced DEQN method under the dynamic spectrum sharing (DSS) scenario, which is a promising technology in 5G and future 6G networks to increase the spectrum utilization. Compared with conventional spectrum management policy that grants a fixed spectrum band to a single system for exclusive access, DSS allows the secondary system to share the spectrum with the primary system. Our work sheds light on the application of an efficient DRL framework in highly dynamic environments with limited available training data.},
  archive      = {J_TNNLS},
  author       = {Hao-Hsuan Chang and Lingjia Liu and Yang Yi},
  doi          = {10.1109/TNNLS.2020.3029711},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {929-939},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep echo state Q-network (DEQN) and its application in dynamic spectrum sharing for 5G and beyond},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward a reinforcement learning environment toolbox for
intelligent electric motor control. <em>TNNLS</em>, <em>33</em>(3),
919–928. (<a href="https://doi.org/10.1109/TNNLS.2020.3029573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric motors are used in many applications, and their efficiency is strongly dependent on their control. Among others, linear feedback approaches or model predictive control methods are well known in the scientific literature and industrial practice. A novel approach is to use reinforcement learning (RL) to have an agent learn electric drive control from scratch merely by interacting with a suitable control environment. RL achieved remarkable results with superhuman performance in many games (e.g., Atari classics or Go) and also becomes more popular in control tasks, such as cart-pole or swinging pendulum benchmarks. In this work, the open-source Python package gym-electric-motor (GEM) is developed for ease of training of RL-agents for electric motor control. Furthermore, this package can be used to compare the trained agents with other state-of-the-art control approaches. It is based on the OpenAI Gym framework that provides a widely used interface for the evaluation of RL-agents. The package covers different dc and three-phase motor variants, as well as different power electronic converters and mechanical load models. Due to the modular setup of the proposed toolbox, additional motor, load, and power electronic devices can be easily extended in the future. Furthermore, different secondary effects, such as converter interlocking time or noise, are considered. An intelligent controller example based on the deep deterministic policy gradient algorithm that controls a series dc motor is presented and compared to a cascaded proportional–integral controller as a baseline for future research. Here, safety requirements are particularly highlighted as an important constraint for data-driven control algorithms applied to electric energy systems. Fellow researchers are encouraged to use the GEM framework in their RL investigations or contribute to the functional scope (e.g., further motor types) of the package.},
  archive      = {J_TNNLS},
  author       = {Arne Traue and Gerrit Book and Wilhelm Kirchgässner and Oliver Wallscheid},
  doi          = {10.1109/TNNLS.2020.3029573},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {919-928},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Toward a reinforcement learning environment toolbox for intelligent electric motor control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Supervised learning achieves human-level performance in MOBA
games: A case study of honor of kings. <em>TNNLS</em>, <em>33</em>(3),
908–918. (<a href="https://doi.org/10.1109/TNNLS.2020.3029475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present JueWu-SL, the first supervised-learning-based artificial intelligence (AI) program that achieves human-level performance in playing multiplayer online battle arena (MOBA) games. Unlike prior attempts, we integrate the macro-strategy and the micromanagement of MOBA-game-playing into neural networks in a supervised and end-to-end manner. Tested on Honor of Kings, the most popular MOBA at present, our AI performs competitively at the level of High King players in standard 5v5 games.},
  archive      = {J_TNNLS},
  author       = {Deheng Ye and Guibin Chen and Peilin Zhao and Fuhao Qiu and Bo Yuan and Wen Zhang and Sheng Chen and Mingfei Sun and Xiaoqian Li and Siqin Li and Jing Liang and Zhenjie Lian and Bei Shi and Liang Wang and Tengfei Shi and Qiang Fu and Wei Yang and Lanxiao Huang},
  doi          = {10.1109/TNNLS.2020.3029475},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {3},
  pages        = {908-918},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Supervised learning achieves human-level performance in MOBA games: A case study of honor of kings},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>33</em>(2), C3. (<a
href="https://doi.org/10.1109/TNNLS.2022.3142460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2022.3142460},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge distillation for face photo–sketch synthesis.
<em>TNNLS</em>, <em>33</em>(2), 893–906. (<a
href="https://doi.org/10.1109/TNNLS.2020.3030536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made with face photo–sketch synthesis in recent years due to the development of deep convolutional neural networks, particularly generative adversarial networks (GANs). However, the performance of existing methods is still limited because of the lack of training data (photo–sketch pairs). To address this challenge, we investigate the effect of knowledge distillation (KD) on training neural networks for the face photo–sketch synthesis task and propose an effective KD model to improve the performance of synthetic images. In particular, we utilize a teacher network trained on a large amount of data in a related task to separately learn knowledge of the face photo and knowledge of the face sketch and simultaneously transfer this knowledge to two student networks designed for the face photo–sketch synthesis task. In addition to assimilating the knowledge from the teacher network, the two student networks can mutually transfer their own knowledge to further enhance their learning. To further enhance the perception quality of the synthetic image, we propose a KD+ model that combines GANs with KD. The generator can produce images with more realistic textures and less noise under the guide of knowledge. Extensive experiments and a user study demonstrate the superiority of our models over the state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Mingrui Zhu and Jie Li and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2020.3030536},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {893-906},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge distillation for face Photo–Sketch synthesis},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-free adaptive optimal control for unknown nonlinear
multiplayer nonzero-sum game. <em>TNNLS</em>, <em>33</em>(2), 879–892.
(<a href="https://doi.org/10.1109/TNNLS.2020.3030127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an online adaptive optimal control algorithm based on adaptive dynamic programming is developed to solve the multiplayer nonzero-sum game (MP-NZSG) for discrete-time unknown nonlinear systems. First, a model-free coupled globalized dual-heuristic dynamic programming (GDHP) structure is designed to solve the MP-NZSG problem, in which there is no model network or identifier. Second, in order to relax the requirement of systems dynamics, an online adaptive learning algorithm is developed to solve the Hamilton–Jacobi equation using the system states of two adjacent time steps. Third, a series of critic networks and action networks are used to approximate value functions and optimal policies for all players. All the neural network (NN) weights are updated online based on real-time system states. Fourth, the uniformly ultimate boundedness analysis of the NN approximation errors is proved based on the Lyapunov approach. Finally, simulation results are given to demonstrate the effectiveness of the developed scheme.},
  archive      = {J_TNNLS},
  author       = {Qinglai Wei and Liao Zhu and Ruizhuo Song and Pinjia Zhang and Derong Liu and Jun Xiao},
  doi          = {10.1109/TNNLS.2020.3030127},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {879-892},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Model-free adaptive optimal control for unknown nonlinear multiplayer nonzero-sum game},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight encoder–decoder path for deep residual
networks. <em>TNNLS</em>, <em>33</em>(2), 866–878. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel lightweight path for deep residual neural networks. The proposed method integrates a simple plug-and-play module, i.e., a convolutional encoder–decoder (ED), as an augmented path to the original residual building block. Due to the abstract design and ability of the encoding stage, the decoder part tends to generate feature maps where highly semantically relevant responses are activated, while irrelevant responses are restrained. By a simple elementwise addition operation, the learned representations derived from the identity shortcut and original transformation branch are enhanced by our ED path. Furthermore, we exploit lightweight counterparts by removing a portion of channels in the original transformation branch. Fortunately, our lightweight processing does not cause an obvious performance drop but brings a computational economy. By conducting comprehensive experiments on ImageNet, MS-COCO, CUB200-2011, and CIFAR, we demonstrate the consistent accuracy gain obtained by our ED path for various residual architectures, with comparable or even lower model complexity. Concretely, it decreases the top-1 error of ResNet-50 and ResNet-101 by 1.22\% and 0.91\% on the task of ImageNet classification and increases the mmAP of Faster R-CNN with ResNet-101 by 2.5\% on the MS-COCO object detection task. The code is available at https://github.com/Megvii-Nanjing/ED-Net .},
  archive      = {J_TNNLS},
  author       = {Xin Jin and Yanping Xie and Xiu-Shen Wei and Bo-Rui Zhao and Yongshun Zhang and Xiaoyang Tan and Yang Yu},
  doi          = {10.1109/TNNLS.2020.3029613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {866-878},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A lightweight Encoder–Decoder path for deep residual networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive tracking control for perturbed strict-feedback
nonlinear systems based on optimized backstepping technique.
<em>TNNLS</em>, <em>33</em>(2), 853–865. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an adaptive optimized control scheme based on neural networks (NNs) is developed for a class of perturbed strict-feedback nonlinear systems. An optimized backstepping (OB) technique is employed for breaking through the limitation of the matching condition. The disturbance of existing nonlinear systems may degrade system performance or even lead to instability. In order to improve the system’s robustness, a disturbance observer is constructed to compensate for the impact coming from the external disturbance. Because the proposed optimized scheme needs to train the adaptive parameters not only for reinforcement learning (RL) but also for the disturbance observer, it will become more challenging no matter designing the control algorithm or deriving the adaptive updating laws. Finally, by virtue of the Lyapunov stability theory, it is proved that all internal signals of the closed-loop systems are semiglobal uniformly ultimately bounded (SGUUB). Simulation results are provided to illustrate the validity of the devised method.},
  archive      = {J_TNNLS},
  author       = {Yongchao Liu and Qidan Zhu and Guoxing Wen},
  doi          = {10.1109/TNNLS.2020.3029587},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {853-865},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive tracking control for perturbed strict-feedback nonlinear systems based on optimized backstepping technique},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilayer spectral–spatial graphs for label noisy robust
hyperspectral image classification. <em>TNNLS</em>, <em>33</em>(2),
839–852. (<a href="https://doi.org/10.1109/TNNLS.2020.3029523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hyperspectral image (HSI) analysis, label information is a scarce resource and it is unavoidably affected by human and nonhuman factors, resulting in a large amount of label noise. Although most of the recent supervised HSI classification methods have achieved good classification results, their performance drastically decreases when the training samples contain label noise. To address this issue, we propose a label noise cleansing method based on spectral–spatial graphs (SSGs). In particular, an affinity graph is constructed based on spectral and spatial similarity, in which pixels in a superpixel segmentation-based homogeneous region are connected, and their similarities are measured by spectral feature vectors. Then, we use the constructed affinity graph to regularize the process of label noise cleansing. In this manner, we transform label noise cleansing to an optimization problem with a graph constraint. To fully utilize spatial information, we further develop multiscale segmentation-based multilayer SSGs (MSSGs). It can efficiently merge the complementary information of multilayer graphs and thus provides richer spatial information compared with any single-layer graph obtained from isolation segmentation. Experimental results show that MSSG reduces the level of label noise. Compared with the state of the art, the proposed MSSG method exhibits significantly enhanced classification accuracy toward the training data with noisy labels. The significant advantages of the proposed method over four major classifiers are also demonstrated. The source code is available at https://github.com/junjun-jiang/MSSG .},
  archive      = {J_TNNLS},
  author       = {Junjun Jiang and Jiayi Ma and Xianming Liu},
  doi          = {10.1109/TNNLS.2020.3029523},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {839-852},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multilayer Spectral–Spatial graphs for label noisy robust hyperspectral image classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Vehicle and person re-identification with support neighbor
loss. <em>TNNLS</em>, <em>33</em>(2), 826–838. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key tasks for an intelligent visual surveillance system is to automatically re-identify objects of interest, e.g., persons or vehicles, from nonoverlapping camera views. This demand incurs the vast investigation of person re-identification (re-ID) and vehicle re-ID techniques, especially those deep learning-based ones. While most recent algorithms focus on designing new convolutional neural networks, less attention is paid to the loss functions, which are of vital roles as well. Triplet loss and softmax loss are the two losses that are extensively used, both of which, however, have limitations. Triplet loss optimizes the model to produce features with which samples from the same class have higher similarity than those from different classes. The problem of triplet loss is that the number of triplets to be constructed grows cubically with training samples, which causes scalability issue, unstable performance, and slow convergence. Softmax loss has favorable scalable property and is widely used for large-scale classification problems. However, since Softmax loss only aims to separate well training classes, its performance for re-ID tasks is not desirable because the model is tested to measure the similarity of samples from unseen classes. We propose the support neighbor (SN) loss, which avoids the limitations of the abovementioned two losses. Unlike triplet loss that is calculated based on triplets, SN loss is derived from $K$ -nearest neighbors (SNs) of anchor samples. The SNs of an anchor are unique, containing more valuable contextual information and neighborhood structure of the anchor, and thus contribute to more stable performance and reliable embedding from image space to feature space. Based on the SNs, a softmax-like separation term and a squeeze term are proposed, which encourage interclass separation and intraclass compactness, respectively. Experiments show that SN loss surpasses triplet and softmax losses with the same backbone network and reaches the state-of-the-art performance for both person and vehicle re-ID using a ResNet50 backbone when combined with training tricks.},
  archive      = {J_TNNLS},
  author       = {Kai Li and Zhengming Ding and Kunpeng Li and Yulun Zhang and Yun Fu},
  doi          = {10.1109/TNNLS.2020.3029299},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {826-838},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Vehicle and person re-identification with support neighbor loss},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixture correntropy-based kernel extreme learning machines.
<em>TNNLS</em>, <em>33</em>(2), 811–825. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel-based extreme learning machine (KELM), as a natural extension of ELM to kernel learning, has achieved outstanding performance in addressing various regression and classification problems. Compared with the basic ELM, KELM has a better generalization ability owing to no needs of the number of hidden nodes given beforehand and random projection mechanism. Since KELM is derived under the minimum mean square error (MMSE) criterion for the Gaussian assumption of noise, its performance may deteriorate under the non-Gaussian cases, seriously. To improve the robustness of KELM, this article proposes a mixture correntropy-based KELM (MC-KELM), which adopts the recently proposed maximum mixture correntropy criterion as the optimization criterion, instead of using the MMSE criterion. In addition, an online sequential version of MC-KELM (MCOS-KELM) is developed to deal with the case that the data arrive sequentially (one-by-one or chunk-by-chunk). Experimental results on regression and classification data sets are reported to validate the performance superiorities of the new methods.},
  archive      = {J_TNNLS},
  author       = {Yunfei Zheng and Badong Chen and Shiyuan Wang and Weiqun Wang and Wei Qin},
  doi          = {10.1109/TNNLS.2020.3029198},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {811-825},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Mixture correntropy-based kernel extreme learning machines},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep multimodal transfer learning for cross-modal retrieval.
<em>TNNLS</em>, <em>33</em>(2), 798–810. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval (CMR) enables flexible retrieval experience across different modalities (e.g., texts versus images), which maximally benefits us from the abundance of multimedia data. Existing deep CMR approaches commonly require a large amount of labeled data for training to achieve high performance. However, it is time-consuming and expensive to annotate the multimedia data manually. Thus, how to transfer valuable knowledge from existing annotated data to new data, especially from the known categories to new categories, becomes attractive for real-world applications. To achieve this end, we propose a deep multimodal transfer learning (DMTL) approach to transfer the knowledge from the previously labeled categories (source domain) to improve the retrieval performance on the unlabeled new categories (target domain). Specifically, we employ a joint learning paradigm to transfer knowledge by assigning a pseudolabel to each target sample. During training, the pseudolabel is iteratively updated and passed through our model in a self-supervised manner. At the same time, to reduce the domain discrepancy of different modalities, we construct multiple modality-specific neural networks to learn a shared semantic space for different modalities by enforcing the compactness of homoinstance samples and the scatters of heteroinstance samples. Our method is remarkably different from most of the existing transfer learning approaches. To be specific, previous works usually assume that the source domain and the target domain have the same label set. In contrast, our method considers a more challenging multimodal learning situation where the label sets of the two domains are different or even disjoint. Experimental studies on four widely used benchmarks validate the effectiveness of the proposed method in multimodal transfer learning and demonstrate its superior performance in CMR compared with 11 state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Liangli Zhen and Peng Hu and Xi Peng and Rick Siow Mong Goh and Joey Tianyi Zhou},
  doi          = {10.1109/TNNLS.2020.3029181},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {798-810},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep multimodal transfer learning for cross-modal retrieval},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Novel data and neural network-based nonlinear adaptive
switching control method. <em>TNNLS</em>, <em>33</em>(2), 789–797. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an adaptive nonlinear control method for a discrete-time dynamical system. First, the nonlinear term is decomposed into a previous sampling instant term and an unknown increment term, which are determined using an intelligent estimation algorithm based on adaptive fuzzy neural networks. The problem of obtaining accurate input data due to the unknown current control signal in unmodeled dynamics using conventional estimation algorithms is addressed, and the conservativeness is reduced. Furthermore, historical data of the controlled plant are leveraged, and the data in the nonlinear term containing repeated estimation information are disregarded. Then, we apply the proposed decomposition method for the nonlinear term to design nonlinear switching controllers. One linear and two nonlinear adaptive controllers are designed, all with compensation of the nonlinear term at the previous sampling instant and increment estimation. These three adaptive controllers coordinately operate the plant by switching rules to guarantee the stability of the controlled plant and to improve the system performance. The stability and convergence of the system are analyzed and verified. Finally, simulation examples are used to verify the effectiveness of the proposed method and compare it with existing methods to confirm its superior performance.},
  archive      = {J_TNNLS},
  author       = {Yajun Zhang and Hong Niu and Jinmei Tao and Xusheng Li},
  doi          = {10.1109/TNNLS.2020.3029113},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {789-797},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Novel data and neural network-based nonlinear adaptive switching control method},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semisupervised feature learning by deep entropy-sparsity
subspace clustering. <em>TNNLS</em>, <em>33</em>(2), 774–788. (<a
href="https://doi.org/10.1109/TNNLS.2020.3029033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While feature learning by deep neural networks is currently widely used, it is still very challenging to perform this task, given the very limited quantity of labeled data. To solve this problem, we propose to unite subspace clustering with deep semisupervised feature learning to form a unified learning framework to pursue feature learning by subspace clustering. More specifically, we develop a deep entropy-sparsity subspace clustering (deep ESSC) model, which forces a deep neural network to learn features using subspace clustering constrained by our designed entropy-sparsity scheme. The model can inherently harmonize deep semisupervised feature learning and subspace clustering simultaneously by the proposed self-similarity preserving strategy. To optimize the deep ESSC model, we introduce two unconstrained variables to eliminate the two constraints via softmax functions. We provide a general algebraic-treatment scheme for solving the proposed deep ESSC model. Extensive experiments with comprehensive analysis substantiate that our deep ESSC model is more effective than the related methods.},
  archive      = {J_TNNLS},
  author       = {Sheng Wu and Wei-Shi Zheng},
  doi          = {10.1109/TNNLS.2020.3029033},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {774-788},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised feature learning by deep entropy-sparsity subspace clustering},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boost 3-d object detection via point clouds segmentation and
fused 3-d GIoU-l₁ loss. <em>TNNLS</em>, <em>33</em>(2), 762–773. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3-D object detection is crucial for many real-world applications, attracting many researchers’ attention. Beyond 2-D object detection, 3-D object detection usually needs to extract appearance, depth, position, and orientation information from light detection and ranging (LiDAR) and camera sensors. However, due to more degrees of freedom and vertices, existing detection methods that directly transform from 2-D to 3-D still face several challenges, such as exploding increase of anchors’ number and inefficient or hard-to-optimize objective. To this end, we present a fast segmentation method for 3-D point clouds to reduce anchors, which can largely decrease the computing cost. Moreover, taking advantage of 3-D generalized Intersection of Union (GIoU) and $L_{1}$ losses, we propose a fused loss to facilitate the optimization of 3-D object detection. A series of experiments show that the proposed method has alleviated the abovementioned issues effectively.},
  archive      = {J_TNNLS},
  author       = {Yaran Chen and Haoran Li and Ruiyuan Gao and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2020.3028964},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {762-773},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Boost 3-D object detection via point clouds segmentation and fused 3-D GIoU-l₁ loss},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A3 CLNN: Spatial, spectral and multiscale attention ConvLSTM
neural network for multisource remote sensing data classification.
<em>TNNLS</em>, <em>33</em>(2), 747–761. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of effectively exploiting the information multiple data sources has become a relevant but challenging research topic in remote sensing. In this article, we propose a new approach to exploit the complementarity of two data sources: hyperspectral images (HSIs) and light detection and ranging (LiDAR) data. Specifically, we develop a new dual-channel spatial, spectral and multiscale attention convolutional long short-term memory neural network (called dual-channel $A^{3}$ CLNN) for feature extraction and classification of multisource remote sensing data. Spatial, spectral, and multiscale attention mechanisms are first designed for HSI and LiDAR data in order to learn spectral- and spatial-enhanced feature representations and to represent multiscale information for different classes. In the designed fusion network, a novel composite attention learning mechanism (combined with a three-level fusion strategy) is used to fully integrate the features in these two data sources. Finally, inspired by the idea of transfer learning, a novel stepwise training strategy is designed to yield a final classification result. Our experimental results, conducted on several multisource remote sensing data sets, demonstrate that the newly proposed dual-channel $A^{\,3}$ CLNN exhibits better feature representation ability (leading to more competitive classification performance) than other state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Heng-Chao Li and Wen-Shuai Hu and Wei Li and Jun Li and Qian Du and Antonio Plaza},
  doi          = {10.1109/TNNLS.2020.3028945},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {747-761},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A3 CLNN: Spatial, spectral and multiscale attention ConvLSTM neural network for multisource remote sensing data classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time synchronization for delayed complex dynamical
networks with synchronizing or desynchronizing impulses. <em>TNNLS</em>,
<em>33</em>(2), 736–746. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the finite-time synchronization problem of delayed complex dynamical networks (CDNs) with impulses is studied, where two types of impulses, namely, synchronizing impulses and desynchronizing impulses, are fully considered, respectively. Since the existence of impulses makes the discontinuity of the states, which means that the classical result for finite-time stability is inapplicable in such a case, the key challenge is how to guarantee the finite-time stability and estimate the settling time in impulse sense. We apply impulsive control theory and finite-time stability theory to CDNs and establish some sufficient conditions for finite-time synchronization, where two kinds of memory controllers are designed for synchronizing impulses and desynchronizing impulses, respectively. Moreover, the upper bounds for settling time of synchronization, which depends on the impulse sequences, are effectively estimated. It shows that the synchronizing impulses can shorten the settling time of synchronization; conversely, the desynchronizing impulses can delay it. Finally, the theoretical analysis is verified by two simulation examples.},
  archive      = {J_TNNLS},
  author       = {Dan Yang and Xiaodi Li and Shiji Song},
  doi          = {10.1109/TNNLS.2020.3028835},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {736-746},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time synchronization for delayed complex dynamical networks with synchronizing or desynchronizing impulses},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised hyperspectral and multispectral images fusion
based on nonlinear variational probabilistic generative model.
<em>TNNLS</em>, <em>33</em>(2), 721–735. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to hardware limitations, it is challenging for sensors to acquire images of high resolution in both spatial and spectral domains, which arouses a trend that utilizing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to fuse an HR-HSI in an unsupervised manner. Considering the fact that most existing methods are restricted by using linear spectral unmixing, we propose a nonlinear variational probabilistic generative model (NVPGM) for the unsupervised fusion task based on nonlinear unmixing. We model the joint full likelihood of the observed pixels in an LR-HSI and an HR-MSI, both of which are assumed to be generated from the corresponding latent representations, i.e., the abundance vectors. The sufficient statistics of the generative conditional distributions are nonlinear functions with respect to the latent variable, realized by neural networks, which results in a nonlinear spectral mixture model. For scalability and efficiency, we construct two recognition models to infer the latent representations, which are parameterized by neural networks as well. Simultaneously inferring the latent representations and optimizing the parameters are achieved using stochastic gradient variational inference, after which the target HR-HSI is retrieved via feedforward mapping. Though without supervised information about the HR-HSI, NVPGM still can be trained based on extra LR-HSI and HR-MSI data sets in advance unsupervisedly and processes the images at the test phase in real time. Three commonly used data sets are used to evaluate the effectiveness and efficiency of NVPGM, illustrating the outperformance of NVPGM in the unsupervised LR-HSI and HR-MSI fusion task.},
  archive      = {J_TNNLS},
  author       = {Zhengjue Wang and Bo Chen and Hao Zhang and Hongwei Liu},
  doi          = {10.1109/TNNLS.2020.3028772},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {721-735},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Unsupervised hyperspectral and multispectral images fusion based on nonlinear variational probabilistic generative model},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wavelet-based dual recursive network for image
super-resolution. <em>TNNLS</em>, <em>33</em>(2), 707–720. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although remarkable progress has been made on single-image super-resolution (SISR), deep learning methods cannot be easily applied to real-world applications due to the requirement of its heavy computation, especially for mobile devices. Focusing on the fewer parameters and faster inference SISR approach, we propose an efficient and time-saving wavelet transform-based network architecture, where the image super-resolution (SR) processing is carried out in the wavelet domain. Different from the existing methods that directly infer high-resolution (HR) image with the input low-resolution (LR) image, our approach first decomposes the LR image into a series of wavelet coefficients (WCs) and the network learns to predict the corresponding series of HR WCs and then reconstructs the HR image. Particularly, in order to further enhance the relationship between WCs and image deep characteristics, we propose two novel modules [wavelet feature mapping block (WFMB) and wavelet coefficients reconstruction block (WCRB)] and a dual recursive framework for joint learning strategy, thus forming a WCs prediction model to realize the efficient and accurate reconstruction of HR WCs. Experimental results show that the proposed method can outperform state-of-the-art methods with more than a $2\times $ reduction in model parameters and computational complexity.},
  archive      = {J_TNNLS},
  author       = {Jingwei Xin and Jie Li and Xinrui Jiang and Nannan Wang and Heng Huang and Xinbo Gao},
  doi          = {10.1109/TNNLS.2020.3028688},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {707-720},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Wavelet-based dual recursive network for image super-resolution},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IPGAN: Generating informative item pairs by adversarial
sampling. <em>TNNLS</em>, <em>33</em>(2), 694–706. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative sampling plays an important role in ranking-based recommender models. However, most existing sampling methods cannot generate informative item pairs with positive and negative instances due to two limitations: 1) they merely treat observed items as positive instances, ignoring the existence of potential positive items (i.e., nonobserved items users may prefer) and the probability of observed but noisy items and 2) they fail to capture the relationship between positive and negative items during negative sampling, which may cause the unexpected selection of potential positive items. In this article, we introduce a dynamic sampling strategy to search informative item pairs. Specifically, we first sample a positive instance from all the items by leveraging the overall features of user’s observed items. Then, we strategically select a negative instance by considering its correlation with the sampled positive one. Formally, we propose an item pair generative adversarial network named IPGAN, where our sampling strategy is realized in two generative models for positive and negative instances, respectively. In addition, IPGAN can also ensure that the sampled item pairs are informative relative to the ground truth by a discriminative model. What is more, we propose a batch-training approach to further enhance both user and item modeling by alleviating the special bias (noise) from different users. This approach can also significantly accelerate the process of model training compared with classical GAN method for recommendation. Experimental results on three real data sets show that our approach outperforms other state-of-the-art approaches in terms of recommendation accuracy.},
  archive      = {J_TNNLS},
  author       = {Guibing Guo and Huan Zhou and Bowei Chen and Zhirong Liu and Xiao Xu and Xu Chen and Zhenhua Dong and Xiuqiang He},
  doi          = {10.1109/TNNLS.2020.3028572},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {694-706},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IPGAN: Generating informative item pairs by adversarial sampling},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observer-based adaptive synchronization control of unknown
discrete-time nonlinear heterogeneous systems. <em>TNNLS</em>,
<em>33</em>(2), 681–693. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with the optimal synchronization problem for discrete-time nonlinear heterogeneous multiagent systems (MASs) with an active leader. To overcome the difficulty in the derivation of the optimal control protocols for these systems, we develop an observer-based adaptive synchronization control approach, including the designs of a distributed observer and a distributed model reference adaptive controller with no prior knowledge of all agents’ dynamics. To begin with, for the purpose of estimating the state of a nonlinear active leader for each follower, an adaptive neural network distributed observer is designed. Such an observer serves as a reference model in the distributed model reference adaptive control (MRAC). Then, a reinforcement learning-based distributed MRAC algorithm is presented to make every follower track its corresponding reference model on behavior in real time. In this algorithm, a distributed actor–critic network is employed to approximate the optimal distributed control protocols and the cost function. Through convergence analysis, the overall observer estimation error, the model reference tracking error, and the weight estimation errors are proved to be uniformly ultimately bounded. The developed approach further achieves the synchronization by means of synthesizing these results. The effectiveness of the developed approach is verified through a numerical example.},
  archive      = {J_TNNLS},
  author       = {Hao Fu and Xin Chen and Wei Wang and Min Wu},
  doi          = {10.1109/TNNLS.2020.3028569},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {681-693},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Observer-based adaptive synchronization control of unknown discrete-time nonlinear heterogeneous systems},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Adaptive neural digital control of hysteretic systems with
implicit inverse compensator and its application on magnetostrictive
actuator. <em>TNNLS</em>, <em>33</em>(2), 667–680. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hysteresis is a complex nonlinear effect in smart materials-based actuators, which degrades the positioning performance of the actuator, especially when the hysteresis shows asymmetric characteristics. In order to mitigate the asymmetric hysteresis effect, an adaptive neural digital dynamic surface control (DSC) scheme with the implicit inverse compensator is developed in this article. The implicit inverse compensator for the purpose of compensating for the hysteresis effect is applied to find the compensation signal by searching the optimal control laws from the hysteresis output, which avoids the construction of the inverse hysteresis model. The adaptive neural digital controller is achieved by using a discrete-time neural network controller to realize the discretization of time and quantizing the control signal to realize the discretization of the amplitude. The adaptive neural digital controller ensures the semiglobally uniformly ultimately bounded (SUUB) of all signals in the closed-loop control system. The effectiveness of the proposed approach is validated via the magnetostrictive-actuated system.},
  archive      = {J_TNNLS},
  author       = {Xiuyu Zhang and Bin Li and Zhi Li and Chenguang Yang and Xinkai Chen and Chun-Yi Su},
  doi          = {10.1109/TNNLS.2020.3028500},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {667-680},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive neural digital control of hysteretic systems with implicit inverse compensator and its application on magnetostrictive actuator},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric graph-guided multitask survival analysis with
self-paced learning. <em>TNNLS</em>, <em>33</em>(2), 654–666. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multitask learning has been successfully applied to survival analysis problems. A critical challenge in real-world survival analysis tasks is that not all instances and tasks are equally learnable. A survival analysis model can be improved when considering the complexities of instances and tasks during the model training. To this end, we propose an asymmetric graph-guided multitask learning approach with self-paced learning for survival analysis applications. The proposed model is able to improve the learning performance by identifying the complex structure among tasks and considering the complexities of training instances and tasks during the model training. Especially, by incorporating the self-paced learning strategy and asymmetric graph-guided regularization, the proposed model is able to learn the model in a progressive way from “easy” to “hard” loss function items. In addition, together with the self-paced learning function, the asymmetric graph-guided regularization allows the related knowledge transfer from one task to another in an asymmetric way. Consequently, the knowledge acquired from those earlier learned tasks can help to solve complex tasks effectively. The experimental results on both synthetic and real-world TCGA data suggest that the proposed method is indeed useful for improving survival analysis and achieves higher prediction accuracies than the previous state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Cheng Liu and Wenming Cao and Si Wu and Wenjun Shen and Dazhi Jiang and Zhiwen Yu and Hau-San Wong},
  doi          = {10.1109/TNNLS.2020.3028453},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {654-666},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Asymmetric graph-guided multitask survival analysis with self-paced learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis on the number of linear regions of piecewise linear
neural networks. <em>TNNLS</em>, <em>33</em>(2), 644–653. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are shown to be excellent solutions to staggering and sophisticated problems in machine learning. A key reason for their success is due to the strong expressive power of function representation. For piecewise linear neural networks (PLNNs), the number of linear regions is a natural measure of their expressive power since it characterizes the number of linear pieces available to model complex patterns. In this article, we theoretically analyze the expressive power of PLNNs by counting and bounding the number of linear regions. We first refine the existing upper and lower bounds on the number of linear regions of PLNNs with rectified linear units (ReLU PLNNs). Next, we extend the analysis to PLNNs with general piecewise linear (PWL) activation functions and derive the exact maximum number of linear regions of single-layer PLNNs. Moreover, the upper and lower bounds on the number of linear regions of multilayer PLNNs are obtained, both of which scale polynomially with the number of neurons at each layer and pieces of PWL activation function but exponentially with the number of layers. This key property enables deep PLNNs with complex activation functions to outperform their shallow counterparts when computing highly complex and structured functions, which, to some extent, explains the performance improvement of deep PLNNs in classification and function fitting.},
  archive      = {J_TNNLS},
  author       = {Qiang Hu and Hao Zhang and Feifei Gao and Chengwen Xing and Jianping An},
  doi          = {10.1109/TNNLS.2020.3028431},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {644-653},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Analysis on the number of linear regions of piecewise linear neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative learning control for discrete-time systems with
full learnability. <em>TNNLS</em>, <em>33</em>(2), 629–643. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers iterative learning control (ILC) for a class of discrete-time systems with full learnability and unknown system dynamics. First, we give a framework to analyze the learnability of the control system and build the relationship between the learnability of the control system and the input–output coupling matrix (IOCM). The control system has full learnability if and only if the IOCM is full-row rank and the control system has no learnability almost everywhere if and only if the rank of the IOCM is less than the dimension of system output. Second, by using the repetitiveness of the control system, some data-based learning schemes are developed. It is shown that we can obtain all the needed information on system dynamics through the developed learning schemes if the control system is controllable. Third, by the dynamic characteristics of system outputs of the ILC system along the iteration direction, we show how to use the available information of system dynamics to design the iterative learning gain matrix and the current state feedback gain matrix. And we strictly prove that the iterative learning scheme with the current state feedback mechanism can guarantee the monotone convergence of the ILC process if the IOCM is full-row rank. Finally, a numerical example is provided to validate the effectiveness of the proposed iterative learning scheme with the current state feedback mechanism.},
  archive      = {J_TNNLS},
  author       = {Jian Liu and Xiaoe Ruan and Yuanshi Zheng},
  doi          = {10.1109/TNNLS.2020.3028388},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {629-643},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Iterative learning control for discrete-time systems with full learnability},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RNN for repetitive motion generation of redundant robot
manipulators: An orthogonal projection-based scheme. <em>TNNLS</em>,
<em>33</em>(2), 615–628. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the existing repetitive motion generation (RMG) schemes for kinematic control of redundant manipulators, the position error always exists and fluctuates. This article gives an answer to this phenomenon and presents the theoretical analyses to reveal that the existing RMG schemes exist a theoretical position error related to the joint angle error. To remedy this weakness of existing solutions, an orthogonal projection RMG (OPRMG) scheme is proposed in this article by introducing an orthogonal projection method with the position error eliminated theoretically, which decouples the joint space error and Cartesian space error with joint constraints considered. The corresponding new recurrent neural networks (NRNNs) are structured by exploiting the gradient descent method with the assistance of velocity compensation with theoretical analyses provided to embody the stability and feasibility. In addition, simulation results on a fixed-based redundant manipulator, a mobile manipulator, and a multirobot system synthesized by the existing RMG schemes and the proposed one are presented to verify the superiority and precise performance of the OPRMG scheme for kinematic control of redundant manipulators. Moreover, via adjusting the coefficient, simulations on the position error and joint drift of the redundant manipulator are conducted for comparison to prove the high performance of the OPRMG scheme. To bring out the crucial point, different controllers for the redundancy resolution of redundant manipulators are compared to highlight the superiority and advantage of the proposed NRNN. This work greatly improves the existing RMG solutions in theoretically eliminating the position error and joint drift, which is of significant contributions to increasing the accuracy and efficiency of high-precision instruments in manufacturing production.},
  archive      = {J_TNNLS},
  author       = {Zhengtai Xie and Long Jin and Xin Luo and Zhongbo Sun and Mei Liu},
  doi          = {10.1109/TNNLS.2020.3028304},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {615-628},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {RNN for repetitive motion generation of redundant robot manipulators: An orthogonal projection-based scheme},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured neural decoding with multitask transfer learning
of deep neural network representations. <em>TNNLS</em>, <em>33</em>(2),
600–614. (<a href="https://doi.org/10.1109/TNNLS.2020.3028167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of visual information from human brain activity is a very important research topic in brain decoding. Existing methods ignore the structural information underlying the brain activities and the visual features, which severely limits their performance and interpretability. Here, we propose a hierarchically structured neural decoding framework by using multitask transfer learning of deep neural network (DNN) representations and a matrix-variate Gaussian prior. Our framework consists of two stages, Voxel2Unit and Unit2Pixel. In Voxel2Unit, we decode the functional magnetic resonance imaging (fMRI) data to the intermediate features of a pretrained convolutional neural network (CNN). In Unit2Pixel, we further invert the predicted CNN features back to the visual images. Matrix-variate Gaussian prior allows us to take into account the structures between feature dimensions and between regression tasks, which are useful for improving decoding effectiveness and interpretability. This is in contrast with the existing single-output regression models that usually ignore these structures. We conduct extensive experiments on two real-world fMRI data sets, and the results show that our method can predict CNN features more accurately and reconstruct the perceived natural images and faces with higher quality.},
  archive      = {J_TNNLS},
  author       = {Changde Du and Changying Du and Lijie Huang and Haibao Wang and Huiguang He},
  doi          = {10.1109/TNNLS.2020.3028167},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {600-614},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Structured neural decoding with multitask transfer learning of deep neural network representations},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Novel discrete-time recurrent neural networks handling
discrete-form time-variant multi-augmented sylvester matrix problems and
manipulator application. <em>TNNLS</em>, <em>33</em>(2), 587–599. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the discrete-form time-variant multi-augmented Sylvester matrix problems, including discrete-form time-variant multi-augmented Sylvester matrix equation (MASME) and discrete-form time-variant multi-augmented Sylvester matrix inequality (MASMI), are formulated first. In order to solve the above-mentioned problems, in continuous time-variant environment, aided with the Kronecker product and vectorization techniques, the multi-augmented Sylvester matrix problems are transformed into simple linear matrix problems, which can be solved by using the proposed discrete-time recurrent neural network (RNN) models. Second, the theoretical analyses and comparisons on the computational performance of the recently developed discretization formulas are presented. Based on these theoretical results, a five-instant discretization formula with superior property is leveraged to establish the corresponding discrete-time RNN (DTRNN) models for solving the discrete-form time-variant MASME and discrete-form time-variant MASMI, respectively. Note that these DTRNN models are zero stable, consistent, and convergent with satisfied precision. Furthermore, illustrative numerical experiments are given to substantiate the excellent performance of the proposed DTRNN models for solving discrete-form time-variant multi-augmented Sylvester matrix problems. In addition, an application of robot manipulator further extends the theoretical research and physical realizability of RNN methods.},
  archive      = {J_TNNLS},
  author       = {Yang Shi and Long Jin and Shuai Li and Jian Li and Jipeng Qiang and Dimitrios K. Gerontitis},
  doi          = {10.1109/TNNLS.2020.3028136},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {587-599},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Novel discrete-time recurrent neural networks handling discrete-form time-variant multi-augmented sylvester matrix problems and manipulator application},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seeds: Sampling-enhanced embeddings. <em>TNNLS</em>,
<em>33</em>(2), 577–586. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a desirable sampling estimator has a profound impact on the development of static word embedding models, such as continue-bag-of-words (CBOW) and skip gram (SG), which have been generally accepted as popular low-resource algorithms to generate task-agnostic word representations. Due to the prevalence of large-scale pretrained models, less attention has been paid to these static models in the recent years. However, compared with the dynamic embedding models (e.g., BERT), these static models are straightforward to interpret, cost effective to train, and out-of-box to deploy, thus are still widely used in various downstream models until now. Therefore, it is still of considerable significance to study and improve them, especially the crucial components shared by these static models. In this article, we focus on negative sampling (NS), a key component shared by the sampling-based static models, by investigating and mitigating some critical problems of the sampling core. Concretely, we propose Seeds, a sampling enhanced embedding framework, to learn static word embeddings by a new algorithmic innovation for replacing the NS estimator, in which multifactor global priors are considered dynamically for different training pairs. Then, we implement this framework by four concrete models. For the first two implementations, namely CBOW-GP and SG-GP, both negative words and positive auxiliaries are sampled. And for the other two implementations, CBOW-GN and SG-GN, estimations are simplified by sampling only the negative instances. Extensive experimental results across a variety of standard intrinsic and extrinsic tasks demonstrate that embeddings learned by the proposed models outperform their NS-based counterparts, such as CBOW-NS and SG-NS, as well as other strong baselines.},
  archive      = {J_TNNLS},
  author       = {Ning Gong and Nianmin Yao and Shun Guo},
  doi          = {10.1109/TNNLS.2020.3028099},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {577-586},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Seeds: Sampling-enhanced embeddings},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Noise-robust projection rule for rotor and matrix-valued
hopfield neural networks. <em>TNNLS</em>, <em>33</em>(2), 567–576. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex-valued Hopfield neural network (CHNN) has weak noise tolerance due to rotational invariance. Some alternatives of CHNN, such as a rotor Hopfield neural network (RHNN) and a matrix-valued Hopfield neural network (MHNN), resolve rotational invariance and improve the noise tolerance. However, the RHNN and MHNN with projection rules have a different problem of self-feedbacks. If the self-feedbacks are reduced, the noise tolerance is expected to be improved further. For reduction in the self-feedbacks, the noise-robust projection rules are introduced. The stability conditions are extended, and the self-feedbacks are reduced based on the extended stability conditions. Computer simulations support that the noise tolerance is improved. In particular, the noise tolerance is more robust against an increase in the number of training patterns.},
  archive      = {J_TNNLS},
  author       = {Masaki Kobayashi},
  doi          = {10.1109/TNNLS.2020.3028091},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {567-576},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Noise-robust projection rule for rotor and matrix-valued hopfield neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep motion sickness predictor induced by visual stimuli
in virtual reality. <em>TNNLS</em>, <em>33</em>(2), 554–566. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a virtual reality (VR) environment, where visual stimuli predominate over other stimuli, the user experiences cybersickness because the balance of the body collapses due to self-motion . Accordingly, the VR experience is accompanied by unavoidable sickness referred to as visually induced motion sickness (VIMS). In this article, our primary purpose is to simultaneously estimate the VIMS score by referring to the content and calculate the temporally induced VIMS sensitivity. To seek our goals, we propose a novel architecture composed of two consecutive networks: 1) neurological representation and 2) spatiotemporal representation. In the first stage, the network imitates and learns the neurological mechanism of motion sickness. In the second stage, the significant feature of the spatial and temporal domains is expressed over the generated frames. After the training procedure, our model can calculate VIMS sensitivity for each frame of the VR content by using the weakly supervised approach for unannotated temporal VIMS scores. Furthermore, we release a massive VR content database. In the experiments, the proposed framework demonstrates excellent performance for VIMS score prediction compared with existing methods, including feature engineering and deep learning-based approaches. Furthermore, we propose a way to visualize the cognitive response to visual stimuli and demonstrate that the induced sickness tends to be activated in a similar tendency, as done in clinical studies.},
  archive      = {J_TNNLS},
  author       = {Jinwoo Kim and Heeseok Oh and Woojae Kim and Seonghwa Choi and Wookho Son and Sanghoon Lee},
  doi          = {10.1109/TNNLS.2020.3028080},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {554-566},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A deep motion sickness predictor induced by visual stimuli in virtual reality},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Domain adversarial reinforcement learning for partial domain
adaptation. <em>TNNLS</em>, <em>33</em>(2), 539–553. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial domain adaptation aims to transfer knowledge from a label-rich source domain to a label-scarce target domain (i.e., the target categories are a subset of the source ones), which relaxes the common assumption in traditional domain adaptation that the label space is fully shared across different domains. In this more general and practical scenario on partial domain adaptation, a major challenge is how to select source instances from the shared categories to ensure positive transfer for the target domain. To address this problem, we propose a domain adversarial reinforcement learning (DARL) framework to progressively select source instances to learn transferable features between domains by reducing the domain shift. Specifically, we employ a deep Q-learning to learn policies for an agent to make selection decisions by approximating the action-value function. Moreover, domain adversarial learning is introduced to learn a common feature subspace for the selected source instances and the target instances, and also to contribute to the reward calculation for the agent that is based on the relevance of the selected source instances with respect to the target domain. Extensive experiments on several benchmark data sets clearly demonstrate the superior performance of our proposed DARL over existing state-of-the-art methods for partial domain adaptation.},
  archive      = {J_TNNLS},
  author       = {Jin Chen and Xinxiao Wu and Lixin Duan and Shenghua Gao},
  doi          = {10.1109/TNNLS.2020.3028078},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {539-553},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Domain adversarial reinforcement learning for partial domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Potential flow generator with l2 optimal transport
regularity for generative models. <em>TNNLS</em>, <em>33</em>(2),
528–538. (<a href="https://doi.org/10.1109/TNNLS.2020.3028042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a potential flow generator with $L_{2}$ optimal transport regularity, which can be easily integrated into a wide range of generative models, including different versions of generative adversarial networks (GANs) and normalizing flow models. With only a slight augmentation to the original generator loss functions, our generator not only tries to transport the input distribution to the target one but also aims to find the one with minimum $L_{2}$ transport cost. We show the effectiveness of our method in several 2-D problems and illustrate the concept of “proximity” due to the $L_{2}$ optimal transport regularity. Subsequently, we demonstrate the effectiveness of the potential flow generator in image translation tasks with unpaired training data from the MNIST data set and the CelebA data set with a comparison against vanilla Wasserstein GAN with gradient penalty (WGAN-GP) and CycleGAN.},
  archive      = {J_TNNLS},
  author       = {Liu Yang and George Em Karniadakis},
  doi          = {10.1109/TNNLS.2020.3028042},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {528-538},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Potential flow generator with l2 optimal transport regularity for generative models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FPGA-based semisupervised multifusion RDCNN of process
robust VMD data with online kernel RVFLN for power quality events
recognition. <em>TNNLS</em>, <em>33</em>(2), 515–527. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improved particle swarm optimization algorithm is integrated with variational mode decomposition (VMD) to extract the efficient band-limited intrinsic mode function (BLIMF) of the single and combined power quality events (PQEs). The selected BLIMF of the robust VMD (RVMD) and the privileged Fourier magnitude spectrum (FMS) information are fed to the proposed reduced deep convolutional neural network (RDCNN) for the extraction of the most discriminative unsupervised features. The RVMD-FMS-RDCNN method shows minimum feature overlapping compared with RDCNN and RVMD-RDCNN methods. The feature vector is imported to the novel supervised online kernel random vector functional link network (OKRVFLN) for quick and accurate categorization of complex PQEs. The proposed RVMD-FMS-RDCNN-OKRVFLN method produces excellent recognition capability over RDCNN, RVMD-RDCNN, and RVMD-RDCNN-OKRVFLN methods in noise-free and noisy environments. The unique BLIMF selection, clear detection, descriptive feature extraction, higher learning speed, superior classification accuracy, and robust antinoise performances are considerable importance of the proposed RVMD-FMS-RDCNN-OKRVFLN method. Finally, the proposed method architecture is developed and implemented in a very-high-speed ML506 Virtex-5 FPGA to text, examine, and validate the feasibility, performances, and practicability for online monitoring of the PQEs.},
  archive      = {J_TNNLS},
  author       = {Mrutyunjaya Sahani and Pradipta Kishore Dash},
  doi          = {10.1109/TNNLS.2020.3027984},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {515-527},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FPGA-based semisupervised multifusion RDCNN of process robust VMD data with online kernel RVFLN for power quality events recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on knowledge graphs: Representation, acquisition,
and applications. <em>TNNLS</em>, <em>33</em>(2), 494–514. (<a
href="https://doi.org/10.1109/TNNLS.2021.3070843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction toward cognition and human-level intelligence. In this survey, we provide a comprehensive review of the knowledge graph covering overall research topics about: 1) knowledge graph representation learning; 2) knowledge acquisition and completion; 3) temporal knowledge graph; and 4) knowledge-aware applications and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning are reviewed. We further explore several emerging topics, including metarelational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of data sets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
  archive      = {J_TNNLS},
  author       = {Shaoxiong Ji and Shirui Pan and Erik Cambria and Pekka Marttinen and Philip S. Yu},
  doi          = {10.1109/TNNLS.2021.3070843},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {494-514},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A survey on knowledge graphs: Representation, acquisition, and applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review of single-source deep unsupervised visual domain
adaptation. <em>TNNLS</em>, <em>33</em>(2), 473–493. (<a
href="https://doi.org/10.1109/TNNLS.2020.3028503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly due to the presence of domain shift or dataset bias . Domain adaptation (DA) is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this article, we review the latest single-source deep unsupervised DA methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different DA strategies and the descriptions of existing benchmark datasets. We then summarize and compare different categories of single-source unsupervised DA methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and self-supervision-based methods. Finally, we discuss future research directions with challenges and possible solutions.},
  archive      = {J_TNNLS},
  author       = {Sicheng Zhao and Xiangyu Yue and Shanghang Zhang and Bo Li and Han Zhao and Bichen Wu and Ravi Krishna and Joseph E. Gonzalez and Alberto L. Sangiovanni-Vincentelli and Sanjit A. Seshia and Kurt Keutzer},
  doi          = {10.1109/TNNLS.2020.3028503},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {473-493},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A review of single-source deep unsupervised visual domain adaptation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning for structure determination in
single-particle cryo-electron microscopy: A systematic review.
<em>TNNLS</em>, <em>33</em>(2), 452–472. (<a
href="https://doi.org/10.1109/TNNLS.2021.3131325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, single-particle cryo-electron microscopy (cryo-EM) has become an indispensable method for determining macromolecular structures at high resolution to deeply explore the relevant molecular mechanism. Its recent breakthrough is mainly because of the rapid advances in hardware and image processing algorithms, especially machine learning. As an essential support of single-particle cryo-EM, machine learning has powered many aspects of structure determination and greatly promoted its development. In this article, we provide a systematic review of the applications of machine learning in this field. Our review begins with a brief introduction of single-particle cryo-EM, followed by the specific tasks and challenges of its image processing. Then, focusing on the workflow of structure determination, we describe relevant machine learning algorithms and applications at different steps, including particle picking, 2-D clustering, 3-D reconstruction, and other steps. As different tasks exhibit distinct characteristics, we introduce the evaluation metrics for each task and summarize their dynamics of technology development. Finally, we discuss the open issues and potential trends in this promising field.},
  archive      = {J_TNNLS},
  author       = {Jia-Geng Wu and Yang Yan and Dong-Xu Zhang and Bo-Wen Liu and Qing-Bing Zheng and Xiao-Liang Xie and Shi-Qi Liu and Sheng-Xiang Ge and Zeng-Guang Hou and Ning-Shao Xia},
  doi          = {10.1109/TNNLS.2021.3131325},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {2},
  pages        = {452-472},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Machine learning for structure determination in single-particle cryo-electron microscopy: A systematic review},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022e). IEEE computational intelligence society information.
<em>TNNLS</em>, <em>33</em>(1), C3. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provides a listing of current committee members and society officers.},
  archive      = {J_TNNLS},
  doi          = {10.1109/TNNLS.2021.3134573},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {C3},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {IEEE computational intelligence society information},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient learning control of uncertain fractional-order
chaotic systems with disturbance. <em>TNNLS</em>, <em>33</em>(1),
445–450. (<a href="https://doi.org/10.1109/TNNLS.2020.3028902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this brief, the problem of synchronization control is investigated for a class of fractional-order chaotic systems with unknown dynamics and disturbance. The controller is constructed using neural approximation and disturbance estimation where the system uncertainty is modeled by neural network (NN) and the time-varying disturbance is handled using disturbance observer (DOB). To evaluate the estimation performance quantitatively, the serial–parallel estimation model is constructed based on the compound uncertainty estimation derived from NN and DOB. Then, the prediction error is constructed and employed to design the composite fractional-order updating law. The boundedness of the system signals is analyzed. The simulation results show that the proposed new design scheme can achieve higher synchronization accuracy and better estimation performance.},
  archive      = {J_TNNLS},
  author       = {Xia Wang and Bin Xu and Peng Shi and Shuai Li},
  doi          = {10.1109/TNNLS.2020.3028902},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {445-450},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Efficient learning control of uncertain fractional-order chaotic systems with disturbance},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A feature-enriched deep convolutional neural network for
JPEG image compression artifacts reduction and its applications.
<em>TNNLS</em>, <em>33</em>(1), 430–444. (<a
href="https://doi.org/10.1109/TNNLS.2021.3124370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of multimedia data, such as images and videos, has been increasing rapidly with the development of various imaging devices and the Internet, bringing more stress and challenges to information storage and transmission. The redundancy in images can be reduced to decrease data size via lossy compression, such as the most widely used standard Joint Photographic Experts Group (JPEG). However, the decompressed images generally suffer from various artifacts (e.g., blocking, banding, ringing, and blurring) due to the loss of information, especially at high compression ratios. This article presents a feature-enriched deep convolutional neural network for compression artifacts reduction (FeCarNet, for short). Taking the dense network as the backbone, FeCarNet enriches features to gain valuable information via introducing multi-scale dilated convolutions, along with the efficient $1 \times 1$ convolution for lowering both parameter complexity and computation cost. Meanwhile, to make full use of different levels of features in FeCarNet, a fusion block that consists of attention-based channel recalibration and dimension reduction is developed for local and global feature fusion. Furthermore, short and long residual connections both in the feature and pixel domains are combined to build a multi-level residual structure, thereby benefiting the network training and performance. In addition, aiming at reducing computation complexity further, pixel-shuffle-based image downsampling and upsampling layers are, respectively, arranged at the head and tail of the FeCarNet, which also enlarges the receptive field of the whole network. Experimental results show the superiority of FeCarNet over state-of-the-art compression artifacts reduction approaches in terms of both restoration capacity and model complexity. The applications of FeCarNet on several computer vision tasks, including image deblurring, edge detection, image segmentation, and object detection, demonstrate the effectiveness of FeCarNet further.},
  archive      = {J_TNNLS},
  author       = {Honggang Chen and Xiaohai He and Hong Yang and Linbo Qing and Qizhi Teng},
  doi          = {10.1109/TNNLS.2021.3124370},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {430-444},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A feature-enriched deep convolutional neural network for JPEG image compression artifacts reduction and its applications},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CRPN-SFNet: A high-performance object detector on
large-scale remote sensing images. <em>TNNLS</em>, <em>33</em>(1),
416–429. (<a href="https://doi.org/10.1109/TNNLS.2020.3027924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited by the GPU memory, the current mainstream detectors fail to directly apply to large-scale remote sensing images for object detection. Moreover, the scale range of objects in remote sensing images is much wider than that of general images, which also greatly hinders the existing methods to effectively detect geospatial objects of various scales. For achieving high-performance object detection on large-scale remote sensing images, this article proposes a much faster and more accurate detecting framework, called cropping region proposal network-based scale folding network (CRPN-SFNet). In our framework, the CRPN includes a weak semantic RPN for quickly locating interesting regions and a strategy of generating cropping regions to effectively filter out meaningless regions, which can greatly reduce the computation and storage burden. Meanwhile, the proposed SFNet leverages the scale folding-based training and testing methods to extend the valid detection range of existing detectors, which is beneficial for detecting remote sensing objects of various scales, including very small and very large geospatial objects. Extensive experiments on the public Dataset for Object deTection in Aerial images data set indicate that our CRPN can help our detector deal the larger image faster with the limited GPU memory; meanwhile, the SFNet is beneficial to achieve more accurate detection of geospatial objects with wide-scale range. For large-scale remote sensing images, the proposed detection framework outperforms the state-of-the-art object detection methods in terms of accuracy and speed.},
  archive      = {J_TNNLS},
  author       = {Qifeng Lin and Jianhui Zhao and Gang Fu and Zhiyong Yuan},
  doi          = {10.1109/TNNLS.2020.3027924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {416-429},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CRPN-SFNet: A high-performance object detector on large-scale remote sensing images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new looped functional to synchronize neural networks with
sampled-data control. <em>TNNLS</em>, <em>33</em>(1), 406–415. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the problem of sampled-data-based synchronization of neural networks with and without considering time delay. A novel looped functional is introduced in the construction of Lyapunov functional, which adequately utilizes the state information of $e(t_{k})$ , $e(t)$ , $e(t_{k+1})$ , $e(t_{k}-{\tau _{c}})$ , $e(t-{\tau _{c}})$ , and $e(t_{k+1}-{\tau _{c}})$ . Then, by using this functional and employing a generalized free-matrix-based integral inequality (GFMBII), several sufficient conditions are derived to ensure that the slave system is synchronous with the master system. Also, the sampled-data controller can be obtained by using the linear matrix inequality (LMI) technique. Finally, two numerical examples are illustrated to show the validity and advantages of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Hong-Bing Zeng and Zheng-Liang Zhai and Huaicheng Yan and Wei Wang},
  doi          = {10.1109/TNNLS.2020.3027862},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {406-415},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A new looped functional to synchronize neural networks with sampled-data control},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An evolutionary orthogonal component analysis method for
incremental dimensionality reduction. <em>TNNLS</em>, <em>33</em>(1),
392–405. (<a href="https://doi.org/10.1109/TNNLS.2020.3027852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to quickly discover the low-dimensional representation of high-dimensional noisy data in online environments, we transform the linear dimensionality reduction problem into the problem of learning the bases of linear feature subspaces. Based on that, we propose a fast and robust dimensionality reduction framework for incremental subspace learning named evolutionary orthogonal component analysis (EOCA). By setting adaptive thresholds to automatically determine the target dimensionality, the proposed method extracts the orthogonal subspace bases of data incrementally to realize dimensionality reduction and avoids complex computations. Besides, EOCA can merge two learned subspaces that are represented by their orthonormal bases to a new one to eliminate the outlier effects, and the new subspace is proved to be unique. Extensive experiments and analysis demonstrate that EOCA is fast and achieves competitive results, especially for noisy data.},
  archive      = {J_TNNLS},
  author       = {Tianyue Zhang and Furao Shen and Tao Zhu and Jian Zhao},
  doi          = {10.1109/TNNLS.2020.3027852},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {392-405},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An evolutionary orthogonal component analysis method for incremental dimensionality reduction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-path deep fusion network for face image hallucination.
<em>TNNLS</em>, <em>33</em>(1), 378–391. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the performance improvement of deep-learning-based face hallucination methods, various face priors (facial shape, facial landmark heatmaps, or parsing maps) have been used to describe holistic and partial facial features, making the cost of generating super-resolved face images expensive and laborious. To deal with this problem, we present a simple yet effective dual-path deep fusion network (DPDFN) for face image super-resolution (SR) without requiring additional face prior, which learns the global facial shape and local facial components through two individual branches. The proposed DPDFN is composed of three components: a global memory subnetwork (GMN), a local reinforcement subnetwork (LRN), and a fusion and reconstruction module (FRM). In particular, GMN characterize the holistic facial shape by employing recurrent dense residual learning to excavate wide-range context across spatial series. Meanwhile, LRN is committed to learning local facial components, which focuses on the patch-wise mapping relations between low-resolution (LR) and high-resolution (HR) space on local regions rather than the entire image. Furthermore, by aggregating the global and local facial information from the preceding dual-path subnetworks, FRM can generate the corresponding high-quality face image. Experimental results of face hallucination on public face data sets and face recognition on real-world data sets (VGGface and SCFace) show the superiority both on visual effect and objective indicators over the previous state-of-the-art methods.},
  archive      = {J_TNNLS},
  author       = {Kui Jiang and Zhongyuan Wang and Peng Yi and Tao Lu and Junjun Jiang and Zixiang Xiong},
  doi          = {10.1109/TNNLS.2020.3027849},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {378-391},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dual-path deep fusion network for face image hallucination},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knowledge transfer via decomposing essential information in
convolutional neural networks. <em>TNNLS</em>, <em>33</em>(1), 366–377.
(<a href="https://doi.org/10.1109/TNNLS.2020.3027837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) from a “teacher” neural network and transfer of the knowledge to a small student network is done to improve the performance of the student network. This method is one of the most popular techniques to lighten convolutional neural networks (CNNs). Many KD algorithms have been proposed recently, but they still cannot properly distill essential knowledge of the teacher network, and the transfer tends to depend on the spatial shape of the teacher’s feature map. To solve these problems, we propose a method to transfer knowledge independently of the spatial shape of the teacher’s feature map, which is major information obtained by decomposing the feature map through singular value decomposition (SVD). In addition, we present a multitask learning method that enables the student to learn the teacher’s knowledge effectively by adaptively adjusting the teacher’s constraints to the student’s learning speed. Experimental results show that the proposed method performs 2.37\% better on the CIFAR100 data set and 2.89\% better on the TinyImageNet data set than the state-of-the-art method. The source code is publicly available at https://github.com/sseung0703/KD_methods_with_TF .},
  archive      = {J_TNNLS},
  author       = {Seunghyun Lee and Byung Cheol Song},
  doi          = {10.1109/TNNLS.2020.3027837},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {366-377},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Knowledge transfer via decomposing essential information in convolutional neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Block sparse variational bayes regression using matrix
variate distributions with application to SSVEP detection.
<em>TNNLS</em>, <em>33</em>(1), 351–365. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nonsparse representation, the use of compressed sensing (CS) for physiological signals, such as a multichannel electroencephalogram (EEG), has been a challenge. We present a generalized Bayesian CS framework that is capable of handling representations that arise in the spatiotemporal setting. The proposed model utilizes the standard linear Gaussian observation model associated with the hierarchical modeling of data using the matrix-variate Gaussian scale mixture (GSM). It deploys various random and deterministic parameters to incorporate the knowledge of spatial and temporal correlation present in data. By varying distributions over random parameters, a family of generalized hyperbolic matrix variate distributions is derived. For estimation, we rely on variational Bayes (VB) for random parameters and expectation-maximization (EM) for deterministic parameters. Furthermore, the model is compared with recent developments in matrix-variate distribution-based modeling of data, and we briefly discuss its extension to finite mixtures of skewed distributions. Finally, the framework is applied to the steady-state visual evoked potential (SSVEP)-based EEG benchmark data set, and a comparative study is conducted to show its effectiveness for the frequency detection task. One of the crucial features of the proposed model is that it simultaneously processes multichannel signals with low computational cost and time, making it suitable for real-time systems, especially in a resource-constrained environment.},
  archive      = {J_TNNLS},
  author       = {Shruti Sharma and Santanu Chaudhury and Jayadeva},
  doi          = {10.1109/TNNLS.2020.3027773},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {351-365},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Block sparse variational bayes regression using matrix variate distributions with application to SSVEP detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Clustering analysis via deep generative models with mixture
models. <em>TNNLS</em>, <em>33</em>(1), 340–350. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental problem that frequently arises in many fields, such as pattern recognition, data mining, and machine learning. Although various clustering algorithms have been developed in the past, traditional clustering algorithms with shallow structures cannot excavate the interdependence of complex data features in latent space. Recently, deep generative models, such as autoencoder (AE), variational AE (VAE), and generative adversarial network (GAN), have achieved remarkable success in many unsupervised applications thanks to their capabilities for learning promising latent representations from original data. In this work, first we propose a novel clustering approach based on both Wasserstein GAN with gradient penalty (WGAN-GP) and VAE with a Gaussian mixture prior. By combining the WGAN-GP with VAE, the generator of WGAN-GP is formulated by drawing samples from the probabilistic decoder of VAE. Moreover, to provide more robust clustering and generation performance when outliers are encountered in data, a variant of the proposed deep generative model is developed based on a Student’s-t mixture prior. The effectiveness of our deep generative models is validated though experiments on both clustering analysis and samples generation. Through the comparison with other state-of-art clustering approaches based on deep generative models, the proposed approach can provide more stable training of the model, improve the accuracy of clustering, and generate realistic samples.},
  archive      = {J_TNNLS},
  author       = {Lin Yang and Wentao Fan and Nizar Bouguila},
  doi          = {10.1109/TNNLS.2020.3027761},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {340-350},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Clustering analysis via deep generative models with mixture models},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-phase switching optimization strategy in deep neural
networks. <em>TNNLS</em>, <em>33</em>(1), 330–339. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization in a deep neural network is always challenging due to the vanishing gradient problem and intensive fine-tuning of network hyperparameters. Inspired by multistage decision control systems, the stochastic diagonal approximate greatest descent (SDAGD) algorithm is proposed in this article to seek for optimal learning weights using a two-phase switching optimization strategy. The proposed optimizer controls the relative step length derived based on the long-term optimal trajectory and adopts the diagonal approximated Hessian for efficient weight update. In Phase-I, it computes the greatest step length at the boundary of each local spherical search region and, subsequently, descends rapidly toward the direction of an optimal solution. In Phase-II, it switches to an approximate Newton method automatically once it is closer to the optimal solution to achieve fast convergence. The experiments show that SDAGD produces steeper learning curves and achieves lower misclassification rates compared with other optimization techniques. Implementation of the proposed optimizer to deeper networks is also investigated in this article to study the vanishing gradient problem.},
  archive      = {J_TNNLS},
  author       = {Hong Hui Tan and King Hann Lim},
  doi          = {10.1109/TNNLS.2020.3027750},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {330-339},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-phase switching optimization strategy in deep neural networks},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topic-based instance and feature selection in multilabel
classification. <em>TNNLS</em>, <em>33</em>(1), 315–329. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel learning has been extensively studied in the past years, as it has many applications in different domains. It aims at annotating the labels for unseen data according to training data, which are often high dimensional in both instance and feature levels. The training data often have noisy and redundant information on these two levels. As an effective data preprocessing step, instance and feature selection should both be performed to find relevant training instances for each testing instance and relevant features for each label, respectively. However, most of the existing methods overlook the input–output correlation in each kind of selection. It will lead to the performance degradation. This article presents a formulation for multilabel learning from a topic view that exploits the dependence between features and labels in a topic space. We can perform effective instance and feature selection in the latent topic space, as the relationship between the input and output spaces is well captured in this space. The results from intensive experiments on various benchmarks demonstrate the effectiveness of the proposed framework.},
  archive      = {J_TNNLS},
  author       = {Jianghong Ma and Tommy W. S. Chow},
  doi          = {10.1109/TNNLS.2020.3027745},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {315-329},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Topic-based instance and feature selection in multilabel classification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flexible cross-modal hashing. <em>TNNLS</em>,
<em>33</em>(1), 304–314. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has been widely adopted for large-scale data retrieval in many domains due to its low storage cost and high retrieval speed. Existing cross-modal hashing methods optimistically assume that the correspondence between training samples across modalities is readily available. This assumption is unrealistic in practical applications. In addition, existing methods generally require the same number of samples across different modalities, which restricts their flexibility. We propose a flexible cross-modal hashing approach (FlexCMH) to learn effective hashing codes from weakly paired data, whose correspondence across modalities is partially (or even totally) unknown. FlexCMH first introduces a clustering-based matching strategy to explore the structure of each cluster and, thus, to find the potential correspondence between clusters (and samples therein) across modalities. To reduce the impact of an incomplete correspondence, it jointly optimizes the potential correspondence, the cross-modal hashing functions derived from the correspondence, and a hashing quantitative loss in a unified objective function. An alternative optimization technique is also proposed to coordinate the correspondence and hash functions and reinforce the reciprocal effects of the two objectives. Experiments on public multimodal data sets show that FlexCMH achieves significantly better results than state-of-the-art methods, and it, indeed, offers a high degree of flexibility for practical cross-modal hashing tasks.},
  archive      = {J_TNNLS},
  author       = {Guoxian Yu and Xuanwu Liu and Jun Wang and Carlotta Domeniconi and Xiangliang Zhang},
  doi          = {10.1109/TNNLS.2020.3027729},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {304-314},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Flexible cross-modal hashing},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). An r-convolution graph kernel based on fast discrete-time
quantum walk. <em>TNNLS</em>, <em>33</em>(1), 292–303. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel R-convolution kernel, named the fast quantum walk kernel (FQWK), is proposed for unattributed graphs. In FQWK, the similarity of the neighborhood-pair substructure between two nodes is measured via the superposition amplitude of quantum walks between those nodes. The quantum interference in this kind of local substructures provides more information on the substructures so that FQWK can capture finer-grained local structural features of graphs. In addition, to efficiently compute the transition amplitudes of multistep discrete-time quantum walks, a fast recursive method is designed. Thus, compared with all the existing kernels based on the quantum walk, FQWK has the highest computation speed. Extensive experiments demonstrate that FQWK outperforms state-of-the-art graph kernels in terms of classification accuracy for unattributed graphs. Meanwhile, it can be applied to distinguish a larger family of graphs, including cospectral graphs, regular graphs, and even strong regular graphs, which are not distinguishable by classical walk-based methods.},
  archive      = {J_TNNLS},
  author       = {Yi Zhang and Lulu Wang and Richard C. Wilson and Kai Liu},
  doi          = {10.1109/TNNLS.2020.3027687},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {292-303},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {An R-convolution graph kernel based on fast discrete-time quantum walk},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anomaly detection based on zero-shot outlier synthesis and
hierarchical feature distillation. <em>TNNLS</em>, <em>33</em>(1),
281–291. (<a href="https://doi.org/10.1109/TNNLS.2020.3027667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection suffers from unbalanced data since anomalies are quite rare. Synthetically generated anomalies are a solution to such ill or not fully defined data. However, synthesis requires an expressive representation to guarantee the quality of the generated data. In this article, we propose a two-level hierarchical latent space representation that distills inliers’ feature descriptors [through autoencoders (AEs)] into more robust representations based on a variational family of distributions (through a variational AE) for zero-shot anomaly generation. From the learned latent distributions, we select those that lie on the outskirts of the training data as synthetic-outlier generators. Also, we synthesize from them, i.e., generate negative samples without seen them before, to train binary classifiers. We found that the use of the proposed hierarchical structure for feature distillation and fusion creates robust and general representations that allow us to synthesize pseudo outlier samples. Also, in turn, train robust binary classifiers for true outlier detection (without the need for actual outliers during training). We demonstrate the performance of our proposal on several benchmarks for anomaly detection.},
  archive      = {J_TNNLS},
  author       = {Adín Ramírez Rivera and Adil Khan and Imad Eddine Ibrahim Bekkouch and Taimoor Shakeel Sheikh},
  doi          = {10.1109/TNNLS.2020.3027667},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {281-291},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Anomaly detection based on zero-shot outlier synthesis and hierarchical feature distillation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Adaptive interleaved reinforcement learning: Robust
stability of affine nonlinear systems with unknown uncertainty.
<em>TNNLS</em>, <em>33</em>(1), 270–280. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates adaptive robust controller design for discrete-time (DT) affine nonlinear systems using an adaptive dynamic programming. A novel adaptive interleaved reinforcement learning algorithm is developed for finding a robust controller of DT affine nonlinear systems subject to matched or unmatched uncertainties. To this end, the robust control problem is converted into the optimal control problem for nominal systems by selecting an appropriate utility function. The performance evaluation and control policy update combined with neural networks approximation are alternately implemented at each time step for solving a simplified Hamilton–Jacobi–Bellman (HJB) equation such that the uniformly ultimately bounded (UUB) stability of DT affine nonlinear systems can be guaranteed, allowing for all realization of unknown bounded uncertainties. The rigorously theoretical proofs of convergence of the proposed interleaved RL algorithm and UUB stability of uncertain systems are provided. Simulation results are given to verify the effectiveness of the proposed method.},
  archive      = {J_TNNLS},
  author       = {Jinna Li and Jinliang Ding and Tianyou Chai and Frank L. Lewis and Sarangapani Jagannathan},
  doi          = {10.1109/TNNLS.2020.3027653},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {270-280},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive interleaved reinforcement learning: Robust stability of affine nonlinear systems with unknown uncertainty},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network self-distillation exploiting data
representation invariance. <em>TNNLS</em>, <em>33</em>(1), 257–269. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To harvest small networks with high accuracies, most existing methods mainly utilize compression techniques such as low-rank decomposition and pruning to compress a trained large model into a small network or transfer knowledge from a powerful large model (teacher) to a small network (student). Despite their success in generating small models of high performance, the dependence of accompanying assistive models complicates the training process and increases memory and time cost. In this article, we propose an elegant self-distillation (SD) mechanism to obtain high-accuracy models directly without going through an assistive model. Inspired by the invariant recognition in the human vision system, different distorted instances of the same input should possess similar high-level data representations. Thus, we can learn data representation invariance between different distorted versions of the same sample. Especially, in our learning algorithm based on SD, the single network utilizes the maximum mean discrepancy metric to learn the global feature consistency and the Kullback–Leibler divergence to constrain the posterior class probability consistency across the different distorted branches. Extensive experiments on MNIST, CIFAR-10/100, and ImageNet data sets demonstrate that the proposed method can effectively reduce the generalization error for various network architectures, such as AlexNet, VGGNet, ResNet, Wide ResNet, and DenseNet, and outperform existing model distillation methods with little extra training efforts.},
  archive      = {J_TNNLS},
  author       = {Ting-Bing Xu and Cheng-Lin Liu},
  doi          = {10.1109/TNNLS.2020.3027634},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {257-269},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep neural network self-distillation exploiting data representation invariance},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled representation learning for multiple attributes
preserving face deidentification. <em>TNNLS</em>, <em>33</em>(1),
244–256. (<a href="https://doi.org/10.1109/TNNLS.2020.3027617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face is one of the most attractive sensitive information in visual shared data. It is an urgent task to design an effective face deidentification method to achieve a balance between facial privacy protection and data utilities when sharing data. Most of the previous methods for face deidentification rely on attribute supervision to preserve a certain kind of identity-independent utility but lose the other identity-independent data utilities. In this article, we mainly propose a novel disentangled representation learning architecture for multiple attributes preserving face deidentification called replacing and restoring variational autoencoders (R 2 VAEs). The R 2 VAEs disentangle the identity-related factors and the identity-independent factors so that the identity-related information can be obfuscated, while they do not change the identity-independent attribute information. Moreover, to improve the details of the facial region and make the deidentified face blends into the image scene seamlessly, the image inpainting network is employed to fill in the original facial region by using the deidentified face as a priori . Experimental results demonstrate that the proposed method effectively deidentifies face while maximizing the preservation of the identity-independent information, which ensures the semantic integrity and visual quality of shared images.},
  archive      = {J_TNNLS},
  author       = {Maoguo Gong and Jialu Liu and Hao Li and Yu Xie and Zedong Tang},
  doi          = {10.1109/TNNLS.2020.3027617},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {244-256},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentangled representation learning for multiple attributes preserving face deidentification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realization of spatial sparseness by deep ReLU nets with
massive data. <em>TNNLS</em>, <em>33</em>(1), 229–243. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great success of deep learning poses urgent challenges for understanding its working mechanism and rationality. The depth, structure, and massive size of the data are recognized to be three key ingredients for deep learning. Most of the recent theoretical studies for deep learning focus on the necessity and advantages of depth and structures of neural networks. In this article, we aim at rigorous verification of the importance of massive data in embodying the outperformance of deep learning. In particular, we prove that the massiveness of data is necessary for realizing the spatial sparseness, and deep nets are crucial tools to make full use of massive data in such an application. All these findings present the reasons why deep learning achieves great success in the era of big data though deep nets and numerous network structures have been proposed at least 20 years ago.},
  archive      = {J_TNNLS},
  author       = {Charles K. Chui and Shao-Bo Lin and Bo Zhang and Ding-Xuan Zhou},
  doi          = {10.1109/TNNLS.2020.3027613},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {229-243},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Realization of spatial sparseness by deep ReLU nets with massive data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shattering distribution for active learning. <em>TNNLS</em>,
<em>33</em>(1), 215–228. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning (AL) aims to maximize the learning performance of the current hypothesis by drawing as few labels as possible from an input distribution. Generally, most existing AL algorithms prune the hypothesis set via querying labels of unlabeled samples and could be deemed as a hypothesis-pruning strategy. However, this process critically depends on the initial hypothesis and its subsequent updates. This article presents a distribution-shattering strategy without an estimation of hypotheses by shattering the number density of the input distribution. For any hypothesis class, we halve the number density of an input distribution to obtain a shattered distribution, which characterizes any hypothesis with a lower bound on VC dimension. Our analysis shows that sampling in a shattered distribution reduces label complexity and error disagreement. With this paradigm guarantee, in an input distribution, a Shattered Distribution-based AL (SDAL) algorithm is derived to continuously split the shattered distribution into a number of representative samples. An empirical evaluation of benchmark data sets further verifies the effectiveness of the halving and querying abilities of SDAL in real-world AL tasks with limited labels. Experiments on active querying with adversarial examples and noisy labels further verify our theoretical insights on the performance disagreement of the hypothesis-pruning and distribution-shattering strategies. Our code is available at https://github.com/XiaofengCao-MachineLearning/Shattering-Distribution-for-Active-Learning .},
  archive      = {J_TNNLS},
  author       = {Xiaofeng Cao and Ivor W. Tsang},
  doi          = {10.1109/TNNLS.2020.3027605},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {215-228},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Shattering distribution for active learning},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep LSAC for fine-grained recognition. <em>TNNLS</em>,
<em>33</em>(1), 200–214. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained recognition emphasizes the identification of subtle differences among object categories given objects that appear in different shapes and poses. These variances should be reduced for reliable recognition. We propose a fine-grained recognition system that incorporates localization, segmentation, alignment, and classification in a unified deep neural network. The input to the classification module includes functions that enable backward-propagation (BP) in constructing the solver. Our major contribution is to propose a valve linkage function (VLF) for BP chaining and form our deep localization, segmentation, alignment, and classification (LSAC) system. The VLF can adaptively compromise errors of classification and alignment when training the LSAC model. It in turn helps to update the localization and segmentation. We evaluate our framework on two widely used fine-grained object data sets. The performance confirms the effectiveness of our LSAC system.},
  archive      = {J_TNNLS},
  author       = {Di Lin and Yi Wang and Lingyu Liang and Ping Li and C. L. Philip Chen},
  doi          = {10.1109/TNNLS.2020.3027603},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {200-214},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep LSAC for fine-grained recognition},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized embedding regression: A framework for supervised
feature extraction. <em>TNNLS</em>, <em>33</em>(1), 185–199. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse discriminative projection learning has attracted much attention due to its good performance in recognition tasks. In this article, a framework called generalized embedding regression (GER) is proposed, which can simultaneously perform low-dimensional embedding and sparse projection learning in a joint objective function with a generalized orthogonal constraint. Moreover, the label information is integrated into the model to preserve the global structure of data, and a rank constraint is imposed on the regression matrix to explore the underlying correlation structure of classes. Theoretical analysis shows that GER can obtain the same or approximate solution as some related methods with special settings. By utilizing this framework as a general platform, we design a novel supervised feature extraction approach called jointly sparse embedding regression (JSER). In JSER, we construct an intrinsic graph to characterize the intraclass similarity and a penalty graph to indicate the interclass separability. Then, the penalty graph Laplacian is used as the constraint matrix in the generalized orthogonal constraint to deal with interclass marginal points. Moreover, the $L_{2,1}$ -norm is imposed on the regression terms for robustness to outliers and data’s variations and the regularization term for jointly sparse projection learning, leading to interesting semantic interpretability. An effective iterative algorithm is elaborately designed to solve the optimization problem of JSER. Theoretically, we prove that the subproblem of JSER is essentially an unbalanced Procrustes problem and can be solved iteratively. The convergence of the designed algorithm is also proved. Experimental results on six well-known data sets indicate the competitive performance and latent properties of JSER.},
  archive      = {J_TNNLS},
  author       = {Jianglin Lu and Zhihui Lai and Hailing Wang and Yudong Chen and Jie Zhou and Linlin Shen},
  doi          = {10.1109/TNNLS.2020.3027602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {185-199},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Generalized embedding regression: A framework for supervised feature extraction},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving speech emotion recognition with adversarial data
augmentation network. <em>TNNLS</em>, <em>33</em>(1), 172–184. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training data are scarce, it is challenging to train a deep neural network without causing the overfitting problem. For overcoming this challenge, this article proposes a new data augmentation network—namely adversarial data augmentation network (ADAN)— based on generative adversarial networks (GANs). The ADAN consists of a GAN, an autoencoder, and an auxiliary classifier. These networks are trained adversarially to synthesize class-dependent feature vectors in both the latent space and the original feature space, which can be augmented to the real training data for training classifiers. Instead of using the conventional cross-entropy loss for adversarial training, the Wasserstein divergence is used in an attempt to produce high-quality synthetic samples. The proposed networks were applied to speech emotion recognition using EmoDB and IEMOCAP as the evaluation data sets. It was found that by forcing the synthetic latent vectors and the real latent vectors to share a common representation, the gradient vanishing problem can be largely alleviated. Also, results show that the augmented data generated by the proposed networks are rich in emotion information. Thus, the resulting emotion classifiers are competitive with state-of-the-art speech emotion recognition systems.},
  archive      = {J_TNNLS},
  author       = {Lu Yi and Man-Wai Mak},
  doi          = {10.1109/TNNLS.2020.3027600},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {172-184},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Improving speech emotion recognition with adversarial data augmentation network},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-horizon optimal control of boolean control networks:
A unified graph-theoretical approach. <em>TNNLS</em>, <em>33</em>(1),
157–171. (<a href="https://doi.org/10.1109/TNNLS.2020.3027599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the finite-horizon optimal control (FHOC) problem of Boolean control networks (BCNs) from a graph theory perspective. We first formulate two general problems to unify various special cases studied in the literature: 1) the horizon length is a priori fixed and 2) the horizon length is unspecified but finite for given destination states. Notably, both problems can incorporate time-variant costs, which are rarely considered in existing work, and a variety of constraints. The existence of an optimal control sequence is analyzed under mild assumptions. Motivated by BCNs’ finite state space and control space, we approach the two general problems intuitively and efficiently under a graph-theoretical framework. A weighted state transition graph and its time-expanded variants are developed, and the equivalence between the FHOC problem and the shortest-path (SP) problem in specific graphs is established rigorously. Two algorithms are developed to find the SP and construct the optimal control sequence for the two problems with reduced computational complexity, though technically, a classical SP algorithm in graph theory is sufficient for all problems. Compared with existing algebraic methods, our graph-theoretical approach can achieve state-of-the-art time efficiency while targeting the most general problems. Furthermore, our approach is the first one capable of solving Problem 2 ) with time-variant costs. Finally, a genetic network in the bacterium E. coli and a signaling network involved in human leukemia are used to validate the effectiveness of our approach. The results of two common tasks for both networks show that our approach can dramatically reduce the running time. Python implementation of our algorithms is available at GitHub https://github.com/ShuhuaGao/FHOC .},
  archive      = {J_TNNLS},
  author       = {Shuhua Gao and Changkai Sun and Cheng Xiang and Kairong Qin and Tong Heng Lee},
  doi          = {10.1109/TNNLS.2020.3027599},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {157-171},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-horizon optimal control of boolean control networks: A unified graph-theoretical approach},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simple and effective: Spatial rescaling for person
reidentification. <em>TNNLS</em>, <em>33</em>(1), 145–156. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global average pooling (GAP) allows convolutional neural networks (CNNs) to localize discriminative information for recognition using only image-level labels. While GAP helps CNNs to attend to the most discriminative features of an object, e.g., head of a bird or one man’s bag, it may suffer if that information is missing due to camera viewpoint changes and intraclass variations in some tasks. To circumvent this issue, we propose one new module to help CNNs to see more, namely, Spatial Rescaling (SpaRs) layer. It introduces spatial relations among the feature map activations back to the model, guiding the model to focus on a broad area in the feature map. With simple implementation, it can be inserted into CNNs of various architectures directly. SpaRs layer consistently improves the performance over the reidentification (re-ID) models. Besides, the new module based on different normalization methods also demonstrates the superiority of fine-grained and general image classification benchmarks. The visualization method shows the changes in activated regions when equipped with the SpaRs layer for better understanding. Our code is publicly available at https://github.com/HRanWang/Spatial-Re-Scaling .},
  archive      = {J_TNNLS},
  author       = {Haoran Wang and Licheng Jiao and Shuyuan Yang and Lingling Li and Zexin Wang},
  doi          = {10.1109/TNNLS.2020.3027589},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {145-156},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Simple and effective: Spatial rescaling for person reidentification},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning robust discriminant subspace based on joint l₂,ₚ-
and l₂,ₛ-norm distance metrics. <em>TNNLS</em>, <em>33</em>(1), 130–144.
(<a href="https://doi.org/10.1109/TNNLS.2020.3027588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there are many works on discriminant analysis, which promote the robustness of models against outliers by using L 1 - or L 2,1 -norm as the distance metric. However, both of their robustness and discriminant power are limited. In this article, we present a new robust discriminant subspace (RDS) learning method for feature extraction, with an objective function formulated in a different form. To guarantee the subspace to be robust and discriminative, we measure the within-class distances based on $\text{L}_{2,s}$ -norm and use $\text{L}_{2,p}$ -norm to measure the between-class distances. This also makes our method include rotational invariance. Since the proposed model involves both $\text{L}_{2,p}$ -norm maximization and $\text{L}_{2,s}$ -norm minimization, it is very challenging to solve. To address this problem, we present an efficient nongreedy iterative algorithm. Besides, motivated by trace ratio criterion, a mechanism of automatically balancing the contributions of different terms in our objective is found. RDS is very flexible, as it can be extended to other existing feature extraction techniques. An in-depth theoretical analysis of the algorithm’s convergence is presented in this article. Experiments are conducted on several typical databases for image classification, and the promising results indicate the effectiveness of RDS.},
  archive      = {J_TNNLS},
  author       = {Liyong Fu and Zechao Li and Qiaolin Ye and Hang Yin and Qingwang Liu and Xiaobo Chen and Xijian Fan and Wankou Yang and Guowei Yang},
  doi          = {10.1109/TNNLS.2020.3027588},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {130-144},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning robust discriminant subspace based on joint l₂,ₚ- and l₂,ₛ-norm distance metrics},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Configurable graph reasoning for visual relationship
detection. <em>TNNLS</em>, <em>33</em>(1), 117–129. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual commonsense knowledge has received growing attention in the reasoning of long-tailed visual relationships biased in terms of object and relation labels. Most current methods typically collect and utilize external knowledge for visual relationships by following the fixed reasoning path of {subject, object $\to $ predicate} to facilitate the recognition of infrequent relationships. However, the knowledge incorporation for such fixed multidependent path suffers from the data set biased and exponentially grown combinations of object and relation labels and ignores the semantic gap between commonsense knowledge and real scenes. To alleviate this, we propose configurable graph reasoning (CGR) to decompose the reasoning path of visual relationships and the incorporation of external knowledge, achieving configurable knowledge selection and personalized graph reasoning for each relation type in each image. Given a commonsense knowledge graph, CGR learns to match and retrieve knowledge for different subpaths and selectively compose the knowledge routed path. CGR adaptively configures the reasoning path based on the knowledge graph, bridges the semantic gap between the commonsense knowledge, and the real-world scenes and achieves better knowledge generalization. Extensive experiments show that CGR consistently outperforms previous state-of-the-art methods on several popular benchmarks and works well with different knowledge graphs. Detailed analyses demonstrated that CGR learned explainable and compelling configurations of reasoning paths.},
  archive      = {J_TNNLS},
  author       = {Yi Zhu and Xiwen Liang and Bingqian Lin and Qixiang Ye and Jianbin Jiao and Liang Lin and Xiaodan Liang},
  doi          = {10.1109/TNNLS.2020.3027575},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {117-129},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Configurable graph reasoning for visual relationship detection},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable and parallel deep bayesian optimization on
attributed graphs. <em>TNNLS</em>, <em>33</em>(1), 103–116. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general and scalable global optimization framework directly operating on annotated graph data by introducing a Bayesian graph neural network to approximate the expensive-to-evaluate objectives. It prevents the cubical complexity of Gaussian processes and can scale linearly with the number of observations. Its parallelized variant makes it scalable. We provide strict theoretical support on its convergence. Intensive experiments conducted on both artificial and real-world problems, including molecular discovery and urban road network design, demonstrate the effectiveness of the proposed methods compared with the current state of the art.},
  archive      = {J_TNNLS},
  author       = {Jiaxu Cui and Bo Yang and Bingyi Sun and Xia Hu and Jiming Liu},
  doi          = {10.1109/TNNLS.2020.3027552},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {103-116},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Scalable and parallel deep bayesian optimization on attributed graphs},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse count data clustering using an exponential
approximation to generalized dirichlet multinomial distributions.
<em>TNNLS</em>, <em>33</em>(1), 89–102. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering frequency vectors is a challenging task on large data sets considering its high dimensionality and sparsity nature. Generalized Dirichlet multinomial (GDM) distribution is a competitive generative model for count data in terms of accuracy, yet its parameters estimation process is slow. The exponential-family approximation of the multivariate Polya distribution has shown to be efficient to train and cluster data directly, without dimensionality reduction. In this article, we derive an exponential-family approximation to the GDM distributions, and we call it (EGDM). A mixture model is developed based on the new member of the exponential-family of distributions, and its parameters are learned through the deterministic annealing expectation-maximization (DAEM) approach as a new clustering algorithm for count data. Moreover, we propose to estimate the optimal number of EGDM mixture components based on the minimum message length (MML) criterion. We have conducted a set of empirical experiments, concerning text, image, and video clustering, to evaluate the proposed approach performance. Results show that the new model attains a superior performance, and it is considerably faster than the corresponding method for GDM distributions.},
  archive      = {J_TNNLS},
  author       = {Nuha Zamzami and Nizar Bouguila},
  doi          = {10.1109/TNNLS.2020.3027539},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {89-102},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Sparse count data clustering using an exponential approximation to generalized dirichlet multinomial distributions},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semisupervised classification with novel graph construction
for high-dimensional data. <em>TNNLS</em>, <em>33</em>(1), 75–88. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based methods have achieved impressive performance on semisupervised classification (SSC). Traditional graph-based methods have two main drawbacks. First, the graph is predefined before training a classifier, which does not leverage the interactions between the classifier training and similarity matrix learning. Second, when handling high-dimensional data with noisy or redundant features, the graph constructed in the original input space is actually unsuitable and may lead to poor performance. In this article, we propose an SSC method with novel graph construction (SSC-NGC), in which the similarity matrix is optimized in both label space and an additional subspace to get a better and more robust result than in original data space. Furthermore, to obtain a high-quality subspace, we learn the projection matrix of the additional subspace by preserving the local and global structure of the data. Finally, we intergrade the classifier training, the graph construction, and the subspace learning into a unified framework. With this framework, the classifier parameters, similarity matrix, and projection matrix of subspace are adaptively learned in an iterative scheme to obtain an optimal joint result. We conduct extensive comparative experiments against state-of-the-art methods over multiple real-world data sets. Experimental results demonstrate the superiority of the proposed method over other state-of-the-art algorithms.},
  archive      = {J_TNNLS},
  author       = {Zhiwen Yu and Fengxu Ye and Kaixiang Yang and Wenming Cao and C. L. Philip Chen and Lianglun Cheng and Jane You and Hau-San Wong},
  doi          = {10.1109/TNNLS.2020.3027526},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {75-88},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Semisupervised classification with novel graph construction for high-dimensional data},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prescribed performance quantized tracking control for a
class of delayed switched nonlinear systems with actuator hysteresis
using a filter-connected switched hysteretic quantizer. <em>TNNLS</em>,
<em>33</em>(1), 61–74. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a prescribed adaptive backstepping scheme with new filter-connected switched hysteretic quantizer (FCSHQ) for switched nonlinear systems with nonstrict-feedback structure and time-delay. The system model is subjected to unknown functions, unknown delays, and unknown Bouc–Wen hysteresis nonlinearity. The coexistence of quantized input and actuator hysteresis may deteriorate the shape of hysteresis loop and, consequently, fail to guarantee the stability. To deal with this issue, a new FCSHQ is introduced to smooth the input hysteresis. This adaptive filter also provides us a degree of freedom at choosing the desired communication rate. The repetitive differentiations of virtual control laws and existing a lot of learning parameters in the neural network (NN)-based controller may result in an algebraic loop problem and high computational time, especially in a nonstrict-feedback form. This challenge is eased by the key advantage of NNs’ property where the upper bound of the weight vector is employed. Then, by an appropriate Lyapunov–Krasovskii functional, a common Lyapunov function is presented for all subsystems. It is shown that the proposed controller ensures the predefined output tracking accuracies and boundedness of the closed-loop signals under any arbitrary switching. Finally, the proposed control scheme is verified on a practical example where simulation results demonstrate the effectiveness of the proposed scheme.},
  archive      = {J_TNNLS},
  author       = {Sara Kamali and Seyyed Mostafa Tabatabaei and Mohammad Mehdi Arefi and Shen Yin},
  doi          = {10.1109/TNNLS.2020.3027492},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {61-74},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Prescribed performance quantized tracking control for a class of delayed switched nonlinear systems with actuator hysteresis using a filter-connected switched hysteretic quantizer},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning tracking over unknown fading channels based on
iterative estimation. <em>TNNLS</em>, <em>33</em>(1), 48–60. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With fast developments in communication technologies, a large number of practical systems adopt the networked control structure. For this structure, the fading problem is an emerging issue among other network problems. It has not been extensively investigated how to guarantee superior control performance in the presence of unknown fading channels. This article presents a learning strategy for gradually improving the tracking performance. To this end, an iterative estimation mechanism is first introduced to provide necessary statistical information such that the biased signals after transmission can be corrected before being utilized. Then, learning control algorithms incorporating with a decreasing step-size sequence are designed for both output and input fading cases. The convergence in both mean-square and almost-sure senses of the proposed schemes is strictly proved under mild conditions. Illustrative simulations verify the effectiveness of the entire learning framework.},
  archive      = {J_TNNLS},
  author       = {Dong Shen and Xinghuo Yu},
  doi          = {10.1109/TNNLS.2020.3027475},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {48-60},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning tracking over unknown fading channels based on iterative estimation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-timescale multilayer recurrent neural networks for
nonlinear programming. <em>TNNLS</em>, <em>33</em>(1), 37–47. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a neurodynamic approach to nonlinear programming. Motivated by the idea of sequential quadratic programming, a class of two-timescale multilayer recurrent neural networks is presented with neuronal dynamics in their output layer operating at a bigger timescale than in their hidden layers. In the two-timescale multilayer recurrent neural networks, the transient states in the hidden layer(s) undergo faster dynamics than those in the output layer. Sufficient conditions are derived on the convergence of the two-timescale multilayer recurrent neural networks to local optima of nonlinear programming problems. Simulation results of collaborative neurodynamic optimization based on the two-timescale neurodynamic approach on global optimization problems with nonconvex objective functions or constraints are discussed to substantiate the efficacy of the two-timescale neurodynamic approach.},
  archive      = {J_TNNLS},
  author       = {Jiasen Wang and Jun Wang},
  doi          = {10.1109/TNNLS.2020.3027471},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {37-47},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Two-timescale multilayer recurrent neural networks for nonlinear programming},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite-time h∞ state estimation for two-time-scale complex
networks under stochastic communication protocol. <em>TNNLS</em>,
<em>33</em>(1), 25–36. (<a
href="https://doi.org/10.1109/TNNLS.2020.3027467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of finite-time $H_{\infty }$ state estimation is studied for a class of discrete-time nonlinear two-time-scale complex networks (TTSCNs) whose measurement outputs are transmitted to a remote estimator via a bandwidth-limited communication network under the stochastic communication protocol (SCP). To reflect different time scales of state evolutions, a new discrete-time TTSCN model is devised by introducing a singular perturbation parameter (SPP). For the sake of avoiding/alleviating the undesirable data collisions, the SCP is adopted to schedule the data transmissions, where the transition probabilities involved are assumed to be partially unknown. By constructing a new Lyapunov function dependent on the information of the SCP and SPP, a sufficient condition is derived which ensures that the resulting error dynamics is stochastically finite-time bounded and satisfies a prescribed $H_{\infty }$ performance index. By resorting to the solutions of several matrix inequalities, the gain matrices of the state estimator are given and the admissible upper bound of the SPP can be evaluated simultaneously. The performance of the designed state estimator is demonstrated by two examples.},
  archive      = {J_TNNLS},
  author       = {Xiongbo Wan and Yongzhi Li and Yuqing Li and Min Wu},
  doi          = {10.1109/TNNLS.2020.3027467},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {25-36},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Finite-time h∞ state estimation for two-time-scale complex networks under stochastic communication protocol},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Focus, fusion, and rectify: Context-aware learning for
COVID-19 lung infection segmentation. <em>TNNLS</em>, <em>33</em>(1),
12–24. (<a href="https://doi.org/10.1109/TNNLS.2021.3126305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 (COVID-19) pandemic is spreading worldwide. Considering the limited clinicians and resources and the evidence that computed tomography (CT) analysis can achieve comparable sensitivity, specificity, and accuracy with reverse-transcription polymerase chain reaction, the automatic segmentation of lung infection from CT scans supplies a rapid and effective strategy for COVID-19 diagnosis, treatment, and follow-up. It is challenging because the infection appearance has high intraclass variation and interclass indistinction in CT slices. Therefore, a new context-aware neural network is proposed for lung infection segmentation. Specifically, the autofocus and panorama modules are designed for extracting fine details and semantic knowledge and capturing the long-range dependencies of the context from both peer level and cross level. Also, a novel structure consistency rectification is proposed for calibration by depicting the structural relationship between foreground and background. Experimental results on multiclass and single-class COVID-19 CT images demonstrate the effectiveness of our work. In particular, our method obtains the mean intersection over union (mIoU) score of 64.8\%, 65.2\%, and 73.8\% on three benchmark datasets for COVID-19 infection segmentation.},
  archive      = {J_TNNLS},
  author       = {Ruxin Wang and Chaojie Ji and Yuxiao Zhang and Ye Li},
  doi          = {10.1109/TNNLS.2021.3126305},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {12-24},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Focus, fusion, and rectify: Context-aware learning for COVID-19 lung infection segmentation},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep radiomic analysis for predicting coronavirus disease
2019 in computerized tomography and x-ray images. <em>TNNLS</em>,
<em>33</em>(1), 3–11. (<a
href="https://doi.org/10.1109/TNNLS.2021.3119071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes to encode the distribution of features learned from a convolutional neural network (CNN) using a Gaussian mixture model (GMM). These parametric features, called GMM-CNN, are derived from chest computed tomography (CT) and X-ray scans of patients with coronavirus disease 2019 (COVID-19). We use the proposed GMM-CNN features as input to a robust classifier based on random forests (RFs) to differentiate between COVID-19 and other pneumonia cases. Our experiments assess the advantage of GMM-CNN features compared with standard CNN classification on test images. Using an RF classifier (80\% samples for training; 20\% samples for testing), GMM-CNN features encoded with two mixture components provided a significantly better performance than standard CNN classification ( $p &amp;lt; 0.05$ ). Specifically, our method achieved an accuracy in the range of 96.00\%–96.70\% and an area under the receiver operator characteristic (ROC) curve in the range of 99.29\%–99.45\%, with the best performance obtained by combining GMM-CNN features from both CT and X-ray images. Our results suggest that the proposed GMM-CNN features could improve the prediction of COVID-19 in chest CT and X-ray scans.},
  archive      = {J_TNNLS},
  author       = {Ahmad Chaddad and Lama Hassan and Christian Desrosiers},
  doi          = {10.1109/TNNLS.2021.3119071},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {3-11},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep radiomic analysis for predicting coronavirus disease 2019 in computerized tomography and X-ray images},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial. <em>TNNLS</em>, <em>33</em>(1), 2. (<a
href="https://doi.org/10.1109/TNNLS.2021.3134174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Happy “New Year!” As you open this January issue of the IEEE Transactions on Neural Networks and Learning Systems (TNNLS), I hope everyone enjoyed a healthy and happy holiday season! At the beginning of 2022, it is my great honor and privilege to serve as the Editor-in-Chief (EiC) of IEEE TNNLS, and I am excited to write this Editorial to start a new journey with you all.},
  archive      = {J_TNNLS},
  author       = {Yongduan Song},
  doi          = {10.1109/TNNLS.2021.3134174},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  number       = {1},
  pages        = {2},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Editorial},
  volume       = {33},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
