<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq---62">JUQ - 62</h2>
<ul>
<li><details>
<summary>
(2022). A multilevel stochastic collocation method for schrödinger
equations with a random potential. <em>JUQ</em>, <em>10</em>(4),
1753–1780. (<a href="https://doi.org/10.1137/21M1440517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose and analyze a numerical method for time-dependent linear Schrödinger equations with uncertain parameters in both the potential and the initial data. The random parameters are discretized by stochastic collocation on a sparse grid, and the sample solutions in the nodes are approximated with the Strang splitting method. The computational work is reduced by a multilevel strategy, i.e., by combining information obtained from sample solutions computed on different refinement levels of the discretization. We prove new error bounds for the time discretization which take the finite regularity in the stochastic variable into account, and which are crucial to obtain convergence of the multilevel approach. The predicted cost savings of the multilevel stochastic collocation method are verified by numerical examples.},
  archive      = {J_JUQ},
  author       = {Tobias Jahnke and Benny Stein},
  doi          = {10.1137/21M1440517},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1753-1780},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A multilevel stochastic collocation method for schrödinger equations with a random potential},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Calibration of inexact computer models with heteroscedastic
errors. <em>JUQ</em>, <em>10</em>(4), 1733–1752. (<a
href="https://doi.org/10.1137/21M1417946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computer models are commonly used to represent a wide range of real systems, but they often involve some unknown parameters. Estimating the parameters by collecting experimental data becomes essential in many scientific fields, ranging from engineering to biology. However, most of the existing methods are developed under the assumption that the experimental data contains homoscedastic measurement errors. Motivated by an experiment of plant relative growth rates where replicates are available, we propose a new calibration method for inexact computer models with heteroscedastic measurement errors. Asymptotic properties of the parameter estimators are derived which can be used to quantify the uncertainty of the estimates, and a goodness-of-fit test is developed to detect the presence of heteroscedasticity. Numerical examples and empirical studies demonstrate that the proposed method not only yields accurate parameter estimation, but also provides accurate predictions for physical data in the presence of both heteroscedasticity and model misspecification. An R package for the proposed methodology is provided in an open repository.},
  archive      = {J_JUQ},
  author       = {Chih-Li Sung and Beau David Barber and Berkley J. Walker},
  doi          = {10.1137/21M1417946},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1733-1752},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Calibration of inexact computer models with heteroscedastic errors},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On negative transfer and structure of latent functions in
multioutput gaussian processes. <em>JUQ</em>, <em>10</em>(4), 1714–1732.
(<a href="https://doi.org/10.1137/21M1436816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The multioutput Gaussian process is based on the assumption that outputs share commonalities; however, if this assumption does not hold, negative transfer will lead to decreased performance relative to learning outputs independently or in subsets. In this article, we first define negative transfer in the context of and then derive necessary conditions for an model to avoid negative transfer. Specifically, under the convolution construction, we show that avoiding negative transfer is mainly dependent on having a sufficient number of latent functions regardless of the flexibility of the kernel or inference procedure used. However, a slight increase in leads to a large increase in the number of parameters to be estimated. To this end, we propose two latent structures which can scale to arbitrarily large datasets, can avoid negative transfer, and allow any kernel or sparse approximations to be used within. We also show that these structures allow regularization which can provide automatic selection of related outputs.},
  archive      = {J_JUQ},
  author       = {Moyan Li and Raed Kontar},
  doi          = {10.1137/21M1436816},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1714-1732},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On negative transfer and structure of latent functions in multioutput gaussian processes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scaling up bayesian uncertainty quantification for inverse
problems using deep neural networks. <em>JUQ</em>, <em>10</em>(4),
1684–1713. (<a href="https://doi.org/10.1137/21M1439456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Due to the importance of uncertainty quantification (UQ), the Bayesian approach to inverse problems has recently gained popularity in applied mathematics, physics, and engineering. However, traditional Bayesian inference methods based on Markov chain Monte Carlo (MCMC) tend to be computationally intensive and inefficient for such high-dimensional problems. To address this issue, several methods based on surrogate models have been proposed to speed up the inference process. More specifically, the calibration-emulation-sampling (CES) scheme has been proven to be successful in large dimensional UQ problems. In this work, we propose a novel CES approach for Bayesian inference based on deep neural network models for the emulation phase. The resulting algorithm is computationally more efficient and more robust against variations in the training set. Further, by using an autoencoder (AE) for dimension reduction, we have been able to speed up our Bayesian inference method up to three orders of magnitude. Overall, our method, henceforth called the dimension-reduced emulative autoencoder Monte Carlo (DREAMC) algorithm, is able to scale Bayesian UQ up to thousands of dimensions for inverse problems. Using two low-dimensional (linear and nonlinear) inverse problems, we illustrate the validity of this approach. Next, we apply our method to two high-dimensional numerical examples (elliptic and advection-diffusion) to demonstrate its computational advantages over existing algorithms.},
  archive      = {J_JUQ},
  author       = {Shiwei Lan and Shuyi Li and Babak Shahbaba},
  doi          = {10.1137/21M1439456},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1684-1713},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scaling up bayesian uncertainty quantification for inverse problems using deep neural networks},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Penalized projected kernel calibration for computer models.
<em>JUQ</em>, <em>10</em>(4), 1652–1683. (<a
href="https://doi.org/10.1137/20M1387614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Projected kernel calibration is a newly proposed frequentist calibration method, which is asymptotic normal and semiparametric. Its loss function is usually referred to as the projected kernel (PK) loss function. In this work, we prove the uniform convergence of PK loss function and show that (1) when the sample size is large, any local minimum point and local maximum point of the loss between the true process and the computer model is a local minimum point of the PK loss function and (2) all the local minima of the PK loss function converge to the same value. These theoretical results imply that it is extremely hard for the PK calibration to identify the global minimum of the loss, i.e., the optimal value of the calibration parameters. To solve this problem, a frequentist method which we term the penalized PK calibration method is suggested and analyzed in detail. We prove that the proposed method is as efficient as the PK calibration method. Through an extensive set of numerical simulations and a real-world case study, we show that the proposed calibration method can accurately estimate the calibration parameters. We also show that its performance compares favorably to other calibration methods regardless of the sample size.},
  archive      = {J_JUQ},
  author       = {Yan Wang},
  doi          = {10.1137/20M1387614},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1652-1683},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Penalized projected kernel calibration for computer models},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A locally adapted reduced-basis method for solving
risk-averse PDE-constrained optimization problems. <em>JUQ</em>,
<em>10</em>(4), 1629–1651. (<a
href="https://doi.org/10.1137/21M1411342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The numerical solution of risk-averse optimization problems constrained by PDEs requires substantial computational effort resulting from the discretization of the underlying PDE in both the physical and stochastic dimensions. To practically solve these challenging optimization problems, one must intelligently manage the individual discretization fidelities throughout the optimization iteration. In this work, we combine an inexact trust-region algorithm with the recently developed local reduced-basis approximation to efficiently solve risk-averse optimization problems with PDE constraints. The main contribution of this work is a numerical framework for systematically constructing surrogate models for the trust-region subproblem and the objective function using local reduced-basis approximations. We demonstrate the effectiveness of our approach through several numerical examples.},
  archive      = {J_JUQ},
  author       = {Zilong Zou and Drew P. Kouri and Wilkins Aquino},
  doi          = {10.1137/21M1411342},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1629-1651},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A locally adapted reduced-basis method for solving risk-averse PDE-constrained optimization problems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty quantification by multilevel monte carlo and
local time-stepping for wave propagation. <em>JUQ</em>, <em>10</em>(4),
1601–1628. (<a href="https://doi.org/10.1137/21M1429047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Because of their robustness, efficiency, and non intrusiveness, Monte Carlo methods are probably the most popular approach in uncertainty quantification for computing expected values of quantities of interest. Multilevel Monte Carlo (MLMC) methods significantly reduce the computational cost by distributing the sampling across a hierarchy of discretizations and allocating most samples to the coarser grids. For time dependent problems, spatial coarsening typically entails an increased time step. Geometric constraints, however, may impede uniform coarsening thereby forcing some elements to remain small across all levels. If explicit time-stepping is used, the time step will then be dictated by the smallest element on each level for numerical stability. Hence, the increasingly stringent CFL condition on the time step on coarser levels significantly reduces the advantages of the multilevel approach. To overcome that bottleneck we propose to combine the multilevel approach of MLMC with local time-stepping. By adapting the time step to the locally refined elements on each level, the efficiency of MLMC methods is restored even in the presence of complex geometry without sacrificing the explicitness and inherent parallelism. In a careful cost comparison, we quantify the reduction in computational cost for local refinement either inside a small fixed region or towards a reentrant corner.},
  archive      = {J_JUQ},
  author       = {Marcus J. Grote and Simon Michel and Fabio Nobile},
  doi          = {10.1137/21M1429047},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1601-1628},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification by multilevel monte carlo and local time-stepping for wave propagation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Test comparison for sobol indices over nested sets of
variables. <em>JUQ</em>, <em>10</em>(4), 1586–1600. (<a
href="https://doi.org/10.1137/21M1457370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sensitivity indices are commonly used to quantify the relative influence of any specific group of input variables on the output of a computer code. One crucial question is then to decide whether a given set of variables has a significant impact on the output. Sobol indices are often used to measure this impact but their estimation can be difficult as they usually require a particular design of experiment. In this work, we take advantage of the monotonicity of Sobol indices with respect to set inclusion to test the influence of some of the input variables. The method does not rely on a direct estimation of the Sobol indices and can be performed under classical independent and identically distributed sampling designs.},
  archive      = {J_JUQ},
  author       = {Thierry Klein and Nicolas Peteilh and Paul Rochet},
  doi          = {10.1137/21M1457370},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1586-1600},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Test comparison for sobol indices over nested sets of variables},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical finite elements via langevin dynamics.
<em>JUQ</em>, <em>10</em>(4), 1560–1585. (<a
href="https://doi.org/10.1137/21M1463094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The recent statistical finite element method (statFEM) provides a coherent statistical framework to synthesize finite element models with observed data. Through embedding uncertainty inside of the governing equations, finite element solutions are updated to give a posterior distribution which quantifies all sources of uncertainty associated with the model. However to incorporate all sources of uncertainty, one must integrate over the uncertainty associated with the model parameters, the known forward problem of uncertainty quantification. In this paper, we make use of Langevin dynamics to solve the statFEM forward problem, studying the utility of the unadjusted Langevin algorithm (ULA), a Metropolis-free Markov chain Monte Carlo sampler, to build a sample-based characterization of this otherwise intractable measure. Due to the structure of the statFEM problem, these methods are able to solve the forward problem without explicit full PDE solves, requiring only sparse matrix-vector products. ULA is also gradient-based, and hence provides a scalable approach up to high degrees-of-freedom. Leveraging the theory behind Langevin-based samplers, we provide theoretical guarantees on sampler performance, demonstrating convergence, for both the prior and posterior, in the Kullback–Leibler divergence and in Wasserstein-2, with further results on the effect of preconditioning. Numerical experiments are also provided, to demonstrate the efficacy of the sampler, with a Python package also included.},
  archive      = {J_JUQ},
  author       = {Ömer Deniz Akyildiz and Connor Duffin and Sotirios Sabanis and Mark Girolami},
  doi          = {10.1137/21M1463094},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1560-1585},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Statistical finite elements via langevin dynamics},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A variational inference approach to inverse problems with
gamma hyperpriors. <em>JUQ</em>, <em>10</em>(4), 1533–1559. (<a
href="https://doi.org/10.1137/21M146209X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Hierarchical models with gamma hyperpriors provide a flexible, sparsity-promoting framework to bridge and regularizations in Bayesian formulations to inverse problems. Despite the Bayesian motivation for these models, existing methodologies are limited to maximum a posteriori estimation. The potential to perform uncertainty quantification has not yet been realized. This paper introduces a variational iterative alternating scheme for hierarchical inverse problems with gamma hyperpriors. The proposed variational inference approach yields accurate reconstruction, provides meaningful uncertainty quantification, and is easy to implement. In addition, it lends itself naturally to conduct model selection for the choice of hyperparameters. We illustrate the performance of our methodology in several computed examples, including a deconvolution problem and sparse identification of dynamical systems from time series data.},
  archive      = {J_JUQ},
  author       = {Shiv Agrawal and Hwanwoo Kim and Daniel Sanz-Alonso and Alexander Strang},
  doi          = {10.1137/21M146209X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1533-1559},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A variational inference approach to inverse problems with gamma hyperpriors},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Projected wasserstein gradient descent for high-dimensional
bayesian inference. <em>JUQ</em>, <em>10</em>(4), 1513–1532. (<a
href="https://doi.org/10.1137/21M1454018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a projected Wasserstein gradient descent method (pWGD) for high-dimensional Bayesian inference problems. The underlying density function of a particle system of Wasserstein gradient descent (WGD) is approximated by kernel density estimation (KDE), which faces the long-standing curse of dimensionality. We overcome this challenge by exploiting the intrinsic low-rank structure in the difference between the posterior and prior distributions. The parameters are projected into a low-dimensional subspace to alleviate the approximation error of KDE in high dimensions. We formulate a projected Wasserstein gradient flow and analyze its convergence property under mild assumptions. Several numerical experiments illustrate the accuracy, convergence, and complexity scalability of pWGD with respect to parameter dimension, sample size, and processor cores.},
  archive      = {J_JUQ},
  author       = {Yifei Wang and Peng Chen and Wuchen Li},
  doi          = {10.1137/21M1454018},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1513-1532},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Projected wasserstein gradient descent for high-dimensional bayesian inference},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model uncertainty and correctability for directed graphical
models. <em>JUQ</em>, <em>10</em>(4), 1461–1512. (<a
href="https://doi.org/10.1137/21M1434453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Probabilistic graphical models are a fundamental tool in probabilistic modeling, machine learning, and artificial intelligence. They allow us to integrate in a natural way expert knowledge, physical modeling, heterogeneous and correlated data, and quantities of interest. For exactly this reason, multiple sources of model uncertainty are inherent within the modular structure of the graphical model. In this paper we develop information-theoretic, robust uncertainty quantification methods and nonparametric stress tests for directed graphical models to assess the effect and the propagation through the graph of multisourced model uncertainties to quantities of interest. These methods allow us to rank the different sources of uncertainty and correct the graphical model by targeting its most impactful components with respect to the quantities of interest. Thus, from a machine learning perspective, we provide a mathematically rigorous approach to correctability that guarantees a systematic selection for improvement of components of a graphical model while controlling potential new errors created in the process in other parts of the model. We demonstrate our methods in two physicochemical examples, namely, quantum scale-informed chemical kinetics and materials screening to improve the efficiency of fuel cells.},
  archive      = {J_JUQ},
  author       = {Panagiota Birmpa and Jinchao Feng and Markos A. Katsoulakis and Luc Rey-Bellet},
  doi          = {10.1137/21M1434453},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1461-1512},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Model uncertainty and correctability for directed graphical models},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A theoretical framework of the scaled gaussian stochastic
process in prediction and calibration. <em>JUQ</em>, <em>10</em>(4),
1435–1460. (<a href="https://doi.org/10.1137/21M1409949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Model calibration or data inversion is one of the fundamental tasks in uncertainty quantification. In this work, we study the theoretical properties of the scaled Gaussian stochastic process (S-GaSP) for modeling the discrepancy between reality and the imperfect mathematical model. We establish an explicit connection between the Gaussian stochastic process (GaSP) and S-GaSP through the orthogonal series representation. The predictive mean estimator in the S-GaSP calibration model converges to reality at the same rate as the GaSP with suitable choices of the regularization and scaling parameters. We also show that the calibrated mathematical model in the S-GaSP calibration converges to the one that minimizes the loss between reality and the mathematical model, whereas the GaSP model with other, widely used covariance functions does not have this property. Numerical examples confirm the excellent finite sample performance of our approaches.},
  archive      = {J_JUQ},
  author       = {Mengyang Gu and Fangzheng Xie and Long Wang},
  doi          = {10.1137/21M1409949},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1435-1460},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A theoretical framework of the scaled gaussian stochastic process in prediction and calibration},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general framework of rotational sparse approximation in
uncertainty quantification. <em>JUQ</em>, <em>10</em>(4), 1410–1434. (<a
href="https://doi.org/10.1137/21M1391602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper proposes a general framework for estimating coefficients of generalized polynomial chaos (gPC) used in uncertainty quantification (UQ) via rotational sparse approximation. In particular, we aim to identify a rotation matrix such that the gPC expansion of a set of random variables after the rotation has a sparser representation. However, this rotational approach alters the underlying linear system to be solved, which makes finding the sparse coefficients more difficult than in the case without rotation. To solve this problem, we examine several popular nonconvex regularizations in compressive sensing (CS) that perform better than the classic approach empirically. All these regularizations can be minimized by the alternating direction method of multipliers (ADMM). Numerical examples show superior performance of the proposed combination of rotation and nonconvex sparsity-promoting regularizations over those with and without rotation but using the convex approach.},
  archive      = {J_JUQ},
  author       = {Mengqi Hu and Yifei Lou and Xiu Yang},
  doi          = {10.1137/21M1391602},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1410-1434},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A general framework of rotational sparse approximation in uncertainty quantification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Superfloe parameterization with physics constraints for
uncertainty quantification of sea ice floes. <em>JUQ</em>,
<em>10</em>(4), 1384–1409. (<a
href="https://doi.org/10.1137/21M1428777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The discrete element method (DEM) provides a new modeling approach for describing sea ice dynamics. It exploits particle-based methods to characterize the physical quantities of each sea ice floe along its trajectory under Lagrangian coordinates. One major challenge in applying DEM models is the heavy computational cost when the number of floes becomes large. In this paper, an efficient Lagrangian parameterization algorithm is developed, which aims at reducing the computational cost of simulating the DEM models while preserving the key features of the sea ice. The new parameterization takes advantage of a small number of artificial ice floes, called the superfloes, to effectively approximate a considerable number of the floes, where the parameterization scheme satisfies several important physics constraints. The physics constraints guarantee the superfloe parameterized system will have short-term dynamical behavior similar to that of the full system. These constraints also allow the superfloe parameterized system to accurately quantify the long-range uncertainty, especially the non-Gaussian statistical features, of the full system. In addition, the superfloe parameterization facilitates a systematic noise inflation strategy that significantly advances an ensemble-based data assimilation algorithm for recovering the unobserved ocean field underneath the sea ice. Such a new noise inflation method avoids ad hoc tunings as in many traditional algorithms and is computationally extremely efficient. Numerical experiments based on an idealized DEM model with multiscale features illustrate the success of the superfloe parameterization in quantifying the uncertainty and assimilating both the sea ice and the associated ocean field.},
  archive      = {J_JUQ},
  author       = {Nan Chen and Quanling Deng and Samuel N. Stechmann},
  doi          = {10.1137/21M1428777},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1384-1409},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Superfloe parameterization with physics constraints for uncertainty quantification of sea ice floes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparative study of polynomial-type chaos expansions for
indicator functions. <em>JUQ</em>, <em>10</em>(4), 1350–1383. (<a
href="https://doi.org/10.1137/21M1413146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a thorough comparison of polynomial chaos expansion (PCE) for indicator functions of the form for some threshold parameter and a random variable associated with classical orthogonal polynomials. We provide tight global and localized estimates for the resulting truncation of the PCE, and numerical experiments support the tightness of the error estimates. We also compare the theoretical and numerical accuracy of PCE when extra quantile/probability transforms are applied, revealing different optimal choices according to the value of in the center and the tails of the distribution of .},
  archive      = {J_JUQ},
  author       = {Florian Bourgey and Emmanuel Gobet and Clément Rey},
  doi          = {10.1137/21M1413146},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1350-1383},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A comparative study of polynomial-type chaos expansions for indicator functions},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite element representations of gaussian processes:
Balancing numerical and statistical accuracy. <em>JUQ</em>,
<em>10</em>(4), 1323–1349. (<a
href="https://doi.org/10.1137/21M144788X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The stochastic partial differential equation approach to Gaussian processes (GPs) represents Matérn GP priors in terms of finite element basis functions and Gaussian coefficients with a sparse precision matrix. Such representations enhance the scalability of GP regression and classification to datasets of large size by setting and exploiting sparsity. In this paper we reconsider the standard choice through an analysis of the estimation performance. Our theory implies that, under certain smoothness assumptions, one can reduce the computation and memory cost without hindering the estimation accuracy by setting in the large asymptotics. Numerical experiments illustrate the applicability of our theory and the effect of the prior lengthscale in the preasymptotic regime.},
  archive      = {J_JUQ},
  author       = {Daniel Sanz-Alonso and Ruiyi Yang},
  doi          = {10.1137/21M144788X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1323-1349},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Finite element representations of gaussian processes: Balancing numerical and statistical accuracy},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum: “Existence and optimality conditions for
risk-averse PDE-constrained optimization.” <em>JUQ</em>, <em>10</em>(3),
1321–1322. (<a href="https://doi.org/10.1137/21M143251X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This corrigendum corrects a discussion in [D. P. Kouri and T. M. Surowiec, SIAM/ASA J. Uncertain. Quantif., 6 (2018), pp. 787–815].},
  archive      = {J_JUQ},
  author       = {Drew P. Kouri and Thomas M. Surowiec},
  doi          = {10.1137/21M143251X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1321-1322},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Corrigendum: “Existence and optimality conditions for risk-averse PDE-constrained optimization”},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid gibbs sampler for edge-preserving tomographic
reconstruction with uncertain view angles. <em>JUQ</em>, <em>10</em>(3),
1293–1320. (<a href="https://doi.org/10.1137/21M1412268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In computed tomography, data consist of measurements of the attenuation of X-rays passing through an object. The goal is to reconstruct the linear attenuation coefficient of the object’s interior. For each position of the X-ray source, characterized by its angle with respect to a fixed coordinate system, one measures a set of data referred to as a view. A common assumption is that these view angles are known, but in some applications they are known with imprecision. We propose a framework to solve a Bayesian inverse problem that jointly estimates the view angles and an image of the object’s attenuation coefficient. We also include a few hyperparameters that characterize the likelihood and the priors. Our approach is based on a Gibbs sampler where the associated conditional densities are simulated using different sampling schemes—hence the term hybrid. In particular, the conditional distribution associated with the reconstruction is nonlinear in the image pixels, and is non-Gaussian and high-dimensional. We approach this distribution by constructing a Laplace approximation that represents the target conditional locally at each Gibbs iteration. This enables sampling of the attenuation coefficients in an efficient manner using iterative reconstruction algorithms. The numerical results show that our algorithm is able to jointly identify the image and the view angles, while also providing uncertainty estimates of both. We demonstrate our method with 2D X-ray computed tomography problems using fan beam configurations.},
  archive      = {J_JUQ},
  author       = {Felipe Uribe and Johnathan M. Bardsley and Yiqiu Dong and Per Christian Hansen and Nicolai A. B. Riis},
  doi          = {10.1137/21M1412268},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1293-1320},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A hybrid gibbs sampler for edge-preserving tomographic reconstruction with uncertain view angles},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble approximate control variate estimators:
Applications to MultiFidelity importance sampling. <em>JUQ</em>,
<em>10</em>(3), 1250–1292. (<a
href="https://doi.org/10.1137/21M1390426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The recent growth in multifidelity uncertainty quantification has given rise to a large set of variance reduction techniques that leverage information from model ensembles to provide variance reduction for estimates of the statistics of a high-fidelity model. In this paper we provide two contributions: (1) we utilize an ensemble estimator to account for uncertainties in the optimal weights of approximate control variate (ACV) approaches and derive lower bounds on the number of samples required to guarantee variance reduction; and (2) we extend an existing multifidelity importance sampling (MFIS) scheme to leverage control variates. Our approach directly addresses a limitation of many multifidelity sampling strategies that require the usage of pilot samples to estimate covariances. As such we make significant progress towards both increasing the practicality of approximate control variates—for instance, by accounting for the effect of pilot samples—and using multifidelity approaches more effectively for estimating low-probability events. The numerical results indicate our hybrid MFIS-ACV estimator achieves up to 50\% improvement in variance reduction over the existing state-of-the-art MFIS estimator, which had already shown an outstanding convergence rate compared to the Monte Carlo method, on several problems of computational mechanics.},
  archive      = {J_JUQ},
  author       = {Trung Pham and Alex A. Gorodetsky},
  doi          = {10.1137/21M1390426},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1250-1292},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Ensemble approximate control variate estimators: Applications to MultiFidelity importance sampling},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampling-based spotlight SAR image reconstruction from phase
history data for speckle reduction and uncertainty quantification.
<em>JUQ</em>, <em>10</em>(3), 1225–1249. (<a
href="https://doi.org/10.1137/20M1379721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Spotlight mode airborne synthetic aperture radar (SAR) is a coherent imaging modality that is an important tool in remote sensing. Existing methods for spotlight SAR image reconstruction from phase history data typically produce a single image estimate which approximates the reflectivity of an unknown ground scene, and therefore provide no quantification of the certainty with which the estimate can be trusted. In addition, speckle affects all coherent imaging modalities causing a degradation of image quality. Many point estimate image reconstruction methods incorrectly treat speckle as additive noise resulting in an unnatural smoothing of the speckle that also reduces image contrast. The purpose of this paper is to address the issues of speckle and uncertainty quantification by introducing a sampling-based approach to SAR image reconstruction directly from phase history data. In particular, a statistical model for speckle as well as a corresponding sparsity technique to reduce it are directly incorporated into the model. Rather than a single point estimate, samples of the resulting joint posterior density are efficiently obtained using a Gibbs sampler, which are in turn used to derive estimates and other statistics which aid in uncertainty quantification. The latter information is particularly important in SAR, where ground truth images even for synthetically created examples are typically unknown. While similar methods have been deployed to process formed images, this paper focuses on the integration of these techniques into image reconstruction from phase history data. An example result using real-world data shows that, when compared with existing methods, the sampling-based approach introduced provides parameter-free estimates with improved contrast and significantly reduced speckle, as well as uncertainty quantification information.},
  archive      = {J_JUQ},
  author       = {Victor Churchill and Anne Gelb},
  doi          = {10.1137/20M1379721},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1225-1249},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sampling-based spotlight SAR image reconstruction from phase history data for speckle reduction and uncertainty quantification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rank bounds for approximating gaussian densities in the
tensor-train format. <em>JUQ</em>, <em>10</em>(3), 1191–1224. (<a
href="https://doi.org/10.1137/20M1314653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Low-rank tensor approximations have shown great potential for uncertainty quantification in high dimensions, for example, to build surrogate models that can be used to speed up large-scale inference problems [M. Eigel, M. Marschall, and R. Schneider, Inverse Problems, 34 (2018), 035010; S. Dolgov et al., Stat. Comput., 30 (2020), pp. 603–625]. The feasibility and efficiency of such approaches depends critically on the rank that is necessary to represent or approximate the underlying distribution. In this paper, a priori rank bounds for approximations in the functional Tensor-Train representation for the case of Gaussian models are developed. It is shown that under suitable conditions on the precision matrix, the Gaussian density can be approximated to high accuracy without suffering from an exponential growth of complexity as the dimension increases. These results provide a rigorous justification of the suitability and the limitations of low-rank tensor methods in a simple but important model case. Numerical experiments confirm that the rank bounds capture the qualitative behavior of the rank structure when varying the parameters of the precision matrix and the accuracy of the approximation. Finally, the practical relevance of the theoretical results is demonstrated in the context of a Bayesian filtering problem.},
  archive      = {J_JUQ},
  author       = {Paul B. Rohrbach and Sergey Dolgov and Lars Grasedyck and Robert Scheichl},
  doi          = {10.1137/20M1314653},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1191-1224},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Rank bounds for approximating gaussian densities in the tensor-train format},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic normalizing flows for inverse problems: A markov
chains viewpoint. <em>JUQ</em>, <em>10</em>(3), 1162–1190. (<a
href="https://doi.org/10.1137/21M1450604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. To overcome topological constraints and improve the expressiveness of normalizing flow architectures, Wu, Köhler, and Noé introduced stochastic normalizing flows which combine deterministic, learnable flow transformations with stochastic sampling methods. In this paper, we consider stochastic normalizing flows from a Markov chain point of view. In particular, we replace transition densities by general Markov kernels and establish proofs via Radon–Nikodym derivatives, which allows us to incorporate distributions without densities in a sound way. Further, we generalize the results for sampling from posterior distributions as required in inverse problems. The performance of the proposed conditional stochastic normalizing flow is demonstrated by numerical examples.},
  archive      = {J_JUQ},
  author       = {Paul Hagemann and Johannes Hertrich and Gabriele Steidl},
  doi          = {10.1137/21M1450604},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1162-1190},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stochastic normalizing flows for inverse problems: A markov chains viewpoint},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lagged particle filter for stable filtering of certain
high-dimensional state-space models. <em>JUQ</em>, <em>10</em>(3),
1130–1161. (<a href="https://doi.org/10.1137/21M1450392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of high-dimensional filtering of state-space models (SSMs) at discrete times. This problem is particularly challenging as analytical solutions are typically not available and many numerical approximation methods can have a cost that scales exponentially with the dimension of the hidden state. Inspired by lag-approximation methods for the smoothing problem [G. Kitagawa and S. Sato, Monte Carlo smoothing and self-organising state-space model, in Sequential Monte Carlo Methods in Practice, Springer, New York, 2001, pp. 178–195; J. Olsson et al., Bernoulli, 14 (2008), pp. 155–179], we introduce a lagged approximation of the smoothing distribution that is necessarily biased. For certain classes of SSMs, particularly those that forget the initial condition exponentially fast in time, the bias of our approximation is shown to be uniformly controlled in the dimension and exponentially small in time. We develop a sequential Monte Carlo (SMC) method to recursively estimate expectations with respect to our biased filtering distributions. Moreover, we prove for a class of SSMs that can contain dependencies amongst coordinates that as the dimension the cost to achieve a stable mean square error in estimation, for classes of expectations, is of per unit time, where is the number of simulated samples in the SMC algorithm. Our methodology is implemented on several challenging high-dimensional examples including the conservative shallow-water model.},
  archive      = {J_JUQ},
  author       = {Hamza Ruzayqat and Aimad Er-raiy and Alexandros Beskos and Dan Crisan and Ajay Jasra and Nikolas Kantas},
  doi          = {10.1137/21M1450392},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1130-1161},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A lagged particle filter for stable filtering of certain high-dimensional state-space models},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic galerkin methods for linear stability analysis of
systems with parametric uncertainty. <em>JUQ</em>, <em>10</em>(3),
1101–1129. (<a href="https://doi.org/10.1137/21M1415595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a method for linear stability analysis of systems with parametric uncertainty formulated in the stochastic Galerkin framework. Specifically, we assume that for a model partial differential equation, the parameter is given in the form of generalized polynomial chaos expansion. The stability analysis leads to the solution of a stochastic eigenvalue problem, and we wish to characterize the rightmost eigenvalue. We focus, in particular, on problems with nonsymmetric matrix operators, for which the eigenvalue of interest may be a complex conjugate pair, and we develop methods for their efficient solution. These methods are based on inexact, line-search Newton iteration, which entails use of preconditioned GMRES. The method is applied to linear stability analysis of the Navier–Stokes equations with stochastic viscosity, its accuracy is compared to that of Monte Carlo and stochastic collocation, and the efficiency is illustrated by numerical experiments.},
  archive      = {J_JUQ},
  author       = {Bedřich Sousedík and Kookjin Lee},
  doi          = {10.1137/21M1415595},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1101-1129},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stochastic galerkin methods for linear stability analysis of systems with parametric uncertainty},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse online variational bayesian regression. <em>JUQ</em>,
<em>10</em>(3), 1070–1100. (<a
href="https://doi.org/10.1137/21M1401188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work considers variational Bayesian inference as an inexpensive and scalable alternative to a fully Bayesian approach in the context of sparsity-promoting priors. In particular, the priors considered arise from scale mixtures of normal distributions with a generalized inverse Gaussian mixing distribution. This includes the variational Bayesian LASSO as an inexpensive and scalable alternative to the Bayesian LASSO introduced in T. Park and G. Casella [J. Amer. Statist. Assoc., 103 (2008), pp. 681–686]. It also includes a family of priors which more strongly promote sparsity. For linear models the method requires only the iterative solution of deterministic least squares problems. Furthermore, for unknown covariates the method can be implemented exactly online with a cost of in computation and in memory per iteration—in other words, the cost per iteration is independent of , and in principle infinite data can be considered. For large an approximation is able to achieve promising results for a cost of per iteration in both computation and memory. Strategies for hyperparameter tuning are also considered. The method is implemented for real and simulated data. It is shown that the performance in terms of variable selection and uncertainty quantification of the variational Bayesian LASSO can be comparable to the Bayesian LASSO for problems which are tractable with that method and for a fraction of the cost. The present method comfortably handles , on a laptop in less than 30 minutes, and , overnight.},
  archive      = {J_JUQ},
  author       = {Kody J. H. Law and Vitaly Zankin},
  doi          = {10.1137/21M1401188},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1070-1100},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sparse online variational bayesian regression},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Goal-oriented shapley effects with special attention to the
quantile-oriented case. <em>JUQ</em>, <em>10</em>(3), 1037–1069. (<a
href="https://doi.org/10.1137/21M1395247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose to study quantile-oriented sensitivity analysis (QOSA) indices and show some of their theoretical properties. These have a number of shortcomings when dealing with both independent and dependent inputs, which leads us to define new generic indices based on the Shapley values named goal-oriented Shapley effects (GOSE). In particular, we focus on quantile-oriented Shapley effects (QOSE) and subsequently perform several calculations of QOSA indices and QOSE in order to better understand the behavior and the respective interest of each.},
  archive      = {J_JUQ},
  author       = {Kevin Elie-Dit-Cosaque and Veronique Maume-Deschamps},
  doi          = {10.1137/21M1395247},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1037-1069},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Goal-oriented shapley effects with special attention to the quantile-oriented case},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic theory of <span
class="math inline"><strong>ℓ</strong><sub>1</sub></span> -regularized
PDE identification from a single noisy trajectory. <em>JUQ</em>,
<em>10</em>(3), 1012–1036. (<a
href="https://doi.org/10.1137/21M1398884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide a formal theoretical analysis on the PDE identification via the -regularized pseudo least square method from the statistical point of view. In this article, we assume that the differential equation governing the dynamic system can be represented as a linear combination of various linear and nonlinear differential terms. Under noisy observations, we employ local-polynomial fitting for estimating state variables and apply the penalty for model selection. Our theory proves that the classical mutual incoherence condition on the feature matrix and the -condition for the ground-truth signal are sufficient for the signed-support recovery of the -PsLS method. We run numerical experiments on two popular PDE models, the viscous Burgers and the Korteweg–de Vries (KdV) equations, and the results from the experiments corroborate our theoretical predictions.},
  archive      = {J_JUQ},
  author       = {Yuchen He and Namjoon Suh and Xiaoming Huo and Sung Ha Kang and Yajun Mei},
  doi          = {10.1137/21M1398884},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1012-1036},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Asymptotic theory of \(\boldsymbol \ell _1\) -regularized PDE identification from a single noisy trajectory},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural network surrogates for nonsmooth quantities of
interest in shape uncertainty quantification. <em>JUQ</em>,
<em>10</em>(3), 975–1011. (<a
href="https://doi.org/10.1137/21M1393078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the point evaluation of the solution to interface problems with geometric uncertainties, where the uncertainty in the obstacle is described by a high-dimensional parameter , . We focus in particular on an elliptic interface problem and a Helmholtz transmission problem. Point values of the solution in the physical domain depend in general nonsmoothly on the high-dimensional parameter, posing a challenge when one is interested in building surrogates. Indeed, high-order methods show poor convergence rates, while methods which are able to track discontinuities usually suffer from the so-called curse of dimensionality. For this reason, in this work we propose to build surrogates for point evaluation using deep neural networks. We provide a theoretical justification for why we expect neural networks to provide good surrogates. Furthermore, we present extensive numerical experiments showing their good performance in practice. We observe in particular that neural networks do not suffer from the curse of dimensionality, and we study the dependence of the error on the number of point evaluations (that is, the number of discontinuities in the parameter space), as well as on several modeling parameters, such as the contrast between the two materials and, for the Helmholtz transmission problem, the wavenumber.},
  archive      = {J_JUQ},
  author       = {Laura Scarabosio},
  doi          = {10.1137/21M1393078},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {975-1011},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Deep neural network surrogates for nonsmooth quantities of interest in shape uncertainty quantification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An inverse random source problem for the biharmonic wave
equation. <em>JUQ</em>, <em>10</em>(3), 949–974. (<a
href="https://doi.org/10.1137/21M1429138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper is concerned with an inverse source problem for the stochastic biharmonic wave equation. The driven source is assumed to be a microlocally isotropic Gaussian random field with its covariance operator being a classical pseudo-differential operator. The well-posedness of the direct problem is examined in the distribution sense, and the regularity of the solution is discussed for the given rough source. For the inverse problem, the strength of the random source, involved in the principal symbol of its covariance operator, is shown to be uniquely determined by a single realization of the magnitude of the wave field averaged over the frequency band with probability one. Numerical experiments are presented to illustrate the validity and effectiveness of the proposed method for the case that the random source is white noise.},
  archive      = {J_JUQ},
  author       = {Peijun Li and Xu Wang},
  doi          = {10.1137/21M1429138},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {949-974},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An inverse random source problem for the biharmonic wave equation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein sensitivity of risk and uncertainty propagation.
<em>JUQ</em>, <em>10</em>(3), 915–948. (<a
href="https://doi.org/10.1137/20M1325459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. When propagating uncertainty in the data of differential equations, the probability laws describing the uncertainty are typically themselves subject to uncertainty. We present a sensitivity analysis of uncertainty propagation for differential equations with random inputs to perturbations of the input measures. We focus on the elliptic diffusion equation with random coefficient and source term, for which the probability measure of the solution random field is shown to be Lipschitz-continuous in both total variation and Wasserstein distance. The result generalizes to the solution map of any differential equation with locally Hölder dependence on input parameters. In addition, these results extend to Lipschitz-continuous quantities of interest of the solution as well as to coherent risk functionals of these applied to evaluate the impact of their uncertainty. Our analysis is based on the sensitivity of risk functionals and pushforward measures for locally Hölder mappings with respect to the Wasserstein distance of perturbed input distributions. The established results are applied, in particular, to the case of lognormal diffusion and the truncation of series representations of input random fields.},
  archive      = {J_JUQ},
  author       = {Oliver G. Ernst and Alois Pichler and Björn Sprungk},
  doi          = {10.1137/20M1325459},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {915-948},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Wasserstein sensitivity of risk and uncertainty propagation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuum covariance propagation for understanding variance
loss in advective systems. <em>JUQ</em>, <em>10</em>(3), 886–914. (<a
href="https://doi.org/10.1137/21M1442449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Motivated by the spurious variance loss encountered during covariance propagation in atmospheric and other large-scale data assimilation systems, we consider the problem for state dynamics governed by the continuity and related hyperbolic partial differential equations. This loss of variance has been attributed to reduced-rank representations of the covariance matrix, as in ensemble methods for example, or else to the use of dissipative numerical methods. Through a combination of analytical work and numerical experiments, we demonstrate that significant variance loss, as well as gain, typically occurs during covariance propagation, even at full rank. The cause of this unusual behavior is a discontinuous change in the continuum covariance dynamics as correlation lengths become small, for instance in the vicinity of sharp gradients in the velocity field. This discontinuity in the covariance dynamics arises from hyperbolicity: the diagonal of the kernel of the covariance operator is a characteristic surface for advective dynamics. Our numerical experiments demonstrate that standard numerical methods for evolving the state are not adequate for propagating the covariance, because they do not capture the discontinuity in the continuum covariance dynamics as correlations lengths tend to zero. Our analytical and numerical results show that this leads to significant, spurious variance loss in certain regions and gain in others. The results suggest that developing local covariance propagation methods designed specifically to capture covariance evolution near the diagonal may prove a useful alternative to current methods of covariance propagation.},
  archive      = {J_JUQ},
  author       = {Shay Gilpin and Tomoko Matsuo and Stephen E. Cohn},
  doi          = {10.1137/21M1442449},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {886-914},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Continuum covariance propagation for understanding variance loss in advective systems},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble markov chain monte carlo with teleporting walkers.
<em>JUQ</em>, <em>10</em>(3), 860–885. (<a
href="https://doi.org/10.1137/21M1425062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce an ensemble Markov chain Monte Carlo approach to sampling from a probability density with known likelihood. This method upgrades an underlying Markov chain by allowing an ensemble of such chains to interact via a process in which one chain’s state is cloned as another’s is deleted. This effective teleportation of states can overcome issues of metastability in the underlying chain, as the scheme enjoys rapid mixing once the modes of the target density have been populated. We derive a mean-field limit for the evolution of the ensemble. We analyze the global and local convergence of this mean-field limit, showing asymptotic convergence independent of the spectral gap of the underlying Markov chain, and moreover we interpret the limiting evolution as a gradient flow. We explain how interaction can be applied selectively to a subset of state variables in order to maintain advantage on very high-dimensional problems. Finally, we present the application of our methodology to Bayesian hyperparameter estimation for Gaussian process regression.},
  archive      = {J_JUQ},
  author       = {Michael Lindsey and Jonathan Weare and Anna Zhang},
  doi          = {10.1137/21M1425062},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {860-885},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Ensemble markov chain monte carlo with teleporting walkers},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Objective frequentist uncertainty quantification for
atmospheric <span class="math inline">CO<sub>2</sub></span> retrievals.
<em>JUQ</em>, <em>10</em>(3), 827–859. (<a
href="https://doi.org/10.1137/20M1356403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The steadily increasing amount of atmospheric carbon dioxide is affecting the global climate system and threatening the long-term sustainability of Earth’s ecosystem. In order to better understand the sources and sinks of , NASA operates the Orbiting Carbon Observatory-2 and -3 satellites to monitor from space. These satellites make passive radiance measurements of the sunlight reflected off the Earth’s surface in different spectral bands, which are then inverted in an ill-posed inverse problem to obtain estimates of the atmospheric concentration. In this work, we propose a new retrieval method that uses known physical constraints on the state variables and direct inversion of the target functional of interest to construct well-calibrated frequentist confidence intervals based on convex programming. We compare the method with the current operational retrieval procedure, which uses prior knowledge in the form of probability distributions to regularize the problem. We demonstrate that the proposed intervals consistently achieve the desired frequentist coverage, while the operational uncertainties are poorly calibrated in a frequentist sense both at individual locations and over a spatial region in a realistic simulation experiment. We also study the influence of specific nuisance state variables on the length of the proposed intervals and identify certain key variables that can greatly reduce the final uncertainty given additional deterministic or probabilistic constraints. We then develop a principled framework to incorporate such additional information into our method.},
  archive      = {J_JUQ},
  author       = {Pratik Patil and Mikael Kuusela and Jonathan Hobbs},
  doi          = {10.1137/20M1356403},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {827-859},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Objective frequentist uncertainty quantification for atmospheric \(\mathrm{CO}_2\) retrievals},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monte carlo methods for the neutron transport equation.
<em>JUQ</em>, <em>10</em>(2), 775–825. (<a
href="https://doi.org/10.1137/21M1390578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper continues our treatment of the neutron transport equation (NTE), building on the work in [A. M. G. Cox et al., J. Stat. Phys., 176 (2019), pp. 425–455; E. Horton, A. E. Kyprianou, and D. Villemonais, Ann Appl. Probab., 30 (2020), pp. 2573–2612; and S. C. Harris, E. Horton, and A. E. Kyprianou, Ann. Appl. Probab., 30 (2020), pp. 2815–2845], which describes the density (equivalently, flux) of neutrons through inhomogeneous fissile media. Our aim is to analyze existing and novel Monte Carlo (MC) algorithms, aimed at simulating the lead eigenvalue associated with the underlying model. This quantity is of principal importance in the nuclear regulatory industry, for which the NTE must be solved on complicated inhomogeneous domains corresponding to nuclear reactor cores, irradiative hospital equipment, food irradiation equipment, and so on. We include a complexity analysis of such MC algorithms, noting that no such undertaking has previously appeared in the literature. The new MC algorithms offer a variety of advantages and disadvantages of accuracy versus cost, as well as the possibility of more convenient computational parallelization.},
  archive      = {J_JUQ},
  author       = {Alexander M. G. Cox and Simon C. Harris and Andreas E. Kyprianou and Minmin Wang},
  doi          = {10.1137/21M1390578},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {775-825},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Monte carlo methods for the neutron transport equation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Empirical bayesian inference using a support informed prior.
<em>JUQ</em>, <em>10</em>(2), 745–774. (<a
href="https://doi.org/10.1137/21M140794X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper develops a new empirical Bayesian inference algorithm for solving a linear inverse problem given multiple measurement vectors of noisy observable data. Specifically, by exploiting the joint sparsity across the multiple measurements in the sparse domain of the underlying signal or image, we construct a new support informed prior. Several applications can be modeled using this framework, including synthetic aperture radar observations using nearby azimuth angles and parallel magnetic resonance imaging. Our numerical experiments suggest that using the support informed prior usually improves accuracy of the recovery in the form of the sampled posterior mean and reduces its uncertainty when compared to posteriors constructed using some more standard priors.},
  archive      = {J_JUQ},
  author       = {Jiahui Zhang and Anne Gelb and Theresa Scarnati},
  doi          = {10.1137/21M140794X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {745-774},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Empirical bayesian inference using a support informed prior},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantifying spatio-temporal boundary condition uncertainty
for the north american deglaciation. <em>JUQ</em>, <em>10</em>(2),
717–744. (<a href="https://doi.org/10.1137/21M1409135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Ice sheet models are used to study the deglaciation of North America at the end of the last ice age (past 21,000 years), so that we might understand whether and how existing ice sheets may reduce or disappear under climate change. Though ice sheet models have a few parameters controlling physical behavior of the ice mass, they also require boundary conditions for climate (spatio-temporal fields of temperature and precipitation, typically on regular grids and at monthly intervals). The behavior of the ice sheet is highly sensitive to these fields, and there is relatively little data from geological records to constrain them as the land was covered with ice. We develop a methodology for generating a range of plausible boundary conditions, using a low-dimensional basis representation of the spatio-temporal input. We derive this basis by combining key patterns, extracted from a small ensemble of climate model simulations of the deglaciation, with sparse spatio-temporal observations. By jointly varying the ice sheet parameters and basis vector coefficients, we run ensembles of the Glimmer ice sheet model that simultaneously explore both climate and ice sheet model uncertainties. We use these to calibrate the ice sheet physics and boundary conditions for Glimmer by ruling out regions of the joint coefficient and parameter space via history matching. We use binary ice/no ice observations from reconstructions of past ice sheet margin position to constrain this space by introducing a novel metric for history matching to binary data.},
  archive      = {J_JUQ},
  author       = {James M. Salter and Daniel B. Williamson and Lauren J. Gregoire and Tamsin L. Edwards},
  doi          = {10.1137/21M1409135},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {717-744},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quantifying spatio-temporal boundary condition uncertainty for the north american deglaciation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-adapted optimal experimental design. <em>JUQ</em>,
<em>10</em>(2), 687–716. (<a
href="https://doi.org/10.1137/20M1357615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Constructing accurate statistical models of critical system responses typically requires an enormous amount of experimental data. Unfortunately, physical experimentation is often expensive and time consuming. Optimal experimental design determines the ``best&quot; allocation of experiments with respect to a criterion that measures the ability to estimate some important aspect of an assumed statistical model. While optimal design has a vast literature, few researchers have developed design paradigms targeting tail statistics, such as quantiles. In this paper, we introduce a new optimality criterion, R-optimality, that attempts to minimize the risk associated with large prediction variances. The R-optimality criterion generalizes the classical I- and G-optimality criteria and can be tailored to the risk preferences of stakeholders. We discuss numerical methods for the case when the design is supported on a finite number of points. This happens if there are only finitely many experimental configurations or if the design space is discretized. In the latter case, we prove consistency of the approximation as the number of design points increases. We demonstrate the R-optimality criterion on various numerical examples, including the calibration of a polynomial model using least-squares and quantile regression, the calibration of the nonlinear Michaelis–Menten model, and microphone placement for direct field acoustic testing—a technique used to test engineered structures in vibration environments by subjecting them to intense acoustic pressure.},
  archive      = {J_JUQ},
  author       = {Drew P. Kouri and John D. Jakeman and J. Gabriel Huerta},
  doi          = {10.1137/20M1357615},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {687-716},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Risk-adapted optimal experimental design},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extrapolated polynomial lattice rule integration in
computational uncertainty quantification. <em>JUQ</em>, <em>10</em>(2),
651–686. (<a href="https://doi.org/10.1137/20M1338137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present an extension of the convergence analysis for Richardson-extrapolated polynomial lattice rules from [J. Dick, T. Goda, and T. Yoshiki, SIAM J. Numer. Anal., 57 (2019), pp. 44–69] for high-dimensional, numerical integration to classes of integrand functions with quantified smoothness and quasi–Monte Carlo (QMC) integration rules with so-called smoothness-driven, product and order dependent (SPOD) weights. We establish in particular sufficient conditions for the existence of an asymptotic expansion of the QMC integration error with respect to suitable powers of N, the number of QMC integration nodes. We derive a dimension-separated criterion for a fast component-by-component (CBC) construction algorithm for the computation of the QMC generating vector with quadratic scaling with respect to the integration dimension s. We prove that the proposed QMC integration strategies (a) are free from the curse of dimensionality, (b) afford higher-order convergence rates subject to suitable summability conditions on the QMC weights, (c) allow for certain classes of high-dimensional integrand functions a computable, asymptotically exact numerical estimate of the QMC quadrature error, with reliability and efficiency independent of the dimension of the integration domain, and (d) accommodate fast, FFT-based matrix-vector multiplication from [J. Dick, F. Y. Kuo, Q. T. Le Gia, C. Schwab, SIAM J. Sci. Comput., 37 (2015), pp. A1436–A1450] when applied to parametric operator equations. The integration methods are applicable for large classes of many-parametric integrand functions with quantified parametric smoothness. We verify all hypotheses and present numerical examples arising from the Galerkin finite-element discretization of a model linear parametric elliptic PDE illustrating (a)–(d). We verify computationally the scaling of the fast CBC construction algorithm with SPOD QMC weights and examine the extrapolation-based a posteriori numerical estimation of the QMC quadrature error. We find in examples with parameter spaces of dimension s = 10, …, 128 that the extrapolation-based, computable QMC integration error indicator has an efficiency index between 0.9 and 1.1 for a moderate number N of QMC points.},
  archive      = {J_JUQ},
  author       = {Josef Dick and Marcello Longo and Christoph Schwab},
  doi          = {10.1137/20M1338137},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {651-686},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Extrapolated polynomial lattice rule integration in computational uncertainty quantification},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian processes with input location error and
applications to the composite parts assembly process. <em>JUQ</em>,
<em>10</em>(2), 619–650. (<a
href="https://doi.org/10.1137/20M1312447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper investigates Gaussian process modeling with input location error, where the inputs are corrupted by noise. Here, the best linear unbiased predictor for two cases is considered, according to whether there is noise at the target location or not. We show that the mean squared prediction error converges to a nonzero constant if there is noise at the target location, and we provide an upper bound of the mean squared prediction error if there is no noise at the target location. We investigate the use of stochastic Kriging in the prediction of Gaussian processes with input location error and show that stochastic Kriging is a good approximation when the sample size is large. Several numerical examples are given to illustrate the results, and a case study on the assembly of composite parts is presented. Technical proofs are provided in the appendices.},
  archive      = {J_JUQ},
  author       = {Wenjia Wang and Xiaowei Yue and Benjamin Haaland and C. F. Jeff Wu},
  doi          = {10.1137/20M1312447},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {619-650},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Gaussian processes with input location error and applications to the composite parts assembly process},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multilevel ensemble kalman–bucy filters. <em>JUQ</em>,
<em>10</em>(2), 584–618. (<a
href="https://doi.org/10.1137/21M1423762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this article we consider the linear filtering problem in continuous time. We develop and apply multilevel Monte Carlo (MLMC) strategies for ensemble Kalman–Bucy filters (EnKBFs). These filters can be viewed as approximations of conditional McKean–Vlasov-type diffusion processes. They are also interpreted as the continuous-time analogue of the ensemble Kalman filter, which has proven to be successful due to its applicability and computational cost. We prove that an ideal version of our multilevel EnKBF can achieve a mean square error (MSE) of , with a cost of order . In order to prove this result we provide a Monte Carlo convergence and approximation bounds associated to time-discretized EnKBFs. This implies a reduction in cost compared to the (single level) EnKBF which requires a cost of to achieve an MSE of . We test our theory on a linear Ornstein–Uhlenbeck process, which we motivate through high-dimensional examples of order and , where we also numerically test an alternative deterministic counterpart of the EnKBF.},
  archive      = {J_JUQ},
  author       = {Neil K. Chada and Ajay Jasra and Fangyuan Yu},
  doi          = {10.1137/21M1423762},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {584-618},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel ensemble Kalman–Bucy filters},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative stochastic modeling of strongly nonlinear flows
with non-gaussian statistics. <em>JUQ</em>, <em>10</em>(2), 555–583. (<a
href="https://doi.org/10.1137/20M1359833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Strongly nonlinear flows, which commonly arise in geophysical and engineering turbulence, are characterized by persistent and intermittent energy transfer between various spatial and temporal scales. These systems are difficult to model and analyze due to combination of high dimensionality and uncertainty, and there has been much interest in obtaining reduced models, in the form of stochastic closures, which can replicate their non-Gaussian statistics in many dimensions. Here, we propose a data-driven framework to model stationary chaotic dynamical systems through nonlinear transformations and a set of decoupled stochastic differential equations (SDEs). Specifically, we use optimal transport to find a transformation from the distribution of time-series data to a multiplicative reference probability measure such as the standard normal distribution. Then we find the set of decoupled SDEs that admit the reference measure as the invariant measure, and also closely match the spectrum of the transformed data. As such, this framework represents the chaotic time series as the evolution of a stochastic system observed through the lens of a nonlinear map. We demonstrate the application of this framework in the Lorenz-96 system, a 10-dimensional model of high-Reynolds cavity flow, and reanalysis climate data. These examples show that SDE models generated by this framework can reproduce the non-Gaussian statistics of systems with moderate dimensions (e.g., 10 and more) and predict super-Gaussian tails that are not readily available from little training data. These findings suggest that this class of models provides an efficient hypothesis space for learning strongly nonlinear flows from small amounts of data.},
  archive      = {J_JUQ},
  author       = {Hassan Arbabi and Themistoklis Sapsis},
  doi          = {10.1137/20M1359833},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {555-583},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Generative stochastic modeling of strongly nonlinear flows with non-gaussian statistics},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scaled vecchia approximation for fast computer-model
emulation. <em>JUQ</em>, <em>10</em>(2), 537–554. (<a
href="https://doi.org/10.1137/20M1352156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many scientific phenomena are studied using computer experiments consisting of multiple runs of a computer model while varying the input settings. Gaussian processes (GPs) are a popular tool for the analysis of computer experiments, enabling interpolation between input settings, but direct GP inference is computationally infeasible for large datasets. We adapt and extend a powerful class of GP methods from spatial statistics to enable the scalable analysis and emulation of large computer experiments. Specifically, we apply Vecchia’s ordered conditional approximation in a transformed input space, with each input scaled according to how strongly it relates to the computer-model response. The scaling is learned from the data by estimating parameters in the GP covariance function using Fisher scoring. Our methods are highly scalable, enabling estimation, joint prediction, and simulation in near-linear time in the number of model runs. In several numerical examples, our approach substantially outperformed existing methods.},
  archive      = {J_JUQ},
  author       = {Matthias Katzfuss and Joseph Guinness and Earl Lawrence},
  doi          = {10.1137/20M1352156},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {537-554},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scaled vecchia approximation for fast computer-model emulation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A stochastic levenberg–marquardt method using random models
with complexity results. <em>JUQ</em>, <em>10</em>(1), 507–536. (<a
href="https://doi.org/10.1137/20M1366253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally convergent variants of the Gauss--Newton algorithm are often the methods of choice to tackle nonlinear least-squares problems. Among such frameworks, Levenberg--Marquardt and trust-region methods are two well-established, similar paradigms. Both schemes have been studied when the Gauss--Newton model is replaced by a random model that is only accurate with a given probability. Trust-region schemes have also been applied to problems where the objective value is subject to noise: this setting is of particular interest in fields such as data assimilation, where efficient methods that can adapt to noise are needed to account for the intrinsic uncertainty in the input data. In this paper, we describe a stochastic Levenberg--Marquardt algorithm that handles noisy objective function values and random models, provided sufficient accuracy is achieved in probability. Our method relies on a specific scaling of the regularization parameter that allows us to leverage existing results for trust-region algorithms. Moreover, we exploit the structure of our objective through the use of a family of stationarity criteria tailored to least-squares problems. Provided the probability of accurate function estimates and models is sufficiently large, we bound the expected number of iterations needed to reach an approximate stationary point, which generalizes results based on using deterministic models or noiseless function values. We illustrate the link between our approach and several applications related to inverse problems and machine learning.},
  archive      = {J_JUQ},
  author       = {El Houcine Bergou and Youssef Diouane and Vyacheslav Kungurtsev and Clément W. Royer},
  doi          = {10.1137/20M1366253},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {507-536},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A stochastic levenberg--marquardt method using random models with complexity results},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). APIK: Active physics-informed kriging model with partial
differential equations. <em>JUQ</em>, <em>10</em>(1), 481–506. (<a
href="https://doi.org/10.1137/20M1389285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kriging (or Gaussian process regression) becomes a popular machine learning method for its flexibility and closed-form prediction expressions. However, one of the key challenges in applying kriging to engineering systems is that the available measurement data are scarce due to the measurement limitations or high sensing costs. On the other hand, physical knowledge of the engineering system is often available and represented in the form of partial differential equations (PDEs). We present in this paper a PDE-informed Kriging model (PIK) that introduces PDE information via a set of PDE points and conducts posterior prediction similar to the standard kriging method. The proposed PIK model can incorporate physical knowledge from both linear and nonlinear PDEs. To further improve learning performance, we propose an active PIK framework (APIK) that designs PDE points to leverage the PDE information based on the PIK model and measurement data. The selected PDE points not only explore the whole input space but also exploit the locations where the PDE information is critical in reducing predictive uncertainty. Finally, an expectation-maximization algorithm is developed for parameter estimation. We demonstrate the effectiveness of APIK in two synthetic examples: a shock wave case study and a laser heating case study.},
  archive      = {J_JUQ},
  author       = {Jialei Chen and Zhehui Chen and Chuck Zhang and C. F. Jeff Wu},
  doi          = {10.1137/20M1389285},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {481-506},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {APIK: Active physics-informed kriging model with partial differential equations},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strong rates of convergence of a splitting scheme for
schrödinger equations with nonlocal interaction cubic nonlinearity and
white noise dispersion. <em>JUQ</em>, <em>10</em>(1), 453–480. (<a
href="https://doi.org/10.1137/20M1378168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a splitting integrator for the time discretization of the Schrödinger equation with nonlocal interaction cubic nonlinearity and white noise dispersion. We prove that this time integrator has order of convergence one in the $p$th mean sense, for any $p\geq1$ in some Sobolev spaces. We prove that the splitting schemes preserves the $L^2$-norm, which is a crucial property for the proof of the strong convergence result. Finally, numerical experiments illustrate the performance of the proposed numerical scheme.},
  archive      = {J_JUQ},
  author       = {Charles-Edouard Bréhier and David Cohen},
  doi          = {10.1137/20M1378168},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {453-480},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Strong rates of convergence of a splitting scheme for schrödinger equations with nonlocal interaction cubic nonlinearity and white noise dispersion},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective generation of compressed stationary gaussian
fields. <em>JUQ</em>, <em>10</em>(1), 439–452. (<a
href="https://doi.org/10.1137/20M1375541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel approach to compression of two-dimensional Gaussian random fields. We build on a circulant embedding method to effectively decompose and generate sample realizations. By employing the structure of the resulting circulant matrix, we propose a truncation algorithm that controls energy through rank and values of retained spectral components. In contrast with naive truncation, such construction ensures that the covariance matrix remains realizable. We discuss the properties and efficiency of the algorithm with numerical examples.},
  archive      = {J_JUQ},
  author       = {Robert Sawko and Małgorzata J. Zimoń},
  doi          = {10.1137/20M1375541},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {439-452},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Effective generation of compressed stationary gaussian fields},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A spline dimensional decomposition for uncertainty
quantification in high dimensions. <em>JUQ</em>, <em>10</em>(1),
404–438. (<a href="https://doi.org/10.1137/20M1364175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study debuts a new spline dimensional decomposition (SDD) for uncertainty quantification analysis of high-dimensional functions, including those endowed with high nonlinearity and nonsmoothness, if they exist, in a proficient manner. The decomposition creates a hierarchical expansion for an output random variable of interest with respect to measure-consistent orthonormalized basis splines (B-splines) in independent input random variables. A dimensionwise decomposition of a spline space into orthogonal subspaces, each spanned by a reduced set of such orthonormal splines, results in SDD. Exploiting the modulus of smoothness, the SDD approximation is shown to converge in mean-square to the correct limit. The computational complexity of the SDD method is polynomial, as opposed to exponential, thus alleviating the curse of dimensionality to the extent possible. Analytical formulae are proposed to calculate the second-moment properties of a truncated SDD approximation for a general output random variable in terms of the expansion coefficients involved. Numerical results indicate that a low-order SDD approximation of nonsmooth functions calculates the probabilistic characteristics of an output variable with an accuracy matching or surpassing those obtained by high-order approximations from several existing methods. Finally, a 34-dimensional random eigenvalue analysis demonstrates the utility of SDD in solving practical problems.},
  archive      = {J_JUQ},
  author       = {Sharif Rahman and Ramin Jahanbin},
  doi          = {10.1137/20M1364175},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {404-438},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A spline dimensional decomposition for uncertainty quantification in high dimensions},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Block-diagonal covariance estimation and application to the
shapley effects in sensitivity analysis. <em>JUQ</em>, <em>10</em>(1),
379–403. (<a href="https://doi.org/10.1137/20M1358839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the estimation of sensitivity indices called “Shapley effects” when the model is linear and when the input vector is high dimensional with a Gaussian distribution. The computation cost of the Shapley effects makes it necessary to focus on the case where the input vector has a block-diagonal covariance matrix. First, we estimate a block-diagonal covariance matrix from Gaussian variables in high dimension. We prove that, under some mild assumptions, we find the block-diagonal structure of the matrix with probability that goes to one. We deduce an estimator of the covariance matrix that is as accurate as if the block-diagonal structure was known, with numerical applications. We also prove the asymptotic efficiency of a similar estimator in fixed dimension. Then, we apply this estimator for the estimation of the Shapley effects, in the Gaussian linear framework. We derive an estimator of the Shapley effects in high dimension with a relative error that converges to 0 at the parametric rate, up to a logarithmic factor. Finally, we apply the Shapley effects estimator on nuclear data.},
  archive      = {J_JUQ},
  author       = {Baptiste Broto and François Bachoc and Laura Clouvel and Jean-Marc Martinez},
  doi          = {10.1137/20M1358839},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {379-403},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Block-diagonal covariance estimation and application to the shapley effects in sensitivity analysis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Varying coefficient models and design choice for bayes
linear emulation of complex computer models with limited model
evaluations. <em>JUQ</em>, <em>10</em>(1), 350–378. (<a
href="https://doi.org/10.1137/20M1318560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer models are widely used to help make decisions about real-world systems. As computer models of large and complex systems can have long run-times and high-dimensional input spaces, it is often necessary to use emulation to assess uncertainties in computer model output. This paper presents methodology for emulation of complex computer models motivated by a real-world example in energy policy. The computer model studied is an economic model of investment in electricity generation in Great Britain. The computer model was used to select parameters in a government policy designed to incentivize investment in renewable technologies to meet government targets. Limited computing time meant that few runs of the computer model were available to fit an emulator. The statistical methodology developed was therefore focused on accurately capturing the uncertainty in computer model output arising from the small number of available model runs. A varying coefficient emulator is proposed to model uncertainty in model output when extrapolating away from model runs. To maximize use of the small number of runs available, this varying coefficient emulator is paired with a criterion-based procedure for design selection.},
  archive      = {J_JUQ},
  author       = {Amy L. Wilson and Michael Goldstein and Chris J. Dent},
  doi          = {10.1137/20M1318560},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {350-378},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Varying coefficient models and design choice for bayes linear emulation of complex computer models with limited model evaluations},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The ensemble kalman filter for rare event estimation.
<em>JUQ</em>, <em>10</em>(1), 317–349. (<a
href="https://doi.org/10.1137/21M1404119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel sampling-based method for estimating probabilities of rare or failure events. Our approach is founded on the ensemble Kalman filter (EnKF) for inverse problems. Therefore, we reformulate the rare event problem as an inverse problem and apply the EnKF to generate failure samples. To estimate the probability of failure, we use the final EnKF samples to fit a distribution model and apply importance sampling with respect to the fitted distribution. This leads to an unbiased estimator if the density of the fitted distribution admits positive values within the whole failure domain. To handle multimodal failure domains, we localize the covariance matrices in the EnKF update step around each particle and fit a mixture distribution model in the importance sampling step. For affine linear limit-state functions, we investigate the continuous time limit and large time properties of the EnKF update. We prove that the mean of the particles converges to a convex combination of the most likely failure point and the mean of the optimal importance sampling density if the EnKF is applied without noise. We provide numerical experiments to compare the performance of the EnKF with sequential importance sampling.},
  archive      = {J_JUQ},
  author       = {Fabian Wagner and I. Papaioannou and E. Ullmann},
  doi          = {10.1137/21M1404119},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {317-349},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {The ensemble kalman filter for rare event estimation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-validation–based adaptive sampling for gaussian
process models. <em>JUQ</em>, <em>10</em>(1), 294–316. (<a
href="https://doi.org/10.1137/21M1404260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, we are interested in approximating black-box, costly functions as accurately as possible with the smallest number of function evaluations. A complex computer code is an example of such a function. In this work, a Gaussian process (GP) emulator is used to approximate the output of complex computer code. We consider the problem of extending an initial experiment (set of model runs) sequentially to improve the emulator. A sequential sampling approach based on leave-one-out (LOO) cross-validation is proposed that can be easily extended to a batch mode. This is a desirable property since it saves the user time when parallel computing is available. After fitting a GP to training data points, the expected squared LOO (ES-LOO) error is calculated at each design point. ES-LOO is used as a measure to identify important data points. More precisely, when this quantity is large at a point it means that the quality of prediction depends a great deal on that point and adding more samples nearby could improve the accuracy of the GP. As a result, it is reasonable to select the next sample where ES-LOO is maximized. However, ES-LOO is only known at the experimental design and needs to be estimated at unobserved points. To do this, a second GP is fitted to the ES-LOO errors, and where the maximum of the modified expected improvement (EI) criterion occurs is chosen as the next sample. EI is a popular acquisition function in Bayesian optimization and is used to trade off between local and global search. However, it has a tendency towards exploitation, meaning that its maximum is close to the (current) “best&quot; sample. To avoid clustering, a modified version of EI, called pseudoexpected improvement, is employed which is more explorative than EI yet allows us to discover unexplored regions. Our results show that the proposed sampling method is promising.},
  archive      = {J_JUQ},
  author       = {Hossein Mohammadi and Peter Challenor and Daniel Williamson and Marc Goodfellow},
  doi          = {10.1137/21M1404260},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {294-316},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Cross-validation--based adaptive sampling for gaussian process models},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intermediate variable emulation: Using internal processes in
simulators to build more informative emulators. <em>JUQ</em>,
<em>10</em>(1), 268–293. (<a
href="https://doi.org/10.1137/20M1370902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex systems are often modeled by intricate and intensive computer simulators. This makes their behavior difficult to study, and so a statistical representation of the simulator is often used, known as an emulator, to enable users to explore the space more thoroughly. These have the disadvantage that they do not allow one to learn about the simulator&#39;s behavior beyond its role as a function from input to output variables. We take a new approach by involving the internal processes modeled within the simulator in our emulator. We introduce a new technique, intermediate variable emulation, which enables a simulator to be understood in terms of the processes it models. This leads to advantages in simulator improvement and in calibration, as the simulator can be scrutinized in more detail and the physical processes can be used to refine the input space. The intermediate variable emulator also allows one to represent more complicated relationships within the simulator, as we show with a simple example. We demonstrate the method using a simulator of the ocean carbon cycle. Using an intermediate variable emulator we are able to discover unrealistic behavior in the simulator that would not be noticeable using a standard input to output emulator and reduce the input space accordingly. We also learn about the subprocesses that drive the output and about the input variables driving each subprocess.},
  archive      = {J_JUQ},
  author       = {Rachel H. Oughton and Michael Goldstein and John C. P. Hemmings},
  doi          = {10.1137/20M1370902},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {268-293},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Intermediate variable emulation: Using internal processes in simulators to build more informative emulators},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear reduced models for state and parameter estimation.
<em>JUQ</em>, <em>10</em>(1), 227–267. (<a
href="https://doi.org/10.1137/20M1380818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State estimation aims at approximately reconstructing the solution $u$ to a parametrized partial differential equation from $m$ linear measurements when the parameter vector $y$ is unknown. Fast numerical recovery methods have been proposed in Maday et al. [Internat. J. Numer. Methods Engrg., 102 (2015), pp. 933--965] based on reduced models which are linear spaces of moderate dimension $n$ that are tailored to approximate the solution manifold ${\cal M}$ where the solution sits. These methods can be viewed as deterministic counterparts to Bayesian estimation approaches and are proved to be optimal when the prior is expressed by approximability of the solution with respect to the reduced model [P. Binev et al., SIAM/ASA J. Uncertain. Quantif., 5 (2017), pp. 1--29]. However, they are inherently limited by their linear nature, which bounds from below their best possible performance by the Kolmogorov width $d_m({\cal M})$ of the solution manifold. In this paper, we propose to break this barrier by using simple nonlinear reduced models that consist of a finite union of linear spaces $V_k$, each having dimension at most $m$ and leading to different estimators $u_k^*$. A model selection mechanism based on minimizing the PDE residual over the parameter space is used to select from this collection the final estimator $u^*$. Our analysis shows that $u^*$ meets optimal recovery benchmarks that are inherent to the solution manifold and not tied to its Kolmogorov width. The residual minimization procedure is computationally simple in the relevant case of affine parameter dependence in the PDE. In addition, it results in an estimator $y^*$ for the unknown parameter vector. In this setting, we also discuss an alternating minimization (coordinate descent) algorithm for joint state and parameter estimation that potentially improves the quality of both estimators.},
  archive      = {J_JUQ},
  author       = {Albert Cohen and Wolfgang Dahmen and Olga Mula and James Nichols},
  doi          = {10.1137/20M1380818},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {227-267},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Nonlinear reduced models for state and parameter estimation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of nested multilevel monte carlo using approximate
normal random variables. <em>JUQ</em>, <em>10</em>(1), 200–226. (<a
href="https://doi.org/10.1137/21M1399385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multilevel Monte Carlo (MLMC) method has been used for a wide variety of stochastic applications. In this paper we consider its use in situations in which input random variables can be replaced by similar approximate random variables which can be computed much more cheaply. A nested MLMC approach is adopted in which a two-level treatment of the approximated random variables is embedded within a standard MLMC application. We analyze the resulting nested MLMC variance in the specific context of an SDE discretization in which normal random variables can be replaced by approximately normal random variables, and we provide numerical results to support the analysis.},
  archive      = {J_JUQ},
  author       = {Michael Giles and Oliver Sheridan-Methven},
  doi          = {10.1137/21M1399385},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {200-226},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Analysis of nested multilevel monte carlo using approximate normal random variables},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter estimation in an SPDE model for cell
repolarization. <em>JUQ</em>, <em>10</em>(1), 179–199. (<a
href="https://doi.org/10.1137/20M1373347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a concrete setting where stochastic partial differential equations (SPDEs) are able to model real phenomena, we propose a stochastic Meinhardt model for cell repolarization and study how parameter estimation techniques developed for simple linear SPDE models apply in this situation. We establish the existence of mild SPDE solutions, and we investigate the impact of the driving noise process on pattern formation in the solution. We then pursue estimation of the diffusion term and show asymptotic normality for our estimator as the space resolution becomes finer. The finite sample performance is investigated for synthetic and real data.},
  archive      = {J_JUQ},
  author       = {Randolf Altmeyer and Till Bretschneider and Josef Janák and Markus Reiß},
  doi          = {10.1137/20M1373347},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {179-199},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Parameter estimation in an SPDE model for cell repolarization},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian inference of an uncertain generalized diffusion
operator. <em>JUQ</em>, <em>10</em>(1), 151–178. (<a
href="https://doi.org/10.1137/21M141659X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper defines a novel Bayesian inverse problem to infer an infinite-dimensional uncertain operator appearing in a differential equation, whose action on an observable state variable affects its dynamics. Inference is made tractable by parametrizing the operator using its eigendecomposition. The plausibility of operator inference in the sparse data regime is explored in terms of an uncertain, generalized diffusion operator appearing in an evolution equation for a contaminant&#39;s transport through a heterogeneous porous medium. Sparse data are augmented with prior information through the imposition of deterministic constraints on the eigendecomposition and the use of qualitative information about the system in the definition of the prior distribution. Limited observations of the state variable&#39;s evolution are used as data for inference, and the dependence on the solution of the inverse problem is studied as a function of the frequency of observations, as well as on whether or not the data is collected as a spatial or time series.},
  archive      = {J_JUQ},
  author       = {Teresa Portone and Robert D. Moser},
  doi          = {10.1137/21M141659X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {151-178},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bayesian inference of an uncertain generalized diffusion operator},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Landmark-warped emulators for models with misaligned
functional response. <em>JUQ</em>, <em>10</em>(1), 125–150. (<a
href="https://doi.org/10.1137/20M135279X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computer models output functional data, and in some cases, these functional data have similar, but misaligned, shape characteristics. We introduce a general approach for building emulators for computer models that output misaligned functional data when key values in the functional response (landmarks) can be easily identified. This approach has two main parts: modeling the aligned (using the landmarks) functional data, and modeling the functions that map the misaligned data to the aligned space (warping functions). As the warping functions are required to be monotonic, we give special attention to modeling monotonic functional response data. We discuss how our approach can be easily applied for a variety of typical emulators, such as Gaussian processes, Bayesian multivariate adaptive regression splines, and Bayesian additive regression trees, and how sensitivity analysis can be performed. We demonstrate our approach by building emulators for two applications: (1) a high-energy-density physics computer model used to simulate inertial confinement fusion ignition experiments, where model outputs are highly misaligned, and (2) a multiphysics continuum hydrocode used to simulate high-velocity impact experiments, where model outputs are only slightly misaligned. In case (1) traditional methods cannot be applied, while in (2) they can be applied, but the proposed method performs significantly better.},
  archive      = {J_JUQ},
  author       = {Devin Francom and Bruno Sansó and Ana Kupresanin},
  doi          = {10.1137/20M135279X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {125-150},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Landmark-warped emulators for models with misaligned functional response},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finite sample approximations of exact and entropic
wasserstein distances between covariance operators and gaussian
processes. <em>JUQ</em>, <em>10</em>(1), 96–124. (<a
href="https://doi.org/10.1137/21M1410488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work studies finite sample approximations of the exact and entropic regularized Wasserstein distances between centered Gaussian processes and, more generally, covariance operators of functional random processes. We first show that these distances/divergences are fully represented by reproducing kernel Hilbert space (RKHS) covariance and cross-covariance operators associated with the corresponding covariance functions. Using this representation, we show that the Sinkhorn divergence between two centered Gaussian processes can be consistently and efficiently estimated from the divergence between their corresponding normalized finite-dimensional covariance matrices or, alternatively, their sample covariance operators. Consequently, this leads to a consistent and efficient algorithm for estimating the Sinkhorn divergence from finite samples generated by the two processes. For a fixed regularization parameter, the convergence rates are dimension-independent and of the same order as those for the Hilbert--Schmidt distance. If at least one of the RKHS is finite-dimensional, we obtain a dimension-dependent sample complexity for the exact Wasserstein distance between the Gaussian processes.},
  archive      = {J_JUQ},
  author       = {Hà Quang Minh},
  doi          = {10.1137/21M1410488},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {96-124},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Finite sample approximations of exact and entropic wasserstein distances between covariance operators and gaussian processes},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint online parameter estimation and optimal sensor
placement for the partially observed stochastic advection-diffusion
equation. <em>JUQ</em>, <em>10</em>(1), 55–95. (<a
href="https://doi.org/10.1137/20M1375073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of jointly performing online parameter estimation and optimal sensor placement for a partially observed infinite-dimensional linear diffusion process. We present a novel solution to this problem in the form of a continuous-time, two-timescale stochastic gradient descent algorithm, which recursively seeks to maximize the asymptotic log-likelihood of the observations with respect to the unknown model parameters and to minimize the expected mean squared error of the hidden state estimate with respect to the sensor locations. We also provide extensive numerical results illustrating the performance of the proposed approach in the case that the hidden signal is governed by the two-dimensional stochastic advection-diffusion equation.},
  archive      = {J_JUQ},
  author       = {Louis Sharrock and Nikolas Kantas},
  doi          = {10.1137/20M1375073},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {55-95},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Joint online parameter estimation and optimal sensor placement for the partially observed stochastic advection-diffusion equation},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized kernel method for global sensitivity analysis.
<em>JUQ</em>, <em>10</em>(1), 27–54. (<a
href="https://doi.org/10.1137/20M1354829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global sensitivity analysis (GSA) is frequently used to analyze how the uncertainty in input parameters of computational models or in experimental setups influences the uncertainty of an output. Here we describe a class of GSA measures based on the embedding of the multiple output&#39;s joint probability distribution into a reproducing kernel Hilbert space (RKHS). In particular, the distance between embeddings is measured utilizing the maximum mean discrepancy, which has several key advantages over many common sensitivity measures. First, the proposed methodology defines measures for an arbitrary type of output, while maintaining easy computability for high-dimensional outputs. Second, by utilizing different kernels, or RKHSs, one can determine how the input parameters influence different features of the output distribution. This new class of sensitivity analysis measures, encapsulated into what are called $\beta^k$-indicators, are shown to contain both moment-independent and moment-based measures as special cases. The specific $\beta^k$-indicator arises from the particular choice of kernel. This analysis includes deriving new GSA measures as well as showing that certain previously proposed GSA measures, such as the variance-based indicators, are special cases of the $\beta^k$-indicators. Some basic test cases are used to showcase that the $\beta^k$-indicator derived from kernel-based GSA provides flexible tools capable of assessing a broad range of applications.},
  archive      = {J_JUQ},
  author       = {John Barr and Herschel Rabitz},
  doi          = {10.1137/20M1354829},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {27-54},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A generalized kernel method for global sensitivity analysis},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computer model calibration with time series data using deep
learning and quantile regression. <em>JUQ</em>, <em>10</em>(1), 1–26.
(<a href="https://doi.org/10.1137/20M1382581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer models play a key role in many scientific and engineering problems. One major source of uncertainty in computer model experiments is input parameter uncertainty. Computer model calibration is a formal statistical procedure to infer input parameters by combining information from model runs and observational data. The existing standard calibration framework suffers from inferential issues when the model output and observational data are high-dimensional dependent data, such as large time series, due to the difficulty in building an emulator and the nonidentifiability between effects from input parameters and data-model discrepancy. To overcome these challenges, we propose a new calibration framework based on a deep neural network (DNN) with long short-term memory layers that directly emulates the inverse relationship between the model output and input parameters. Adopting the “learning with noise” idea, we train our DNN model to filter out the effects from data-model discrepancy on input parameter inference. We also formulate a new way to construct interval predictions for DNN using quantile regression to quantify the uncertainty in input parameter estimates. Through a simulation study and real data application with the Weather Research and Forecasting Model Hydrological modeling system (WRF-Hydro), we show our approach can yield accurate point estimates and well-calibrated interval estimates for input parameters.},
  archive      = {J_JUQ},
  author       = {Saumya Bhatnagar and Won Chang and Seonjin Kim and Jiali Wang},
  doi          = {10.1137/20M1382581},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {1-26},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Computer model calibration with time series data using deep learning and quantile regression},
  volume       = {10},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
