<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods---58">SIMODS - 58</h2>
<ul>
<li><details>
<summary>
(2022). Double double descent: On generalization errors in transfer
learning between linear regression tasks. <em>SIMODS</em>,
<em>4</em>(4), 1447–1472. (<a
href="https://doi.org/10.1137/22M1469559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the transfer learning process between two linear regression problems. An important and timely special case is when the regressors are overparameterized and perfectly interpolate their training data. We examine a parameter transfer mechanism whereby a subset of the parameters of the target task solution are constrained to the values learned for a related source task. We analytically characterize the generalization error of the target task in terms of the salient factors in the transfer learning architecture, i.e., the number of examples available, the number of (free) parameters in each of the tasks, the number of parameters transferred from the source to target task, and the relation between the two tasks. Our nonasymptotic analysis shows that the generalization error of the target task follows a two-dimensional double descent trend (with respect to the number of free parameters in each of the tasks) that is controlled by the transfer learning factors. Our analysis points to specific cases where the transfer of parameters is beneficial as a substitute for extra overparameterization (i.e., additional free parameters in the target task). Specifically, we show that the usefulness of a transfer learning setting is fragile and depends on a delicate interplay among the set of transferred parameters, the relation between the tasks, and the true solution. We also demonstrate that overparameterized transfer learning is not necessarily more beneficial when the source task is closer or identical to the target task.},
  archive      = {J_SIMODS},
  author       = {Yehuda Dar and Richard G. Baraniuk},
  doi          = {10.1137/22M1469559},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1447-1472},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Double double descent: On generalization errors in transfer learning between linear regression tasks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Biwhitening reveals the rank of a count matrix.
<em>SIMODS</em>, <em>4</em>(4), 1420–1446. (<a
href="https://doi.org/10.1137/21M1456807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Estimating the rank of a corrupted data matrix is an important task in data analysis, most notably for choosing the number of components in principal component analysis. Significant progress on this task was achieved using random matrix theory by characterizing the spectral properties of large noise matrices. However, utilizing such tools is not straightforward when the data matrix consists of count random variables, e.g., a Poisson matrix, in which case the noise can be heteroskedastic with an unknown variance in each entry. In this work, we focus on a Poisson random matrix with independent entries and propose a simple procedure, termed biwhitening, for estimating the rank of the underlying signal matrix (i.e., the Poisson parameter matrix) without any prior knowledge. Our approach is based on the key observation that one can scale the rows and columns of the data matrix simultaneously so that the spectrum of the corresponding noise agrees with the standard Marchenko–Pastur (MP) law, justifying the use of the MP upper edge as a threshold for rank selection. Importantly, the required scaling factors can be estimated directly from the observations by solving a matrix scaling problem via the Sinkhorn–Knopp algorithm. Aside from the Poisson, our approach is extended to families of distributions that satisfy a quadratic relation between the mean and the variance, such as the generalized Poisson, binomial, negative binomial, gamma, and many others. This quadratic relation can also account for missing entries in the data. We conduct numerical experiments that corroborate our theoretical findings, and showcase the advantage of our approach for rank estimation in challenging regimes. Furthermore, we demonstrate the favorable performance of our approach on several real datasets of single-cell RNA sequencing, high-throughput chromosome conformation capture, and document topic modeling.},
  archive      = {J_SIMODS},
  author       = {Boris Landa and Thomas T. C. K. Zhang and Yuval Kluger},
  doi          = {10.1137/21M1456807},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1420-1446},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Biwhitening reveals the rank of a count matrix},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sharp estimates on random hyperplane tessellations.
<em>SIMODS</em>, <em>4</em>(4), 1396–1419. (<a
href="https://doi.org/10.1137/22M1485826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of generating a hyperplane tessellation of an arbitrary set in , ensuring that the Euclidean distance between any two points corresponds to the fraction of hyperplanes separating them up to a prespecified error . We focus on random Gaussian tessellations with uniformly distributed shifts and derive sharp bounds on the number of hyperplanes that are required. This result has natural applications in data dimension reduction—it yields a binary version of the Johnson–Lindenstrauss embedding—and signal processing under coarse quantization. Surprisingly, our lower estimates falsify the conjecture that , where is the Gaussian width of , is optimal. This conjecture is the natural analogue of a conjecture by Plan and Vershynin on random tessellations of subsets of the Euclidean sphere. As it turns out, the true optimal rate is larger by an order of magnitude in the accuracy parameter and depends in an intricate way on the geometry of the set. In particular, we give an explicit example where is required. To the best of our knowledge, the fact that the optimal error decay rate depends on the geometry of the set is a new phenomenon in the general context of random embeddings.},
  archive      = {J_SIMODS},
  author       = {Sjoerd Dirksen and Shahar Mendelson and Alexander Stollenwerk},
  doi          = {10.1137/22M1485826},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1396-1419},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Sharp estimates on random hyperplane tessellations},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accelerated bregman primal-dual methods applied to optimal
transport and wasserstein barycenter problems. <em>SIMODS</em>,
<em>4</em>(4), 1369–1395. (<a
href="https://doi.org/10.1137/22M1481865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper discusses the efficiency of Hybrid Primal-Dual (HPD) type algorithms to approximately solve discrete Optimal Transport (OT) and Wasserstein Barycenter (WB) problems, with and without entropic regularization. Our first contribution is an analysis showing that these methods yield state-of-the-art convergence rates, both theoretically and practically. Next, we extend the HPD algorithm with the linesearch proposed by Malitsky and Pock in 2018 to the setting where the dual space has a Bregman divergence, and the dual function is relatively strongly convex to the Bregman’s kernel. This extension yields a new method for OT and WB problems based on smoothing of the objective that also achieves state-of-the-art convergence rates. Finally, we introduce a new Bregman divergence based on a scaled entropy function that makes the algorithm numerically stable and reduces the smoothing, leading to sparse solutions of OT and WB problems. We complement our findings with numerical experiments and comparisons.},
  archive      = {J_SIMODS},
  author       = {Antonin Chambolle and Juan Pablo Contreras},
  doi          = {10.1137/22M1481865},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1369-1395},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Accelerated bregman primal-dual methods applied to optimal transport and wasserstein barycenter problems},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exponential-wrapped distributions on symmetric spaces.
<em>SIMODS</em>, <em>4</em>(4), 1347–1368. (<a
href="https://doi.org/10.1137/21M1461551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In many applications, the curvature of the space supporting the data makes the statistical modeling challenging. In this paper we discuss the construction and use of probability distributions wrapped around manifolds using exponential maps. These distributions have already been used on specific manifolds. We describe their construction in the unifying framework of affine locally symmetric spaces. Affine locally symmetric spaces are a broad class of manifolds containing many manifolds encountered in the data sciences. We show that on these spaces, exponential-wrapped distributions enjoy interesting properties for practical use. We provide the generic expression of the Jacobian appearing in these distributions and compute it on two particular examples: Grassmannians and pseudohyperboloids. We illustrate the interest of such distributions in a classification experiment on simulated data.},
  archive      = {J_SIMODS},
  author       = {Emmanuel Chevallier and Didong Li and Yulong Lu and David Dunson},
  doi          = {10.1137/21M1461551},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1347-1368},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Exponential-wrapped distributions on symmetric spaces},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Operator shifting for general noisy matrix systems.
<em>SIMODS</em>, <em>4</em>(4), 1320–1346. (<a
href="https://doi.org/10.1137/21M1416849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the computational sciences, one must often estimate model parameters from data subject to noise and uncertainty, leading to inaccurate results. In order to improve the accuracy of models with noisy parameters, we consider the problem of reducing error in a linear system with the operator corrupted by noise. Our contribution in this paper is to extend the elliptic operator shifting framework from Etter and Ying, 2020 to the general nonsymmetric matrix case. Roughly, the operator shifting technique is a matrix analogue of the James–Stein estimator. The key insight is that a shift of the matrix inverse estimate in an appropriately chosen direction will reduce average error. In our extension, we interrogate a number of questions—namely, whether or not shifting towards the origin for general matrix inverses always reduces error as it does in the elliptic case. We show that this is usually the case, but that there are three key features of the general nonsingular matrices that allow for counterexamples not possible in the symmetric case. We prove that when these possibilities are eliminated by the assumption of noise symmetry and the use of the residual norm as the error metric, the optimal shift is always towards the origin, mirroring results from Etter and Ying, 2020. We also investigate behavior in the small noise regime and other scenarios. We conclude by presenting numerical experiments (with accompanying source code) inspired by reinforcement learning to demonstrate that operator shifting can yield substantial reductions in error.},
  archive      = {J_SIMODS},
  author       = {Philip A. Etter and Lexing Ying},
  doi          = {10.1137/21M1416849},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1320-1346},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Operator shifting for general noisy matrix systems},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Markov kernels local aggregation for noise vanishing
distribution sampling. <em>SIMODS</em>, <em>4</em>(4), 1293–1319. (<a
href="https://doi.org/10.1137/22M1469626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A novel strategy that combines a given collection of -reversible Markov kernels is proposed. At each Markov transition, one of the available kernels is selected via a state-dependent probability distribution. In contrast to random-scan type approaches that assume a constant (i.e., state-independent) selection probability distribution, the state-dependent distribution is specified so as to privilege moving according to a kernel which is relevant for the local geometry of the target distribution. This approach leverages paths or other low-dimensional manifolds that are typically present in noise vanishing distributions. Some examples for which we show (theoretically or empirically) that a locally weighted aggregation converges substantially faster and yields smaller asymptotic variances than an equivalent random-scan algorithm are provided.},
  archive      = {J_SIMODS},
  author       = {Florian Maire and Pierre Vandekerkhove},
  doi          = {10.1137/22M1469626},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1293-1319},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Markov kernels local aggregation for noise vanishing distribution sampling},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the exact computation of linear frequency principle
dynamics and its generalization. <em>SIMODS</em>, <em>4</em>(4),
1272–1292. (<a href="https://doi.org/10.1137/21M1444400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent works show the intriguing phenomenon of the frequency principle (F-Principle) that deep neural networks (DNNs) fit the target function from low to high frequency during training, which provides insight into the training and generalization behavior of DNNs in complex tasks. In this paper, through analysis of an infinite-width two-layer NN in the neural tangent kernel regime, we derive the exact differential equation, namely the linear frequency-principle (LFP) model, governing the evolution of NN output function in the frequency domain during training. Our exact computation applies for general activation functions with no assumption on size and distribution of training data. This LFP model unravels that higher frequencies evolve polynomially or exponentially slower than lower frequencies depending on the smoothness/regularity of the activation function. We further bridge the gap between training dynamics and generalization by proving that the LFP model implicitly minimizes a frequency-principle norm (FP-norm) of the learned function, by which higher frequencies are more severely penalized depending on the inverse of their evolution rate. Finally, we derive an a priori generalization error bound controlled by the FP-norm of the target function, which provides a theoretical justification for the empirical results that DNNs often generalize well for low-frequency functions.},
  archive      = {J_SIMODS},
  author       = {Tao Luo and Zheng Ma and Zhi-Qin John Xu and Yaoyu Zhang},
  doi          = {10.1137/21M1444400},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1272-1292},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On the exact computation of linear frequency principle dynamics and its generalization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Local versions of sum-of-norms clustering. <em>SIMODS</em>,
<em>4</em>(4), 1250–1271. (<a
href="https://doi.org/10.1137/21M1448732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sum-of-norms clustering is a convex optimization problem whose solution can be used for the clustering of multivariate data. We propose and study a localized version of this method, and show in particular that it can separate arbitrarily close balls in the stochastic ball model. More precisely, we prove a quantitative bound on the error incurred in the clustering of disjoint connected sets. Our bound is expressed in terms of the number of datapoints and the localization length of the functional.},
  archive      = {J_SIMODS},
  author       = {Alexander Dunlap and Jean-Christophe Mourrat},
  doi          = {10.1137/21M1448732},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1250-1271},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Local versions of sum-of-norms clustering},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bi-invariant dissimilarity measures for sample distributions
in lie groups. <em>SIMODS</em>, <em>4</em>(4), 1223–1249. (<a
href="https://doi.org/10.1137/21M1410373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Data sets sampled in Lie groups are widespread, and as with multivariate data, it is important for many applications to assess the differences between the sets in terms of their distributions. Indices for this task are usually derived by considering the Lie group as a Riemannian manifold. Then, however, compatibility with the group operation is guaranteed only if a bi-invariant metric exists, which is not the case for most noncompact and noncommutative groups. We show here that if one considers an affine connection structure instead, one obtains bi-invariant generalizations of well-known dissimilarity measures: Hotelling statistic, Bhattacharyya distance, and Hellinger distance. Each of the dissimilarity measures matches its multivariate counterpart for Euclidean data and is translation invariant, so that biases, e.g., through an arbitrary choice of reference, are avoided. We further derive nonparametric two-sample tests that are bi-invariant and consistent. We demonstrate the potential of these dissimilarity measures by performing group tests on data of knee configurations and epidemiological shape data. Significant differences are revealed in both cases.},
  archive      = {J_SIMODS},
  author       = {Martin Hanik and Hans-Christian Hege and Christoph von Tycowicz},
  doi          = {10.1137/21M1410373},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1223-1249},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Bi-invariant dissimilarity measures for sample distributions in lie groups},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An AO-ADMM approach to constraining PARAFAC2 on all modes.
<em>SIMODS</em>, <em>4</em>(3), 1191–1222. (<a
href="https://doi.org/10.1137/21M1450033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing multiway measurements with variations across one mode of the dataset is a challenge in various fields including data mining, neuroscience, and chemometrics. For example, measurements may evolve over time or have unaligned time profiles. The PARAFAC2 model has been successfully used to analyze such data by allowing the underlying factor matrices in one mode (i.e., the evolving mode) to change across slices. The traditional approach to fit a PARAFAC2 model is to use an alternating least squares--based algorithm, which handles the constant cross-product constraint of the PARAFAC2 model by implicitly estimating the evolving factor matrices. This approach makes imposing regularization on these factor matrices challenging. There is currently no algorithm to flexibly impose such regularization with general penalty functions and hard constraints. In order to address this challenge and to avoid the implicit estimation, in this paper, we propose an algorithm for fitting PARAFAC2 based on alternating optimization with the alternating direction method of multipliers (AO-ADMM). With numerical experiments on simulated data, we show that the proposed PARAFAC2 AO-ADMM approach allows for flexible constraints, recovers the underlying patterns accurately, and is computationally efficient compared to the state-of-the-art. We also apply our model to two real-world datasets from neuroscience and chemometrics, and show that constraining the evolving mode improves the interpretability of the extracted patterns.},
  archive      = {J_SIMODS},
  author       = {Marie Roald and Carla Schenker and Vince D. Calhoun and Tülay Adali and Rasmus Bro and Jeremy E. Cohen and Evrim Acar},
  doi          = {10.1137/21M1450033},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1191-1222},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {An AO-ADMM approach to constraining PARAFAC2 on all modes},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Determinantal point processes implicitly regularize
semiparametric regression problems. <em>SIMODS</em>, <em>4</em>(3),
1171–1190. (<a href="https://doi.org/10.1137/21M1403977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric regression models are used in several applications which require comprehensibility without sacrificing accuracy. Typical examples are spline interpolation in geophysics and nonlinear time series problems, where the system includes a linear and nonlinear component. We discuss here the use of a finite determinantal point process (DPP) for approximating semiparametric models. Recently, Barthelmé, Tremblay, Usevich, and Amblard introduced a novel representation of finite DPPs. These authors formulated extended L-ensembles that can conveniently represent partial-projection DPPs and suggest their use for optimal interpolation. With the help of this formalism, we derive a key identity illustrating the implicit regularization effect of determinantal sampling for semiparametric regression and interpolation. Also, a novel projected Nyström approximation is defined and used to derive a bound on the expected in-sample prediction error for the corresponding approximation of semiparametric regression. This work naturally extends similar results obtained for kernel ridge regression.},
  archive      = {J_SIMODS},
  author       = {Michaël Fanuel and Joachim Schreurs and Johan A. K. Suykens},
  doi          = {10.1137/21M1403977},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1171-1190},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Determinantal point processes implicitly regularize semiparametric regression problems},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep neural networks and PIDE discretizations.
<em>SIMODS</em>, <em>4</em>(3), 1145–1170. (<a
href="https://doi.org/10.1137/21M1438554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose neural networks that tackle the problems of stability and field-of-view of a convolutional neural network. As an alternative to increasing the network&#39;s depth or width to improve performance, we propose integral-based spatially nonlocal operators which are related to global weighted Laplacian, fractional Laplacian, and inverse fractional Laplacian operators that arise in several problems in the physical sciences. The forward propagation of such networks is inspired by partial integro-differential equations. We test the effectiveness of the proposed neural architectures on benchmark image classification datasets and semantic segmentation tasks in autonomous driving. Moreover, we investigate the extra computational costs of these dense operators and the stability of forward propagation of the proposed neural networks.},
  archive      = {J_SIMODS},
  author       = {Bastian Bohn and Michael Griebel and Dinesh Kannan},
  doi          = {10.1137/21M1438554},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1145-1170},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Deep neural networks and PIDE discretizations},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of spatial and spatiotemporal anomalies using
persistent homology: Case studies with COVID-19 data. <em>SIMODS</em>,
<em>4</em>(3), 1116–1144. (<a
href="https://doi.org/10.1137/21M1435033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method for analyzing spatial and spatiotemporal anomalies in geospatial data using topological data analysis (TDA). To do this, we use persistent homology (PH), which allows one to algorithmically detect geometric voids in a data set and quantify the persistence of such voids. We construct an efficient filtered simplicial complex (FSC) such that the voids in our FSC are in one-to-one correspondence with the anomalies. Our approach goes beyond simply identifying anomalies; it also encodes information about the relationships between anomalies. We use vineyards, which one can interpret as time-varying persistence diagrams (which are an approach for visualizing PH), to track how the locations of the anomalies change with time. We conduct two case studies using spatially heterogeneous COVID-19 data. First, we examine vaccination rates in New York City by zip code at a single point in time. Second, we study a year-long data set of COVID-19 case rates in neighborhoods of the city of Los Angeles.},
  archive      = {J_SIMODS},
  author       = {Abigail Hickok and Deanna Needell and Mason A. Porter},
  doi          = {10.1137/21M1435033},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1116-1144},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Analysis of spatial and spatiotemporal anomalies using persistent homology: Case studies with COVID-19 data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Private sampling: A noiseless approach for generating
differentially private synthetic data. <em>SIMODS</em>, <em>4</em>(3),
1082–1115. (<a href="https://doi.org/10.1137/21M1449944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a world where artificial intelligence and data science become omnipresent, data sharing is increasingly locking horns with data-privacy concerns. Differential privacy has emerged as a rigorous framework for protecting individual privacy in a statistical database, while releasing useful statistical information about the database. The standard way to implement differential privacy is to inject a sufficient amount of noise into the data. However, in addition to other limitations of differential privacy, this process of adding noise will affect data accuracy and utility. Another approach to enable privacy in data sharing is based on the concept of synthetic data. The goal of synthetic data is to create an as-realistic-as-possible dataset, one that not only maintains the nuances of the original data, but does so without risk of exposing sensitive information. The combination of differential privacy with synthetic data has been suggested as a best-of-both-worlds solutions. In this work, we propose the first noisefree method to construct differentially private synthetic data; we do this through a mechanism called “private sampling.” Using the Boolean cube as benchmark data model, we derive explicit bounds on accuracy and privacy of the constructed synthetic data. The key mathematical tools are hypercontractivity, duality, and empirical processes. A core ingredient of our private sampling mechanism is a rigorous “marginal correction” method, which has the remarkable property that importance reweighting can be utilized to exactly match the marginals of the sample to the marginals of the population.},
  archive      = {J_SIMODS},
  author       = {March Boedihardjo and Thomas Strohmer and Roman Vershynin},
  doi          = {10.1137/21M1449944},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1082-1115},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Private sampling: A noiseless approach for generating differentially private synthetic data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint community detection and rotational synchronization via
semidefinite programming. <em>SIMODS</em>, <em>4</em>(3), 1052–1081. (<a
href="https://doi.org/10.1137/21M1419702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the presence of heterogeneous data, where randomly rotated objects fall into multiple underlying categories, it is challenging to simultaneously classify them into clusters and synchronize them based on pairwise relations. This gives rise to the joint problem of community detection and synchronization. We propose a series of semidefinite relaxations and prove their exact recovery when extending the celebrated stochastic block model to this new setting where both rotations and cluster identities are to be determined. Numerical experiments demonstrate the efficacy of our proposed algorithms and confirm our theoretical result, which indicates a sharp phase transition for exact recovery.},
  archive      = {J_SIMODS},
  author       = {Yifeng Fan and Yuehaw Khoo and Zhizhen Zhao},
  doi          = {10.1137/21M1419702},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1052-1081},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Joint community detection and rotational synchronization via semidefinite programming},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DESTRESS: Computation-optimal and communication-efficient
decentralized nonconvex finite-sum optimization. <em>SIMODS</em>,
<em>4</em>(3), 1031–1051. (<a
href="https://doi.org/10.1137/21M1450677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications in multiagent environments such as internet-of-things, networked sensing, autonomous systems, and federated learning, call for decentralized algorithms for finite-sum optimizations that are resource efficient in terms of both computation and communication. In this paper, we consider the prototypical setting where the agents work collaboratively to minimize the sum of local loss functions by only communicating with their neighbors over a predetermined network topology. We develop a new algorithm, called DEcentralized STochastic REcurSive gradient methodS (DESTRESS) for nonconvex finite-sum optimization, which matches the optimal incremental first-order oracle complexity of centralized algorithms for finding first-order stationary points, while maintaining communication efficiency. Detailed theoretical and numerical comparisons corroborate that the resource efficiencies of DESTRESS improve upon prior decentralized algorithms over a wide range of parameter regimes. DESTRESS leverages several key algorithm design ideas including stochastic recursive gradient updates with minibatches for local computation, gradient tracking with extra mixing (i.e., multiple gossiping rounds) for periteration communication, together with careful choices of hyperparameters and new analysis frameworks to provably achieve a desirable computation-communication trade-off.},
  archive      = {J_SIMODS},
  author       = {Boyue Li and Zhize Li and Yuejie Chi},
  doi          = {10.1137/21M1450677},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1031-1051},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {DESTRESS: Computation-optimal and communication-efficient decentralized nonconvex finite-sum optimization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence of a piggyback-style method for the
differentiation of solutions of standard saddle-point problems.
<em>SIMODS</em>, <em>4</em>(3), 1003–1030. (<a
href="https://doi.org/10.1137/21M1455887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a “piggyback&#39;&#39;-style method for computing the derivative of a loss which depends on the solution of a convex-concave saddle-point problem, with respect to the bilinear term. We attempt to derive guarantees for the algorithm under minimal regularity assumptions on the functions. Our final convergence results include possibly nonsmooth objectives. We illustrate the versatility of the proposed piggyback algorithm by learning optimized shearlet transforms, which are a class of popular sparsifying transforms in the field of imaging.},
  archive      = {J_SIMODS},
  author       = {Lea Bogensperger and Antonin Chambolle and Thomas Pock},
  doi          = {10.1137/21M1455887},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {1003-1030},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Convergence of a piggyback-style method for the differentiation of solutions of standard saddle-point problems},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence of a constrained vector extrapolation scheme.
<em>SIMODS</em>, <em>4</em>(3), 979–1002. (<a
href="https://doi.org/10.1137/21M1428030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among extrapolation methods, Anderson acceleration (AA) is a popular technique for speeding up convergence of iterative processes toward their limit points. AA proceeds by extrapolating a better approximation of the limit using a weighted combination of previous iterates. Whereas AA was originally developed in the context of nonlinear integral equations, or to accelerate the convergence of iterative methods for solving linear systems, it is also used to extrapolate the solution of nonlinear systems. Simple additional stabilization strategies can be used in this context to control conditioning issues. In this work, we study a constrained vector extrapolation scheme based on an offline version of AA with fixed window size, for solving nonlinear systems arising in optimization problems, where the stabilization strategy consists in bounding the magnitude of the extrapolation weights. We provide explicit convergence bounds for this method and, as a by-product, upper bounds on a constrained version of the Chebyshev problem on polynomials.},
  archive      = {J_SIMODS},
  author       = {Mathieu Barré and Adrien Taylor and Alexandre d&#39;Aspremont},
  doi          = {10.1137/21M1428030},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {979-1002},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Convergence of a constrained vector extrapolation scheme},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span class="math inline"><em>k</em></span>-variance: A
clustered notion of variance. <em>SIMODS</em>, <em>4</em>(3), 957–978.
(<a href="https://doi.org/10.1137/20M1385895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce $k$-variance, a generalization of variance built on the machinery of random bipartite matchings. $k$-variance measures the expected cost of matching two sets of $k$ samples from a distribution to each other, capturing local rather than global information about a measure as $k$ increases; it is easily approximated stochastically using sampling and linear programming. In addition to defining $k$-variance and proving its basic properties, we provide in-depth analysis of this quantity in several key cases, including one-dimensional measures, clustered measures, and measures concentrated on low-dimensional subsets of ${\mathbb R}^n$. We conclude with experiments and open problems motivated by this new way to summarize distributional shape.},
  archive      = {J_SIMODS},
  author       = {Justin Solomon and Kristjan Greenewald and Haikady Nagaraja},
  doi          = {10.1137/20M1385895},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {957-978},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {$k$-variance: A clustered notion of variance},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Benefit of interpolation in nearest neighbor algorithms.
<em>SIMODS</em>, <em>4</em>(2), 935–956. (<a
href="https://doi.org/10.1137/21M1437457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some studies (e.g., [C. Zhang et al. in Proceedings of the 5th International Conference on Learning Representations, OpenReview.net, 2017]) of deep learning, it is observed that overparametrized deep neural networks achieve a small testing error even when the training error is almost zero. Despite numerous works toward understanding this so-called double-descent phenomenon (e.g., [M. Belkin et al., Proc. Natl. Acad. Sci. USA, 116 (2019), pp. 15849--15854; M. Belkin, D. Hsu, and J. Xu, SIAM J. Math. Data Sci., 2 (2020), pp. 1167--1180]), in this paper, we turn to another way to enforce zero training error (without overparametrization) through a data interpolation mechanism. Specifically, we consider a class of interpolated weighting schemes in the nearest neighbors (NN) algorithms. By carefully characterizing the multiplicative constant in the statistical risk, we reveal a U-shaped performance curve for the level of data interpolation in both classification and regression setups. This sharpens the existing result [M. Belkin, A. Rakhlin, and A. B. Tsybakov, in Proceedings of Machine Learning Research 89, PMLR, 2019, pp. 1611--1619] that zero training error does not necessarily jeopardize predictive performances and claims a counterintuitive result that a mild degree of data interpolation actually strictly improves the prediction performance and statistical stability over those of the (uninterpolated) $k$-NN algorithm. In the end, the universality of our results, such as change of distance measure and corrupted testing data, will also be discussed.},
  archive      = {J_SIMODS},
  author       = {Yue Xing and Qifan Song and Guang Cheng},
  doi          = {10.1137/21M1437457},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {935-956},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Benefit of interpolation in nearest neighbor algorithms},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GNMR: A provable one-line algorithm for low rank matrix
recovery. <em>SIMODS</em>, <em>4</em>(2), 909–934. (<a
href="https://doi.org/10.1137/21M1433812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low rank matrix recovery problems appear in a broad range of applications. In this work we present GNMR---an extremely simple iterative algorithm for low rank matrix recovery, based on a Gauss--Newton linearization. On the theoretical front, we derive recovery guarantees for GNMR in both matrix sensing and matrix completion settings. Some of these results improve upon the best currently known for other methods. A key property of GNMR is that it implicitly keeps the factor matrices approximately balanced throughout its iterations. On the empirical front, we show that for matrix completion with uniform sampling, GNMR performs better than several popular methods, especially when given very few observations close to the information limit.},
  archive      = {J_SIMODS},
  author       = {Pini Zilber and Boaz Nadler},
  doi          = {10.1137/21M1433812},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {909-934},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {GNMR: A provable one-line algorithm for low rank matrix recovery},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Overparameterization and generalization error: Weighted
trigonometric interpolation. <em>SIMODS</em>, <em>4</em>(2), 885–908.
(<a href="https://doi.org/10.1137/21M1390955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by surprisingly good generalization properties of learned deep neural networks in overparameterized scenarios and by the related double descent phenomenon, this paper analyzes the relation between smoothness and low generalization error in an overparameterized linear learning problem. We study a random Fourier series model, where the task is to estimate the unknown Fourier coefficients from equidistant samples. We derive exact expressions for the generalization error of both plain and weighted least squares estimators. We show precisely how a bias toward smooth interpolants, in the form of weighted trigonometric interpolation, can lead to smaller generalization error in the overparameterized regime compared to the underparameterized regime. This provides insight into the power of overparameterization, which is common in modern machine learning.},
  archive      = {J_SIMODS},
  author       = {Yuege Xie and Hung-Hsu Chou and Holger Rauhut and Rachel Ward},
  doi          = {10.1137/21M1390955},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {885-908},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Overparameterization and generalization error: Weighted trigonometric interpolation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Persistent laplacians: Properties, algorithms and
implications. <em>SIMODS</em>, <em>4</em>(2), 858–884. (<a
href="https://doi.org/10.1137/21M1435471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a thorough study of the theoretical properties and devise efficient algorithms for the persistent Laplacian, an extension of the standard combinatorial Laplacian to the setting of pairs (or, in more generality, sequences) of simplicial complexes $K \hookrightarrow L$, which was recently introduced by Wang, Nguyen, and Wei. In particular, in analogy with the nonpersistent case, we first prove that the nullity of the $q$th persistent Laplacian $\Delta_q^{K,L}$ equals the $q$th persistent Betti number of the inclusion $(K \hookrightarrow L)$. We then present an initial algorithm for finding a matrix representation of $\Delta_q^{K,L}$, which itself helps interpret the persistent Laplacian. We exhibit a novel relationship between the persistent Laplacian and the notion of Schur complement of a matrix which has several important implications. In the graph case, it both uncovers a link with the notion of effective resistance and leads to a persistent version of the Cheeger inequality. This relationship also yields an additional, very simple algorithm for finding (a matrix representation of) the $q$th persistent Laplacian which in turn leads to a novel and fundamentally different algorithm for computing the $q$th persistent Betti number for a pair $K\hookrightarrow L$ which can be significantly more efficient than standard algorithms. Finally, we study persistent Laplacians for simplicial filtrations and establish novel functoriality properties and stability results for their eigenvalues. Our work brings methods from spectral graph theory, circuit theory, and persistent homology together with a topological view of the combinatorial Laplacian on simplicial complexes.},
  archive      = {J_SIMODS},
  author       = {Facundo Mémoli and Zhengchao Wan and Yusu Wang},
  doi          = {10.1137/21M1435471},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {858-884},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Persistent laplacians: Properties, algorithms and implications},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Feel-good thompson sampling for contextual bandits and
reinforcement learning. <em>SIMODS</em>, <em>4</em>(2), 834–857. (<a
href="https://doi.org/10.1137/21M140924X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thompson sampling has been widely used for contextual bandit problems due to the flexibility of its modeling power. However, a general theory for this class of methods in the frequentist setting is still lacking. In this paper, we present a theoretical analysis of Thompson sampling, with a focus on frequentist regret bounds. In this setting, we show that the standard Thompson sampling is not aggressive enough in exploring new actions, leading to suboptimality in some pessimistic situations. A simple modification called Feel-Good Thompson sampling, which favors high reward models more aggressively than the standard Thompson sampling, is proposed to remedy this problem. We show that the theoretical framework can be used to derive Bayesian regret bounds for standard Thompson sampling and frequentist regret bounds for Feel-Good Thompson sampling. It is shown that in both cases, we can reduce the bandit regret problem to online least squares regression estimation. For the frequentist analysis, the online least squares regression bound can be directly obtained using online aggregation techniques which have been well studied. The resulting bandit regret bound matches the minimax lower bound in the finite action case. Moreover, the analysis can be generalized to handle a class of linearly embeddable contextual bandit problems (which generalizes the popular linear contextual bandit model). The obtained result again matches the minimax lower bound. Finally we illustrate that the analysis can be extended to handle some MDP problems.},
  archive      = {J_SIMODS},
  author       = {Tong Zhang},
  doi          = {10.1137/21M140924X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {834-857},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Feel-good thompson sampling for contextual bandits and reinforcement learning},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autodifferentiable ensemble kalman filters. <em>SIMODS</em>,
<em>4</em>(2), 801–833. (<a
href="https://doi.org/10.1137/21M1434477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data assimilation is concerned with sequentially estimating a temporally evolving state. This task, which arises in a wide range of scientific and engineering applications, is particularly challenging when the state is high-dimensional and the state-space dynamics are unknown. This paper introduces a machine learning framework for learning dynamical systems in data assimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend ensemble Kalman filters for state recovery with machine learning tools for learning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble Kalman filters to scale to high-dimensional states and the power of automatic differentiation to train high-dimensional surrogate models for the dynamics. Numerical results using the Lorenz-96 model show that AD-EnKFs outperform existing methods that use expectation-maximization or particle filters to merge data assimilation and machine learning. In addition, AD-EnKFs are easy to implement and require minimal tuning.},
  archive      = {J_SIMODS},
  author       = {Yuming Chen and Daniel Sanz-Alonso and Rebecca Willett},
  doi          = {10.1137/21M1434477},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {801-833},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Autodifferentiable ensemble kalman filters},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential construction and dimension reduction of gaussian
processes under inequality constraints. <em>SIMODS</em>, <em>4</em>(2),
772–800. (<a href="https://doi.org/10.1137/21M1407513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accounting for inequality constraints, such as boundedness, monotonicity, or convexity, is challenging when modeling costly-to-evaluate black box functions. In this regard, finite-dimensional Gaussian process (GP) regression models bring a valuable solution, as they guarantee that the inequality constraints are satisfied everywhere. Nevertheless, these models are currently restricted to small dimensional situations (up to dimension 5). Addressing this issue, we introduce the MaxMod algorithm that sequentially inserts one-dimensional knots or adds active variables, thereby performing at the same time dimension reduction and efficient knot allocation. We prove the convergence of this algorithm. In intermediary steps of the proof, we propose the notion of multiaffine extension and study its properties. We also prove the convergence of finite-dimensional GPs, when the knots are not dense in the input space, extending the recent literature. With simulated and real data, we demonstrate that the MaxMod algorithm remains efficient in higher dimension (at least in dimension 20), and needs fewer knots than other constrained GP models from the state of the art, to reach a given approximation error.},
  archive      = {J_SIMODS},
  author       = {François Bachoc and Andrés F. López-Lopera and Olivier Roustant},
  doi          = {10.1137/21M1407513},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {772-800},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Sequential construction and dimension reduction of gaussian processes under inequality constraints},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two steps at a time—taking GAN training in stride with
tseng’s method. <em>SIMODS</em>, <em>4</em>(2), 750–771. (<a
href="https://doi.org/10.1137/21M1420939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the training of generative adversarial networks (GANs), we study methods for solving minimax problems with additional nonsmooth regularizers. We do so by employing monotone operator theory, in particular the forward-backward-forward method, which avoids the known issue of limit cycling by correcting each update by a second gradient evaluation and does so requiring fewer projection steps compared to the extragradient method in the presence of constraints. Furthermore, we propose a seemingly new scheme which recycles old gradients to mitigate the additional computational cost. In doing so we rediscover a known method, related to optimistic gradient descent ascent. For both schemes we prove novel convergence rates for convex-concave minimax problems via a unifying approach. The derived error bounds are in terms of the gap function for the ergodic iterates. For the deterministic and the stochastic problem we show a convergence rate of $\mathcal{O}({1}/{k})$ and $\mathcal{O}({1}/{\sqrt{k}})$, respectively. We complement our theoretical results with empirical improvements in the training of Wasserstein GANs on the CIFAR10 dataset.},
  archive      = {J_SIMODS},
  author       = {Axel Böhm and Michael Sedlmayer and Ernö Robert Csetnek and Radu Ioan Boţ},
  doi          = {10.1137/21M1420939},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {750-771},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Two steps at a time---taking GAN training in stride with tseng&#39;s method},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrinsic dimension adaptive partitioning for kernel
methods. <em>SIMODS</em>, <em>4</em>(2), 721–749. (<a
href="https://doi.org/10.1137/21M1435690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove minimax optimal learning rates for kernel ridge regression, respectively, support vector machines, based on a data dependent partition of the input space, where the dependence of the dimension of the input space is replaced by the fractal dimension of the support of the data generating distribution. We further show that these optimal rates can be achieved by a training validation procedure without any prior knowledge on this intrinsic dimension of the data. Finally, we conduct extensive experiments which demonstrate that our considered learning methods are actually able to generalize from a dataset that is nontrivially embedded in a much higher dimensional space just as well as from the original dataset.},
  archive      = {J_SIMODS},
  author       = {Thomas Hamm and Ingo Steinwart},
  doi          = {10.1137/21M1435690},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {721-749},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Intrinsic dimension adaptive partitioning for kernel methods},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear weighted directed acyclic graph and a priori
estimates for neural networks. <em>SIMODS</em>, <em>4</em>(2), 694–720.
(<a href="https://doi.org/10.1137/21M140955X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an attempt to better understand structural benefits and generalization power of deep neural networks, we first present a novel graph theoretical formulation of neural network models, including fully connected, residual network (ResNet) and densely connected networks (DenseNet). Second, we extend the error analysis of the population risk for a two-layer network [W. E., C. Ma, and L. Wu, Commun. Math. Sci., 17 (2019), pp. 1407--1425] and ResNet [W. E., C. Ma, and Q. Wang, Commun. Math. Sci., 18 (2020), pp. 1755--1774] to DenseNet, and show further that for neural networks satisfying certain mild conditions, similar estimates can be obtained. These estimates are a priori in nature since they depend solely on the information prior to the training process, in particular, the bounds for the estimation errors do not suffer from the curse of dimensionality.},
  archive      = {J_SIMODS},
  author       = {Yuqing Li and Tao Luo and Chao Ma},
  doi          = {10.1137/21M140955X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {694-720},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonlinear weighted directed acyclic graph and a priori estimates for neural networks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speedy categorical distributional reinforcement learning and
complexity analysis. <em>SIMODS</em>, <em>4</em>(2), 675–693. (<a
href="https://doi.org/10.1137/20M1364436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributional reinforcement learning, the entire distribution of the return instead of just the expected return is modeled. The approach with categorical distributions as the approximation method is well-known in Q-learning, and convergence results have been established in the tabular case. In this work, speedy Q-learning is extended to categorical distributions, a finite-time analysis is performed, and probably approximately correct bounds in terms of the Cramér distance are established. It is shown that also in the distributional case the new update rule yields faster policy evaluation in comparison to the standard Q-learning one and that the sample complexity is essentially the same as the one of the value-based algorithmic counterpart. Without the need for more state-action-reward samples, one gains significantly more information about the return with categorical distributions. Even though the results do not easily extend to the case of policy control, a slight modification to the update rule yields promising numerical results.},
  archive      = {J_SIMODS},
  author       = {Markus Böck and Clemens Heitzinger},
  doi          = {10.1137/20M1364436},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {675-693},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Speedy categorical distributional reinforcement learning and complexity analysis},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A variational formulation of accelerated optimization on
riemannian manifolds. <em>SIMODS</em>, <em>4</em>(2), 649–674. (<a
href="https://doi.org/10.1137/21M1395648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It was shown recently by [W. Su, S. Boyd, and E. Candes, J. Mach. Learn. Res., 17 (2016), pp. 1--43] that Nesterov&#39;s accelerated gradient method for minimizing a smooth convex function $f$ can be thought of as the time discretization of a second-order ODE and that $f(x(t))$ converges to its optimal value at a rate of $\mathcal{O}(1/t^2)$ along any trajectory $x(t)$ of this ODE. A variational formulation was introduced in [A. Wibisono, A. Wilson, and M. Jordan, Proc Natl. Acad. Sci. USA, 113 (2016), pp. E7351--E7358] which allowed for accelerated convergence at a rate of $\mathcal{O}(1/t^p)$, for arbitrary $p&gt;0$, in normed vector spaces. This framework was exploited in [V. Duruisseaux, J. Schmitt, and M. Leok, SIAM J. Sci. Comput., 43 (2021), pp. A2949--A2980] using time-adaptive geometric integrators to design efficient explicit algorithms for symplectic accelerated optimization. In [F. Alimisis, A. Orvieto, G. Bécigneul, and A. Lucchi, Proceedings of the 23rd International AISTATS Conference, 2020, pp. 1297--1307], a second-order ODE was proposed as the continuous-time limit of a Riemannian accelerated algorithm, and it was shown that the objective function $f(x(t))$ converges to its optimal value at a rate of $\mathcal{O}(1/t^2)$ along solutions of this ODE, thereby generalizing the earlier Euclidean result to the Riemannian manifold setting. In this paper, we show that on Riemannian manifolds, the convergence rate of $f(x(t))$ to its optimal value can also be accelerated to an arbitrary convergence rate $\mathcal{O}(1/t^p)$, by considering a family of time-dependent Bregman Lagrangian and Hamiltonian systems on Riemannian manifolds. This generalizes the results of Wibisono, Wilson, and Jordan to Riemannian manifolds and also provides a variational framework for accelerated optimization on Riemannian manifolds. In particular, we will establish results for objective functions on Riemannian manifolds that are geodesically convex, weakly quasi-convex, and strongly convex. An approach based on the time-invariance property of the family of Bregman Lagrangians and Hamiltonians was used to construct very efficient optimization algorithms by Duruisseaux, Schmitt, and Leok, and we establish a similar time-invariance property in the Riemannian setting. This lays the foundation for constructing similarly efficient optimization algorithms on Riemannian manifolds, once the Riemannian analogues of time-adaptive Hamiltonian variational integrators have been developed. The experience with the numerical discretization of variational accelerated optimization flows on vector spaces suggests that the combination of time-adaptivity and symplecticity is important for the efficient, robust, and stable discretization of these variational flows describing accelerated optimization. One expects that a geometric numerical integrator that is time-adaptive, symplectic, and Riemannian manifold preserving will yield a class of similarly promising optimization algorithms on manifolds.},
  archive      = {J_SIMODS},
  author       = {Valentin Duruisseaux and Melvin Leok},
  doi          = {10.1137/21M1395648},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {649-674},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A variational formulation of accelerated optimization on riemannian manifolds},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptivity of stochastic gradient methods for nonconvex
optimization. <em>SIMODS</em>, <em>4</em>(2), 634–648. (<a
href="https://doi.org/10.1137/21M1394308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptivity is an important yet under-studied property in modern optimization theory. The gap between the state-of-the-art theory and the current practice is striking in that algorithms with desirable theoretical guarantees typically involve drastically different settings of hyperparameters, such as step size schemes and batch sizes, in different regimes. Despite the appealing theoretical results, such divisive strategies provide little, if any, insight to practitioners to select algorithms that work broadly without tweaking the hyperparameters. In this work, blending the “geometrization” technique introduced by [L. Lei and M. I. Jordan, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017, pp. 148--156] and the SARAH algorithm of [L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takáč, Proceedings of the 34th International Conference on Machine Learning, 2017, pp. 2613--2621], we propose the geometrized SARAH algorithm for nonconvex finite-sum and stochastic optimization. Our algorithm is proved to achieve adaptivity to both the magnitude of the target accuracy and the Polyak--Łojasiewicz (PL) constant, if present. In addition, it achieves the best-available convergence rate for non-PL objectives simultaneously while outperforming existing algorithms for PL objectives.},
  archive      = {J_SIMODS},
  author       = {Samuel Horváth and Lihua Lei and Peter Richtárik and Michael I. Jordan},
  doi          = {10.1137/21M1394308},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {634-648},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Adaptivity of stochastic gradient methods for nonconvex optimization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tukey depths and hamilton–jacobi differential equations.
<em>SIMODS</em>, <em>4</em>(2), 604–633. (<a
href="https://doi.org/10.1137/21M1411998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Widespread application of modern machine learning has increased the need for robust statistical algorithms. This work studies one such fundamental statistical concept known as the Tukey depth. We study the problem in the continuum (population) limit. In particular, we formally derive the associated necessary conditions, which take the form of a first-order partial differential equation which is necessarily satisfied at points where the Tukey depth is smooth. We discuss the interpretation of this formal necessary condition in terms of the viscosity solution of a Hamilton--Jacobi equation, but with a nonclassical Hamiltonian with discontinuous dependence on the gradient at zero. We prove that this equation possesses a unique viscosity solution and that this solution always bounds the Tukey depth from below. In certain cases we prove that the Tukey depth is equal to the viscosity solution, and we give some illustrations of standard numerical methods from the optimal control community which deal directly with the partial differential equation. We conclude by outlining several promising research directions both in terms of new numerical algorithms and theoretical challenges.},
  archive      = {J_SIMODS},
  author       = {Martin Molina-Fructuoso and Ryan Murray},
  doi          = {10.1137/21M1411998},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {604-633},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Tukey depths and hamilton--jacobi differential equations},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein-based projections with applications to inverse
problems. <em>SIMODS</em>, <em>4</em>(2), 581–603. (<a
href="https://doi.org/10.1137/20M1376790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse problems consist of recovering a signal from a collection of noisy measurements. These are typically cast as optimization problems, with classic approaches using a data fidelity term and an analytic regularizer that stabilizes recovery. Recent plug-and-play (PnP) works propose replacing the operator for analytic regularization in optimization methods by a data-driven denoiser. These schemes obtain state-of-the-art results, but at the cost of limited theoretical guarantees. To bridge this gap, we present a new algorithm that takes samples from the manifold of true data as input and outputs an approximation of the projection operator onto this manifold. Under standard assumptions, we prove this algorithm generates a learned operator, called Wasserstein-based projection (WP), that approximates the true projection with high probability. Thus, WPs can be inserted into optimization methods in the same manner as PnP, but now with theoretical guarantees. Provided numerical examples show WPs obtain state-of-the-art results for unsupervised PnP signal recovery. All codes for this work can be found at https://github.com/swufung/WassersteinBasedProjections.},
  archive      = {J_SIMODS},
  author       = {Howard Heaton and Samy Wu Fung and Alex Tong Lin and Stanley Osher and Wotao Yin},
  doi          = {10.1137/20M1376790},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {581-603},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Wasserstein-based projections with applications to inverse problems},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantitative approximation results for complex-valued neural
networks. <em>SIMODS</em>, <em>4</em>(2), 553–580. (<a
href="https://doi.org/10.1137/21M1429540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Until recently, applications of neural networks in machine learning have almost exclusively relied on real-valued networks. It was recently observed, however, that complex-valued neural networks (CVNNs) exhibit superior performance in applications in which the input is naturally complex-valued, such as MRI fingerprinting. While the mathematical theory of real-valued networks has, by now, reached some level of maturity, this is far from true for complex-valued networks. In this paper, we analyze the expressivity of complex-valued networks by providing explicit quantitative error bounds for approximating $C^n$ functions on compact subsets of $\mathbb{C}^d$ by CVNNs that employ the modReLU activation function, given by $\sigma(z) = {ReLU}(|z| - 1), {sgn} (z)$, which is one of the most popular complex activation functions used in practice. We show that the derived approximation rates are optimal (up to log factors) in the class of modReLU networks with weights of moderate growth.},
  archive      = {J_SIMODS},
  author       = {Andrei Caragea and Dae Gwan Lee and Johannes Maly and Götz Pfander and Felix Voigtlaender},
  doi          = {10.1137/21M1429540},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {553-580},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Quantitative approximation results for complex-valued neural networks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic geometry to generalize the mondrian process.
<em>SIMODS</em>, <em>4</em>(2), 531–552. (<a
href="https://doi.org/10.1137/20M1354490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stable under iteration (STIT) tessellation process is a stochastic process that produces a recursive partition of space with cut directions drawn independently from a distribution over the sphere. The case of random axis-aligned cuts is known as the Mondrian process. Random forests and Laplace kernel approximations built from the Mondrian process have led to efficient online learning methods and Bayesian optimization. In this work, we utilize tools from stochastic geometry to resolve some fundamental questions concerning STIT processes in machine learning. First, we show that STIT processes can be efficiently simulated by lifting to a higher-dimensional axis-aligned Mondrian process. Second, we characterize all possible kernels that STIT processes and their mixtures can approximate. We also give a uniform convergence rate for the approximation error of the STIT kernels to the targeted kernels, completely generalizing the work of Balog et al. [The Mondrian kernel, 2016] from the Mondrian case. Third, we obtain consistency results for STIT forests in density estimation and regression. Finally, we give a precise formula for the density estimator arising from a STIT forest. This allows for precise comparisons between the STIT forest, the STIT kernel, and the targeted kernel in density estimation. Our paper calls for further developments at the novel intersection of stochastic geometry and machine learning.},
  archive      = {J_SIMODS},
  author       = {Eliza O&#39;Reilly and Ngoc Mai Tran},
  doi          = {10.1137/20M1354490},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {531-552},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stochastic geometry to generalize the mondrian process},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximation bounds for sparse programs. <em>SIMODS</em>,
<em>4</em>(2), 514–530. (<a
href="https://doi.org/10.1137/21M1398677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that sparsity-constrained optimization problems over low-dimensional spaces tend to have a small duality gap. We use the Shapley--Folkman theorem to derive both data-driven bounds on the duality gap and an efficient primalization procedure to recover feasible points satisfying these bounds. These error bounds are proportional to the rate of growth of the objective with the target cardinality, which means in particular that the relaxation is nearly tight as soon as the target cardinality is large enough so that only uninformative features are added.},
  archive      = {J_SIMODS},
  author       = {Armin Askari and Alexandre d&#39;Aspremont and Laurent El Ghaoui},
  doi          = {10.1137/21M1398677},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {514-530},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximation bounds for sparse programs},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical methods for minimax estimation in linear models
with unknown design over finite alphabets. <em>SIMODS</em>,
<em>4</em>(2), 490–513. (<a
href="https://doi.org/10.1137/21M1398860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a minimax optimal estimation procedure for ${F}$ and ${\Omega}$ in matrix valued linear models $Y = {{F}} {{\Omega}} + Z$, where the parameter matrix ${\Omega}$ and the design matrix ${F}$ are unknown but the latter takes values in a known finite set. The proposed finite alphabet linear model is justified in a variety of applications, ranging from signal processing to cancer genetics. We show that this allows one to separate ${F}$ and ${\Omega}$ uniquely under weak identifiability conditions, a task which is not doable, in general. To this end we quantify in the noiseless case, that is, $Z = 0$, the perturbation range of $Y$ in order to obtain stable recovery of ${F}$ and ${\Omega}$. Based on this, we derive an iterative Lloyd&#39;s type estimation procedure that attains minimax estimation rates for ${\Omega}$ and ${F}$ for Gaussian error matrix $Z$. In contrast to the least squares solution the estimation procedure can be computed efficiently and scales linearly with the total number of observations. We confirm our theoretical results in a simulation study and illustrate it with a genetic sequencing data example.},
  archive      = {J_SIMODS},
  author       = {Merle Behr and Axel Munk},
  doi          = {10.1137/21M1398860},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {490-513},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Statistical methods for minimax estimation in linear models with unknown design over finite alphabets},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What kinds of functions do deep neural networks learn?
Insights from variational spline theory. <em>SIMODS</em>, <em>4</em>(2),
464–489. (<a href="https://doi.org/10.1137/21M1418642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit (ReLU) activations to data. We propose a new function space, which is related to classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data-fitting problems over functions from this space. The function space consists of compositions of functions from the Banach space of second-order bounded variation in the Radon domain. This Banach space has a sparsity-promoting norm, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank-bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.},
  archive      = {J_SIMODS},
  author       = {Rahul Parhi and Robert D. Nowak},
  doi          = {10.1137/21M1418642},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {464-489},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {What kinds of functions do deep neural networks learn? insights from variational spline theory},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A nonlinear matrix decomposition for mining the zeros of
sparse data. <em>SIMODS</em>, <em>4</em>(2), 431–463. (<a
href="https://doi.org/10.1137/21M1405769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a simple iterative solution to a widely recurring problem in multivariate data analysis: given a sparse nonnegative matrix ${\mathbf{X}}$, how to estimate a low-rank matrix ${{\Theta}}$ such that ${{X}} \approx f({{\Theta}})$, where $f$ is an elementwise nonlinearity? We develop a latent variable model for this problem and consider those sparsifying nonlinearities, popular in neural networks, that map all negative values to zero. The model seeks to explain the variability of sparse high-dimensional data in terms of a smaller number of degrees of freedom. We show that exact inference in this model is tractable and derive an expectation-maximization (EM) algorithm to estimate the low-rank matrix ${{\Theta}}$. Notably, we do not parameterize ${{\Theta}}$ as a product of smaller matrices to be alternately optimized; instead, we estimate ${{\Theta}}$ directly via the singular value decomposition of matrices that are repeatedly inferred (at each iteration of the EM algorithm) from the model&#39;s posterior distribution. We use the model to analyze large sparse matrices that arise from data sets of binary, grayscale, and color images. In all of these cases, we find that the model discovers much lower-rank decompositions than purely linear approaches.},
  archive      = {J_SIMODS},
  author       = {Lawrence K. Saul},
  doi          = {10.1137/21M1405769},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {431-463},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A nonlinear matrix decomposition for mining the zeros of sparse data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spectral discovery of jointly smooth features for multimodal
data. <em>SIMODS</em>, <em>4</em>(1), 410–430. (<a
href="https://doi.org/10.1137/21M141590X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a spectral method for deriving functions that are jointly smooth on multiple observed manifolds. This allows us to register measurements of the same phenomenon by heterogeneous sensors and to reject sensor-specific noise. Our method is unsupervised and primarily consists of two steps. First, using kernels, we obtain a subspace spanning smooth functions on each separate manifold. Then, we apply a spectral method to the obtained subspaces and discover functions that are jointly smooth on all manifolds. We show analytically that our method is guaranteed to provide a set of orthogonal functions that are as jointly smooth as possible, ordered by increasing Dirichlet energy from the smoothest to the least smooth. In addition, we show that the extracted functions can be efficiently extended to unseen data using the Nyström method. We demonstrate the proposed method on both simulated and real measured data and compare the results to nonlinear, kernel-based variants of the seminal canonical correlation analysis. Particularly, we show superior results for sleep stage identification. In addition, we show how the proposed method can be leveraged for finding minimal realizations of parameter spaces of nonlinear dynamical systems.},
  archive      = {J_SIMODS},
  author       = {Felix Dietrich and Or Yair and Rotem Mulayoff and Ronen Talmon and Ioannis G. Kevrekidis},
  doi          = {10.1137/21M141590X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {410-430},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Spectral discovery of jointly smooth features for multimodal data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized CUR decomposition for matrix pairs.
<em>SIMODS</em>, <em>4</em>(1), 386–409. (<a
href="https://doi.org/10.1137/21M1432119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generalized CUR (GCUR) decomposition for matrix pairs $(A,B)$. Given matrices $A$ and $B$ with the same number of columns, such a decomposition provides low-rank approximations of both matrices simultaneously in terms of some of their rows and columns. We obtain the indices for selecting the subset of rows and columns of the original matrices using the discrete empirical interpolation method (DEIM) on the generalized singular vectors. When $B$ is square and nonsingular, there are close connections between the GCUR of $(A,B)$ and the DEIM-induced CUR of $AB^{-1}$. When $B$ is the identity, the GCUR decomposition of $A$ coincides with the DEIM-induced CUR decomposition of $A$. We also show similar connection between the GCUR of $(A,B)$ and the CUR of $AB^+$ for a nonsquare but full-rank matrix $B$, where $B^+$ denotes the Moore--Penrose pseudoinverse of $B$. While a CUR decomposition acts on one data set, a GCUR factorization jointly decomposes two data sets. The algorithm may be suitable for applications where one is interested in extracting the most discriminative features from one data set relative to another data set. In numerical experiments, we demonstrate the advantages of the new method over the standard CUR approximation for recovering data perturbed with colored noise and subgroup discovery.},
  archive      = {J_SIMODS},
  author       = {Perfect Y. Gidisu and Michiel E. Hochstenbach},
  doi          = {10.1137/21M1432119},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {386-409},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A generalized CUR decomposition for matrix pairs},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust classification under <span
class="math inline"><em>ℓ</em><sub>0</sub></span> attack for the
gaussian mixture model. <em>SIMODS</em>, <em>4</em>(1), 362–385. (<a
href="https://doi.org/10.1137/21M1426286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that machine learning models are vulnerable to small but cleverly designed adversarial perturbations that can cause misclassification. While there has been major progress in designing attacks and defenses for various adversarial settings, many fundamental and theoretical problems are yet to be resolved. In this paper, we consider classification in the presence of $\ell_0$-bounded adversarial perturbations, also known as sparse attacks. This setting is significantly different from other $\ell_p$-adversarial settings with $p\geq1$, as the $\ell_0$-ball is nonconvex and highly nonsmooth. Under the assumption that data is distributed according to the Gaussian mixture model, our goal is to characterize the optimal robust classifier and the corresponding robust classification error as well as a variety of tradeoffs between robustness, accuracy, and the adversary&#39;s budget. To this end, we develop a novel classification algorithm called FilTrun that has two main modules: filtration and truncation. The key idea of our method is to first filter out the nonrobust coordinates of the input and then apply a carefully designed truncated inner product for classification. By analyzing the performance of FilTrun, we derive an upper bound on the optimal robust classification error. We further find a lower bound by designing a specific adversarial strategy that enables us to derive the corresponding robust classifier and its achieved error. For the case that the covariance matrix of the Gaussian mixtures is diagonal, we show that as the input&#39;s dimension gets large, the upper and lower bounds converge; i.e., we characterize the asymptotically optimal robust classifier. Throughout, we discuss several examples that illustrate interesting behaviors such as the existence of a phase transition for the adversary&#39;s budget determining whether the effect of adversarial perturbation can be fully neutralized or not.},
  archive      = {J_SIMODS},
  author       = {Payam Delgosha and Hamed Hassani and Ramtin Pedarsani},
  doi          = {10.1137/21M1426286},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {362-385},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Robust classification under $\ell_0$ attack for the gaussian mixture model},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Overcomplete order-3 tensor decomposition, blind
deconvolution, and gaussian mixture models. <em>SIMODS</em>,
<em>4</em>(1), 336–361. (<a
href="https://doi.org/10.1137/21M1399415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithm for tensor decomposition, based on the simultaneous diagonalization algorithm, and apply our new algorithmic ideas to blind deconvolution and Gaussian mixture models. Our first contribution is a simple and efficient algorithm to decompose certain symmetric overcomplete order-3 tensors, that is, three dimensional arrays of the form $T = \sum_{i=1}^n a_i \otimes a_i \otimes a_i$ where the $a_i$s are not linearly independent. Our algorithm comes with a detailed robustness analysis. Our second contribution builds on top of our tensor decomposition algorithm to expand the family of Gaussian mixture models whose parameters can be estimated efficiently. These ideas are also presented in a more general framework of blind deconvolution that makes them applicable to mixture models of identical but very general distributions, including all centrally symmetric distributions with finite 6th moment.},
  archive      = {J_SIMODS},
  author       = {Haolin Chen and Luis Rademacher},
  doi          = {10.1137/21M1399415},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {336-361},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Overcomplete order-3 tensor decomposition, blind deconvolution, and gaussian mixture models},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generative variational model for inverse problems in
imaging. <em>SIMODS</em>, <em>4</em>(1), 306–335. (<a
href="https://doi.org/10.1137/21M1414978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the development, analysis, and numerical realization of a novel variational model for the regularization of inverse problems in imaging. The proposed model is inspired by the architecture of generative convolutional neural networks; it aims to generate the unknown from variables in a latent space via multilayer convolutions and nonlinear penalties, and penalizes an associated cost. In contrast to conventional neural-network-based approaches, however, the convolution kernels are learned directly from the measured data such that no training is required. The present work provides a mathematical analysis of the proposed model in a function space setting, including proofs for regularity and existence/stability of solutions, and convergence for vanishing noise. Moreover, in a discretized setting, a numerical algorithm for solving various types of inverse problems with the proposed model is derived. Numerical results are provided for applications in inpainting, denoising, deblurring under noise, superresolution, and JPEG decompression with multiple test images.},
  archive      = {J_SIMODS},
  author       = {Andreas Habring and Martin Holler},
  doi          = {10.1137/21M1414978},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {306-335},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A generative variational model for inverse problems in imaging},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast cluster detection in networks by first order
optimization. <em>SIMODS</em>, <em>4</em>(1), 285–305. (<a
href="https://doi.org/10.1137/21M1408658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster detection plays a fundamental role in the analysis of data. In this paper, we focus on the use of $s$-defective clique models for network-based cluster detection and propose a nonlinear optimization approach that efficiently handles those models in practice. In particular, we introduce an equivalent continuous formulation for the problem under analysis, and we analyze some tailored variants of the Frank--Wolfe algorithm that enable us to quickly find maximal $s$-defective cliques. The good practical behavior of those algorithmic tools, which is closely connected to their support identification properties, makes them very appealing in practical applications. The reported numerical results clearly show the effectiveness of the proposed approach.},
  archive      = {J_SIMODS},
  author       = {Immanuel M. Bomze and Francesco Rinaldi and Damiano Zeffiro},
  doi          = {10.1137/21M1408658},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {285-305},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Fast cluster detection in networks by first order optimization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Binary classification of gaussian mixtures: Abundance of
support vectors, benign overfitting, and regularization.
<em>SIMODS</em>, <em>4</em>(1), 260–284. (<a
href="https://doi.org/10.1137/21M1415121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks generalize well despite being exceedingly overparameterized and being trained without explicit regularization. This curious phenomenon has inspired extensive research activity in establishing its statistical principles: Under what conditions is it observed? How do these depend on the data and on the training algorithm? When does regularization benefit generalization? While such questions remain wide open for deep neural nets, recent works have attempted gaining insights by studying simpler, often linear, models. Our paper contributes to this growing line of work by examining binary linear classification under a generative Gaussian mixture model in which the feature vectors take the form ${{\it x}}=\pm{{\eta}}+{{\it q}}$, where for a mean vector $\eta$ and feature noise ${{\it q}} \sim \mathcal{N}(0,{{\Sigma}})$. Motivated by recent results on the implicit bias of gradient descent, we study both max-margin support vector machine (SVM) classifiers (corresponding to logistic loss) and min-norm interpolating classifiers (corresponding to least-squares loss). First, we leverage an idea introduced in [V. Muthukumar et al., arXiv:2005.08054, 2020a] to relate the SVM solution to the min-norm interpolating solution. Second, we derive novel nonasymptotic bounds on the classification error of the latter. Combining the two, we present novel sufficient conditions on the covariance spectrum and on the signal-to-noise ratio (SNR) $SNR={||{{\eta}}||_2^4}/{{\eta}}^T{{\Sigma\eta}}$ under which interpolating estimators achieve asymptotically optimal performance as overparameterization increases. Interestingly, our results extend to a noisy model with constant probability noise flips. Contrary to previously studied discriminative data models, our results emphasize the crucial role of the SNR and its interplay with the data covariance. Finally, via a combination of analytical arguments and numerical demonstrations we identify conditions under which the interpolating estimator performs better than corresponding regularized estimates.},
  archive      = {J_SIMODS},
  author       = {Ke Wang and Christos Thrampoulidis},
  doi          = {10.1137/21M1415121},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {260-284},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Binary classification of gaussian mixtures: Abundance of support vectors, benign overfitting, and regularization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Randomized wasserstein barycenter computation: Resampling
with statistical guarantees. <em>SIMODS</em>, <em>4</em>(1), 229–259.
(<a href="https://doi.org/10.1137/20M1385263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a hybrid resampling method to approximate finitely supported Wasserstein barycenters on large-scale datasets, which can be combined with any exact solver. Nonasymptotic bounds on the expected error of the objective value as well as the barycenters themselves allow one to calibrate computational cost and statistical accuracy. The rate of these upper bounds is shown to be optimal and independent of the underlying dimension, which appears only in the constants. Using a simple modification of the subgradient descent algorithm of Cuturi and Doucet, we showcase the applicability of our method on myriad simulated datasets, as well as a real-data example from cell microscopy, which are out of reach for state-of-the-art algorithms for computing Wasserstein barycenters.},
  archive      = {J_SIMODS},
  author       = {Florian Heinemann and Axel Munk and Yoav Zemel},
  doi          = {10.1137/20M1385263},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {229-259},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Randomized wasserstein barycenter computation: Resampling with statistical guarantees},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying 3D genome organization in diploid organisms via
euclidean distance geometry. <em>SIMODS</em>, <em>4</em>(1), 204–228.
(<a href="https://doi.org/10.1137/21M1390372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial organization of the genome in the cell nucleus plays an important role for gene regulation, replication of the deoxyribonucleic acid (DNA), and genomic integrity. Through the development of chromosome conformation capture experiments (such as 3C, 4C, and Hi-C) it is now possible to obtain the contact frequencies of the DNA at the whole-genome level. In this paper, we study the problem of reconstructing the three-dimensional (3D) organization of the genome from such whole-genome contact frequencies. A standard approach is to transform the contact frequencies into noisy distance measurements and then apply semidefinite programming formulations to obtain the 3D configuration. However, neglected in such reconstructions is the fact that most eukaryotes including humans are diploid and therefore contain two copies of each genomic locus. We prove that the 3D organization of the DNA is not identifiable from the distance measurements derived from contact frequencies in diploid organisms. In fact, there are infinitely many solutions even in the noise-free setting. We then discuss various additional biologically relevant and experimentally measurable constraints (including distances between neighboring genomic loci and higher-order interactions) and prove identifiability under these conditions. Furthermore, we provide semidefinite programming formulations for computing the 3D embedding of the DNA with these additional constraints and show that we can recover the true 3D embedding with high accuracy from both noiseless and noisy measurements. Finally, we apply our algorithm to real pairwise and higher-order contact frequency data and show that we can recover known genome organization patterns.},
  archive      = {J_SIMODS},
  author       = {Anastasiya Belyaeva and Kaie Kubjas and Lawrence J. Sun and Caroline Uhler},
  doi          = {10.1137/21M1390372},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {204-228},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Identifying 3D genome organization in diploid organisms via euclidean distance geometry},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein barycenters are NP-hard to compute.
<em>SIMODS</em>, <em>4</em>(1), 179–203. (<a
href="https://doi.org/10.1137/21M1390062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing Wasserstein barycenters (a.k.a. optimal transport barycenters) is a fundamental problem in geometry which has recently attracted considerable attention due to many applications in data science. While there exist polynomial-time algorithms in any fixed dimension, all known running times suffer exponentially in the dimension. It is an open question whether this exponential dependence is improvable to a polynomial dependence. This paper proves that unless ${P} = {NP}$, the answer is no. This uncovers a “curse of dimensionality” for Wasserstein barycenter computation which does not occur for optimal transport computation. Moreover, our hardness results for computing Wasserstein barycenters extend to approximate computation, to seemingly simple cases of the problem, and to averaging probability distributions in other optimal transport metrics.},
  archive      = {J_SIMODS},
  author       = {Jason M. Altschuler and Enric Boix-Adserà},
  doi          = {10.1137/21M1390062},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {179-203},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Wasserstein barycenters are NP-hard to compute},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Positive semi-definite embedding for dimensionality
reduction and out-of-sample extensions. <em>SIMODS</em>, <em>4</em>(1),
153–178. (<a href="https://doi.org/10.1137/20M1370653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning or statistics, it is often desirable to reduce the dimensionality of a sample of data points in a high dimensional space $\mathbb{R}^d$. This paper introduces a dimensionality reduction method where the embedding coordinates are the eigenvectors of a positive semi-definite kernel obtained as the solution of an infinite dimensional analogue of a semi-definite program. This embedding is adaptive and non-linear. We discuss this problem both with weak and strong smoothness assumptions about the learned kernel. A main feature of our approach is the existence of an out-of-sample extension formula of the embedding coordinates in both cases. This extrapolation formula yields an extension of the kernel matrix to a data-dependent Mercer kernel function. Our empirical results indicate that this embedding method is more robust with respect to the influence of outliers compared with a spectral embedding method.},
  archive      = {J_SIMODS},
  author       = {Michaël Fanuel and Antoine Aspeel and Jean-Charles Delvenne and Johan A. K. Suykens},
  doi          = {10.1137/20M1370653},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {153-178},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Positive semi-definite embedding for dimensionality reduction and out-of-sample extensions},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimensionality reduction, regularization, and generalization
in overparameterized regressions. <em>SIMODS</em>, <em>4</em>(1),
126–152. (<a href="https://doi.org/10.1137/20M1387821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overparameterization in deep learning is powerful: Very large models fit the training data perfectly and yet often generalize well. This realization brought back the study of linear models for regression, including ordinary least squares (OLS), which, like deep learning, shows a “double-descent” behavior: (1) The risk (expected out-of-sample prediction error) can grow arbitrarily when the number of parameters $p$ approaches the number of samples $n$, and (2) the risk decreases with $p$ for $p&gt;n$, sometimes achieving a lower value than the lowest risk for $p&lt;n$. The divergence of the risk for OLS can be avoided with regularization. In this work, we show that for some data models it can also be avoided with a principal component analysis--based dimensionality reduction (PCA-OLS, also known as principal component regression). We provide nonasymptotic bounds for the risk of PCA-OLS by considering the alignments of the population and empirical principal components. We show that dimensionality reduction improves robustness while OLS is arbitrarily susceptible to adversarial attacks, particularly in the overparameterized regime. We compare PCA-OLS theoretically and empirically with a wide range of projection-based methods, including random projections, partial least squares, and certain classes of linear two-layer neural networks. These comparisons are made for different data generation models to assess the sensitivity to signal-to-noise and the alignment of regression coefficients with the features. We find that methods in which the projection depends on the training data can outperform methods where the projections are chosen independently of the training data, even those with oracle knowledge of population quantities, another seemingly paradoxical phenomenon that has been identified previously. This suggests that overparameterization may not be necessary for good generalization.},
  archive      = {J_SIMODS},
  author       = {Ningyuan Teresa Huang and David W. Hogg and Soledad Villar},
  doi          = {10.1137/20M1387821},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {126-152},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Dimensionality reduction, regularization, and generalization in overparameterized regressions},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unifying generative model for graph learning algorithms:
Label propagation, graph convolutions, and combinations.
<em>SIMODS</em>, <em>4</em>(1), 100–125. (<a
href="https://doi.org/10.1137/21M1395351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning on graphs is a widely applicable problem in network science and machine learning. Two standard algorithms---label propagation and graph neural networks---both operate by repeatedly passing information along edges, the former by passing labels and the latter by passing node features, modulated by neural networks. These two types of algorithms have largely developed separately, and there is little understanding about the structure of network data that would make one of these approaches work particularly well compared to the other or when the approaches can be meaningfully combined. Here, we develop a Markov random field model for the data generation process of node attributes, based on correlations of attributes on and between vertices, that motivates and unifies these algorithmic approaches. We show that label propagation, a linearized graph convolutional network, and their combination can all be derived as conditional expectations under our model. In addition, the data model highlights problems with existing graph neural networks (and provides solutions), serves as a rigorous statistical framework for understanding issues such as over-smoothing, creates a testbed for evaluating inductive learning performance, and provides a way to sample graph attributes that resemble empirical data. We also find that a new algorithm derived from our data generation model, which we call a linear graph convolution, performs extremely well in practice on empirical data, and we provide theoretical justification for why this is the case.},
  archive      = {J_SIMODS},
  author       = {Junteng Jia and Austin R. Benson},
  doi          = {10.1137/21M1395351},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {100-125},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A unifying generative model for graph learning algorithms: Label propagation, graph convolutions, and combinations},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balancing geometry and density: Path distances on
high-dimensional data. <em>SIMODS</em>, <em>4</em>(1), 72–99. (<a
href="https://doi.org/10.1137/20M1386657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New geometric and computational analyses of power-weighted shortest path distances (PWSPDs) are presented. By illuminating the way these metrics balance geometry and density in the underlying data, we clarify their key parameters and illustrate how they provide multiple perspectives for data analysis. Comparisons are made with related data-driven metrics, which illustrate the broader role of density in kernel-based unsupervised and semisupervised machine learning. Computationally, we relate PWSPDs on complete weighted graphs to their analogues on weighted nearest neighbor graphs, providing high probability guarantees on their equivalence that are near-optimal. Connections with percolation theory are developed to establish estimates on the bias and variance of PWSPDs in the finite sample setting. The theoretical results are bolstered by illustrative experiments, demonstrating the versatility of PWSPDs for a wide range of data settings. Throughout the paper, our results generally require only that the underlying data is sampled from a compact low-dimensional manifold, and depend most crucially on the intrinsic dimension of this manifold, rather than its ambient dimension.},
  archive      = {J_SIMODS},
  author       = {Anna Little and Daniel McKenzie and James M. Murphy},
  doi          = {10.1137/20M1386657},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {72-99},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Balancing geometry and density: Path distances on high-dimensional data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards compact neural networks via end-to-end training: A
bayesian tensor approach with automatic rank determination.
<em>SIMODS</em>, <em>4</em>(1), 46–71. (<a
href="https://doi.org/10.1137/21M1391444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-training model compression can reduce the inference costs of deep neural networks, but uncompressed training still consumes enormous hardware resources and energy. To enable low-energy training on edge devices, it is highly desirable to directly train a compact neural network from scratch with a low memory cost. Low-rank tensor decomposition is an effective approach to reduce the memory and computing costs of large neural networks. However, directly training low-rank tensorized neural networks is a very challenging task because it is hard to determine a proper tensor rank a priori, and the tensor rank controls both model complexity and accuracy. This paper presents a novel end-to-end framework for low-rank tensorized training. We first develop a Bayesian model that supports various low-rank tensor formats (e.g., CANDECOMP/PARAFAC, Tucker, tensor-train, and tensor-train matrix) and reduces neural network parameters with automatic rank determination during training. Then we develop a customized Bayesian solver to train large-scale tensorized neural networks. Our training methods shows orders-of-magnitude parameter reduction and little accuracy loss (or even better accuracy) in the experiments. On a very large deep learning recommendation system with over $4.2\times 10^9$ model parameters, our method can reduce the parameter number to $1.6\times 10^5$ automatically in the training process (i.e., by $2.6\times 10^4$ times) while achieving almost the same accuracy. Code is available at https://github.com/colehawkins/bayesian-tensor-rank-determination.},
  archive      = {J_SIMODS},
  author       = {Cole Hawkins and Xing Liu and Zheng Zhang},
  doi          = {10.1137/21M1391444},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {46-71},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Towards compact neural networks via end-to-end training: A bayesian tensor approach with automatic rank determination},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensitivity-informed provable pruning of neural networks.
<em>SIMODS</em>, <em>4</em>(1), 26–45. (<a
href="https://doi.org/10.1137/20M1383239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a family of pruning algorithms that sparsifies the parameters of a trained model in a way that approximately preserves the model&#39;s predictive accuracy. Our algorithms use a small batch of input points to construct a data-informed importance sampling distribution over the network&#39;s parameters and use either a sampling-based or a deterministic pruning procedure, or an adaptive mixture of both, to discard redundant weights. Our methods are simultaneously computationally efficient, provably accurate, and broadly applicable to various network architectures and data distributions. The presented approaches are simple to implement and can be easily integrated into standard prune-retrain pipelines. We present empirical comparisons showing that our algorithms reliably generate highly compressed networks that incur minimal loss in performance, regardless of whether the original network is fully trained or randomly initialized.},
  archive      = {J_SIMODS},
  author       = {Cenk Baykal and Lucas Liebenwein and Igor Gilitschenski and Dan Feldman and Daniela Rus},
  doi          = {10.1137/20M1383239},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {26-45},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Sensitivity-informed provable pruning of neural networks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Block bregman majorization minimization with extrapolation.
<em>SIMODS</em>, <em>4</em>(1), 1–25. (<a
href="https://doi.org/10.1137/21M1432661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a class of nonsmooth nonconvex optimization problems whose objective is the sum of a block relative smooth function and a proper and lower semicontinuous block separable function. Although the analysis of block proximal gradient (BPG) methods for the class of block $L$-smooth functions has been successfully extended to Bregman BPG methods that deal with the class of block relative smooth functions, accelerated Bregman BPG methods are scarce and challenging to design. Taking our inspiration from Nesterov-type acceleration and the majorization-minimization scheme, we propose a block alternating Bregman majorization minimization framework with extrapolation (BMME). We prove subsequential convergence of BMME to a first-order stationary point under mild assumptions and study its global convergence under stronger conditions. We illustrate the effectiveness of BMME on the penalized orthogonal nonnegative matrix factorization problem.},
  archive      = {J_SIMODS},
  author       = {Le Thi Khanh Hien and Duy Nhat Phan and Nicolas Gillis and Masoud Ahookhosh and Panagiotis Patrinos},
  doi          = {10.1137/21M1432661},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {1-25},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Block bregman majorization minimization with extrapolation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
