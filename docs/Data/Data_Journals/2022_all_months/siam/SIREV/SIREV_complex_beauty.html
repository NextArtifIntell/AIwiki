<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIREV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sirev---50">SIREV - 50</h2>
<ul>
<li><details>
<summary>
(2022a). Book reviews. <em>SIREV</em>, <em>64</em>(4), 1083–1095.
(<a href="https://doi.org/10.1137/22N975597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The book reviews sections starts off with a featured review that is dedicated in great depth to the collected volume Control Applications for Biomedical Engineering Systems, edited by Ahmad Taher Azar. The review originated as a joint project of Anita Layton&#39;s group at the University of Waterloo and has by far the largest number of authors in the history of our section. The section continues with biomathematical aspects. The book Physical Models of Living Systems: Probability, Simulation, Dynamics, by Philip C. Nelson, is reviewed by Krešimir Josić, who recommends it as a “core reference” for courses in the field. A review of the book The Mathematics of Fluid Flow through Porous Media, by Myron B. Allen, follows. Javier E. Santos concludes his review with a very nuanced assessment. Subsequently, Cornelius W. Oosterlee, in his review of Malliavin Calculus in Finance: Theory and Practice, by Elisa Alos and David Garcia Lorite, praises it as a “fascinating texbook” providing “balanced and unique content.” Stochastic aspects also appear in the book Stochastic Modelling of Reaction-Diffusion Processes, by Radek Erban and S. Jonathan Chapman. This book is reviewed by Mario Annunziato, who discusses the pros and cons of the book in detail. The section is concluded by Calvin Tadmon&#39;s review of the book Optimal Control of Dynamical Systems Driven by Vector Measures: Theory and Applications, who praises this book highly.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/22N975597},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {1083-1095},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When randomness helps in undersampling. <em>SIREV</em>,
<em>64</em>(4), 1062–1080. (<a
href="https://doi.org/10.1137/21M1441006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signals cannot always be sampled at their full desired resolution. In this Education article, we explore the benefits of randomly subsampling a signal&#39;s frequency spectrum. Whereas uniform subsampling introduces structural artifacts in the time series, random subsampling introduces a type of noise whose behavior we quantify. This analysis gives insight into the reasons why random sampling is employed in more sophisticated processing techniques such as compressive sensing. Our treatment involves topics such as frequency analysis, aliasing, and convolution, which are commonly encountered in undergraduate courses on signal processing or engineering mathematics. Meanwhile, we also draw from concepts in probability and statistics which are rarely discussed at the undergraduate level in the contexts of frequency analysis, aliasing, and convolution. The signal processing codes and data used in this work can be downloaded from https://mines.edu/~mwakin/software.},
  archive      = {J_SIREV},
  author       = {Roel Snieder and Michael B. Wakin},
  doi          = {10.1137/21M1441006},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {1062-1080},
  shortjournal = {SIAM Rev.},
  title        = {When randomness helps in undersampling},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized dual transform: Linear algebra and geometry of
(pseudo)inverting a matrix. <em>SIREV</em>, <em>64</em>(4), 1031–1061.
(<a href="https://doi.org/10.1137/19M1270410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new theory of a generalized dual transform of a list of vectors is presented, emphasizing its geometry as well as its linear algebra. We review the theory for independent vectors and find a new butterfly identity. This leads us to a parallel process to compute the dual vectors. Next we take the general case of arbitrary vectors, which may be linearly dependent. We start with two very symmetric axioms for the general dual transform. We show how the dual transform is closely related to the pseudoinverse of a matrix, defined by the Moore and Penrose axioms. We find that each dual vector is a sort of contrast of its corresponding original vector against the background of the rest of the vectors. To get the dual vector, we operate on the original vector by either an orthogonal projector or a protractor defined in terms of the rest of the vectors. Which operator is used depends on whether or not the original vector is linearly independent of the rest. When we update the protractor to include a new vector, a stereographic projection appears unexpectedly. Basic identities for dual vectors are proved, which result in closed-form algebraic processes to compute the dual list. The processes transform any list of vectors into its dual list, regardless of dependencies among the primal vectors. Linear-algebraic recipes are developed for a complete, reversible Gram--Schmidt process, a vector version of the Greville process, and a simple parallel butterfly process to get the dual list.},
  archive      = {J_SIREV},
  author       = {L. P. Withers, Jr.},
  doi          = {10.1137/19M1270410},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {1031-1061},
  shortjournal = {SIAM Rev.},
  title        = {A generalized dual transform: Linear algebra and geometry of (Pseudo)Inverting a matrix},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Education. <em>SIREV</em>, <em>64</em>(4), 1029. (<a
href="https://doi.org/10.1137/22N975585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SIAM Review contains two papers in the Education section. The first, “A Generalized Dual Transform: Linear Algebra and Geometry of (Pseudo)Inverting a Matrix,” is presented by L. P. Withers, Jr. For a linear subspace $A$ of a vector space $V$, we may have a nonorthogonal basis of $A$. We could obtain an orthogonal basis (e.g., by the Gram--Schmidt orthogonalization procedure) and the orthogonality helps us to represent each element $b\in A$ as a linear combination of the new basis in an easy way. How should we express $b$ as a linear combination of the original, unorthogonalized vectors? One suggestion is to construct a complementary list of vectors, called a dual list, such that each pair of vectors $a^i,a^j$ on that list are orthogonal and each vector has a length of one. The construction is called the dual transform. Involving the complementary subspace of $A$ in $V$ and orthogonal projections, we obtain again a simple formula for the representation of $b$. Next to generating dual vectors, the Gram--Schmidt orthogonalization procedure exhibits other interesting properties, which lead to a parallel so-called butterfly process for computing the dual transform. The article proceeds to explain how the dual transform is generalized via axioms and how the respective procedures are performed in the general setting. Several examples supplement the discussion. The paper is accessible to advanced undergraduate students with basic knowledge in linear algebra and complex analysis. The second article, “When Randomness Helps in Undersampling,” was written by Roel Snieder and Michael B. Wakin. In our digital era, we use many recordings: music, sounds of nature, and others. Many other signals such as telecommunication, temperature, and air pressure, are recorded for practical and scientific purposes. When signals are stored in computers, they are digitized by collecting and storing values of some functions at discrete times. A straightforward thought on how to accomplish that is to collect values uniformly in time and in all frequency components. However, the measurements might be feasible only for some times or frequencies; otherwise data acquisition or data transmission of a full collection might be too burdensome. When some times or frequencies are left out, it is said that the signal is undersampled. In that case, it is best to choose the times or frequencies to be left out randomly instead of uniformly. The authors focus on the problem of reconstructing a signal in the time domain using undersampled frequency components. The benefits of random undersampling are illustrated with an example of the air pressure recorded at a volcano in Costa Rica, but the authors cite other sources on seismic surveys as well as magnetic resonance imaging where benefits from undersampling are evidenced. The key to understand the phenomena is to analyze the effect of the sampling on the discrete Fourier transform, which allows a discrete-time signal of length $N$ to be represented as a sum of $N$ complex exponential terms. The authors have made the signal processing codes and the data that are used for the article available at https://mines.edu/~mwakin/software. The paper is directed toward undergraduate students in engineering with a mathematical background and interests.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/22N975585},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {1029},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proximal markov chain monte carlo method for bayesian
inference in imaging inverse problems: When langevin meets moreau.
<em>SIREV</em>, <em>64</em>(4), 991–1028. (<a
href="https://doi.org/10.1137/22M1522917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern imaging methods rely strongly on Bayesian inference techniques to solve challenging imaging problems. Currently, the predominant Bayesian computational approach is convex optimization, which scales very efficiently to high-dimensional image models and delivers accurate point estimation results. However, in order to perform more complex analyses, for example, image uncertainty quantification or model selection, it is often necessary to use more computationally intensive Bayesian computation techniques such as Markov chain Monte Carlo methods. This paper presents a new and highly efficient Markov chain Monte Carlo methodology to perform Bayesian computation for high-dimensional models that are log-concave and nonsmooth, a class of models that is central in imaging sciences. The methodology is based on a regularized unadjusted Langevin algorithm that exploits tools from convex analysis, namely, Moreau--Yosida envelopes and proximal operators, to construct Markov chains with favorable convergence properties. In addition to scaling efficiently to high dimensions, the method can be applied in a straightforward manner to models that are currently solved using proximal optimization algorithms. We provide a detailed theoretical analysis of the proposed methodology, including asymptotic and nonasymptotic convergence results with easily verifiable conditions, and explicit bounds on the convergence rates. The proposed methodology is demonstrated with five experiments related to image deconvolution and tomographic reconstruction with total-variation and $\ell_1$ priors, where we conduct a range of challenging Bayesian analyses related to uncertainty quantification, hypothesis testing, and model selection in the absence of ground truth.},
  archive      = {J_SIREV},
  author       = {Alain Durmus and Éric Moulines and Marcelo Pereyra},
  doi          = {10.1137/22M1522917},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {991-1028},
  shortjournal = {SIAM Rev.},
  title        = {A proximal markov chain monte carlo method for bayesian inference in imaging inverse problems: When langevin meets moreau},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). SIGEST. <em>SIREV</em>, <em>64</em>(4), 989. (<a
href="https://doi.org/10.1137/22N975573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is “A Proximal Markov Chain Monte Carlo Method for Bayesian Inference in Imaging Inverse Problems: When Langevin Meets Moreau,” by Alain Durmus, Éric Moulines, and Marcelo Pereyra. The authors provide new algorithms to sample from high-dimensional log-concave probability measures, where they combine Moreau--Yosida envelopes with the Euler--Maruyama discretization of Langevin diffusions. This allows for an efficient Markov chain Monte Carlo methodology that is applicable to inverse problems arising in imaging sciences. Asymptotic and nonasymptotic convergence results are provided, along with extensive computational experiments on realistic imaging problems involving deconvolution and tomographic reconstruction. The original article, which appeared in the SIAM Journal on Imaging Sciences in 2018, has attracted substantial interest. In preparing this highlighted SIGEST version, the authors have expanded the introduction to make it accessible to a wide audience. The final section also discusses follow-up work arising from the original publication.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/22N975573},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {989},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sparse approximations with interior point methods.
<em>SIREV</em>, <em>64</em>(4), 954–988. (<a
href="https://doi.org/10.1137/21M1401103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimization problems that seek sparse solutions have become ubiquitous. They are routinely solved with various specialized first-order methods. Although such methods are often fast, they usually struggle with not-so-well-conditioned problems. In this paper, specialized variants of an interior point-proximal method of multipliers are proposed and analyzed for problems of this class. Computational experience on a variety of problems, namely, multiperiod portfolio optimization, classification of data coming from functional magnetic resonance imaging, restoration of images corrupted by Poisson noise, and classification via regularized logistic regression, provides substantial evidence that interior point methods, equipped with suitable linear algebra, can offer a noticeable advantage over first-order approaches.},
  archive      = {J_SIREV},
  author       = {Valentina De Simone and Daniela di Serafino and Jacek Gondzio and Spyridon Pougkakiotis and Marco Viola},
  doi          = {10.1137/21M1401103},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {954-988},
  shortjournal = {SIAM Rev.},
  title        = {Sparse approximations with interior point methods},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupling techniques for nonlinear ensemble filtering.
<em>SIREV</em>, <em>64</em>(4), 921–953. (<a
href="https://doi.org/10.1137/20M1312204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider filtering in high-dimensional non-Gaussian state-space models with intractable transition kernels, nonlinear and possibly chaotic dynamics, and sparse observations in space and time. We propose a novel filtering methodology that harnesses transportation of measures, convex optimization, and ideas from probabilistic graphical models to yield robust ensemble approximations of the filtering distribution in high dimensions. Our approach can be understood as the natural generalization of the ensemble Kalman filter (EnKF) to nonlinear updates, using stochastic or deterministic couplings. The use of nonlinear updates can reduce the intrinsic bias of the EnKF at a marginal increase in computational cost. We avoid any form of importance sampling and introduce non-Gaussian localization approaches for dimension scalability. Our framework achieves state-of-the-art tracking performance on challenging configurations of the Lorenz-96 model in the chaotic regime.},
  archive      = {J_SIREV},
  author       = {Alessio Spantini and Ricardo Baptista and Youssef Marzouk},
  doi          = {10.1137/20M1312204},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {921-953},
  shortjournal = {SIAM Rev.},
  title        = {Coupling techniques for nonlinear ensemble filtering},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Research spotlights. <em>SIREV</em>, <em>64</em>(4), 919.
(<a href="https://doi.org/10.1137/22N975561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first Research Spotlights article in this issue is concerned with filtering, a task of paramount importance in a great many applications such as numerical weather prediction and geophysical data assimilation. Authors Alessio Spantini, Ricardo Baptista, and Youssef M. Marzouk, in their article “Coupling Techniques for Nonlinear Ensemble Filtering,” describe discrete-time filtering as the act of characterizing the sequence of conditional distributions of the latent field at observation times, given all currently available measurements. Despite the existing literature on filtering, issues such as high-dimensional state spaces and sparse (in both space and time) observations still prove formidable in practice. The traditional approach of ensemble-based data assimilation is the ensemble Kalman filter (EnKF), involving a prediction (forecasting) step followed by an analysis step. However, the authors note an intrinsic bias of EnKF due to the linearity of the transformation, estimated under Gaussian assumptions, that is used in the analysis step, which limits its accuracy. To overcome this, they propose two non-Gaussian generalizations of the EnKF---the so-called stochastic and deterministic map filters---using nonlinear transformations derived from couplings between the forecast distribution and the filtering distribution. What is crucial is that the transformations “can be estimated efficiently...perhaps using only convex optimization,” that they “are easy to `localize&#39; in high dimensions,” and that their computation “should not become increasingly challenging as the variance of the observation noise decreases.” Following a comprehensive description of their new approaches, the authors demonstrate numerically the superiority of their stochastic map filter approach over traditional EnKF. The subsequent discussion offers the reader several jumping off points for future research. Recovery of a sparse solution to a large-scale optimization problem is another ubiquitous problem arising in many applications such as image reconstruction, signal processing, and machine learning. The cost functional typically includes a regularization term in the form of an $\ell_1$ norm term on the solution and/or regularized solution to enforce sparsity. Designing suitable algorithms for such recovery problems is the subject of our second Research Spotlights article. In “Sparse Approximations with Interior Point Methods,” authors Valentina De Simone, Daniela di Serafino, Jacek Gondzio, Spyridon Pougkakiotis, and Marco Viola set out to correct the misconception that first-order methods are to be preferred over second-order methods out of hand. Through case studies, they offer evidence that interior point methods (IPMs) which are constructed to “exploit special features of the problems in the linear algebra of IPMs” and which are designed “to take advantage of the expected sparsity of the optimal solution” can in fact be the method of choice for solving this class of optimization problems. The key to their approach is a reformulation of the original sparse approximation problem to one which is seemingly larger but which has properties upon which one can capitalize for computational gain. For each of four representative applications, the authors show how to take computational advantage of the problem-specific structure of the underlying linear systems involved at each iteration. These efforts are complemented by leveraging the expected sparsity: employing heuristics to drop near zero variables, thereby replacing very large, ill-conditioned intermediate systems by better conditioned, smaller systems. Their conclusion is that time invested in tailoring solvers to structure admitted by the reformulated variant, and in taking advantage of expected sparsity, may be well spent, since their demonstrations have shown it is possible for IPMs to have a “noticeable advantage” over state-of-the-art first-order methods for sparse approximation problems.},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/22N975561},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {919},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifting for simplicity: Concise descriptions of convex sets.
<em>SIREV</em>, <em>64</em>(4), 866–918. (<a
href="https://doi.org/10.1137/20M1324417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a selected tour through the theory and applications of lifts of convex sets. A lift of a convex set is a higher-dimensional convex set that projects onto the original set. Many convex sets have lifts that are dramatically simpler to describe than the original set. Finding such simple lifts has significant algorithmic implications, particularly for optimization problems. We consider both the classical case of polyhedral lifts, described by linear inequalities, as well as that of spectrahedral lifts, defined by linear matrix inequalities, with a focus on recent developments related to spectrahedral lifts. Given a convex set, ideally we would like to either find a (low-complexity) polyhedral or spectrahedral lift or find an obstruction proving that no such lift is possible. To this end, we explain the connection between the existence of lifts of a convex set and certain structured factorizations of its associated slack operator. Based on this characterization, we describe a uniform approach, via sums of squares, to the construction of spectrahedral lifts of convex sets and illustrate the method on several families of examples. Finally, we discuss two flavors of obstruction to the existence of lifts: one related to facial structure, and the other related to algebraic properties of the set in question. Rather than being exhaustive, our aim is to illustrate the richness of the area. We touch on a range of different topics related to the existence of lifts and present many examples of lifts from different areas of mathematics and its applications.},
  archive      = {J_SIREV},
  author       = {Hamza Fawzi and Joao Gouveia and Pablo A. Parrilo and James Saunderson and Rekha R. Thomas},
  doi          = {10.1137/20M1324417},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {866-918},
  shortjournal = {SIAM Rev.},
  title        = {Lifting for simplicity: Concise descriptions of convex sets},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Love–lieb integral equations: Applications, theory,
approximations, and computations. <em>SIREV</em>, <em>64</em>(4),
831–865. (<a href="https://doi.org/10.1137/20M1371038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned mainly with the deceptively simple integral equation $$ \hskip2cm u(x) \pm \frac{1}{\pi}\int_{-1}^{1} \frac{\alpha u(y)}{\alpha^2+(x-y)^2}\, {\rm d} y =1,\qquad -1\leq x\leq 1, $$ where $\alpha$ is a real nonzero parameter and $u$ is the unknown function. This equation is classified as a Fredholm integral equation of the second kind with a continuous kernel. As such, it falls into a class of equations for which there is a well-developed theory. The theory shows that there is exactly one continuous real solution $u$. Although this solution is not known in closed form, it can be computed numerically, using a variety of methods. All this would be a mere curiosity were it not for the fact that the integral equation arises in several contexts in classical and quantum physics. We review the literature on these applications, survey the main analytical and numerical tools available, and investigate methods for constructing approximate solutions. We also consider the same integral equation when the constant on the right-hand side is replaced by a given function.},
  archive      = {J_SIREV},
  author       = {Leandro Farina and Guillaume Lang and P. A. Martin},
  doi          = {10.1137/20M1371038},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {831-865},
  shortjournal = {SIAM Rev.},
  title        = {Love--lieb integral equations: Applications, theory, approximations, and computations},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Randomized reference models for temporal networks.
<em>SIREV</em>, <em>64</em>(4), 763–830. (<a
href="https://doi.org/10.1137/19M1242252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many dynamical systems can be successfully analyzed by representing them as networks. Empirically measured networks and dynamic processes that take place in these situations show heterogeneous, non-Markovian, and intrinsically correlated topologies and dynamics. This makes their analysis particularly challenging. Randomized reference models (RRMs) have emerged as a general and versatile toolbox for studying such systems. Defined as random networks with given features constrained to match those of an input (empirical) network, they may, for example, be used to identify important features of empirical networks and their effects on dynamical processes unfolding in the network. RRMs are typically implemented as procedures that reshuffle an empirical network, making them very generally applicable. However, the effects of most shuffling procedures on network features remain poorly understood, rendering their use nontrivial and susceptible to misinterpretation. Here we propose a unified framework for classifying and understanding microcanonical RRMs (MRRMs) that sample networks with uniform probability. Focusing on temporal networks, we survey applications of MRRMs found in the literature, and we use this framework to build a taxonomy of MRRMs that proposes a canonical naming convention, classifies them, and deduces their effects on a range of important network features. We furthermore show that certain classes of MRRMs may be applied in sequential composition to generate new MRRMs from the existing ones surveyed in this article. We finally provide a tutorial showing how to apply a series of MRRMs to analyze how different network features affect a dynamic process in an empirical temporal network. Our taxonomy provides a reference for the use of MRRMs, and the theoretical foundations laid here may further serve as a base for the development of a principled and automatized way to generate and apply randomized reference models for the study of networked systems.},
  archive      = {J_SIREV},
  author       = {Laetitia Gauvin and Mathieu Génois and Márton Karsai and Mikko Kivelä and Taro Takaguchi and Eugenio Valdano and Christian L. Vestergaard},
  doi          = {10.1137/19M1242252},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {763-830},
  shortjournal = {SIAM Rev.},
  title        = {Randomized reference models for temporal networks},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Survey and review. <em>SIREV</em>, <em>64</em>(4), 761. (<a
href="https://doi.org/10.1137/22N97555X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SIAM Review contains three Survey and Review papers. When taken together, they show the enormous variety of mathematical ideas and techniques currently being used by applied mathematicians and also the wide spectrum of application fields now being tackled, ranging from very classic areas like electromagnetism to the investigation of the functioning of the brain or to the analysis of human mobility. The first paper is “Randomized Reference Models for Temporal Networks,” by Laetitia Gauvin, Mathieu Génois, Márton Karsai, Mikko Kivelä, Taro Takaguchi, Eugenio Valdano, and Christian L. Vestergaard. Random graphs are used to gain understanding of complex networks. For instance, one may estimate the size of the largest connected component of a graph with $N$ nodes and $E$ edges by generating random samples from the family of all such graphs. Techniques to generate those samples are referred to as randomized reference models. The present paper does not deal with (static) graphs, but with temporal networks, such as the network of all mobile phone users in a given city, where an edge from a node $i$ to a node $j$ appears during a time interval $(t,\tau)$ if users $i$ and $j$ hold a conversation in such interval. The paper surveys many recent developments and provides tools that systematize and unify the approaches currently being used in different application areas. The second paper is “Love--Lieb Integral Equations: Applications, Theory, Approximations, and Computations,” by Leandro Farina, Guillaume Lang, and P. A. Martin. The Love--Lieb equation $$ \hskip2cm u(x) \pm \frac{1}{\pi}\int_{-1}^{1} \frac{\alpha u(y)}{\alpha^2+(x-y)^2}\, {\rm d} y =1,\qquad -1\leq x\leq 1, $$ where $\alpha &gt;0$ is a real parameter and $u$ the unknown function, arises, e.g., in the study of circular plate capacitors and in certain quantum integrable models. The authors present the origin of the equation in a number of applications, a discussion of the existence and uniqueness of the solution, and a number of approaches to approximate $u$ numerically or analytically. Hamza Fawzi, Joao Gouveia, Pablo A. Parrilo, James Saunderson, and Rekha R. Thomas are the authors of the third paper, “Lifting for Simplicity: Concise Description of Convex Sets.” Convex sets $P$ in $\mathbb{R}^n$ appear frequently in optimization and in many other branches of applied mathematics. The paper studies how to represent a given convex set $P$ as the projection onto $\mathbb{R}^n$ of a convex subset $Q$ of a larger space $\mathbb{R}^m$, $m &gt;n$. Such a representation may be very useful in cases where the description of $Q$ is much easier than the description of $P$. For instance, in linear programming, where we seek to maximize $\langle c, x\rangle$ as $x$ ranges in $P$, if $P=\pi(Q)$ with $\pi$ a linear map, one may equivalently maximize the linear function $\langle c,\pi(y)\rangle$ as $y$ ranges in $Q$, an optimization problem over $Q$ that can potentially be much easier to solve. A simple illustration is afforded for the case where $P$ is the unit $\ell_1$-norm ball in $\mathbb{R}^n$, a polytope with $2^n$ facets that nevertheless may be written as a projection of a polytope $Q\subset \mathbb{R}^n$ with only $2n$ facets, thus removing the exponential dependence on $n$ of the number of constraints.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/22N97555X},
  journal      = {SIAM Review},
  number       = {4},
  pages        = {761},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Book reviews. <em>SIREV</em>, <em>64</em>(3), 751–759. (<a
href="https://doi.org/10.1137/22N975548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The section starts with two hot topics combined into one book: data-driven science and COVID-19. In our featured review, Anita Layton reviews the book Computational Epidemiology: Data-Driven Modeling of COVID-19, by Ellen Kuhl. She concludes her detailed review with the assessment: “This is a timely and truly wonderful book.” Dynamics continues in the second review. The book An Introduction to Nonautonomous Dynamical Systems and Their Attractors, by Peter E. Kloeden and Meihua Yang, is reviewed by Felix Ye, who recommends this book as much more accessible than others. Mathematical fundamentals in a modern guise are presented in A. J. Roberts&#39;s book Linear Algebra for the 21st Century. In his review of the book, Joakim Sundnes emphasizes the very contemporary examples it contains. In the subsequent review of the book Numerical Methods for Conservation Laws: From Analysis to Algorithms, by Jan Hesthaven, Jing-Mei Qiu describes it as a future “classic reference book on the computational fluid dynamics shelf.” Afterwards, A. H. Dooley reviews the book Ergodic Dynamics: From Basic Theory to Applications, by Jane Hawkins, and praises it as an introduction to modern ergodic theory and dynamical systems. The section is concluded by my review of Tomé Almeida Borges and Rui Neves&#39;s compact book Financial Data Resampling for Machine Learning Based Trading---Application to Cryptocurrency Markets.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/22N975548},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {751-759},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing pattern formation in the gray–scott model: An
XPPAUT tutorial. <em>SIREV</em>, <em>64</em>(3), 728–747. (<a
href="https://doi.org/10.1137/21M1402868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gray--Scott model is a widely studied autocatalytic model that exhibits a range of interesting pattern formation behavior, as well as a rich structure of dynamics that includes many ideas from a typical undergraduate dynamical systems course, and some from beyond. Gaining an understanding of the solutions to this model is arguably most easily conducted via a bifurcation analysis of corresponding ODE problems within the software XPPAUT. In this paper, we provide an introductory XPPAUT tutorial, through which we begin to expose the range of intricate patterns that the Gray--Scott model emits.},
  archive      = {J_SIREV},
  author       = {Demi L. Gandy and Martin R. Nelson},
  doi          = {10.1137/21M1402868},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {728-747},
  shortjournal = {SIAM Rev.},
  title        = {Analyzing pattern formation in the gray--scott model: An XPPAUT tutorial},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deforming <span class="math inline">||.||<sub>1</sub></span>
into <span class="math inline">||.||<sub>∞</sub></span> via polyhedral
norms: A pedestrian approach. <em>SIREV</em>, <em>64</em>(3), 713–727.
(<a href="https://doi.org/10.1137/21M1391481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider, and study with elementary calculus, the polyhedral norms $||x||_{(k)}=$ sum of the $\mathit{k}$ largest among the $|x_{i}|$&#39;s. Besides their basic properties, we provide various expressions of the unit balls associated with them and determine all the facets and vertices of these balls. We do the same with the dual norm $||.||_{(k)}^{\ast }$ of $||.||_{(k)}$. The study of these polyhedral norms is motivated, among other reasons, by the necessity of handling sparsity in some modern optimization problems, as is explained at the end of the paper.},
  archive      = {J_SIREV},
  author       = {Manlio Gaudioso and Jean-Baptiste Hiriart-Urruty},
  doi          = {10.1137/21M1391481},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {713-727},
  shortjournal = {SIAM Rev.},
  title        = {Deforming $||.||_{1}$ into $||.||_{\infty}$ via polyhedral norms: A pedestrian approach},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Education. <em>SIREV</em>, <em>64</em>(3), 711–712. (<a
href="https://doi.org/10.1137/22N975536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SIAM Review presents two papers in the Education section. The first paper, “Deforming $\|\cdot\|_1$ into $\|\cdot\|_\infty$ via Polyhedral Norms: A Pedestrian Approach,” is contributed by Manlio Gaudioso and Jean-Baptiste Hiriart-Urruty. The paper starts from the well-known properties of the $\ell_p$-norms in the Euclidean space $\mathbb{R}^n$. For $x\in\mathbb{R}^n$ and $p\geq 1$, the norm is given by $\|x\|_p=\big(\sum_{i=1}^n |x_i|^p\big)^\frac{1}{p}$ with the limiting case $p\to\infty$ providing the norm $\|x\|_\infty=\max_{1\leq i\leq n} |x_i|.$ The unit balls determined by the norms for $p=1$ and $p=\infty$ are polyhedral, while for $p\in (1,\infty)$ the unit ball has a smooth surface. Furthermore, the unit balls become larger when $p$ grows. One can represent the polyhedral norms (for $p=1$ or $p=\infty$) as maxima of linear forms, and this observation motivates the following notion. For an integer $k$ with value between 1 and $n$, we define the following real-valued function: \[ N_k(x)=\max\|x_i_1|+|x_i_2|+\dots+|x_i_k|: 1łeq i_1 &lt;\dots &lt;i_k łeq n\. \] It is not difficult to check that $N_k$ is a norm, $N_1$ is the infinity norm, $N_n$ is the $\ell_1$-norm, and $N_k$ provides a value between them. In this way, the $\ell_1$ ball is deformed into the $\ell_\infty$ ball using only polyhedral convex sets. The authors discuss the properties and some representations of these norms. They provide the distance between the unit balls when $k$ changes, and discuss the relations of $N_k$ to support functions and gauges. Special cases for $n=3$ or 4 are examined and illustrated. Conjugate duality is invoked to present the dual norm to $N_k$ and the respective dual ball. The authors point out the relevance of these norms to some modern optimization problems, in which a sparse solution is desired. The polyhedral representation of the norms also facilitates numerical methods. The paper is written in a clear and engaging manner and is accessible to advanced undergraduate students who are familiar with basic notions of convex analysis. The paper “Analyzing Pattern Formation in the Gray--Scott Model: An XPPAUT Tutorial” is presented by Demi L. Gandy and Martin R. Nelson. It discusses a mathematical model of autocatalytic chemical processes that may lead to spontaneous formation of spatial and spatio-temporal patterns. The authors focus on a reaction-diffusion model, introduced by Gray and Scott, in which a pair of two chemicals, one reacting and one diffusing, can give rise to formation of spots, stripes, spirals, and other patterns. The model consists of two partial differential equations (PDEs) describing the process, in which two generic chemicals U and V react to produce a product P. It assumes that U is continuously supplied and the inert product P is continuously removed. The model uses the concentrations of U and V, the rate of replenishment of U, and the rate at which V decays to produce P. It also uses two diffusion constants, representing the rate at which each chemical moves spatially. The paper explains the main steps of the analysis of this model for the purpose of identifying pattern formation and illustrates this using an example. The first step is to understand the dynamics of the corresponding homogeneous system, which is obtained by neglecting the diffusion terms. Once the spatial dependence is removed the PDE system reduces to a pair of ordinary differential equations. The steady states are identified and their stability is characterized. The key step is to understand how changes of the parameters impact the steady states. At this point, software named XPPAUT may be used to compute bifurcation diagrams in order to gain more detailed information about periodic structures. Further, the spatial terms are reintroduced and one has to examine how these terms give rise to spatially inhomogeneous solutions. The authors describe the capabilities of XPPAUT and give instructions for its use. They also use MATLAB to visualize some patterns and make all code used in the paper freely available online at https://github.com/martinrnelson/GrayScott. The targeted audience is students who have completed an introductory course on dynamical systems and bifurcation theory, have some familiarity with PDEs such as the heat equation, and have knowledge of basic numerical method techniques.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/22N975536},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {711-712},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimizing the repayment cost of federal student loans.
<em>SIREV</em>, <em>64</em>(3), 689–709. (<a
href="https://doi.org/10.1137/22M1505840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federal student loans are fixed-rate debt contracts with three main special features: (i) borrowers can use income-driven schemes to make payments proportional to their income above subsistence; (ii) after several years of being in good standing, the remaining balance is forgiven but taxed as ordinary income; and (iii) accrued interest is simple, i.e., not capitalized. For a very small loan, the cost-minimizing repayment strategy dictates maximum payments until full repayment, forgoing both income-driven schemes and forgiveness. For a very large loan, the minimal payments allowed by income-driven schemes are optimal. For intermediate balances, the optimal repayment strategy may entail an initial period of minimum payments to exploit the noncapitalization of accrued interest, but when the principal is being reimbursed maximal payments always precede minimum payments. Income-driven schemes and simple accrued interest mostly benefit borrowers with very large balances.},
  archive      = {J_SIREV},
  author       = {Paolo Guasoni and Yu-Jui Huang},
  doi          = {10.1137/22M1505840},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {689-709},
  shortjournal = {SIAM Rev.},
  title        = {Minimizing the repayment cost of federal student loans},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). SIGEST. <em>SIREV</em>, <em>64</em>(3), 687. (<a
href="https://doi.org/10.1137/21N975527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this section we present “Minimizing the Repayment Cost of Federal Student Loans,” by Paolo Guasoni and Yu-Jui Huang. This is the highlighted SIGEST version of an article that first appeared in the SIAM Journal on Financial Mathematics (SIFIN) in 2021. The article studies optimal strategies for repayment of federal student loans in the USA. These loans, which can be used to cover tuition and living expenses, have specific features that make them tricky to evaluate. Sections 2 and 4, as in the original SIFIN article, take account of two key student loan features: capping repayment levels at a proportion of current income, and forgiving the debt after a sufficiently long period of good standing. Sections 3 and 5, which are new to the revised SIGEST version, give results that incorporate a third feature: the effect of simple interest. The modeling and analysis presented here shed light on an important, mathematically challenging financial product and give practical insights into a complicated and potentially daunting set of choices available to those who must negotiate their way through these contracts.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/21N975527},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {687},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hypergraph cuts with general splitting functions.
<em>SIREV</em>, <em>64</em>(3), 650–685. (<a
href="https://doi.org/10.1137/20M1321048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum $s$-$t$ cut problem in graphs is one of the most fundamental problems in combinatorial optimization, and graph cuts underlie algorithms throughout discrete mathematics, theoretical computer science, operations research, and data science. While graphs are a standard model for pairwise relationships, hypergraphs provide the flexibility to model multiway relationships and are now a standard model for complex data and systems. However, when generalizing from graphs to hypergraphs, the notion of a “cut hyperedge” is less clear, as a hyperedge&#39;s nodes can be split in several ways. Here, we develop a framework for hypergraph cuts by considering the problem of separating two terminal nodes in a hypergraph in a way that minimizes a sum of penalties at split hyperedges. In our setup, different ways of splitting the same hyperedge have different penalties, and the penalty is encoded by what we call a splitting function. Our framework opens a rich space on the foundations of hypergraph cuts. We first identify a natural class of cardinality-based hyperedge splitting functions that depend only on the number of nodes on each side of the split. In this case, we show that the general hypergraph $s$-$t$ cut problem can be reduced to a tractable graph $s$-$t$ cut problem if and only if the splitting functions are submodular. We also identify a wide regime of non-submodular splitting functions for which the problem is NP-hard. Finally, we outline several open questions on general hypergraph cut problems.},
  archive      = {J_SIREV},
  author       = {Nate Veldt and Austin R. Benson and Jon Kleinberg},
  doi          = {10.1137/20M1321048},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {650-685},
  shortjournal = {SIAM Rev.},
  title        = {Hypergraph cuts with general splitting functions},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Some comments on preconditioning for normal equations and
least squares. <em>SIREV</em>, <em>64</em>(3), 640–649. (<a
href="https://doi.org/10.1137/20M1387948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution of systems of linear(ized) equations lies at the heart of many problems in scientific computing. In particular, for large systems, iterative methods are a primary approach. For many symmetric (or self-adjoint) systems, there are effective solution methods based on the conjugate gradient method (for definite problems) or MINRES (for indefinite problems) in combination with an appropriate preconditioner, which is required in almost all cases. For nonsymmetric systems there are two principal lines of attack: the use of a nonsymmetric iterative method such as GMRES or transformation into a symmetric problem via the normal equations and application of LSQR. In either case, an appropriate preconditioner is generally required. We consider the possibilities here, particularly the idea of preconditioning the normal equations via approximations to the original nonsymmetric matrix. We highlight dangers that readily arise in this approach. Our comments also apply in the context of linear least squares problems.},
  archive      = {J_SIREV},
  author       = {Andy Wathen},
  doi          = {10.1137/20M1387948},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {640-649},
  shortjournal = {SIAM Rev.},
  title        = {Some comments on preconditioning for normal equations and least squares},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mathematical model for the origin of name brands and
generics. <em>SIREV</em>, <em>64</em>(3), 625–639. (<a
href="https://doi.org/10.1137/20M1360888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms in the U.S. spend over $200 billion each year advertising their products to consumers, around one percent of the country&#39;s gross domestic product. It is of great interest to understand how that aggregate expenditure affects prices, market efficiency, and overall welfare. Here, we present a mathematical model for the dynamics of competition through advertising and find a surprising prediction: when advertising is relatively cheap compared to the maximum benefit advertising offers, rational firms split into two groups, one with significantly less advertising (a “generic” group) and one with significantly more advertising (a “name-brand” group). Our model predicts that this segmentation will also be reflected in price distributions; we use large consumer data sets to test this prediction and find good qualitative agreement.},
  archive      = {J_SIREV},
  author       = {Joseph D. Johnson and Adam M. Redlich and Daniel M. Abrams},
  doi          = {10.1137/20M1360888},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {625-639},
  shortjournal = {SIAM Rev.},
  title        = {A mathematical model for the origin of name brands and generics},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Research spotlights. <em>SIREV</em>, <em>64</em>(3),
623–624. (<a href="https://doi.org/10.1137/22N975512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What&#39;s in a name [brand]? In the first of three Research Spotlights articles this issue, authors Joseph D. Johnson, Adam M. Redlich, and Daniel M. Abrams address this issue by developing a mathematical model for the dynamics of competition through advertising. Specifically, in “A Mathematical Model for the Origin of Name Brands and Generics,&quot; readers are presented with a mathematical model for the dynamics of competition through advertising. Here, the terms “generic” and “name brand” refer to low and high advertising investment states, respectively. A distinguishing feature of the approach taken by the authors in this work is that they model “monopolistic competition,&quot; meaning that although there may be many suppliers of a product/service, these are distinguished only by brand and/or quality. The analysis of the existence and stability of equilibria of their ordinary differential equation system model predicts that “when advertising is relatively cheap compared to the benefit of advertising,&quot; these two advertising investment states will arise. This segmentation “contrasts starkly with (often implicit) assumptions of smooth, singly peaked functions for economic metrics.&quot; The authors note that their model predicts that segmentation should be reflected in price distributions. Indeed, although there are limitations in their model which readers are invited to consider addressing in future work, they show good qualitative agreement to this prediction on a large consumer data set. The need to solve linear least squares problems is ubiquitous in science and engineering applications, and it is at the heart of our second article, “Some Comments on Preconditioning for Normal Equations and Least Squares,&quot; authored by Andy Wathen. Iterative solvers are often preferred over direct methods for large-scale least squares as they require only the ability to perform matrix-vector products with the system matrix and its (conjugate) transpose. A preconditioner, which is an approximation to the original matrix with desirable properties (e.g., easy to apply, easy to “invert&#39;&#39;), is usually employed to speed convergence. To be effective, the spectrum of the preconditioned normal equations operator should be appropriately clustered. Using several concrete examples as motivation, the author explores a subtle and underappreciated difficulty in designing preconditioners for least squares problems called the “matrix squaring problem.&quot; Simply put, a good approximation, $P$, to the original matrix, $B$, may not translate into having an effective preconditioner, $P^T P$, for the normal equations matrix $B^T B$. The article includes theory and discussion about when the matrix squaring problem can be expected versus when it is a nonissue in the case of invertible matrices. The author&#39;s final example shows that the matrix squaring problem can occur even in the full rank rectangular case, which should serve as a warning to practitioners that, indeed, it may not be sufficient to seek approximations to the original operator in the design of an effective preconditioner for iterative solution to the least squares problem. The final article, “Hypergraph Cuts with General Splitting Functions,&quot; by Nate Veldt, Austin R. Benson, and Jon Kleinberg, tackles a problem that is central to the study of hypergraphs. While readers may be familiar with a standard graph representation in which an edge connects exactly two vertices, in a hypergraph, an edge (a.k.a. hyperedge) refers to a grouping of (possibly) more than two vertices. This extra dimensionality associated to an edge complicates the generalization of graph splitting to hypergraphs, a fact that can be appreciated by studying the graphical illustrations provided in the article. Nevertheless, the authors provide a framework for hypergraph cuts that leads to a rich set of results and illuminates new research questions. Specifically, their framework utilizes so-called “splitting functions&quot; which assign a penalty to each rearrangement of a hyperedge&#39;s nodes. These functions enable them to characterize the minimum $s$-$t$ cut problem of finding a minimum weight set of hyperedges to cut in order to separate (e.g., as might be required in data clustering applications) nodes $s$ and $t$ from each other. The paper contains many new contributions, among them algorithms for some variants of the hypergraph $s$-$t$ cut problem that are polynomial time and theoretical results on the NP-hardness of other variants. As this article “includes broad contributions at the intersection of graph theory, optimization, scientific computing, and other subdisciplines in applied mathematics,&quot; and offers several suggestions for follow-up research questions, it is likely to appeal to many SIREV readers.},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/22N975512},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {623-624},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable load balancing in networked systems: A survey of
recent advances. <em>SIREV</em>, <em>64</em>(3), 554–622. (<a
href="https://doi.org/10.1137/20M1323746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this survey we provide an overview of recent advances on scalable load balancing schemes which provide favorable delay performance and yet require minimal implementation overhead. The basic load balancing scenario involves a single dispatcher where tasks arrive that must immediately be forwarded to one of $N$ single-server queues. The join-the-shortest-queue (JSQ) policy yields vanishing delays as $N$ grows large, as in a centralized queuing arrangement, but involves a prohibitive communication burden. In contrast, JSQ($d$) schemes that assign an incoming task to a server with the shortest queue among $d$ servers selected uniformly at random require little communication, but lead to constant delays. In order to examine this fundamental trade-off between delay performance and implementation overhead, we discuss a body of recent research on JSQ($d(N)$) schemes in which the diversity parameter $d(N)$ depends on $N$ and investigate the growth rate of $d(N)$ required to match the optimal JSQ performance on fluid and diffusion scales. Stochastic coupling techniques and scaling limits play an instrumental role in establishing this asymptotic optimality. We demonstrate how this methodology carries over to infinite-server settings, finite buffers, multiple dispatchers, servers arranged on graph topologies, and token-based load balancing schemes such as join-the-idle-queue (JIQ), thus providing a broad overview of the main trends in the field.},
  archive      = {J_SIREV},
  author       = {Mark Van der Boor and Sem C. Borst and Johan S. H. Van Leeuwaarden and Debankur Mukherjee},
  doi          = {10.1137/20M1323746},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {554-622},
  shortjournal = {SIAM Rev.},
  title        = {Scalable load balancing in networked systems: A survey of recent advances},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delayed gradient methods for symmetric and positive definite
linear systems. <em>SIREV</em>, <em>64</em>(3), 517–553. (<a
href="https://doi.org/10.1137/20M1321140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary aim of this paper is to provide a review of the last few decades of research focused on delayed gradient methods for solving symmetric positive definite linear systems. Recent developments on spectral properties are stressed, as well as their implications for the formulation of new methods. Numerical experiments are conducted with emphasis on the comparison with the conjugate gradient method for solving ill-conditioned problems.},
  archive      = {J_SIREV},
  author       = {Qinmeng Zou and Frédéric Magoulès},
  doi          = {10.1137/20M1321140},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {517-553},
  shortjournal = {SIAM Rev.},
  title        = {Delayed gradient methods for symmetric and positive definite linear systems},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Survey and review. <em>SIREV</em>, <em>64</em>(3), 515. (<a
href="https://doi.org/10.1137/22N975500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue contains two Survey and Review papers. The first, by Qinmeng Zou and Frédéric Magoulès, is “Delayed Gradient Methods for Symmetric and Positive Definite Linear Systems.” Gradient methods are the oldest and simplest algorithms to minimize a real objective function (f(x)). The ((n+1))st approximation to the minimizer is defined as (x_n+1 = x_n -\alpha_n g_n), where \(g_n\) is the gradient (\nabla f(x_n)) and (\alpha_n&gt;0) is a steplength that depends on the specific method being used. Old and simple as the idea may be, gradient algorithms are of much current interest in the literature; for instance, they played a major role in the influential survey devoted to optimization in machine learning published in Volume 60, Issue 2 of this journal. The paper by Zou and Magoulès focuses on the quadratic case $f(x) = (1/2)x^TAx-b^Tx$ ($A$ symmetric and positive definite), where finding the minimizer is of course equivalent to solving the linear system $Ax=b$ and the gradient $g_n$ coincides with the residual $Ax_n-b$. Well-known strategies to determine the steplength include steepest descent, where $\alpha_n$ is chosen so as to minimize $f(x_{n+1})$, and minimal gradient (or minimal residual), where one rather minimizes the length of $g_{n+1}$. In both of these strategies, the value of $\alpha_n$ depends only on $g_n$. The term “delayed” in the title of the article refers to methods where the recipe to determine $\alpha_n$ includes information from past gradients $g_{n-1}, g_{n-2}$, \dots, and/or past stepsizes $\alpha_{n-1}$, $\alpha_{n-2}$, \dots. The numerical experiments reported clearly indicate that such delayed strategies may give rise to algorithms that are competitive with conjugate gradient methods in large ill-conditioned problems. The paper presents a neat summary of the recent results in this area and of the techniques used to derive them. Mark Van der Boor, Sem C. Borst, Johan S. H. Van Leeuwaarden, and Debankur Mukherjee are the authors of the second paper, “Scalable Load Balancing in Networked Systems: A Survey of Recent Advances.” The problem under consideration is as follows. A dispatcher receives clients that arrive randomly, and her job is to direct them to one of $N\gg 1$ servers. The time required by each client to be served is also random, so that a queue (of random length) of waiting clients will be formed at each server. How should the dispatcher proceed to expedite the service? As one would expect in these days of cloud networks and data systems with massive number of individual centers, the problem is currently receiving much attention in the literature. A strategy that suggests itself is the so-called “join the shortest queue” (JSQ), where on their arrival clients are directed to the server having the shortest queue. While JSQ has been proved to possess several favorable properties, it may not be the best option, due to communication overheads: each time a client arrives, the dispatcher has to communicate with all servers to find the lengths of their queues. The paper analyzes, in the limit $N\rightarrow \infty$, many alternative strategies. Nonspecialists will have little difficulty in reading the easily accessible first few sections and may be interested in discovering how even small tweaks in the algorithms may result in substantial improvements of their performance.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/22N975500},
  journal      = {SIAM Review},
  number       = {3},
  pages        = {515},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Book reviews. <em>SIREV</em>, <em>64</em>(2), 503–513. (<a
href="https://doi.org/10.1137/22N975469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We start this section with Andreas Mang&#39;s in-depth featured review on the book Introduction to the Tools of Scientific Computing, written by Einar Smith. Seemingly the level of the book is somewhat more fundamental as it investigates the tools of the trade rather than its theoretical sophistication. We continue with aspects of scientific computing in Ekkehard Sachs&#39;s review of the book An Introduction to the Finite Element Method for Differential Equations, written by Mohammad Asadzadeh. Another important aspect of scientific computing is, of course, dimensional analysis, which is the topic of the third reviewed book. Richard Brown, in his review of Fundamentals of Dimensional Analysis: Theory and Applications in Metallurgy, by Alberto N. Conejo, recommends the book in particular for engineers in metallurgy. Following these are reviews of four more books loosely related to data science. Lectures on Optimal Transport, by Luigi Ambrosio, Elia Brué, and Daniele Semola, is reviewed by Matthew Thorpe, who recommends it as an introduction to the field with an emphasis on theory. Arvind K. Saibara reviews Nonnegative Matrix Factorization, written by Nicolas Gillis, and recommends it as a book which “manages to find a good balance of theory, algorithms, and applications.” The subsequent review on Hiroyuki Sato&#39;s Riemannian Optimization and Its Applications is written by the Ph.D. student Lena Sembach, who underlines the broad and timely overview on the field provided by the book. The section is concluded by a review of Daniela Calvetti and Erkki Somersalo&#39;s book Mathematics of Data Science---A Computational Approach to Clustering and Classification, which is highly recommended for inclusion in student projects.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/22N975469},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {503-513},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generating resonant and repeated root solutions to ordinary
differential equations using perturbation methods. <em>SIREV</em>,
<em>64</em>(2), 485–499. (<a
href="https://doi.org/10.1137/21M1395922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of ordinary differential equations (ODEs) of the form $\hat{L}[y(x)]=f(x)$, where $\hat{L}$ is a linear differential operator, two related phenomena can arise: resonance, where $f(x)\propto u(x)$ and $\hat{L}[u(x)]=0$, and repeated roots, where $f(x)=0$ and $\hat{L}=\hat{D}^n$ for $n\geq 2$. We illustrate a method to generate exact solutions to these problems by taking a known homogeneous solution $u(x)$, introducing a parameter $\epsilon$ such that $u(x)\rightarrow u(x;\epsilon)$, and Taylor expanding $u(x;\epsilon)$ about $\epsilon = 0$. The coefficients of this expansion $\frac{\partial^k u}{\partial\epsilon^k}\big{|}_{\epsilon=0}$ yield the desired resonant or repeated root solutions to the ODE. This approach, whenever it can be applied, is more insightful and less tedious than standard methods such as reduction of order or variation of parameters. We provide examples of many common ODEs, including constant coefficient, equidimensional, Airy, Bessel, Legendre, and Hermite equations. While the ideas can be introduced at the undergraduate level, we could not find any existing elementary or advanced text that illustrates these ideas with appropriate generality.},
  archive      = {J_SIREV},
  author       = {Bernardo Gouveia and Howard A. Stone},
  doi          = {10.1137/21M1395922},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {485-499},
  shortjournal = {SIAM Rev.},
  title        = {Generating resonant and repeated root solutions to ordinary differential equations using perturbation methods},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computed origami tomography. <em>SIREV</em>, <em>64</em>(2),
469–484. (<a href="https://doi.org/10.1137/20M1378247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide assembly instructions for an easy-to-build experimental setup in order to gain practical experience with tomography. As such, this paper can be seen as a complementary work to excellent undergraduate-level mathematical textbooks concerned with the basic mathematical principles of tomography. Since the setup uses light for tomographic imaging, the objects investigated need to be light transparent, such as origami figures. Should the reader want to experiment with computational tomographic reconstructions without assembling the device, we provide a database of several objects together with their tomographic measurements and publicly available software. Moreover, recent advances in cryo-imaging have enabled three-dimensional high-resolution visualization of single particles such as, for instance, viruses. To exemplify, and demonstrate, single particle cryo-electron microscopy, we provide an advanced assembly that we use to generate data simulating a cryo-recording. We also discuss some of the major practical difficulties in reconstructing particles from cryo-microscopic data.},
  archive      = {J_SIREV},
  author       = {Axel Kittenberger and Leonidas Mindrinos and Otmar Scherzer},
  doi          = {10.1137/20M1378247},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {469-484},
  shortjournal = {SIAM Rev.},
  title        = {Computed origami tomography},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Education. <em>SIREV</em>, <em>64</em>(2), 467. (<a
href="https://doi.org/10.1137/22N975457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue contains two papers in the Education section. The first paper, “Computed Origami Tomography,” is presented by Axel Kittenberger, Leonidas Mindrinos, and Otmar Scherzer. The paper is motivated by advances in scanner technology and the increased sophistication of the accompanying 3D image reconstruction methods. The authors have implemented the idea of constructing a system that eliminates dangerous X-ray radiation but preserves the main features of CT scanners. Such a system is safe to use in training. The proposal is to replace the X-rays with optical illumination and detection of thin objects that are translucent in the visible spectrum. The origami figures, which are made out of thin sheets of cellulose acetate, become an attractive surrogate model for the objects in real CT scans. The first part of the paper contains an explanation of the physics assumptions and laws on which the X-ray tomography is based. The theory of light-ray propagation is similar to the theory of X-rays and provides a geometric optics approximation. The second part of the paper contains instructions for building a crafting device that can record digital images of small 3D objects at different rotation directions. Open source software for reconstructing the 3D object from the collected images is provided. An introduction to cryo-imaging and cryo-EM data provides additional insight into new directions in computerized tomography (``cryo” means “cold” in Greek). In this context, “cryo” refers to the process of immobilizing a sample specimen (a virus or a molecular cluster in a cell) by freezing it. The authors maintain that the described setup allows for conducting tomographic imaging experiments even at home. They have offered a training course along the lines of this paper multiple times and have produced a YouTube video explaining the basic ideas (the link to the video is included in the reference list). For those who wish to experiment with computational tomographic reconstructions without assembling the device, the authors have a database of several objects together with their tomographic measurements. The paper is written in an engaging style, focusing on the main ideas and pointing to further references. The second paper, authored by Bernardo Gouveia and Howard A. Stone, is “Generating Resonant and Repeated Root Solutions to Ordinary Differential Equations Using Perturbation Methods.” The paper highlights a general, very flexible, yet relatively simple method to construct a resonant or repeated root solution to ordinary differential equations (ODEs). The idea of the method is to introduce a small parameter $\epsilon$ in the forcing function and then to construct a suitable Taylor expansion of a known homogeneous solution in that parameter. Next to the natural desire to analyze any situation mathematically, unwanted oscillations are highly relevant in civil engineering and other areas. Therefore, software systems provide solutions addressing resonance, but the results might be cumbersome or not insightful at times. The paper starts with background material on methods to obtain resonant solutions or repeated root solutions to ODEs. Then the authors present seven examples of increasing complexity, on which the ideas are developed and the advocated approach is explained. In the context of some of those examples, the advantages of the method are illustrated by comparison of its solutions to the solutions obtained by Mathematica. The concluding section of the paper contains a more general derivation, which encompasses all of the examples discussed before. The authors assert that the method is beneficial to students as well as to practitioners who come across this type of problem in their work. The paper is written in a clear and deductive manner.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/22N975457},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {467},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hamiltonicity of cubic planar graphs with bounded face
sizes. <em>SIREV</em>, <em>64</em>(2), 425–465. (<a
href="https://doi.org/10.1137/22M1476915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fullerene graphs, i.e., 3-connected planar cubic graphs with pentagonal and hexagonal faces, are conjectured to be Hamiltonian. This is a special case of a conjecture of Barnette and Goodey, stating that 3-connected planar cubic graphs with faces of size at most 6 are Hamiltonian. We prove Barnette and Goodey&#39;s conjecture.},
  archive      = {J_SIREV},
  author       = {František Kardoš},
  doi          = {10.1137/22M1476915},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {425-465},
  shortjournal = {SIAM Rev.},
  title        = {Hamiltonicity of cubic planar graphs with bounded face sizes},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). SIGEST. <em>SIREV</em>, <em>64</em>(2), 423. (<a
href="https://doi.org/10.1137/21N975448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIGEST article in this issue is Hamiltonicity of Cubic Planar Graphs with Bounded Face Sizes, by František Kardoš. The original version of this article appeared in the SIAM Journal on Discrete Mathematics in 2020. Barnette&#39;s Conjecture is a famous problem in graph theory which asserts that every cubic bipartite 3-connected planar graph has a Hamiltonian cycle. The conjecture stems from an attempt in 1880 by P. G. Tait to solve the Four Color Theorem by establishing that every cubic 3-connected planar graph has a Hamiltonian cycle. This is a stronger statement than the Four Color Theorem and it actually turns out to be false, as shown by W. T. Tutte in 1946. Barnette&#39;s Conjecture is equivalent to the statement that every plane cubic 3-connected graph with all faces of even size has a Hamiltonian cycle. Our SIGEST paper proves a closely related conjecture of Barnette and Goodey asserting that every plane cubic 3-connected graph with all faces of size at most six has a Hamiltonian cycle; this also solves an open problem from mathematical chemistry, which posited the existence of a Hamiltonian cycle under the stronger assumption that all faces have size five or six. The proof consists of several reduction steps and eventually boils the problem down to checking a finite number of configurations, which is handled with the assistance of a computer. The derivations are accompanied by high-quality graphical illustrations, and the author has provided computer code for the automated parts of the proof.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/21N975448},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {423},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bilinear optimal control of an advection-reaction-diffusion
system. <em>SIREV</em>, <em>64</em>(2), 392–421. (<a
href="https://doi.org/10.1137/21M1389778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the bilinear optimal control of an advection-reaction-diffusion system, where the control arises as the velocity field in the advection term. Such a problem is generally challenging from both theoretical analysis and algorithmic design perspectives, mainly because the state variable depends nonlinearly on the control variable and, an additional divergence-free constraint on the control is coupled together with the state equation. Mathematically, the proof of the existence of optimal solutions is delicate, and, up to now, only some results have been known for a few special cases where additional restrictions are imposed on the space dimension and the regularity of the control. We prove the existence of optimal controls and derive the first-order optimality conditions in general settings without any extra assumptions. Computationally, the well-known conjugate gradient (CG) method can be applied conceptually. However, due to the additional divergence-free constraint on the control variable and the nonlinear relation between the state and control variables, it is challenging to compute the gradient and the optimal stepsize at each CG iteration, and thus nontrivial to implement the CG method. To address these issues, we advocate a fast inner preconditioned CG method to ensure the divergence-free constraint and an efficient inexactness strategy to determine an appropriate stepsize. An easily implementable nested CG method is thus proposed for solving such a complicated problem. For the numerical discretization, we combine finite difference methods for the time discretization and finite element methods for the space discretization. Efficiency of the proposed nested CG method is promisingly validated by the results of some preliminary numerical experiments.},
  archive      = {J_SIREV},
  author       = {Roland Glowinski and Yongcun Song and Xiaoming Yuan and Hangrui Yue},
  doi          = {10.1137/21M1389778},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {392-421},
  shortjournal = {SIAM Rev.},
  title        = {Bilinear optimal control of an advection-reaction-diffusion system},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The network HHD: Quantifying cyclic competition in
trait-performance models of tournaments. <em>SIREV</em>, <em>64</em>(2),
360–391. (<a href="https://doi.org/10.1137/20M1321012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive tournaments appear in sports, politics, population ecology, and animal behavior. All of these fields have developed methods for rating competitors and ranking them accordingly. A tournament is intransitive if it is not consistent with any ranking. Intransitive tournaments contain rock-paper-scissors type cycles. The discrete Helmholtz--Hodge decomposition (HHD) is well adapted to describing intransitive tournaments. It separates a tournament into perfectly transitive and perfectly cyclic components, where the perfectly transitive component is associated with a set of ratings. The size of the cyclic component can be used as a measure of intransitivity. Here we show that the HHD arises naturally from two classes of tournaments with simple statistical interpretations. We then discuss six different sets of assumptions that define equivalent decompositions. This analysis motivates the choice to use the HHD among other existing methods. Success in competition is often mediated by the traits of the competitors. A trait-performance model assumes that the probability that one competitor beats another is a function of their traits. We show that if the traits of each competitor are drawn independently and identically from a trait distribution, then the expected degree of intransitivity in the network can be computed explicitly. We show that increasing the number of pairs of competitors who could compete promotes cyclic competition, and that correlation in the performance of $A$ against $B$ with the performance of $A$ against $C$ promotes transitive competition. The expected size of cyclic competition can thus be understood by analyzing this correlation.},
  archive      = {J_SIREV},
  author       = {Alexander Strang and Karen C. Abbott and Peter J. Thomas},
  doi          = {10.1137/20M1321012},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {360-391},
  shortjournal = {SIAM Rev.},
  title        = {The network HHD: Quantifying cyclic competition in trait-performance models of tournaments},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variance and covariance of distributions on graphs.
<em>SIREV</em>, <em>64</em>(2), 343–359. (<a
href="https://doi.org/10.1137/20M1361328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a theory to measure the variance and covariance of probability distributions defined on the nodes of a graph, which takes into account the distance between nodes. Our approach generalizes the usual (co)variance to the setting of weighted graphs and retains many of its intuitive and desired properties. Interestingly, we find that a number of famous concepts in graph theory and network science can be reinterpreted in this setting as variances and covariances of particular distributions. As a particular application, we define the maximum variance problem on graphs with respect to the effective resistance distance, and we characterize the solutions to this problem both numerically and theoretically. We show how the maximum variance distribution is concentrated on the boundary of the graph, and illustrate this in the case of random geometric graphs. Our theoretical results are supported by a number of experiments on a network of mathematical concepts, where we use the variance and covariance as analytical tools to study the (co)occurrence of concepts in scientific papers with respect to the (network) relations between these concepts.},
  archive      = {J_SIREV},
  author       = {Karel Devriendt and Samuel Martin-Gutierrez and Renaud Lambiotte},
  doi          = {10.1137/20M1361328},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {343-359},
  shortjournal = {SIAM Rev.},
  title        = {Variance and covariance of distributions on graphs},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Research spotlights. <em>SIREV</em>, <em>64</em>(2),
341–342. (<a href="https://doi.org/10.1137/22N975433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue features three Research Spotlight articles. The first of these is entitled “Variance and Covariance of Distributions on Graphs&quot; and is coauthored by Karel Devriendt, Samuel Martin-Gutierrez, and Renaud Lambiotte. Given a distribution on the graph (that is, a function $p$ from the set of nodes to $[0,1]$ such that the sum of all values is one) the key ingredient in the authors&#39; definition of the variance and covariance is a notion of distance between the nodes of a graph. Although their definitions of (co)variance are valid for different choices of distance, the authors focus on a metric called the effective resistance. The effective resistance resembles geodesic distance in that it reflects the length of the paths between a pair of nodes but differs in that it takes into account all paths, and their lengths, between a node pair. Conveniently, the resulting variance and covariance values can be calculated via evaluation of a quadratic product and matrix trace. In support of the newly introduced measures, the balance of the paper is devoted to the application of the new definitions in practice and to the explanation of the conceptual correspondence between their (co)variance measures and some known graph characteristics. Citing the benefits of the framework induced by their (co)variance measures, the authors leave the reader with suggestions for future application in fields such as neuroscience, economics, and social networks. Competitive tournaments are an integral part of daily life, from schoolyard games to professional sports, to politics and biology. Authors Alexander Strang, Karen C. Abbott, and Peter J. Thomas tackle the difficult problem of quantifying competition in tournament modeling. Their article, “The Network HHD: Quantifying Cyclic Competition in Trait-Performance Models of Tournaments,” outlines how to adapt the discrete Helmholtz--Hodge decomposition (HHD), first introduced in the literature as a method for ranking objects from incomplete and imbalanced data, to study competitive tournaments characterized by intransitivity, the presence of which provides a challenge in ranking. Specifically, an intransitive tournament is one in which there is no clear global ranking of all competitors, and it corresponds to a cycle in the model. The authors show that the HHD arises in the context of representing a generic tournament as a weighted sum of so-called perfectly transitive and perfectly cyclic components. Since a trait-performance model assumes that “the probability that one competitor beats another is a function of their traits” the goal becomes that of identifying which traits and performance functions influence the weights in the HHD. This is addressed with the theoretical results in section 4. Schematics and graphs complement the discussion. Using the code made available by the authors, the interested reader may wish to try an analysis of a tournament for themselves. The third article, “Bilinear Optimal Control of an Advection-Reaction-Diffusion System,&quot; authored by Roland Glowinski, Yongcun Song, Xiaoming Yuan, and Hangrui Yue, offers both theoretical and computational insights to solving the mathematical problem specified in the title. Readers may appreciate the space devoted to the motivation and introduction to the problem as well as to the description of the associated difficulties both in deriving existence results and in numerical computation of solutions. Following a few preliminaries, the authors derive a proof of existence of optimal controls for the problem, labeled in the introduction as BCP, without the special case assumptions used elsewhere in the literature. The balance of the article is devoted to issues associated with the numerical implementation. Although their proposed nested conjugate gradient method is straightforward to present on paper, several obstacles arise in the implementation, and the authors tackle each of these in turn. For computational efficiency, for example, an inexact line search is proposed to replace the computationally intractable exact line search; preconditioned CG is employed for two internal subproblems that arise. The choice of the time and space discretizations is explained as well. There are many technical details to consume, but a key point for the readers is well summarized by the authors just before the detailed experiments, namely, “despite its apparent complexity, the nested CG method” given here “is easy to implement&quot; with aspects that are amenable to parallelizability. Misha E. Kilmer Section Editor Misha.Kilmer@tufts.edu},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/22N975433},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {341-342},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modern koopman theory for dynamical systems. <em>SIREV</em>,
<em>64</em>(2), 229–340. (<a
href="https://doi.org/10.1137/21M1401243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator-theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: (1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems; (2) the approach is formulated in terms of measurements, making it ideal for leveraging big data and machine learning techniques; and (3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.},
  archive      = {J_SIREV},
  author       = {Steven L. Brunton and Marko Budišić and Eurika Kaiser and J. Nathan Kutz},
  doi          = {10.1137/21M1401243},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {229-340},
  shortjournal = {SIAM Rev.},
  title        = {Modern koopman theory for dynamical systems},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022c). Survey and review. <em>SIREV</em>, <em>64</em>(2), 227. (<a
href="https://doi.org/10.1137/22N975421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steven L. Brunton, Marko Budišić, Eurika Kaiser, and J. Nathan Kutz are the authors of the Survey and Review paper in this issue, “Modern Koopman Theory for Dynamical Systems.” Koopman theory is a valuable formalism for the study of dynamical systems; it has gained popularity in recent years in connection with data-driven analysis, control theory, and other areas. The basic idea is simple. If $(d/dt) {x} = {f}({x})$ is a dynamical system in ${\mathbb R}^n$, its solution flow is the one-parameter family of maps ${F}^t: {\mathbb R}^n\rightarrow {\mathbb R}^n$ such that $t\mapsto {F}^t({x}_0)$ is the solution that at time $t=0$ takes the value ${x}_0$. In an alternative description, rather than looking at points in ${\mathbb R}^n$ being moved by the flow, one may consider real-valued functions $g({x})$ being transformed by the dynamical system as $g \mapsto {\cal K}^t(g)$, where the Koopman (or composition) operator ${\cal K}^t$ is defined by ${\cal K}(g)({x}) = g({F}^t({x}))$. Thus the Koopman operator acts on an infinite-dimensional space of functions $g$ (bad news), but it is linear, even if the original dynamical system is not (good news). The idea behind the operator ${\cal K}^t$ is particularly useful when the dynamical system is being studied by big data$/$machine learning techniques in cases where $f$ and ${F}^t$ are unknown but measurements $g_i({x}(t_j))$ along a solution ${x}(t)$ are available. While the use of composition operators is very much older, B. O. Koopman noted in a seminal 1931 paper that, in the particular case of conservative dynamics, the operator ${\cal K}^t$ will be unitary in a suitable $L^2$ space, an observation that allowed him to apply to classical mechanics the theory of Hilbert space operators being developed at the time to formulate mathematically quantum mechanics. Koopman&#39;s results have been extended in many directions to dissipative or conservative systems, in continuous or discrete time. The survey in this issue does not assume a previous knowledge of Koopman theory and reviews recent developments on Koopman operators, with particular emphasis on the dynamic mode decomposition (DMD) algorithm and its variants used in many real-life applications. The paper contains almost five hundred references, shows applications to fluid mechanics, epidemiology, neuroscience, plasma physics, finance, robotics, the power grid, and other fields, and discusses connections with many areas, including numerical linear algebra, control theory, statistics, model reduction, and uncertainty qualification. I believe it will be of interest to a wide range of readers.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/22N975421},
  journal      = {SIAM Review},
  number       = {2},
  pages        = {227},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Book reviews. <em>SIREV</em>, <em>64</em>(1), 215–225. (<a
href="https://doi.org/10.1137/22N97541X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By now, we have all become somewhat accustomed to COVID-19. So it is a good time to read an introduction to epidemiological dynamics from a mathematical perspective. Our first featured review examines in detail Igor Nesteruk&#39;s introductory book COVID-19 Pandemic Dynamics: Mathematical Simulations. Aspects such as the $R_0$ value, SIR model, and herd immunity play a role in it. For our reviewer Chris Bauch, this book could include more detailed models, but on the other hand he believes “the book&#39;s lucid approach with in-depth country case studies will be helpful for learning more about applying mathematical models to inform pandemic response.” In Kevin Painter&#39;s review of the book Non-local Cell Adhesion Models, by Andreas Buttenschön and Thomas Hillen, we continue with biological models. Non-local models are in vogue right now, and this book shows a very interesting application area that poses great challenges to mathematical analysis. Subsequently, Karl Johan \AAström and Richard M. Murray&#39;s book on Feedback Systems: An Introduction for Scientists and Engineers is praised by reviewer Mario di Bernardo as “one of the best introductions to the analysis and design of feedback systems” available. The fourth review is written by our Senegalese colleague Diaraf Seck. As an internationally recognized expert on shape and topology optimization, he reviews the book Shapes and Diffeomorphisms, by Laurent Younes, praising it and sharing the author&#39;s fascination with the subject. Finally, the last two books deal again with modeling issues: Pablo Raúl Stinga reviews Ciprian G. Gal and Mahamadi Warma&#39;s book, Fractional-in-Time Semilinear Parabolic Equations and Applications, which he recommends to every researcher or graduate student. Finally, Robert Marangell reviews PDE Dynamics: An Introduction, by Christian Kuehn, and recommends it as a “nice book to use as the starting point for a course.”},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/22N97541X},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {215-225},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Newton’s method in mixed precision. <em>SIREV</em>,
<em>64</em>(1), 191–211. (<a
href="https://doi.org/10.1137/20M1342902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of reduced precision arithmetic to solve the linear equation for the Newton step. If one neglects the backward error in the linear solve, then well-known convergence theory implies that using single precision in the linear solve has very little negative effect on the nonlinear convergence rate. However, if one considers the effects of backward error, then the usual textbook estimates are very pessimistic and even the state-of-the-art estimates using probabilistic rounding analysis do not fully conform to experiments. We report on experiments with a specific example. We store and factor Jacobians in double, single, and half precision. In the single precision case we observe that the convergence rates for the nonlinear iteration do not degrade as the dimension increases and that the nonlinear iteration statistics are essentially identical to the double precision computation. In half precision we see that the nonlinear convergence rates, while poor, do not degrade as the dimension increases.},
  archive      = {J_SIREV},
  author       = {C. T. Kelley},
  doi          = {10.1137/20M1342902},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {191-211},
  shortjournal = {SIAM Rev.},
  title        = {Newton&#39;s method in mixed precision},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LU and CR elimination. <em>SIREV</em>, <em>64</em>(1),
181–190. (<a href="https://doi.org/10.1137/20M1358694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reduced row echelon form ${\bf rref}(A)$ has traditionally been used for classroom examples: small matrices $A$ with integer entries and low rank $r$. This paper creates a column-row rank-revealing factorization ${A=CR}$, with the first $r$ independent columns of $A$ in $C$ and the $r$ nonzero rows of ${\bf rref}(A)$ in $R$. We want to reimagine the start of a linear algebra course by helping students to see the independent columns of $A$ and the rank and the column space. If $B$ contains the first $r$ independent rows of $A$, then those rows of ${A=CR}$ produce ${B=WR}$. The $r$ by $r$ matrix $W$ has full rank $r$, where $B$ meets $C$. Then the triple factorization ${A=CW^{-1}B}$ treats columns and rows of $A$ ($C$ and $B$) in the same way.},
  archive      = {J_SIREV},
  author       = {Gilbert Strang and Cleve Moler},
  doi          = {10.1137/20M1358694},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {181-190},
  shortjournal = {SIAM Rev.},
  title        = {LU and CR elimination},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Education. <em>SIREV</em>, <em>64</em>(1), 179. (<a
href="https://doi.org/10.1137/22N975408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Education section in this issue presents two contributions. The first paper, “$LU$ and $CR$ Elimination,” is written by Gilbert Strang and Cleve Moler. This is an expository paper which is helpful to instructors and students interested in linear algebra and matrix manipulations. The focus of the paper is the column-row factorization for any matrix $A$ of rank $r$. The matrix is represented as $A=CR$, where the matrix $C$ contains the first $r$ independent columns of $A$, and the matrix $R$ contains the nonzero rows of the reduced row echelon form of $A$. The authors explain how to obtain the factorization and contrast it with $LU$, $QR$ factorizations, the single-value decomposition, and another factorization, $A= CW^{-1}B$, which they call magic. In section 6, the authors point to an important timely application pertaining to randomized algorithms for a low-rank approximation of large sparse matrices. The paper is clearly and nicely written. It includes a discussion about relevant MATLAB functions, examples, and several references to randomized linear algebra literature. The second paper is “Newton&#39;s Method in Mixed Precision,” presented by C. T. Kelley. The paper discusses the impact of precision in calculations on the rate of convergence of Newton&#39;s method for solving a system of nonlinear equations $F(x)=0.$ The vector-function $F$ is differentiable and its Jacobian $F&#39; (x)$ is nonsingular. It is assumed that the step $s$ in each iteration is determined by solving the linear equation $F&#39; (x)s= - F(x)$ via Gaussian elimination with column pivoting. The paper starts with the classical result on the local convergence of the method under standard assumptions and then passes onto analyzing the effect of errors arising in calculation of the function and in approximation of the Jacobian. The author&#39;s derivations predict no significant difference in the convergence of the nonlinear iteration between a double-precision analytic Jacobian with double precision in the linear solver and a forward-difference approximate Jacobian with a single precision in the linear solver. An interesting part of the paper deals with the question of estimating the effect of the backward error in the solver. The author observes that the worst case estimates are too pessimistic and rarely seen in practice. Invoking recent techniques in probabilistic rounding analysis, he derives more realistic results. The theoretical statements are supplemented by a numerical illustration in section 3. An additional example demonstrates how the theory breaks down if the Jacobian is singular at the solution.},
  archive      = {J_SIREV},
  author       = {Darinka Dentcheva},
  doi          = {10.1137/22N975408},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {179},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dimensionality reduction via dynamical systems: The case of
t-SNE. <em>SIREV</em>, <em>64</em>(1), 153–178. (<a
href="https://doi.org/10.1137/21M1446769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {t-distributed stochastic neighborhood embedding (t-SNE), a clustering and visualization method proposed by van der Maaten and Hinton in 2008, has rapidly become a standard tool in the natural sciences. Despite its overwhelming success, it has a distinct lack of mathematical foundations and the inner workings of the algorithm are not well understood. The purpose of this paper is to prove that t-SNE is able to recover well-separated clusters. As a by-product, the proof suggests that t-SNE is merely one of many possible algorithms of a large family of methods generated by dynamical systems---this perspective suggests new questions and problems, some of which we discuss.},
  archive      = {J_SIREV},
  author       = {George C. Linderman and Stefan Steinerberger},
  doi          = {10.1137/21M1446769},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {153-178},
  shortjournal = {SIAM Rev.},
  title        = {Dimensionality reduction via dynamical systems: The case of t-SNE},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). SIGEST. <em>SIREV</em>, <em>64</em>(1), 151. (<a
href="https://doi.org/10.1137/22N975391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The SIAM Journal on Mathematics of Data Science (SIMODS) launched in February 2019. The SIGEST article in this issue, “Dimensionality Reduction via Dynamical Systems: The Case of t-SNE,” by George C. Linderman and Stefan Steinerberger, is therefore our first SIMODS representative. Here the authors study t-SNE, a widely adopted clustering and visualization algorithm proposed by Laurens van der Maaten and Geoffrey Hinton in 2008---that publication has received over 24,000 Google Scholar citations to date. The SIMODS editorial board commented that ``[f]or anyone who tries to interact with people in bioinformatics, they will know that t-SNE is truly one of the most commonly used visualization algorithms, and in those visualizations the interpretation is always about the clusters that arise. Basically, t-SNE was something that was one principled way for dimensionality reduction that got co-opted for so much more. It&#39;s a rare theorist that takes what&#39;s actually already being used and working, and tries to explain it. This paper is certain to make a long term impact.&quot; In this work the authors use a dynamical systems perspective to explain why, and at what rate, the algorithm is guaranteed to converge successfully when applied to data that contains well-separated clusters. They also discuss fundamental connections to spectral clustering algorithms. Throughout, ideas are illustrated via colorful visualizations on both synthetic and real datasets. In preparing the article for SIGEST, the authors have appended the new section 7, which gives a big-picture, accessible overview of the topic. This new section also outlines recent developments in the area and lists a number of open problems.},
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/22N975391},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {151},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exactness of quadrature formulas. <em>SIREV</em>,
<em>64</em>(1), 132–150. (<a
href="https://doi.org/10.1137/20M1389522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard design principle for quadrature formulas is that they should be exact for integrands of a given class, such as polynomials of a fixed degree. We review the subject from this point of view and show that this principle fails to predict the actual behavior in four of the best-known cases: Newton--Cotes, Clenshaw--Curtis, Gauss--Legendre, and Gauss--Hermite quadrature. New results include (i) the observation that $x^k$ is integrated accurately by the Newton--Cotes formula even though the Chebyshev polynomial $T_k(x)$ is not; (ii) the introduction of a parameter-free variant of band-limited quadrature for arbitrary integrands, which is demonstrated to have a factor $\pi/2$ advantage over Gauss quadrature in integrating complex exponentials; (iii) a theorem establishing that chopping the real line to a finite interval achieves $O(\exp(-Cn^{2/3}))$ convergence for $n$-point quadrature of Gauss--Hermite integrands, whereas for the Gauss--Hermite formula it is just $O(\exp(-Cn^{1/2}))$; and (iv) an explanation of how this result is consistent with the “optimality” of the Gauss--Hermite formula.},
  archive      = {J_SIREV},
  author       = {Lloyd N. Trefethen},
  doi          = {10.1137/20M1389522},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {132-150},
  shortjournal = {SIAM Rev.},
  title        = {Exactness of quadrature formulas},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How exponentially ill-conditioned are contiguous submatrices
of the fourier matrix? <em>SIREV</em>, <em>64</em>(1), 105–131. (<a
href="https://doi.org/10.1137/20M1336837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear systems involving contiguous submatrices of the discrete Fourier transform (DFT) matrix arise in many applications, such as Fourier extension, superresolution, and coherent diffraction imaging. We show that the condition number of any such $p\times q$ submatrix of the $N\times N$ DFT matrix is at least $ \exp \left( \frac{\pi}{2} \left[\min(p,q)- \frac{pq}{N}\right]\right)$, up to algebraic prefactors. That is, fixing the shape parameters $(\al,\bt):=(p/N,q/N)\in(0,1)^2$, the growth is $e^{\rho N}$ as $N\to\infty$, the exponential rate being $\rho = \frac{\pi}{2}[\min(\alpha,\beta)- \alpha\beta]$. Our proof uses the Kaiser--Bessel transform pair (of which we give a self-contained proof), plus estimates on sums over distorted sinc functions, to construct a localized trial vector whose DFT is also localized. We warm up with an elementary proof of the above but with half the rate, via a periodized Gaussian trial vector. Using low-rank approximation of the kernel $e^{ixt}$, we also prove another lower bound $(4/e\pi \al)^q$, up to algebraic prefactors, which is stronger than the above for small $\al$ and $\bt$. When combined, the bounds are within a factor of two of the empirical asymptotic rate, uniformly over $(0,1)^2$, and become sharp in certain regions. However, the results are not asymptotic: they apply to essentially all $N$, $p$, and $q$, and with all constants explicit.},
  archive      = {J_SIREV},
  author       = {Alex H. Barnett},
  doi          = {10.1137/20M1336837},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {105-131},
  shortjournal = {SIAM Rev.},
  title        = {How exponentially ill-conditioned are contiguous submatrices of the fourier matrix?},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flow and elastic networks on the 𝑛-torus: Geometry,
analysis, and computation. <em>SIREV</em>, <em>64</em>(1), 59–104. (<a
href="https://doi.org/10.1137/18M1242056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks with phase-valued nodal variables are central in modeling several important societal and physical systems, including power grids, biological systems, and coupled oscillator networks. One of the distinctive features of phase-valued networks is the existence of multiple operating conditions corresponding to critical points of an energy function or feasible flows of a balance equation. For networks with phase-valued states it is not yet fully understood how many operating conditions exist, how to characterize them, and how to compute them efficiently. A deeper understanding of feasible operating conditions, including their dependence upon network structures, may lead to more reliable and efficient network systems. This paper introduces flow and elastic network problems on the ��-torus and provides a rigorous and comprehensive framework for their study. Based on a monotonicity assumption, this framework localizes the solutions, bounds their number, and leads to an algorithm to compute them. Our analysis is based on a novel winding partition of the ��-torus into winding cells, induced by Kirchhoff&#39;s angle law for undirected graphs. The winding partition has several useful properties, e.g., each winding cell contains at most one solution. The proposed algorithm is based on a novel contraction mapping and is guaranteed to compute all solutions. Finally, we apply our results to study numerically the active power flow equations in several test cases and estimate the power transmission capacity and congestion of a power network.},
  archive      = {J_SIREV},
  author       = {Saber Jafarpour and Elizabeth Y. Huang and Kevin D. Smith and Francesco Bullo},
  doi          = {10.1137/18M1242056},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {59-104},
  shortjournal = {SIAM Rev.},
  title        = {Flow and elastic networks on the 𝑛-torus: Geometry, analysis, and computation},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Research spotlights. <em>SIREV</em>, <em>64</em>(1), 57–58.
(<a href="https://doi.org/10.1137/22N97538X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are fortunate to have three Research Spotlights articles, on a broad range of topics, in the current issue of SIAM Review. The first of these articles is “Flow and Elastic Networks on the $n$-Torus: Geometry, Analysis, and Computation.&quot; In it, authors Saber Jafarpour, Elizabeth Y. Huang, Kevin D. Smith, and Francesco Bullo explore a special case of network-based modeling. Instead of representing a physical quantity (e.g., density) or abstract quantity (e.g., opinions), in this regime the nodal variables represent phases (points on the circle) and the network state belongs to the $n$-torus instead of $R^{n}$. A flow network is described by a commodity (e.g., electricity) that is being exchanged between adjacent nodes and is defined in terms of its flow balance equation, flow equation, and angle constraint. The authors use AC power flow equations as an illustration of a flow network and explain how the general theoretical questions posed by the authors relate to important questions about stability of power supply. An elastic network is described via an interaction energy between adjacent nodes. For insight into this type of network model on the $n$-torus, the authors describe the model of collective motion in biological and engineering networks. To frame the balance of the paper, the authors enumerate five important theoretical questions they will address to greater or lesser extent. Since they prove early on that “any result about the flow network setting is directly applicable to the elastic network setting, and vice versa,&quot; the authors are free to focus only on the flow network setting in the remainder. What follows is the development of a rigorous graph-theoretic and geometric framework to study the solutions of flow network problems on the $n$-torus. Proofs are left to the appendices to enable the reader to better follow the stages of this development. Those whose interest is piqued by this line of research will find suggestions for further consideration and application in the final section of the paper. The topic of the second Research Spotlights article in this issue deals with, to quote the author, “one of the most fundamental matrices in applied mathematics,&quot; the discrete Fourier transform (DFT) matrix. In the article “How Exponentially Ill-Conditioned Are Contiguous Submatrices of the Fourier Matrix?&quot; author Alex Barnett considers the conditioning of its $p \times q$ submatrices as functions of their scaled submatrix shape, where shape is defined by the pair ($p/N,q/N$), with $N$ denoting the DFT matrix dimension. The topic is of importance in many applications where the amplification of noise during the process must be avoided. Such applications, described in some level of detail in the introduction, include Fourier continuation, superresolution imaging, and coherent X-ray diffraction imaging. The paper focuses on finding good lower bounds on the condition number of a $p \times q$ submatrix in terms of $p$, $q$, and $N$. The author has chosen a creative presentation format. The three main theorems are presented immediately in section 1.1. The second of these largely supersedes the first, but, as the author notes, the first is included because “its prefactor is slightly stronger and, moreover, its elementary proof (section 2) is instructive.&quot; From there, the reader is presented with a discussion of previous work and an overview of the tools and techniques needed for the proofs presented in the following sections. With the background laid, each of the next three sections is devoted to a proof of one of three theorems. The interested reader might wonder about the sharpness of the bounds in practice, and indeed the discussion in section 5.1 addresses precisely this issue. Many open questions, such as upper bounds on the condition numbers, are described in the final section, leaving the curious reader with avenues for future research in this area. Our third article addresses a technique familiar to many of our RS readers: numerical quadrature. The article “Exactness of Quadrature Formulas,&quot; by Lloyd N. Trefethen, offers a survey and new results pertaining to four well-known quadrature rules. The jumping off point for the discussion is the concept of exactness, the principle around which quadrature rules are typically designed. A quadrature rule is said to be exact for a certain class of integrands---say, polynomials of degree less than or equal to $n-1$---if the rule returns the exact value of the integral for functions in this class. One point the paper drives home is the fact that “the exactness principle is not a reliable guide to the actual accuracy&quot; of the rule. A simple but striking example of this fact lies in the author&#39;s comparison of the accuracy of an $n$-point Newton--Cotes for fixed $n$ on two polynomial classes of degree $n$ and higher. The error for the rule on the monomials remains small as the degree is slowly increased beyond $n$, an observation the author actually explains in terms of the exactness principle, while the accuracy for Chebyshev polynomials blows up almost immediately. In the case of the Gauss--Hermite quadrature rule, that the exactness principle leads to inefficiency and suboptimality in a practical sense is illustrated artfully in section 5. Here, the author also gives a theoretical result suggesting the preferred method for evaluating integrals of type (5.1) would be to instead truncate the interval to a finite length and then apply an unweighted rule like Gauss--Legendre. In another section, modifications to Gauss quadrature are derived that have a small potential gain in the convergence rate by taking factors other than exactness into consideration. The moral of the story woven together through survey and new results is, in the parting words of the author, “if you are told that a certain formula is optimal, do not assume this is the end of the discussion.&quot;},
  archive      = {J_SIREV},
  author       = {Misha E. Kilmer},
  doi          = {10.1137/22N97538X},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {57-58},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional gaussian sampling: A review and a unifying
approach based on a stochastic proximal point algorithm. <em>SIREV</em>,
<em>64</em>(1), 3–56. (<a
href="https://doi.org/10.1137/20M1371026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient sampling from a high-dimensional Gaussian distribution is an old but high-stakes issue. Vanilla Cholesky samplers imply a computational cost and memory requirements that can rapidly become prohibitive in high dimensions. To tackle these issues, multiple methods have been proposed from different communities ranging from iterative numerical linear algebra to Markov chain Monte Carlo (MCMC) approaches. Surprisingly, no complete review and comparison of these methods has been conducted. This paper aims to review all these approaches by pointing out their differences, close relations, benefits, and limitations. In addition to reviewing the state of the art, this paper proposes a unifying Gaussian simulation framework by deriving a stochastic counterpart of the celebrated proximal point algorithm in optimization. This framework offers a novel and unifying revisiting of most of the existing MCMC approaches while also extending them. Guidelines to choosing the appropriate Gaussian simulation method for a given sampling problem in high dimensions are proposed and illustrated with numerical examples.},
  archive      = {J_SIREV},
  author       = {Maxime Vono and Nicolas Dobigeon and Pierre Chainais},
  doi          = {10.1137/20M1371026},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {3-56},
  shortjournal = {SIAM Rev.},
  title        = {High-dimensional gaussian sampling: A review and a unifying approach based on a stochastic proximal point algorithm},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022d). Survey and review. <em>SIREV</em>, <em>64</em>(1), 1. (<a
href="https://doi.org/10.1137/22N975378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian (also known as normal) probability distributions in (\mathbb R^d\) play a central role in statistics and are important in many branches of pure and applied mathematics, statistical physics, and several other fields. In the univariate (d=1) case, the corresponding bell curve is known even to many people outside the sciences. At present, there is much interest in algorithms that generate random vectors (X) in (\mathbb R^d) distributed according to a target Gaussian density function. In the scalar (d=1) case, or when (d&gt;1) but the scalar components of (X) are not correlated, there are several simple efficient algorithms, including the well-known Box--Muller method. In the general case, where (d&gt;1) and there are correlations between the scalar components, the standard approach is to perform a linear change of variables (X = LY) so that the components (Y) are uncorrelated and may be sampled easily. Typically, the matrix (L) is obtained via a Cholesky factorization of the precision matrix of the target distribution, an approach which works well if (d) is not too large (for a laptop if, say, (d &lt; 10^5)). However, many recent applications, including image analysis, spatial statistics, graphical structures, and others, operate with very large values of (d) and the Cholesky algorithm, with an (\mathcalO(d^3)) complexity and (\mathcalO(d^2)) memory requirements, may not be feasible. The Survey and Review paper in this issue, “High-Dimensional Gaussian Sampling: A Review and a Unifying Approach Based on a Stochastic Proximal Point Algorithm” by Maxime Vono, Nicolas Dobigeon, and Pierre Chainais, addresses the problem of obtaining Gaussian samples when the Cholesky factorization is not an option. The paper presents in a unified way and compares many algorithms suggested in different scientific communities. The algorithms may be grouped into two classes. In the first, numerical linear algebra techniques are used to reduce the computational complexity and/or the memory requirements. In the second, the samples are obtained via Markov chain Monte Carlo approaches; surprisingly, the resulting algorithms are very much related to classical iterative methods for the solution of linear systems, including Jacobi, Gauss--Seidel, and SOR. For this reason the paper will be relevant to readers interested in numerical linear algebra. For those whose work requires generating random samples, the paper includes a neat decision tree to choose, in a given application, among the many available algorithms. The authors have also made available software for all the methods considered in their survey.},
  archive      = {J_SIREV},
  author       = {J. M. Sanz-Serna},
  doi          = {10.1137/22N975378},
  journal      = {SIAM Review},
  number       = {1},
  pages        = {1},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {64},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
