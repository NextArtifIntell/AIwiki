<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMAX_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simax---72">SIMAX - 72</h2>
<ul>
<li><details>
<summary>
(2022). Theoretically and computationally convenient geometries on
full-rank correlation matrices. <em>SIMAX</em>, <em>43</em>(4),
1851–1872. (<a href="https://doi.org/10.1137/22M1471729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In contrast to SPD matrices, few tools exist to perform Riemannian statistics on the open elliptope of full-rank correlation matrices. The quotient-affine metric was recently built as the quotient of the affine-invariant metric by the congruence action of positive diagonal matrices. The space of SPD matrices had always been thought of as a Riemannian homogeneous space. In contrast, we view in this work SPD matrices as a Lie group and the affine-invariant metric as a left-invariant metric. This unexpected new viewpoint allows us to generalize the construction of the quotient-affine metric and to show that the main Riemannian operations can be computed numerically. However, the uniqueness of the Riemannian logarithm or the Fréchet mean are not ensured, which is bad for computing on the elliptope. Hence, we define three new families of Riemannian metrics on full-rank correlation matrices which provide Hadamard structures, including two flat. Thus the Riemannian logarithm and the Fréchet mean are unique. The two (flat) vector space structures are particularly appealing because they reduce the manifold of full-rank correlation matrices to a vector space. We also define a nilpotent group structure for which the affine logarithm and the group mean are unique. We provide the main Riemannian/group operations of these four structures in closed form.},
  archive      = {J_SIMAX},
  author       = {Yann Thanwerdas and Xavier Pennec},
  doi          = {10.1137/22M1471729},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1851-1872},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Theoretically and computationally convenient geometries on full-rank correlation matrices},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Johnson–lindenstrauss embeddings with kronecker structure.
<em>SIMAX</em>, <em>43</em>(4), 1806–1850. (<a
href="https://doi.org/10.1137/21M1432491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove the Johnson–Lindenstrauss property for matrices , where has the restricted isometry property and is a diagonal matrix containing the entries of a Kronecker product of independent Rademacher vectors. Such embeddings have been proposed in recent works for a number of applications concerning compression of tensor structured data, including the oblivious sketching procedure by Ahle et al. for approximate tensor computations. For preserving the norms of points simultaneously, our result requires to have the restricted isometry property for sparsity . In the case of subsampled Hadamard matrices, this can improve the dependence of the embedding dimension on to while the best previously known result required . That is, for the case of at the core of the oblivious sketching procedure by Ahle et al., the scaling improves from cubic to quadratic. We provide a counterexample to prove that the scaling established in our result is optimal under mild assumptions.},
  archive      = {J_SIMAX},
  author       = {Stefan Bamberger and Felix Krahmer and Rachel Ward},
  doi          = {10.1137/21M1432491},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1806-1850},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Johnson–Lindenstrauss embeddings with kronecker structure},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust eigenvectors of symmetric tensors. <em>SIMAX</em>,
<em>43</em>(4), 1784–1805. (<a
href="https://doi.org/10.1137/21M1462052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The tensor power method generalizes the matrix power method to higher-order arrays, or tensors. Like in the matrix case, the fixed points of the tensor power method are the eigenvectors of the tensor. While every real symmetric matrix has an eigendecomposition, the vectors generating a symmetric decomposition of a real symmetric tensor are not always eigenvectors of the tensor. In this paper we show that whenever an eigenvector is a generator of the symmetric decomposition of a symmetric tensor, then (if the order of the tensor is sufficiently high) this eigenvector is robust, i.e., it is an attracting fixed point of the tensor power method. We exhibit new classes of symmetric tensors whose symmetric decomposition consists of eigenvectors. Generalizing orthogonally decomposable tensors, we consider equiangular tight frame decomposable and equiangular set decomposable tensors. Our main result implies that such tensors can be decomposed using the tensor power method.},
  archive      = {J_SIMAX},
  author       = {Tommi Muller and Elina Robeva and Konstantin Usevich},
  doi          = {10.1137/21M1462052},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1784-1805},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Robust eigenvectors of symmetric tensors},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linear asymptotic convergence of anderson acceleration:
Fixed-point analysis. <em>SIMAX</em>, <em>43</em>(4), 1755–1783. (<a
href="https://doi.org/10.1137/21M1449579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the asymptotic convergence of AA, i.e., Anderson acceleration (AA) with window size for accelerating fixed-point methods , . Convergence acceleration by AA has been widely observed but is not well understood. We consider the case where the fixed-point iteration function is differentiable and the convergence of the fixed-point method itself is root-linear. We identify numerically several conspicuous properties of AA convergence: First, AA sequences converge root-linearly, but the root-linear convergence factor depends strongly on the initial condition. Second, the AA acceleration coefficients do not converge but oscillate as converges to . To shed light on these observations, we write the AA iteration as an augmented fixed-point iteration , and analyze the continuity and differentiability properties of and . We find that the vector of acceleration coefficients is not continuous at the fixed point . However, we show that, despite the discontinuity of , the iteration function is Lipschitz continuous and directionally differentiable at for AA(1), and we generalize this to AA with for most cases. Furthermore, we find that is not differentiable at . We then discuss how these theoretical findings relate to the observed convergence behavior of AA. The discontinuity of at allows to oscillate as converges to , and the nondifferentiability of allows AA sequences to converge with root-linear convergence factors that strongly depend on the initial condition. Additional numerical results illustrate our findings for several linear and nonlinear fixed-point iterations and for various values of the window size .},
  archive      = {J_SIMAX},
  author       = {Hans De Sterck and Yunhui He},
  doi          = {10.1137/21M1449579},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1755-1783},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Linear asymptotic convergence of anderson acceleration: Fixed-point analysis},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Verified error bounds for all eigenvalues and eigenvectors
of a matrix. <em>SIMAX</em>, <em>43</em>(4), 1736–1754. (<a
href="https://doi.org/10.1137/21M1451440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A verification method is presented to compute error bounds for all eigenvectors and eigenvalues, including clustered and/or multiple ones of a general, real, or complex matrix. In case of a narrow cluster, error bounds for an invariant subspace are computed because computation of a single eigenvector may be ill-posed. Computer algebra and verification methods have in common that the computed results are correct with mathematical certainty. Unlike a computer algebra method, a verification method may fail in the sense that only partial or no inclusions at all are computed. That may happen for very ill conditioned problems being too sensitive for the arithmetical precision in use. That cannot happen for computer algebra methods which are “never-failing” because potentially infinite precision is used. In turn, however, that may slow down computer algebra methods significantly and may impose limitations on the problem size. In contrast, verification methods solely use floating-point operations so that their computing time and treatable problem size is of the order of that of purely numerical algorithms. For our problem it is proved that the union of the eigenvalue bounds contains the whole spectrum of the matrix, and bounds for corresponding invariant subspaces are computed. The computational complexity to compute inclusions of all eigenpairs of an -matrix is .},
  archive      = {J_SIMAX},
  author       = {Siegfried M. Rump},
  doi          = {10.1137/21M1451440},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1736-1754},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Verified error bounds for all eigenvalues and eigenvectors of a matrix},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A columnwise update algorithm for sparse stochastic matrix
factorization. <em>SIMAX</em>, <em>43</em>(4), 1712–1735. (<a
href="https://doi.org/10.1137/21M145313X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Nonnegative matrix factorization arises widely in machine learning and data analysis. In this paper, for a given factorization of rank , we consider the sparse stochastic matrix factorization (SSMF) of decomposing a prescribed -by- stochastic matrix into a product of an -by- stochastic matrix and an -by- stochastic matrix , where both and are required to be sparse. With the prescribed sparsity level, we reformulate the SSMF as an unconstrained nonconvex-nonsmooth minimization problem and introduce a columnwise update algorithm for solving the minimization problem. We show that our algorithm converges globally. The main advantage of our algorithm is that the generated sequence converges to a special critical point of the cost function, which is nearly a global minimizer over each column vector of the -factor and is a global minimizer over the -factor as a whole if there is no sparsity requirement on . Numerical experiments on both synthetic and real data sets are given to demonstrate the effectiveness of our proposed algorithm.},
  archive      = {J_SIMAX},
  author       = {Guiyun Xiao and Zheng-Jian Bai and Wai-Ki Ching},
  doi          = {10.1137/21M145313X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1712-1735},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A columnwise update algorithm for sparse stochastic matrix factorization},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient preconditioners for interior point methods via a
new schur complement-based strategy. <em>SIMAX</em>, <em>43</em>(4),
1680–1711. (<a href="https://doi.org/10.1137/21M1416552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a novel preconditioned inexact primal-dual interior point method for constrained convex quadratic programming problems. The algorithm we describe invokes the preconditioned conjugate gradient method on a new reduced Schur complement Karush–Kuhn–Tucker (KKT) system in implicit form. In contrast to standard approaches, the Schur complement formulation we consider enables reuse of the factorization of the KKT matrix with rows and columns corresponding to inequality constraints excluded across all interior point iterations. Further, two new preconditioners are presented for the resulting reduced system that alleviate the ill-conditioning associated with slack variables in primal-dual interior point methods. Each of the preconditioners we propose also provably reduces the number of unique eigenvalues for the coefficient matrix and thus the conjugate gradient method iteration count. One preconditioner is efficient when the number of equality constraints is small, while the other is efficient when the number of remaining degrees of freedom is small. Numerical experiments with synthetic problems and problems from the Maros–Mészáros quadratic programming collection show that our preconditioned inexact interior point solvers are effective at improving conditioning and reducing cost. Across all test problems for which the direct method is not fastest, our preconditioned methods achieve a reduction in cost by a geometric mean of 1.432 relative to the best alternative preconditioned method for each problem.},
  archive      = {J_SIMAX},
  author       = {Samah Karim and Edgar Solomonik},
  doi          = {10.1137/21M1416552},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1680-1711},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient preconditioners for interior point methods via a new schur complement-based strategy},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic spectral properties of the hilbert <span
class="math inline"><em>L</em></span> -matrix. <em>SIMAX</em>,
<em>43</em>(4), 1658–1679. (<a
href="https://doi.org/10.1137/22M1476794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study asymptotic spectral properties of the generalized Hilbert -matrix for large order . First, for general , we deduce the asymptotic distribution of eigenvalues of outside the origin. Second, for , asymptotic formulas for small eigenvalues of are derived. Third, in the classical case , we also prove asymptotic formulas for large eigenvalues of . In particular, we obtain an asymptotic expansion of improving Wilf’s formula for the best constant in truncated Hardy’s inequality.},
  archive      = {J_SIMAX},
  author       = {František Štampach},
  doi          = {10.1137/22M1476794},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1658-1679},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Asymptotic spectral properties of the hilbert \(L\) -matrix},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bootstrap multigrid eigensolver. <em>SIMAX</em>,
<em>43</em>(4), 1627–1657. (<a
href="https://doi.org/10.1137/20M131151X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper introduces bootstrap multigrid methods for solving eigenvalue problems arising from the discretization of partial differential equations. Inspired by the full bootstrap algebraic multigrid setup algorithm that includes an AMG eigensolver, we illustrate how the algorithm can be simplified for the case of a discretized partial differential equation, thereby developing a bootstrap geometric multigrid (BMG) approach. We illustrate numerically the efficacy of the BMG method for (1) recovering eigenvalues having large multiplicity, (2) computing interior eigenvalues, and (3) approximating shifted indefinite eigenvalue problems. Numerical experiments are presented to illustrate the basic components and ideas behind the success of the overall bootstrap multigrid approach. For completeness, we present a simplified error analysis of a two-grid bootstrap algorithm for the Laplace–Beltrami eigenvalue problem.},
  archive      = {J_SIMAX},
  author       = {James Brannick and Shuhao Cao},
  doi          = {10.1137/20M131151X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1627-1657},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A bootstrap multigrid eigensolver},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Structured matrix approximations via tensor decompositions.
<em>SIMAX</em>, <em>43</em>(4), 1599–1626. (<a
href="https://doi.org/10.1137/21M1418290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide a computational framework for approximating a class of structured matrices; here, the term structure is very general, and may refer to a regular sparsity pattern (e.g., block banded), or be more highly structured (e.g., symmetric block Toeplitz). The goal is to uncover additional latent structure that will in turn lead to computationally efficient algorithms when the new structured matrix approximations are employed in place of the original operator. Our approach has three steps: map the structured matrix to tensors, use tensor compression algorithms, and map the compressed tensors back to obtain two different matrix representations—sum of Kronecker products and block low-rank format. The use of tensor decompositions enables us to uncover latent structure in the problem and leads to compressed representations of the original matrix that can be used efficiently in applications. The resulting matrix approximations are memory efficient, easy to compute with, and preserve the error that is due to the tensor compression in the Frobenius norm. Our framework is quite general. We illustrate the ability of our method to uncover block-low-rank format on structured matrices from two applications: system identification and space-time covariance matrices. In addition, we demonstrate that our approach can uncover the sum of structured Kronecker products structure on several matrices from the SuiteSparse collection. Finally, we show that our framework is broad enough to encompass and improve on other related results from the literature, as we illustrate with the approximation of a three-dimensional blurring operator.},
  archive      = {J_SIMAX},
  author       = {Misha E. Kilmer and Arvind K. Saibaba},
  doi          = {10.1137/21M1418290},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1599-1626},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structured matrix approximations via tensor decompositions},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fast iterative algorithm for near-diagonal eigenvalue
problems. <em>SIMAX</em>, <em>43</em>(4), 1573–1598. (<a
href="https://doi.org/10.1137/21M1401474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a novel eigenvalue algorithm for near-diagonal matrices inspired by Rayleigh–Schrödinger perturbation theory and termed iterative perturbative theory (IPT). Contrary to standard eigenvalue algorithms, which are either “direct” (to compute all eigenpairs) or “iterative” (to compute just a few), IPT computes any number of eigenpairs with the same basic iterative procedure. Thanks to this perfect parallelism, IPT proves more efficient than classical methods (LAPACK or CUSOLVER for the full-spectrum problem, preconditioned Davidson solvers for extremal eigenvalues). We give sufficient conditions for linear convergence and demonstrate performance on dense and sparse test matrices, including one from quantum chemistry. The code is available at http://github.com/msmerlak/IterativePerturbationTheory.jl.},
  archive      = {J_SIMAX},
  author       = {Maseim Kenmoe and Ronald Kriemann and Matteo Smerlak and Anton S. Zadorin},
  doi          = {10.1137/21M1401474},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1573-1598},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A fast iterative algorithm for near-diagonal eigenvalue problems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lipschitz analysis of generalized phase retrievable matrix
frames. <em>SIMAX</em>, <em>43</em>(3), 1518–1571. (<a
href="https://doi.org/10.1137/21M1435446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical phase retrieval problem arises in contexts ranging from speech recognition to x-ray crystallography and quantum state tomography. The generalization to matrix frames is natural in the sense that it corresponds to quantum tomography of impure states. We provide computable global stability bounds for the quasi-linear analysis map $\beta$ and a path forward for understanding related problems in terms of the differential geometry of key spaces. In particular, we manifest a Whitney stratification of the positive semidefinite matrices of low rank which allows us to “stratify” the computation of the global stability bound. We show that for the impure state case no such global stability bounds can be obtained for the nonlinear analysis map $\alpha$ with respect to certain natural distance metrics. Finally, our computation of the global lower Lipschitz constant for the $\beta$ analysis map provides novel conditions for a frame to be generalized phase retrievable.},
  archive      = {J_SIMAX},
  author       = {Radu Balan and Chris B. Dock},
  doi          = {10.1137/21M1435446},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1518-1571},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Lipschitz analysis of generalized phase retrievable matrix frames},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Practical leverage-based sampling for low-rank tensor
decomposition. <em>SIMAX</em>, <em>43</em>(3), 1488–1517. (<a
href="https://doi.org/10.1137/21M1441754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank canonical polyadic tensor decomposition is useful in data analysis and can be computed by solving a sequence of overdetermined least squares subproblems. Motivated by consideration of sparse tensors, we propose sketching each subproblem using leverage scores to select a subset of the rows, with probabilistic guarantees on the solution accuracy. We randomly sample rows proportional to leverage score upper bounds that can be efficiently computed using the special Khatri--Rao subproblem structure inherent in tensor decomposition. Crucially, for a $(d+1)$-way tensor, the number of rows in the sketched system is $O(r^d/\epsilon)$ for a decomposition of rank $r$ and $\epsilon$-accuracy in the least squares solve, independent of both the size and the number of nonzeros in the tensor. Along the way, we provide a practical solution to the generic matrix sketching problem of sampling overabundance for high-leverage-score rows, proposing to include such rows deterministically and combine repeated samples in the sketched system; we conjecture that this can lead to improved theoretical bounds. Numerical results on real-world large-scale tensors show the method is significantly faster than deterministic methods at nearly the same level of accuracy.},
  archive      = {J_SIMAX},
  author       = {Brett W. Larsen and Tamara G. Kolda},
  doi          = {10.1137/21M1441754},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1488-1517},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Practical leverage-based sampling for low-rank tensor decomposition},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the condition number of the shifted real ginibre
ensemble. <em>SIMAX</em>, <em>43</em>(3), 1469–1487. (<a
href="https://doi.org/10.1137/21M1424408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive an accurate lower tail estimate on the lowest singular value $\sigma_1(X-z)$ of a real Gaussian (Ginibre) random matrix $X$ shifted by a complex parameter $z$. Such shift effectively changes the upper tail behavior of the condition number $\kappa(X-z)$ from the slower $(\kappa(X-z)\ge t)\lesssim 1/t$ decay typical for real Ginibre matrices to the faster $1/t^2$ decay seen for complex Ginibre matrices as long as $z$ is away from the real axis. This sharpens and resolves a recent conjecture in [J. Banks et al., https://arxiv.org/abs/2005.08930, 2020] on the regularizing effect of the real Ginibre ensemble with a genuinely complex shift. As a consequence we obtain an improved upper bound on the eigenvalue condition numbers (known also as the eigenvector overlaps) for real Ginibre matrices. The main technical tool is a rigorous supersymmetric analysis from our earlier work [Probab. Math. Phys., 1 (2020), pp. 101--146].},
  archive      = {J_SIMAX},
  author       = {Giorgio Cipolloni and László Erdös and Dominik Schröder},
  doi          = {10.1137/21M1424408},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1469-1487},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On the condition number of the shifted real ginibre ensemble},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RidgeSketch: A fast sketching based solver for large scale
ridge regression. <em>SIMAX</em>, <em>43</em>(3), 1440–1468. (<a
href="https://doi.org/10.1137/21M1422963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new variants of the sketch-and-project method for solving large scale ridge regression problems. First, we propose a new momentum alternative and provide a theorem showing it can speed up the convergence of sketch-and-project, through a fast sublinear convergence rate. We carefully delimit under what settings this new sublinear rate is faster than the previously known linear rate of convergence of sketch-and-project without momentum. Second, we consider combining the sketch-and-project method with new modern sketching methods such as Count sketch, SubCount sketch (a new method we propose), and subsampled Hadamard transforms. We show experimentally that when combined with the sketch-and-project method, the (Sub)Count sketch is very effective on sparse data and the standard Subsample sketch is effective on dense data. Indeed, we show that these sketching methods, combined with our new momentum scheme, result in methods that are competitive even when compared to the conjugate gradient method on real large scale data. On the contrary, we show the subsampled Hadamard transform does not perform well in this setting, despite the use of fast Hadamard transforms, and nor do recently proposed acceleration schemes work well in practice. To support all of our experimental findings, and invite the community to validate and extend our results, with this paper we are also releasing an open source software package: RidgeSketch. We designed this object-oriented package in Python for testing sketch-and-project methods and benchmarking ridge regression solvers. RidgeSketch is highly modular, and new sketching methods can easily be added as subclasses. We provide code snippets of our package in the appendix.},
  archive      = {J_SIMAX},
  author       = {Nidham Gazagnadou and Mark Ibrahim and Robert M. Gower},
  doi          = {10.1137/21M1422963},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1440-1468},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {RidgeSketch: A fast sketching based solver for large scale ridge regression},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Factorization of singular matrix polynomials and matrices
with circular higher rank numerical ranges. <em>SIMAX</em>,
<em>43</em>(3), 1423–1439. (<a
href="https://doi.org/10.1137/22M1475934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorization of regular Hermitian valued trigonometric polynomials (on the unit circle) and Hermitian valued polynomials (on the real line) have been studied well. In this paper we drop the condition of regularity and study factorization of singular Hermitian valued (trigonometric) polynomials. We subsequently apply the results to obtain a characterization of matrices with a circular higher rank numerical range and derive a new version of Anderson&#39;s theorem. As a special case, we obtain a new characterization of matrices with a circular numerical range.},
  archive      = {J_SIMAX},
  author       = {Edward Poon and Ilya M. Spitkovsky and Hugo J. Woerdeman},
  doi          = {10.1137/22M1475934},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1423-1439},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Factorization of singular matrix polynomials and matrices with circular higher rank numerical ranges},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A note on inexact inner products in GMRES. <em>SIMAX</em>,
<em>43</em>(3), 1406–1422. (<a
href="https://doi.org/10.1137/20M1320018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show to what extent the accuracy of the inner products computed in the GMRES iterative solver can be reduced as the iterations proceed without affecting the convergence rate or final accuracy achieved by the iterates. We bound the loss of orthogonality in GMRES with inexact inner products. We use this result to bound the ratio of the residual norm in inexact GMRES to the residual norm in exact GMRES and give a condition under which this ratio remains close to 1. We illustrate our results with examples in variable floating-point arithmetic.},
  archive      = {J_SIMAX},
  author       = {Serge Gratton and Ehouarn Simon and David Titley-Peloquin and Philippe L. Toint},
  doi          = {10.1137/20M1320018},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1406-1422},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A note on inexact inner products in GMRES},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infinite GMRES for parameterized linear systems.
<em>SIMAX</em>, <em>43</em>(3), 1382–1405. (<a
href="https://doi.org/10.1137/21M1410324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider linear parameterized systems $A(\mu) x(\mu) = b$ for many different $\mu$, where $A$ is large and sparse and depends nonlinearly on $\mu$. Solving such systems individually for each $\mu$ would require great computational effort. In this work we propose to compute a partial parameterization $\tilde{x} \approx x(\mu)$, where $\tilde{x}(\mu)$ is cheap to evaluate for many $\mu$. Our methods are based on the observation that a companion linearization can be formed where the dependence on $\mu$ is only linear. In particular, methods are presented that combine the well-established Krylov subspace method for linear systems, GMRES, with algorithms for nonlinear eigenvalue problems (NEPs) to generate a basis for the Krylov subspace. Within this new approach, the basis matrix is constructed in three different ways, using a tensor structure and exploiting that certain problems have low-rank properties. The methods are analyzed analogously to the standard convergence theory for the method GMRES for linear systems. More specifically, the error is estimated based on the magnitude of the parameter $\mu$ and the spectrum of the linear companion matrix, which corresponds to the reciprocal solutions to the corresponding NEP. Numerical experiments illustrate the competitiveness of the methods for large-scale problems. The simulations are reproducible and publicly available online.},
  archive      = {J_SIMAX},
  author       = {Elias Jarlebring and Siobhán Correnty},
  doi          = {10.1137/21M1410324},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1382-1405},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Infinite GMRES for parameterized linear systems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Strongly minimal self-conjugate linearizations for
polynomial and rational matrices. <em>SIMAX</em>, <em>43</em>(3),
1354–1381. (<a href="https://doi.org/10.1137/21M1453542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that we can always construct strongly minimal linearizations of an arbitrary rational matrix from its Laurent expansion around the point at infinity, which happens to be the case for polynomial matrices expressed in the monomial basis. If the rational matrix has a particular self-conjugate structure, we show how to construct strongly minimal linearizations that preserve it. The structures that are considered are the Hermitian and skew-Hermitian rational matrices with respect to the real line, and the para-Hermitian and para-skew-Hermitian matrices with respect to the imaginary axis. We pay special attention to the construction of strongly minimal linearizations for the particular case of structured polynomial matrices. The proposed constructions lead to efficient numerical algorithms for constructing strongly minimal linearizations. The fact that they are valid for any rational matrix is an improvement on any other previous approach for constructing other classes of structure preserving linearizations, which are not valid for any structured rational or polynomial matrix. The use of the recent concept of strongly minimal linearization is the key for getting such generality. Strongly minimal linearizations are Rosenbrock&#39;s polynomial system matrices of the given rational matrix, but with a quadruple of linear polynomial matrices (i.e., pencils): $L(\lambda):=\Big[\begin{array}{ccc} A(\lambda) &amp; -B(\lambda) \\ C(\lambda) &amp; D(\lambda) \end{array}\Big]$, where $A(\lambda)$ is regular, and the pencils $ \left[\begin{array}{ccc} A(\lambda) &amp; -B(\lambda) \end{array}\right]$ and $ \Big[\begin{array}{ccc} A(\lambda) \\ C(\lambda) \end{array}\Big]$ have no finite or infinite eigenvalues. Strongly minimal linearizations contain the complete information about the zeros, poles, and minimal indices of the rational matrix and allow one to very easily recover its eigenvectors and minimal bases. Thus, they can be combined with algorithms for the generalized eigenvalue problem for computing the complete spectral information of the rational matrix.},
  archive      = {J_SIMAX},
  author       = {Froilán M. Dopico and María C. Quintana and Paul Van Dooren},
  doi          = {10.1137/21M1453542},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1354-1381},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Strongly minimal self-conjugate linearizations for polynomial and rational matrices},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A uniform spectral analysis for a preconditioned all-at-once
system from first-order and second-order evolutionary problems.
<em>SIMAX</em>, <em>43</em>(3), 1331–1353. (<a
href="https://doi.org/10.1137/21M145358X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving evolutionary equations in a parallel-in-time manner is an attractive topic. The iterative algorithm based on the block $\alpha$-circulant preconditioning technique has shown promising advantages, especially for hyperbolic problems. By fast Fourier transform for factorizing the involved circulant matrices, the preconditioned iteration can be computed efficiently via the so-called diagonalization technique, which yields a direct parallel implementation across all time levels. In recent years, considerable efforts have been devoted to exploring the spectral property of the iteration matrix arising from the used time-integrator, which leads to many case-by-case studies. Denoting by $\mathcal{K} $ and $\mathcal{P}_\alpha$ the all-at-once matrix of the evolutionary PDEs and the corresponding block $\alpha$-circulant preconditioner, we will present a systematic spectral analysis for the matrix $\mathcal{P}_\alpha^{-1}\mathcal{K}$ for both the first-order and second-order evolutionary problems. For the first-order problems our analysis works for all stable single-step time-integrators, while for the second-order problems our analysis works for a large class of symmetric two-step methods which could be arbitrarily high-order. Illustrative numerical experiments are presented to complement our theory.},
  archive      = {J_SIMAX},
  author       = {Shu-Lin Wu and Tao Zhou and Zhi Zhou},
  doi          = {10.1137/21M145358X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1331-1353},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A uniform spectral analysis for a preconditioned all-at-once system from first-order and second-order evolutionary problems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-interactive residual smoothing for global and block
lanczos-type solvers for linear systems with multiple right-hand sides.
<em>SIMAX</em>, <em>43</em>(3), 1308–1330. (<a
href="https://doi.org/10.1137/21M1436774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global and block Krylov subspace methods are efficient iterative solvers for large sparse linear systems with multiple right-hand sides. However, global or block Lanczos-type solvers often exhibit large oscillations in the residual norms and may have a large residual gap relating to the loss of attainable accuracy of the approximations. Conventional residual smoothing schemes suppress these oscillations but cannot improve the attainable accuracy, whereas a recent residual smoothing scheme enables the improvement of the attainable accuracy for single right-hand-side Lanczos-type solvers. The underlying concept of this scheme is that the primary and smoothed sequences of the approximations and residuals influence one another, thereby avoiding the severe propagation of rounding errors. In the present study, we extend this cross-interactive residual smoothing to the case of solving linear systems with multiple right-hand sides. The resulting smoothed methods can reduce the residual gap with a low additional cost compared to their original counterparts. We demonstrate the effectiveness of the proposed approach through rounding error analysis and numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Kensuke Aihara and Akira Imakura and Keiichi Morikuni},
  doi          = {10.1137/21M1436774},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1308-1330},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Cross-interactive residual smoothing for global and block lanczos-type solvers for linear systems with multiple right-hand sides},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subspaces analysis for random projection UTV framework.
<em>SIMAX</em>, <em>43</em>(3), 1291–1307. (<a
href="https://doi.org/10.1137/21M1451592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The UTV decompositions are promising and computationally efficient alternatives to the singular value decomposition (SVD), which can provide high-quality information about rank, range, and nullspace. However, for large-scale matrices, we want more computationally efficient algorithms. Recently, randomized algorithms with their surprising reliability and computational efficiency have become increasingly popular in many application areas. In this paper, we analyze the subspace properties of a random projection UTV framework and give the bounds of subspace distances between the exact and the approximate singular subspaces, the approximate methods including random projection ULV, random projection URV, and random projection SVD. Numerical experiments demonstrate the effectiveness of the proposed bounds.},
  archive      = {J_SIMAX},
  author       = {Yuan Jian},
  doi          = {10.1137/21M1451592},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1291-1307},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Subspaces analysis for random projection UTV framework},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A theory of quantum subspace diagonalization.
<em>SIMAX</em>, <em>43</em>(3), 1263–1290. (<a
href="https://doi.org/10.1137/21M145954X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum subspace diagonalization methods are an exciting new class of algorithms for solving large-scale eigenvalue problems using quantum computers. Unfortunately, these methods require the solution of an ill-conditioned generalized eigenvalue problem, with a matrix pencil corrupted by a nonnegligible amount of noise that is far above the machine precision. Despite pessimistic predictions from classical worst-case perturbation theories, these methods can perform reliably well if the generalized eigenvalue problem is solved using a standard truncation strategy. By leveraging and advancing classical results in matrix perturbation theory, we provide a theoretical analysis of this surprising phenomenon, proving that under certain natural conditions, a quantum subspace diagonalization algorithm can accurately compute the smallest eigenvalue of a large Hermitian matrix. We give numerical experiments demonstrating the effectiveness of the theory and providing practical guidance for the choice of truncation level. Our new results can also be of independent interest to solving eigenvalue problems outside the context of quantum computation.},
  archive      = {J_SIMAX},
  author       = {Ethan N. Epperly and Lin Lin and Yuji Nakatsukasa},
  doi          = {10.1137/21M145954X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1263-1290},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A theory of quantum subspace diagonalization},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convergence to singular triplets in the two-sided
block-jacobi SVD algorithm with dynamic ordering. <em>SIMAX</em>,
<em>43</em>(3), 1238–1262. (<a
href="https://doi.org/10.1137/21M1411895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the convergence of computed quantities to singular triplets in the serial and parallel block-Jacobi singular value decomposition (SVD) algorithm with dynamic ordering. After eliminating possible zero singular values by two finite decompositions of a matrix $A\in\mathbb{C}^{m\times n},\, m\geq n$, which reduce the matrix dimensions to $n\times n$, it is shown that an iterated nonsingular matrix $A^{(k)}$ converges to a fixed diagonal matrix and its diagonal elements are the singular values of an initial matrix $A$. For the case of simple singular values, it is proved that the corresponding columns of the matrices of accumulated unitary transformations converge to corresponding left and right singular vectors. When a multiple singular value (or a cluster of singular values) is well separated from the other singular values, the convergence of two sequences of appropriate orthogonal projectors towards the orthogonal projectors onto the corresponding left and right subspaces is proved. Additionally, the convergence of orthogonal projectors leads to the convergence of certain computed subspaces towards the singular left and right subspaces spanned by left and right singular vectors corresponding to a multiple singular value or a cluster. An example computed in MATLAB illustrates the developed theory.},
  archive      = {J_SIMAX},
  author       = {Gabriel Okša and Yusaku Yamamoto and Marián Vajteršic},
  doi          = {10.1137/21M1411895},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1238-1262},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Convergence to singular triplets in the two-sided block-jacobi SVD algorithm with dynamic ordering},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Diagonal scalings for the eigenstructure of arbitrary
pencils. <em>SIMAX</em>, <em>43</em>(3), 1213–1237. (<a
href="https://doi.org/10.1137/20M1364011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we show how to construct diagonal scalings for arbitrary matrix pencils $\lambda B-A$, in which both $A$ and $B$ are complex matrices (square or nonsquare). The goal of such diagonal scalings is to “balance” in some sense the row and column norms of the pencil. We see that the problem of scaling a matrix pencil is equivalent to the problem of scaling the row and column sums of a particular nonnegative matrix. However, it is known that there exist square and nonsquare nonnegative matrices that cannot be scaled arbitrarily. To address this issue, we consider an approximate embedded problem, in which the corresponding nonnegative matrix is square and can always be scaled. The new scaling methods are then based on the Sinkhorn--Knopp algorithm for scaling a square nonnegative matrix with total support to be doubly stochastic or on a variant of it. In addition, using results of U. G. Rothblum and H. Schneider [Linear Algebra Appl., 114--115 (1989), pp. 737--764], we give simple sufficient conditions on the zero pattern for the existence of diagonal scalings of square nonnegative matrices to have any prescribed common vector for the row and column sums. We illustrate numerically that the new scaling techniques for pencils improve the accuracy of the computation of their eigenvalues.},
  archive      = {J_SIMAX},
  author       = {Froilán M. Dopico and María C. Quintana and Paul Van Dooren},
  doi          = {10.1137/20M1364011},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1213-1237},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Diagonal scalings for the eigenstructure of arbitrary pencils},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Matrix pencils with coefficients that have positive
semidefinite hermitian parts. <em>SIMAX</em>, <em>43</em>(3), 1186–1212.
(<a href="https://doi.org/10.1137/21M1439997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze when an arbitrary matrix pencil is strictly equivalent to a dissipative Hamiltonian pencil and show that this heavily restricts the spectral properties. In order to relax the spectral properties, we introduce matrix pencils with coefficients that have positive semidefinite Hermitian parts. We relate the Kronecker structure of these pencils to that of an underlying skew-Hermitian pencil and discuss their regularity, index, numerical range, and location of eigenvalues. Further, we study matrix polynomials with positive semidefinite Hermitian coefficients and use linearizations with positive semidefinite Hermitian parts to derive sufficient conditions for having a spectrum in the left half plane and to derive bounds on the index.},
  archive      = {J_SIMAX},
  author       = {C. Mehl and V. Mehrmann and M. Wojtylak},
  doi          = {10.1137/21M1439997},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1186-1212},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Matrix pencils with coefficients that have positive semidefinite hermitian parts},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved variants of the hutch++ algorithm for trace
estimation. <em>SIMAX</em>, <em>43</em>(3), 1162–1185. (<a
href="https://doi.org/10.1137/21M1447623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with two improved variants of the Hutch++ algorithm for estimating the trace of a square matrix, implicitly given through matrix-vector products. Hutch++ combines randomized low-rank approximation in a first phase with stochastic trace estimation in a second phase. In turn, Hutch++ only requires $O\left(\varepsilon^{-1}\right)$ matrix-vector products to approximate the trace within a relative error $\varepsilon$ with high probability, provided that the matrix is symmetric positive semidefinite. This compares favorably with the $O\left(\varepsilon^{-2}\right)$ matrix-vector products needed when using stochastic trace estimation alone. In Hutch++, the number of matrix-vector products is fixed a priori and distributed in a prescribed fashion among the two phases. In this work, we derive an adaptive variant of Hutch++, which outputs an estimate of the trace that is within some prescribed error tolerance with a controllable failure probability, while splitting the matrix-vector products in a near-optimal way among the two phases. For the special case of a symmetric positive semidefinite matrix, we present another variant of Hutch++, called NystrÃ¶m++, which utilizes the so-called NystrÃ¶m approximation and requires only one pass over the matrix, as compared to two passes with Hutch++. We extend the analysis of Hutch++ to NystrÃ¶m++. Numerical experiments demonstrate the effectiveness of our two new algorithms.},
  archive      = {J_SIMAX},
  author       = {David Persson and Alice Cortinovis and Daniel Kressner},
  doi          = {10.1137/21M1447623},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1162-1185},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Improved variants of the hutch++ algorithm for trace estimation},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Takagi factorization of matrices depending on parameters and
locating degeneracies of singular values. <em>SIMAX</em>,
<em>43</em>(3), 1148–1161. (<a
href="https://doi.org/10.1137/21M1456273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider the Takagi factorization of a matrix valued function depending on parameters. We give smoothness and genericity results and pay particular attention to the concerns caused by having either a singular value equal to 0 or multiple singular values. For these phenomena, we give theoretical results showing that their codimension is 2, and we further develop and test numerical methods to locate in parameter space values where these occurrences take place. Numerical study of the density of these occurrences is performed.},
  archive      = {J_SIMAX},
  author       = {Luca Dieci and Alessandra Papini and Alessandro Pugliese},
  doi          = {10.1137/21M1456273},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1148-1161},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Takagi factorization of matrices depending on parameters and locating degeneracies of singular values},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analytical singular value decomposition for a class of
stoichiometry matrices. <em>SIMAX</em>, <em>43</em>(3), 1109–1147. (<a
href="https://doi.org/10.1137/21M1418927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the analytical singular value decomposition of the stoichiometry matrix for a spatially discrete reaction-diffusion system. The motivation for this work is to develop a matrix decomposition that can reveal hidden spatial flux patterns of chemical reactions. We consider a 1D domain with two subregions sharing a single common boundary. Each of the subregions is further partitioned into a finite number of compartments. Chemical reactions can occur within a compartment, whereas diffusion is represented as movement between adjacent compartments. Inspired by biology, we study both (1) the case where the reactions on each side of the boundary are different and only certain species diffuse across the boundary and (2) the case where reactions and diffusion are spatially homogeneous. We write the stoichiometry matrix for these two classes of systems using a Kronecker product formulation. For the first scenario, we apply linear perturbation theory to derive an approximate singular value decomposition in the limit as diffusion becomes much faster than reactions. For the second scenario, we derive an exact analytical singular value decomposition for all relative diffusion and reaction time scales. By writing the stoichiometry matrix using Kronecker products, we show that the singular vectors and values can also be written concisely using Kronecker products. Ultimately, we find that the singular value decomposition of the reaction-diffusion stoichiometry matrix depends on the singular value decompositions of smaller matrices. These smaller matrices represent modified versions of the reaction-only stoichiometry matrices and the analytically known diffusion-only stoichiometry matrix. Lastly, we present the singular value decomposition of the model for the Calvin cycle in cyanobacteria and demonstrate the accuracy of our formulation. The MATLAB code, available at www.github.com/MathBioCU/ReacDiffStoicSVD, provides routines for efficiently calculating the SVD for a given reaction network on a 1D spatial domain.},
  archive      = {J_SIMAX},
  author       = {Jacqueline Wentz and Jeffrey C. Cameron and David M. Bortz},
  doi          = {10.1137/21M1418927},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1109-1147},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Analytical singular value decomposition for a class of stoichiometry matrices},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An algebraic quantum circuit compression algorithm for
hamiltonian simulation. <em>SIMAX</em>, <em>43</em>(3), 1084–1108. (<a
href="https://doi.org/10.1137/21M1439298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing is a promising technology that harnesses the peculiarities of quantum mechanics to deliver computational speedups for some problems that are intractable to solve on a classical computer. Current generation noisy intermediate-scale quantum (NISQ) computers are severely limited in terms of chip size and error rates. Shallow quantum circuits with uncomplicated topologies are essential for successful applications in the NISQ era. Based on matrix analysis, we derive localized circuit transformations to efficiently compress quantum circuits for simulation of certain spin Hamiltonians known as free fermions. The depth of the compressed circuits is independent of simulation time and grows linearly with the number of spins. The proposed numerical circuit compression algorithm behaves backward stable and scales cubically in the number of spins enabling circuit synthesis beyond $\mathcal{O}(10^3)$ spins. The resulting quantum circuits have a simple nearest-neighbor topology, which makes them ideally suited for NISQ devices.},
  archive      = {J_SIMAX},
  author       = {Daan Camps and Efekan Kökcü and Lindsay Bassman Oftelie and Wibe A. de Jong and Alexander F. Kemper and Roel Van Beeumen},
  doi          = {10.1137/21M1439298},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1084-1108},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An algebraic quantum circuit compression algorithm for hamiltonian simulation},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stable and efficient computation of generalized polar
decompositions. <em>SIMAX</em>, <em>43</em>(3), 1058–1083. (<a
href="https://doi.org/10.1137/21M1411986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present methods for computing the generalized polar decomposition of a matrix based on the dynamically weighted Halley iteration. This method is well established for computing the standard polar decomposition. A stable implementation is available, where matrix inversion is avoided and QR decompositions are used instead. We establish a natural generalization of this approach for computing generalized polar decompositions with respect to signature matrices. Again the inverse can be avoided by using a generalized QR decomposition called hyperbolic QR decomposition. However, this decomposition does not show the same favorable stability properties as its orthogonal counterpart. We overcome the numerical difficulties by generalizing the CholeskyQR2 method. This method computes the standard QR factorization in a stable way via two successive Cholesky factorizations. An even better numerical stability is achieved by employing permuted graph bases, yielding residuals of order $10^{-14}$ even for badly conditioned matrices, where other methods fail.},
  archive      = {J_SIMAX},
  author       = {Peter Benner and Yuji Nakatsukasa and Carolin Penke},
  doi          = {10.1137/21M1411986},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1058-1083},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Stable and efficient computation of generalized polar decompositions},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refinement of hottopixx method for nonnegative matrix
factorization under noisy separability. <em>SIMAX</em>, <em>43</em>(3),
1029–1057. (<a href="https://doi.org/10.1137/21M1442206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hottopixx, proposed by Bittorf et al. at NIPS 2012, is an algorithm for solving nonnegative matrix factorization (NMF) problems under the separability assumption. Separable NMFs have important applications, such as topic extraction from documents and unmixing of hyperspectral images. In such applications, the robustness of the algorithm to noise is the key to success. Hottopixx has been shown to be robust to noise, and its robustness can be further enhanced through postprocessing. However, there is a drawback. Hottopixx and its postprocessing require us to estimate the noise level involved in the matrix we want to factorize before running, since they use it as part of the input data. The noise-level estimation is not an easy task. In this paper, we overcome this drawback. We present a refinement of Hottopixx and its postprocessing that runs without prior knowledge of the noise level. We show that the refinement has almost the same robustness to noise as the original algorithm.},
  archive      = {J_SIMAX},
  author       = {Tomohiko Mizutani},
  doi          = {10.1137/21M1442206},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1029-1057},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Refinement of hottopixx method for nonnegative matrix factorization under noisy separability},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast deterministic approximation of symmetric indefinite
kernel matrices with high dimensional datasets. <em>SIMAX</em>,
<em>43</em>(2), 1003–1028. (<a
href="https://doi.org/10.1137/21M1424627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel methods are used frequently in various applications of machine learning. For large-scale high dimensional applications, the success of kernel methods hinges on the ability to operate certain large dense kernel matrix $K$. An enormous amount of literature has been devoted to the study of symmetric positive semidefinite (SPSD) kernels, where Nyström methods compute a low-rank approximation to the kernel matrix via choosing landmark points. In this paper, we study the Nyström method for approximating both symmetric indefinite kernel matrices as well SPSD ones. We first develop a theoretical framework for general symmetric kernel matrices, which provides a theoretical guidance for the selection of landmark points. We then leverage discrepancy theory to propose the anchor net method for computing accurate Nyström approximations with optimal complexity. The anchor net method operates entirely on the dataset without requiring the access to $K$ or its matrix-vector product. Results on various types of kernels (both indefinite and SPSD ones) and machine learning datasets demonstrate that the new method achieves better accuracy and stability with lower computational cost compared to the state-of-the-art Nyström methods.},
  archive      = {J_SIMAX},
  author       = {Difeng Cai and James Nagy and Yuanzhe Xi},
  doi          = {10.1137/21M1424627},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {1003-1028},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast deterministic approximation of symmetric indefinite kernel matrices with high dimensional datasets},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the change of the weyr characteristics of matrix pencils
after rank-one perturbations. <em>SIMAX</em>, <em>43</em>(2), 981–1002.
(<a href="https://doi.org/10.1137/21M1416497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The change of the Kronecker structure of a matrix pencil perturbed by another pencil of rank one has been characterized in terms of the homogeneous invariant factors and the chains of column and row minimal indices of the initial and the perturbed pencils. We obtain here a new characterization in terms of the homogeneous invariant factors and the conjugate partitions of the corresponding chains of column and row minimal indices of both pencils. We also define the generalized Weyr characteristic of an arbitrary matrix pencil and obtain bounds for the change of it when the pencil is perturbed by another pencil of rank one. The results improve known results on the problem and hold for arbitrary perturbation pencils of rank one and for any algebraically closed field.},
  archive      = {J_SIMAX},
  author       = {Itziar Baragan͂a and Alicia Roca},
  doi          = {10.1137/21M1416497},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {981-1002},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On the change of the weyr characteristics of matrix pencils after rank-one perturbations},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing the riemannian logarithm on the stiefel manifold:
Metrics, methods, and performance. <em>SIMAX</em>, <em>43</em>(2),
953–980. (<a href="https://doi.org/10.1137/21M1425426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of computing Riemannian normal coordinates on the real, compact Stiefel manifold of orthonormal frames. The Riemannian normal coordinates are based on the so-called Riemannian exponential and the associated Riemannian logarithm map and enable one to transfer almost any computational procedure to the realm of the Stiefel manifold. To compute the Riemannian logarithm is to solve the (local) geodesic endpoint problem. Instead of restricting the consideration to geodesics with respect to a single selected metric, we consider a family of Riemannian metrics introduced by Hüper, Markina, and Silva Leite that includes the Euclidean and the canonical metric as prominent examples. As main contributions, we provide (1) a unified, structured, reduced formula for the Stiefel geodesics. The formula is unified in the sense that it works for the full family of metrics under consideration. It is structured in the sense that it relies on matrix exponentials of skew-symmetric matrices exclusively. It is reduced in relation to the dimension of the matrices of which matrix exponentials have to be calculated. We provide (2) a unified method to tackle the geodesic endpoint problem numerically, and (3) we improve the existing Riemannian log algorithm under the canonical metric in terms of the computational efficiency. The findings are illustrated by means of numerical examples, where the novel algorithms prove to be the most efficient methods known to this date.},
  archive      = {J_SIMAX},
  author       = {Ralf Zimmermann and Knut Hüper},
  doi          = {10.1137/21M1425426},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {953-980},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computing the riemannian logarithm on the stiefel manifold: Metrics, methods, and performance},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sketching with kerdock’s crayons: Fast sparsifying
transforms for arbitrary linear maps. <em>SIMAX</em>, <em>43</em>(2),
939–952. (<a href="https://doi.org/10.1137/21M1438992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an arbitrary matrix $A\in\mathbb{R}^{m\times n}$, we consider the fundamental problem of computing $Ax$ for any $x\in\mathbb{R}^n$ such that $Ax$ is $s$-sparse. While fast algorithms exist for particular choices of $A$, such as the discrete Fourier transform, there are hardly any approaches that beat standard matrix-vector multiplication for realistic problem dimensions without such structural assumptions. In this paper, we devise a randomized approach to tackle the unstructured case. Our method relies on a representation of $A$ in terms of certain real-valued mutually unbiased bases derived from Kerdock sets. In the preprocessing phase of our algorithm, we compute this representation of $A$ in $O(mn^2\log n + n^2 \log^2n)$ operations. Next, given any unit vector $x\in\mathbb{R}^n$ such that $Ax$ is $s$-sparse, our randomized fast transform uses this representation of $A$ to compute the entrywise $\epsilon$-hard threshold of $Ax$ with high probability in only $O(sn + \epsilon^{-2}\|A\|_{2\to\infty}^2 (m+n)\log m)$ operations. In addition to a performance guarantee, we provide numerical results that demonstrate the plausibility of real-world implementation of our algorithm.},
  archive      = {J_SIMAX},
  author       = {Tim Fuchs and David Gross and Felix Krahmer and Richard Kueng and Dustin Mixon},
  doi          = {10.1137/21M1438992},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {939-952},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Sketching with kerdock&#39;s crayons: Fast sparsifying transforms for arbitrary linear maps},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). <span
class="math inline">(<em>L</em><sub><em>r</em></sub>, <em>L</em><sub><em>r</em></sub>, 1)</span>-decompositions,
sparse component analysis, and the blind separation of sums of
exponentials. <em>SIMAX</em>, <em>43</em>(2), 912–938. (<a
href="https://doi.org/10.1137/21M1426444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive new uniqueness results for $(L_r,L_r,1)$-type block-term decompositions of third-order tensors by drawing connections to sparse component analysis. It is shown that our uniqueness results have a natural application in the context of the blind source separation problem, since they ensure uniqueness even among $(L_r,L_r,1)$-decompositions with incomparable rank profiles, allowing for stronger separation results for signals consisting of sums of exponentials in the presence of common poles among the source signals. As a byproduct, this line of ideas also suggests a new approach for computing $(L_r,L_r,1)$-decompositions, which proceeds by sequentially computing a canonical polyadic decomposition of the input tensor, followed by performing a sparse factorization on the third factor matrix.},
  archive      = {J_SIMAX},
  author       = {Nithin Govindarajan and Ethan N. Epperly and Lieven De Lathauwer},
  doi          = {10.1137/21M1426444},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {912-938},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {$(L_r,L_r,1)$-decompositions, sparse component analysis, and the blind separation of sums of exponentials},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous diagonalization via congruence of hermitian
matrices: Some equivalent conditions and a numerical solution.
<em>SIMAX</em>, <em>43</em>(2), 882–911. (<a
href="https://doi.org/10.1137/21M1390657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at solving the Hermitian SDC problem, which is the simultaneous diagonalization via $*$-congruence of a finite collection of Hermitian matrices. The matrices do not need to pairwise commute. We first provide some equivalent conditions for such a matrix collection to be simultaneously diagonalizable via congruence. Interestingly, one of these conditions requires a positive definite solution to an appropriate system of linear equations over Hermitian matrices. Based on this theoretical result, we propose a polynomial-time algorithm for numerically solving the Hermitian SDC problem. The proposed algorithm is a combination of (1) a detection of whether the initial matrix collection is simultaneously diagonalizable via congruence by solving an appropriate semidefinite program and (2) an existing Jacobi-like algorithm for simultaneously diagonalizing (via congruence) the new collection of commuting Hermitian matrices derived from the previous stage. Illustrating examples and numerical tests with MATLAB are also presented. Consequently, the solvable condition for the SDC problem of arbitrarily square matrices will be obtained by dealing with their Hermitian and skew-Hermitian parts.},
  archive      = {J_SIMAX},
  author       = {Thanh Hieu Le and Thi Ngan Nguyen},
  doi          = {10.1137/21M1390657},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {882-911},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Simultaneous diagonalization via congruence of hermitian matrices: Some equivalent conditions and a numerical solution},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximal rank of <span
class="math inline"><em>m</em> × <em>n</em> × 2</span> tensors over
arbitrary fields. <em>SIMAX</em>, <em>43</em>(2), 867–881. (<a
href="https://doi.org/10.1137/21M1431424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we completely characterize the maximum rank of $m\times n\times 2$ tensors over an arbitrary field. We note that the maximal tensor rank may be different over different fields, and we find the maximum rank of a given $m\times n\times 2$ tensor is consistent over any field except $\mathbb{F}_2$. We discuss the field in two cases: one is in $\mathbb{F}_2$, and the other is in any field except $\mathbb{F}_2$.},
  archive      = {J_SIMAX},
  author       = {Xiaoyu Song and Baodong Zheng and Riguang Huang},
  doi          = {10.1137/21M1431424},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {867-881},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Maximal rank of ${m}\times{n}\times 2$ tensors over arbitrary fields},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New riemannian preconditioned algorithms for tensor
completion via polyadic decomposition. <em>SIMAX</em>, <em>43</em>(2),
840–866. (<a href="https://doi.org/10.1137/21M1394734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new Riemannian preconditioned algorithms for low-rank tensor completion via the polyadic decomposition of a tensor. These algorithms exploit a non-Euclidean metric on the product space of the factor matrices of the low-rank tensor in the polyadic decomposition form. This new metric is designed using an approximation of the diagonal blocks of the Hessian of the tensor completion cost function and thus has a preconditioning effect on these algorithms. We prove that the proposed Riemannian gradient descent algorithm globally converges to a stationary point of the tensor completion problem, with convergence rate estimates using the Łojasiewicz property. Numerical results on synthetic and real-world data suggest that the proposed algorithms are more efficient in memory and time compared to state-of-the-art algorithms. Moreover, the proposed algorithms display a greater tolerance for overestimated rank parameters in terms of the tensor recovery performance and thus enable a flexible choice of the rank parameter.},
  archive      = {J_SIMAX},
  author       = {Shuyu Dong and Bin Gao and Yu Guan and François Glineur},
  doi          = {10.1137/21M1394734},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {840-866},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {New riemannian preconditioned algorithms for tensor completion via polyadic decomposition},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solving the cubic regularization model by a nested
restarting lanczos method. <em>SIMAX</em>, <em>43</em>(2), 812–839. (<a
href="https://doi.org/10.1137/21M1436324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a variant of the classical trust-region method for unconstrained optimization, the cubic regularization of the Newton method introduces a cubic regularization term in the surrogate objective to adaptively adjust the updating step and deals with cases with both indefinite and definite Hessians. It has been demonstrated that the cubic regularization of the Newton method enjoys a good global convergence and is an efficient solver for the unconstrained minimization. The main computational cost in each iteration is to solve a cubic regularization subproblem. The Newton iteration is a common and efficient method for this task, especially for small- to medium-size problems. For large size problems, a Lanczos type method was proposed in [C. Cartis, N. I. M. Gould, and P. L. Toint, Math. Program., 127 (2011), pp. 245--295]. This method relies on a Lanczos procedure to reduce the large-scale cubic regularization subproblem to a small one and solve it by the Newton iteration. For large and ill-conditioned problems, the Lanczos method still needs to produce a large dimensional subspace to achieve a relatively highly accurate approximation, which declines its performance overall. In this paper, we first show that the cubic regularization subproblem can be equivalently transformed into a quadratic eigenvalue problem, which provides an eigensolver alternative to the Newton iteration. We then establish the convergence of the Lanczos method and also propose a nested restarting version for the large scale and ill-conditioned case. By integrating the nested restarting Lanczos iteration into the cubic regularization of the Newton method, we verify its efficiency for solving large scale minimization problems in CUTEst collection.},
  archive      = {J_SIMAX},
  author       = {Xiaojing Jia and Xin Liang and Chungen Shen and Lei-Hong Zhang},
  doi          = {10.1137/21M1436324},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {812-839},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Solving the cubic regularization model by a nested restarting lanczos method},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Error bounds for lanczos-based matrix function
approximation. <em>SIMAX</em>, <em>43</em>(2), 787–811. (<a
href="https://doi.org/10.1137/21M1427784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the Lanczos method for matrix function approximation (Lanczos-FA), an iterative algorithm for computing ( f(A) b) when (A) is a Hermitian matrix and (b) is a given vector. Assuming that ( f : \mathbbC \rightarrow \mathbbC) is piecewise analytic, we give a framework, based on the Cauchy integral formula, which can be used to derive a priori and a posteriori error bounds for Lanczos-FA in terms of the error of Lanczos used to solve linear systems. Unlike many error bounds for Lanczos-FA, these bounds account for fine-grained properties of the spectrum of (A), such as clustered or isolated eigenvalues. Our results are derived assuming exact arithmetic, but we show that they are easily extended to finite precision computations using existing theory about the Lanczos algorithm in finite precision. We also provide generalized bounds for the Lanczos method used to approximate quadratic forms (b^H f(A) b) and demonstrate the effectiveness of our bounds with numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Tyler Chen and Anne Greenbaum and Cameron Musco and Christopher Musco},
  doi          = {10.1137/21M1427784},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {787-811},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Error bounds for lanczos-based matrix function approximation},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linearizable eigenvector nonlinearities. <em>SIMAX</em>,
<em>43</em>(2), 764–786. (<a
href="https://doi.org/10.1137/21M142931X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to linearize, without approximation, a specific class of eigenvalue problems with eigenvector nonlinearities (NEPv), where the nonlinearities are expressed by scalar functions that are defined by a quotient of linear functions of the eigenvector. The exact linearization relies on an equivalent multiparameter eigenvalue problem (MEP) that contains the exact solutions of the NEPv. Due to the characterization of MEPs in terms of a generalized eigenvalue problem this provides a direct way to compute all NEPv solutions for small problems, and it opens up the possibility to develop locally convergent iterative methods for larger problems. Moreover, the linear formulation allows us to easily determine the number of solutions of the NEPv. We propose two numerical schemes that exploit the structure of the linearization: inverse iteration and residual inverse iteration. We show how symmetry in the MEP can be used to improve reliability and reduce computational cost of both methods. Two numerical examples verify the theoretical results, and a third example shows the potential of a hybrid scheme that is based on a combination of the two proposed methods.},
  archive      = {J_SIMAX},
  author       = {Rob Claes and Elias Jarlebring and Karl Meerbergen and Parikshit Upadhyaya},
  doi          = {10.1137/21M142931X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {764-786},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Linearizable eigenvector nonlinearities},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of structured distances to singularity for matrix
pencils with symmetry structures: A linear algebra–based approach.
<em>SIMAX</em>, <em>43</em>(2), 740–763. (<a
href="https://doi.org/10.1137/21M1423269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the structured distance to singularity for a given regular matrix pencil $A+sE$, where $(A,E)\in \mathbb S \subseteq (\mathbb C^{n,n})^2$. This includes Hermitian, skew-Hermitian, $*$-even, $*$-odd, $*$-palindromic, T-palindromic, and dissipative Hamiltonian pencils. We present a purely linear algebra-based approach to derive explicit computable formulas for the distance to the nearest structured pencil $(A-\Delta_A)+s(E-\Delta_E)$ such that $A-\Delta_A$ and $E-\Delta_E$ have a common null vector. We then obtain a family of computable lower bounds for the unstructured and structured distances to singularity. Numerical experiments suggest that in many cases, there is a significant difference between structured and unstructured distances. This approach extends to structured matrix polynomials with higher degrees.},
  archive      = {J_SIMAX},
  author       = {Anshul Prajapati and Punit Sharma},
  doi          = {10.1137/21M1423269},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {740-763},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Estimation of structured distances to singularity for matrix pencils with symmetry structures: A linear algebra--based approach},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A block minimum residual norm subspace solver with partial
convergence management for sequences of linear systems. <em>SIMAX</em>,
<em>43</em>(2), 710–739. (<a
href="https://doi.org/10.1137/21M1401127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are concerned with the iterative solution of linear systems with multiple right-hand sides available one group after another with possibly slowly varying left-hand sides. For such sequences of linear systems, we first develop a new block minimum norm residual approach that combines two main ingredients. The first component exploits ideas from GCRO-DR [Parks et al., SIAM J. Sci. Comput., 28 (2006), pp. 1651--1674], enabling us to recycle information from one solve to the next. The second component is the numerical mechanism for managing the partial convergence of the right-hand sides, referred to as inexact breakdown detection in IB-BGMRES [Robbé and Sadkane, Linear Algebra Appl., 419 (2006), pp. 265--285], that enables the monitoring of the rank deficiency in the residual space basis expanded blockwise. Next, for the class of block minimum norm residual approaches that relies on a block Arnoldi-like equality between the search space and the residual space (e.g., any block GMRES or block GCRO variants), we introduce new search space expansion policies defined on novel criteria to detect the partial convergence. These novel detection criteria are tuned to the selected stopping criterion and targeted convergence threshold to best cope with the selected normwise backward error stopping criterion, enabling us to monitor the computational effort while ensuring the final accuracy of each individual solution. Numerical experiments are reported to illustrate the numerical and computational features of both the new block Krylov solvers and the new search space block expansion polices.},
  archive      = {J_SIMAX},
  author       = {Luc Giraud and Yan-Fei Jing and Yanfei Xiang},
  doi          = {10.1137/21M1401127},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {710-739},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A block minimum residual norm subspace solver with partial convergence management for sequences of linear systems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deflation for the symmetric arrowhead and
diagonal-plus-rank-one eigenvalue problems. <em>SIMAX</em>,
<em>43</em>(2), 681–709. (<a
href="https://doi.org/10.1137/21M139205X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss the eigenproblem for the symmetric arrowhead matrix $C = (\begin{smallmatrix} D \&amp; {z} {z}^T &amp; \alpha \end{smallmatrix})$, where $D \in \mathbb{R}^{n \times n}$ is diagonal, ${z} \in \mathbb{R}^n$, and $\alpha \in \mathbb{R}$, in order to examine criteria for when components of ${z}$ may be set to zero. We show that whenever two eigenvalues of $C$ are sufficiently close, some component of ${z}$ may be deflated to zero, without significantly perturbing the eigenvalues of $C$, by either substituting zero for that component or performing a Givens rotation on each side of $C$. The strategy for this deflation requires ${\mathcal{O}(n^2)}$ comparisons. Although it is too costly for many applications, when we use it as a benchmark, we can analyze the effectiveness of ${{O}(n)}$ heuristics that are more practical approaches to deflation. We show that one such ${\mathcal{O}(n)}$ heuristic finds all sets of three or more nearby eigenvalues, misses sets of two or more nearby eigenvalues under limited circumstances, and produces a reduced matrix whose eigenvalues are distinct in double the working precision. Using the ${\mathcal{O}(n)}$ heuristic, we develop a more aggressive method for finding converged eigenvalues in the symmetric Lanczos algorithm. It is shown that except for pathological exceptions, the ${\mathcal{O}(n)}$ heuristic finds nearly as much deflation as the ${\mathcal{O}(n^2)}$ algorithm that reduces an arrowhead matrix to one that cannot be deflated further. The deflation algorithms and their analysis are extended to the symmetric diagonal-plus-rank-one eigenvalue problem and lead to a better deflation strategy for the LAPACK routine dstedc.f.},
  archive      = {J_SIMAX},
  author       = {Jesse L. Barlow and Stanley C. Eisenstat and Nevena Jakovčević Stor and Ivan Slapnicar},
  doi          = {10.1137/21M139205X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {681-709},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Deflation for the symmetric arrowhead and diagonal-plus-rank-one eigenvalue problems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A block bidiagonalization method for fixed-accuracy low-rank
matrix approximation. <em>SIMAX</em>, <em>43</em>(2), 661–680. (<a
href="https://doi.org/10.1137/21M1397866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present randUBV, a randomized algorithm for matrix sketching based on the block Lanzcos bidiagonalization process. Given a matrix $\mathbb{A}$, it produces a low-rank approximation of the form $\mathbb{UBV}^T$, where $\mathbb{U}$ and $\mathbb{V}$ have orthonormal columns in exact arithmetic and $\mathbb{B}$ is block bidiagonal. In finite precision, the columns of both $\mathbb{U}$ and $\mathbb{V}$ will be close to orthonormal. Our algorithm is closely related to the randQB algorithms of Yu, Gu, and Li [SIAM J. Matrix Anal. Appl., 39 (2018), pp. 1339--1359]. in that the entries of $\mathbb{B}$ are incrementally generated and the Frobenius norm approximation error may be efficiently estimated. It is therefore suitable for the fixed-accuracy problem and so is designed to terminate as soon as a user input error tolerance is reached. Numerical experiments suggest that the block Lanczos method is generally competitive with or superior to algorithms that use power iteration, even when $\mathbb{A}$ has significant clusters of singular values.},
  archive      = {J_SIMAX},
  author       = {Eric Hallman},
  doi          = {10.1137/21M1397866},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {661-680},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A block bidiagonalization method for fixed-accuracy low-rank matrix approximation},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed precision recursive block diagonalization for
bivariate functions of matrices. <em>SIMAX</em>, <em>43</em>(2),
638–660. (<a href="https://doi.org/10.1137/21M1407872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various numerical linear algebra problems can be formulated as evaluating bivariate function of matrices. The most notable examples are the Fréchet derivative along a direction, the evaluation of (univariate) functions of Kronecker-sum-structured matrices, and the solution of, Sylvester matrix equations. In this work, we propose a recursive block diagonalization algorithm for computing bivariate functions of matrices of small to medium size, for which dense linear algebra is appropriate. The algorithm combines a blocking strategy, as in the Schur--Parlett scheme, and an evaluation procedure for the diagonal blocks. We discuss two implementations of the latter. The first is a natural choice based on Taylor expansions, whereas the second is derivative-free and relies on a multiprecision perturb-and-diagonalize approach. In particular, the appropriate use of multiprecision guarantees backward stability without affecting the efficiency in the generic case. This makes the second approach more robust. The whole method has cubic complexity, and it is closely related to the well-known Bartels--Stewart algorithm for Sylvester matrix equations when applied to $f(x,y)=\frac{1}{x+y}$. We validate the performances of the proposed numerical method on several problems with different conditioning properties.},
  archive      = {J_SIMAX},
  author       = {Stefano Massei and Leonardo Robol},
  doi          = {10.1137/21M1407872},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {638-660},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Mixed precision recursive block diagonalization for bivariate functions of matrices},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantile-based iterative methods for corrupted systems of
linear equations. <em>SIMAX</em>, <em>43</em>(2), 605–637. (<a
href="https://doi.org/10.1137/21M1429187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often in applications ranging from medical imaging and sensor networks to error correction and data science (and beyond), one needs to solve large-scale linear systems in which a fraction of the measurements have been corrupted. We consider solving such large-scale systems of linear equations $Ax = b$ that are inconsistent due to corruptions in the measurement vector $b$. We develop several variants of iterative methods that converge to the solution of the uncorrupted system of equations, even in the presence of large corruptions. These methods make use of a quantile of the absolute values of the residual vector in determining the iterate update. We present both theoretical and empirical results that demonstrate the promise of these iterative approaches.},
  archive      = {J_SIMAX},
  author       = {Jamie Haddock and Deanna Needell and Elizaveta Rebrova and William Swartworth},
  doi          = {10.1137/21M1429187},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {605-637},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Quantile-based iterative methods for corrupted systems of linear equations},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theoretical and computable optimal subspace expansions for
matrix eigenvalue problems. <em>SIMAX</em>, <em>43</em>(2), 584–604. (<a
href="https://doi.org/10.1137/20M1331032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the optimal subspace expansion problem for the matrix eigenvalue problem $Ax=\lambda x$: Which vector $w$ in the current subspace $\mathcal{V}$, after multiplied by $A$, provides an optimal subspace expansion for approximating a desired eigenvector $x$ in the sense that $x$ has the smallest angle with the expanded subspace $\mathcal{V}_w=\mathcal{V}+{span}{Aw}$, i.e., $w_{opt}=\arg\max_{w\in\mathcal{V}}\cos\angle(\mathcal{V}_w,x)$? This problem is important as many iterative methods construct nested subspaces that successively expand $\mathcal{V}$ to $\mathcal{V}_w$. An expression of $w_{opt}$ by Ye [Linear Algebra Appl., 428 (2008), pp. 911--918] for $A$ general, but it could not be exploited to construct a computable (nearly) optimally expanded subspace. Ye turns to deriving a maximization characterization of $\cos\angle(\mathcal{V}_w,x)$ for a given $w\in \mathcal{V}$ when $A$ is Hermitian. We generalize Ye&#39;s maximization characterization to the general case and find its maximizer. Our main contributions consist of explicit expressions of $w_{opt}$, $(I-P_V)Aw_{opt}$ and the optimally expanded subspace $\mathcal{V}_{w_{opt}}$ for $A$ general, where $P_V$ is the orthogonal projector onto $\mathcal{V}$. These results are fully exploited to obtain computable optimally expanded subspaces within the framework of the standard, harmonic, refined, and refined harmonic Rayleigh--Ritz methods. We show how to efficiently implement the proposed subspace expansion approaches. Numerical experiments demonstrate the effectiveness of our computable optimal expansions.},
  archive      = {J_SIMAX},
  author       = {Zhongxiao Jia},
  doi          = {10.1137/20M1331032},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {584-604},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Theoretical and computable optimal subspace expansions for matrix eigenvalue problems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A boundary-layer preconditioner for singularly perturbed
convection diffusion. <em>SIMAX</em>, <em>43</em>(2), 561–583. (<a
href="https://doi.org/10.1137/21M1443297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a wide range of real-world problems whose solutions exhibit boundary and interior layers, the numerical analysis of discretizations of singularly perturbed differential equations is an established subdiscipline within the study of the numerical approximation of solutions to differential equations. Consequently, much is known about how to accurately and stably discretize such equations on a priori adapted meshes in order to properly resolve the layer structure present in their continuum solutions. However, despite being a key step in the numerical simulation process, much less is known about the efficient and accurate solution of the linear systems of equations corresponding to these discretizations. In this paper, we discuss problems associated with the application of direct solvers to these discretizations, and we propose a preconditioning strategy that is tuned to the matrix structure induced by using layer-adapted meshes for convection-diffusion equations, proving a strong condition-number bound on the preconditioned system in one spatial dimension and a weaker bound in two spatial dimensions. Numerical results confirm the efficiency of the resulting preconditioners in one and two dimensions, with time-to-solution of less than one second for representative problems on 1024 x 1024 meshes and up to 40x speedup over standard sparse direct solvers.},
  archive      = {J_SIMAX},
  author       = {Scott P. MacLachlan and Niall Madden and Thái Anh Nhan},
  doi          = {10.1137/21M1443297},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {561-583},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A boundary-layer preconditioner for singularly perturbed convection diffusion},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Geometric inexact newton method for generalized singular
values of grassmann matrix pair. <em>SIMAX</em>, <em>43</em>(2),
535–560. (<a href="https://doi.org/10.1137/20M1383720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first give new model formulations for computing arbitrary generalized singular value of a Grassmann matrix pair or a real matrix pair. In these new formulations, we need to solve matrix optimization problems with unitary constraints or orthogonal constraints. We propose a geometric inexact Newton--conjugate gradient (Newton-CG) method for solving the resulting matrix optimization problems. Under some mild assumptions, we establish the global and quadratic convergence of the proposed method for the complex case. Some numerical examples are given to illustrate the effectiveness and high accuracy of the proposed method.},
  archive      = {J_SIMAX},
  author       = {Wei-Wei Xu and Michael K. Ng and Zheng-Jian Bai},
  doi          = {10.1137/20M1383720},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {535-560},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Geometric inexact newton method for generalized singular values of grassmann matrix pair},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new analytical framework for the convergence of inexact
two-grid methods. <em>SIMAX</em>, <em>43</em>(1), 512–533. (<a
href="https://doi.org/10.1137/21M140448X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-grid methods with exact solution of the Galerkin coarse-grid system have been well studied by the multigrid community: an elegant identity has been established to characterize the convergence factor of exact two-grid methods. In practice, however, it is often too costly to solve the Galerkin coarse-grid system exactly, especially when its size is large. Instead, without essential loss of convergence speed, one may solve the coarse-grid system approximately. In this paper, we develop a new framework for analyzing the convergence of inexact two-grid methods: two-sided bounds for the energy norm of the error propagation matrix of inexact two-grid methods are presented. In the framework, a restricted smoother involved in the identity for exact two-grid convergence is used to measure how far the actual coarse-grid matrix deviates from the Galerkin one. As an application, we establish a new and unified convergence theory for multigrid methods.},
  archive      = {J_SIMAX},
  author       = {Xuefeng Xu and Chen-Song Zhang},
  doi          = {10.1137/21M140448X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {512-533},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A new analytical framework for the convergence of inexact two-grid methods},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-infinite linear regression and its applications.
<em>SIMAX</em>, <em>43</em>(1), 479–511. (<a
href="https://doi.org/10.1137/21M1411950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite linear least squares is one of the core problems of numerical linear algebra, with countless applications across science and engineering. Consequently, there is a rich and ongoing literature on algorithms for solving linear least squares problems. In this paper, we explore a variant in which the system&#39;s matrix has one infinite dimension (i.e., it is a quasimatrix). We call such problems semi-infinite linear regression problems. As we show, the semi-infinite case arises in several applications, such as supervised learning and function approximation, and allows for novel interpretations of existing algorithms. We explore semi-infinite linear regression rigorously and algorithmically. To that end, we give a formal framework for working with quasimatrices, and generalize several algorithms designed for the finite problem to the infinite case. Finally, we suggest the use of various sampling methods for obtaining an approximate solution.},
  archive      = {J_SIMAX},
  author       = {Paz Fink Shustin and Haim Avron},
  doi          = {10.1137/21M1411950},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {479-511},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Semi-infinite linear regression and its applications},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A measure concentration effect for matrices of high, higher,
and even higher dimension. <em>SIMAX</em>, <em>43</em>(1), 464–478. (<a
href="https://doi.org/10.1137/20M1376029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let n&gt;m, and let $A$ be an (m x n)-matrix of full rank. Then obviously the estimate $\|Ax\|\leq\|A\|\|x\|$ holds for the euclidean norm of $x$ and $Ax$ and the spectral norm as the assigned matrix norm. We study the sets of all $x$ for which, for fixed $\delta&lt;1$, conversely $\|Ax\|\geq\delta\,\|A\|\|x\|$ holds. It turns out that these sets fill, in the high-dimensional case, almost the complete space once $\delta$ falls below a bound that depends on the extremal singular values of $A$ and on the ratio of the dimensions. This effect has much to do with the random projection theorem, which plays an important role in the data sciences. As a byproduct, we calculate the probabilities this theorem deals with exactly.},
  archive      = {J_SIMAX},
  author       = {Harry Yserentant},
  doi          = {10.1137/20M1376029},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {464-478},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A measure concentration effect for matrices of high, higher, and even higher dimension},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exactly solving sparse rational linear systems via
roundoff-error-free cholesky factorizations. <em>SIMAX</em>,
<em>43</em>(1), 439–463. (<a
href="https://doi.org/10.1137/20M1371592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exactly solving sparse symmetric positive definite (SPD) linear systems is a key problem in mathematics, engineering, and computer science. This paper derives two new sparse roundoff-error-free (REF) Cholesky factorization algorithms which exactly solve sparse SPD linear systems $A {x} = {b}$, where $A \in \mathbb{Q}^{n x n}$ and ${x}, {b} \in {Q}^{n x p}$. The key properties of these factorizations are that (1) they exclusively use integer-arithmetic and (2) in the bit-complexity model, they solve the linear system $A {x} = {b}$ in time proportional to the cost of the integer-arithmetic operations. Namely, the overhead related to data structures and ancillary operations (those not strictly required to perform the factorization) is subsumed by the cost of the integer-arithmetic operations that are essential/intrinsic to the factorization. Notably, to date our algorithms are the only exact algorithm for solving SPD linear systems with this asymptotically efficient complexity bound. Computationally, we show that the novel factorizations are faster than both sparse rational-arithmetic LDL and sparse exact LU factorization. Altogether, the derived sparse REF Cholesky factorizations present a framework to solve any rational SPD linear system exactly and efficiently.},
  archive      = {J_SIMAX},
  author       = {Christopher J. Lourenco and Erick Moreno-Centeno},
  doi          = {10.1137/20M1371592},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {439-463},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Exactly solving sparse rational linear systems via roundoff-error-free cholesky factorizations},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A symbol-based analysis for multigrid methods for
block-circulant and block-toeplitz systems. <em>SIMAX</em>,
<em>43</em>(1), 405–438. (<a
href="https://doi.org/10.1137/21M1390554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature, there exist several studies on symbol-based multigrid methods for the solution of linear systems having structured coefficient matrices. In particular, the convergence analysis for such methods has been obtained in an elegant form in the case of Toeplitz matrices generated by a scalar-valued function. In the block-Toeplitz setting, that is, in the case where the matrix entries are small generic matrices instead of scalars, some algorithms have already been proposed regarding specific applications, and a first rigorous convergence analysis has been performed in [M. Donatelli et al., Numer. Linear Algebra Appl., 28 (2021), e2356]. However, with the existent symbol-based theoretical tools, it is still not possible to prove the convergence of many multigrid methods known in the literature. This paper aims to generalize the previous results, giving more general sufficient conditions on the symbol of the grid transfer operators. In particular, we treat matrix-valued trigonometric polynomials which can be nondiagonalizable and singular at all points, and we express the new conditions in terms of the eigenvectors associated with the ill-conditioned subspace. Moreover, we extend the analysis to the V-cycle method, proving a linear convergence rate under stronger conditions, which resemble those given in the scalar case. In order to validate our theoretical findings, we present a classical block structured problem stemming from an FEM approximation of a second order differential problem. We focus on two multigrid strategies that use the geometric and the standard bisection grid transfer operators and prove that both fall into the category of projectors satisfying the proposed conditions. In addition, using a tensor product argument, we provide a strategy to construct efficient V-cycle procedures in the block multilevel setting.},
  archive      = {J_SIMAX},
  author       = {Matthias Bolten and Marco Donatelli and Paola Ferrari and Isabella Furci},
  doi          = {10.1137/21M1390554},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {405-438},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A symbol-based analysis for multigrid methods for block-circulant and block-toeplitz systems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Singularly perturbed markov modulated fluid queues.
<em>SIMAX</em>, <em>43</em>(1), 377–404. (<a
href="https://doi.org/10.1137/21M1395387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a Markov modulated fluid queue for which the environment is nearly completely decomposable. Under the basic assumption that both the nearly completely decomposable Markov modulated fluid model and the unperturbed fluid models are positive recurrent, we show that the stationary density of the level can be expanded as convergent power series of the aggregated stationary densities. We go further in the analysis by assuming that one or more of the unperturbed fluid queues is not necessarily positive recurrent. We provide numerical illustration.},
  archive      = {J_SIMAX},
  author       = {Sarah Dendievel and Guy Latouche and Yuanyuan Liu and Yingchun Tang},
  doi          = {10.1137/21M1395387},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {377-404},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Singularly perturbed markov modulated fluid queues},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new lower bound on the size of the smallest vertex
separator of a graph. <em>SIMAX</em>, <em>43</em>(1), 370–376. (<a
href="https://doi.org/10.1137/20M1382118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separator minimization is an important problem in graph partitioning. Although finding an optimum partitioning for a given graph is NP-hard, estimating the size of the smallest vertex separator is an interesting problem since it can be used to assess the quality of a vertex separator. In [A. Pothen, H. Simon, and K. Liou, SIAM J. Matrix Anal. Appl., 11 (1990), pp. 430--452], two classical lower bounds on the size of the smallest vertex separator of a graph were established. In the present work, we revisit this problem and establish a new and easily computable lower bound on the smallest vertex separator.},
  archive      = {J_SIMAX},
  author       = {Yongyan Guo and Gang Wu},
  doi          = {10.1137/20M1382118},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {370-376},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A new lower bound on the size of the smallest vertex separator of a graph},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Guarantees for existence of a best canonical polyadic
approximation of a noisy low-rank tensor. <em>SIMAX</em>,
<em>43</em>(1), 328–369. (<a
href="https://doi.org/10.1137/20M1381046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical polyadic decomposition (CPD) of a low-rank tensor plays a major role in data analysis and signal processing by allowing for unique recovery of underlying factors. However, it is well known that the low-rank CPD approximation problem is ill-posed. That is, a tensor may fail to have a best rank $R$ CPD approximation when $R&gt;1$. This article gives deterministic bounds for the existence of best low-rank tensor approximations over ${\mathbb{K}}={\mathbb{R}}$ or ${\mathbb{K}}={\mathbb{C}}$. More precisely, given a tensor ${\mathcal T} \in {\mathbb{K}}^{I \times I \times I}$ of rank $R \leq I$, we compute the radius of a Frobenius norm ball centered at ${\mathcal T}$ in which best ${\mathbb{K}}$-rank $R$ approximations are guaranteed to exist. In addition we show that every ${\mathbb{K}}$-rank $R$ tensor inside of this ball has a unique canonical polyadic decomposition. This neighborhood may be interpreted as a neighborhood of “mathematical truth&quot; in which CPD approximation and computation are well-posed. In pursuit of these bounds, we describe low-rank tensor decomposition as a “joint generalized eigenvalue&quot; problem. Using this framework, we show that, under mild assumptions, a low-rank tensor which has rank strictly greater than border rank is defective in the sense of algebraic and geometric multiplicities for joint generalized eigenvalues. Bounds for existence of best low-rank approximations are then obtained by establishing perturbation theoretic results for the joint generalized eigenvalue problem. In this way we establish a connection between existence of best low-rank approximations and the tensor spectral norm. In addition we solve a “tensor Procrustes problem&quot; which examines orthogonal compressions for pairs of tensors. The main results of the article are illustrated by a variety of numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Eric Evert and Lieven De Lathauwer},
  doi          = {10.1137/20M1381046},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {328-369},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Guarantees for existence of a best canonical polyadic approximation of a noisy low-rank tensor},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sharp estimation of convergence rate for self-consistent
field iteration to solve eigenvector-dependent nonlinear eigenvalue
problems. <em>SIMAX</em>, <em>43</em>(1), 301–327. (<a
href="https://doi.org/10.1137/20M136606X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive convergence analysis for the self-consistent field (SCF) iteration to solve a class of nonlinear eigenvalue problems with eigenvector dependency (NEPvs). Using the tangent-angle matrix as an intermediate measure for approximation error, we establish new formulas for two fundamental quantities that characterize the local convergence behavior of the plain SCF: the local contraction factor and the local asymptotic average contraction factor. In comparison with previously established results, new convergence rate estimates provide much sharper bounds on the convergence speed. As an application, we extend the convergence analysis to a popular SCF variant---the level-shifted SCF. The effectiveness of the convergence rate estimates is demonstrated numerically for NEPvs arising from solving the Kohn--Sham equation in electronic structure calculation and the Gross--Pitaevskii equation for modeling of the Bose--Einstein condensation.},
  archive      = {J_SIMAX},
  author       = {Zhaojun Bai and Ren-Cang Li and Ding Lu},
  doi          = {10.1137/20M136606X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {301-327},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Sharp estimation of convergence rate for self-consistent field iteration to solve eigenvector-dependent nonlinear eigenvalue problems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A recursive eigenspace computation for the canonical
polyadic decomposition. <em>SIMAX</em>, <em>43</em>(1), 274–300. (<a
href="https://doi.org/10.1137/21M1423026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The canonical polyadic decomposition (CPD) is a compact decomposition which expresses a tensor as a sum of its rank-1 components. A common step in the computation of a CPD is computing a generalized eigenvalue decomposition (GEVD) of the tensor. A GEVD provides an algebraic approximation of the CPD which can then be used as an initialization in optimization routines. While in the noiseless setting GEVD exactly recovers the CPD, it has recently been shown that pencil-based computations such as GEVD are not stable. In this article we present an algebraic method for approximation of a CPD which greatly improves the accuracy of GEVD. Our method is still fundamentally pencil based; however, rather than using a single pencil and computing all of its generalized eigenvectors, we use many different pencils and in each pencil compute generalized eigenspaces corresponding to sufficiently well-separated generalized eigenvalues. The resulting “generalized eigenspace decomposition&quot; is significantly more robust to noise than the classical GEVD. Accuracy of the generalized eigenspace decomposition is examined both empirically and theoretically. In particular, we provide a deterministic perturbation theoretic bound which is predictive of error in the computed factorization.},
  archive      = {J_SIMAX},
  author       = {Eric Evert and Michiel Vandecappelle and Lieven De Lathauwer},
  doi          = {10.1137/21M1423026},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {274-300},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A recursive eigenspace computation for the canonical polyadic decomposition},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A sampling algorithm to compute the set of feasible
solutions for NonNegative matrix factorization with an arbitrary rank.
<em>SIMAX</em>, <em>43</em>(1), 257–273. (<a
href="https://doi.org/10.1137/20M1378971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) is a useful method to extract features from multivariate data, but an important and sometimes neglected concern is that NMF can result in nonunique solutions. Often, there exist a set of feasible solutions (SFS), which makes it more difficult to interpret the factorization. This problem is especially ignored in cancer genomics, where NMF is used to infer information about the mutational processes present in the evolution of cancer. In this paper the extent of nonuniqueness is investigated for two mutational counts data, and a new sampling algorithm that can find the SFS is introduced. Our sampling algorithm is easy to implement and applies to an arbitrary rank of NMF. This is in contrast to state of the art, where the NMF rank must be smaller than or equal to four. For lower ranks we show that our algorithm performs similar to the polygon inflation algorithm that is developed in relation to chemometrics. Furthermore, we show how the size of the SFS can have a high influence on the appearing variability of a solution. Our sampling algorithm is implemented in the R package SFS (https://github.com/ragnhildlaursen/SFS).},
  archive      = {J_SIMAX},
  author       = {Ragnhild Laursen and Asger Hobolth},
  doi          = {10.1137/20M1378971},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {257-273},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A sampling algorithm to compute the set of feasible solutions for NonNegative matrix factorization with an arbitrary rank},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Arbitrary precision algorithms for computing the matrix
cosine and its fréchet derivative. <em>SIMAX</em>, <em>43</em>(1),
233–256. (<a href="https://doi.org/10.1137/21M1441043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing algorithms for computing the matrix cosine are tightly coupled to a specific precision of floating-point arithmetic for optimal efficiency so they do not conveniently extend to an arbitrary precision environment. We develop an algorithm for computing the matrix cosine that takes the unit roundoff of the working precision as input, and so works in an arbitrary precision. The algorithm employs a Taylor approximation with scaling and recovering and it can be used with a Schur decomposition or in a decomposition-free manner. We also derive a framework for computing the Fréchet derivative, construct an efficient evaluation scheme for computing the cosine and its Fréchet derivative simultaneously in arbitrary precision, and show how this scheme can be extended to compute the matrix sine, cosine, and their Fréchet derivatives all together. Numerical experiments show that the new algorithms behave in a forward stable way over a wide range of precisions. The transformation-free version of the algorithm for computing the cosine is competitive in accuracy with the state-of-the-art algorithms in double precision and surpasses existing alternatives in both speed and accuracy in working precisions higher than double.},
  archive      = {J_SIMAX},
  author       = {Awad H. Al-Mohy and Nicholas J. Higham and Xiaobo Liu},
  doi          = {10.1137/21M1441043},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {233-256},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Arbitrary precision algorithms for computing the matrix cosine and its fréchet derivative},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiply accelerated value iteration for NonSymmetric affine
fixed point problems and application to markov decision processes.
<em>SIMAX</em>, <em>43</em>(1), 199–232. (<a
href="https://doi.org/10.1137/20M1367192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a modified version of the Nesterov accelerated gradient algorithm, which applies to affine fixed point problems with non-self-adjoint matrices, such as the ones appearing in the theory of Markov decision processes with discounted or mean payoff criteria. We characterize the spectra of matrices for which this algorithm does converge with an accelerated asymptotic rate. We also introduce a $d$th-order algorithm and show that it yields a multiply accelerated rate under more demanding conditions on the spectrum. We subsequently apply these methods to develop accelerated schemes for nonlinear fixed point problems arising from Markov decision processes. This is illustrated by numerical experiments.},
  archive      = {J_SIMAX},
  author       = {Marianne Akian and Stéphane Gaubert and Zheng Qu and Omar Saadi},
  doi          = {10.1137/20M1367192},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {199-232},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Multiply accelerated value iteration for NonSymmetric affine fixed point problems and application to markov decision processes},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast approximation of the <span
class="math inline"><em>p</em></span>-radius, matrix pressure, or
generalized lyapunov exponent for positive and dominated matrices.
<em>SIMAX</em>, <em>43</em>(1), 178–198. (<a
href="https://doi.org/10.1137/19M1303964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If $A_1,\ldots,A_N$ are real $d \times d$ matrices, then the $p$-radius, generalized Lyapunov exponent, or matrix pressure is defined to be the asymptotic exponential growth rate of the sum $\sum_{i_1,\ldots,i_n=1}^N \|A_{i_n}\cdots A_{i_1}\|^p$, where $p$ is a real parameter. Under its various names this quantity has been investigated for its applications to topics including wavelet regularity and refinement equations, fractal geometry, and the large deviations theory of random matrix products. In this article we present a new algorithm for computing the $p$-radius under the hypothesis that the matrices are all positive (or more generally under the hypothesis that they satisfy a weaker condition called domination) and of very low dimension. This algorithm is based on interpreting the $p$-radius as the leading eigenvalue of a trace-class operator on a Hilbert space and estimating that eigenvalue via approximations to the Fredholm determinant of the operator. In this respect our method is closely related to the work of Z.-Q. Bai and M. Pollicott on computing the top Lyapunov exponent of a random matrix product. For pairs of positive matrices of dimension two our method yields substantial improvements over existing methods.},
  archive      = {J_SIMAX},
  author       = {Ian D. Morris},
  doi          = {10.1137/19M1303964},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {178-198},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast approximation of the $p$-radius, matrix pressure, or generalized lyapunov exponent for positive and dominated matrices},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Divide-and-conquer methods for functions of matrices with
banded or hierarchical low-rank structure. <em>SIMAX</em>,
<em>43</em>(1), 151–177. (<a
href="https://doi.org/10.1137/21M1432594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with approximating matrix functions for banded matrices, hierarchically semiseparable matrices, and related structures. We develop a new divide-and-conquer method based on (rational) Krylov subspace methods for performing low-rank updates of matrix functions. Our convergence analysis of the newly proposed method proceeds by establishing relations to best polynomial and rational approximation. When only the trace or the diagonal of the matrix function is of interest, we demonstrate---in practice and in theory---that convergence can be faster. For the special case of a banded matrix, we show that the divide-and-conquer method reduces to a much simpler algorithm, which proceeds by computing matrix functions of small submatrices. Numerical experiments confirm the effectiveness of the newly developed algorithms for computing large-scale matrix functions from a wide variety of applications.},
  archive      = {J_SIMAX},
  author       = {Alice Cortinovis and Daniel Kressner and Stefano Massei},
  doi          = {10.1137/21M1432594},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {151-177},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Divide-and-conquer methods for functions of matrices with banded or hierarchical low-rank structure},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multidimensional total least squares problem with linear
equality constraints. <em>SIMAX</em>, <em>43</em>(1), 124–150. (<a
href="https://doi.org/10.1137/21M1400420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many recent data analysis models are mathematically characterized by a multidimensional total least squares problem with linear equality constraints (TLSE). In this paper, an explicit solution is firstly derived for the multidimensional TLSE problem, as well as the solvability conditions. With applying the perturbation theory of invariant subspace, the multidimensional TLSE problem is proved equivalent to a multidimensional unconstrained weighed total least squares problem in the limit sense. The Kronecker product-based formulae are also given for the normwise, mixed, and componentwise condition numbers of the multidimensional TLSE solution of minimum Frobenius norm, and their computable upper bounds are also provided to reduce the storage and computational cost. All these results are appropriate for the single right-hand-side case and the multidimensional total least squares problem, which are two especial cases of the multidimensional TLSE problem. In numerical experiments, the multidimensional TLSE model is successfully applied to color image deblurring and denoising for the first time, and the numerical results also indicate the effectiveness of the condition numbers.},
  archive      = {J_SIMAX},
  author       = {Qiaohua Liu and Zhigang Jia and Yimin Wei},
  doi          = {10.1137/21M1400420},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {124-150},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Multidimensional total least squares problem with linear equality constraints},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical orthogonal factorization: Sparse square
matrices. <em>SIMAX</em>, <em>43</em>(1), 94–123. (<a
href="https://doi.org/10.1137/20M1373475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop a new fast algorithm, spaQR---sparsified QR---for solving large, sparse linear systems. The key to our approach lies in using low-rank approximations to sparsify the separators in a nested dissection based Householder QR factorization. First, a modified version of nested dissection is used to identify vertex separators and reorder the matrix. Then, classical Householder QR is used to factorize the separators, going from the leaves to the top of the elimination tree. After every level of separator factorization, we sparsify all the remaining separators by using low-rank approximations. This operation reduces the size of the separators without introducing any fill-in in the matrix. However, it introduces a small approximation error which can be controlled by the user. The resulting approximate factorization is stored as a sequence of sparse orthogonal and sparse upper-triangular factors. Hence, it can be applied efficiently to solve linear systems. We further improve the algorithm by using a block diagonal scaling. Then, we show a systematic analysis of the approximation error and effectiveness of the algorithm in solving linear systems. Finally, we perform numerical tests on benchmark unsymmetric problems to evaluate the performance of the algorithm. The factorization time scales as $\mathcal{O}(N \log N)$ and the solve time scales as $\mathcal{O}(N)$.},
  archive      = {J_SIMAX},
  author       = {Abeynaya Gnanasekaran and Eric Darve},
  doi          = {10.1137/20M1373475},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {94-123},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Hierarchical orthogonal factorization: Sparse square matrices},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Twice is enough for dangerous eigenvalues. <em>SIMAX</em>,
<em>43</em>(1), 68–93. (<a
href="https://doi.org/10.1137/20M1385330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the stability of a class of eigensolvers that target interior eigenvalues with rational filters. We show that subspace iteration with a rational filter is robust even when an eigenvalue is near a filter&#39;s pole. These dangerous eigenvalues contribute to large round-off errors in the first iteration but are self-correcting in later iterations. For matrices with orthogonal eigenvectors (e.g., real-symmetric or complex Hermitian), two iterations are enough to reduce round-off errors to the order of the unit round-off. In contrast, Krylov methods accelerated by rational filters with fixed poles typically fail to converge to unit round-off accuracy when an eigenvalue is close to a pole. In the context of Arnoldi with shift-and-invert enhancement, we demonstrate a simple restart strategy that recovers full precision in the target eigenpairs.},
  archive      = {J_SIMAX},
  author       = {Andrew Horning and Yuji Nakatsukasa},
  doi          = {10.1137/20M1385330},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {68-93},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Twice is enough for dangerous eigenvalues},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximate generalized inverses with iterative refinement
for <span class="math inline"><em>ϵ</em></span>-accurate preconditioning
of singular systems. <em>SIMAX</em>, <em>43</em>(1), 40–67. (<a
href="https://doi.org/10.1137/20M1364126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of preconditioners to enable flexible GMRES to find a least-squares solution, and potentially the pseudoinverse solution, of large-scale sparse, asymmetric, singular, and potentially inconsistent systems. We develop the preconditioners based on a new observation that generalized inverses (i.e., ${A}^{g}\in{{G}\mid{A}{G}{A}={A}}$) enable the preconditioned Krylov subspaces to converge in a single step. We then compute an approximate generalized inverse (AGI) efficiently using a hybrid incomplete factorization (HIF), which combines multilevel incomplete LU with rank-revealing QR on its final Schur complement. We define the criteria of $\epsilon$-accuracy and stability of AGI to guarantee the convergence of preconditioned GMRES for consistent systems. For inconsistent systems, we fortify HIF with iterative refinement to obtain HIFIR, which allows accurate computations of the null-space vectors. By combining the two techniques, we then obtain a new solver, called PIPIT, for obtaining the pseudoinverse solutions for systems with low-dimensional null spaces. We demonstrate the robustness of HIF and HIFIR and show that they improve both accuracy and efficiency of the prior state of the art by orders of magnitude for systems with up to a million unknowns.},
  archive      = {J_SIMAX},
  author       = {Xiangmin Jiao and Qiao Chen},
  doi          = {10.1137/20M1364126},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {40-67},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Approximate generalized inverses with iterative refinement for $\epsilon$-accurate preconditioning of singular systems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Input-tailored system-theoretic model order reduction for
quadratic-bilinear systems. <em>SIMAX</em>, <em>43</em>(1), 1–39. (<a
href="https://doi.org/10.1137/18M1216699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we suggest a moment matching method for quadratic-bilinear dynamical systems. Most system-theoretic reduction methods for nonlinear systems rely on multivariate frequency representations. Our approach instead uses univariate frequency representations tailored toward user-predefined families of inputs. Then moment matching corresponds to a one-dimensional interpolation problem, not to multidimensional interpolation as for the multivariate approaches, i.e., it also involves fewer interpolation frequencies to be chosen. Compared to former contributions toward nonlinear model reduction with univariate frequency representations, our approach shows profound differences: Our derivation is more rigorous and general and reveals additional tensor-structured approximation conditions, which should be incorporated. Moreover, the proposed implementation exploits the inherent low-rank tensor structure, which enhances its efficiency. In addition, our approach allows for the incorporation of more general input relations in the state equations---not only affine-linear ones as in existing system-theoretic methods---in an elegant way. As a byproduct of the latter, also a novel modification for the multivariate methods falls off, which is able to handle more general input-relations.},
  archive      = {J_SIMAX},
  author       = {Björn Liljegren-Sailer and Nicole Marheineke},
  doi          = {10.1137/18M1216699},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {1-39},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Input-tailored system-theoretic model order reduction for quadratic-bilinear systems},
  volume       = {43},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
