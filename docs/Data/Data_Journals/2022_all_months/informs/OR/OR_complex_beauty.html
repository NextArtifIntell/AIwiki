<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or---191">OR - 191</h2>
<ul>
<li><details>
<summary>
(2022). How endogenization of the reference point affects loss
aversion: A study of portfolio selection. <em>OR</em>, <em>70</em>(6),
iii–vii. (<a href="https://doi.org/10.1287/opre.2022.2309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the implications of various models of partially endogenous reference point formation on optimal decision making in the context of portfolio optimization under loss aversion. Specifically, we first consider the partially endogenous model of De Giorgi and Post [ Management Science (2011) 57(6):1094–1110], where the reference point is determined in equilibrium but contains an exogenous component. We find that optimal trading behavior is as if the reference point were completely exogenous and that allowing for a mental adjustment of the reference point solely manifests itself in a lower degree of loss aversion. We then propose two novel models of partially endogenous reference point formation: A model of a reference point determined by optimal expectations and a model of mental reference point updating. Our conclusions on the effect of a partially endogenized reference point on portfolio selection under loss aversion are also confirmed under these two models. These findings suggest that it is difficult to separately identify an agent’s degree of loss aversion and his or her reference point and may help to explain why experienced and sophisticated agents appear to be less loss averse than expected in some field settings. Our models also predict that displays of loss aversion are decreasing in the duration an agent is given to contemplate a decision. Funding: This work was supported by Research Grants Council of Hong Kong [Grant 14205718], Southern University of Science and Technology (Start-up fund), and National Natural Science Foundation of China [Grant 72050410356]. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2022.2309 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2309},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {How endogenization of the reference point affects loss aversion: A study of portfolio selection},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Procurement mechanisms with post-auction pre-award
cost-reduction investigations. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A buyer seeking to outsource production may be able to find ways to reduce a potential supplier’s cost, for example, by suggesting improvements to the supplier’s proposed production methods. We study how a buyer could use such cost-reduction investigations by proposing a three-step supplier selection mechanism: First, each of several potential suppliers submits a price bid for a contract. Second, for each potential supplier, the buyer can exert an effort to see if the buyer can identify how the supplier could reduce their cost to perform the contract; the understanding is that if savings are found, they are passed on to the buyer if the supplier is awarded the contract. Third, the buyer awards the contract to whichever supplier has the lowest updated bid (the supplier’s initial bid price minus any cost reduction the buyer is able to identify for that supplier). For this proposed process, we characterize how the buyer’s decision on which suppliers for which to investigate cost reductions in step 2 is affected by the aggressiveness of the suppliers’ bids in step 1. We show that, even if the buyer does not share the cost savings the buyer identifies in step 2, ex ante symmetric suppliers are actually better off (ex ante) in our proposed mechanism than in a setting without such cost-reduction investigations, resulting in a win–win for the buyer and suppliers. When suppliers’ cost and cost-reduction distributions become very heterogeneous, the win–win situation may no longer hold, but every supplier still has an incentive to allow the buyer to investigate them in step 2 because it increases their chance of winning the contract. Using an optimal mechanism analysis, our numerical studies show that our proposed bid–investigate–award mechanism helps the buyer achieve near-optimal performance despite its simplicity. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2349 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2349},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Procurement mechanisms with post-auction pre-award cost-reduction investigations},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous patrolling games. <em>OR</em>, <em>70</em>(6),
iii–vii. (<a href="https://doi.org/10.1287/opre.2022.2346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a patrolling game played on a network Q , considered as a metric space. The Attacker chooses a point of Q (not necessarily a node) to attack during a chosen time interval of fixed duration. The Patroller chooses a unit speed path on Q and intercepts the attack (and wins) if she visits the attacked point during the attack-time interval. This zero-sum game models the problem of protecting roads or pipelines from an adversarial attack. The payoff to the maximizing Patroller is the probability that the attack is intercepted. Our results include the following: (i) a solution to the game for any network Q , as long as the time required to carry out the attack is sufficiently short; (ii) a solution to the game for all tree networks that satisfy a certain condition on their extremities; and (iii) a solution to the game for any attack duration for stars with one long arc and the remaining arcs equal in length. We present a conjecture on the solution of the game for arbitrary trees and establish it in certain cases. Funding: Financial support from the National Science Foundation [Grant CMMI-1935826] is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2346 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2346},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Continuous patrolling games},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Building a location-based set of social media users.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many instances, one may want to gain situational awareness in an environment by monitoring the content of local social media users. Often the challenge is how to build a set of users from a target location. Here, we introduce a method for building such a set of users by using an expand–classify approach, which begins with a small set of seed users from the target location and then iteratively collects their neighbors and classifies their locations. We perform this classification using maximum likelihood estimation on a factor graph model that incorporates features of the user profiles and social network connections. We show that maximum likelihood estimation reduces to solving a minimum cut problem on an appropriately defined graph. We are able to obtain several thousand users within a few hours for many diverse locations using our approach. Using geolocated data, we find that our approach typically achieves good accuracy for population centers with fewer than 500,000 inhabitants but is less effective for larger cities. We also find that our approach is able to collect many more users with higher accuracy than existing search methods. Finally, we show that, by studying the content of location-specific users obtained with our approach, we can identify the onset of significant social unrest in locations such as the Philippines. Funding: This work was supported in part by Charles Stark Draper Laboratory, Inc. (Draper). Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2357 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2357},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Building a location-based set of social media users},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dark matter in (volatility and) equity option risk premiums.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emphasizing the statistics of jumps crossing the strike and local time, we develop a decomposition of equity option risk premiums. Operationalizing this theoretical treatment, we equip the pricing kernel process with unspanned risks, embed (unspanned) jump risks, and allow equity return volatility to contain unspanned risks. Unspanned risks are consistent with negative risk premiums for jumps crossing the strike and local time and imply negative risk premiums for out-of-the-money call options and straddles. The empirical evidence from weekly and farther-dated index options is supportive of our theory of economically relevant unspanned risks and reveals “dark matter” in option risk premiums. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2360 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2360},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Dark matter in (Volatility and) equity option risk premiums},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advertising cycling to manage exclusivity loss in fashion
styles. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper uses dynamic optimization to study the optimal advertising of fashion products over time. For fashion products, brand advertising and exclusivity are important sales drivers. Therefore, we propose a dynamic model of the sales of multiple styles of a fashion brand based on these variables. The model is estimated using a particle filter method on data from two fashion categories (handbags and sunglasses) and has good fit and prediction. We also derive explicit analytical solutions of the optimal, closed-loop advertising control and use it to explain advertising and fashion cycles. A managerial prescription is that advertising cycling should be out of phase with sales, for example, trend down when fashion sales is trending up. Thus, advertising flattens the fashion sales cycle over time rather than reinforcing it.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2376},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Advertising cycling to manage exclusivity loss in fashion styles},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative collaborative filtering for sparse matrix
estimation. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider sparse matrix estimation where the goal is to estimate an n -by- n matrix from noisy observations of a small subset of its entries. We analyze the estimation error of the popularly used collaborative filtering algorithm for the sparse regime. Specifically, we propose a novel iterative variant of the algorithm, adapted to handle the setting of sparse observations. We establish that as long as the number of entries observed at random scale logarithmically larger than linear in n , the estimation error with respect to the entry-wise max norm decays to zero as n goes to infinity, assuming the underlying matrix of interest has constant rank r . Our result is robust to model misspecification in that if the underlying matrix is approximately rank r , then the estimation error decays to the approximation error with respect to the max -norm. In the process, we establish the algorithm’s ability to handle arbitrary bounded noise in the observations. Funding: This work was supported in part by Microsoft Research New England; the National Science Foundation’s Division of Computing and Communication Foundations [Grant 1948256], Division of Computer and Network Systems [Grant 1955997], and Division of Civil, Mechanical and Manufacturing Innovation [Grants 1462158 and 1634259]; and the Directorate for Computer and Information Science and Engineering [TRIPODS Phase I Project]. C. L. Yu is also supported by an Intel Rising Stars Award.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2193},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Iterative collaborative filtering for sparse matrix estimation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High-dimensional learning under approximate sparsity with
applications to nonsmooth estimation and regularized neural networks.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional statistical learning (HDSL) has wide applications in data analysis, operations research, and decision making. Despite the availability of multiple theoretical frameworks, most existing HDSL schemes stipulate the following two conditions: (a) the sparsity and (b) restricted strong convexity (RSC). This paper generalizes both conditions via the use of the folded concave penalty (FCP). More specifically, we consider an M-estimation problem where (i) (conventional) sparsity is relaxed into the approximate sparsity and (ii) RSC is completely absent. We show that the FCP-based regularization leads to poly-logarithmic sample complexity; the training data size is only required to be poly-logarithmic in the problem dimensionality. This finding can facilitate the analysis of two important classes of models that are currently less understood: high-dimensional nonsmooth learning and (deep) neural networks (NNs). For both problems, we show that poly-logarithmic sample complexity can be maintained. In particular, our results indicate that the generalizability of NNs under overparameterization can be theoretically ensured with the aid of regularization. Funding: This work was supported by the National Science Foundation [Grant 2016571] and a University of Florida AI Catalyst Grant. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2217 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2217},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {High-dimensional learning under approximate sparsity with applications to nonsmooth estimation and regularized neural networks},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothness-adaptive contextual bandits. <em>OR</em>,
<em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a nonparametric multiarmed bandit problem with stochastic covariates, where a key complexity driver is the smoothness of payoff functions with respect to covariates. Previous studies have focused on deriving minimax-optimal algorithms in cases where it is a priori known how smooth the payoff functions are. In practice, however, the smoothness of payoff functions is typically not known in advance, and misspecification of smoothness may severely deteriorate the performance of existing methods. In this work, we consider a framework where the smoothness of payoff functions is not known and study when and how algorithms may adapt to unknown smoothness. First, we establish that designing algorithms that adapt to unknown smoothness of payoff functions is, in general, impossible. However, under a self-similarity condition (which does not reduce the minimax complexity of the dynamic optimization problem at hand), we establish that adapting to unknown smoothness is possible and further devise a general policy for achieving smoothness-adaptive performance. Our policy infers the smoothness of payoffs throughout the decision-making process while leveraging the structure of off-the-shelf nonadaptive policies. We establish that for problem settings with either differentiable or nondifferentiable payoff functions, this policy matches (up to a logarithmic scale) the regret rate that is achievable when the smoothness of payoffs is known a priori. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2215 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2215},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Smoothness-adaptive contextual bandits},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data science for motion and time analysis with modern motion
sensor data. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of motion and time has become significant in operations research, especially for analyzing work performance in manufacturing and service operations in the development of lean manufacturing and smart factory. This paper develops a framework for data-driven analysis of work motions and studies their correlations to work speeds or execution rates, using data collected from modern motion sensors. Past efforts primarily relied on manual steps involving time-consuming stop-watching, videotaping, and manual data analysis. Whereas modern sensing devices have automated motion data collection, the motion analytics that transform the new data into knowledge are largely underdeveloped. Unsolved technical questions include: How can the motion and time information be extracted from the motion sensor data? How are work motions and execution rates statistically modeled and compared? How are the motions correlated to the rates? This paper develops a novel mathematical framework for motion and time analysis using motion sensor data by defining new mathematical representation spaces of human motions and execution rates and developing statistical tools on these new spaces. The paper demonstrates this comprehensive methodology using five use cases applied to manufacturing motion data. Funding: Support from the Brain Pool Fellowship of the National Research Foundation Korea [Grant 2019H1D3A2A01100649] is gratefully acknowledged. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2216 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2216},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Data science for motion and time analysis with modern motion sensor data},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonasymptotic analysis of monte carlo tree search.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider the popular tree-based search strategy within the framework of reinforcement learning, the Monte Carlo tree search (MCTS), in the context of the infinite-horizon discounted cost Markov decision process (MDP). Although MCTS is believed to provide an approximate value function for a given state with enough simulations, the claimed proof of this property is incomplete. This is because the variant of MCTS, the upper confidence bound for trees (UCT), analyzed in prior works, uses “logarithmic” bonus term for balancing exploration and exploitation within the tree-based search, following the insights from stochastic multiarm bandit (MAB) literature. In effect, such an approach assumes that the regret of the underlying recursively dependent nonstationary MABs concentrates around their mean exponentially in the number of steps, which is unlikely to hold, even for stationary MABs. As the key contribution of this work, we establish polynomial concentration property of regret for a class of nonstationary MABs. This in turn establishes that the MCTS with appropriate polynomial rather than logarithmic bonus term in UCB has a claimed property. Interestingly enough, empirically successful approaches use a similar polynomial form of MCTS as suggested by our result. Using this as a building block, we argue that MCTS, combined with nearest neighbor supervised learning, acts as a “policy improvement” operator; that is, it iteratively improves value function approximation for all states because of combining with supervised learning, despite evaluating at only finitely many states. In effect, we establish that to learn an ε approximation of the value function with respect to ℓ ∞ norm, MCTS combined with nearest neighbor requires a sample size scaling as O ˜ ( ε − ( d + 4 ) ) , where d is the dimension of the state space. This is nearly optimal because of a minimax lower bound of Ω ˜ ( ε − ( d + 2 ) ) , suggesting the strength of the variant of MCTS we propose here and our resulting analysis. Funding: This work was supported by the National Science Foundation [Grant CNS-1955997 and TRIPODS Phase II Grant] and MIT-IBM project on “Representation Learning as a Tool for causal Discovery,” Siemens Futuremakers Fellowship.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2239},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Nonasymptotic analysis of monte carlo tree search},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smooth contextual bandits: Bridging the parametric and
nondifferentiable regret regimes. <em>OR</em>, <em>70</em>(6), iii–vii.
(<a href="https://doi.org/10.1287/opre.2021.2237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a nonparametric contextual bandit problem in which the expected reward functions belong to a Hölder class with smoothness parameter β . We show how this interpolates between two extremes that were previously studied in isolation: nondifferentiable bandits (β at most 1), with which rate-optimal regret is achieved by running separate noncontextual bandits in different context regions, and parametric-response bandits (infinite β ), with which rate-optimal regret can be achieved with minimal or no exploration because of infinite extrapolatability. We develop a novel algorithm that carefully adjusts to all smoothness settings, and we prove its regret is rate-optimal by establishing matching upper and lower bounds, recovering the existing results at the two extremes. In this sense, our work bridges the gap between the existing literature on parametric and nondifferentiable contextual bandit problems and between bandit algorithms that exclusively use global or local information, shedding light on the crucial interplay of complexity and regret in contextual bandits. Funding: N. Kallus acknowledges support from the National Science Foundation [Grant 1846210]. X. Mao acknowledges support from the National Science Foundation of China [Grant 72201150, Grant 72293561]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2237 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2237},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Smooth contextual bandits: Bridging the parametric and nondifferentiable regret regimes},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficiently breaking the curse of horizon in off-policy
evaluation with double reinforcement learning. <em>OR</em>,
<em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-policy evaluation (OPE) in reinforcement learning is notoriously difficult in long- and infinite-horizon settings due to diminishing overlap between behavior and target policies. In this paper, we study the role of Markovian and time-invariant structure in efficient OPE. We first derive the efficiency bounds and efficient influence functions for OPE when one assumes each of these structures. This precisely characterizes the curse of horizon: in time-variant processes, OPE is only feasible in the near-on-policy setting, where behavior and target policies are sufficiently similar. But, in time-invariant Markov decision processes, our bounds show that truly off-policy evaluation is feasible, even with only just one dependent trajectory, and provide the limits of how well we could hope to do. We develop a new estimator based on double reinforcement learning (DRL) that leverages this structure for OPE. Our DRL estimator simultaneously uses estimated stationary density ratios and q -functions and remains efficient when both are estimated at slow, nonparametric rates and remains consistent when either is estimated consistently. We investigate these properties and the performance benefits of leveraging the problem structure for more efficient OPE. Funding: This work was supported by the National Science Foundation Division of Information and Intelligent Systems [1846210], and by theMasason Foundation. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.2249 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2249},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A classifier to decide on the linearization of mixed-integer
quadratic problems in CPLEX. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the aim of fully embedding learned predictions in the algorithmic design of a mixed-integer quadratic programming (MIQP) solver, we translate the algorithmic question of whether to linearize convex MIQPs into a classification task and use machine learning (ML) techniques to tackle it. We represent MIQPs and the linearization decision by careful target and feature engineering. Computational experiments and evaluation metrics are designed to further incorporate the optimization knowledge in the learning pipeline. As a practical result, a classifier deciding on MIQP linearization is successfully deployed in CPLEX 12.10.0: to the best of our knowledge, we establish the first example of an end-to-end integration of ML into a commercial optimization solver and ultimately contribute a general-purpose methodology for combining ML-based decisions and mixed-integer programming technology.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2267},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {A classifier to decide on the linearization of mixed-integer quadratic problems in CPLEX},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed-projection conic optimization: A new paradigm for
modeling rank constraints. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for modeling and solving low-rank optimization problems to certifiable optimality. We introduce symmetric projection matrices that satisfy Y 2 = Y , the matrix analog of binary variables that satisfy z 2 = z , to model rank constraints. By leveraging regularization and strong duality, we prove that this modeling paradigm yields convex optimization problems over the nonconvex set of orthogonal projection matrices. Furthermore, we design outer-approximation algorithms to solve low-rank problems to certifiable optimality, compute lower bounds via their semidefinite relaxations, and provide near optimal solutions through rounding and local search techniques. We implement these numerical ingredients and, to our knowledge, for the first time solve low-rank optimization problems to certifiable optimality. Our algorithms also supply certifiably near-optimal solutions for larger problem sizes and outperform existing heuristics by deriving an alternative to the popular nuclear norm relaxation. Using currently available spatial branch-and-bound codes, not tailored to projection matrices, we can scale our exact (respectively, near-exact) algorithms to matrices with up to 30 (600) rows/columns. All in all, our framework, which we name mixed-projection conic optimization , solves low-rank problems to certifiable optimality in a tractable and unified fashion. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2182 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2182},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Mixed-projection conic optimization: A new paradigm for modeling rank constraints},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When service times depend on customers’ delays: A
relationship between two models of dependence. <em>OR</em>,
<em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As empirically observed in restaurants, call centers, and intensive care units, service times needed by customers are often related to the delay they experience in queue. Two forms of dependence mechanisms in service systems with customer abandonment immediately come to mind: First, the service requirement of a customer may evolve while waiting in queue, in which case the service time of each customer is endogenously determined by the system’s dynamics. Second, customers may arrive ( exogenously ) to the system with a service and patience time that are stochastically dependent, so that the service-time distribution of the customers that end up in service is different than that of the entire customer population. We refer to the former type of dependence as endogenous and to the latter as exogenous . Because either dependence mechanism can have significant impacts on a system’s performance, it should be identified and taken into consideration for performance-evaluation and decision-making purposes. However, identifying the source of dependence from observed data is hard because both the service times and patience times are censored due to customer abandonment. Further, even if the dependence is known to be exogenous, there remains the difficult problem of fitting a joint service-patience times distribution to the censored data. We address these two problems and provide a solution to the corresponding statistical challenges by proving that both problems can be avoided. We show that, for any exogenous dependence, there exists a corresponding endogenous dependence, such that the queuing dynamics under either dependence have the same law. We also prove that there exist endogenous dependencies for which no equivalent exogenous dependence exists. Therefore, the endogenous dependence can be considered as a generalization of the exogenous dependence. As a result, if dependence is observed in data, one can always consider the system as having an endogenous dependence, regardless of the true underlying dependence mechanism. Because estimating the structure of an endogenous dependence is substantially easier than estimating a joint service-patience distribution from censored data, our approach facilitates statistical estimations considerably. Funding: C. A. Wu received financial support from the Hong Kong Research Grant Council [Early Career Scheme, Project 26206419]. A. Bassamboo and O. Perry received partial financial support from the National Science Foundation [Grant CMMI 2006350].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2179},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {When service times depend on customers’ delays: A relationship between two models of dependence},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotically optimal control of a centralized dynamic
matching market with general utilities. <em>OR</em>, <em>70</em>(6),
iii–vii. (<a href="https://doi.org/10.1287/opre.2021.2186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a matching market where buyers and sellers arrive according to independent Poisson processes at the same rate and independently abandon the market if not matched after an exponential amount of time with the same mean. In this centralized market, the utility for the system manager from matching any buyer and any seller is a general random variable. We consider a sequence of systems indexed by n where the arrivals in the n th system are sped up by a factor of n . We analyze two families of one-parameter policies: the population threshold policy immediately matches an arriving agent to its best available mate only if the number of mates in the system is above a threshold, and the utility threshold policy matches an arriving agent to its best available mate only if the corresponding utility is above a threshold. Using an asymptotic fluid analysis of the two-dimensional Markov process of buyers and sellers, we show that when the matching utility distribution is light-tailed, the population threshold policy with threshold n ln n is asymptotically optimal among all policies that make matches only at agent arrival epochs. In the heavy-tailed case, we characterize the optimal threshold level for both policies. We also study the utility threshold policy in an unbalanced matching market with heavy-tailed matching utilities and find that the buyers and sellers have the same asymptotically optimal utility threshold. To illustrate our theoretical results, we use extreme value theory to derive optimal thresholds when the matching utility distribution is exponential, uniform, Pareto, and correlated Pareto. In general, we find that as the right tail of the matching utility distribution gets heavier, the threshold level of each policy (and hence market thickness) increases, as does the magnitude by which the utility threshold policy outperforms the population threshold policy. Funding: J. H. Blanchet received financial support from the U.S. National Science Foundation [Grants 1915967, 1820942, and 1838576]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2186 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2186},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal control of a centralized dynamic matching market with general utilities},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weighted scoring rules and convex risk measures.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes a new relationship between proper scoring rules and convex risk measures. Specifically, we demonstrate that the entropy function associated with any weighted scoring rule is equal to the maximum value of an optimization problem where an investor maximizes a concave certainty equivalent (the negation of a convex risk measure). Using this connection, we construct two classes of proper weighted scoring rules with associated entropy functions based on ϕ -divergences. These rules are generalizations of the weighted power and weighted pseudospherical rules.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2190},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Weighted scoring rules and convex risk measures},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A probabilistic approach to growth networks. <em>OR</em>,
<em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Widely used closed product-form networks have emerged recently as a primary model of stochastic growth of subcellular structures, for example, cellular filaments. The baseline bio-molecular model is equivalent to a single-class closed queueing network, consisting of single-server and infinite-server queues. Although this model admits a seemingly tractable product-form solution, explicit analytical characterization of its partition function is difficult due to the large-scale nature of bio-molecular networks. To this end, we develop a novel methodology, based on a probabilistic representation of product-form solutions and large-deviations concentration inequalities, which identifies distinct operating regimes and yields explicit expressions for the marginal distributions of queue lengths. The parameters of the derived distributions can be computed from equations involving large-deviations rate functions, often admitting closed-form algebraic expressions. From a methodological perspective, a fundamental feature of our approach is that it provides exact results for order-one probabilities, even though our analysis involves large-deviations rate functions, which characterize only vanishing probabilities on a logarithmic scale. Supplemental Material: The e-companion is available at https://doi.org/10.1287.opre.2021.2195 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2195},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {A probabilistic approach to growth networks},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scheduling parallel-task jobs subject to packing and
placement constraints. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by modern parallel computing applications, we consider the problem of scheduling parallel-task jobs with heterogeneous resource requirements in a cluster of machines. Each job consists of a set of tasks that can be processed in parallel; however, the job is considered completed only when all its tasks finish their processing, which we refer to as the synchronization constraint. Furthermore, assignment of tasks to machines is subject to placement constraints, that is, each task can be processed only on a subset of machines, and processing times can also be machine dependent. Once a task is scheduled on a machine, it requires a certain amount of resource from that machine for the duration of its processing. A machine can process ( pack ) multiple tasks at the same time; however, the cumulative resource requirement of the tasks should not exceed the machine’s capacity. Our objective is to minimize the weighted average of the jobs’ completion times. The problem, subject to synchronization, packing, and placement constraints, is NP-hard, and prior theoretical results only concern much simpler models. For the case that migration of tasks among the placement-feasible machines is allowed, we propose a preemptive algorithm with an approximation ratio of ( 6 + ϵ ) . In the special case that only one machine can process each task, we design an algorithm with an improved approximation ratio of four. Finally, in the case that migrations (and preemptions) are not allowed, we design an algorithm with an approximation ratio of 24. Our algorithms use a combination of linear program relaxation and greedy packing techniques. We present extensive simulation results, using a real traffic trace, that demonstrate that our algorithms yield significant gains over the prior approaches. Funding: This work was supported by the National Science Foundation [Grants CNS-1652115 and CNS-1717867]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.2198 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2198},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Scheduling parallel-task jobs subject to packing and placement constraints},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Naive learning through probability overmatching.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze boundedly rational updating in a repeated interaction network model with binary actions and binary states. Agents form beliefs according to discretized DeGroot updating and apply a decision rule that assigns a (mixed) action to each belief. We first show that under weak assumptions, random decision rules are sufficient to achieve agreement in finite time in any strongly connected network. Our main result establishes that naive learning can be achieved in any large strongly connected network. That is, if beliefs satisfy a high level of inertia, then there exist corresponding decision rules coinciding with probability overmatching such that the eventual agreement action matches the true state, with a probability converging to one as the network size goes to infinity. Funding: I. Arieli acknowledges support from the Ministry of Science and Technology [Grant 19400214] and the Israel Science Foundation [Grant 2029464]. Y. Babichenko acknowledges support from the Israel Science Foundation [Grant 2021296]. M. Mueller-Frank acknowledges the financial support of the Spanish Ministry of Science, Innovation, and Universities [Grant ECO2015-63711-P] and of the Department of the Economy and Knowledge of the Generalitat de Catalunya [Grant 2017 SGR 1244].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2202},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Naive learning through probability overmatching},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimistic gittins indices. <em>OR</em>, <em>70</em>(6),
iii–vii. (<a href="https://doi.org/10.1287/opre.2021.2207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen a resurgence of interest in Bayesian algorithms for the multiarmed bandit (MAB) problem, such as Thompson sampling. These algorithms seek to exploit prior information on arm biases. The empirically observed performance of these algorithms makes them a compelling alternative to their frequentist counterparts. Nonetheless, there appears to be a wide range in empirical performance among such Bayesian algorithms. These algorithms also vary substantially in their design (as opposed to being variations on a theme). In contrast, if one cared about Bayesian regret discounted over an infinite horizon at a fixed, prespecified rate, the celebrated Gittins index theorem offers an optimal algorithm. Unfortunately, the Gittins analysis does not appear to carry over to minimizing Bayesian regret over all sufficiently large horizons and computing a Gittins index is onerous relative to essentially any incumbent index scheme for the Bayesian MAB problem. The present paper proposes a tightening sequence of optimistic approximations to the Gittins index. We show that the use of these approximations in concert with the use of an increasing discount factor appears to offer a compelling alternative to state-of-the-art index schemes proposed for the Bayesian MAB problem in recent years. We prove that these optimistic indices constitute a regret optimal algorithm, in the sense of meeting the Lai-Robbins lower bound, including matching constants. Perhaps more interestingly, the use of even the loosest of these approximations appears to offer substantial performance improvements over state-of-the-art alternatives (including Thompson sampling, information direct sampling, and the Bayes UCB algorithm) while incurring little to no additional computational overhead relative to the simplest of these alternatives. Funding: Both authors were partially supported by NSG Grant CMMI 1727239.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2207},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Optimistic gittins indices},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian optimization allowing for common random numbers.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimization is a powerful tool for expensive stochastic black-box optimization problems, such as simulation-based optimization or machine learning hyperparameter tuning. Many stochastic objective functions implicitly require a random number seed as input. By explicitly reusing a seed, a user can exploit common random numbers, comparing two or more inputs under the same randomly generated scenario, such as a common customer stream in a job shop problem or the same random partition of training data into training and validation sets for a machine learning algorithm. With the aim of finding an input with the best average performance over infinitely many seeds, we propose a novel Gaussian process model that jointly models both the output for each seed and the average over seeds. We then introduce the knowledge gradient for common random numbers that iteratively determines a combination of input and a random seed to evaluate the objective and automatically trades off reusing old seeds and querying new seeds, thus overcoming the need to evaluate inputs in batches or measure differences of pairs as suggested in previous methods. We investigate the knowledge gradient for common random numbers both theoretically and empirically, finding that it achieves significant performance improvements with only moderate added computational cost. Funding: The first author gratefully acknowledges funding through the UK Engineering and Physical Sciences Research Council [Grant EP/101358X/1]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2208 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2208},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Bayesian optimization allowing for common random numbers},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Plausible screening using functional properties for
simulations with large solution spaces. <em>OR</em>, <em>70</em>(6),
iii–vii. (<a href="https://doi.org/10.1287/opre.2021.2206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When working with models that allow for many candidate solutions, simulation practitioners can benefit from screening out unacceptable solutions in a statistically controlled way. However, for large solution spaces, estimating the performance of all solutions through simulation can prove impractical. We propose a statistical framework for screening solutions even when only a relatively small subset of them is simulated. Our framework derives its superiority over exhaustive screening approaches by leveraging available properties of the function that describes the performance of solutions. The framework is designed to work with a wide variety of available functional information and provides guarantees on both the confidence and consistency of the resulting screening inference. We provide explicit formulations for the properties of convexity and Lipschitz continuity and show through numerical examples that our procedures can efficiently screen out many unacceptable solutions. Funding: This work was supported by the National Science Foundation [Grants DMS-1854562, DMS-1953111]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2206 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2206},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Plausible screening using functional properties for simulations with large solution spaces},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning manipulation through information dissemination.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study optimal manipulation of a Bayesian learner through adaptive provisioning of information. The problem is motivated by settings in which a firm can disseminate possibly biased information at a cost, to influence the public’s belief about a hidden parameter related to the firm’s payoffs. For example, firms advertise to sell products. We study a sequential optimization model in which the firm dynamically decides on the quantity and content of information sent to the public, aiming to maximize its expected total discounted profits over an infinite horizon. We solve the associated Bayesian dynamic programming equation and explicitly characterize the optimal manipulation policy in closed form. The explicit solution allows us to further characterize the evolution of the public’s posterior belief under such manipulation over time. We also extend our analysis to consider the public as partially Bayesian social learners who rely on public reviews to resist manipulation. We show that the public asymptotically learns the truth in this extended setting. Funding: M. J. Kim is supported in part by the Natural Sciences and Engineering Research Council [Discovery Grant RGPIN-2015-04019]. J. Keppo is supported in part by the Institute of Operations Research and Analytics at the National University of Singapore [Grant R-726-000-009-646]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2209 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2209},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Learning manipulation through information dissemination},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shortfall risk models when information on loss function is
incomplete. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utility-based shortfall risk (SR) measure effectively captures a decision maker’s risk attitude on tail losses by an increasing convex loss function. In this paper, we consider a situation where the decision maker’s risk attitude toward tail losses is ambiguous and introduce a robust version of SR, which mitigates the risk arising from such ambiguity. Specifically, we use some available partial information or subjective judgement to construct a set of utility-based loss functions and define a so-called preference robust shortfall risk (PRSR) through the worst loss function from the (ambiguity) set. We then apply the PRSR to optimal decision-making problems and demonstrate how the latter can be reformulated as tractable convex programs when the underlying exogenous uncertainty is discretely distributed. Funding: This research was partially supported by the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2016-05208]; the Canada Research Chairs [Grant 950-230057]; the Research Grants Council Hong Kong [Grant 14500620]; and the National Natural Science Foundation of China [Grant 11801057].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2212},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Shortfall risk models when information on loss function is incomplete},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Actor-critic–like stochastic adaptive search for continuous
simulation optimization. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a random search method for solving a class of simulation optimization problems with Lipschitz continuity properties. The algorithm samples candidate solutions from a parameterized probability distribution over the solution space and estimates the performance of the sampled points through an asynchronous learning procedure based on the so-called shrinking ball method. A distinctive feature of the algorithm is that it fully retains the previous simulation information and incorporates an approximation architecture to exploit knowledge of the objective function in searching for improved solutions. Each step of the algorithm involves simultaneous adaptation of a parameterized distribution and an approximator of the objective function, which is akin to the actor-critic structure used in reinforcement learning. We establish a finite-time probability bound on the algorithm’s performance and show its global convergence when only a single simulation observation is collected at each iteration. Empirical results indicate that the algorithm is promising and may outperform some of the existing procedures in terms of efficiency and reliability. Funding: This work was supported by the National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1634627]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2214 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2214},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Actor-Critic–Like stochastic adaptive search for continuous simulation optimization},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constant regret resolving heuristics for price-based revenue
management. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Price-based revenue management is an important problem in operations management with many practical applications. The problem considers a seller who sells one or multiple products over T consecutive periods and is subject to constraints on the initial inventory levels of resources. Whereas, in theory, the optimal pricing policy could be obtained via dynamic programming, computing the exact dynamic programming solution is often intractable. Approximate policies, such as the resolving heuristics, are often applied as computationally tractable alternatives. In this paper, we show the following two results for price-based network revenue management under a continuous price set. First, we prove that a natural resolving heuristic attains O (1) regret compared with the value of the optimal policy. This improves the O ( ln T ) regret upper bound established in the prior work by Jasin in 2014. Second, we prove that there is an Ω ( ln T ) gap between the value of the optimal policy and that of the fluid model. This complements our upper bound result by showing that the fluid is not an adequate information-relaxed benchmark when analyzing price-based revenue management algorithms. Funding: This work was supported in part by the National Science Foundation [Grant CMMI-2145661].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2219},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Constant regret resolving heuristics for price-based revenue management},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regularized aggregation of one-off probability predictions.
<em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasters predicting the chances of a future event may disagree because of differing evidence or noise. To harness the collective evidence of the crowd, we propose a Bayesian aggregator that is regularized by analyzing the forecasters’ disagreement and ascribing overdispersion to noise. Our aggregator requires no user intervention and can be computed efficiently even for a large number of predictions. To illustrate, we evaluate our aggregator on subjective probability predictions collected during a four-year forecasting tournament sponsored by the U.S. intelligence community. Our aggregator improves the squared error (a.k.a., the Brier score) of simple averaging by around 20\% and other commonly used aggregators by 10\%–25\%. This advantage stems almost exclusively from improved calibration. An R package called braggR implements our method and is available on CRAN. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2224 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2224},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Regularized aggregation of one-off probability predictions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ALSO-x and ALSO-x+: Better convex approximations for chance
constrained programs. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a chance constrained program (CCP), decision makers seek the best decision whose probability of violating the uncertainty constraints is within the prespecified risk level. As a CCP is often nonconvex and is difficult to solve to optimality, much effort has been devoted to developing convex inner approximations for a CCP, among which the conditional value-at-risk ( CVaR ) has been known to be the best for more than a decade. This paper studies and generalizes the ALSO - X , originally proposed by A hmed, L uedtke, SO ng, and X ie in 2017 , for solving a CCP. We first show that the ALSO - X resembles a bilevel optimization, where the upper-level problem is to find the best objective function value and enforce the feasibility of a CCP for a given decision from the lower-level problem, and the lower-level problem is to minimize the expectation of constraint violations subject to the upper bound of the objective function value provided by the upper-level problem. This interpretation motivates us to prove that when uncertain constraints are convex in the decision variables, ALSO - X always outperforms the CVaR approximation. We further show (i) sufficient conditions under which ALSO - X can recover an optimal solution to a CCP; (ii) an equivalent bilinear programming formulation of a CCP, inspiring us to enhance ALSO - X with a convergent alternating minimization method ( ALSO - X + ); and (iii) an extension of ALSO - X and ALSO - X + to distributionally robust chance constrained programs (DRCCPs) under the ∞ − Wasserstein ambiguity set. Our numerical study demonstrates the effectiveness of the proposed methods. Funding: This work was supported by the Division of Civil, Mechanical and Manufacturing Innovation [Grant 2046426]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.2225 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2225},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {ALSO-X and ALSO-x+: Better convex approximations for chance constrained programs},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable reinforcement learning for multiagent networked
systems. <em>OR</em>, <em>70</em>(6), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study reinforcement learning (RL) in a setting with a network of agents whose states and actions interact in a local manner where the objective is to find localized policies such that the (discounted) global reward is maximized. A fundamental challenge in this setting is that the state-action space size scales exponentially in the number of agents, rendering the problem intractable for large networks. In this paper, we propose a scalable actor critic (SAC) framework that exploits the network structure and finds a localized policy that is an O ( ρ κ + 1 ) -approximation of a stationary point of the objective for some ρ ∈ ( 0 , 1 ) , with complexity that scales with the local state-action space size of the largest κ -hop neighborhood of the network. We illustrate our model and approach using examples from wireless communication, epidemics, and traffic. Funding: This work was supported by the Caltech Center for Autonomous Systems and Technologies; Office of Naval Research [Grant YIP N00014-19-1-2217]; Air Force Office of Scientific Research [Grant YIP FA9550-18-1-0150]; National Science Foundation [Grants AitF-1637598, CAREER 1553407, and CNS-1518941]; PIMCO [PIMCO Fellowship]; Amazon Web Services [Amazon AI4Science Fellowship]; and Resnick Sustainability Institute for Science, Energy and Sustainability, California Institute of Technology [Postdoctoral Fellowship].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2226},
  journal      = {Operations Research},
  number       = {6},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Scalable reinforcement learning for multiagent networked systems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data aggregation and demand prediction. <em>OR</em>,
<em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how retailers can use data aggregation and clustering to improve demand prediction. High accuracy in demand prediction allows retailers to effectively manage their inventory as well as mitigate stock-outs and excess supply. A typical retail setting involves predicting demand for hundreds of items simultaneously. Although some items have a large amount of historical data, others were recently introduced and, thus, transaction data can be scarce. A common approach is to cluster several items and estimate a joint model for each cluster. In this vein, one can estimate some model parameters by aggregating the data from several items and other parameters at the individual-item level. We propose a practical method referred to as data aggregation with clustering ( DAC ), which balances the tradeoff between data aggregation and model flexibility. DAC allows us to predict demand while optimally identifying the features that should be estimated at the (i) item, (ii) cluster, and (iii) aggregate levels. We show that the DAC algorithm yields a consistent and normal estimate, along with improved prediction errors relative to the decentralized benchmark, which estimates a different model for each item. Using both simulated and real data, we illustrate DAC ’s improvement in prediction accuracy relative to a wide range of common benchmarks. Interestingly, the DAC algorithm has theoretical and practical advantages and helps retailers uncover meaningful managerial insights.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2301},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Data aggregation and demand prediction},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of reactive capacity on product quality and firm
profitability in an uncertain market. <em>OR</em>, <em>70</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2022.2310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many supply chains, the brand-owning retailer designs product quality and decides the retail price but often outsources its production to suppliers. For products with a short selling season, low reactive capacity in the supply chain requires the supplier to carry out production before the selling season; but the uncertain market demand creates risks of stockout or excess inventory. Suppliers’ reactive capacity and demand uncertainty can influence brand owners’ product pricing and quality decisions. For example, during the COVID-19 pandemic, automakers faced supply shortages for automotive chips because of the upstream suppliers’ limited parts inventory and production capacities, which have prompted the automakers to increase the quality (e.g., producing higher trims with more optional upgrade features) and price of their products to target fewer (high-valuation) consumers. This paper studies the impacts of the supplier’s reactive capacity and demand uncertainty on product quality and firm profitability under pull contracts in the supply chain. We find that an increase in the supplier’s reactive capacity can lead to higher or lower equilibrium product quality, benefiting the retailer but potentially reducing the supplier’s profit. Interestingly, both the retailer and the supplier can be worse off with a higher probability for the high market state (with more high-valuation consumers). Further, a higher probability for the high market state can lead to lower product quality.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2310},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Effects of reactive capacity on product quality and firm profitability in an uncertain market},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Star-shaped risk measures. <em>OR</em>, <em>70</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2022.2303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, monetary risk measures that are positively superhomogeneous, called star-shaped risk measures , are characterized and their properties are studied. The measures in this class, which arise when the subadditivity property of coherent risk measures is dispensed with and positive homogeneity is weakened, include all practically used risk measures, in particular, both convex risk measures and value-at-risk. From a financial viewpoint, our relaxation of convexity is necessary to quantify the capital requirements for risk exposure in the presence of liquidity risk, competitive delegation, or robust aggregation mechanisms. From a decision theoretical perspective, star-shaped risk measures emerge from variational preferences when risk mitigation strategies can be adopted by a rational decision maker.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2303},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Star-shaped risk measures},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new approach for vehicle routing with stochastic demand:
Combining route assignment with process flexibility. <em>OR</em>,
<em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach for the vehicle routing problem with stochastic customer demands revealed before vehicles are dispatched. We combine ideas from vehicle routing and manufacturing process flexibility to propose overlapped routing strategies with customer sharing. We characterize the asymptotic performance of the overlapped routing strategies under probabilistic analysis while also providing an upper bound on the asymptotic performance that depends only on the mean and standard deviation of the customer demand distribution. Moreover, we show that the optimality gap of our approach decays exponentially as the size of overlapped routes increases. We demonstrate that our overlapped routing strategies perform close to the theoretical lower bound derived from the reoptimization strategy and significantly outperform the routing strategy without overlapped routes. The effectiveness of the proposed overlapped routing strategies in nonasymptotic regimes is further verified through numerical analysis.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2304},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A new approach for vehicle routing with stochastic demand: Combining route assignment with process flexibility},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal control framework for online job scheduling with
general cost functions. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of online job scheduling on a single machine or multiple unrelated machines with general job and machine-dependent cost functions. In this model, each job has a processing requirement and arrives with a nonnegative nondecreasing cost function and this information is revealed to the system on arrival of that job. The goal is to dispatch the jobs to the machines in an online fashion and process them preemptively on the machines to minimize the generalized integral completion time. It is assumed that jobs cannot migrate between machines and that each machine has a fixed unit processing speed that can work on a single job at any time instance. In particular, we are interested in finding an online scheduling policy whose objective cost is competitive with respect to a slower optimal offline benchmark, that is, the one that knows all the job specifications a priori and is slower than the online algorithm. We first show that for the case of a single machine and special cost functions the highest-density-first rule is optimal for the generalized fractional completion time. We then extend this result by giving a speed-augmented competitive algorithm for the general nondecreasing cost functions by using a novel optimal control framework. This approach provides a principled method for identifying dual variables in different settings of online job scheduling with general cost functions. Using this method, we also provide a speed-augmented competitive algorithm for multiple unrelated machines with nondecreasing convex functions, where the competitive ratio depends on the curvature of the cost functions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2321},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {An optimal control framework for online job scheduling with general cost functions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Endogenous inverse demand functions. <em>OR</em>,
<em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present an equilibrium formulation for price impacts. This is motivated by the Bühlmann equilibrium in which assets are sold into a system of market participants, for example, a fire sale in systemic risk, and can be viewed as a generalization of the Esscher premium. Existence and uniqueness of clearing prices for the liquidation of a portfolio are studied. We also investigate other desired portfolio properties including monotonicity and concavity. Price per portfolio unit sold is also calculated. In special cases, we study price impacts generated by market participants who follow the exponential utility and power utility.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2325},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Endogenous inverse demand functions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimizing multimodular functions and allocating capacity in
bike-sharing systems. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of bike-sharing systems around the world has motivated recent attention to models and algorithms for their effective operation. Most of this literature focuses on their daily operation for managing asymmetric demand. In this work, we consider the more strategic question of how to (re)allocate dock-capacity in such systems. We develop mathematical formulations for variations of this problem (either for service performance over the course of one day or for a long-run-average) and exhibit discrete convex properties in associated optimization problems. This allows us to design a polynomial-time allocation algorithm to compute an optimal solution for this problem, which can also handle practically motivated constraints, such as a limit on the number of docks moved in the system. We apply our algorithm to data sets from Boston, New York City, and Chicago to investigate how different dock allocations can yield better service in these systems. Recommendations based on our analysis have led to changes in the system design in Chicago and New York City. Beyond optimizing for improved quality of service through better allocations, our results also provide a metric to compare the impact of strategically reallocating docks and the daily rebalancing of bikes.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2320},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Minimizing multimodular functions and allocating capacity in bike-sharing systems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Delegated concept testing in new product development.
<em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing a large variety of different product concepts is an integral part of nearly all new product development initiatives—especially in the concept selection phase, where firms seek to identify the most promising concept for further development. Test results are usually collected by agents, who must be incentivized not only to exert effort in testing the concepts but also to report their findings truthfully. We ask: How should a firm structure its concept testing processes when testing efforts must be delegated to self-interested agents? To answer this question, we devise a principal–(multi)agent model that allows us to analyze different testing processes and to compare their relative benefits. We find that several factors determine how the firm should construct its testing processes: the efficiency of concept testing, the extent of heterogeneity in the quality of test outcomes, and the severity of information asymmetry between agents and the firm. Finally, we determine how the firm should adjust agents’ incentives to reflect its chosen testing process and which product concepts should (or should not) be tested.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2318},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Delegated concept testing in new product development},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Individualized dynamic patient monitoring under alarm
fatigue. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospitals are rife with alarms, many of which are false. This leads to alarm fatigue, in which clinicians become desensitized and may inadvertently ignore real threats. We develop a partially observable Markov decision process model for recommending dynamic, patient-specific alarms in which we incorporate a cry-wolf feedback loop of repeated false alarms. Our model takes into account patient heterogeneity in safety limits for vital signs and learns a patient’s safety limits by performing Bayesian updates during a patient’s hospital stay. We develop structural results of the optimal policy and perform a numerical case study based on clinical data from an intensive care unit. We find that compared with current approaches of setting patients’ alarms, our dynamic patient-centered model significantly reduces the risk of patient harm.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2300},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Individualized dynamic patient monitoring under alarm fatigue},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Revenue-maximizing auctions: A bidder’s standpoint.
<em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of improving bidders’ strategies in prior-dependent revenue-maximizing auctions and introduce a simple and generic method to design novel bidding strategies whenever the seller uses past bids to optimize her mechanism. We propose a simple and agnostic strategy, independent of the distribution of the competition, that is robust to mechanism changes and local optimization of reserve prices by the seller. In many settings, it consists in overbidding for low values, then underbidding and finally bidding truthfully. This strategy guarantees an increase in utility compared with the truthful strategy for any distribution of the competition. We generalize this result by showing that a best response for maximizing bidder’s utility in a large class of possible strategies is a simple extension of this first strategy. Our new variational approach naturally yields itself to numerical optimization and algorithms for designing or improving strategies in any given selling mechanisms. Our formulation enables the study of some important robustness properties of the strategies, showing their impact even when the seller is using a data-driven approach to set the reserve prices, whether the sample sizes are finite or infinite. The gist of our approach is to see optimal auctions in practice as a Stackelberg game where the buyer is the leader, as he is the first to move (here bid) and where the seller is the follower as she has no prior information on the bidder.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2316},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Revenue-maximizing auctions: A bidder’s standpoint},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—assortment planning for two-sided sequential
matching markets. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sided matching platforms provide users with menus of match recommendations. To maximize the number of realized matches between the two sides (referred to herein as customers and suppliers), the platform must balance the inherent tension between recommending more suppliers to customers to increase the chances that they like one of them and avoiding the conflicts that arise when customers, who are given more options, end up choosing the same suppliers. We introduce a stylized model to study the above tradeoff. The platform offers each customer a menu of suppliers, and customers choose, simultaneously and independently, to either select a supplier from their menu or remain unmatched. Suppliers then see the set of customers that have selected them and choose to either match with one of these customers or remain unmatched. A match occurs if a customer and a supplier choose each other (in sequence). Agents’ choices are probabilistic and proportional to the public scores of agents in their menu and a score that is associated with the outside option of remaining unmatched. The platform’s problem is to construct menus for customers, so as to maximize the total number of matches. We first show that this problem is strongly NP-hard. Then, we provide an intuitive efficient algorithm that achieves a constant-factor approximation to the optimal expected number of matches. Our algorithm uses bucketing (grouping similar suppliers into buckets) together with linear-programming-based relaxations and rounding techniques.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2327},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Assortment planning for two-sided sequential matching markets},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Allocation with weak priorities and general constraints.
<em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a resource allocation problem that combines three general features: complex resource constraints, weak priority rankings over the agents, and ordinal preferences over bundles of resources. We develop a mechanism based on a new concept called competitive stable equilibrium . It has several attractive properties, commonly captures two different frameworks of one-sided and two-sided markets, and extends them to richer environments. Our framework also allows for an alternative and more flexible tie-breaking rule by giving agents different budgets. We empirically apply our mechanism to reassign season tickets to families in the presence of social distancing. Our simulation results show that our method outperforms existing ones in both efficiency and fairness measures.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2329},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Allocation with weak priorities and general constraints},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—approximation schemes for
capacity-constrained assortment optimization under the nested logit
model. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main contribution of this paper resides in proposing a carefully crafted dynamic programming approach for capacitated assortment optimization under the nested logit model in its utmost generality. Specifically, we show that the optimal revenue can be efficiently approached within any degree of accuracy by synthesizing ideas related to continuous-state dynamic programming, state space discretization, and sensitivity analysis of modified revenue functions. These developments allow us to devise the first fully polynomial-time approximation scheme in this context, thus resolving fundamental open questions posed in previous papers.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2336},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Approximation schemes for capacity-constrained assortment optimization under the nested logit model},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—product-based approximate linear programs for
network revenue management. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2022.2354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approximate linear programming approach has received significant attention in the network revenue management literature. A popular approximation in the existing literature is separable piecewise linear (SPL) approximation, which estimates the value of each unit of each resource over time. SPL approximation can be used to construct resource-based bid-price policies. In this paper, we propose a product-based SPL approximation. The coefficients of the product-based SPL approximation can be interpreted as each product’s revenue contribution to the value of each unit of each resource in a given period. We show that the resulting approximate linear program admits compact reformulations, such as its resource-based counterpart. Furthermore, the new approximation allows us to derive a set of valid inequalities to (i) speed up the computation and (ii) select optimal solutions to construct more effective policies. We conduct an extensive numerical study to illustrate our results. In a set of 192 problem instances, bid-price policies based on the new approximation generate higher expected revenues than resource-based bid-price policies with an average revenue lift of 0.72\% and a maximum revenue lift of 5.3\%. In addition, the new approximation can be solved 1.42 times faster than the resource-based approximation and shows better numerical stability. The valid inequalities derived from the new approximation further improve the computational performance and are critical for achieving additional gains in the expected revenue. The policy performance is competitive compared with the dynamic programming decomposition method, which is the strongest heuristic known in the literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2354},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Product-based approximate linear programs for network revenue management},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—ranking distributions when only means and
variances are known. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.2072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a choice between two random variables, for which only means and variances are known. Is it possible to rank them by putting some constraints on risk preferences? We provide such a ranking by bounding how much marginal utility can change. Such bounds enable us to rank all distributions with given means and variances by first-order almost-stochastic dominance. We show how our results can be used to compare a risky project and a sure payoff and also provide a new connection between the Sharpe and Omega ratios from finance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2072},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Ranking distributions when only means and variances are known},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—optimal patrol of a perimeter. <em>OR</em>,
<em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A defender dispatches patrollers to circumambulate a perimeter to guard against potential attacks. The defender decides on the time points to dispatch patrollers and each patroller’s direction and speed, as long as the long-run rate at which patrollers are dispatched is capped at some constant. An attack at any point on the perimeter requires the same amount of time, during which it will be detected by each passing patroller independently with the same probability. The defender wants to maximize the probability of detecting an attack before it completes, while the attacker wants to minimize it. We study two scenarios, depending on whether the patrollers are undercover or wear a uniform. Conventional wisdom would suggest that the attacker gains advantage if he can see the patrollers going by so as to time his attack, but we show that the defender can achieve the same optimal detection probability by carefully spreading out the patrollers probabilistically against a learning attacker.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2117},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Optimal patrol of a perimeter},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding minimum volume circumscribing ellipsoids using
generalized copositive programming. <em>OR</em>, <em>70</em>(5), iii–vi.
(<a href="https://doi.org/10.1287/opre.2021.2156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of finding the Löwner–John ellipsoid (i.e., an ellipsoid with minimum volume that contains a given convex set). We reformulate the problem as a generalized copositive program and use that reformulation to derive tractable semidefinite programming approximations for instances where the set is defined by affine and quadratic inequalities. We prove that, when the underlying set is a polytope, our method never provides an ellipsoid of higher volume than the one obtained by scaling the maximum volume-inscribed ellipsoid. We empirically demonstrate that our proposed method generates high-quality solutions and can be solved much faster than solving the problem to optimality. Furthermore, we outperform the existing approximation schemes in terms of solution time and quality. We present applications of our method to obtain piecewise linear decision rule approximations for dynamic distributionally robust problems with random recourse and to generate ellipsoidal approximations for the set of reachable states in a linear dynamical system when the set of allowed controls is a polytope.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2156},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Finding minimum volume circumscribing ellipsoids using generalized copositive programming},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consistency cuts for dantzig-wolfe reformulations.
<em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a family of valid inequalities, which we term consistency cuts, to be applied to a Dantzig-Wolfe reformulation (or decomposition) with linking variables. We prove that these cuts ensure an integer solution to the corresponding Dantzig-Wolfe relaxation when certain criteria to the structure of the decomposition are met. We implement the cuts and use them to solve a commonly used test set of 200 instances of the temporal knapsack problem. We assess the performance with and without the cuts and compare further to CPLEX and other solution methods that have historically been used to solve the test set. By separating consistency cuts, we show that we can obtain optimal integer solutions much faster than the other methods and even solve the remaining unsolved problems in the test set. We also perform a second test on instances from the MIPLIB 2017 online library of mixed-integer programs, showing the potential of the cuts on a wider range of problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2160},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Consistency cuts for dantzig-wolfe reformulations},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adjustable robust optimization reformulations of two-stage
worst-case regret minimization problems. <em>OR</em>, <em>70</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2021.2159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the idea that two-stage worst-case regret minimization problems with either objective or right-hand side uncertainty can be reformulated as two-stage robust optimization problems and can therefore benefit from the solution schemes and theoretical knowledge that have been developed in the last decade for this class of problems. In particular, we identify conditions under which a first-stage decision can be obtained either exactly using popular adjustable robust optimization decomposition schemes or approximately by conservatively using affine decision rules. Furthermore, we provide both numerical and theoretical evidence that, in practice, the first-stage decision obtained using affine decision rules is of high quality. Initially, this is done by establishing mild conditions under which these decisions can be proven exact, which effectively extends the space of regret minimization problems known to be solvable in polynomial time. We further evaluate both the suboptimality and computational efficiency of this tractable approximation scheme in a multi-item newsvendor problem and a production transportation problem.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2159},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Adjustable robust optimization reformulations of two-stage worst-case regret minimization problems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global convergence of stochastic gradient hamiltonian monte
carlo for nonconvex stochastic optimization: Nonasymptotic performance
bounds and momentum-based acceleration. <em>OR</em>, <em>70</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2021.2162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient Hamiltonian Monte Carlo (SGHMC) is a variant of stochastic gradients with momentum where a controlled and properly scaled Gaussian noise is added to the stochastic gradients to steer the iterates toward a global minimum. Many works report its empirical success in practice for solving stochastic nonconvex optimization problems; in particular, it has been observed to outperform overdamped Langevin Monte Carlo–based methods, such as stochastic gradient Langevin dynamics (SGLD), in many applications. Although the asymptotic global convergence properties of SGHMC are well known, its finite-time performance is not well understood. In this work, we study two variants of SGHMC based on two alternative discretizations of the underdamped Langevin diffusion. We provide finite-time performance bounds for the global convergence of both SGHMC variants for solving stochastic nonconvex optimization problems with explicit constants. Our results lead to nonasymptotic guarantees for both population and empirical risk minimization problems. For a fixed target accuracy level on a class of nonconvex problems, we obtain complexity bounds for SGHMC that can be tighter than those available for SGLD.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2162},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Global convergence of stochastic gradient hamiltonian monte carlo for nonconvex stochastic optimization: Nonasymptotic performance bounds and momentum-based acceleration},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online linear programming: Dual convergence, new algorithms,
and regret bounds. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an online linear programming (OLP) problem under a random input model in which the columns of the constraint matrix along with the corresponding coefficients in the objective function are independently and identically drawn from an unknown distribution and revealed sequentially over time. Virtually all existing online algorithms were based on learning the dual optimal solutions/prices of the linear programs (LPs), and their analyses were focused on the aggregate objective value and solving the packing LP, where all coefficients in the constraint matrix and objective are nonnegative. However, two major open questions were as follows. (i) Does the set of LP optimal dual prices learned in the existing algorithms converge to those of the “offline” LP? (ii) Could the results be extended to general LP problems where the coefficients can be either positive or negative? We resolve these two questions by establishing convergence results for the dual prices under moderate regularity conditions for general LP problems. Specifically, we identify an equivalent form of the dual problem that relates the dual LP with a sample average approximation to a stochastic program. Furthermore, we propose a new type of OLP algorithm, action-history-dependent learning algorithm, which improves the previous algorithm performances by taking into account the past input data and the past decisions/actions. We derive an O ( log n log log n ) regret bound (under a locally strong convexity and smoothness condition) for the proposed algorithm, against the O ( n ) bound for typical dual-price learning algorithms, where n is the number of decision variables. Numerical experiments demonstrate the effectiveness of the proposed algorithm and the action-history-dependent design.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2164},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Online linear programming: Dual convergence, new algorithms, and regret bounds},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An optimal approximation for submodular maximization under a
matroid constraint in the adaptive complexity model. <em>OR</em>,
<em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study submodular maximization under a matroid constraint in the adaptive complexity model. This model was recently introduced in the context of submodular optimization to quantify the information theoretic complexity of black-box optimization in a parallel computation model. Despite the burst in work on submodular maximization in the adaptive complexity model, the fundamental problem of maximizing a monotone submodular function under a matroid constraint has remained elusive. In particular, all known techniques fail for this problem and there are no known constant factor approximation algorithms whose adaptivity is sublinear in the rank of the matroid k or in the worst case sublinear in the size of the ground set n . We present an algorithm that has an approximation guarantee arbitrarily close to the optimal 1 − 1 / e for monotone submodular maximization under a matroid constraint and has near-optimal adaptivity of O ( log ( n ) log ( k ) ) . This result is obtained using a novel technique of adaptive sequencing , which departs from previous techniques for submodular maximization in the adaptive complexity model. In addition to our main result, we show how to use this technique to design other approximation algorithms with strong approximation guarantees and polylogarithmic adaptivity.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2170},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {An optimal approximation for submodular maximization under a matroid constraint in the adaptive complexity model},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New venture creation: A drift-variance diffusion control
model. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model the creation of a new venture with a novel drift-variance diffusion control framework in which the state of the venture is captured by a diffusion process. The entrepreneur creating the venture chooses costly controls, which determine both the drift and the variance of the process. When the process reaches an upper boundary, the venture succeeds and the entrepreneur receives a reward. When the process reaches a lower boundary, the venture fails. The entrepreneur can choose between two different controls and wishes to determine the policy that maximizes the expected total reward minus total cost. We consider two variations of the model: one in which both boundaries are fixed and one in which only the upper boundary is fixed but the lower is free. We derive closed-form expressions under which the optimal policy will be dynamic versus static; we prove that when the policy is dynamic, it switches between the two controls at most once. The results reveal a subtle trade-off between the cost of the two controls, their drift and their variances, in which controls that are more expensive may be utilized more than controls that are less expensive. We also demonstrate that in the fixed boundary case, the entrepreneur may wastefully use a more expensive control near the lower boundary to avoid hitting that boundary. This implies that efficient utilization of the two controls cannot happen when the entrepreneur does not have the freedom to choose when to abandon the venture.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2171},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {New venture creation: A drift-variance diffusion control model},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extremizing and antiextremizing in bayesian ensembles of
binary-event forecasts. <em>OR</em>, <em>70</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2021.2176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability forecasts of binary events are often gathered from multiple models or experts and averaged to provide inputs regarding uncertainty in important decision-making problems. Averages of well-calibrated probabilities are underconfident, and methods have been proposed to make them more extreme. To aggregate probabilities, we introduce a class of ensembles that are generalized additive models. These ensembles are based on Bayesian principles and can help us learn why and when extremizing is appropriate. Extremizing is typically viewed as shifting the average probability farther from one half; we emphasize that it is more suitable to define extremizing as shifting it farther from the base rate. We introduce the notion of antiextremizing to learn instances in which it might be beneficial to make average probabilities less extreme. Analytically, we find that our Bayesian ensembles often extremize the average forecast but sometimes antiextremize instead. On several publicly available data sets, we demonstrate that our Bayesian ensemble performs well and antiextremizes anywhere from 18\% to 73\% of the cases. It antiextremizes much more often when there is bracketing with respect to the base rate among the probabilities being aggregated than with no bracketing, suggesting that bracketing is a promising indicator of when we should consider antiextremizing.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2176},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Extremizing and antiextremizing in bayesian ensembles of binary-event forecasts},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic programs with shared resources and signals: Dynamic
fluid policies and asymptotic optimality. <em>OR</em>, <em>70</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2021.2181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a sequential decision problem involving shared resources and signals in which a decision maker repeatedly observes some exogenous information (the signal), modeled as a finite-state Markov process, then allocates a limited amount of a shared resource across a set of projects. The framework includes a number of applications and generalizes Markovian multiarmed bandit problems by (a) incorporating exogenous information through the signal and (b) allowing for more general resource allocation decisions. Such problems are naturally formulated as stochastic dynamic programs (DPs), but solving the DP is impractical unless the number of projects is small. In this paper, we develop a Lagrangian relaxation and a DP formulation of the corresponding fluid relaxation—a dynamic fluid relaxation —that provide upper bounds on the optimal value function as well as a feasible policy. We develop an iterative primal-dual algorithm for solving the dynamic fluid relaxation and analyze the performance of the feasible dynamic fluid policy. Our performance analysis implies, under mild conditions, that the dynamic fluid relaxation bound and feasible policy are asymptotically optimal as the number of projects grows large. Our Lagrangian relaxation uses Lagrange multipliers that depend on the history of past signals in each period: we show that the bounds and analogous policies using restricted forms of Lagrange multipliers (e.g., only depending on the current signal state in each period) in general lead to a performance gap that is linear in the number of projects and thus are not asymptotically optimal in the regime of many projects. We demonstrate the model and results in two applications: (i) a dynamic capital budgeting problem and (ii) a multilocation inventory management problem with limited production capacity and demands that are correlated across locations by a changing market state.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2181},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Dynamic programs with shared resources and signals: Dynamic fluid policies and asymptotic optimality},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—a permutation-dependent separability approach
for capacitated two-echelon inventory systems. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimal inventory replenishment policies for capacitated 2-echelon serial inventory systems, where the capacity of upstream echelon can be the bottleneck. We show that the optimal replenishment decisions in each period can be made one echelon at a time by introducing a procedure that can sequentially decompose a multidimensional optimization problem to a series of one-dimensional problems. We also introduce the notion of permutation-dependent separability . A permutation-dependent separable function is a function that can be decomposed as a sum of single-variable component functions under each nondecreasing order of variables. We find that the value function for the capacitated 2-echelon system in each period is permutation-dependent separable, and that, for each echelon, a permutation-dependent echelon base stock policy is optimal.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2194},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A permutation-dependent separability approach for capacitated two-echelon inventory systems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capacity games with supply function competition.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general model for suppliers competing for a buyer’s procurement business. The buyer faces uncertain demand, and there is a requirement to reserve capacity in advance of knowing the demand. Each supplier has costs that are two-dimensional, with some capacity costs incurred prior to production and some production costs incurred at the time of delivery. These costs are general functions of quantity, and this naturally leads us to a supply function competition framework in which each supplier offers a schedule of prices and quantities. We show that there is an equilibrium of a particular form: the buyer makes a reservation choice that maximizes the overall supply chain profit, each supplier makes a profit equal to their marginal contribution to the supply chain, and the buyer takes the remaining profit. This is a natural equilibrium for the suppliers to coordinate on, since no supplier can do better in any other equilibrium. These results make use of a submodularity property for the supply chain optimal profits as a function of the suppliers available and build on the assumption that the buyer breaks a tie in favor of the solutions that give the largest supply chain profit. We demonstrate the applications of our model in three operations management problems: a newsvendor problem with unreliable suppliers, a portfolio procurement problem with supply options and a spot market, and a bundling problem with nonsubstitutable products.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2221},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Capacity games with supply function competition},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—on matrix exponential differentiation with
application to weighted sum distributions. <em>OR</em>, <em>70</em>(4),
iii–vii. (<a href="https://doi.org/10.1287/opre.2021.2257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we revisit the innovative transform approach introduced by Cai, Song, and Kou [(2015) A general framework for pricing Asian options under Markov processes. Oper. Res. 63(3):540–554] for accurately approximating the probability distribution of a weighted stochastic sum or time integral under general one-dimensional Markov processes. Since then, Song, Cai, and Kou [(2018) Computable error bounds of Laplace inversion for pricing Asian options. INFORMS J. Comput. 30(4):625–786] and Cui, Lee, and Liu [(2018) Single-transform formulas for pricing Asian options in a general approximation framework under Markov processes. Eur. J. Oper. Res. 266(3):1134–1139] have achieved an efficient reduction of the original double to a single-transform approach. We move one step further by approaching the problem from a new angle and, by dealing with the main obstacle relating to the differentiation of the exponential of a matrix, we bypass the transform inversion. We highlight the benefit from the new result by means of some numerical examples.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2257},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On matrix exponential differentiation with application to weighted sum distributions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—data-driven newsvendor problem: Performance
of the sample average approximation. <em>OR</em>, <em>70</em>(4),
iii–vii. (<a href="https://doi.org/10.1287/opre.2022.2307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the data-driven newsvendor problem in which a manager makes inventory decisions sequentially and learns the unknown demand distribution based on observed samples of continuous demand (no truncation). We study the widely used sample average approximation (SAA) approach and analyze its performance with respect to regret, which is the difference between its expected cost and the optimal cost of the clairvoyant who knows the underlying demand distribution. We characterize how the regret performance depends on a minimal separation assumption that restricts the local flatness of the demand distribution around the optimal order quantity. In particular, we consider two separation parameters, γ and ε , where γ denotes the minimal possible value of the density function in a small neighborhood of the optimal quantity and ε defines the size of the neighborhood. We establish a lower bound on the worst case regret of any policy that depends on the product of the separation parameters γ ε and the time horizon N . We also show a finite-time upper bound of SAA that matches the lower bound in terms of the separation parameters and the time horizon (up to a logarithmic factor of N ). This illustrates the near-optimal performance of SAA with respect to not only the time horizon, but also the local flatness of the demand distribution around the optimal quantity. Our analysis also shows upper bounds of O ( log N ) and O ( N ) on the worst case regret of SAA over N periods with and without the minimal separation assumption. Both bounds match the lower bounds implied by the literature, which illustrates the asymptotic optimality of the SAA approach.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2307},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Data-driven newsvendor problem: Performance of the sample average approximation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Order now, pickup in 30 minutes: Managing queues with static
delivery guarantees. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of managing queues in online food-ordering services where customers, who place orders online and pick up at the store, are offered a common quote time, that is, the promised pick-up time minus the time the order is placed. The objective is to minimize the long-run average expected earliness and tardiness cost incurred by the customers. We introduce the family of static threshold policies for managing such queues. A static threshold policy is one that starts serving the first customer in the queue as soon as the server is free and the time remaining until the promised pick-up time of that customer falls below a fixed threshold. In important technical contributions for establishing the attractiveness of the optimal static threshold policy, we develop two sets of lower bounds on the optimal cost. The first set of lower bounds uses the idea of a clairvoyant optimal policy by considering a decision maker who has either full or partial knowledge of the outcomes of future uncertainties. To obtain our second set of lower bounds, we develop bounds on the optimal earliness and tardiness costs by establishing lower and upper bounds on the steady-state waiting time under an optimal policy. The optimal static threshold policy is asymptotically optimal in several cases, including the heavy traffic and the light traffic regimes. We also develop a dynamic threshold policy in which the threshold depends on the queue length. Finally, through a comprehensive numerical study, we demonstrate the excellent performance of both the static and the dynamic threshold policies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2203},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Order now, pickup in 30 minutes: Managing queues with static delivery guarantees},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vessel service planning in seaports. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Berth allocation and pilotage planning are the two most important decisions made by a seaport for serving incoming vessels. Traditionally, the berth allocation problem and the pilotage planning problem are solved sequentially, leading to suboptimal or even infeasible solutions for vessel services. This paper investigates a vessel service planning problem (VSPP) in seaports that addresses berth allocation and pilotage planning in combination. We introduce a compact mixed-integer linear programming formulation for the problem, which can be solved by general-purpose solvers. To solve large-scale instances, we develop an exact solution approach that combines Benders decomposition and column generation within an efficient branch-and-bound framework. Unlike the traditional three-phase Benders decomposition and column generation method, which does not guarantee optimality, we propose a branching scheme that enables the approach to determine an optimal solution to the VSPP. The approach is enhanced through practical acceleration strategies. Extensive computational results using data instances from one of the world’s largest seaports show that these acceleration strategies significantly improve the performance of our solution approach and that it can obtain optimal or near-optimal solutions for instances of realistic scale. We show that our solution approach outperforms the method commonly used for solving similar problems. We perform sensitivity tests to demonstrate the robustness of the approach against variations in problem settings. We also show the benefits brought by integrated optimization by comparing our solution approach with a method that handles berth allocation and pilotage planning sequentially.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2228},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Vessel service planning in seaports},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal dynamic momentum strategies. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explicitly solve for the optimal dynamic trading strategy between a riskless asset and a risky asset with momentum. The optimal portfolio weight depends not only on the momentum, as in Merton’s framework, but also on the historical price path; this contrasts with Merton. Because of their path dependence, optimal portfolio weights have a wide distribution for a given level of momentum; for example, investors may short the risky asset if it has rebound price paths but leverage if it has hump-shaped price paths. This effect tends to be the most significant after large price swings. Path dependence is solved with explicit formulas and presented with heuristic statistics.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2254},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Optimal dynamic momentum strategies},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling the risk in mortality projections. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents and applies models for the valuation and management of mortality-contingent exposures. Such exposures include insurance and pension benefits, as well as novel mortality-linked securities traded in financial markets. Unlike conventional approaches to modeling mortality, we consider the stochastic evolution of mortality projections rather than realized mortality rates . Relying on a time series of age-specific mortality forecasts, we develop a set of stochastic models that—unlike conventional mortality models—capture the evolution of mortality forecasts over the past 50 years. In particular, the dynamics of our models reflect the substantial observed variability of long-term projections and are therefore particularly well-suited for financial applications where long-term demographic uncertainty is relevant.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2255},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Modeling the risk in mortality projections},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pricing of debt and equity in a financial network with
comonotonic endowments. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present formulas for the valuation of debt and equity of firms in a financial network under comonotonic endowments. We demonstrate that the comonotonic setting provides a lower bound and Jensen’s inequality provides an upper bound to the price of debt under Eisenberg-Noe financial networks with bankruptcy costs. Such financial networks encode the interconnection of firms through debt claims. The proposed pricing formulas consider the realized, endogenous recovery rate on debt claims. We endogenously construct the comonotonic endowment setting from an equity maximizing standpoint with capital transfers. We conclude by, numerically, comparing the network valuation problem with two single firm baseline heuristics that can, respectively, approximate the price of debt and equity.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2275},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Pricing of debt and equity in a financial network with comonotonic endowments},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Easy cases of deadlock detection in train scheduling.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deadlock occurs when two or more trains are preventing each other from moving forward by occupying the required tracks. Deadlocks are rare but pernicious events in railroad operations and, in most cases, are caused by human errors. Recovering is a time-consuming and costly operation, producing large delays and often requiring crew rescheduling and complex switching moves. In practice, most deadlocks involve only two long trains missing their last potential meet location. In this paper, we prove that, for any network configuration, the identification of two-train deadlocks can be performed in polynomial time. This is the first exact polynomial algorithm for such a practically relevant combinatorial problem. We also develop a pseudo-polynomial but efficient oracle that allows real-time early detection and prevention of any (potential) two-train deadlock in the Union Pacific (a U.S. class 1 rail company) railroad network. A deadlock prevention module based on the work in this paper will be put in place at Union Pacific to prevent all deadlocks of this kind.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2283},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Easy cases of deadlock detection in train scheduling},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing opinions with stubborn agents. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of optimizing the placement of stubborn agents in a social network in order to maximally influence the population. We assume the network contains stubborn users whose opinions do not change, and nonstubborn users who can be persuaded. We further assume that the opinions in the network are in an equilibrium that is common to many opinion dynamics models, including the well-known DeGroot model. We develop a discrete optimization formulation for the problem of maximally shifting the equilibrium opinions in a network by targeting users with stubborn agents. The opinion objective functions that we consider are the opinion mean, the opinion variance, and the number of individuals whose opinion exceeds a fixed threshold. We show that the mean opinion is a monotone submodular function, allowing us to find a good solution using a greedy algorithm. We find that on real social networks in Twitter consisting of tens of thousands of individuals, a small number of stubborn agents can nontrivially influence the equilibrium opinions. Furthermore, we show that our greedy algorithm outperforms several common benchmarks. We then propose an opinion dynamics model where users communicate noisy versions of their opinions, communications are random, users grow more stubborn with time, and there is heterogeneity in how users’ stubbornness increases. We prove that, under fairly general conditions on the stubbornness rates of the individuals, the opinions in this model converge to the same equilibrium as the DeGroot model, despite the randomness and user heterogeneity in the model.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2291},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Optimizing opinions with stubborn agents},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online resource allocation with personalized learning.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint online learning and resource allocation is a fundamental problem inherent in many applications. In a general setting, heterogeneous customers arrive sequentially, each of which can be allocated to a resource in an online fashion. Customers stochastically consume the resources, allocations yield stochastic rewards, and the system receives feedback outcomes with delay. We introduce a generic framework that judiciously synergizes online learning with a broad class of online resource allocation mechanisms, where the sequence of customer contexts is adversarial, and the customer reward and the resource consumption are stochastic and unknown. First, we propose an online algorithm for a general resource allocation problem, called personalized resource allocation while learning with delay, which strikes a three-way balance between exploration, exploitation, and hedging against adversarial arrival sequence. We provide a performance guarantee for this online algorithm in terms of Bayesian regret. Next, we develop our second online algorithm for an advance scheduling problem, called personalized advance scheduling while learning with delay (PAS-LD), and evaluate its theoretical performance. The PAS-LD algorithm has a more delicate structure and offers multiday scheduling while accounting for the no-show behavior of customers. We demonstrate the practicality and efficacy of our PAS-LD algorithm using clinical data from a partner health system. Our results show that the proposed algorithm provides promising results compared with several benchmark policies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2294},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Online resource allocation with personalized learning},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—the multinomial logit model with sequential
offerings: Algorithmic frameworks for product recommendation displays.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the assortment problem under the multinomial logit (MNL) model with sequential offerings recently proposed by Liu et al. [INFORMS J. Comput., 2020 ] to capture a multitude of applications, ranging from appointment scheduling in hospitals, restaurants, and fitness centers to product recommendations in e-commerce settings. In this problem, the purchasing dynamics of customers sequentially unfold over T stages. Within each stage, the retailer selects an assortment of products to make available for purchase with the intent of maximizing expected revenue. However, motivated by practical applications, the caveat is that each product can be offered in at most one stage. Moving from one stage to the next, the customer either purchases one of the currently offered products according to MNL preferences and leaves the system or decides not to make any purchase at that time. In the former scenario, the retailer gains a product-associated revenue; in the latter scenario, the customer progresses to the next stage or eventually leaves the system once all T stages have been traversed. We focus our attention on the most general formulation of this problem, in which purchasing decisions are governed by a stage-dependent MNL choice model, reflecting the notion that customers’ preferences may change from stage to stage because of updated perceptions, patience waning over time, etc. Concurrently, we consider a more structured formulation in which purchasing decisions are stage-invariant, utilizing a single MNL model across all stages. Our main contribution comes in the form of a strongly polynomial-time approximation scheme for both formulations of the sequential assortment problem in their utmost generality. We provide evidence for the practical relevance of these theoretical findings through extensive numerical experiments. Finally, we fit our sequential model to historical search data from Expedia’s hotel booking platform. We observe substantial gains in fitting accuracy when our model is benchmarked against other well-known choice models designed for the setting at hand.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2218},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—The multinomial logit model with sequential offerings: Algorithmic frameworks for product recommendation displays},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiproduct pricing with discrete price sets. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a multiproduct pricing problem in which the prices of the products are restricted to discrete and finite sets. The demand for a product is a function of the prices of all the products. The prices of the products can be changed through time, subject to the aggregate consumption of each resource not exceeding its availability over the planning horizon. The focus of our work is the deterministic variant of this problem (wherein customer-arrival rates are deterministic), which is a key subproblem whose solution can be used to build effective policies for the stochastic variant (as in Gallego and Van Ryzin 1997 ). When the demand rate of each product is a concave function of the prices, we obtain an efficient and effective solution to the deterministic problem; the worst-case optimality gap of our solution depends on the curvature of the objective function. We obtain a similar performance guarantee for our solution under the linear attraction demand model and a special case of the multinomial logit (MNL) demand model. For a general demand function, the worst-case optimality gap of our solution depends on the curvature of both the objective function and the demand function. For the special case where the demand rate of a product depends only on its own price and not on the prices of the other products, we show that the deterministic problem can be efficiently solved via a linear program.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2222},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Multiproduct pricing with discrete price sets},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—online hypergraph matching with delays.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an online hypergraph matching problem inspired by ridesharing and delivery applications. The vertices of a hypergraph are revealed sequentially and must be matched shortly thereafter, otherwise they will leave the system in favor of an outside option. Hyperedges are revealed to the algorithm once all of its vertices have arrived, and can only be included into the matching before any of its vertices leave the system. The cardinality of hyperedges are bounded by a small constant which represents the capacity of service vehicles. We study utility maximization and cost minimization objectives in this model. In the utility maximization setting, we present a polynomial time algorithm which provably achieves the optimal competitive ratio provided that the vehicle capacity is at least 3. For the cost minimization setting, we assume costs are monotone, which is a natural assumption in ridesharing and delivery problems. When the vehicle capacity is 2, we present a polynomial-time thresholding algorithm and prove that it has the optimal competitive ratio among deterministic algorithms. For higher vehicle capacities, we show that the performance of a randomized batching algorithm is within a small constant of the optimal competitive ratio achievable in polynomial time.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2277},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Online hypergraph matching with delays},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—a near-optimal algorithm for real-time order
acceptance: An application in postacute healthcare services.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an application at a postacute healthcare provider, we study an infinite-horizon, stochastic optimization problem with a set of long-term capacity investment decisions and a sequence of real-time order acceptance/rejection decisions. The goal is to maximize the long-run average expected profit per period. The firm employs full-time resources of various kinds, such as nurses and therapists. For each kind of resource, multiple types are available. For example, registered nurses (RNs) are more expensive to employ than licensed practical nurses (LPNs); however, RNs can serve a greater range of patients than LPNs. Thus, the long-term capacity decision for this firm is the number of each type of resource to employ full time. A full-time resource may, at times, become unavailable (i.e., be absent); this absenteeism is stochastic. When the need for resources cannot be met from the pool of full-time employees, the firm has access to on-demand, part-time resources, who are paid a higher hourly rate than an equivalent full-time resource. On the demand side, the firm receives referrals —requests to commit service to patients over a time window (whose duration is stochastic), which is referred to as an episode —in real time. The referral arrival process is stochastic. A referral is characterized by the revenue it provides to the firm, the resources required to serve that patient, the frequency with which each of these resources is required, and the distribution of the episode duration. The decision to accept or reject a referral has to be instantaneous; if accepted, the service episode starts immediately. We develop a simple solution to the optimization problem, derive a worst-case guarantee on its optimality gap, and demonstrate that this gap vanishes in a meaningful asymptotic regime. We also illustrate the impressive performance of our solution numerically on a testbed of problem instances whose input parameters are drawn using publicly available healthcare data.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2278},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A near-optimal algorithm for real-time order acceptance: An application in postacute healthcare services},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—bifurcating constraints to improve
approximation ratios for network revenue management with reusable
resources. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network revenue management (NRM) describes a general online allocation problem in which combinations of capacity-constrained resources are sold to a stream of arriving customers. Existing papers propose one-size-fits-all methods for controlling the resource capacities over time. In this paper, we study how different methods can be used to control different resource constraints based on the network structure of each instance. Specifically, we propose a heuristic that “bifurcates” the resources of a given NRM instance into a structured part resembling a matroid and an unstructured part. We then derive an NRM policy with an approximation ratio of 1 / ( 2 ( 1 + L ′ ) ) , where L ′ denotes the maximum number of distinct unstructured resources required by a customer. Our approach improves theoretical and empirical performance both in applications where the structured constraints arise naturally and in randomly generated NRM instances where the structured constraints must be found by our heuristic. Along the way, our paper also provides a unified framework for analyzing NRM problems, which simplifies existing results and allows for the resources to be reusable .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2282},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Bifurcating constraints to improve approximation ratios for network revenue management with reusable resources},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network revenue management under a spiked multinomial logit
choice model. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airline booking data have shown that the fraction of customers who choose the cheapest available fare class often is much greater than that predicted by the multinomial logit choice model calibrated with the data. For example, the fraction of customers who choose the cheapest available fare class is much greater than the fraction of customers who choose the next cheapest available one, even if the price difference is small. To model this spike in demand for the cheapest available fare class, a choice model called the spiked multinomial logit (spiked-MNL) model was proposed. We study a network revenue management problem under the spiked-MNL choice model. We show that efficient sets, that is, assortments that offer a Pareto-optimal tradeoff between revenue and resource use, are nested-by-revenue when the spike effect is nonnegative. We use this result to show how a deterministic approximation of the stochastic dynamic program can be solved efficiently by solving a small linear program. The solution of the small linear program is used to construct a booking limit policy, and we prove that the policy is asymptotically optimal . This is the first such result for a booking limit policy under a choice model, and our proof uses an approach that is different from those used for previous asymptotic optimality results. Finally, we evaluate different revenue management policies in numerical experiments using both synthetic and airline data.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2281},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Network revenue management under a spiked multinomial logit choice model},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—revenue volatility under uncertain network
effects. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the revenue volatility of a monopolist selling a divisible good to consumers in the presence of local network externalities among consumers. The utility of consumers depends on their consumption level as well as those of their neighbors in a network through network externalities. In the eye of the seller, there exist uncertainties in the network externalities, which may be the result of unanticipated shocks or a lack of exact knowledge of the externalities. However, the seller has to commit to prices ex ante. We quantify the magnitude of revenue volatility under the optimal pricing in the presence of those random externalities. We consider both a given uncertainty set (from a robust optimization perspective) and a known uncertainty distribution (from a stochastic optimization perspective) and carry out the analyses separately. For a given uncertainty set, we show that the worst case of revenue fluctuation is determined by the largest eigenvalue of the matrix that represents the underlying network. Our results indicate that in networks with a smaller largest eigenvalue, the monopolist has a less volatile revenue. For the known uncertainty, we model the random noise in the form of a Wigner matrix and investigate large networks such as social networks. For such networks, we establish that the expected revenue is the sum of the revenue associated with the underlying expected network externalities and a term that depends on the noise variance and the weighted sum of all walks of different lengths in the expected network. We demonstrate that in a less connected network, the revenue is less volatile to uncertainties, and perhaps counterintuitively, the expected revenue increases with the level of uncertainty in the network. We show that a seller in the two settings favors the opposite type of network. In particular, if the underlying network is such that all the edge weights equal 1, the seller in the robust optimization setting prefers more asymmetry and the seller in the stochastic optimization setting prefers less asymmetry in the underlying network; by contrast, if the underlying network is such that the sum of all the edge weights is fixed, the seller in the robust optimization setting prefers less symmetry and the seller in the stochastic optimization setting prefers more asymmetry.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2302},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Revenue volatility under uncertain network effects},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Persuasion in networks: Public signals and cores.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a setting where agents in a social network take binary actions that exhibit local strategic complementarities. Their payoffs are affine and increasing in an underlying real-valued state of the world. An information designer commits to a signaling mechanism that publicly reveals a signal that is potentially informative about the state. She wants to maximize the expected number of agents who take action 1. We study the structure and design of optimal public signaling mechanisms. The designer’s payoff is an increasing step function of the posterior mean (of the state) induced by the realization of her signal. We provide a convex optimization formulation and an algorithm that obtain an optimal public signaling mechanism whenever the designer’s payoff admits this structure. This structure is prevalent, making our formulation and results useful well beyond persuasion in networks. In our problem, the step function is characterized in terms of the cores of the underlying network. The optimal mechanism is based on a “double-interval partition” of the set of states: it associates up to two subintervals of the set of states with each core, and when the state realization belongs to the interval(s) associated with a core, the mechanism publicly reveals this fact. In turn, this induces the agents in the relevant core to take action 1. We also provide a framework for obtaining asymptotically optimal public signaling mechanisms for a class of random networks. Our approach uses only the limiting degree distribution information, thereby making it useful even when the network structure is not fully known. Finally, we explore which networks are more amenable to persuasion, and show that more assortative connection structures lead to larger payoffs for the designer. Conversely, the dependence of the designer’s payoff on the agents’ degrees can be quite counterintuitive. In particular, we focus on networks sampled uniformly at random from the set of all networks consistent with a degree sequence and illustrate that when the degrees of some nodes increase, this can reduce the designer’s expected payoff, despite an increase in the extent of (positive) network externalities.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2246},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Persuasion in networks: Public signals and cores},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic double auctions: Toward first best. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of designing dynamic double auctions for two-sided markets in which a platform intermediates the trade between one seller offering independent items to multiple buyers, repeatedly over a finite horizon, when agents have private values. Motivated by online platforms for advertising, ride-sharing, and freelancing markets, we seek to design mechanisms satisfying the following properties: no positive transfers , that is, the platform never asks the seller to make payments nor are buyers ever paid, and periodic individual rationality , that is, every agent derives a nonnegative utility from every trade opportunity. We provide mechanisms satisfying these requirements that are asymptotically efficient and budget balanced with high probability as the number of trading opportunities grows. Our mechanisms thus overcome well-known impossibility results preventing efficient bilateral trade without subsidies in static environments. Moreover, we show that the average expected profit obtained by the platform under these mechanisms asymptotically approaches “first best” (the maximum possible welfare generated by the market). We also extend our approach to general environments with complex, combinatorial preferences.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2266},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Dynamic double auctions: Toward first best},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seeding with costly network information. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the task of selecting k nodes, in a social network of size n , to seed a diffusion with maximum expected spread size, under the independent cascade model with cascade probability p . Most of the previous work on this problem (known as influence maximization) focuses on efficient algorithms to approximate the optimal seed set with provable guarantees given knowledge of the entire network; however, obtaining full knowledge of the network is often very costly in practice. Here we develop algorithms and guarantees for approximating the optimal seed set while bounding how much network information is collected. First, we study the achievable guarantees using a sublinear influence sample size. We provide an almost tight approximation algorithm with an additive ε n loss and show that the squared dependence of sample size on k is asymptotically optimal when ε is small. We then propose a probing algorithm that queries edges from the graph and use them to find a seed set with the same almost tight approximation guarantee. We also provide a matching (up to logarithmic factors) lower-bound on the required number of edges. This algorithm is implementable in field surveys or in crawling online networks. Our probing takes p as an input which may not be known in advance, and we show how to down-sample the probed edges to match the best estimate of p if they are collected with a higher probability. Finally, we test our algorithms on an empirical network to quantify the tradeoff between the cost of obtaining more refined network information and the benefit of the added information for guiding improved seeding strategies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2290},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Seeding with costly network information},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic stochastic matching under limited time. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2022.2293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In centralized matching markets such as carpooling platforms and kidney exchange schemes, new participants constantly enter the market and remain available for potential matches during a limited period of time. To reach an efficient allocation, the “timing” of the matching decisions is a critical aspect of the platform’s operations. There is a fundamental tradeoff between increasing market thickness and mitigating the risk that participants abandon the market. Nonetheless, the dynamic properties of matching markets have been mostly overlooked in the algorithmic literature. In this paper, we introduce a general dynamic matching model over edge-weighted graphs, where the agents’ arrivals and abandonments are stochastic and heterogeneous. Our main contribution is to design simple matching algorithms that admit strong worst-case performance guarantees for a broad class of graphs. In contrast, we show that the performance of widely used batching algorithms can be arbitrarily bad on certain graph-theoretic structures motivated by carpooling services. Our approach involves the development of a host of new techniques, including linear programming benchmarks, value function approximations, and proxies for continuous-time Markov chains, which may be of broader interest. In extensive experiments, we simulate the matching operations of a car-pooling platform using real-world taxi demand data. The newly developed algorithms can significantly improve cost efficiency against batching algorithms.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2293},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Dynamic stochastic matching under limited time},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning markov models via low-rank optimization.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling unknown systems from data is a precursor of system optimization and sequential decision making. In this paper, we focus on learning a Markov model from a single trajectory of states. Suppose that the transition model has a small rank despite having a large state space, meaning that the system admits a low-dimensional latent structure. We show that one can estimate the full transition model accurately using a trajectory of length that is proportional to the total number of states. We propose two maximum-likelihood estimation methods: a convex approach with nuclear norm regularization and a nonconvex approach with rank constraint. We explicitly derive the statistical rates of both estimators in terms of the Kullback-Leiber divergence and the ℓ 2 ℓ 2 ℓ2 error and also establish a minimax lower bound to assess the tightness of these rates. For computing the nonconvex estimator, we develop a novel DC (difference of convex function) programming algorithm that starts with the convex M-estimator and then successively refines the solution till convergence. Empirical experiments demonstrate consistent superiority of the nonconvex estimator over the convex one.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2115},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Learning markov models via low-rank optimization},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Casting light on the hidden bilevel combinatorial structure
of the capacitated vertex separator problem. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an undirected graph, we study the capacitated vertex separator problem that asks to find a subset of vertices of minimum cardinality, the removal of which induces a graph having a bounded number of pairwise disconnected shores (subsets of vertices) of limited cardinality. The problem is of great importance in the analysis and protection of communication or social networks against possible viral attacks and for matrix decomposition algorithms. In this article, we provide a new bilevel interpretation of the problem and model it as a two-player Stackelberg game in which the leader interdicts the vertices (i.e., decides on the subset of vertices to remove), and the follower solves a combinatorial optimization problem on the resulting graph. This approach allows us to develop a computational framework based on an integer programming formulation in the natural space of the variables. Thanks to this bilevel interpretation, we derive three different families of strengthening inequalities and show that they can be separated in polynomial time. We also show how to extend these results to a min-max version of the problem. Our extensive computational study conducted on available benchmark instances from the literature reveals that our new exact method is competitive against the state-of-the-art algorithms for the capacitated vertex separator problem and is able to improve the best-known results for several difficult classes of instances. The ideas exploited in our framework can also be extended to other vertex/edge deletion/insertion problems or graph partitioning problems by modeling them as two-player Stackelberg games and solving them through bilevel optimization.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2110},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Casting light on the hidden bilevel combinatorial structure of the capacitated vertex separator problem},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smart policies for multisource inventory systems and general
tandem queues with order tracking and expediting. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an inventory system with multiple supply sources and expediting options. The replenishment lead times from each supply source are stochastic, representing congestion and disruption. We construct a family of smart ordering and expediting policies that utilize real-time supply information. Such dynamic policies are generally difficult to evaluate, because the corresponding supply system is a tandem queue with state-dependent arrivals and routing, whose queue-length steady-state distribution is usually not in product form. Our main result is to identify two appealing special cases of the general policy, Policy-M and Policy-E, which possess simple product-form solutions and lead to closed-form performance measures. Policy-M retains full sourcing flexibility, but ignores upstream congestion in making expediting decisions. Policy-E only orders from the normal, farthest source, but makes expediting decisions based on both upstream and downstream information. A numerical study shows that the best Policy-M leads to a lower average cost than the best Policy-E in almost all cases. Also, implementing the best Policy-M parameters, the general policy only performs slightly better than Policy-M. These observations reveal the value of combining sourcing flexibility with some, but limited, dynamic expediting. Our findings are equally applicable to the equivalent tandem queue. They thus may aid dynamic routing and expediting decisions for online retailers and logistics providers, among others.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2124},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Smart policies for multisource inventory systems and general tandem queues with order tracking and expediting},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-averse stochastic programming: Time consistency and
optimal stopping. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses time consistency of risk-averse optimal stopping in stochastic optimization. It is demonstrated that time-consistent optimal stopping entails a specific structure of the functionals describing the transition between consecutive stages. The stopping risk measures capture this structural behavior and allow natural dynamic equations for risk-averse decision making over time. Consequently, associated optimal policies satisfy Bellman’s principle of optimality, which characterizes optimal policies for optimization by stating that a decision maker should not reconsider previous decisions retrospectively. We also discuss numerical approaches to solving such problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2120},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Risk-averse stochastic programming: Time consistency and optimal stopping},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stability of parallel server systems. <em>OR</em>,
<em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental problem in the study of parallel-server systems is that of finding and analyzing routing policies of arriving jobs to the servers that efficiently balance the load on the servers. The most well-studied policies are (in decreasing order of efficiency) join the shortest workload (JSW), which assigns arrivals to the server with the least workload; join the shortest queue (JSQ), which assigns arrivals to the smallest queue; the power-of- d (PW( d )), which assigns arrivals to the shortest among d ≥ 1 queues that are sampled from the total of s queues uniformly at random; and uniform routing, under which arrivals are routed to one of the s queues uniformly at random. In this paper we study the stability problem of parallel-server systems, assuming that routing errors may occur, so that arrivals may be routed to the wrong queue (not the smallest among the relevant queues) with a positive probability. We treat this routing mechanism as a probabilistic routing policy, named a p -allocation policy, that generalizes the PW( d ) policy, and thus also the JSQ and uniform routing, where p is an s -dimensional vector whose components are the routing probabilities. Our goal is to study the (in)stability problem of the system under this routing mechanism, and under its “nonidling” version, which assigns new arrivals to an idle server, if such a server is available, and otherwise routes according to the p -allocation rule. We characterize a sufficient condition for stability, and prove that the stability region, as a function of the system’s primitives and p , is in general smaller than the set { ρ &lt; 1 } . Our analyses build on representing the queue process as a continuous-time Markov chain in an ordered space of s -dimensional real-valued vectors, and using a generalized form of the Schur-convex order.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2125},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Stability of parallel server systems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A repeated route-then-schedule approach to coordinated
vehicle platooning: Algorithms, valid inequalities and computation.
<em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platooning of vehicles is a promising approach for reducing fuel consumption, increasing vehicle safety, and using road space more efficiently. We consider the important, but difficult, problem of assigning optimal routes and departure schedules to a collection of vehicles. We propose an iterative route-then-schedule heuristic for centralized planning that quickly converges to high-quality solutions. We also propose and analyze a collection of valid inequalities for the individual problems of assigning vehicles to routes and scheduling the times that vehicles traverse their routes. These inequalities are shown to reduce the computational time or optimality gap of solving the routing and scheduling problem instances. Our approach uses the valid inequalities in both the routing and scheduling portions of each iteration; numerical experiments highlight the speed of the approach for routing vehicles on a real-world road network.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2126},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {A repeated route-then-schedule approach to coordinated vehicle platooning: Algorithms, valid inequalities and computation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting random lead times for significant inventory cost
savings. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the classical single-item inventory system in which unsatisfied demands are backlogged. Replenishment lead times are random, independent identically distributed, causing orders to cross in time. We develop a new inventory policy to exploit implications of lead time randomness and order crossover, and evaluate its performance by asymptotic analysis and simulations. Our policy does not follow the basic principle of constant base stock (CBS) policy, or more generally, ( s , S ) and ( R , q ) policies, which is to keep the inventory position within a fixed range. Instead, it uses the current inventory level (= inventory-on-hand minus backlog) to set a dynamic target for inventory in-transit, and place orders to follow this target. Our policy includes CBS policy as a special case, under a particular choice of a policy parameter. We show that our policy can significantly reduce the average inventory cost compared with CBS policy. Specifically, we prove that if the lead time is exponentially distributed, then under our policy, with properly chosen policy parameters, the expected (absolute) inventory level scales as o ( r ) , as the demand rate r → ∞ . In comparison, it is known to scale as Θ ( r ) under CBS policy. In particular, this means that, as r → ∞ , the average inventory cost under our policy vanishes in comparison with that under CBS policy. Furthermore, our simulations show that the advantage of our policy remains to be substantial under nonexponential lead time distributions, and may even be greater than under exponential distribution. We also use simulations to compare the average cost under our policy with that achieved under an optimal policy for some cases where computing the optimal cost is tractable. The results show that our policy removes a majority of excess costs of CBS policy over the minimum cost, leading to much smaller optimality gaps.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2129},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Exploiting random lead times for significant inventory cost savings},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic resource allocation in the cloud with near-optimal
efficiency. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has motivated renewed interest in resource allocation problems with new consumption models. A common goal is to share a resource, such as CPU or I/O bandwidth, among distinct users with different demand patterns as well as different quality of service requirements. To ensure these service requirements, cloud offerings often come with a service level agreement (SLA) between the provider and the users. A SLA specifies the amount of a resource a user is entitled to utilize. In many cloud settings, providers would like to operate resources at high utilization while simultaneously respecting individual SLAs. There is typically a trade-off between these two objectives; for example, utilization can be increased by shifting away resources from idle users to “scavenger” workload, but with the risk of the former then becoming active again. We study this fundamental tradeoff by formulating a resource allocation model that captures basic properties of cloud computing systems, including SLAs, highly limited feedback about the state of the system, and variable and unpredictable input sequences. Our main result is a simple and practical algorithm that achieves near-optimal performance on the above two objectives. First, we guarantee nearly optimal utilization of the resource even if compared with the omniscient offline dynamic optimum. Second, we simultaneously satisfy all individual SLAs up to a small error. The main algorithmic tool is a multiplicative weight update algorithm and a primal-dual argument to obtain its guarantees. We also provide numerical validation on real data to demonstrate the performance of our algorithm in practical applications.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2138},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Dynamic resource allocation in the cloud with near-optimal efficiency},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantile inverse optimization: Improving stability in
inverse linear programming. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse linear programming (LP) has received increasing attention because of its potential to infer efficient optimization formulations that can closely replicate the behavior of a complex system. However, inversely inferred parameters and corresponding forward solutions from the existing inverse LP methods can be highly sensitive to noise, errors, and uncertainty in the input data, limiting their applicability in data-driven settings. We introduce the notion of inverse and forward stability in inverse LP and propose a novel inverse LP method that determines a set of objective functions that are stable under data imperfection and generate forward solutions close to the relevant subset of the data. We formulate the inverse model as a large-scale mixed-integer program (MIP) and elucidate its connection to biclique problems, which we exploit to develop efficient algorithms that solve much smaller MIPs instead to construct a solution to the original problem. We numerically evaluate the stability of the proposed method and demonstrate its use in the diet recommendation and transshipment applications.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2143},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Quantile inverse optimization: Improving stability in inverse linear programming},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast global convergence of natural policy gradient methods
with entropy regularization. <em>OR</em>, <em>70</em>(4), iii–vii. (<a
href="https://doi.org/10.1287/opre.2021.2151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural policy gradient (NPG) methods are among the most widely used policy optimization algorithms in contemporary reinforcement learning. This class of methods is often applied in conjunction with entropy regularization—an algorithmic scheme that encourages exploration—and is closely related to soft policy iteration and trust region policy optimization. Despite the empirical success, the theoretical underpinnings for NPG methods remain limited even for the tabular setting. This paper develops nonasymptotic convergence guarantees for entropy-regularized NPG methods under softmax parameterization, focusing on discounted Markov decision processes (MDPs). Assuming access to exact policy evaluation, we demonstrate that the algorithm converges linearly—even quadratically, once it enters a local region around the optimal policy—when computing optimal value functions of the regularized MDP. Moreover, the algorithm is provably stable vis-à-vis inexactness of policy evaluation. Our convergence results accommodate a wide range of learning rates and shed light upon the role of entropy regularization in enabling fast convergence.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2151},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {Fast global convergence of natural policy gradient methods with entropy regularization},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A fluid-diffusion-hybrid limiting approximation for priority
systems with fast and slow customers. <em>OR</em>, <em>70</em>(4),
iii–vii. (<a href="https://doi.org/10.1287/opre.2021.2154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a large service system with two customer classes that are distinguished by their urgency and service requirements. In particular, one of the customer classes is considered urgent, and is therefore prioritized over the other class; further, the average service time of customers from the urgent class is significantly larger than that of the nonurgent class. We therefore refer to the urgent class as “slow,” and to the nonurgent class as “fast.” Due to the complexity and intractability of the system’s dynamics, our goal is to develop and analyze an asymptotic approximation, which captures the prevalent fact that, in practice, customers from both classes are likely to experience delays before entering service. However, under existing many-server limiting regimes, only two of the following options can be captured in the limit: (i) either the customers from the prioritized (slow) customer class do not wait at all, or (ii) the fast-class customers do not receive any service. We therefore propose a novel Fluid-Diffusion Hybrid (FDH) many-server asymptotic regime, under which the queue of the slow class behaves like a diffusion limit, whereas the queue of the fast class evolves as a (random) fluid limit that is driven by the diffusion process. That FDH limit is achieved by assuming that the service rate of the fast class scales with the system’s size, whereas the service rate of the slow class is kept fixed. Numerical examples demonstrate that our FDH limit is accurate when the difference between the service rates of the two classes is sufficiently large. We then employ the FDH approximation to study the costs and benefits of de-pooling the service pool, by reserving a small number of servers for the fast class. We prove that, in some cases, a two-pool structure is the asymptotically optimal system design.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2154},
  journal      = {Operations Research},
  number       = {4},
  pages        = {iii-vii},
  shortjournal = {Oper. Res.},
  title        = {A fluid-diffusion-hybrid limiting approximation for priority systems with fast and slow customers},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introduction: Special issue honoring kenneth arrow.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2022.2296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When our successors write about the first century of Operations Research, the name of Kenneth Arrow will be in lights. It is fitting that at this time, not long after his death, that we take a moment to consider Ken Arrow’s academic legacy. This introduction to the Special Issue honoring Ken Arrow highlights his contributions in the field of Operations Research and summarizes the papers published in the special issue that speak to his legacy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2296},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Introduction: Special issue honoring kenneth arrow},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Majority judgment vs. Approval voting. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Majority judgment (MJ) and approval voting (AV) are compared in theory and practice. Criticisms of MJ and claims that AV is superior are refuted. The two primary criticisms have been that MJ is not “Condorcet consistent” and that it admits the “no-show paradox.” That MJ is not Condorcet consistent is a good property shared with AV: the domination paradox shows that majority rule may well err in an election between two. Whereas the no-show paradox is in theory possible with MJ, it is as a practical matter impossible. For those who believe that this extremely rare phenomenon is important, it is proven that MJ with three grades cannot admit the no-show paradox. In contrast, AV suffers from serious drawbacks because voters can only “tick” or “approve” candidates—at best, only Approve or Disapprove each candidate. With AV, voters cannot express their opinions adequately; experiments show that Approve is not the opposite of Disapprove , and although AV does not admit the no-show paradox, it admits the very closely allied no-show syndrome and “insensitivity.” Two are too few. Substantive debate must concern three or more grades.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1877},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Majority judgment vs. approval voting},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal learning under robustness and time-consistency.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model learning in a continuous-time Brownian setting where there is prior ambiguity. The associated model of preference values robustness and is time-consistent. It is applied to study optimal learning when the choice between actions can be postponed, at a per-unit-time cost, in order to observe a signal that provides information about an unknown parameter. The corresponding optimal stopping problem is solved in closed form, with a focus on two specific settings: Ellsberg’s two-urn thought experiment expanded to allow learning before the choice of bets, and a robust version of the classical problem of sequential testing of two simple hypotheses about the unknown drift of a Wiener process. In both cases, the link between robustness and the demand for learning is studied.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1899},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Optimal learning under robustness and time-consistency},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual moments and risk attitudes. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In decision under risk, the primal moments of mean and variance play a central role to define the local index of absolute risk aversion. In this paper, we show that in the canonical nonexpected utility models provided by the dual theory and rank-dependent utility, dual moments have to be used instead of, or on par with, their primal counterparts to obtain an equivalent index of absolute risk aversion.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2040},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Dual moments and risk attitudes},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Axiomatizing the bayesian paradigm in parallel small worlds.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is currently much interest in scenario-focused decision analysis (SFDA), a methodology that provides, among other things, supporting analyses in circumstances in which there are deep uncertainties about the future (that is, when experts and decision makers (DMs) cannot come to any agreement on some of the probabilities to use in a Bayesian model). This lack of agreement can mean that sensitivity and robustness analyses show that virtually any strategy may be optimal under the beliefs of one or more participants. Scenario-focused analyses fix the deep uncertainties at interesting values in different scenarios and conduct a (Bayesian) decision analysis within each. The results can be informative to the DMs, helping them understand different possible futures and their reactions to them. However, theoretical axiomatizations of subjective expected utility (SEU), the core of decision analysis, do not immediately extend to the context of SFDA. The purpose of this paper is to provide an axiomatization of SEU that supports SFDA. Scenarios have much in common with Savage’s concept of small worlds . We discuss the parallels and then explore two difficulties in extending his and other writers’ axiomatizations. The development of SEU offered here overcomes these difficulties. Throughout, attention is given to the implications of the theoretical development for the practice of decision analysis.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1896},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Axiomatizing the bayesian paradigm in parallel small worlds},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Game of variable contributions to the common good under
uncertainty. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a stochastic game of contribution to the common good in which the players have continuous control over the degree of contribution, and we examine the gradualism arising from the free rider effect. This game belongs to the class of variable concession games that generalize wars of attrition. Previously known examples of variable concession games in the literature yield equilibria characterized by singular control strategies without any delay of concession. However, these no-delay equilibria are in contrast to mixed-strategy equilibria of canonical wars of attrition in which each player delays concession by a randomized time. We find that a variable contribution game with a single state variable, which extends the Nerlove–Arrow model, possesses an equilibrium characterized by regular control strategies that result in a gradual concession. This equilibrium naturally generalizes the mixed-strategy equilibria from the canonical wars of attrition. Stochasticity of the problem accentuates the qualitative difference between a singular control solution and a regular control equilibrium solution. We also find that asymmetry between the players can mitigate the inefficiency caused by the gradualism.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1879},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Game of variable contributions to the common good under uncertainty},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When risk perception gets in the way: Probability weighting
and underprevention. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personal decisions about health hazards are the main cause of impaired health and premature death. People smoke and eat too much, and they exercise too little. The lack of preventive efforts is surprising given their proven effectiveness. In the early 1960s, Arrow suggested that moral hazard might be a reason for underprevention, but this explanation was later challenged. In this paper, we show that underprevention might be caused by misperceived probabilities. We derive when and how probability weighting gets in the way of prevention by blurring its benefits. We use a general model of prevention, encompassing several special cases from the literature. We also show how perceived ambiguity makes the problem of underprevention even worse by amplifying the effect of probability weighting.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1910},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {When risk perception gets in the way: Probability weighting and underprevention},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Search under accumulated pressure. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrow et al. [Arrow K, Blackwell D, Girshick M (1949) Bayes and minimax solutions of sequential decision problems. Econometrica 17(3/4):213–244.] introduced the first sequential search problem “where at each stage the options available are to stop and take a definite action or to continue sampling for more information.” We study how time pressure in the form of task accumulation may affect this decision problem. To that end, we consider a search problem where the decision maker (DM) faces a stream of random decision tasks to be treated one at a time that accumulate when not attended to. We formulate the problem of managing this form of pressure as a partially observable Markov decision process and characterize the corresponding optimal policy. We find that the DM needs to alleviate this pressure very differently depending on how the search on the current task has unfolded thus far. As the search progresses, the DM is less and less willing to sustain high levels of workloads in the beginning and end of the search but actually increases the maximum workload that she is willing to handle in the middle of the process. The DM manages this workload first by making a priori decisions to release some accumulated tasks and later, by aborting the current search and deciding based on her updated belief. This novel search strategy critically depends on the DM’s prior belief about the tasks and stems, in part, from an effect related to the decision ambivalence. These findings are robust to various extensions of our basic setup.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1880},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Search under accumulated pressure},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The strategic benefit of request for proposal/quotation.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how the procurement process may help a buyer reduce purchasing costs under a dual-sourcing setting. First, we analyze the equilibrium outcomes when the buyer simultaneously or sequentially bilaterally negotiates with the suppliers. We establish coordination results and characterize the buyer’s maximum equilibrium profit under such bilateral bargaining settings. Moreover, we show that the buyer can benefit from a request for proposal/quotation (RFx) stage that precedes the negotiation stage when the suppliers are imperfect substitutes. Specifically, by endogenizing the sequence of negotiations via the offers tendered in the RFx stage, the buyer’s equilibrium profit with an RFx is (weakly) higher than his or her maximum equilibrium profit without an RFx. Furthermore, under a complete information setting, the buyer accepts both offers generated in the RFx stage and additional negotiation needs not to be carried out in equilibrium. Our insights extend to a random demand setting, in which the buyer first negotiates contracts with the suppliers and then decides order quantities after demand realization.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1964},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {The strategic benefit of request for Proposal/Quotation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantile markov decision processes. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of a traditional Markov decision process (MDP) is to maximize expected cumulative reward over a defined horizon (possibly infinite). In many applications, however, a decision maker may be interested in optimizing a specific quantile of the cumulative reward instead of its expectation. In this paper, we consider the problem of optimizing the quantiles of the cumulative rewards of an MDP, which we refer to as a quantile Markov decision process (QMDP). We provide analytical results characterizing the optimal QMDP value function and present a dynamic programming-based algorithm to solve for the optimal policy. The algorithm also extends to the MDP problem with a conditional value-at-risk objective. We illustrate the practical relevance of our model by evaluating it on an HIV treatment initiation problem, in which patients aim to balance the potential benefits and risks of the treatment.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2123},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Quantile markov decision processes},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact penalization of generalized nash equilibrium problems.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an exact penalization theory of the generalized Nash equilibrium problem (GNEP) that has its origin from the renowned Arrow–Debreu general economic equilibrium model. Whereas the latter model is the foundation of much of mathematical economics, the GNEP provides a mathematical model of multiagent noncooperative competition that has found many contemporary applications in diverse engineering domains. The most salient feature of the GNEP that distinguishes it from a standard noncooperative (Nash) game is that each player’s optimization problem contains constraints that couple all players’ decision variables. Extending results for stand-alone optimization problems, the penalization theory aims to convert the GNEP into a game of the standard kind without the coupled constraints, which is known to be more readily amenable to solution methods and analysis. Starting with an illustrative example to motivate the development, this paper focuses on two kinds of coupled constraints, shared (i.e., common) and finitely representable. Constraint residual functions and the associated error bound theory play an important role throughout the development.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1942},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Exact penalization of generalized nash equilibrium problems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On solving a class of continuous traffic equilibrium
problems and planning facility location under congestion. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents methods to obtain analytical solutions to a class of continuous traffic equilibrium problems, where continuously distributed customers from a bounded two-dimensional service region seek service from one of several discretely located facilities via the least congested travel path. We show that under certain conditions, the traffic flux at equilibrium, which is governed by a set of partial differential equations, can be decomposed with respect to each facility and solved analytically. This finding paves the foundation for an efficient solution scheme. Closed-form solution to the equilibrium problem can be obtained readily when the service region has a certain regular shape, or through an additional conformal mapping if the service region has an arbitrary simply connected shape. These results shed light on some interesting properties of traffic equilibrium in a continuous space. This paper also discusses how service facility locations can be easily optimized by incorporating analytical formulas for the total generalized cost of spatially distributed customers under congestion. Examples of application contexts include gates or booths for pedestrian traffic, as well as launching sites for air vehicles. Numerical examples are used to show the superiority of the proposed optimization framework, in terms of both solution quality and computation time, as compared with traditional approaches based on discrete mathematical programming and partial differential equation solution methods. An example with the metro station entrances at the Beijing Railway Station is also presented to illustrate the usefulness of the proposed traffic equilibrium and location design models.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2213},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {On solving a class of continuous traffic equilibrium problems and planning facility location under congestion},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conveying demand information in serial supply chains with
capacity limits. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For serial multiechelon systems subject to production capacity limits at every stage, we consider a class of modified echelon base stock ( MEBS ) policies. To evaluate information requirements of such systems, we consider two separate inventory management mechanisms operated in a decentralized manner. For ordering decisions, these mechanisms utilize local knowledge only and are distinguished by the timing of the orders being conveyed upstream from installation to installation. We demonstrate that these mechanisms can duplicate the shipment quantities in the modified echelon base-stock policy that uses full information. Thus, although full demand information will not be conveyed up the channel due to the demand censoring effects of capacity, we demonstrate that sufficient information about the market demand is conveyed via the orders. This suggests that local information is sufficient to make ordering decisions that replicate the policy’s orders, a significant finding for implementing supply chain inventory policies in practice, where dynamic state information may not be readily accessible. We extend this local information result to serial channels with completely general capacity configurations acting under the corresponding echelon policies. We demonstrate the strong relationship between these two mechanisms that relate to serial capacitated channels of differing lengths. We augment our main results with two important extensions. (1) Because our focus is on MEBS policies, which are not necessarily optimal for longer supply chains, we evaluate their performance. We numerically show that they perform very well in general. We also provide upper and lower bounds, which further justify their strong performance. Given that these policies are close to optimal, that they are easy to interpret, and that they can operate with only local information, they are appealing in practical applications. (2) We compare the usage of local information to operate a capacitated system versus incentives to sustain such a system. We show that, similar to the noncapacitated case, it is possible to design an alternative incentive-compatible performance mechanism such that local managers will follow the centralized solution, albeit with more demanding information requirements.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2251},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Conveying demand information in serial supply chains with capacity limits},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parallel innovation contests. <em>OR</em>, <em>70</em>(3),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study multiple parallel contests in which contest organizers elicit solutions to innovation-related problems from a set of solvers. Each solver may participate in multiple contests and exert effort to improve the solution for each contest the solver enters, but the quality of the solver’s solution in each contest also depends on an output uncertainty. We first analyze whether an organizer’s profit can be improved by discouraging solvers from participating in multiple contests. We show, interestingly, that organizers benefit from solvers participating in multiple contests when the solver’s output uncertainty in these contests is sufficiently large. A managerial insight from this result is that, when all organizers are eliciting innovative solutions rather than low-novelty solutions, they may benefit from solvers participating in multiple contests. We also show that organizers’ average profit increases when solvers participate in multiple contests even when some contests seek low-novelty solutions as long as other contests seek cutting-edge innovation. We further show that an organizer’s profit is unimodal in the number of contests, and the optimal number of contests increases with the solver’s output uncertainty. This finding may explain why many organizations run multiple contests in practice, and it suggests running a larger number of contests when the majority of these organizations are seeking innovative solutions rather than low-novelty solutions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2250},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Parallel innovation contests},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simultaneous magnanti-wong method to accelerate benders
decomposition for the metropolitan container transportation problem.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most Australian cities, container ports are located close to the city, with transportation to and from the port facilitated by trucks. Recently, with a view to reducing container-truck induced city congestion and pollution, state and federal governments have begun championing a modal switch to short-haul rail for these transportation tasks. In this paper, we describe a metropolitan container transportation problem arising from this context that seeks to effectively leverage both modes of transport from a least-cost perspective. We propose a mathematical programming formulation and develop a new modified Benders decomposition method for the problem. We show that the simultaneous Magnanti-Wong method finds Pareto-optimal cuts by solving an augmented version of the subproblem that exploits subproblem dual-degeneracy without destroying its underlying structure. Computational results demonstrate the effectiveness of this routine over the performance of commercial solver implementations of the mathematical programming formulation.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2032},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {A simultaneous magnanti-wong method to accelerate benders decomposition for the metropolitan container transportation problem},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic inventory control with fixed setup costs and unknown
discrete demand distribution. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2022.2272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a dynamic inventory control problem involving fixed setup costs and random demand distributions. With an infinite planning horizon, model primitives including costs and distributions are set to be stationary. Under a given demand distribution, an ( s , S ) policy has been known to minimize the long-run per-period average cost. Out of the need to model situations involving new products or unencountered economic conditions, however, we depart from the traditional model by allowing the stationary demand distribution to be largely unknown, to the effect that it could be anywhere in a given ambiguity set. Our goal is to rein in the long-run growth of the regret resulting from applying a policy that strives to learn the underlying demand while simultaneously meting out ordering decisions based on its learning. We propose a policy that controls the pace at which a traditional ( s , S )-computing algorithm is applied to the empirical distribution of the demand learned over time. The regret incurred from the policy has a bound of O ( T 1 / 2 · ( ln T ) 1 / 2 ) .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2272},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Dynamic inventory control with fixed setup costs and unknown discrete demand distribution},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Misinformation and disinformation in modern warfare.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced information technology has changed the landscape of modern warfare, as it facilitates communication among nonconventional actors such as violent extremist groups. This paper examines the value of misinformation and disinformation to a military leader who through investment in people, programs, and technology is able to affect the accuracy of information communicated between other actors. We model the problem as a partially observable stochastic game with three agents, a leader and two followers. We determine the value to the leader of misinformation or disinformation being communicated between two: (i) adversarial and (ii) allied followers. We demonstrate that only under certain conditions, the prevalent intuition that the leader would benefit from less (more) accurate communication between adversarial (allied) followers is valid. We discuss why the intuition may fail and show the necessity of embracing both the reward structures and policies of agents to correctly manage information. Our research identifies efficient targeted investments to affect the accuracy of information in communication to the leader’s advantage. We demonstrate the application of the developed methodology to warfare situations in the Battle of Mosul.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2253},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Misinformation and disinformation in modern warfare},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systemic risk-driven portfolio selection. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an investor who trades off tail risk and expected growth of the investment. We measure tail risk through the portfolio’s expected losses conditioned on the occurrence of a systemic event: financial market loss being exactly at, or at least at, its value-at-risk (VaR) level and investor’s portfolio losses being above their conditional value-at-risk (CoVaR) level. We decompose the solution to the investment problem in terms of the Markowitz mean-variance portfolio and an adjustment for systemic risk. We show that VaR and CoVaR confidence levels control the relative sensitivity of the investor’s objective function to portfolio-market correlation and portfolio variance, respectively. Our empirical analysis demonstrates that the investor attains higher risk-adjusted returns, compared with well-known benchmark portfolio criteria, during times of market downturn. Portfolios that perform best under adverse market conditions are less diversified and invest on a few stocks that have low correlation with the market.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2234},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Systemic risk-driven portfolio selection},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous assortment optimization with logit choice
probabilities and incomplete information. <em>OR</em>, <em>70</em>(3),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider assortment optimization over a continuous spectrum of products represented by the unit interval, where the seller’s problem consists of determining the optimal subset of products to offer to potential customers. To describe the relation between assortment and customer choice, we propose a probabilistic choice model that forms the continuous counterpart of the widely studied discrete multinomial logit model. We consider the seller’s problem under incomplete information, propose a stochastic-approximation type of policy, and show that its regret, its performance loss compared with the optimal policy, is only logarithmic in the time horizon. We complement this result by showing a matching lower bound on the regret of any policy, implying that our policy is asymptotically optimal. We then show that adding a capacity constraint significantly changes the structure of the problem: we construct a policy and show that its regret after T time periods is bounded above by a constant times T 2 / 3 (up to a logarithmic term); in addition, we show that the regret of any policy is bounded from below by a positive constant times T 2 / 3 , so that also in the capacitated case, we obtain asymptotic optimality. Numerical illustrations show that our policies outperform or are on par with alternatives.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2235},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Continuous assortment optimization with logit choice probabilities and incomplete information},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal price/advertising menus for two-sided media
platforms. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider ad-supported media platforms with incomplete information about the disutility consumers’ experience from exposure to advertising. We characterize the platform’s optimal menu of subscription prices and advertising quantities in monopoly and competitive settings, revealing insights on key factors influencing market outcomes. In particular, we show that incomplete information on advertising disutility decreases the optimal subscription price for consumers with low advertising disutility while also decreasing the optimal advertising quantity for consumers with high advertising disutility, suggestive of the “free use with ads” or “paid use without ads” menu pricing observed in media streaming markets. We also demonstrate that competition improves prices more for high-disutility consumers than low-disutility consumers, and in some settings, competition may decrease prices for high types while increasing prices for low types. Further, we characterize the value of offering a menu of differentiated prices to the consumer, relative to offering a single price and show that competition can make this value higher, suggesting that platforms may have more incentive to adopt menu pricing in competitive markets. We establish these results using a Lagrangian dual approach, allowing us to systematically analyze a multiplicity of constraints in the platform’s optimization problem arising from the consumer’s endogenous homing decision.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2230},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Optimal Price/Advertising menus for two-sided media platforms},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning in structured MDPs with convex cost functions:
Improved regret bounds for inventory management. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2022.2263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a stochastic inventory control problem under censored demand, lost sales, and positive lead times. This is a fundamental problem in inventory management, with significant literature establishing near optimality of a simple class of policies called “base-stock policies” as well as the convexity of long-run average cost under those policies. We consider a relatively less studied problem of designing a learning algorithm for this problem when the underlying demand distribution is unknown. The goal is to bound the regret of the algorithm when compared with the best base-stock policy. Our main contribution is a learning algorithm with a regret bound of O ˜ ( ( L + 1 ) T + D ) for the inventory control problem. Here, L ≥ 0 is the fixed and known lead time, and D is an unknown parameter of the demand distribution described roughly as the expected number of time steps needed to generate enough demand to deplete one unit of inventory. Notably, our regret bounds depend linearly on L , which significantly improves the previously best-known regret bounds for this problem where the dependence on L was exponential. Our techniques utilize the convexity of the long-run average cost and a newly derived bound on the “bias” of base-stock policies to establish an almost black box connection between the problem of learning in Markov decision processes (MDPs) with these properties and the stochastic convex bandit problem. The techniques presented here may be of independent interest for other settings that involve large structured MDPs but with convex asymptotic average cost functions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2263},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Learning in structured MDPs with convex cost functions: Improved regret bounds for inventory management},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Transparency and control in platforms for networked markets.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyze the worst-case efficiency loss of online platform designs under a networked Cournot competition model. Inspired by some of the largest platforms in operation today, we study a variety of platform designs to examine the impacts of market transparency and control on the worst-case efficiency loss of Nash equilibria in networked Cournot games. Our results show that open access designs incentivize increased production toward perfectly competitive levels and limit efficiency loss, while controlled allocation designs lead to producer-platform incentive misalignment, resulting in low participation rates and unbounded efficiency loss. We also show that discriminatory access designs balance transparency and control, achieving the best of both worlds by maintaining high participation rates while limiting efficiency loss.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2244},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Transparency and control in platforms for networked markets},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of incentive programs for optimal medication
adherence in the presence of observable consumption. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Premature cessation of antibiotic therapy (nonadherence) is common in long treatment regimens and can severely compromise health outcomes. In this work, we investigate the problem of designing a schedule of incentive payments to induce socially optimal treatment adherence levels in a setting in which treatment adherence can be observed (e.g., through directly observed therapy for tuberculosis), but patient preferences for treatment adherence are heterogeneous and unobservable to a health provider. The novel elements of this problem stem from its institutional features: there is a single incentive schedule applied to all patients, incentive payments must be increasing in patients’ adherence, and patients cannot be a priori prohibited from any given levels of adherence. We develop models to design optimal incentives incorporating these features, and they are also applicable in other problem contexts that share the same features. We also conduct a numerical study using representative data in the context of the tuberculosis epidemic in India. Our study shows that our optimally designed incentive schedules are generally cost-effective compared with a linear incentive benchmark.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2227},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Design of incentive programs for optimal medication adherence in the presence of observable consumption},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Objective selection for cancer treatment: An inverse
optimization approach. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In radiation therapy treatment plan optimization, selecting a set of clinical objectives that are tractable and parsimonious yet effective is a challenging task. In clinical practice, this is typically done by trial and error based on the treatment planner’s subjective assessment, which often makes the planning process inefficient and inconsistent. We develop the objective selection problem that infers a sparse set of objectives for prostate cancer treatment planning based on historical treatment data. We formulate the problem as a nonconvex bilevel mixed-integer program using inverse optimization and highlight its connection with feature selection to propose multiple solution approaches, including greedy heuristics and regularized problems and application-specific methods that use anatomical information of the patients. Our results show that the proposed heuristics find objectives that are near optimal. Via curve analysis on dose-volume histograms, we show that the learned objectives closely represent latent clinical preferences.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2192},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Objective selection for cancer treatment: An inverse optimization approach},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial price integration in commodity markets with
capacitated transportation networks. <em>OR</em>, <em>70</em>(3),
iii–viii. (<a href="https://doi.org/10.1287/opre.2022.2288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial price integration is extensively studied in commodity markets as a means of examining the degree of integration between regions of a geographically diverse market. Many commodity markets that are commonly studied are supported by stable and well-defined transportation networks. In this paper, we analyze the relationship between spatial price integration, that is, the distribution of prices across geographically distinct locations in the market and the features of the underlying transportation network. We characterize this relationship and show that price integration is strongly influenced by the characteristics of the network, especially when there are capacity constraints on links in the network. Our results are summarized using a price decomposition that explicitly isolates the influences of market forces (supply and demand), transportation costs, and capacity constraints among a set of equilibrium prices. We use these theoretical insights to develop a unique discrete optimization methodology to capture spatiotemporal price variations indicative of underlying network bottlenecks. We apply the methodology to gasoline prices in the southeastern United States, where the methodology effectively characterizes the price effects of a series of well-documented network and supply chain disruptions, providing important implications for operations and supply chain management.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2288},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Spatial price integration in commodity markets with capacitated transportation networks},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient fair division with minimal sharing. <em>OR</em>,
<em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2022.2279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A collection of objects, some of which are good and some of which are bad, is to be divided fairly among agents with different tastes, modeled by additive utility functions. If the objects cannot be shared, so that each of them must be entirely allocated to a single agent, then a fair division may not exist. What is the smallest number of objects that must be shared between two or more agents to attain a fair and efficient division? In this paper, fairness is understood as proportionality or envy-freeness and efficiency as fractional Pareto-optimality. We show that, for a generic instance of the problem (all instances except a zero-measure set of degenerate problems), a fair fractionally Pareto-optimal division with the smallest possible number of shared objects can be found in polynomial time , assuming that the number of agents is fixed. The problem becomes computationally hard for degenerate instances, where agents’ valuations are aligned for many objects.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2279},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Efficient fair division with minimal sharing},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pricing and optimization in shared vehicle systems: An
approximation framework. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing shared vehicle systems (bike-/scooter-/car-/ride-sharing) are more challenging compared with traditional resource allocation settings because of the presence of complex network externalities —changes in the demand/supply at any location affect future supply throughout the system within short timescales. These externalities are well captured by steady-state Markovian models, which are therefore widely used to analyze such systems. However, using such models to design pricing and other control policies is computationally difficult because the resulting optimization problems are high dimensional and nonconvex. To this end, we develop a rigorous approximation framework for shared vehicle systems, providing a unified approach for a wide range of controls (pricing, matching, rebalancing), objective functions (throughput, revenue, welfare), and system constraints (travel times, welfare benchmarks, posted-price constraints). Our approach is based on the analysis of natural convex relaxations and obtains as special cases existing approximate optimal policies for limited settings, asymptotic optimality results, and heuristic policies. The resulting guarantees are nonasymptotic and parametric and provide operational insights into the design of real-world systems. In particular, for any shared vehicle system with n stations and m vehicles, our framework obtains an approximation ratio of 1 + ( n − 1 ) / m , which is particularly meaningful when m / n , the average number of vehicles per station, is large, as is often the case in practice.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2165},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Pricing and optimization in shared vehicle systems: An approximation framework},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Always valid inference: Continuous monitoring of a/b tests.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A/B tests are typically analyzed via frequentist p -values and confidence intervals, but these inferences are wholly unreliable if users endogenously choose samples sizes by continuously monitoring their tests. We define always valid p -values and confidence intervals that let users try to take advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. Always valid inference can be interpreted as a natural interface for a sequential hypothesis test, which empowers users to implement a modified test tailored to them. In particular, we show in an appropriate sense that the measures we develop trade off sample size and power efficiently, despite a lack of prior knowledge of the user’s relative preference between these two goals. We also use always valid p -values to obtain multiple hypothesis testing control in the sequential context. Our methodology has been implemented in a large-scale commercial A/B testing platform to analyze hundreds of thousands of experiments to date.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2135},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Always valid inference: Continuous monitoring of A/B tests},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributionally robust linear and discrete optimization
with marginals. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study linear and discrete optimization problems in which the objective coefficients are random, and the goal is to evaluate a robust bound on the expected optimal value, where the set of admissible joint distributions is assumed to be specified only up to the marginals. We study a primal-dual formulation for this problem, and in the process, unify existing results with new results. We establish NP-hardness of computing the bound for general polytopes and identify two sufficient conditions: one based on a dual formulation and one based on sublattices that provide a class of polytopes where the robust bounds are efficiently computable. We discuss several examples and applications in areas such as scheduling.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2243},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Distributionally robust linear and discrete optimization with marginals},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic learning and decision making via basis weight
vectors. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new methodology to solve a general model of dynamic decision making with a continuous unknown parameter or state. The methodology centers on the “continuation-value functions” (mappings from the parameter space to the continuation-value space), created by feasible continuation policies. When the model primitives can be described through a family of basis functions (e.g., polynomials), a continuation-value function retains that property and can be represented by a basis weight vector. The set of efficient basis weight vectors can be constructed through backward induction, which leads to a significant reduction of problem complexity and enables an exact solution for small-sized problems. A set of approximation methods based on the new methodology is developed to tackle larger problems. The methodology is also extended to the multidimensional (multiparameter) setting, which features the problem of contextual multiarmed bandits with linear expected rewards. The approximation algorithm developed in this paper outperforms three benchmark algorithms (epsilon-greedy, Thompson sampling, and LinUCB) in learning situations with many actions and short horizons.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2240},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Dynamic learning and decision making via basis weight vectors},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decomposition branching for mixed integer programming.
<em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel and powerful approach for solving certain classes of mixed integer programs (MIPs): decomposition branching . Two seminal and widely used techniques for solving MIPs, branch-and-bound and decomposition, form its foundation. Computational experiments with instances of a weighted set covering problem and a regionalized p -median facility location problem with assignment range constraints demonstrate its efficacy: it explores far fewer nodes and can be orders of magnitude faster than a commercial solver and an automatic Dantzig-Wolfe approach.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2210},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Decomposition branching for mixed integer programming},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Legal assignments and fast EADAM with consent via classic
theory of stable matchings. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gale and Shapley’s stable assignment problem has been extensively studied, applied, and extended. In the context of school choice, mechanisms often aim at finding an assignment that is more favorable to students. We investigate two extensions introduced in this framework— legal assignments and the efficiency adjusted deferred acceptance mechanism (EADAM) algorithm—through the lens of the classic theory of stable matchings. In any instance, the set L of legal assignments is known to contain all stable assignments. We prove that L is exactly the set of stable assignments in another instance. Moreover, we show that essentially all optimization problems over L can be solved within the same time bound needed for solving it over the set of stable assignments. A key tool for this latter result is an algorithm that finds the student-optimal legal assignment. We then generalize our algorithm to obtain the assignment output of EADAM with any given set of consenting students without sacrificing the running time, hence largely improving in both theory and practice over known algorithms. Finally, we show that the set L can be much larger than the set of stable matchings, connecting legal matchings with certain concepts and open problems in the literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2199},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Legal assignments and fast EADAM with consent via classic theory of stable matchings},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Subsampling to enhance efficiency in input uncertainty
quantification. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stochastic simulation, input uncertainty refers to the output variability arising from the statistical noise in specifying the input models. This uncertainty can be measured by a variance contribution in the output, which, in the nonparametric setting, is commonly estimated via the bootstrap. However, due to the convolution of the simulation noise and the input noise, the bootstrap consists of a two-layer sampling and typically requires substantial simulation effort. This paper investigates a subsampling framework to reduce the required effort, by leveraging the form of the variance and its estimation error in terms of the data size and the sampling requirement in each layer. We show how the total required effort can be reduced from an order bigger than the data size in the conventional approach to an order independent of the data size in subsampling. We explicitly identify the procedural specifications in our framework that guarantee relative consistency in the estimation and the corresponding optimal simulation budget allocations. We substantiate our theoretical results with numerical examples.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2168},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Subsampling to enhance efficiency in input uncertainty quantification},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using ℓ1-relaxation and integer programming to obtain dual
bounds for sparse PCA. <em>OR</em>, <em>70</em>(3), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is one of the most widely used dimensionality reduction tools in scientific data analysis. The PCA direction, given by the leading eigenvector of a covariance matrix, is a linear combination of all features with nonzero loadings; this impedes interpretability. Sparse principal component analysis (SPCA) is a framework that enhances interpretability by incorporating an additional sparsity requirement in the feature weights (factor loadings) while finding a direction that explains the maximal variation in the data. However, unlike PCA, the optimization problem associated with the SPCA problem is NP-hard. Most conventional methods for solving SPCA are heuristics with no guarantees, such as certificates of optimality on the solution quality via associated dual bounds. Dual bounds are available via standard semidefinite programming (SDP)–based relaxations, which may not be tight, and the SDPs are difficult to scale using off-the-shelf solvers. In this paper, we present a convex integer programming (IP) framework to derive dual bounds. At the heart of our approach is the so-called ℓ 1 -relaxation of SPCA. Although the ℓ 1 -relaxation leads to convex optimization problems for ℓ 0 -sparse linear regressions and relatives, it results in a nonconvex optimization problem for the PCA problem. We first show that the ℓ 1 -relaxation gives a tight multiplicative bound on SPCA. Then, we show how to use standard integer programming techniques to further relax the ℓ 1 -relaxation into a convex IP for which there are good commercial solvers. We present worst-case results on the quality of the dual bound provided by the convex IP. We empirically observe that the dual bounds are significantly better than the worst-case performance and are superior to the SDP bounds on some real-life instances. Moreover, solving the convex IP model using commercial IP solvers appears to scale much better that solving the SDP-relaxation using commercial solvers. To the best of our knowledge, we obtain the best dual bounds for real and artificial instances for SPCA problems involving covariance matrices of size up to 2,000 × 2,000.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2153},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Using ℓ1-relaxation and integer programming to obtain dual bounds for sparse PCA},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic risked equilibrium. <em>OR</em>, <em>70</em>(3),
iii–viii. (<a href="https://doi.org/10.1287/opre.2019.1958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a competitive partial equilibrium in markets where risk-averse agents solve multistage stochastic optimization problems formulated in scenario trees. The agents trade a commodity that is produced from an uncertain supply of resources. Both resources and the commodity can be stored for later consumption. Several examples of a multistage risked equilibrium are outlined, including aspects of battery and hydroelectric storage in electricity markets, distributed ownership of competing technologies relying on shared resources, and aspects of water control and pricing. The agents are assumed to have nested coherent risk measures based on one-step risk measures with polyhedral risk sets that have a nonempty intersection over agents. Agents can trade risk in a complete market of Arrow-Debreu securities. In this setting, we define a risk-trading competitive market equilibrium and establish two welfare theorems. Competitive equilibrium will yield a social optimum (with a suitably defined social risk measure) when agents have strictly monotone one-step risk measures. Conversely, a social optimum with an appropriately chosen risk measure will yield a risk-trading competitive market equilibrium when all agents have strictly monotone risk measures. The paper also demonstrates versions of these theorems when risk measures are not strictly monotone.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1958},
  journal      = {Operations Research},
  number       = {3},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Dynamic risked equilibrium},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized retail promotions through a directed acyclic
graph–based representation of customer preferences. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a back-to-back procedure for running personalized promotions in retail operations contexts, from the construction of a nonparametric choice model where customer preferences are represented by directed acyclic graphs (DAGs) to the design of such promotions. The source data include a history of purchases tagged by customer ID jointly with product availability and promotion data for a category of products. In each customer DAG, nodes represent products and directed edges represent the relative preference order between two products. Upon arrival to the store, a customer samples a full ranking of products within the category consistent with her DAG and purchases the most preferred option among the available ones. We describe the construction process to obtain the DAGs and explain how to mount a parametric, multinomial logit model (MNL) over them. We provide new bounds for the likelihood of a DAG and show how to conduct the MNL estimation. We test our model to predict purchases at the individual level on real retail data and characterize conditions under which it outperforms state-of-the-art benchmarks. Finally, we illustrate how to use the model to run personalized promotions. Our framework leads to significant revenue gains that make it an attractive candidate to be pursued in practice.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2108},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Personalized retail promotions through a directed acyclic Graph–Based representation of customer preferences},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Separation of perishable inventories in offline retailing
through transshipment. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transshipment in retailing is a practice where one outlet ships its excess inventory to another outlet with inventory shortages. By balancing inventories, transshipment can reduce waste and increase fill rate at the same time. In this paper, we explore the idea of transshipping perishable goods with a fixed finite lifetime in offline grocery retailing. In the offline retailing of perishable goods, customers typically choose the newest items first, which can lead to substantial waste. We show that in this context, transshipment plays two roles. One is inventory balancing, which is well known in the literature. The other is inventory separation, which is new to the literature. That is, transshipment allows a retailer to put newer inventory in one outlet and older inventory in the other. This makes it easier to sell older inventory and reduces waste as a result. To understand how exactly inventories should be separated, we study a class of heuristic policies whose computation relies on only two pieces of information, namely the number of items expiring in one period (old items) and that of the rest (new items). We show that the optimal policy among them can be characterized by two increasing switching curves. The two switching curves divide the entire state space into three regions. In the first region, only one outlet holds old items while both hold new items. In the second, one outlet holds old items and the other holds new items. In the third, only one outlet holds new items while both hold old items. We also conduct numerical studies to quantify the value of transshipment.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2144},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Separation of perishable inventories in offline retailing through transshipment},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crew assignment with duty time limits for transport
services: Tight multicommodity models. <em>OR</em>, <em>70</em>(2),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crew costs account for a significant portion of the operating expenses for transportation service providers, and so utilizing crews effectively is an important priority for these organizations. This paper addresses the core problem of assigning crews to urban transit and other scheduled transportation services at minimum total cost for crew usage, assignment, and transfers, taking into account crew work rules that limit their duty and working times. We propose a new multicommodity flow model with polynomial number of variables and constraints that is well suited to capture these work rules. This model can also readily incorporate additional desired features of crew assignments, such as balancing task assignments across crew members. It is more compact than previous flow-based and set partitioning models that have exponential constraints or variables. When work rules impose only duty time restrictions, our model is at least as tight as these previous models and can be strictly tighter than single-commodity models. We develop several classes of valid inequalities to further strengthen our model and discuss how to exploit any limits on working time to reduce model size and tighten the constraints. The model is easy to implement and apply, without requiring specialized decomposition procedures. To accelerate computational performance for large problems, we propose an effective optimization-based approach that entails first solving a restricted problem and then applying an optimality test to eliminate variables. We demonstrate the effectiveness of our modeling approach by applying it to several large-scale problem instances from the literature, solving most problems within a few minutes using a standard solver.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2155},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Crew assignment with duty time limits for transport services: Tight multicommodity models},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Market efficient portfolios in a systemic economy.
<em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the ex ante minimization of market inefficiency, defined in terms of minimum deviation of market prices from fundamental values, from a centralized planner’s perspective. Prices are pressured from exogenous trading actions of leverage-targeting banks, which rebalance their portfolios in response to asset shocks. We characterize market inefficiency in terms of two key drivers, the banks’ systemic significance and the statistical moments of asset shocks, and develop an explicit expression for the matrix of asset holdings that minimizes such inefficiency. Our analysis shows that to reduce inefficiencies, portfolio holdings should deviate more from a full diversification strategy if there is little heterogeneity in banks’ systemic significance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2172},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Market efficient portfolios in a systemic economy},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stochastic knapsack revisited: The service level
perspective. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in the resource allocation problem is to find near-optimal policies to serve different customers with random demands/revenues, using a fixed pool of capacity (properly configured). In this paper, we study the properties of three classes of allocation policies—responsive (with perfect hindsight), adaptive (with information updates), and anticipative (with forecast information) policies. These policies differ in how the information on actual demand and revenue of each customer is being revealed and integrated into the allocation decisions. We show that the analysis of these policies can be unified through the notion of “persistency” (or service level) values—the probability that a customer is being (completely) served in the optimal responsive policy. We analyze and compare the performances of these policies for both capacity minimization (with given persistency targets) and revenue maximization (with given capacity) models. In both models, the performance gaps between optimal anticipative policies and adaptive policies are shown to be bounded when the demand and revenue of each item are independently generated. In contrast, the gaps between the optimal adaptive policies and responsive policies can be arbitrarily large. More importantly, we show that the techniques developed, and the persistency values obtained from the optimal responsive policies can be used to design good adaptive and anticipative policies for the other two variants of resource allocation problems. This provides a unified approach to the design and analysis of algorithms for these problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2173},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Stochastic knapsack revisited: The service level perspective},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Menu costs and the bullwhip effect: Supply chain
implications of dynamic pricing. <em>OR</em>, <em>70</em>(2), iii–viii.
(<a href="https://doi.org/10.1287/opre.2021.2175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the supply chain implications of dynamic pricing. Specifically, we estimate how reducing menu costs—the operational burden of adjusting prices—would affect supply chain volatility. Fitting a structural econometric model to data from a large Chinese supermarket chain, we estimate that removing menu costs would (i) reduce the mean shipment coefficient of variation by 7.2 percentage points (pp), (ii) reduce the mean sales coefficient of variation by 4.3 pp, and (iii) reduce the mean bullwhip effect by 2.9 pp. These stabilizing changes are almost entirely attributable to an increase in the mean sales rate.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2175},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Menu costs and the bullwhip effect: Supply chain implications of dynamic pricing},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single allocation hub location with heterogeneous economies
of scale. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the single allocation hub location problem with heterogeneous economies of scale (SAHLP-h). The SAHLP-h is a generalization of the classical single allocation hub location problem (SAHLP), in which the hub-hub connection costs are piecewise linear functions of the amounts of flow. We model the problem as an integer nonlinear program, which we then reformulate as a mixed integer linear program (MILP) and as a mixed integer quadratically constrained program (MIQCP). We exploit the special structures of these models to develop Benders-type decomposition methods with integer subproblems. We use an integer L-shaped decomposition to solve the MILP formulation. For the MIQCP, we dualize a set of complicating constraints to generate a Lagrangian function, which offers us a subproblem decomposition and a tight lower bound. We develop linear dual functions to underestimate the integer subproblem, which helps us obtain optimality cuts with a convergence guarantee by solving a linear program. Moreover, we develop a specialized polynomial-time algorithm to generate enhanced cuts. To evaluate the efficiency of our models and solution approaches, we perform extensive computational experiments on both uncapacitated and capacitated SAHLP-h instances derived from the classical Australian Post data set. The results confirm the efficacy of our solution methods in solving large-scale instances.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2185},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Single allocation hub location with heterogeneous economies of scale},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained assortment optimization under the paired
combinatorial logit model. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the assortment optimization problem when customer choices are governed by the paired combinatorial logit model. We study unconstrained, cardinality-constrained, and knapsack-constrained versions of this problem, which are all known to be NP-hard. We design efficient algorithms that compute approximately optimal solutions, using a novel relation to the maximum directed cut problem and suitable linear-program rounding algorithms. We obtain a randomized polynomial time approximation scheme for the unconstrained version and performance guarantees of 50\% and ≈ 50\% for the cardinality-constrained and knapsack-constrained versions, respectively. These bounds improve significantly over prior work. We also obtain a performance guarantee of 38.5\% for the assortment problem under more general constraints, such as multidimensional knapsack (where products have multiple attributes and there is a knapsack constraint on each attribute) and partition constraints (where products are partitioned into groups and there is a limit on the number of products selected from each group). In addition, we implemented our algorithms and tested them on random instances available in prior literature. We compared our algorithms against an upper bound obtained using a linear program. Our average performance bounds for the unconstrained, cardinality-constrained, knapsack-constrained, and partition-constrained versions are over 99\%, 99\%, 96\%, and 99\%, respectively.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2188},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Constrained assortment optimization under the paired combinatorial logit model},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—a monge sequence-based approach to
characterize the competitive newsvendor problem. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the stochastic inventory game in which n players compete by setting their individual inventory levels in a market with stockout-based demand substitution. Because of specific tractability issues, the prior literature has largely focused on versions of this competitive newsvendor problem with assumptions on the number of players and their substitution behavior. In this note, we develop an approach to solve instances of this problem with any number of players and multistage spillovers of unsatisfied demand. We (i) establish that for multistage stockout-based substitution models explored in the literature, the search (substitution) behavior of customers can be replicated using a Monge sequence; (ii) obtain the first-order conditions that can be then used to determine equilibrium inventory levels; and (iii) discuss other structural properties of the solution based on Bottleneck Monge matrices. Special cases of our approach provide the well-known equilibrium results for two newsvendors.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2189},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A monge sequence-based approach to characterize the competitive newsvendor problem},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A risk extended version of merton’s optimal consumption and
portfolio selection. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to study the optimal consumption and portfolio choice problem of risk-controlled investors who strive to maximize total expected discounted utility of both consumption and terminal wealth. Risk is measured by the variance of terminal wealth, which introduces a nonlinear function of the expected value into the control problem. The control problem presented is no longer a standard stochastic control problem but rather, a mean field-type control problem. The optimal portfolio and consumption rules are obtained explicitly. Numerical results shed light on the importance of controlling variance risk. The optimal investment policy is nonmyopic, and consumption is not sacrificed.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2197},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {A risk extended version of merton’s optimal consumption and portfolio selection},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-sectional variation of intraday liquidity,
cross-impact, and their effect on portfolio execution. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An analysis of intraday volumes for the S&amp;P 500 constituent stocks illustrates that (i) volume surprises (i.e., deviations from forecasted trading volumes) are correlated across stocks and that (ii) this correlation increases during the last few hours of the trading session. These observations can be attributed partly to the prevalence of portfolio trading activity that is implicit in the growth of passive (systematic) investment strategies and partly to the increased trading intensity of such strategies toward the end of the trading session. In this paper, we investigate the consequences of such portfolio liquidity on price impact and portfolio execution. We derive a linear cross-asset market impact from a stylized model that explicitly captures the fact that a certain fraction of natural liquidity providers trade only portfolios of stocks whenever they choose to execute. We find that because of cross-impact and its intraday variation, it is optimal for a risk-neutral cost-minimizing liquidator to execute a portfolio of orders in a coupled manner, as opposed to the separable volume-weighted average price execution schedule that is often assumed. The optimal schedule couples the execution on the individual stocks so as to take advantage of increased portfolio liquidity toward the end of the day. A worst case analysis shows that the potential cost reduction from this optimized execution schedule over the separable approach can be as high as 15\% for plausible model parameters. Finally, we discuss how to estimate cross-sectional price impact if one had a data set of realized portfolio transaction records by exploiting the low-rank structure of its coefficient matrix suggested by our analysis.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2201},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Cross-sectional variation of intraday liquidity, cross-impact, and their effect on portfolio execution},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven pricing for a new product. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions regarding new products are often difficult to make, and mistakes can have grave consequences for a firm’s bottom line. Often, firms lack important information about a new product, such as its potential market size and the speed of its adoption by consumers. One of the most popular frameworks that has been used for modeling new product adoption is the Bass model. Although the Bass model and its many variants are used to study dynamic pricing of new products, the vast majority of these models require a priori knowledge of parameters that can only be estimated from historical data or guessed using institutional knowledge. In this paper, we study the interplay between pricing and learning for a monopolist whose objective is to maximize the expected revenue of a new product over a finite selling horizon. We extend the generalized Bass model to a stochastic setting by modeling adoption through a continuous-time Markov chain with which the adoption rate depends on the selling price and on the number of past sales. We study a pricing problem in which the parameters of this demand model are unknown, but the seller can utilize real-time demand data for learning the parameters. We propose two simple and computationally tractable pricing policies with O ( ln m ) regret, where m is the market size.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2204},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Data-driven pricing for a new product},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imposing contiguity constraints in political districting
models. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beginning in the 1960s, techniques from operations research began to be used to generate political districting plans. A classical example is the integer programming model of Hess et al. [Hess SW, Weaver JB, Siegfeldt HJ, Whelan JN, Zitlau PA ( 1965 ) Oper. Res. 13(6):998–1006.]. Because of the model’s compactness-seeking objective, it tends to generate contiguous or nearly contiguous districts, although none of the model’s constraints explicitly impose contiguity. Consequently, Hess et al. had to manually adjust their solutions to make them contiguous. Since then, there have been several attempts to adjust the Hess model and other models so that contiguity is explicitly ensured. In this paper, we review two existing models for imposing contiguity, propose two new ones, and analytically compare them in terms of their strength and size. We conduct an extensive set of numerical experiments to evaluate their performance. Although many believe that contiguity constraints are particularly difficult to deal with, we find that the districting problem considered by Hess et al. does not become harder when contiguity is imposed. In fact, a branch-and-cut implementation of a cut-based model generates, for the first time, optimally compact districting plans for 21 different U.S. states at the census tract level. To encourage future research in this area, and for purposes of transparency, we make our test instances and source code publicly available.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2141},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Imposing contiguity constraints in political districting models},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—capacitated assortment optimization: Hardness
and approximation. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assortment optimization is an important problem that arises in many practical applications such as retailing and online advertising. In this problem, the goal is to select a subset of items that maximizes the expected revenue in the presence of (1) the substitution behavior of consumers specified by a choice model , and (2) a potential capacity constraint bounding the total weight of items in the assortment. The latter is a natural constraint arising in many applications. We begin by showing how challenging these two aspects are from an optimization perspective. First, we show that adding a general capacity constraint makes the problem NP-hard even for the simplest choice model, namely the multinomial logit model. Second, we show that even the unconstrained assortment optimization for the mixture of multinomial logit model is hard to approximate within any reasonable factor when the number of mixtures is not constant. In view of these hardness results, we present near-optimal algorithms for the capacity constrained assortment optimization problem under a large class of parametric choice models including the mixture of multinomial logit, Markov chain, nested logit, and d -level nested logit choice models. In fact, we develop near-optimal algorithms for a general class of capacity constrained optimization problems whose objective function depends on a small number of linear functions. For the mixture of multinomial logit model (resp. Markov chain model), the running time of our algorithm depends exponentially on the number of segments (resp. rank of the transition matrix). Therefore, we get efficient algorithms only for the case of constant number of segments (resp. constant rank). However, in light of our hardness result, any near-optimal algorithm will have a super polynomial dependence on the number of mixtures for the mixture of multinomial logit choice model.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2142},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Capacitated assortment optimization: Hardness and approximation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—optimal pricing under multiple-discrete
customer choices and diminishing return of consumption. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a utility-based customer-choice model where the customer may purchase multiple products and even possibly multiple units of each product. We show that the set of products with strictly positive optimal consumption quantities is one of the ordered sets based on product prices and certain model parameters. We study the firm’s optimal pricing problem and present how to find the optimal prices. We show that the optimal solution exhibits a property that the set of products that induces strictly positive consumption quantities under optimal prices is also ordered based on a price-independent index composed of product cost and choice-model parameters. As extensions, we present an alternative formulation that constrains the customer’s total expenditure instead of total purchase quantity and develop a solution approach. We also consider a stochastic model that accounts for customer heterogeneity, establishes a connection to the mixed Multinomial Logit (MNL) model, and numerically investigates how the heuristic policy based on the deterministic approximation performs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2146},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Optimal pricing under multiple-discrete customer choices and diminishing return of consumption},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust learning of consumer preferences. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a class of ranking and selection problems faced by a company that wants to identify the most preferred product out of a finite set of alternatives when consumer preferences are a priori unknown. The only information available is that consumer preferences satisfy two key properties: (i) they are consistent with some unknown true ranking of the alternatives, and (ii) they are strict, namely, no two products are equally preferred. To learn the unknown ranking, the company is able to sample consumer preferences by sequentially showing different subsets of products to different consumers and asking them to report their top preference within the displayed set. The objective of the company is to design a display policy that minimizes the expected number of samples needed to identify the top-ranked product with high probability. We prove an instance-specific lower bound on the sample complexity of any policy that identifies the top-ranked product within a given (probabilistic) confidence. We also propose a robust formulation of the company’s problem and derive a sampling policy (myopic tracking policy), which is both worst-case asymptotically optimal and intuitive to implement. Roughly speaking, the myopic tracking policy randomly alternates between two extreme types of displaying strategies: (i) full display , which shows a consumer the entire menu so as to learn something about every product, and (ii) pair display , which shows a consumer only two products so as to maximize the informativeness of the choice made by the consumer. To assess the performance of our proposed myopic tracking policy, we conduct a comprehensive set of computational studies and compare it to alternative methods in the literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2157},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Robust learning of consumer preferences},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiplicative pacing equilibria in auction markets.
<em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Budgets play a significant role in real-world sequential auction markets such as those implemented by internet companies. To maximize the value provided to auction participants, spending is smoothed across auctions so budgets are used for the best opportunities. Motivated by a mechanism used in practice by several companies, this paper considers a smoothing procedure that relies on pacing multipliers : on behalf of each buyer, the auction market applies a factor between 0 and 1 that uniformly scales the bids across all auctions. Reinterpreting this process as a game between buyers, we introduce the notion of pacing equilibrium , and prove that they are always guaranteed to exist. We demonstrate through examples that a market can have multiple pacing equilibria with large variations in several natural objectives. We show that pacing equilibria refine another popular solution concept, competitive equilibria, and show further connections between the two solution concepts. Although we show that computing either a social-welfare-maximizing or a revenue-maximizing pacing equilibrium is NP-hard, we present a mixed-integer program (MIP) that can be used to find equilibria optimizing several relevant objectives. We use the MIP to provide evidence that: (1) equilibrium multiplicity occurs very rarely across several families of random instances, (2) static MIP solutions can be used to improve the outcomes achieved by a dynamic pacing algorithm with instances based on a real-world auction market, and (3) for the instances we study, buyers do not have an incentive to misreport bids or budgets provided there are enough participants in the market.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2167},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Multiplicative pacing equilibria in auction markets},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surgical case-mix and discharge decisions: Does
within-hospital coordination matter? <em>OR</em>, <em>70</em>(2),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem faced by a profit-maximizing, resource-constrained hospital that controls patient inflows by designing a case-mix of its elective procedures and patient outflows via patient discharges. At the center of our analysis is the model that evaluates hospital profit for any combination of elective portfolio and patient discharge policies. Our model analyzes the impact of patient flow management decisions on the utilization of two main classes of hospital resources: “front end” (e.g., operating rooms) and “backroom” (e.g., recovery beds). We introduce a new approach for modeling the patient recovery process and use it to characterize the relationship between patient length of stay and probability of readmission. Using this modeling approach, we develop a two-moment approximation for the utilization of front-end and backroom resources. We focus on assessing the benefits associated with the hospital employing a coordinated decision-making process in which both portfolio and discharge decisions are made in tandem. Specifically, we compare the hospital’s profits in the coordinated setting to those under two decentralized approaches: a front-end approach, under which both decisions are made based exclusively on the front-end costs, and a “siloed” approach, in which discharge decisions are made based on backroom costs and the case-mix is determined as the optimal match for the discharge policy. We show that hospitals operating under the front-end policy can significantly benefit from coordination when backroom costs are sufficiently high even if they do not exceed surgical costs. On the other hand, for hospitals operating under the siloed policy, coordination brings significant benefits only when surgical costs are high and significantly dominate the cost structure.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2177},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Surgical case-mix and discharge decisions: Does within-hospital coordination matter?},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network inspection for detecting strategic attacks.
<em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a problem of strategic network inspection, in which a defender (agency) is tasked with detecting the presence of multiple attacks in the network. An inspection strategy entails monitoring the network components, possibly in a randomized manner, using a given number of detectors. We formulate the network inspection problem ( P ) as a large-scale bilevel optimization problem, in which the defender seeks to determine an inspection strategy with minimum number of detectors that ensures a target expected detection rate under worst-case attacks. We show that optimal solutions of ( P ) can be obtained from the equilibria of a large-scale zero-sum game. Our equilibrium analysis involves both game-theoretic and combinatorial arguments and leads to a computationally tractable approach to solve ( P ) . First, we construct an approximate solution by using solutions of minimum set cover (MSC) and maximum set packing (MSP) problems and evaluate its detection performance. In fact, this construction generalizes some of the known results in network security games. Second, we leverage properties of the optimal detection rate to iteratively refine our MSC/MSP-based solution through a column generation procedure. Computational results on benchmark water networks demonstrate the scalability, performance, and operational feasibility of our approach. The results indicate that utilities can achieve a high level of protection in large-scale networks by strategically positioning a small number of detectors.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2180},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Network inspection for detecting strategic attacks},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatio-temporal pricing for ridesharing platforms.
<em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridesharing platforms match drivers and riders to trips, using dynamic prices to balance supply and demand. A challenge is to set prices that are appropriately smooth in space and time, so that drivers with the flexibility to decide how to work will nevertheless choose to accept their dispatched trips rather than drive to another area or wait for higher prices or a better trip. In this work, we propose a complete information model that is simple yet rich enough to incorporate spatial imbalance and temporal variations in supply and demand—conditions that lead to market failures in today’s platforms. We introduce the spatio-temporal pricing (STP) mechanism . The mechanism is incentive-aligned, in that it is a subgame-perfect equilibrium for drivers to always accept their trip dispatches. From any history onward, the equilibrium outcome of the STP mechanism is welfare-optimal, envy-free, individually rational, budget balanced, and core-selecting. We also prove the impossibility of achieving the same economic properties in a dominant-strategy equilibrium. Simulation results show that the STP mechanism can achieve substantially improved social welfare and earning equity than a myopic mechanism.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2178},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Spatio-temporal pricing for ridesharing platforms},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strategic workforce planning under uncertainty. <em>OR</em>,
<em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The workforce planning problem of hiring, dismissing, and promoting has been the perennial difficulty of Human Resource (HR) management. To cope with uncertain attrition, we propose a new approach of finding a course of action that safeguards against violating organizational target-meeting constraints, such as productivity, budget, headcount, dismissal threshold, and managerial span of control. As such, this approach leads to a tractable conic optimization model that minimizes a decision criterion that is inspired by the riskiness index of Aumann and Serrano, for which its value can be associated with probabilistic and robustness guarantees in meeting constraints under uncertainty. Additionally, our model departs from the literature by considering employees’ time-in-grade, which is known to affect resignations, as a decision variable. In our formulation, decisions and the uncertainty are related. To solve the model, we introduce the technique of pipeline invariance , which yields an exact reformulation that may be tractably solved. Computational performance of the model is studied by running simulations on a real data set of employees performing the same job function in the Singapore Civil Service. Using our model, we are able to numerically illustrate insights into HR, such as the consequences of a lack of organizational renewal. Our model is also likely the first numerical illustration that lends weight to a time-based progression policy common to bureaucracies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2183},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Strategic workforce planning under uncertainty},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). School choice in chile. <em>OR</em>, <em>70</em>(2),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centralized school admission mechanisms are an attractive way of improving social welfare and fairness in large educational systems. In this paper, we report the design and implementation of the newly established school choice system in Chile, where over 274,000 students applied to more than 6,400 schools. The Chilean system presents unprecedented design challenges that make it unique. First, it is a simultaneous nationwide system, making it one of the largest school choice problems worldwide. Second, the system is used for all school grade levels, from prekindergarten to 12th grade. One of our primary goals is to favor the assignment of siblings to the same school. By adapting the standard notions of stability, we show that a stable assignment may not exist. Hence, we propose a heuristic approach that elicits preferences and breaks ties between students in the same priority group at the family level. In terms of implementation, we adapt the deferred acceptance algorithm as in other systems around the world.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2184},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {School choice in chile},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pricing with samples. <em>OR</em>, <em>70</em>(2), iii–viii.
(<a href="https://doi.org/10.1287/opre.2021.2200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper, we study a fundamental data-driven pricing problem: how should a decision maker (optimally) price based on a finite and limited number of samples from the customers’ value distribution. The decision maker’s objective is to select a general pricing policy with maximum worst-case ratio of revenue compared with an oracle with knowledge of the value distribution, when the latter is only known to belong to some general nonparametric class. We study achievable performance for two central classes: regular and monotone hazard rate (mhr) distributions. We develop a novel unified general approach to quantify the performance of mechanisms. The approach allows us to characterize optimal performance for the fundamental case of a single sample through lower and upper bounds on the maximin ratio, with corresponding near-optimal mechanisms and near-worst-case distributions. Furthermore, by extending this class of mechanisms to the cases in which more samples are available, we leverage our general approach to analyze a novel family of policies leading to new results on achievable performance as the number of samples increases. At a higher level, this work also uncovers insights on the value of samples for pricing purposes. For example, against mhr distributions, a single sample guarantees 64\% of the performance an oracle with full knowledge of the distribution would achieve, two samples suffice to ensure 71\%, and 10 samples guarantee 80\% of such performance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2200},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Pricing with samples},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian exploration: Incentivizing exploration in bayesian
games. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a ubiquitous scenario in the internet economy when individual decision makers (henceforth, agents ) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way trade-off between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before to the ones that arrive later and strives to coordinate the agents toward a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal that respects agents’ incentives and minimizes a suitable notion of regret . We extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game , receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment. Our main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (the constant depends on the prior distribution but not on the time horizon) and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions , the actions that some incentive-compatible policy can recommend with nonzero probability. We show how the principal can identify (and explore) all explorable actions and use the revealed information to perform optimally. In particular, our results significantly improve over the prior work on the special case of a single agent per round, which relies on assumptions to guarantee that all actions are explorable. Interestingly, we do not require the principal’s utility to be aligned with the cumulative utility of the agents; instead, the principal can optimize an arbitrary notion of per-round reward.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2205},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Bayesian exploration: Incentivizing exploration in bayesian games},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—bootstrap-based budget allocation for nested
simulation. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulation budget allocation is at the heart of a nested (also referred to as two-level) simulation approach to estimating functionals of a conditional expectation. In this paper, we propose a sample-driven budget allocation rule under a unified nested simulation framework that allows for different forms of functionals. The proposed method employs bootstrap sampling to guide an effective choice of outer- and inner-level sample sizes. Furthermore, we establish a central limit theorem for nested simulation estimators, and incorporate the sample-driven allocation rule into the construction of asymptotically valid confidence intervals (CIs). Effectiveness of the sample-driven allocation rule and validity of the constructed CIs are confirmed by numerical experiments.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2071},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Bootstrap-based budget allocation for nested simulation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—static pricing: Universal guarantees for
reusable resources. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a fundamental pricing model in which a fixed number of units of a reusable resource are used to serve customers. Customers arrive to the system according to a stochastic process and, upon arrival, decide whether to purchase the service, depending on their willingness to pay and the current price. The service time during which the resource is used by the customer is stochastic, and the firm may incur a service cost. This model represents various markets for reusable resources, such as cloud computing, shared vehicles, rotable parts, and hotel rooms. In the present paper, we analyze this pricing problem when the firm attempts to maximize a weighted combination of three central metrics: profit, market share, and service level. Under Poisson arrivals, exponential service times, and standard assumptions on the willingness-to-pay distribution, we establish a series of results that characterize the performance of static pricing in such environments. In particular, although an optimal policy is fully dynamic in such a context, we prove that a static pricing policy simultaneously guarantees 78.9\% of the profit, market share, and service level from the optimal policy. Notably, this result holds for any service rate and number of units the firm operates. Our proof technique relies on a judicious construction of a static price that is derived directly from the optimal dynamic pricing policy. In the special case in which there are two units and the induced demand is linear, we also prove that the static policy guarantees 95.5\% of the profit from the optimal policy. Our numerical findings on a large test bed of instances suggest that the latter result is quite indicative of the profit obtained by the static pricing policy across all parameters.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2054},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Static pricing: Universal guarantees for reusable resources},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A polynomial-time approximation scheme for sequential batch
testing of series systems. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2019.1967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a recently introduced generalization of the classic sequential testing problem for series systems, consisting of multiple stochastic components. The conventional assumption in such settings is that the overall system state can be expressed as an AND function, defined with respect to the states of individual components. However, unlike the classic setting, rather than testing components separately, one after the other, we allow aggregating multiple tests to be conducted simultaneously, while incurring an additional set-up cost. This feature is present in numerous practical applications, where decision makers are incentivized to exploit economy of scale by testing subsets of components in batches. The main contribution of this paper is to devise a polynomial-time approximation scheme (PTAS) for the sequential batch-testing problem, thereby significantly improving on the constant-factor performance guarantee of 6.829 + ε due to Daldal et al. [Daldal R, Özlük O, Selçuk B, Shahmoradi Z, Ünlüyurt T (2017) Sequential testing in batches. Ann. Oper. Res . 253(1):97–116.]. Our approach is based on developing and leveraging a number of techniques in approximate dynamic programming, based on ideas related to efficient enumeration methods, state-space collapse, and charging schemes. These theoretical findings are complemented by extensive computational experiments, where we demonstrate the practical advantages of our methods.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1967},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {A polynomial-time approximation scheme for sequential batch testing of series systems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated online learning and adaptive control in queueing
systems with uncertain payoffs. <em>OR</em>, <em>70</em>(2), iii–viii.
(<a href="https://doi.org/10.1287/opre.2021.2100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study task assignment in online service platforms, where unlabeled clients arrive according to a stochastic process and each client brings a random number of tasks. As tasks are assigned to servers, they produce client/server-dependent random payoffs. The goal of the system operator is to maximize the expected payoff per unit time subject to the servers’ capacity constraints. However, both the statistics of the dynamic client population and the client-specific payoff vectors are unknown to the operator. Thus, the operator must design task-assignment policies that integrate adaptive control (of the queueing system) with online learning (of the clients’ payoff vectors). A key challenge in such integration is how to account for the nontrivial closed-loop interactions between the queueing process and the learning process, which may significantly degrade system performance. We propose a new utility-guided online learning and task assignment algorithm that seamlessly integrates learning with control to address such difficulty. Our analysis shows that, compared with an oracle that knows all client dynamics and payoff vectors beforehand, the gap of the expected payoff per unit time of our proposed algorithm can be analytically bounded by three terms, which separately capture the impact of the client-dynamic uncertainty, client-server payoff uncertainty, and the loss incurred by backlogged clients in the system. Further, our bound holds for any finite time horizon. Through simulations, we show that our proposed algorithm significantly outperforms a myopic-matching policy and a standard queue-length-based policy that does not explicitly address the closed-loop interactions between queueing and learning.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2100},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Integrated online learning and adaptive control in queueing systems with uncertain payoffs},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fluid models of parallel service systems under FCFS.
<em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study deterministic fluid approximation models of parallel service systems with a fixed set of servers, operating under first come first served (FCFS) policy, when the service time distributions may depend on both the server and the customer type. We explore the relations between fluid models and the properties of stability, resource pooling, and matching rates. We find that stability and resource pooling are determined by the unique fluid model in two cases: when service rates are of product form given by server speed and customer-type average work requirement and when the bipartite compatibility graph is a tree or a complete graph. For these cases, we are able to give a complete description of the unique fluid model of the system. In general, when service rates depend on both server and customer type and the graph is not one of those listed previously, stability and resource pooling cannot be determined from first moment information. Matching rates between pairs of compatible server and customer type cannot be determined from the fluid model unless the compatibility graph is complete or a tree. In particular, we discuss an example and show by simulation that matching rates and stability depend on the service time distributions beyond the first moments. Further simulations show that matching rates depend on the distributions of service times even when service times depend only on the server type and the fluid model is unique. On the other hand, we solve a static planning linear program and obtain a maximum throughput compatibility subgraph that is a tree or a forest. We show that using only links of this subgraph, FCFS is a throughput optimal policy. We also show that FCFS is a throughput optimal policy for systems with product form service rates.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2102},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Fluid models of parallel service systems under FCFS},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonconvex low-rank tensor completion from noisy data.
<em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a noisy tensor completion problem of broad practical interest, namely, the reconstruction of a low-rank tensor from highly incomplete and randomly corrupted observations of its entries. Whereas a variety of prior work has been dedicated to this problem, prior algorithms either are computationally too expensive for large-scale applications or come with suboptimal statistical guarantees. Focusing on “incoherent” and well-conditioned tensors of a constant canonical polyadic rank, we propose a two-stage nonconvex algorithm—(vanilla) gradient descent following a rough initialization—that achieves the best of both worlds. Specifically, the proposed nonconvex algorithm faithfully completes the tensor and retrieves all individual tensor factors within nearly linear time, while at the same time enjoying near-optimal statistical guarantees (i.e., minimal sample complexity and optimal estimation accuracy). The estimation errors are evenly spread out across all entries, thus achieving optimal ℓ ∞ statistical accuracy. We also discuss how to extend our approach to accommodate asymmetric tensors. The insight conveyed through our analysis of nonconvex optimization might have implications for other tensor estimation problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2106},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Nonconvex low-rank tensor completion from noisy data},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable and constant returns-to-scale production
technologies with component processes. <em>OR</em>, <em>70</em>(2),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nonparametric production technologies characterized by several component production processes and allow both component-specific and shared inputs and outputs. Each process uses its specific inputs and an unknown part of the shared inputs to produce its specific outputs and an unknown part of the shared outputs. For the described setting, we develop two new models of production technologies, under the assumptions of variable and constant returns to scale (VRS and CRS). These models are based on the worst-case assumption about the allocation of the shared inputs and outputs to component processes and, therefore, do not require exact knowledge of the actual allocation. The new models are larger than the standard VRS and CRS technologies. We provide a formal axiomatic derivation of the new technologies, explore their dual interpretation, and demonstrate their usefulness in an application.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2103},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Variable and constant returns-to-scale production technologies with component processes},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The gap function: Evaluating integer programming models over
multiple right-hand sides. <em>OR</em>, <em>70</em>(2), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an integer programming model with fixed data, the linear programming relaxation gap is considered one of the most important measures of model quality. There is no consensus, however, on appropriate measures of model quality that account for data variation. In particular, when the right-hand side is not known exactly, one must assess a model based on its behavior over many right-hand sides. Gap functions are the linear programming relaxation gaps parametrized by the right-hand side. Despite drawing research interest in the early days of integer programming, the properties and applications of these functions have been little studied. In this paper, we construct measures of integer programming model quality over sets of right-hand sides based on the absolute and relative gap functions. In particular, we formulate optimization problems to compute the expectation and extrema of gap functions over finite discrete sets and bounded hyperrectangles. These optimization problems are linear programs (albeit of an exponentially large size) that contain at most one special ordered-set constraint. These measures for integer programming models, along with their associated formulations, provide a framework for determining a model’s quality over a range of right-hand sides.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2003},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {The gap function: Evaluating integer programming models over multiple right-hand sides},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial capacity planning. <em>OR</em>, <em>70</em>(2),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the relationship between capacity and performance for a service firm with spatial operations, in the sense that requests arrive with origin-destination pairs. An example of such a system is a ride-hailing platform in which each customer arrives in the system with the need to travel from an origin to a destination. We propose a parsimonious representation of a spatial multiserver system through a state-dependent queueing model that captures spatial frictions as well as spatial economies of scale through the service rate. In a classical M / M / n queueing model, the square root safety (SRS) staffing rule is known to balance server utilization and customer wait times. By contrast, we find that the SRS rule does not lead to such a balance in spatial systems. In a spatial environment, pick-up times increase the load in the system; furthermore, they are an endogenous source of extra workload that leads the system to only operate efficiently if there is sufficient imbalance between supply and demand. In heavy traffic, we derive the mapping from load to operating regimes and establish implications on various metrics of interest. In particular, to obtain a balance of utilization and wait times, the service firm should use a higher safety factor, proportional to the offered load to the power of 2 / 3 . We also discuss implications of these results for general systems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2112},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Spatial capacity planning},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting bots and assessing their impact in social
networks. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks are often subject to influence campaigns by malicious actors through the use of automated accounts known as bots. We consider the problem of detecting bots in online social networks and assessing their impact on the opinions of individuals. We begin by analyzing the behavior of bots in social networks and identify that they exhibit heterophily, meaning that they interact with humans more than other bots. We use this property to develop a detection algorithm based on the Ising model from statistical physics. The bots are identified by solving a minimum cut problem. We show that this Ising model algorithm can identify bots with higher accuracy while utilizing much less data than other state of the art methods. We then develop a function, which we call generalized harmonic influence centrality , to estimate the impact that bots have on the opinions of users in social networks. This function is based on a generalized opinion dynamics model and captures how the activity level and network connectivity of the bots shift equilibrium opinions. To apply generalized harmonic influence centrality to real social networks, we develop a deep neural network to measure the opinions of users based on their social network posts. Using this neural network, we then calculate the generalized harmonic influence centrality of bots in multiple real social networks. For some networks, we find that a limited number of bots can cause nontrivial shifts in the population opinions. In other networks, we find that the bots have little impact. Overall, we find that generalized harmonic influence centrality is a useful operational tool to measure the impact of bots in social networks.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2118},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Detecting bots and assessing their impact in social networks},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Time-varying risk aversion and dynamic portfolio allocation.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the implications of time-varying risk aversion for dynamic portfolio allocation under the framework of regime-switching models. In our model, both asset returns and investor risk aversion are regime dependent: In a bull regime, asset return is high, volatility is low, and risk aversion is low, and the opposite happens in a bear regime. We develop an efficient dynamic programming algorithm that overcomes the challenges imposed by regime-dependent preference in obtaining time-consistent portfolio policies. Empirically, we show that CBOE Volatility Index (VIX) is an important predictor of regime shifts and investors with regime-dependent risk aversion achieve better investment performance than those with constant risk aversion.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2095},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Time-varying risk aversion and dynamic portfolio allocation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combined custom hedging: Optimal design, noninsurable
exposure, and operational risk management. <em>OR</em>, <em>70</em>(1),
iii–viii. (<a href="https://doi.org/10.1287/opre.2021.2133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a normative framework for the optimal design, value assessment, and risk management integration of combined custom contingent claims. A risk-averse firm faces a mix of financially insurable and noninsurable risk. The firm seeks optimal positioning in a pair of custom claims, one written on the insurable term and another written on any listed index correlated to the noninsurable term. We prove that a unique optimum always exists unless the index is redundant and show that the optimal payoff schedules satisfy a design integral equation. We assess the firm’s incremental benefit in terms of both an indifference value and an efficiency rating; this benefit increases with the correlation of the index to the noninsurable term, and it decreases with the correlation of the index to the insurable term. Our hedge proves to be empirically relevant for a highly risk-averse firm facing a market shock (COVID-19 pandemic). In the context of a newsvendor model featuring random price and demand, we show that (i) integrating our optimal combined custom hedge with the corresponding optimal procurement policy allows the firm to obtain a significant improvement in both risk and return, and (ii) this gain may be traded off for a substantial enhancement in operational flexibility.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2133},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Combined custom hedging: Optimal design, noninsurable exposure, and operational risk management},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal portfolio diversification via independent component
analysis. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A natural approach to enhance portfolio diversification is to rely on factor-risk parity, which yields the portfolio whose risk is equally spread among a set of uncorrelated factors. The standard choice is to take the variance as risk measure, and the principal components (PCs) of asset returns as factors. Although PCs are unique and useful for dimension reduction, they are an arbitrary choice: any rotation of the PCs results in uncorrelated factors. This is problematic because we demonstrate that any portfolio is a factor-variance-parity portfolio for some rotation of the PCs. More importantly, choosing the PCs does not account for the higher moments of asset returns. To overcome these issues, we propose using the independent components (ICs) as factors, which are the rotation of the PCs that are maximally independent, and care about higher moments of asset returns. We demonstrate that using the IC-variance-parity portfolio helps to reduce the return kurtosis. We also show how to exploit the near independence of the ICs to parsimoniously estimate the factor-risk-parity portfolio based on value at risk. Finally, we empirically demonstrate that portfolios based on ICs outperform those based on PCs, and several state-of-the-art benchmarks.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2140},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Optimal portfolio diversification via independent component analysis},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven robust resource allocation with monotonic cost
functions. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two-stage planning problems (arising, e.g., in city logistics) in which a resource is first divided among a set of independent regions and then costs are incurred based on the allocation to each region. Costs are assumed to be decreasing in the quantity of the resource, but their precise values are unknown, for example, if they represent difficult expected values. We develop a new data-driven uncertainty model for monotonic cost functions, which can be used in conjunction with robust optimization to obtain tractable allocation decisions that significantly improve worst-case performance outcomes. Our model uses a novel uncertainty set construction that rigorously handles monotonic structure based on a statistical goodness-of-fit test with respect to a given sample of data. The practical value of this approach is demonstrated in three realistic case studies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2145},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Data-driven robust resource allocation with monotonic cost functions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robustness in the optimization of risk measures.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study issues of robustness in the context of Quantitative Risk Management and Optimization. We develop a general methodology for determining whether a given risk-measurement-related optimization problem is robust, which we call “robustness against optimization.” The new notion is studied for various classes of risk measures and expected utility and loss functions. Motivated by practical issues from financial regulation, special attention is given to the two most widely used risk measures in the industry, Value-at-Risk (VaR) and Expected Shortfall (ES). We establish that for a class of general optimization problems, VaR leads to nonrobust optimizers, whereas convex risk measures generally lead to robust ones. Our results offer extra insight on the ongoing discussion about the comparative advantages of VaR and ES in banking and insurance regulation. Our notion of robustness is conceptually different from the field of robust optimization, to which some interesting links are derived.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2147},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Robustness in the optimization of risk measures},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 1.79-approximation algorithms for continuous review
single-sourcing lost-sales and dual-sourcing inventory models.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic inventory systems with lead times are often challenging to optimize, including single-sourcing lost-sales and dual-sourcing inventory systems. Recent numerical results suggest that capped policies demonstrate superior performance over existing heuristics. However, the superior performance lacks a theoretical foundation, and why such policies generally perform so well remains a major open question. In this paper, we provide a theoretical foundation for this phenomenon in two classical inventory models with lead times. First, in a continuous-review lost-sales inventory model with lead times and Poisson demand, we prove that a so-called capped base-stock policy has a worst-case performance guarantee of 1.79 by conducting an asymptotic analysis under a large penalty cost and lead time. Second, we extend the analysis to a more complex continuous-review dual-sourcing inventory model with general lead times and Poisson demand and also prove that a so-called capped dual-index policy has a worst-case performance guarantee of 1.79 under large lead time and ordering cost differences. Our results provide a deeper understanding of the superior numerical performance of capped policies and present a new approach to proving worst-case performance guarantees of simple policies in hard inventory problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2150},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {1.79-approximation algorithms for continuous review single-sourcing lost-sales and dual-sourcing inventory models},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial patrolling in a uniform. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patrolling games were introduced by Alpern, Morton, and Papadaki in 2011 to model the adversarial problem where a mobile Patroller can thwart an attack at some location only by visiting it during the attack period, which has a prescribed integer duration. In this note, we modify the problem by allowing the Attacker to go to his planned attack location early and observe the presence or the absence there of the Patroller (who wears a uniform). To avoid being too predictable, the Patroller may sometimes remain at her base when she could have been visiting a possible attack location. The Attacker can then choose to delay attacking for some number of periods after the Patroller leaves his planned attack location. As shown here, this extra information for the Attacker can reduce thwarted attacks by as much as a factor of four in some cases. Our main finding is that the attack should begin in the second period the Patroller is away and the Patroller should never visit the same location (other than her base) in consecutive periods.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2152},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Adversarial patrolling in a uniform},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotically optimal lagrangian policies for
multi-warehouse, multi-store systems with lost sales. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a periodic-review inventory control problem for the Multi-Warehouse Multi-Store system with lost sales. We focus on a time horizon during which the system receives no external replenishment. Specifically, each warehouse has a finite initial inventory at the beginning of the horizon, which is then periodically allocated to the stores in each period in order to minimize the total expected lost-sales costs, holding costs, and shipping costs. This is a hard problem and the structure of its optimal policy is extremely complex. We develop simple heuristics based on Lagrangian relaxation that are easy to compute and implement, and have provably near-optimal performances. In particular, we show that the losses of our heuristics are sublinear in both the length of the time horizon and the number of stores . This improves the performance of existing heuristics in the literature whose losses are only sublinear in the number of stores. Numerical study shows that the heuristics perform very well. We also extend our analysis to the setting of positive delivery lead times.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2161},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal lagrangian policies for multi-warehouse, multi-store systems with lost sales},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing constrained shortest-paths at scale. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the needs of modern transportation service platforms, we study the problem of computing constrained shortest paths (CSP) at scale via preprocessing techniques. Our work makes two contributions in this regard: 1) We propose a scalable algorithm for CSP queries and show how its performance can be parametrized in terms of a new network primitive, the constrained highway dimension . This development extends recent work that established the highway dimension as the appropriate primitive for characterizing the performance of unconstrained shortest-path (SP) algorithms. Our main theoretical contribution is deriving conditions relating the two notions, thereby providing a characterization of networks where CSP and SP queries are of comparable hardness. 2) We develop practical algorithms for scalable CSP computation, augmenting our theory with additional network clustering heuristics. We evaluate these algorithms on real-world data sets to validate our theoretical findings. Our techniques are orders of magnitude faster than existing approaches while requiring only limited additional storage and preprocessing.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2166},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Computing constrained shortest-paths at scale},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the profitability of selfish blockchain mining under
consideration of ruin. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining blocks on a blockchain equipped with a proof of work consensus protocol is well known to be resource consuming. A miner bears the operational cost, mainly electricity consumption and IT gear, of mining and is compensated by a capital gain when a block is discovered. This paper aims at quantifying the profitability of mining when the possible event of ruin is also considered. This is done by formulating a tractable stochastic model and using tools from applied probability and analysis, including the explicit solution of a certain type of advanced functional differential equation. The expected profit at a future time point is determined for the situation when the miner follows the protocol as well as when the miner withholds blocks. The obtained explicit expressions allow us to analyze the sensitivity with respect to the different model components and to identify conditions under which selfish mining is a strategic advantage.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2169},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {On the profitability of selfish blockchain mining under consideration of ruin},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal sequential multiclass diagnosis. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential multiclass diagnosis, also known as multihypothesis testing, is a classical sequential decision problem with broad applications. However, the optimal solution remains, in general, unknown as the dynamic program suffers from the curse of dimensionality in the posterior belief space. We consider a class of practical problems in which the observation distributions associated with different classes are related through exponential tilting and show that the reachable beliefs could be restricted on, or near, a set of low-dimensional, time-dependent manifolds with closed-form expressions. This sparsity is driven by the low dimensionality of the observation distributions (which is intuitive) as well as by specific structural interrelations among them (which is less intuitive). We use a matrix factorization approach to uncover the potential low dimensionality hidden in high-dimensional beliefs and reconstruct the beliefs using a diagnostic statistic in lower dimension. For common univariate distributions, for example, normal, binomial, and Poisson, the belief reconstruction is exact and the optimal policies can be efficiently computed for a large number of classes. We also characterize the structure of the optimal policy in the reduced dimension. For multivariate distributions, we propose a low-rank matrix approximation scheme that works well when the beliefs are near the low-dimensional manifolds. The optimal policy significantly outperforms the state-of-the-art heuristic policy in quick diagnosis with noisy data.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2114},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Optimal sequential multiclass diagnosis},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast core pricing for rich advertising auctions.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard ad auction formats do not immediately extend to settings where multiple size configurations and layouts are available to advertisers. In these settings, the sale of web advertising space increasingly resembles a combinatorial auction with complementarities, where truthful auctions such as the Vickrey–Clarke–Groves (VCG) auction can yield unacceptably low revenue. We therefore study core-selecting auctions, which boost revenue by setting payments so that no group of agents, including the auctioneer, can jointly improve their utilities by switching to a different outcome. Our main result is a combinatorial algorithm that finds an approximate bidder-optimal core point with an almost linear number of calls to the welfare-maximization oracle. Our algorithm is faster than previously proposed heuristics in the literature and has theoretical guarantees. We conclude that core pricing is implementable even for very time-sensitive practical use cases such as real-time auctions for online advertising and can yield more revenue. We justify this claim experimentally usingMicrosoft Bing Ad Auction data, through which we show our core pricing algorithm generates almost 26\% more revenue than the VCG auction on average, about 9\% more revenue than other core pricing rules known in the literature, and almost matches the revenue of the standard generalized second price auction.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2104},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Fast core pricing for rich advertising auctions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Core pricing in combinatorial exchanges with financially
constrained buyers: Computational hardness and algorithmic solutions.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of market equilibria is a fundamental and practically relevant problem. Current advances in computational optimization allow for the organization of large combinatorial markets in the field. Although we know the computational complexity and the types of price functions necessary for combinatorial exchanges with quasilinear preferences, the respective literature does not consider financially constrained buyers. We show that computing market outcomes that respect budget constraints but are core stable is Σ 2 p -hard. Problems in this complexity class are rare, but ignoring budget constraints can lead to significant efficiency losses and instability, as we demonstrate in this paper. We introduce mixed integer bilevel linear programs (MIBLP) to compute core-stable market outcomes and provide effective column and constraint generation algorithms to solve these problems. Although full core stability quickly becomes intractable, we show that realistic problem sizes can actually be solved if the designer limits attention to deviations of small coalitions. This n -coalition stability is a practical approach to tame the computational complexity of the general problem and at the same time provides a reasonable level of stability for markets in the field where buyers have budget constraints.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2132},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Core pricing in combinatorial exchanges with financially constrained buyers: Computational hardness and algorithmic solutions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal genetic screening for cystic fibrosis. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cystic fibrosis (CF) is a life-threatening genetic disorder. Early treatment of CF-positive newborns can extend life span, improve quality of life, and reduce healthcare expenditures. As a result, newborns are screened for CF throughout the United States. Genetic testing is costly; therefore, CF screening processes start with a relatively inexpensive but not highly accurate biomarker test. Newborns with elevated biomarker levels are further screened via genetic testing for a panel of variants (types of mutations), selected from among hundreds of CF-causing variants, and newborns with mutations detected are referred for diagnostic testing, which corrects any false-positive screening results. Conversely, a false negative represents a missed CF diagnosis and delayed treatment. Therefore, an important decision is which CF-causing variants to include in the genetic testing panel so as to reduce the probability of a false negative under a testing budget that limits the number of variants in the panel. We develop novel deterministic and robust optimization models and identify key structural properties of optimal genetic testing panels. These properties lead to efficient, exact algorithms and key insights. Our case study underscores the value of our optimization-based approaches for CF newborn screening compared with current practices. Our findings have important implications for public policy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2134},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Optimal genetic screening for cystic fibrosis},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fair resource allocation in a volatile marketplace.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a setting where a platform dynamically allocates a collection of goods that arrive to the platform in an online fashion to budgeted buyers, as exemplified by online advertising systems where platforms decide which impressions to serve to various advertisers. Such dynamic resource allocation problems are challenging for two reasons. (a) The platform must strike a balance between optimizing the advertiser’s own revenues and guaranteeing fairness to the advertiser’s (repeat) buyers, and (b) the problem is inherently dynamic due to the uncertain, time-varying supply of goods available with the platform. We propose a stochastic approximation scheme akin to a dynamic market equilibrium. Our scheme relies on frequent resolves of an Eisenberg-Gale convex program and does not require the platform to have any knowledge about how the goods’ arrival processes evolve over time. The scheme fully extracts buyer budgets (thus maximizing platform revenues) and at the same time provides a 0.64 approximation of the proportionally fair allocation of goods achievable in the offline case, as long as the supply of goods comes from a wide family of (possibly nonstationary) Gaussian processes. We then deal with a multi-objective problem where the platform is concerned with both the proportional fairness and efficiency of the allocation and propose a hybrid algorithm that achieves a 0.3 bicriteria guarantee against fairness and efficiency. Finally, we build a sequence of datasets, one public dataset released by the DSP iPinYou and the second based on real Google AdX data, and use them to test the empirical performance of our schemes. We find that across these datasets there is a surprising relationship between fairness and efficiency that can be used to tune the schemes to nearly optimal performance in practice.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2049},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Fair resource allocation in a volatile marketplace},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Customer choice models vs. Machine learning: Finding optimal
product displays on alibaba. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We compare the performance of two approaches for finding the optimal set of products to display to customers landing on Alibaba’s two online marketplaces, Tmall and Taobao. We conducted a large-scale field experiment, in which we randomly assigned 10,421,649 customer visits during a one-week-long period to one of the two approaches and measured the revenue generated per customer visit. The first approach we tested was Alibaba’s current practice, which embeds product and customer features within a sophisticated machine-learning algorithm to estimate the purchase probabilities of each product for the customer at hand. The products with the largest expected revenue (revenue × predicted purchase probability) are then made available for purchase. Our second approach, which we developed and implemented in collaboration with Alibaba engineers, uses a featurized multinomial logit (MNL) model to predict purchase probabilities for each arriving customer. We used historical sales data to fit the MNL model, and then, for each arriving customer, we solved a cardinality-constrained assortment-optimization problem under the MNL model to find the optimal set of products to display. Our field experiments revealed that the MNL-based approach generated 5.17 renminbi (RMB) per customer visit, compared with the 4.04 RMB per customer visit generated by the machine-learning-based approach when both approaches were given access to the same set of the 25 most important features. This improvement represents a 28\% gain in revenue per customer visit, which corresponds to a 4 million RMB improvement over the week in which the experiments were conducted. Motivated by the results of our initial field experiment, Alibaba then implemented a full-featured version of our MNL-based approach, which now serves the majority of customers in this setting. Using another small-scale field experiment, we estimate that our new MNL-based approach that utilizes the full feature set is able to increase Alibaba’s annual revenue by 87.26 million RMB (12.42 million U.S. dollars).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2158},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Customer choice models vs. machine learning: Finding optimal product displays on alibaba},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing large market equilibria using abstractions.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing market equilibria is an important practical problem for market design, for example, in fair division of items. However, computing equilibria requires large amounts of information (typically the valuation of every buyer for every item) and computing power. We consider ameliorating these issues by applying a method used for solving complex games: constructing a coarsened abstraction of a given market, solving for the equilibrium in the abstraction, and lifting the prices and allocations back to the original market. We show how to bound important quantities such as regret, envy, Nash social welfare, Pareto optimality, and maximin share/proportionality when the abstracted prices and allocations are used in place of the real equilibrium. We then study two abstraction methods of interest for practitioners: (1) filling in unknown valuations using techniques from matrix completion and (2) reducing the problem size by aggregating groups of buyers/items into smaller numbers of representative buyers/items and solving for equilibrium in this coarsened market. We find that in real data allocations/prices that are relatively close to equilibria can be computed from even very coarse abstractions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2163},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Computing large market equilibria using abstractions},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal market-integration decisions by policymakers:
Modeling and analysis of agriculture market data. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2021.2191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policymakers often seek to integrate markets as a way to maximize social welfare. Prior research has examined the effect of market integration on social welfare (surplus) only at two extremes—when the markets are fully integrated and when they are fully isolated. But there is scarce information available for (i) how large the social surplus is at intermediate levels of market integration and (ii) whether social surplus is maximized when markets are fully integrated, fully isolated, or partially integrated. In this article, we consider the spectrum of all possible integration policies spanning full isolation to complete integration, and characterize the socially optimal market integration, under general demands. Our setting consists of a policymaker, a price-setting firm, and a continuum of consumers in two markets. We identify market conditions under which social surplus is indeed maximized at partial market integration. For the linear price-responsive demand model, these conditions are identified as thresholds on (i) the relative size of the markets being integrated and (ii) the relative price sensitivity of consumers in these markets. We then apply the model to the commercial seed market in the European Union (EU). We first identify the optimal level of market integration between the markets for seed corn in various countries in the EU. Subsequent analysis shows that socially optimal market integration for these countries provides a further improvement in the social surplus for the EU by 2.80\%, relative to complete integration. Overall, our results show that policymakers should exercise caution in determining the extent to which markets are integrated.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2191},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Optimal market-integration decisions by policymakers: Modeling and analysis of agriculture market data},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the optimal design of a bipartite matching queueing
system. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multiclass multiserver queueing system and study the problem of designing an optimal matching topology (or service compatibility structure) between customer classes and servers under a first come first served—assign longest idle server (FCFS-ALIS) service discipline. Specifically, we are interested in finding matching topologies that optimize—in a Pareto efficiency sense—the trade-off between two competing objectives: (i) minimizing customers’ waiting time delays and (ii) maximizing matching rewards generated by pairing customers and servers. Our analysis of the problem is divided into three main parts. First, under heavy-traffic conditions, we show that any bipartite matching system can be partitioned into a collection of complete resource pooling (CRP) subsystems, which are interconnected by means of a direct acyclic graph (DAG). We show that this, together with the aggregate service capacity on each CRP, fully determines the vector of steady-state waiting times. In particular, we show that the average (scaled) steady-state delay across all customer classes is asymptotically equal to the number of CRP components divided by the total system capacity. Second, since computing matching rewards under a FCFS-ALIS service discipline is computationally infeasible as the number of customer classes and servers grow large, we propose a quadratic programming (QP) formulation to approximate matching rewards. We show that the QP formulation is exact for a number of instances of the problem and provides a very good approximation in general. Extensive numerical experiments show that in over 98\% of problem instances the relative error between the exact rewards and the QP approximate rewards is less than 2\%. Lastly, combining our characterization of average delays in terms of the number of CRP components and the QP formulation to compute matching rewards, we propose a mixed-integer linear program that can be used to find the set of matching topologies that define the Pareto frontier of reward-delay pairs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2027},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {On the optimal design of a bipartite matching queueing system},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nash social welfare approximation for strategic agents.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central goal in the long literature on fair division is the design of mechanisms that implement fair outcomes, despite the participants’ strategic behavior. We study this question by measuring the fairness of an allocation using the geometric mean of the agents’ values, known as the Nash social welfare (NSW). This objective is maximized by widely known concepts such as the Nash bargaining solution, proportional fairness, and the competitive equilibrium with equal incomes; we focus on (approximately) implementing this objective and analyze the Trading Post mechanism. We consider allocating goods that are substitutes or complements and show that this mechanism achieves an approximation of two for concave utility functions and becomes essentially optimal for complements, where it can reach ( 1 + ɛ ) for any ( ɛ &gt; 0 ) . Moreover, we show that the Nash equilibria of this mechanism are pure and provide individual fairness in the sense of proportionality.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2056},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Nash social welfare approximation for strategic agents},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A restless bandit model for resource allocation,
competition, and reservation. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a resource allocation problem with varying requests and with resources of limited capacity shared by multiple requests. It is modeled as a set of heterogeneous restless multiarmed bandit problems (RMABPs) connected by constraints imposed by resource capacity. Following Whittle’s relaxation idea and Weber and Weiss’ asymptotic optimality proof, we propose a simple policy and prove it to be asymptotically optimal in a regime where both arrival rates and capacities increase. We provide a simple sufficient condition for asymptotic optimality of the policy and, in complete generality, propose a method that generates a set of candidate policies for which asymptotic optimality can be checked. The effectiveness of these results is demonstrated by numerical experiments. To the best of our knowledge, this is the first work providing asymptotic optimality results for such a resource allocation problem and such a combination of multiple RMABPs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2066},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {A restless bandit model for resource allocation, competition, and reservation},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Knockout-tournament procedures for large-scale ranking and
selection in parallel computing environments. <em>OR</em>,
<em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On one hand, large-scale ranking and selection (R&amp;S) problems require a large amount of computation. On the other hand, parallel computing environments that provide a large capacity for computation are becoming prevalent today, and they are accessible by ordinary users. Therefore, solving large-scale R&amp;S problems in parallel computing environments has emerged as an important research topic in recent years. However, directly implementing traditional stagewise procedures and fully sequential procedures in parallel computing environments may encounter problems because either the procedures require too many simulation observations or the procedures’ selection structures induce too many comparisons and too frequent communications among the processors. In this paper, inspired by the knockout-tournament arrangement of tennis Grand Slam tournaments, we develop new R&amp;S procedures to solve large-scale problems in parallel computing environments. We show that no matter whether the variances of the alternatives are known or not, our procedures can theoretically achieve the lowest growth rate on the expected total sample size with respect to the number of alternatives and thus, are optimal in rate. Moreover, common random numbers can be easily adopted in our procedures to further reduce the total sample size. Meanwhile, the comparison time in our procedures is negligible compared with the simulation time, and our procedures barely request for communications among the processors.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2065},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Knockout-tournament procedures for large-scale ranking and selection in parallel computing environments},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven optimization: A reproducing kernel hilbert space
approach. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two methods, based on regression in reproducing kernel Hilbert spaces, for solving an optimization problem with uncertain parameters for which we have historical data, including auxiliary data. The first method approximates the objective function and the second approximates the optimizer. We provide finite sample guarantees and prove asymptotic optimality for both methods. Computational experiments suggest that at least the second method overcomes a curse of dimensionality that afflicts existing methods, extrapolates better to unseen data, and achieves a many-fold decrease in sample complexity even for small dimensions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2069},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Data-driven optimization: A reproducing kernel hilbert space approach},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network pricing: How to induce optimal flows under strategic
link operators. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pricing games provide a framework for modeling real-world settings with two types of strategic agents: operators of a network and users of the network. Operators of the network post a price so as to attract users and maximize profit; users of the network select routes based on these prices and congestion from other users. Motivated by the fact that equilibrium in these games may not exist, may not be unique, and may induce an inefficient network performance, our main result is to observe that a simple regulation on the network owners’ market solves these three issues. Specifically, if an authority could set appropriate caps (upper bounds) on the tolls (prices) operators can charge, then the game among the link operators has a unique and strong Nash equilibrium and the users’ game results in a Wardrop equilibrium that achieves the optimal total delay. We call any price vector with these properties a great set of tolls and investigate the efficiency of great tolls with respect to the users’ surplus. We derive a bicriteria bound that compares the users’ surplus under great tolls with the users’ surplus under optimal tolls. Finally, we consider two different extensions of the model. First, we assume that operators face operating costs that depend on the amount of flow on the link, for which we prove existence of great tolls. Second, we allow operators to own more than one link. In this case, we prove that, when operators own complementary links (i.e., links for which an increase in toll value may only increase the flow on the other owned links), any toll vector that induces the optimal flow and that is upper bounded by the marginal tolls is a great set of tolls, and furthermore, we show that, when all links in the network are complementary, then the aforementioned toll vector is also a strong cap equilibrium.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2067},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Network pricing: How to induce optimal flows under strategic link operators},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributionally robust inverse covariance estimation: The
wasserstein shrinkage estimator. <em>OR</em>, <em>70</em>(1), iii–viii.
(<a href="https://doi.org/10.1287/opre.2020.2076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a distributionally robust maximum likelihood estimation model with a Wasserstein ambiguity set to infer the inverse covariance matrix of a p -dimensional Gaussian random vector from n independent samples. The proposed model minimizes the worst case (maximum) of Stein’s loss across all normal reference distributions within a prescribed Wasserstein distance from the normal distribution characterized by the sample mean and the sample covariance matrix. We prove that this estimation problem is equivalent to a semidefinite program that is tractable in theory but beyond the reach of general-purpose solvers for practically relevant problem dimensions p . In the absence of any prior structural information, the estimation problem has an analytical solution that is naturally interpreted as a nonlinear shrinkage estimator. Besides being invertible and well conditioned even for p&gt;n , the new shrinkage estimator is rotation equivariant and preserves the order of the eigenvalues of the sample covariance matrix. These desirable properties are not imposed ad hoc but emerge naturally from the underlying distributionally robust optimization model. Finally, we develop a sequential quadratic approximation algorithm for efficiently solving the general estimation problem subject to conditional independence constraints typically encountered in Gaussian graphical models.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2076},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Distributionally robust inverse covariance estimation: The wasserstein shrinkage estimator},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long-term behavior of dynamic equilibria in fluid queuing
networks. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fluid queuing network constitutes one of the simplest models in which to study flow dynamics over a network. In this model we have a single source-sink pair, and each link has a per-time-unit capacity and a transit time. A dynamic equilibrium (or equilibrium flow over time) is a flow pattern over time such that no flow particle has incentives to unilaterally change its path. Although the model has been around for almost 50 years, only recently results regarding existence and characterization of equilibria have been obtained. In particular, the long-term behavior remains poorly understood. Our main result in this paper is to show that, under a natural (and obviously necessary) condition on the queuing capacity, a dynamic equilibrium reaches a steady state (after which queue lengths remain constant) in finite time. Previously, it was not even known that queue lengths would remain bounded. The proof is based on the analysis of a rather nonobvious potential function that turns out to be monotone along the evolution of the equilibrium. Furthermore, we show that the steady state is characterized as an optimal solution of a certain linear program. When this program has a unique solution, which occurs generically, the long-term behavior is completely predictable. On the contrary, if the linear program has multiple solutions, the steady state is more difficult to identify as it depends on the whole temporal evolution of the equilibrium.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2081},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Long-term behavior of dynamic equilibria in fluid queuing networks},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scheduling to differentiate service in a multiclass service
system. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by large-scale service systems, we study a multiclass queueing system having class-dependent service rates and heterogeneous abandonment distributions. Our objective is to devise proper staffing and scheduling schemes to achieve differentiated services for each class. Formally, for a class-specific delay target w i &gt; 0 and threshold α i ∈ ( 0,1 ) , we concurrently determine an appropriate staffing level (number of servers) and a server-assignment rule (assigning newly idle servers to a waiting customer from one of the classes), under which the percentage of class- i customers waiting more than w i does not exceed α i . We tackle the problem under the efficiency-driven many-server heavy-traffic limiting regime, where both the demand volume and the number of servers grow proportionally to infinity. Our main findings are as follows: (a) class-level service differentiation is obtained by using a delay-based dynamic prioritization scheme; (b) the proposed scheduling rule achieves an important state-space collapse, in which all waiting time processes evolve as fixed proportions of a one-dimensional state-descriptor called the frontier process ; (c) the frontier process solves a stochastic Volterra equation and is thus a non-Markovian process; (d) the proposed staffing-and-scheduling solution can be readily extended to time-varying settings. In this paper, we establish heavy-traffic limit theorems to show that our solution is asymptotically correct for large systems, and we numerically demonstrate that it performs reasonably well even for relatively small systems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2075},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Scheduling to differentiate service in a multiclass service system},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A token-based central queue with order-independent service
rates. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a token-based central queue with multiple customer types. Customers of each type arrive according to a Poisson process and have an associated set of compatible tokens. Customers may only receive service when they have claimed a compatible token. If, upon arrival, more than one compatible token is available, then an assignment rule determines which token will be claimed. The service rate obtained by a customer is state-dependent, that is, it depends on the set of claimed tokens and on the number of customers in the system. Our first main result shows that, provided the assignment rule and the service rates satisfy certain conditions, the steady-state distribution has a product form. We show that our model subsumes known families of models that have product-form steady-state distributions, including the order-independent queue of Krzesinski and the multi-type customer and server model of Visschers et al. Our second main contribution involves the derivation of expressions for relevant performance measures such as the sojourn time and the number of customers present in the system. We apply our framework to relevant models, including an M/M/K queue with heterogeneous service rates, the MSCCC queue, and multiserver models with redundancy. For some of these models, we present expressions for performance measures that have not been derived before.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2088},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {A token-based central queue with order-independent service rates},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mechanism design for correlated valuations: Efficient
methods for revenue maximization. <em>OR</em>, <em>70</em>(1), iii–viii.
(<a href="https://doi.org/10.1287/opre.2020.2092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, the mechanism design literature has been primarily focused on settings where the bidders’ valuations are independent. However, in settings where valuations are correlated , much stronger results are possible. For example, the entire surplus of efficient allocations can be extracted as revenue. These stronger results are true, in theory, under generic conditions on parameter values. However, in practice, they are rarely, if ever, implementable because of the stringent requirement that the mechanism designer knows the distribution of the bidders types exactly. In this work, we provide a computationally efficient and sample efficient method for designing mechanisms that can robustly handle imprecise estimates of the distribution over bidder valuations. This method guarantees that the selected mechanism will perform at least as well as any ex post mechanism with high probability. The mechanism also performs nearly optimally with sufficient information and correlation. Furthermore, we show that when the distribution is not known and must be estimated from samples from the true distribution, a sufficiently high degree of correlation is essential to implement optimal mechanisms. Finally, we demonstrate through simulations that this new mechanism design paradigm generates mechanisms that perform significantly better than traditional mechanism design techniques given sufficient samples.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2092},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Mechanism design for correlated valuations: Efficient methods for revenue maximization},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mean field equilibrium: Uniqueness, existence, and
comparative statics. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard solution concept for stochastic games is Markov perfect equilibrium; however, its computation becomes intractable as the number of players increases. Instead, we consider mean field equilibrium (MFE), which has been popularized in recent literature. MFE takes advantage of averaging effects in models with a large number of players. We make three main contributions. First, our main result provides conditions that ensure the uniqueness of an MFE. We believe this uniqueness result is the first of its nature in the class of models we study. Second, we generalize previous MFE existence results. Third, we provide general comparative statics results. We apply our results to dynamic oligopoly models and to heterogeneous agent macroeconomic models commonly used in previous work in economics and operations.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2090},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Mean field equilibrium: Uniqueness, existence, and comparative statics},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to approximate industrial problems by operations
research classic problems. <em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practitioners of operations research often consider difficult variants of well-known optimization problems and struggle to find a good algorithm for their variants although decades of research have produced highly efficient algorithms for the well-known problems. We introduce a machine learning for operations research paradigm to build efficient heuristics for such variants: use a machine learning predictor to turn an instance of the variant into an instance of the well-known problem, then solve the instance of the well-known problem, and finally retrieve a solution of the variant from the solution of the well-known problem. This paradigm requires learning the predictor that transforms an instance of the variant into an instance of the well-known problem. We introduce a structured learning methodology to learn that predictor. We illustrate our paradigm and learning methodology on path problems. We, therefore, introduce a maximum likelihood approach to approximate an arbitrary path problem on an acyclic digraph by a usual shortest path problem. Because path problems play an important role as pricing subproblems of column-generation approaches, we introduce matheuristics that leverage our approximations in that context. Numerical experiments show their efficiency on two stochastic vehicle scheduling problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2094},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Learning to approximate industrial problems by operations research classic problems},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Technical note—two-stage sample robust optimization.
<em>OR</em>, <em>70</em>(1), iii–viii. (<a
href="https://doi.org/10.1287/opre.2020.2096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a simple approximation scheme, based on overlapping linear decision rules, for solving data-driven two-stage distributionally robust optimization problems with the type- ∞ Wasserstein ambiguity set. Our main result establishes that this approximation scheme is asymptotically optimal for two-stage stochastic linear optimization problems; that is, under mild assumptions, the optimal cost and optimal first-stage decisions obtained by approximating the robust optimization problem converge to those of the underlying stochastic problem as the number of data points grows to infinity. These guarantees notably apply to two-stage stochastic problems that do not have relatively complete recourse , which arise frequently in applications. In this context, we show through numerical experiments that the approximation scheme is practically tractable and produces decisions that significantly outperform those obtained from state-of-the-art data-driven alternatives.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.2096},
  journal      = {Operations Research},
  number       = {1},
  pages        = {iii-viii},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Two-stage sample robust optimization},
  volume       = {70},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
