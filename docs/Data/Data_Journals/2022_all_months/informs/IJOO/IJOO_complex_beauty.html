<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJOO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijoo---18">IJOO - 18</h2>
<ul>
<li><details>
<summary>
(2022). Gradient sampling methods with inexact subproblem solutions
and gradient aggregation. <em>IJOO</em>, <em>4</em>(4), 426–445. (<a
href="https://doi.org/10.1287/ijoo.2022.0073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient sampling (GS) methods for the minimization of objective functions that may be nonconvex and/or nonsmooth are proposed, analyzed, and tested. One of the most computationally expensive components of contemporary GS methods is the need to solve a convex quadratic subproblem in each iteration. By contrast, the methods proposed in this paper allow the use of inexact solutions of these subproblems, which, as proved in the paper, can be incorporated without the loss of theoretical convergence guarantees. Numerical experiments show that, by exploiting inexact subproblem solutions, one can consistently reduce the computational effort required by a GS method. Additionally, a strategy is proposed for aggregating gradient information after a subproblem is solved (potentially inexactly) as has been exploited in bundle methods for nonsmooth optimization. It is proved that the aggregation scheme can be introduced without the loss of theoretical convergence guarantees. Numerical experiments show that incorporating this gradient aggregation approach can also reduce the computational effort required by a GS method.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0073},
  journal      = {INFORMS Journal on Optimization},
  number       = {4},
  pages        = {426-445},
  shortjournal = {INFORMS J. Optim.},
  title        = {Gradient sampling methods with inexact subproblem solutions and gradient aggregation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A subsampling line-search method with second-order results.
<em>IJOO</em>, <em>4</em>(4), 403–425. (<a
href="https://doi.org/10.1287/ijoo.2022.0072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many contemporary optimization problems such as those arising in machine learning, it can be computationally challenging or even infeasible to evaluate an entire function or its derivatives. This motivates the use of stochastic algorithms that sample problem data, which can jeopardize the guarantees obtained through classical globalization techniques in optimization, such as a line search. Using subsampled function values is particularly challenging for the latter strategy, which relies upon multiple evaluations. For nonconvex data-related problems, such as training deep learning models, one aims at developing methods that converge to second-order stationary points quickly, that is, escape saddle points efficiently. This is particularly difficult to ensure when one only accesses subsampled approximations of the objective and its derivatives. In this paper, we describe a stochastic algorithm based on negative curvature and Newton-type directions that are computed for a subsampling model of the objective. A line-search technique is used to enforce suitable decrease for this model; for a sufficiently large sample, a similar amount of reduction holds for the true objective. We then present worst-case complexity guarantees for a notion of stationarity tailored to the subsampling context. Our analysis encompasses the deterministic regime and allows us to identify sampling requirements for second-order line-search paradigms. As we illustrate through real data experiments, these worst-case estimates need not be satisfied for our method to be competitive with first-order strategies in practice.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0072},
  journal      = {INFORMS Journal on Optimization},
  number       = {4},
  pages        = {403-425},
  shortjournal = {INFORMS J. Optim.},
  title        = {A subsampling line-search method with second-order results},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The backhaul profit maximization problem: Optimization
models and solution procedures. <em>IJOO</em>, <em>4</em>(4), 373–402.
(<a href="https://doi.org/10.1287/ijoo.2022.0071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a compact mixed integer program (MIP) for the backhaul profit maximization problem in which a freight carrier seeks to generate profit from an empty delivery vehicle’s backhaul trip from its last scheduled delivery to its depot by allowing it to deviate from the least expensive (or fastest) route to accept pickup-and-delivery requests between various points on the route as allowed by its capacity and required return time. The MIP is inspired by a novel representation of multicommodity flow that significantly reduces the size of the constraint matrix compared with a formulation based on the classical node-arc representation. This, in turn, leads to faster solution times when using a state-of-the-art MIP solver. In an empirical study of both formulations, problem instances with 10 potential pickup/drop-off locations and up to 72 pickup-and-delivery requests were solved an average 1.44 times faster in real time with our formulation, whereas instances with 20 locations and up to 332 pickup-and-delivery requests were solved an average of 11.88 times faster. The largest instances in the comparative study had 60 locations and up to 3,267 pickup-and-delivery requests; these instances required an average of more than 54 hours of real time to solve with the node-arc–based formulation but were solved in an average of under two hours of real time using our compact formulation. We also present a heuristic algorithm based on our compact formulation that finds near optimal solutions to each of the 60-location instances within 22 minutes of real time and near optimal solutions to instances with up to 80 locations within four and a half hours of real time.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0071},
  journal      = {INFORMS Journal on Optimization},
  number       = {4},
  pages        = {373-402},
  shortjournal = {INFORMS J. Optim.},
  title        = {The backhaul profit maximization problem: Optimization models and solution procedures},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Satisficing models under uncertainty. <em>IJOO</em>,
<em>4</em>(4), 347–372. (<a
href="https://doi.org/10.1287/ijoo.2021.0070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satisficing, as an approach to decision making under uncertainty, aims at achieving solutions that satisfy the problem’s constraints as well as possible. Mathematical optimization problems that are related to this form of decision making include the P-model. In this paper, we propose a general framework of satisficing decision criteria and show a representation termed the S-model, of which the P-model and robust optimization models are special cases. We then focus on the linear optimization case and obtain a tractable probabilistic S-model, termed the T-model, whose objective is a lower bound of the P-model. We show that when probability densities of the uncertainties are log-concave, the T-model can admit a tractable concave objective function. In the case of discrete probability distributions, the T-model is a linear mixed integer optimization problem of moderate dimensions. Our computational experiments on a stochastic maximum coverage problem suggest that the T-model solutions can be highly competitive compared with standard sample average approximation models.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0070},
  journal      = {INFORMS Journal on Optimization},
  number       = {4},
  pages        = {347-372},
  shortjournal = {INFORMS J. Optim.},
  title        = {Satisficing models under uncertainty},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimization under connected uncertainty. <em>IJOO</em>,
<em>4</em>(3), 326–346. (<a
href="https://doi.org/10.1287/ijoo.2021.0067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust optimization methods have shown practical advantages in a wide range of decision-making applications under uncertainty. Recently, their efficacy has been extended to multiperiod settings. Current approaches model uncertainty either independent of the past or in an implicit fashion by budgeting the aggregate uncertainty. In many applications, however, past realizations directly influence future uncertainties. For this class of problems, we develop a modeling framework that explicitly incorporates this dependence via connected uncertainty sets, whose parameters at each period depend on previous uncertainty realizations. To find optimal here-and-now solutions, we reformulate robust and distributionally robust constraints for popular set structures and demonstrate this modeling framework numerically on broadly applicable knapsack and portfolio-optimization problems.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0067},
  journal      = {INFORMS Journal on Optimization},
  number       = {3},
  pages        = {326-346},
  shortjournal = {INFORMS J. Optim.},
  title        = {Optimization under connected uncertainty},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interdicting low-diameter cohesive subgroups in large-scale
social networks. <em>IJOO</em>, <em>4</em>(3), 304–325. (<a
href="https://doi.org/10.1287/ijoo.2021.0068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The s-clubs model cohesive social subgroups as vertex subsets that induce subgraphs of diameter at most s . In defender-attacker settings, for low values of s , they can represent tightly knit communities, whose operation is undesirable for the defender. For instance, in online social networks, large communities of malicious accounts can effectively propagate undesirable rumors. In this article, we consider a defender that can disrupt vertices of the adversarial network to minimize its threat, which leads us to consider a maximum s -club interdiction problem, where interdiction is penalized in the objective function. Using a new notion of H -heredity in s -clubs, we provide a mixed-integer linear programming formulation for this problem that uses far fewer constraints than the formulation based on standard techniques. We show that the linear programming relaxation of this formulation has no redundant constraints and identify facets of the convex hull of integral feasible solutions under special conditions. We further relate H -heredity to latency- s -connected dominating sets and design a decomposition branch-and-cut algorithm for the problem. Our implementation solves benchmark instances with more than 10,000 vertices in a matter of minutes and is orders of magnitude faster than algorithms based on the standard formulation.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0068},
  journal      = {INFORMS Journal on Optimization},
  number       = {3},
  pages        = {304-325},
  shortjournal = {INFORMS J. Optim.},
  title        = {Interdicting low-diameter cohesive subgroups in large-scale social networks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal order batching in warehouse management: A
data-driven robust approach. <em>IJOO</em>, <em>4</em>(3), 278–303. (<a
href="https://doi.org/10.1287/ijoo.2021.0066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing warehouse processes has direct impact on supply chain responsiveness, timely order fulfillment, and customer satisfaction. In this work, we focus on the picking process in warehouse management and study it from a data perspective. Using historical data from an industrial partner, we introduce, model, and study the robust order batching problem (ROBP) that groups orders into batches to minimize total order processing time accounting for uncertainty caused by system congestion and human behavior. We provide a generalizable, data-driven approach that overcomes warehouse-specific assumptions characterizing most of the work in the literature. We analyze historical data to understand the processes in the warehouse, to predict processing times, and to improve order processing. We introduce the ROBP and develop an efficient learning-based branch-and-price algorithm based on simultaneous column and row generation, embedded with alternative prediction models such as linear regression and random forest that predict processing time of a batch. We conduct extensive computational experiments to test the performance of the proposed approach and to derive managerial insights based on real data. The data-driven prescriptive analytics tool we propose achieves savings of seven to eight minutes per order, which translates into a 14.8\% increase in daily picking operations capacity of the warehouse.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0066},
  journal      = {INFORMS Journal on Optimization},
  number       = {3},
  pages        = {278-303},
  shortjournal = {INFORMS J. Optim.},
  title        = {Optimal order batching in warehouse management: A data-driven robust approach},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effective budget of uncertainty for classes of robust
optimization. <em>IJOO</em>, <em>4</em>(3), 249–277. (<a
href="https://doi.org/10.1287/ijoo.2021.0069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust optimization (RO) tackles data uncertainty by optimizing for the worst-case scenario of an uncertain parameter and, in its basic form, is sometimes criticized for producing overly conservative solutions. To reduce the level of conservatism in RO, one can use the well-known budget-of-uncertainty approach, which limits the amount of uncertainty to be considered in the model. In this paper, we study a class of problems with resource uncertainty and propose a robust optimization methodology that produces solutions that are even less conservative than the conventional budget-of-uncertainty approach. We propose a new tractable two-stage robust optimization approach that identifies the “ineffective” parts of the uncertainty set and optimizes for the “effective” worst-case scenario only. In the first stage, we identify the effective range of the uncertain parameter, and in the second stage, we provide a formulation that eliminates the unnecessary protection for the ineffective parts and, hence, produces less conservative solutions and provides intuitive insights on the trade-off between robustness and solution conservatism. We demonstrate the applicability of the proposed approach using a power dispatch optimization problem with wind uncertainty. We also provide examples of other application areas that would benefit from the proposed approach.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0069},
  journal      = {INFORMS Journal on Optimization},
  number       = {3},
  pages        = {249-277},
  shortjournal = {INFORMS J. Optim.},
  title        = {Effective budget of uncertainty for classes of robust optimization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A class of convex quadratic nonseparable resource allocation
problems with generalized bound constraints. <em>IJOO</em>,
<em>4</em>(2), 215–247. (<a
href="https://doi.org/10.1287/ijoo.2021.0065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a convex quadratic nonseparable resource allocation problem that arises in the area of decentralized energy management (DEM), where unbalance in electricity networks has to be minimized. In this problem, the given resource is allocated over a set of activities that is divided into subsets, and a cost is assigned to the overall allocated amount of resources to activities within the same subset. We derive two efficient algorithms with O ( n log n ) worst-case time complexity to solve this problem. For the special case where all subsets have the same size, one of these algorithms even runs in linear time given the subset size. Both algorithms are inspired by well-studied breakpoint search methods for separable convex resource allocation problems. Numerical evaluations on both real and synthetic data confirm the theoretical efficiency of both algorithms and demonstrate their suitability for integration in DEM systems.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0065},
  journal      = {INFORMS Journal on Optimization},
  number       = {2},
  pages        = {215-247},
  shortjournal = {INFORMS J. Optim.},
  title        = {A class of convex quadratic nonseparable resource allocation problems with generalized bound constraints},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A manifold proximal linear method for sparse spectral
clustering with application to single-cell RNA sequencing data analysis.
<em>IJOO</em>, <em>4</em>(2), 200–214. (<a
href="https://doi.org/10.1287/ijoo.2021.0064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is one of the fundamental unsupervised learning methods and is widely used in data analysis. Sparse spectral clustering (SSC) imposes sparsity to the spectral clustering, and it improves the interpretability of the model. One widely adopted model for SSC in the literature is an optimization problem over the Stiefel manifold with nonsmooth and nonconvex objective. Such an optimization problem is very challenging to solve. Existing methods usually solve its convex relaxation or need to smooth its nonsmooth objective using certain smoothing techniques. Therefore, they were not targeting solving the original formulation of SSC. In this paper, we propose a manifold proximal linear method (ManPL) that solves the original SSC formulation without twisting the model. We also extend the algorithm to solve multiple-kernel SSC problems, for which an alternating ManPL algorithm is proposed. Convergence and iteration complexity results of the proposed methods are established. We demonstrate the advantage of our proposed methods over existing methods via clustering of several data sets, including University of California Irvine and single-cell RNA sequencing data sets.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0064},
  journal      = {INFORMS Journal on Optimization},
  number       = {2},
  pages        = {200-214},
  shortjournal = {INFORMS J. Optim.},
  title        = {A manifold proximal linear method for sparse spectral clustering with application to single-cell RNA sequencing data analysis},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning in sequential bilevel linear programming.
<em>IJOO</em>, <em>4</em>(2), 174–199. (<a
href="https://doi.org/10.1287/ijoo.2021.0063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a framework for sequential bilevel linear programming in which a leader and a follower interact over multiple time periods. In each period, the follower observes the actions taken by the leader and reacts optimally, according to the follower’s own objective function, which is initially unknown to the leader. By observing various forms of information feedback from the follower’s actions, the leader is able to refine the leader’s knowledge about the follower’s objective function and, hence, adjust the leader’s actions at subsequent time periods, which ought to help in maximizing the leader’s cumulative benefit. We show that greedy and robust policies adapted from previous work in the max-min (symmetric) setting might fail to recover the optimal full-information solution to the problem (i.e., a solution implemented by an oracle with complete prior knowledge of the follower’s objective function) in the asymmetric case. In contrast, we present a family of greedy and best-case policies that are able to recover the full-information optimal solution and also provide real-time certificates of optimality. In addition, we show that the proposed policies can be computed by solving a series of linear mixed-integer programs. We test policy performance through exhaustive numerical experiments in the context of asymmetric shortest path interdiction, considering various forms of feedback and several benchmark policies.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0063},
  journal      = {INFORMS Journal on Optimization},
  number       = {2},
  pages        = {174-199},
  shortjournal = {INFORMS J. Optim.},
  title        = {Learning in sequential bilevel linear programming},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning deep models: Critical points and local openness.
<em>IJOO</em>, <em>4</em>(2), 148–173. (<a
href="https://doi.org/10.1287/ijoo.2021.0062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of nonconvex deep models , developing a unifying theory for studying the optimization problems that arise from training these models becomes very significant. Toward this end, we present in this paper a unifying landscape analysis framework that can be used when the training objective function is the composite of simple functions. Using the local openness property of the underlying training models, we provide simple sufficient conditions under which any local optimum of the resulting optimization problem is globally optimal. We first completely characterize the local openness of the symmetric and nonsymmetric matrix multiplication mapping. Then we use our characterization to (1) provide a simple proof for the classical result of Burer-Monteiro and extend it to noncontinuous loss functions; (2) show that every local optimum of two-layer linear networks is globally optimal. Unlike many existing results in the literature, our result requires no assumption on the target data matrix Y , and input data matrix X ; (3) develop a complete characterization of the local/global optima equivalence of multilayer linear neural networks (we provide various counterexamples to show the necessity of each of our assumptions); and (4) show global/local optima equivalence of overparameterized nonlinear deep models having a certain pyramidal structure. In contrast to existing works, our result requires no assumption on the differentiability of the activation functions and can go beyond “full-rank” cases.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0062},
  journal      = {INFORMS Journal on Optimization},
  number       = {2},
  pages        = {148-173},
  shortjournal = {INFORMS J. Optim.},
  title        = {Learning deep models: Critical points and local openness},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A solution approach to distributionally robust
joint-chance-constrained assignment problems. <em>IJOO</em>,
<em>4</em>(2), 125–147. (<a
href="https://doi.org/10.1287/ijoo.2021.0060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the assignment problem with chance constraints (CAP) and its distributionally robust counterpart DR-CAP. We present a technique for estimating big-M in such a formulation that takes advantage of the ambiguity set. We consider a 0-1 bilinear knapsack set to develop valid inequalities for CAP and DR-CAP. This is generalized to the joint chance constraint problem. A probability cut framework is also developed to solve DR-CAP. A computational study on problem instances obtained from using real hospital surgery data shows that the developed techniques allow us to solve certain model instances and reduce the computational time for others. The use of Wasserstein ambiguity set in the DR-CAP model improves the out-of-sample performance of satisfying the chance constraints more significantly than the one possible by increasing the sample size in the sample average approximation technique. The solution time for DR-CAP model instances is of the same order as that for solving the CAP instances. This finding is important because chance constrained optimization models are very difficult to solve when the coefficients in the constraints are random.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0060},
  journal      = {INFORMS Journal on Optimization},
  number       = {2},
  pages        = {125-147},
  shortjournal = {INFORMS J. Optim.},
  title        = {A solution approach to distributionally robust joint-chance-constrained assignment problems},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving sample average approximation using distributional
robustness. <em>IJOO</em>, <em>4</em>(1), 90–124. (<a
href="https://doi.org/10.1287/ijoo.2021.0061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider stochastic optimization problems in which we aim to minimize the expected value of an objective function with respect to an unknown distribution of random parameters. We analyse the out-of-sample performance of solutions obtained by solving a distributionally robust version of the sample average approximation problem for unconstrained quadratic problems and derive conditions under which these solutions are improved in comparison with those of the sample average approximation. We compare different mechanisms for constructing a robust solution: phi-divergence using both total variation and standard smooth ϕ functions; a CVaR-based risk measure; and a Wasserstein metric.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0061},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {90-124},
  shortjournal = {INFORMS J. Optim.},
  title        = {Improving sample average approximation using distributional robustness},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributionally robust optimization with confidence bands
for probability density functions. <em>IJOO</em>, <em>4</em>(1), 65–89.
(<a href="https://doi.org/10.1287/ijoo.2021.0059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributionally robust optimization (DRO) has been introduced for solving stochastic programs in which the distribution of the random variables is unknown and must be estimated by samples from that distribution. A key element of DRO is the construction of the ambiguity set, which is a set of distributions that contains the true distribution with a high probability. Assuming that the true distribution has a probability density function, we propose a class of ambiguity sets based on confidence bands of the true density function. As examples, we consider the shape-restricted confidence bands and the confidence bands constructed with a kernel density estimation technique. The former allows us to incorporate the prior knowledge of the shape of the underlying density function (e.g., unimodality and monotonicity), and the latter enables us to handle multidimensional cases. Furthermore, we establish the convergence of the optimal value of DRO to that of the underlying stochastic program as the sample size increases. The DRO with our ambiguity set involves functional decision variables and infinitely many constraints. To address this challenge, we apply duality theory to reformulate the DRO to a finite-dimensional stochastic program, which is amenable to a stochastic subgradient scheme as a solution method.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0059},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {65-89},
  shortjournal = {INFORMS J. Optim.},
  title        = {Distributionally robust optimization with confidence bands for probability density functions},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive stochastic variance reduction for subsampled newton
method with cubic regularization. <em>IJOO</em>, <em>4</em>(1), 45–64.
(<a href="https://doi.org/10.1287/ijoo.2021.0058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cubic regularized Newton method of Nesterov and Polyak has become increasingly popular for nonconvex optimization because of its capability of finding an approximate local solution with a second order guarantee and its low iteration complexity. Several recent works extend this method to the setting of minimizing the average of N smooth functions by replacing the exact gradients and Hessians with subsampled approximations. It is shown that the total Hessian sample complexity can be reduced to be sublinear in N per iteration by leveraging stochastic variance reduction techniques. We present an adaptive variance reduction scheme for a subsampled Newton method with cubic regularization and show that the expected Hessian sample complexity is O ( N + N 2 / 3 ϵ − 3 / 2 ) for finding an ( ϵ , ϵ ) -approximate local solution (in terms of first and second order guarantees, respectively). Moreover, we show that the same Hessian sample complexity is retained with fixed sample sizes if exact gradients are used. The techniques of our analysis are different from previous works in that we do not rely on high probability bounds based on matrix concentration inequalities. Instead, we derive and utilize new bounds on the third and fourth order moments of the average of random matrices, which are of independent interest on their own.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0058},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {45-64},
  shortjournal = {INFORMS J. Optim.},
  title        = {Adaptive stochastic variance reduction for subsampled newton method with cubic regularization},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smooth and flexible dual optimal inequalities.
<em>IJOO</em>, <em>4</em>(1), 29–44. (<a
href="https://doi.org/10.1287/ijoo.2021.0057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of accelerating column generation (CG) for set-covering formulations via dual optimal inequalities (DOIs). We study two novel classes of DOIs, which are referred to as Flexible DOIs (F-DOIs) and Smooth-DOIs (S-DOIs), respectively (and jointly as SF-DOIs). F-DOIs provide rebates for covering items more than necessary. S-DOIs describe the payment of a penalty to permit the undercoverage of items in exchange for the overinclusion of other items. Unlike other classes of DOIs from the literature, the S-DOIs and F-DOIs rely on very little problem-specific knowledge and, as such, have the potential to be applied to a vast number of problem domains. In particular, we discuss the application of the new DOIs to three relevant problems: the single-source capacitated facility location problem, the capacitated p-median problem, and the capacitated vehicle-routing problem. We provide computational evidence of the strength of the new inequalities by embedding them within a column-generation solver for these problems. Substantial speedups can be observed as when compared with a nonstabilized variant of the same CG procedure to achieve the linear-relaxation lower bound on problems with dense columns and structured assignment costs.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0057},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {29-44},
  shortjournal = {INFORMS J. Optim.},
  title        = {Smooth and flexible dual optimal inequalities},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Critical-path-search logic-based benders decomposition
approaches for flexible job shop scheduling. <em>IJOO</em>,
<em>4</em>(1), 1–28. (<a
href="https://doi.org/10.1287/ijoo.2021.0056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We solve the flexible job shop scheduling problems (F-JSSPs) to minimize makespan. First, we compare the constraint programming (CP) model with the mixed-integer programming (MIP) model for F-JSSPs. Second, we exploit the decomposable structure within the models and develop an efficient CP–logic-based Benders decomposition (CP-LBBD) technique that combines the complementary strengths of MIP and CP models. Using 193 instances from the literature, we demonstrate that MIP, CP, and CP-LBBD achieve average optimality gaps of 25.50\%, 13.46\%, and 0.37\% and find optima in 49, 112, and 156 instances of the problem, respectively. We also compare the performance of the CP-LBBD with an efficient Greedy Randomized Adaptive Search Procedure (GRASP) algorithm, which has been appraised for finding 125 optima on 178 instances. CP-LBBD finds 143 optima on the same set of instances. We further examine the performance of the algorithms on 96 newly (and much larger) generated instances and demonstrate that the average optimality gap of the CP increases to 47.26\%, whereas the average optimality of CP-LBBD remains around 1.44\%. Finally, we conduct analytics on the performance of our models and algorithms and counterintuitively find out that as flexibility increases in data sets the performance CP-LBBD ameliorates, whereas that of the CP and MIP significantly deteriorates.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0056},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {1-28},
  shortjournal = {INFORMS J. Optim.},
  title        = {Critical-path-search logic-based benders decomposition approaches for flexible job shop scheduling},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
