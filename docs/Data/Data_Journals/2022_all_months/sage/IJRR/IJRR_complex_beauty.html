<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJRR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijrr---49">IJRR - 49</h2>
<ul>
<li><details>
<summary>
(2022). Supervised learning and reinforcement learning of feedback
models for reactive behaviors: Tactile feedback testbed. <em>The
International Journal of Robotics Research</em>, <em>41</em>(13-14),
1121–1145. (<a href="https://doi.org/10.1177/02783649221143399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robots need to be able to adapt to unexpected changes in the environment such that they can autonomously succeed in their tasks. However, hand-designing feedback models for adaptation is tedious, if at all possible, making data-driven methods a promising alternative. In this paper, we introduce a full framework for learning feedback models for reactive motion planning. Our pipeline starts by segmenting demonstrations of a complete task into motion primitives via a semi-automated segmentation algorithm. Then, given additional demonstrations of successful adaptation behaviors, we learn initial feedback models through learning-from-demonstrations. In the final phase, a sample-efficient reinforcement learning algorithm fine-tunes these feedback models for novel task settings through few real system interactions. We evaluate our approach on a real anthropomorphic robot in learning a tactile feedback task.},
  archive  = {J},
  author   = {Giovanni Sutanto and Katharina Rombach and Yevgen Chebotar and Zhe Su and Stefan Schaal and Gaurav S. Sukhatme and Franziska Meier},
  doi      = {10.1177/02783649221143399},
  journal  = {The International Journal of Robotics Research},
  month    = {11-12},
  number   = {13-14},
  pages    = {1121-1145},
  title    = {Supervised learning and reinforcement learning of feedback models for reactive behaviors: Tactile feedback testbed},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuum robot state estimation using gaussian process
regression on SE(3). <em>The International Journal of Robotics
Research</em>, <em>41</em>(13-14), 1099–1120. (<a
href="https://doi.org/10.1177/02783649221128843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continuum robots have the potential to enable new applications in medicine, inspection, and countless other areas due to their unique shape, compliance, and size. Excellent progress has been made in the mechanical design and dynamic modeling of continuum robots, to the point that there are some canonical designs, although new concepts continue to be explored. In this paper, we turn to the problem of state estimation for continuum robots that can been modeled with the common Cosserat rod model. Sensing for continuum robots might comprise external camera observations, embedded tracking coils, or strain gauges. We repurpose a Gaussian process (GP) regression approach to state estimation, initially developed for continuous-time trajectory estimation in SE(3). In our case, the continuous variable is not time but arclength and we show how to estimate the continuous shape (and strain) of the robot (along with associated uncertainties) given discrete, noisy measurements of both pose and strain along the length. We demonstrate our approach quantitatively through simulations as well as through experiments. Our evaluations show that accurate and continuous estimates of a continuum robot’s shape can be achieved, resulting in average end-effector errors between the estimated and ground truth shape as low as 3.5 mm and 0.016° in simulation or 3.3 mm and 0.035° for unloaded configurations and 6.2 mm and 0.041° for loaded ones during experiments, when using discrete pose measurements.},
  archive  = {J},
  author   = {Sven Lilge and Timothy D. Barfoot and Jessica Burgner-Kahrs},
  doi      = {10.1177/02783649221128843},
  journal  = {The International Journal of Robotics Research},
  month    = {11-12},
  number   = {13-14},
  pages    = {1099-1120},
  title    = {Continuum robot state estimation using gaussian process regression on SE(3)},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constraint-consistent task-oriented whole-body robot
formulation: Task, posture, constraints, multiple contacts, and balance.
<em>The International Journal of Robotics Research</em>,
<em>41</em>(13-14), 1079–1098. (<a
href="https://doi.org/10.1177/02783649221120029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a comprehensive formulation to the problem of controlling a high-dimensional robotic system involving complex tasks subject to a variety of constraints, obstacles, balance, and contact challenges. Using intuitive and natural representations, the approach is initiated by establishing individual objectives for a task and its constraints. Simple independent controllers using artificial potential fields are then designed for each objective to reach goals while enforcing the constraints. Dynamically consistent projections in nullspaces associated with task and constraint representations are employed to deliver a coherent whole-body robot control. In multi-link multi-contact tasks, contact forces produce both resulting and internal forces. Internal forces play a critical role in robot balance and stability, achieved in this framework through modeling and controlling virtual linkages that explicitly describe the relationship between active/passive contact force, resultant force, controlled/uncontrolled internal force for multi-link multi-contact underactuated robots. Control of contacts with the environment involves material considerations such as friction and geometric constraints. Potential barriers direct the selection of contact forces ensuring stability and balance. This approach of dynamic projection and the Virtual Linkage Model addresses robot underactuation. In addition, the framework introduces a coordinate completion mechanism to establish a generalized coordinates representation of the task, removing redundancy and maintaining the full operational space dynamics description. This enables task-space dynamic control based on the relevant inertial properties. We present the experimental validation on a physical humanoid platform.},
  archive  = {J},
  author   = {Oussama Khatib and Mikael Jorda and Jaeheung Park and Luis Sentis and Shu-Yun Chung},
  doi      = {10.1177/02783649221120029},
  journal  = {The International Journal of Robotics Research},
  month    = {11-12},
  number   = {13-14},
  pages    = {1079-1098},
  title    = {Constraint-consistent task-oriented whole-body robot formulation: Task, posture, constraints, multiple contacts, and balance},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The split-belt rimless wheel. <em>The International Journal
of Robotics Research</em>, <em>41</em>(11-12), 1043–1076. (<a
href="https://doi.org/10.1177/02783649221110260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Split-belt treadmill walking, in which the two belts move at different speeds, reveals a mechanism through which energy can be extracted from the environment. When a person walks with positive step length asymmetry on a split-belt treadmill, the treadmill can perform net positive work on the person. Here we use a split-belt rimless wheel model to explore how people could take advantage of the treadmill. We show that a split-belt rimless wheel can passively walk steadily by capturing energy from the treadmill to overcome collision losses, whereas it loses energy on each step with no way to recover the losses when walking on tied belts. Our simulated split-belt rimless wheel can walk steadily for a variety of leg angle and belt speed combinations, tolerating both speed disturbances and ground height variability. The wheel can even capture enough energy to walk uphill. We also built a physical split-belt rimless wheel robot and demonstrated that it can walk continuously without additional energy input. In comparing the wheel solutions to human split-belt gait, we found that humans do not maximize positive work performed by the treadmill. Other aspects of walking, such as costs associated with swing, balance, and free vertical moments, likely limit people’s ability to benefit from the treadmill. This study uses a simple walking model to characterize the mechanics and energetics of split-belt walking, demonstrating that energy capture through intermittent contact with two belts is possible and providing a simple model framework for understanding human adaptation during split-belt walking.},
  archive  = {J},
  author   = {Julia K Butterfield and Surabhi N Simha and J Maxwell Donelan and Steven H Collins},
  doi      = {10.1177/02783649221110260},
  journal  = {The International Journal of Robotics Research},
  month    = {9-10},
  number   = {11-12},
  pages    = {1043-1076},
  title    = {The split-belt rimless wheel},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SVIn2: A multi-sensor fusion-based underwater SLAM system.
<em>The International Journal of Robotics Research</em>,
<em>41</em>(11-12), 1022–1042. (<a
href="https://doi.org/10.1177/02783649221110259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper presents SVIn2, a novel tightly-coupled keyframe-based Simultaneous Localization and Mapping (SLAM) system, which fuses Scanning Profiling Sonar, Visual, Inertial, and water-pressure information in a non-linear optimization framework for small and large scale challenging underwater environments. The developed real-time system features robust initialization, loop-closing, and relocalization capabilities, which make the system reliable in the presence of haze, blurriness, low light, and lighting variations, typically observed in underwater scenarios. Over the last decade, Visual-Inertial Odometry and SLAM systems have shown excellent performance for mobile robots in indoor and outdoor environments, but often fail underwater due to the inherent difficulties in such environments. Our approach combats the weaknesses of previous approaches by utilizing additional sensors and exploiting their complementary characteristics. In particular, we use (1) acoustic range information for improved reconstruction and localization, thanks to the reliable distance measurement; (2) depth information from water-pressure sensor for robust initialization, refining the scale, and assisting to limit the drift in the tightly-coupled integration. The developed software—made open source—has been successfully used to test and validate the proposed system in both benchmark datasets and numerous real world underwater scenarios, including datasets collected with a custom-made underwater sensor suite and an autonomous underwater vehicle Aqua2. SVIn2 demonstrated outstanding performance in terms of accuracy and robustness on those datasets and enabled other robotic tasks, for example, planning for underwater robots in presence of obstacles.},
  archive  = {J},
  author   = {Sharmin Rahman and Alberto Quattrini Li and Ioannis Rekleitis},
  doi      = {10.1177/02783649221110259},
  journal  = {The International Journal of Robotics Research},
  month    = {9-10},
  number   = {11-12},
  pages    = {1022-1042},
  title    = {SVIn2: A multi-sensor fusion-based underwater SLAM system},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid sparse monocular visual odometry with online
photometric calibration. <em>The International Journal of Robotics
Research</em>, <em>41</em>(11-12), 993–1021. (<a
href="https://doi.org/10.1177/02783649221107703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most monocular visual Simultaneous Localization and Mapping (vSLAM) and visual odometry (VO) algorithms focus on either feature-based methods or direct methods. Hybrid (semi-direct) approach is less studied although it is equally important. In this paper, a hybrid sparse visual odometry (HSO) algorithm with online photometric calibration is proposed for monocular vision. HSO introduces two novel measures, that is, direct image alignment with adaptive mode selection and image photometric description using ratio factors, to enhance the robustness against dramatic image intensity changes and motion blur. Moreover, HSO is able to establish pose constraints between keyframes far apart in time and space by using KLT tracking enhanced with a local-global brightness consistency. The convergence speed of candidate map points is adopted as the basis for keyframe selection, which strengthens the coordination between the front end and the back end. Photometric calibration is elegantly integrated into the VO system working in tandem: (1) Photometric interference from the camera, such as vignetting and changes in exposure time, is accurately calibrated and compensated in HSO, thereby improving the accuracy and robustness of VO. (2) On the other hand, VO provides pre-calculated data for the photometric calibration algorithm, which reduces resource consumption and improves the estimation accuracy of photometric parameters. Extensive experiments are performed on various public datasets to evaluate the proposed HSO against the state-of-the-art monocular vSLAM/VO and online photometric calibration methods. The results show that the proposed HSO achieves superior performance on VO and photometric calibration in terms of accuracy, robustness, and efficiency, being comparable with the state-of-the-art VO/vSLAM systems. We open source HSO for the benefit of the community.},
  archive  = {J},
  author   = {Dongting Luo and Yan Zhuang and Sen Wang},
  doi      = {10.1177/02783649221107703},
  journal  = {The International Journal of Robotics Research},
  month    = {9-10},
  number   = {11-12},
  pages    = {993-1021},
  title    = {Hybrid sparse monocular visual odometry with online photometric calibration},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wireless signal-based sensing framework for robotics.
<em>The International Journal of Robotics Research</em>,
<em>41</em>(11-12), 955–992. (<a
href="https://doi.org/10.1177/02783649221097989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we develop the analytical framework for a novel Wireless signal-based Sensing capability for Robotics (WSR) by leveraging a robots’ mobility in 3D space. It allows robots to primarily measure relative direction, or Angle-of-Arrival (AOA), to other robots, while operating in non-line-of-sight unmapped environments and without requiring external infrastructure. We do so by capturing all of the paths that a wireless signal traverses as it travels from a transmitting to a receiving robot in the team, which we term as an AOA profile . The key intuition behind our approach is to enable a robot to emulate antenna arrays as it moves freely in 2D and 3D space. The small differences in the phase of the wireless signals are thus processed with knowledge of robots’ local displacement to obtain the profile, via a method akin to Synthetic Aperture Radar (SAR). The main contribution of this work is the development of (i) a framework to accommodate arbitrary 2D and 3D motion, as well as continuous mobility of both signal transmitting and receiving robots, while computing AOA profiles between them and (ii) a Cramer–Rao Bound analysis, based on antenna array theory, that provides a lower bound on the variance in AOA estimation as a function of the geometry of robot motion. This is a critical distinction with previous work on SAR-based methods that restrict robot mobility to prescribed motion patterns, do not generalize to the full 3D space, and require transmitting robots to be stationary during data acquisition periods. We show that allowing robots to use their full mobility in 3D space while performing SAR results in more accurate AOA profiles and thus better AOA estimation. We formally characterize this observation as the informativeness of the robots’ motion, a computable quantity for which we derive a closed form. All analytical developments are substantiated by extensive simulation and hardware experiments on air/ground robot platforms using 5 GHz WiFi. Our experimental results bolster our analytical findings, demonstrating that 3D motion provides enhanced and consistent accuracy, with a total AOA error of less than 10 ◦ for 95% of trials. We also analytically characterize the impact of displacement estimation errors on the measured AOA and validate this theory empirically using robot displacements obtained using an off-the-shelf Intel Tracking Camera T265. Finally, we demonstrate the performance of our system on a multi-robot task where a heterogeneous air/ground pair of robots continuously measure AOA profiles over a WiFi link to achieve dynamic rendezvous in an unmapped, 300 m 2 environment with occlusions.},
  archive  = {J},
  author   = {Ninad Jadhav and Weiying Wang and Diana Zhang and Oussama Khatib and Swarun Kumar and Stephanie Gil},
  doi      = {10.1177/02783649221097989},
  journal  = {The International Journal of Robotics Research},
  month    = {9-10},
  number   = {11-12},
  pages    = {955-992},
  title    = {A wireless signal-based sensing framework for robotics},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Method for generating real-time interactive virtual fixture
for shared teleoperation in unknown environments. <em>The International
Journal of Robotics Research</em>, <em>41</em>(9-10), 925–951. (<a
href="https://doi.org/10.1177/02783649221102980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A virtual fixture (VF) is a constraint built into software that assists a human operator in moving a remote tool along a preferred path via an augmented guidance force to improve teleoperation performance. However, teleoperation generally applies to unknown or dynamic environments, which are challenging for VF use. Most researchers have assumed that VFs are pre-defined or generated automatically; however, these processes are complicated and unreliable in unknown environments where teleoperation is in high demand. Recently, a few researchers have addressed this issue by introducing a user-interactive method of generating VFs in unknown environments. However, these methods are limited to generating a single type of primitive for a single robot tool. Moreover, the accuracy of the VF generated by these methods depends on the accuracy of the human input. Thus, applications of these methods are limited. To overcome those limitations, this work introduces a novel interactive VF generation method that includes a new method of representing VFs as a composition of components. A feature-based user interface allows the human operator to intuitively specify the VF components. The new VF representation accommodates a variety of robot tools and actions. Using the feature-based interface, the process of VF generation is more intuitive and accurate. In this study, the proposed method is evaluated with human subjects in three teleoperation experiments: peg-in-hole, pipe-sawing, and pipe-welding. The experimental results show that the VFs generated by the proposed approach result in a higher manipulation quality while demonstrating the lowest total workload in all experiments. The peg-in-hole task teleoperation was the safest in terms of failure proportion and exerted force of the robot tool. In the pipe-sawing task, the positioning of the robot tool was the most accurate. In the pipe-welding task, the quality of weld was the best in terms of measured tool-trajectory smoothness and visual weld observation.},
  archive  = {J},
  author   = {Vitalii Pruks and Jee-Hwan Ryu},
  doi      = {10.1177/02783649221102980},
  journal  = {The International Journal of Robotics Research},
  month    = {8-9},
  number   = {9-10},
  pages    = {925-951},
  title    = {Method for generating real-time interactive virtual fixture for shared teleoperation in unknown environments},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Offline motion libraries and online MPC for advanced
mobility skills. <em>The International Journal of Robotics
Research</em>, <em>41</em>(9-10), 903–924. (<a
href="https://doi.org/10.1177/02783649221102473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We describe an optimization-based framework to perform complex locomotion skills for robots with legs and wheels. The generation of complex motions over a long-time horizon often requires offline computation due to current computing constraints and is mostly accomplished through trajectory optimization (TO). In contrast, model predictive control (MPC) focuses on the online computation of trajectories, robust even in the presence of uncertainty, albeit mostly over shorter time horizons and is prone to generating nonoptimal solutions over the horizon of the task’s goals. Our article’s contributions overcome this trade-off by combining offline motion libraries and online MPC, uniting a complex, long-time horizon plan with reactive, short-time horizon solutions. We start from offline trajectories that can be, for example, generated by TO or sampling-based methods. Also, multiple offline trajectories can be composed out of a motion library into a single maneuver. We then use these offline trajectories as the cost for the online MPC, allowing us to smoothly blend between multiple composed motions even in the presence of discontinuous transitions. The MPC optimizes from the measured state, resulting in feedback control, which robustifies the task’s execution by reacting to disturbances and looking ahead at the offline trajectory. With our contribution, motion designers can choose their favorite method to iterate over behavior designs offline without tuning robot experiments, enabling them to author new behaviors rapidly. Our experiments demonstrate complex and dynamic motions on our traditional quadrupedal robot ANYmal and its roller-walking version. Moreover, the article’s findings contribute to evaluating five planning algorithms.},
  archive  = {J},
  author   = {Marko Bjelonic and Ruben Grandia and Moritz Geilinger and Oliver Harley and Vivian S Medeiros and Vuk Pajovic and Edo Jelavic and Stelian Coros and Marco Hutter},
  doi      = {10.1177/02783649221102473},
  journal  = {The International Journal of Robotics Research},
  month    = {8-9},
  number   = {9-10},
  pages    = {903-924},
  title    = {Offline motion libraries and online MPC for advanced mobility skills},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On-manifold probabilistic iterative closest point:
Application to underwater karst exploration. <em>The International
Journal of Robotics Research</em>, <em>41</em>(9-10), 875–902. (<a
href="https://doi.org/10.1177/02783649221101418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes MpIC, an on-manifold derivation of the probabilistic Iterative Correspondence (pIC) algorithm, which is a stochastic version of the original Iterative Closest Point. It is developed in the context of autonomous underwater karst exploration based on acoustic sonars. First, a derivation of pIC based on the Lie group structure of S E ( 3 ) is developed. The closed-form expression of the covariance modeling the estimated rigid transformation is also provided. In a second part, its application to 3D scan matching between acoustic sonar measurements is proposed. It is a prolongation of previous work on elevation angle estimation from wide-beam acoustic sonar. While the pIC approach proposed is intended to be a key component in a Simultaneous Localization and Mapping framework, this paper focuses on assessing its viability on a unitary basis. As ground truth data in karst aquifer are difficult to obtain, quantitative experiments are carried out on a simulated karst environment and show improvement compared to previous state-of-the-art approach. The algorithm is also evaluated on a real underwater cave dataset demonstrating its practical applicability.},
  archive  = {J},
  author   = {Yohan Breux and André Mas and Lionel Lapierre},
  doi      = {10.1177/02783649221101418},
  journal  = {The International Journal of Robotics Research},
  month    = {8-9},
  number   = {9-10},
  pages    = {875-902},
  title    = {On-manifold probabilistic iterative closest point: Application to underwater karst exploration},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian iterative closest point for mobile robot
localization. <em>The International Journal of Robotics Research</em>,
<em>41</em>(9-10), 851–874. (<a
href="https://doi.org/10.1177/02783649221101417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate localization of a robot in a known environment is a fundamental capability for successfully performing path planning, manipulation, and grasping tasks. Particle filters, also known as Monte Carlo localization (MCL), are a commonly used method to determine the robot’s pose within its environment. For ground robots, noisy wheel odometry readings are typically used as a motion model to predict the vehicle’s location. Such a motion model requires tuning of various parameters based on terrain and robot type. However, such an ego-motion estimation is not always available for all platforms. Scan matching using the iterative closest point (ICP) algorithm is a popular alternative approach, providing ego-motion estimates for localization. Iterative closest point computes a point estimate of the transformation between two poses given point clouds captured at these locations. Being a point estimate method, ICP does not deal with the uncertainties in the scan alignment process, which may arise due to sensor noise, partial overlap, or the existence of multiple solutions. Another challenge for ICP is the high computational cost required to align two large point clouds, limiting its applicability to less dynamic problems. In this paper, we address these challenges by leveraging recent advances in probabilistic inference. Specifically, we first address the run-time issue and propose SGD-ICP, which employs stochastic gradient descent (SGD) to solve the optimization problem of ICP. Next, we leverage SGD-ICP to obtain a distribution over transformations and propose a Markov Chain Monte Carlo method using stochastic gradient Langevin dynamics (SGLD) updates. Our ICP variant, termed Bayesian-ICP, is a full Bayesian solution to the problem. To demonstrate the benefits of Bayesian-ICP for mobile robotic applications, we propose an adaptive motion model employing Bayesian-ICP to produce proposal distributions for Monte Carlo Localization. Experiments using both Kinect and 3D LiDAR data show that our proposed SGD-ICP method achieves the same solution quality as standard ICP while being significantly more efficient. We then demonstrate empirically that Bayesian-ICP can produce accurate distributions over pose transformations and is fast enough for online applications. Finally, using Bayesian-ICP as a motion model alleviates the need to tune the motion model parameters from odometry, resulting in better-calibrated localization uncertainty.},
  archive  = {J},
  author   = {Fahira Afzal Maken and Fabio Ramos and Lionel Ott},
  doi      = {10.1177/02783649221101417},
  journal  = {The International Journal of Robotics Research},
  month    = {8-9},
  number   = {9-10},
  pages    = {851-874},
  title    = {Bayesian iterative closest point for mobile robot localization},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive task and motion planning for robust whole-body
dynamic locomotion in constrained environments. <em>The International
Journal of Robotics Research</em>, <em>41</em>(8), 812–847. (<a
href="https://doi.org/10.1177/02783649221077714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Contact-based decision and planning methods are becoming increasingly important to endow higher levels of autonomy for legged robots. Formal synthesis methods derived from symbolic systems have great potential for reasoning about high-level locomotion decisions and achieving complex maneuvering behaviors with correctness guarantees. This study takes a first step toward formally devising an architecture composed of task planning and control of whole-body dynamic locomotion behaviors in constrained and dynamically changing environments. At the high level, we formulate a two-player temporal logic game between the multi-limb locomotion planner and its dynamic environment to synthesize a winning strategy that delivers symbolic locomotion actions. These locomotion actions satisfy the desired high-level task specifications expressed in a fragment of temporal logic. Those actions are sent to a robust finite transition system that synthesizes a locomotion controller that fulfills state reachability constraints. This controller is further executed via a low-level motion planner that generates feasible locomotion trajectories. We construct a set of dynamic locomotion models for legged robots to serve as a template library for handling diverse environmental events. We devise a replanning strategy that takes into consideration sudden environmental changes or large state disturbances to increase the robustness of the resulting locomotion behaviors. We formally prove the correctness of the layered locomotion framework guaranteeing a robust implementation by the motion planning layer. Simulations of reactive locomotion behaviors in diverse environments indicate that our framework has the potential to serve as a theoretical foundation for intelligent locomotion behaviors.},
  archive  = {J},
  author   = {Ye Zhao and Yinan Li and Luis Sentis and Ufuk Topcu and Jun Liu},
  doi      = {10.1177/02783649221077714},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {812-847},
  title    = {Reactive task and motion planning for robust whole-body dynamic locomotion in constrained environments},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time recognition of team behaviors by multisensory
graph-embedded robot learning. <em>The International Journal of Robotics
Research</em>, <em>41</em>(8), 798–811. (<a
href="https://doi.org/10.1177/02783649211043155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Awareness of team behaviors (e.g., individual activities and team intents) plays a critical role in human–robot teaming. Autonomous robots need to be aware of the overall intent of the team they are collaborating with in order to effectively aid their human peers or augment the team’s capabilities. Team intents encode the goal of the team, which cannot be simply identified from a collection of individual activities. Instead, teammate relationships must also be encoded for team intent recognition. In this article, we introduce a novel representation learning approach to recognizing team intent awareness in real-time, based upon both individual human activities and the relationship between human peers in the team. Our approach formulates the task of robot learning for team intent recognition as a joint regularized optimization problem, which encodes individual activities as latent variables and represents teammate relationships through graph embedding. In addition, we design a new algorithm to efficiently solve the formulated regularized optimization problem, which possesses a theoretical guarantee to converge to the optimal solution. To evaluate our approach’s performance on team intent recognition, we test our approach on a public benchmark group activity dataset and a multisensory underground search and rescue team behavior dataset newly collected from robots in an underground environment, as well as perform a proof-of-concept case study on a physical robot. The experimental results have demonstrated both the superior accuracy of our proposed approach and its suitability for real-time applications on mobile robots.},
  archive  = {J},
  author   = {Brian Reily and Peng Gao and Fei Han and Hua Wang and Hao Zhang},
  doi      = {10.1177/02783649211043155},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {798-811},
  title    = {Real-time recognition of team behaviors by multisensory graph-embedded robot learning},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining learned and analytical models for predicting
action effects from sensory data. <em>The International Journal of
Robotics Research</em>, <em>41</em>(8), 778–797. (<a
href="https://doi.org/10.1177/0278364920954896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the most basic skills a robot should possess is predicting the effect of physical interactions with objects in the environment. This enables optimal action selection to reach a certain goal state. Traditionally, dynamics are approximated by physics-based analytical models. These models rely on specific state representations that may be hard to obtain from raw sensory data, especially if no knowledge of the object shape is assumed. More recently, we have seen learning approaches that can predict the effect of complex physical interactions directly from sensory input. It is, however, an open question how far these models generalize beyond their training data. In this work, we investigate the advantages and limitations of neural-network-based learning approaches for predicting the effects of actions based on sensory input and show how analytical and learned models can be combined to leverage the best of both worlds. As physical interaction task, we use planar pushing, for which there exists a well-known analytical model and a large real-world dataset. We propose the use of a convolutional neural network to convert raw depth images or organized point clouds into a suitable representation for the analytical model and compare this approach with using neural networks for both, perception and prediction. A systematic evaluation of the proposed approach on a very large real-world dataset shows two main advantages of the hybrid architecture. Compared with a pure neural network, it significantly (i) reduces required training data and (ii) improves generalization to novel physical interaction.},
  archive  = {J},
  author   = {Alina Kloss and Stefan Schaal and Jeannette Bohg},
  doi      = {10.1177/0278364920954896},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {778-797},
  title    = {Combining learned and analytical models for predicting action effects from sensory data},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coupled recursive estimation for online interactive
perception of articulated objects. <em>The International Journal of
Robotics Research</em>, <em>41</em>(8), 741–777. (<a
href="https://doi.org/10.1177/0278364919848850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present online multi-modal perception systems for extracting kinematic and dynamic models of articulated objects from physical interactions with the environment. The systems rely on a RGB-D stream, contact wrenches, and proprioception. The proposed systems share an algorithmic foundation: they are based on an architecture of coupled recursive estimation processes. We present and advocate this architecture as a general, versatile, and robust solution for online interactive perception problems. We validate the architecture in extensive experiments to extract kinematic models interactively, varying the appearance, size, structure, and dynamic properties of objects for different tasks and under different environmental conditions. In addition, we experimentally show that the information acquired by the online perception systems enables robot manipulation of articulated objects. Furthermore, we discuss the relationship between the proposed architecture for robot perception and insights about biological perception systems.},
  archive  = {J},
  author   = {Roberto Martín-Martín and Oliver Brock},
  doi      = {10.1177/0278364919848850},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {741-777},
  title    = {Coupled recursive estimation for online interactive perception of articulated objects},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision and wi-fi fusion in probabilistic appearance-based
localization. <em>The International Journal of Robotics Research</em>,
<em>41</em>(7), 721–738. (<a
href="https://doi.org/10.1177/0278364920910485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article introduces an indoor topological localization algorithm that uses vision and Wi-Fi signals. Its main contribution is a novel way of merging data from these sensors. The designed system does not require knowledge of the building plan or the positions of the Wi-Fi access points. By making the Wi-Fi signature suited to the FABMAP algorithm, this work develops an early fusion framework that solves global localization and kidnapped robot problems. The resulting algorithm has been tested and compared with FABMAP visual localization, over data acquired by a Pepper robot in three different environments: an office building, a middle school, and a private apartment. Numerous runs of different robots have been realized over several months for a total covered distance of 6.4 km. Constraints were applied during acquisitions to make the experiments fitted to real use cases of Pepper robots. Without any tuning, our early fusion framework outperforms visual localization in all testing situations and with a significant margin in environments where vision faces problems such as moving objects or perceptual aliasing. In such conditions, 90.6% of estimated localizations are less than 5 m away from ground truth with our early fusion framework compared with 77.6% with visual localization. Furthermore, compared with other classical fusion strategies, the early fusion framework produces the best localization results because in all tested situations, it improves visual localization results without damaging them where Wi-Fi signals carry little information.},
  archive  = {J},
  author   = {Mathieu Nowakowski and Cyril Joly and Sébastien Dalibard and Nicolas Garcia and Fabien Moutarde},
  doi      = {10.1177/0278364920910485},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {721-738},
  title    = {Vision and wi-fi fusion in probabilistic appearance-based localization},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot perceptual adaptation to environment changes for
long-term human teammate following. <em>The International Journal of
Robotics Research</em>, <em>41</em>(7), 706–720. (<a
href="https://doi.org/10.1177/0278364919896625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Perception is one of the several fundamental abilities required by robots, and it also poses significant challenges, especially in real-world field applications. Long-term autonomy introduces additional difficulties to robot perception, including short- and long-term changes of the robot operation environment (e.g., lighting changes). In this article, we propose an innovative human-inspired approach named robot perceptual adaptation (ROPA) that is able to calibrate perception according to the environment context, which enables perceptual adaptation in response to environmental variations. ROPA jointly performs feature learning, sensor fusion, and perception calibration under a unified regularized optimization framework. We also implement a new algorithm to solve the formulated optimization problem, which has a theoretical guarantee to converge to the optimal solution. In addition, we collect a large-scale dataset from physical robots in the field, called perceptual adaptation to environment changes (PEAC), with the aim to benchmark methods for robot adaptation to short-term and long-term, and fast and gradual lighting changes for human detection based upon different feature modalities extracted from color and depth sensors. Utilizing the PEAC dataset, we conduct extensive experiments in the application of human recognition and following in various scenarios to evaluate ROPA. Experimental results have validated that the ROPA approach obtains promising performance in terms of accuracy and efficiency, and effectively adapts robot perception to address short-term and long-term lighting changes in human detection and following applications.},
  archive  = {J},
  author   = {Sriram Siva and Hao Zhang},
  doi      = {10.1177/0278364919896625},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {706-720},
  title    = {Robot perceptual adaptation to environment changes for long-term human teammate following},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic pick-and-place of novel objects in clutter with
multi-affordance grasping and cross-domain image matching. <em>The
International Journal of Robotics Research</em>, <em>41</em>(7),
690–705. (<a href="https://doi.org/10.1177/0278364919868017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses an object-agnostic grasping framework to map from visual observations to actions: inferring dense pixel-wise probability maps of the affordances for four different grasping primitive actions. It then executes the action with the highest affordance and recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional data collection or re-training. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT–Princeton Team system that took first place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu/},
  archive  = {J},
  author   = {Andy Zeng and Shuran Song and Kuan-Ting Yu and Elliott Donlon and Francois R. Hogan and Maria Bauza and Daolin Ma and Orion Taylor and Melody Liu and Eudald Romo and Nima Fazeli and Ferran Alet and Nikhil Chavan Dafle and Rachel Holladay and Isabella Morona and Prem Qu Nair and Druck Green and Ian Taylor and Weber Liu and Thomas Funkhouser and Alberto Rodriguez},
  doi      = {10.1177/0278364919868017},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {690-705},
  title    = {Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robocentric visual–inertial odometry. <em>The International
Journal of Robotics Research</em>, <em>41</em>(7), 667–689. (<a
href="https://doi.org/10.1177/0278364919853361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a novel robocentric formulation of the visual–inertial navigation system (VINS) within a sliding-window filtering framework and design an efficient, lightweight, robocentric visual–inertial odometry (R-VIO) algorithm for consistent motion tracking even in challenging environments using only a monocular camera and a six-axis inertial measurement unit (IMU). The key idea is to deliberately reformulate the VINS with respect to a moving local frame, rather than a fixed global frame of reference as in the standard world-centric VINS, in order to obtain relative motion estimates of higher accuracy for updating global pose. As an immediate advantage of this robocentric formulation, the proposed R-VIO can start from an arbitrary pose, without the need to align the initial orientation with the global gravitational direction. More importantly, we analytically show that the linearized robocentric VINS does not undergo the observability mismatch issue as in the standard world-centric counterparts that has been identified in the literature as the main cause of estimation inconsistency. Furthermore, we investigate in depth the special motions that degrade the performance in the world-centric formulation and show that such degenerate cases can be easily compensated for by the proposed robocentric formulation, without resorting to additional sensors as in the world-centric formulation, thus leading to better robustness. The proposed R-VIO algorithm has been extensively validated through both Monte Carlo simulation and real-world experiments with different sensing platforms navigating in different environments, and shown to achieve better (or competitive at least) performance than the state-of-the-art VINS, in terms of consistency, accuracy, and efficiency.},
  archive  = {J},
  author   = {Zheng Huai and Guoquan Huang},
  doi      = {10.1177/0278364919853361},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {667-689},
  title    = {Robocentric visual–inertial odometry},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale outdoor scene reconstruction and correction with
vision. <em>The International Journal of Robotics Research</em>,
<em>41</em>(6), 637–663. (<a
href="https://doi.org/10.1177/0278364920937052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We provide the theory and the system needed to create large-scale dense reconstructions for mobile-robotics applications: this stands in contrast to the object-centric reconstructions dominant in the literature. Our BOR 2 G system fuses data from multiple sensor modalities (cameras, lidars, or both) and regularizes the resulting 3D model. We use a compressed 3D data structure, which allows us to operate over a large scale. In addition, because of the paucity of surface observations by the camera and lidar sensors, we regularize over both two (camera depth maps) and three dimensions (voxel grid) to provide a local contextual prior for the reconstruction. Our regularizer reduces the median error between 27% and 36% in 7.3 km of dense reconstructions with a median accuracy between 4 and 8 cm. Our pipeline does not end with regularization. We take the unusual step to apply a learned correction mechanism that takes the global context of the reconstruction and adjusts the constructed mesh, addressing errors that are pathological to the first-pass camera-derived reconstruction. We evaluate our system using the Stanford Burghers of Calais, Imperial College ICL-NUIM, Oxford Broad Street (released with this paper), and the KITTI datasets. These latter datasets see us operating at a combined scale and accuracy not seen in the literature. We provide statistics for the metric errors in all surfaces created compared with those measured with 3D lidar as ground truth. We demonstrate our system in practice by reconstructing the inside of the EUROfusion Joint European Torus (JET) fusion reactor, located at the Culham Centre for Fusion Energy (UK Atomic Energy Authority) in Oxfordshire.},
  archive  = {J},
  author   = {Michael Tanner and Pedro Piniés and Lina María Paz and Ştefan Săftescu and Alex Bewley and Emil Jonasson and Paul Newman},
  doi      = {10.1177/0278364920937052},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {637-663},
  title    = {Large-scale outdoor scene reconstruction and correction with vision},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physics-based scene-level reasoning for object pose
estimation in clutter. <em>The International Journal of Robotics
Research</em>, <em>41</em>(6), 615–636. (<a
href="https://doi.org/10.1177/0278364919846551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper focuses on vision-based pose estimation for multiple rigid objects placed in clutter, especially in cases involving occlusions and objects resting on each other. Progress has been achieved recently in object recognition given advancements in deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort to label objects. This limits their applicability in robotics, where solutions must scale to a large number of objects and variety of conditions. Moreover, the combinatorial nature of the scenes that could arise from the placement of multiple objects is difficult to capture in the training dataset. Thus, the learned models might not produce the desired level of precision required for tasks, such as robotic manipulation. This work proposes an autonomous process for pose estimation that spans from data generation to scene-level reasoning and self-learning. In particular, the proposed framework first generates a labeled dataset for training a convolutional neural network (CNN) for object detection in clutter. These detections are used to guide a scene-level optimization process, which considers the interactions between the different objects present in the clutter to output pose estimates of high precision. Furthermore, confident estimates are used to label online real images from multiple views and re-train the process in a self-learning pipeline. Experimental results indicate that this process is quickly able to identify in cluttered scenes physically consistent object poses that are more precise than those found by reasoning over individual instances of objects. Furthermore, the quality of pose estimates increases over time given the self-learning process.},
  archive  = {J},
  author   = {Chaitanya Mitash and Abdeslam Boularias and Kostas Bekris},
  doi      = {10.1177/0278364919846551},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {615-636},
  title    = {Physics-based scene-level reasoning for object pose estimation in clutter},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Track deformable objects from point clouds with structure
preserved registration. <em>The International Journal of Robotics
Research</em>, <em>41</em>(6), 599–614. (<a
href="https://doi.org/10.1177/0278364919841431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Manipulating deformable objects is a challenging task for robots. The major difficulty lies in how to track these objects accurately, robustly, and efficiently, considering they have infinite-dimensional configuration space. To deal with these problems, this paper proposes a novel state estimator to track deformable objects from point clouds. A non-rigid registration method, named structure preserved registration (SPR), is developed to update the estimation by registering the object model towards the current point cloud measurement. Both the local structure and the global topology of the deformable object are considered during registration, which improves the estimation robustness under noise, outliers, and occlusions. The tracking result is further refined by running a dynamic simulation in parallel, which enforces the estimates to satisfy the physical constraints of the object. A series of real-time tracking experiments on 1D objects (ropes) and 2D objects (clothes) are performed to evaluate the proposed state estimator. A wire harness manipulation platform is also introduced where robots can manipulate soft wires to desired shapes and autonomously evaluate the manipulation quality through visual feedback.},
  archive  = {J},
  author   = {Te Tang and Masayoshi Tomizuka},
  doi      = {10.1177/0278364919841431},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {599-614},
  title    = {Track deformable objects from point clouds with structure preserved registration},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic–geometric visual place recognition: A new
perspective for reconciling opposing views. <em>The International
Journal of Robotics Research</em>, <em>41</em>(6), 573–598. (<a
href="https://doi.org/10.1177/0278364919839761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Human drivers are capable of recognizing places from a previous journey even when viewing them from the opposite direction during the return trip under radically different environmental conditions, without needing to look back or employ a 360 ° camera or LIDAR sensor. Such navigation capabilities are attributed in large part to the robust semantic scene understanding capabilities of humans. However, for an autonomous robot or vehicle, achieving such human-like visual place recognition capability presents three major challenges: (1) dealing with a limited amount of commonly observable visual content when viewing the same place from the opposite direction; (2) dealing with significant lateral viewpoint changes caused by opposing directions of travel taking place on opposite sides of the road; and (3) dealing with a radically changed scene appearance due to environmental conditions such as time of day, season, and weather. Current state-of-the-art place recognition systems have only addressed these three challenges in isolation or in pairs, typically relying on appearance-based, deep-learnt place representations. In this paper, we present a novel, semantics-based system that for the first time solves all three challenges simultaneously. We propose a hybrid image descriptor that semantically aggregates salient visual information, complemented by appearance-based description, and augment a conventional coarse-to-fine recognition pipeline with keypoint correspondences extracted from within the convolutional feature maps of a pre-trained network. Finally, we introduce descriptor normalization and local score enhancement strategies for improving the robustness of the system. Using both existing benchmark datasets and extensive new datasets that for the first time combine the three challenges of opposing viewpoints, lateral viewpoint shifts, and extreme appearance change, we show that our system can achieve practical place recognition performance where existing state-of-the-art methods fail.},
  archive  = {J},
  author   = {Sourav Garg and Niko Suenderhauf and Michael Milford},
  doi      = {10.1177/0278364919839761},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {573-598},
  title    = {Semantic–geometric visual place recognition: A new perspective for reconciling opposing views},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential contact-based adaptive grasping for robotic
hands. <em>The International Journal of Robotics Research</em>,
<em>41</em>(5), 543–570. (<a
href="https://doi.org/10.1177/02783649221081154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes a novel type of grasping strategy that draws inspiration from the role of touch and the importance of wrist motions in human grasping. The proposed algorithm, which we call Sequential Contact-based Adaptive Grasping , can be used to reactively modify a given grasp plan according to contacts arising between the hand and the object. This technique, based on a systematic constraint categorization and an iterative task inversion procedure, is shown to lead to synchronized motions of the fingers and the wrist, as it can be observed in humans, and to increase grasp success rate by substantially mitigating the relevant problems of object slippage during hand closure and of uncertainties caused by the environment and by the perception system. After describing the grasping problem in its quasi-static aspects, the algorithm is derived and discussed with some simple simulations. The proposed method is general as it can be applied to different kinds of robotic hands. It refines a priori defined grasp plans and significantly reduces their accuracy requirements by relying only on a forward kinematic model and elementary contact information. The efficacy of our approach is confirmed by experimental results of tests performed on a collaborative robot manipulator equipped with a state-of-the-art underactuated soft hand.},
  archive  = {J},
  author   = {George Jose Pollayil and Mathew Jose Pollayil and Manuel Giuseppe Catalano and Antonio Bicchi and Giorgio Grioli},
  doi      = {10.1177/02783649221081154},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {543-570},
  title    = {Sequential contact-based adaptive grasping for robotic hands},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RadarSLAM: A robust simultaneous localization and mapping
system for all weather conditions. <em>The International Journal of
Robotics Research</em>, <em>41</em>(5), 519–542. (<a
href="https://doi.org/10.1177/02783649221080483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A Simultaneous Localization and Mapping (SLAM) system must be robust to support long-term mobile vehicle and robot applications. However, camera and LiDAR based SLAM systems can be fragile when facing challenging illumination or weather conditions which degrade the utility of imagery and point cloud data. Radar, whose operating electromagnetic spectrum is less affected by environmental changes, is promising although its distinct sensor model and noise characteristics bring open challenges when being exploited for SLAM. This paper studies the use of a Frequency Modulated Continuous Wave radar for SLAM in large-scale outdoor environments. We propose a full radar SLAM system, including a novel radar motion estimation algorithm that leverages radar geometry for reliable feature tracking. It also optimally compensates motion distortion and estimates pose by joint optimization. Its loop closure component is designed to be simple yet efficient for radar imagery by capturing and exploiting structural information of the surrounding environment. Extensive experiments on three public radar datasets, ranging from city streets and residential areas to countryside and highways, show competitive accuracy and reliability performance of the proposed radar SLAM system compared to the state-of-the-art LiDAR, vision and radar methods. The results show that our system is technically viable in achieving reliable SLAM in extreme weather conditions on the RADIATE Dataset, for example, heavy snow and dense fog, demonstrating the promising potential of using radar for all-weather localization and mapping.},
  archive  = {J},
  author   = {Ziyang Hong and Yvan Petillot and Andrew Wallace and Sen Wang},
  doi      = {10.1177/02783649221080483},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {519-542},
  title    = {RadarSLAM: A robust simultaneous localization and mapping system for all weather conditions},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inducing structure in reward learning by learning features.
<em>The International Journal of Robotics Research</em>, <em>41</em>(5),
497–518. (<a href="https://doi.org/10.1177/02783649221078031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Reward learning enables robots to learn adaptable behaviors from human input. Traditional methods model the reward as a linear function of hand-crafted features, but that requires specifying all the relevant features a priori, which is impossible for real-world tasks. To get around this issue, recent deep Inverse Reinforcement Learning (IRL) methods learn rewards directly from the raw state but this is challenging because the robot has to implicitly learn the features that are important and how to combine them, simultaneously. Instead, we propose a divide-and-conquer approach: focus human input specifically on learning the features separately, and only then learn how to combine them into a reward. We introduce a novel type of human input for teaching features and an algorithm that utilizes it to learn complex features from the raw state space. The robot can then learn how to combine them into a reward using demonstrations, corrections, or other reward learning frameworks. We demonstrate our method in settings where all features have to be learned from scratch, as well as where some of the features are known. By first focusing human input specifically on the feature(s), our method decreases sample complexity and improves generalization of the learned reward over a deep IRL baseline. We show this in experiments with a physical 7-DoF robot manipulator, and in a user study conducted in a simulated environment.},
  archive  = {J},
  author   = {Andreea Bobu and Marius Wiggert and Claire Tomlin and Anca D Dragan},
  doi      = {10.1177/02783649221078031},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {497-518},
  title    = {Inducing structure in reward learning by learning features},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simplified decision making in the belief space using belief
sparsification. <em>The International Journal of Robotics Research</em>,
<em>41</em>(5), 470–496. (<a
href="https://doi.org/10.1177/02783649221076381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we introduce a new and efficient solution approach for the problem of decision making under uncertainty, which can be formulated as decision making in a belief space, over a possibly high-dimensional state space. Typically, to solve a decision problem, one should identify the optimal action from a set of candidates, according to some objective. We claim that one can often generate and solve an analogous yet simplified decision problem, which can be solved more efficiently. A wise simplification method can lead to the same action selection, or one for which the maximal loss in optimality can be guaranteed. Furthermore, such simplification is separated from the state inference and does not compromise its accuracy, as the selected action would finally be applied on the original state. First, we present the concept for general decision problems and provide a theoretical framework for a coherent formulation of the approach. We then practically apply these ideas to decision problems in the belief space, which can be simplified by considering a sparse approximation of their initial belief. The scalable belief sparsification algorithm we provide is able to yield solutions which are guaranteed to be consistent with the original problem. We demonstrate the benefits of the approach in the solution of a realistic active-SLAM problem and manage to significantly reduce computation time, with no loss in the quality of solution. This work is both fundamental and practical and holds numerous possible extensions.},
  archive  = {J},
  author   = {Khen Elimelech and Vadim Indelman},
  doi      = {10.1177/02783649221076381},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {470-496},
  title    = {Simplified decision making in the belief space using belief sparsification},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AURORA, a multi-sensor dataset for robotic ocean
exploration. <em>The International Journal of Robotics Research</em>,
<em>41</em>(5), 461–469. (<a
href="https://doi.org/10.1177_02783649221078612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The current maturity of autonomous underwater vehicles (AUVs) has made their deployment practical and cost-effective, such that many scientific, industrial and military applications now include AUV operations. However, the logistical difficulties and high costs of operating at sea are still critical limiting factors in further technology development, the benchmarking of new techniques and the reproducibility of research results. To overcome this problem, this paper presents a freely available dataset suitable to test control, navigation, sensor processing algorithms and others tasks. This dataset combines AUV navigation data, sidescan sonar, multibeam echosounder data and seafloor camera image data, and associated sensor acquisition metadata to provide a detailed characterisation of surveys carried out by the National Oceanography Centre (NOC) in the Greater Haig Fras Marine Conservation Zone (MCZ) of the U.K in 2015.},
  archive  = {J},
  author   = {Marco Bernardi and Brett Hosking and Chiara Petrioli and Brian J Bett and Daniel Jones and Veerle AI Huvenne and Rachel Marlow and Maaten Furlong and Steve McPhail and Andrea Munafò},
  doi      = {10.1177_02783649221078612},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {461-469},
  title    = {AURORA, a multi-sensor dataset for robotic ocean exploration},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An efficient, modular controller for flapping flight
composing model-based and model-free components. <em>The International
Journal of Robotics Research</em>, <em>41</em>(4), 441–457. (<a
href="https://doi.org/10.1177/02783649211063225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a controller that combines model-based methods with model-free data-driven methods hierarchically, utilizing the predictive power of template models with the strengths of model-free methods to account for model error, such as due to manufacturing variability in the RoboBee, a 100 mg flapping-wing micro aerial vehicle (FWMAV). Using a large suite of numerical trials, we show that the model-predictive high-level component of the proposed controller is more performant, easier to tune, and able to stabilize more dynamic tasks than a baseline reactive controller, while the data-driven inverse dynamics controller is able to better compensate for biases arising from manufacturing variability. At the same time, the formulated controller is very computationally efficient, with the MPC implemented at 5 KHz on a Simulink embedded target, via which we empirically demonstrate controlled hovering on a RoboBee.},
  archive  = {J},
  author   = {Avik De and Rebecca McGill and Robert J Wood},
  doi      = {10.1177/02783649211063225},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4},
  pages    = {441-457},
  title    = {An efficient, modular controller for flapping flight composing model-based and model-free components},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Magnetic concentric tube robots: Introduction and analysis.
<em>The International Journal of Robotics Research</em>, <em>41</em>(4),
418–440. (<a href="https://doi.org/10.1177/02783649211071113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose a new type of continuum robot, referred to as a magnetic concentric tube robot (M-CTR), for performing minimally invasive surgery in narrow and difficult-to-access areas. The robot combines concentric tubes and magnetic actuation to benefit from the ‘follow the leader’ behaviour, the dexterity and stability of existing robots, while targeting millimetre-sized external diameters. These three kinematic properties are assessed through numerical and experimental studies performed on a prototype of a M-CTR. They are performed with general forward and inverse kineto-static models of the robot, continuation and bifurcation analysis, and a specific experimental setup. The prototype presents unique capabilities in terms of deployment and active stability management, while its dexterity in terms of tip orientability is also among the best reported for other robots at its scale.},
  archive  = {J},
  author   = {Quentin Peyron and Quentin Boehler and Patrick Rougeot and Pierre Roux and Bradley J Nelson and Nicolas Andreff and Kanty Rabenorosoa and Pierre Renaud},
  doi      = {10.1177/02783649211071113},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4},
  pages    = {418-440},
  title    = {Magnetic concentric tube robots: Introduction and analysis},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptively informed trees (AIT*) and effort informed trees
(EIT*): Asymmetric bidirectional sampling-based path planning. <em>The
International Journal of Robotics Research</em>, <em>41</em>(4),
390–417. (<a href="https://doi.org/10.1177/02783649211069572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Optimal path planning is the problem of finding a valid sequence of states between a start and goal that optimizes an objective. Informed path planning algorithms order their search with problem-specific knowledge expressed as heuristics and can be orders of magnitude more efficient than uninformed algorithms. Heuristics are most effective when they are both accurate and computationally inexpensive to evaluate, but these are often conflicting characteristics. This makes the selection of appropriate heuristics difficult for many problems. This paper presents two almost-surely asymptotically optimal sampling-based path planning algorithms to address this challenge, Adaptively Informed Trees (AIT*) and Effort Informed Trees (EIT*). These algorithms use an asymmetric bidirectional search in which both searches continuously inform each other. This allows AIT* and EIT* to improve planning performance by simultaneously calculating and exploiting increasingly accurate, problem-specific heuristics. The benefits of AIT* and EIT* relative to other sampling-based algorithms are demonstrated on 12 problems in abstract, robotic, and biomedical domains optimizing path length and obstacle clearance. The experiments show that AIT* and EIT* outperform other algorithms on problems optimizing obstacle clearance, where a priori cost heuristics are often ineffective, and still perform well on problems minimizing path length, where such heuristics are often effective.},
  archive  = {J},
  author   = {Marlin P Strub and Jonathan D Gammell},
  doi      = {10.1177/02783649211069572},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4},
  pages    = {390-417},
  title    = {Adaptively informed trees (AIT*) and effort informed trees (EIT*): Asymmetric bidirectional sampling-based path planning},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GKNet: Grasp keypoint network for grasp candidates
detection. <em>The International Journal of Robotics Research</em>,
<em>41</em>(4), 361–389. (<a
href="https://doi.org/10.1177/02783649211069569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Contemporary grasp detection approaches employ deep learning to achieve robustness to sensor and object model uncertainty. The two dominant approaches design either grasp-quality scoring or anchor-based grasp recognition networks. This paper presents a different approach to grasp detection by treating it as keypoint detection in image-space. The deep network detects each grasp candidate as a pair of keypoints, convertible to the grasp representation g = { x , y , w , θ } T , rather than a triplet or quartet of corner points. Decreasing the detection difficulty by grouping keypoints into pairs boosts performance. To promote capturing dependencies between keypoints, a non-local module is incorporated into the network design. A final filtering strategy based on discrete and continuous orientation prediction removes false correspondences and further improves grasp detection performance. GKNet, the approach presented here, achieves a good balance between accuracy and speed on the Cornell and the abridged Jacquard datasets (96.9% and 98.39% at 41.67 and 23.26 fps). Follow-up experiments on a manipulator evaluate GKNet using four types of grasping experiments reflecting different nuisance sources: static grasping, dynamic grasping, grasping at varied camera angles, and bin picking. GKNet outperforms reference baselines in static and dynamic grasping experiments while showing robustness to varied camera viewpoints and moderate clutter. The results confirm the hypothesis that grasp keypoints are an effective output representation for deep grasp networks that provide robustness to expected nuisance factors.},
  archive  = {J},
  author   = {Ruinian Xu and Fu-Jen Chu and Patricio A Vela},
  doi      = {10.1177/02783649211069569},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4},
  pages    = {361-389},
  title    = {GKNet: Grasp keypoint network for grasp candidates detection},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ColoRadar: The direct 3D millimeter wave radar dataset.
<em>The International Journal of Robotics Research</em>, <em>41</em>(4),
351–360. (<a href="https://doi.org/10.1177/02783649211068535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work presents two different forms of dense, high-resolution radar data from two frequency modulated continuous wave radar sensors, along sparse radar pointclouds produced by one of the radar sensors. In addition, all datasets include 3D lidar and inertial measurements, and a lidar-based simultaneous localization and mapping pose estimation. Over 2 h of 6D pose data was generated across 52 datasets collected in highly diverse 3D environments including lab spaces, outside and inside large buildings, urban walkways, and a mine. One dataset, from the ASPEN Lab, also includes precision groundtruth generated from a motion capture system. Intrinsic radar calibration and measured extrinsic sensor position calibrations are also provided along with python based development tools to interact with the various datasets. This data is designed to assist with generating radar based localization algorithms and calibrations between radar and other sensors.},
  archive  = {J},
  author   = {Andrew Kramer and Kyle Harlow and Christopher Williams and Christoffer Heckman},
  doi      = {10.1177/02783649211068535},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {4},
  pages    = {351-360},
  title    = {ColoRadar: The direct 3D millimeter wave radar dataset},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locally active globally stable dynamical systems: Theory,
learning, and experiments. <em>The International Journal of Robotics
Research</em>, <em>41</em>(3), 312–347. (<a
href="https://doi.org/10.1177/02783649211030952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {State-dependent dynamical systems (DSs) offer adaptivity, reactivity, and robustness to perturbations in motion planning and physical human–robot interaction tasks. Learning DS-based motion plans from non-linear reference trajectories is an active research area in robotics. Most approaches focus on learning DSs that can (i) accurately mimic the demonstrated motion, while (ii) ensuring convergence to the target, i.e., they are globally asymptotically (or exponentially) stable. When subject to perturbations, a compliant robot guided with a DS will continue following the next integral curves of the DS towards the target. If the task requires the robot to track a specific reference trajectory, this approach will fail. To alleviate this shortcoming, we propose the locally active globally stable DS (LAGS-DS), a novel DS formulation that provides both global convergence and stiffness-like symmetric attraction behaviors around a reference trajectory in regions of the state space where trajectory tracking is important. This allows for a unified approach towards motion and impedance encoding in a single DS-based motion model, i.e., stiffness is embedded in the DS. To learn LAGS-DS from demonstrations we propose a learning strategy based on Bayesian non-parametric Gaussian mixture models, Gaussian processes, and a sequence of constrained optimization problems that ensure estimation of stable DS parameters via Lyapunov theory. We experimentally validated LAGS-DS on writing tasks with a KUKA LWR 4+ arm and on navigation and co-manipulation tasks with iCub humanoid robots.},
  archive  = {J},
  author   = {Nadia Figueroa and Aude Billard},
  doi      = {10.1177/02783649211030952},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {312-347},
  title    = {Locally active globally stable dynamical systems: Theory, learning, and experiments},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion planning by learning the solution manifold in
trajectory optimization. <em>The International Journal of Robotics
Research</em>, <em>41</em>(3), 281–311. (<a
href="https://doi.org/10.1177/02783649211044405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The objective function used in trajectory optimization is often non-convex and can have an infinite set of local optima. In such cases, there are diverse solutions to perform a given task. Although there are a few methods to find multiple solutions for motion planning, they are limited to generating a finite set of solutions. To address this issue, we present an optimization method that learns an infinite set of solutions in trajectory optimization. In our framework, diverse solutions are obtained by learning latent representations of solutions. Our approach can be interpreted as training a deep generative model of collision-free trajectories for motion planning. The experimental results indicate that the trained model represents an infinite set of homotopic solutions for motion planning problems.},
  archive  = {J},
  author   = {Takayuki Osa},
  doi      = {10.1177/02783649211044405},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {281-311},
  title    = {Motion planning by learning the solution manifold in trajectory optimization},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NTU VIRAL: A visual-inertial-ranging-lidar dataset, from an
aerial vehicle viewpoint. <em>The International Journal of Robotics
Research</em>, <em>41</em>(3), 270–280. (<a
href="https://doi.org/10.1177/02783649211052312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, autonomous robots have become ubiquitous in research and daily life. Among many factors, public datasets play an important role in the progress of this field, as they waive the tall order of initial investment in hardware and manpower. However, for research on autonomous aerial systems, there appears to be a relative lack of public datasets on par with those used for autonomous driving and ground robots. Thus, to fill in this gap, we conduct a data collection exercise on an aerial platform equipped with an extensive and unique set of sensors: two 3D lidars, two hardware-synchronized global-shutter cameras, multiple Inertial Measurement Units (IMUs), and especially, multiple Ultra-wideband (UWB) ranging units. The comprehensive sensor suite resembles that of an autonomous driving car, but features distinct and challenging characteristics of aerial operations. We record multiple datasets in several challenging indoor and outdoor conditions. Calibration results and ground truth from a high-accuracy laser tracker are also included in each package. All resources can be accessed via our webpage https://ntu-aris.github.io/ntu_viral_dataset/ .},
  archive  = {J},
  author   = {Thien-Minh Nguyen and Shenghai Yuan and Muqing Cao and Yang Lyu and Thien H Nguyen and Lihua Xie},
  doi      = {10.1177/02783649211052312},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {270-280},
  title    = {NTU VIRAL: A visual-inertial-ranging-lidar dataset, from an aerial vehicle viewpoint},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BenchBot environments for active robotics (BEAR): Simulated
data for active scene understanding research. <em>The International
Journal of Robotics Research</em>, <em>41</em>(3), 259–269. (<a
href="https://doi.org/10.1177/02783649211069404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a platform to foster research in active scene understanding, consisting of high-fidelity simulated environments and a simple yet powerful API that controls a mobile robot in simulation and reality. In contrast to static, pre-recorded datasets that focus on the perception aspect of scene understanding, agency is a top priority in our work. We provide three levels of robot agency, allowing users to control a robot at varying levels of difficulty and realism. While the most basic level provides pre-defined trajectories and ground-truth localisation, the more realistic levels allow us to evaluate integrated behaviours comprising perception, navigation, exploration and SLAM. In contrast to existing simulation environments, we focus on robust scene understanding research using our environment interface (BenchBot) that provides a simple API for seamless transition between the simulated environments and real robotic platforms. We believe this scaffolded design is an effective approach to bridge the gap between classical static datasets without any agency and the unique challenges of robotic evaluation in reality. Our BenchBot Environments for Active Robotics (BEAR) consist of 25 indoor environments under day and night lighting conditions, a total of 1443 objects to be identified and mapped, and ground-truth 3D bounding boxes for use in evaluation. BEAR website: https://qcr.github.io/dataset/benchbot-bear-data/ .},
  archive  = {J},
  author   = {David Hall and Ben Talbot and Suman Raj Bista and Haoyang Zhang and Rohan Smith and Feras Dayoub and Niko Sünderhauf},
  doi      = {10.1177/02783649211069404},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {3},
  pages    = {259-269},
  title    = {BenchBot environments for active robotics (BEAR): Simulated data for active scene understanding research},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GRSTAPS: Graphically recursive simultaneous task allocation,
planning, and scheduling. <em>The International Journal of Robotics
Research</em>, <em>41</em>(2), 232–256. (<a
href="https://doi.org/10.1177/02783649211052066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Effective deployment of multi-robot teams requires solving several interdependent problems at varying levels of abstraction. Specifically, heterogeneous multi-robot systems must answer four important questions: what (task planning), how (motion planning), who (task allocation), and when (scheduling). Although there are rich bodies of work dedicated to various combinations of these questions, a fully integrated treatment of all four questions lies beyond the scope of the current literature, which lacks even a formal description of the complete problem. In this article, we address this absence, first by formalizing this class of multi-robot problems under the banner Simultaneous Task Allocation and Planning with Spatiotemporal Constraints (STAP-STC) , and then by proposing a solution that we call Graphically Recursive Simultaneous Task Allocation, Planning, and Scheduling (GRSTAPS). GRSTAPS interleaves task planning, task allocation, scheduling, and motion planning, performing a multi-layer search while effectively sharing information among system modules. In addition to providing a unified solution to STAP-STC problems, GRSTAPS includes individual innovations both in task planning and task allocation. At the task planning level, our interleaved approach allows the planner to abstract away which agents will perform a task using an approach that we refer to as agent-agnostic planning . At the task allocation level, we contribute a search-based algorithm that can simultaneously satisfy planning constraints and task requirements while optimizing the associated schedule. We demonstrate the efficacy of GRSTAPS using detailed ablative and comparative experiments in a simulated emergency-response domain. Results of these experiments conclusively demonstrate that GRSTAPS outperforms both ablative baselines and state-of-the-art temporal planners in terms of computation time, solution quality, and problem coverage.},
  archive  = {J},
  author   = {Andrew Messing and Glen Neville and Sonia Chernova and Seth Hutchinson and Harish Ravichandar},
  doi      = {10.1177/02783649211052066},
  journal  = {The International Journal of Robotics Research},
  month    = {2},
  number   = {2},
  pages    = {232-256},
  title    = {GRSTAPS: Graphically recursive simultaneous task allocation, planning, and scheduling},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation, learning, and planning algorithms for
geometric task and motion planning. <em>The International Journal of
Robotics Research</em>, <em>41</em>(2), 210–231. (<a
href="https://doi.org/10.1177/02783649211038280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a framework for learning to guide geometric task-and-motion planning ( G- TAMP ). G- TAMP is a subclass of task-and-motion planning in which the goal is to move multiple objects to target regions among movable obstacles. A standard graph search algorithm is not directly applicable, because G- TAMP problems involve hybrid search spaces and expensive action feasibility checks. To handle this, we introduce a novel planner that extends basic heuristic search with random sampling and a heuristic function that prioritizes feasibility checking on promising state–action pairs. The main drawback of such pure planners is that they lack the ability to learn from planning experience to improve their efficiency. We propose two learning algorithms to address this. The first is an algorithm for learning a rank function that guides the discrete task-level search, and the second is an algorithm for learning a sampler that guides the continuous motion-level search. We propose design principles for designing data-efficient algorithms for learning from planning experience and representations for effective generalization. We evaluate our framework in challenging G- TAMP problems, and show that we can improve both planning and data efficiency.},
  archive  = {J},
  author   = {Beomjoon Kim and Luke Shimanuki and Leslie Pack Kaelbling and Tomás Lozano-Pérez},
  doi      = {10.1177/02783649211038280},
  journal  = {The International Journal of Robotics Research},
  month    = {2},
  number   = {2},
  pages    = {210-231},
  title    = {Representation, learning, and planning algorithms for geometric task and motion planning},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained stochastic optimal control with learned
importance sampling: A path integral approach. <em>The International
Journal of Robotics Research</em>, <em>41</em>(2), 189–209. (<a
href="https://doi.org/10.1177/02783649211047890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Modern robotic systems are expected to operate robustly in partially unknown environments. This article proposes an algorithm capable of controlling a wide range of high-dimensional robotic systems in such challenging scenarios. Our method is based on the path integral formulation of stochastic optimal control, which we extend with constraint-handling capabilities. Under our control law, the optimal input is inferred from a set of stochastic rollouts of the system dynamics. These rollouts are simulated by a physics engine, placing minimal restrictions on the types of systems and environments that can be modeled. Although sampling-based algorithms are typically not suitable for online control, we demonstrate in this work how importance sampling and constraints can be used to effectively curb the sampling complexity and enable real-time control applications. Furthermore, the path integral framework provides a natural way of incorporating existing control architectures as ancillary controllers for shaping the sampling distribution. Our results reveal that even in cases where the ancillary controller would fail, our stochastic control algorithm provides an additional safety and robustness layer. Moreover, in the absence of an existing ancillary controller, our method can be used to train a parametrized importance sampling policy using data from the stochastic rollouts. The algorithm may thereby bootstrap itself by learning an importance sampling policy offline and then refining it to unseen environments during online control. We validate our results on three robotic systems, including hardware experiments on a quadrupedal robot.},
  archive  = {J},
  author   = {Jan Carius and René Ranftl and Farbod Farshidian and Marco Hutter},
  doi      = {10.1177/02783649211047890},
  journal  = {The International Journal of Robotics Research},
  month    = {2},
  number   = {2},
  pages    = {189-209},
  title    = {Constrained stochastic optimal control with learned importance sampling: A path integral approach},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from demonstration using products of experts:
Applications to manipulation and task prioritization. <em>The
International Journal of Robotics Research</em>, <em>41</em>(2),
163–188. (<a href="https://doi.org/10.1177/02783649211040561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Probability distributions are key components of many learning from demonstration (LfD) approaches, with the spaces chosen to represent tasks playing a central role. Although the robot configuration is defined by its joint angles, end-effector poses are often best explained within several task spaces. In many approaches, distributions within relevant task spaces are learned independently and only combined at the control level. This simplification implies several problems that are addressed in this work. We show that the fusion of models in different task spaces can be expressed as products of experts (PoE), where the probabilities of the models are multiplied and renormalized so that it becomes a proper distribution of joint angles. Multiple experiments are presented to show that learning the different models jointly in the PoE framework significantly improves the quality of the final model. The proposed approach particularly stands out when the robot has to learn hierarchical objectives that arise when a task requires the prioritization of several sub-tasks (e.g. in a humanoid robot, keeping balance has a higher priority than reaching for an object). Since training the model jointly usually relies on contrastive divergence, which requires costly approximations that can affect performance, we propose an alternative strategy using variational inference and mixture model approximations. In particular, we show that the proposed approach can be extended to PoE with a nullspace structure (PoENS), where the model is able to recover secondary tasks that are masked by the resolution of tasks of higher-importance.},
  archive  = {J},
  author   = {Emmanuel Pignat and Joāo Silvério and Sylvain Calinon},
  doi      = {10.1177/02783649211040561},
  journal  = {The International Journal of Robotics Research},
  month    = {2},
  number   = {2},
  pages    = {163-188},
  title    = {Learning from demonstration using products of experts: Applications to manipulation and task prioritization},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Force sensing in robot-assisted keyhole endoscopy: A
systematic survey. <em>The International Journal of Robotics
Research</em>, <em>41</em>(2), 136–162. (<a
href="https://doi.org/10.1177/02783649211052067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Instrument–tissue interaction forces in minimally invasive surgery (MIS) provide valuable information that can be used to provide haptic perception, monitor tissue trauma, develop training guidelines, and evaluate the skill level of novice and expert surgeons. Force and tactile sensing is lost in many robot-assisted surgery (RAS) systems. Therefore, many researchers have focused on recovering this information through sensing systems and estimation algorithms. This article provides a comprehensive systematic review of the current force sensing research aimed at RAS and, more generally, keyhole endoscopy, in which instruments enter the body through small incisions. Articles published between January 2011 and May 2020 are considered, following the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines. The literature search resulted in 110 papers on different force estimation algorithms and sensing technologies, sensor design specifications, and fabrication techniques.},
  archive  = {J},
  author   = {Amir Hossein Hadi Hosseinabadi and Septimiu E. Salcudean},
  doi      = {10.1177/02783649211052067},
  journal  = {The International Journal of Robotics Research},
  month    = {2},
  number   = {2},
  pages    = {136-162},
  title    = {Force sensing in robot-assisted keyhole endoscopy: A systematic survey},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A large-scale dataset for indoor visual localization with
high-precision ground truth. <em>The International Journal of Robotics
Research</em>, <em>41</em>(2), 129–135. (<a
href="https://doi.org/10.1177/02783649211052064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a challenging new dataset for indoor localization research. We have recorded the whole internal structure of Fengtai Wanda Plaza which is an area of over 15,800 m 2 with a Navvis M6 device. The dataset contains 679 RGB-D panoramas and 2,664 query images collected by three different smartphones. In addition to the data, an aligned 3D point cloud is produced after the elimination of moving objects based on the building floorplan. Furthermore, a method is provided to generate corresponding high-resolution depth images for each panorama. By fixing the smartphones on the device using a specially designed bracket, six-degree-of-freedom camera poses can be calculated precisely. We believe it can give a new benchmark for indoor visual localization and the full dataset can be downloaded from http://vision.ia.ac.cn/Faculty/wgao/data_code/data_indoor_localizaiton/data_indoor_localization.htm},
  archive  = {J},
  author   = {Yuchen Liu and Wei Gao and Zhanyi Hu},
  doi      = {10.1177/02783649211052064},
  journal  = {The International Journal of Robotics Research},
  month    = {2},
  number   = {2},
  pages    = {129-135},
  title    = {A large-scale dataset for indoor visual localization with high-precision ground truth},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive navigation in partially familiar planar
environments using semantic perceptual feedback. <em>The International
Journal of Robotics Research</em>, <em>41</em>(1), 85–126. (<a
href="https://doi.org/10.1177/02783649211048931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article solves the planar navigation problem by recourse to an online reactive scheme that exploits recent advances in simultaneous localization and mapping (SLAM) and visual object recognition to recast prior geometric knowledge in terms of an offline catalog of familiar objects. The resulting vector field planner guarantees convergence to an arbitrarily specified goal, avoiding collisions along the way with fixed but arbitrarily placed instances from the catalog as well as completely unknown fixed obstacles so long as they are strongly convex and well separated. We illustrate the generic robustness properties of such deterministic reactive planners as well as the relatively modest computational cost of this algorithm by supplementing an extensive numerical study with physical implementation on both a wheeled and legged platform in different settings.},
  archive  = {J},
  author   = {Vasileios Vasilopoulos and Georgios Pavlakos and Karl Schmeckpeper and Kostas Daniilidis and Daniel E. Koditschek},
  doi      = {10.1177/02783649211048931},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {85-126},
  title    = {Reactive navigation in partially familiar planar environments using semantic perceptual feedback},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enabling impedance-based physical human–multi–robot
collaboration: Experiments with four torque-controlled manipulators.
<em>The International Journal of Robotics Research</em>, <em>41</em>(1),
68–84. (<a href="https://doi.org/10.1177/02783649211053650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robotics research into multi-robot systems so far has concentrated on implementing intelligent swarm behavior and contact-less human interaction. Studies of haptic or physical human-robot interaction, by contrast, have primarily focused on the assistance offered by a single robot. Consequently, our understanding of the physical interaction and the implicit communication through contact forces between a human and a team of multiple collaborative robots is limited. We here introduce the term Physical Human Multi-Robot Collaboration (PHMRC) to describe this more complex situation, which we consider highly relevant in future service robotics. The scenario discussed in this article covers multiple manipulators in close proximity and coupled through physical contacts. We represent this set of robots as fingers of an up-scaled agile robot hand. This perspective enables us to employ model-based grasping theory to deal with multi-contact situations. Our torque-control approach integrates dexterous multi-manipulator grasping skills, optimization of contact forces, compensation of object dynamics, and advanced impedance regulation into a coherent compliant control scheme. For this to achieve, we contribute fundamental theoretical improvements. Finally, experiments with up to four collaborative KUKA LWR IV+ manipulators performed both in simulation and real world validate the model-based control approach. As a side effect, we notice that our multi-manipulator control framework applies identically to multi-legged systems, and we execute it also on the quadruped ANYmal subject to non-coplanar contacts and human interaction.},
  archive  = {J},
  author   = {Niels Dehio and Joshua Smith and Dennis L. Wigand and Pouya Mohammadi and Michael Mistry and Jochen J. Steil},
  doi      = {10.1177/02783649211053650},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {68-84},
  title    = {Enabling impedance-based physical human–multi–robot collaboration: Experiments with four torque-controlled manipulators},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning reward functions from diverse sources of human
feedback: Optimally integrating demonstrations and preferences. <em>The
International Journal of Robotics Research</em>, <em>41</em>(1), 45–67.
(<a href="https://doi.org/10.1177/02783649211041652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Reward functions are a common way to specify the objective of a robot. As designing reward functions can be extremely challenging, a more promising approach is to directly learn reward functions from human teachers. Importantly, data from human teachers can be collected either passively or actively in a variety of forms: passive data sources include demonstrations (e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings) are actively elicited. Prior research has independently applied reward learning to these different data sources. However, there exist many domains where multiple sources are complementary and expressive. Motivated by this general problem, we present a framework to integrate multiple sources of information, which are either passively or actively collected from human users. In particular, we present an algorithm that first utilizes user demonstrations to initialize a belief about the reward function, and then actively probes the user with preference queries to zero-in on their true reward. This algorithm not only enables us combine multiple data sources, but it also informs the robot when it should leverage each type of information. Further, our approach accounts for the human’s ability to provide data: yielding user-friendly preference queries which are also theoretically optimal. Our extensive simulated experiments and user studies on a Fetch mobile manipulator demonstrate the superiority and the usability of our integrated framework.},
  archive  = {J},
  author   = {Erdem Bıyık and Dylan P. Losey and Malayandi Palan and Nicholas C. Landolfi and Gleb Shevchuk and Dorsa Sadigh},
  doi      = {10.1177/02783649211041652},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {45-67},
  title    = {Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physical interaction as communication: Learning robot
objectives online from human corrections. <em>The International Journal
of Robotics Research</em>, <em>41</em>(1), 20–44. (<a
href="https://doi.org/10.1177/02783649211050958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {When a robot performs a task next to a human, physical interaction is inevitable: the human might push, pull, twist, or guide the robot. The state of the art treats these interactions as disturbances that the robot should reject or avoid. At best, these robots respond safely while the human interacts; but after the human lets go, these robots simply return to their original behavior. We recognize that physical human–robot interaction (pHRI) is often intentional: the human intervenes on purpose because the robot is not doing the task correctly. In this article, we argue that when pHRI is intentional it is also informative: the robot can leverage interactions to learn how it should complete the rest of its current task even after the person lets go. We formalize pHRI as a dynamical system, where the human has in mind an objective function they want the robot to optimize, but the robot does not get direct access to the parameters of this objective: they are internal to the human. Within our proposed framework human interactions become observations about the true objective. We introduce approximations to learn from and respond to pHRI in real-time. We recognize that not all human corrections are perfect: often users interact with the robot noisily, and so we improve the efficiency of robot learning from pHRI by reducing unintended learning. Finally, we conduct simulations and user studies on a robotic manipulator to compare our proposed approach with the state of the art. Our results indicate that learning from pHRI leads to better task performance and improved human satisfaction.},
  archive  = {J},
  author   = {Dylan P. Losey and Andrea Bajcsy and Marcia K. O’Malley and Anca D. Dragan},
  doi      = {10.1177/02783649211050958},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {20-44},
  title    = {Physical interaction as communication: Learning robot objectives online from human corrections},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A bathymetric mapping and SLAM dataset with high-precision
ground truth for marine robotics. <em>The International Journal of
Robotics Research</em>, <em>41</em>(1), 12–19. (<a
href="https://doi.org/10.1177/02783649211044749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, sonar systems for surface and underwater vehicles have increased in resolution and become significantly less expensive. As such, these systems are viable at a wide range of price points and are appropriate for a broad set of applications on surface and underwater vehicles. However, to take full advantage of these high-resolution sensors for seafloor mapping tasks an adequate navigation solution is also required. In GPS-denied environments this usually necessitates a simultaneous localization and mapping (SLAM) technique to maintain good accuracy with minimal error accumulation. Acoustic positioning systems such as ultra short baseline (USBL) and long baseline (LBL) are sometimes deployed to provide additional bounds on the navigation solution, but the positional uncertainty of these systems is often much greater than the resolution of modern multibeam or interferometric side scan sonars. As such, subsurface vehicles often lack the means to adequately ground-truth navigation solutions and the resulting bathymetic maps. In this article, we present a dataset with four separate surveys designed to test bathymetric SLAM algorithms using two modern sonars, typical underwater vehicle navigation sensors, and high-precision (2 cm horizontal, 10 cm vertical) real-time kinematic (RTK) GPS ground truth. In addition, these data can be used to refine and improve other aspects of multibeam sonar mapping such as ray-tracing, gridding techniques, and time-varying attitude corrections.},
  archive  = {J},
  author   = {Kristopher Krasnosky and Christopher Roman and David Casagrande},
  doi      = {10.1177/02783649211044749},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {12-19},
  title    = {A bathymetric mapping and SLAM dataset with high-precision ground truth for marine robotics},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HARMONIC: A multimodal dataset of assistive human–robot
collaboration. <em>The International Journal of Robotics Research</em>,
<em>41</em>(1), 3–11. (<a
href="https://doi.org/10.1177/02783649211050677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present the Human And Robot Multimodal Observations of Natural Interactive Collaboration (HARMONIC) dataset. This is a large multimodal dataset of human interactions with a robotic arm in a shared autonomy setting designed to imitate assistive eating. The dataset provides human, robot, and environmental data views of 24 different people engaged in an assistive eating task with a 6-degree-of-freedom (6-DOF) robot arm. From each participant, we recorded video of both eyes, egocentric video from a head-mounted camera, joystick commands, electromyography from the forearm used to operate the joystick, third-person stereo video, and the joint positions of the 6-DOF robot arm. Also included are several features that come as a direct result of these recordings, such as eye gaze projected onto the egocentric video, body pose, hand pose, and facial keypoints. These data streams were collected specifically because they have been shown to be closely related to human mental states and intention. This dataset could be of interest to researchers studying intention prediction, human mental state modeling, and shared autonomy. Data streams are provided in a variety of formats such as video and human-readable CSV and YAML files.},
  archive  = {J},
  author   = {Benjamin A. Newman and Reuben M. Aronson and Siddhartha S. Srinivasa and Kris Kitani and Henny Admoni},
  doi      = {10.1177/02783649211050677},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {3-11},
  title    = {HARMONIC: A multimodal dataset of assistive human–robot collaboration},
  volume   = {41},
  year     = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
