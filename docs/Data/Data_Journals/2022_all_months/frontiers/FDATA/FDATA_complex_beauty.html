<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FDATA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fdata---137">FDATA - 137</h2>
<ul>
<li><details>
<summary>
(2022). Natural and artificial dynamics in graphs: Concept,
progress, and future. <em>FDATA</em>, <em>5</em>, 1062637. (<a
href="https://doi.org/10.3389/fdata.2022.1062637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph structures have attracted much research attention for carrying complex relational information. Based on graphs, many algorithms and tools are proposed and developed for dealing with real-world tasks such as recommendation, fraud detection, molecule design, etc. In this paper, we first discuss three topics of graph research, i.e., graph mining, graph representations, and graph neural networks (GNNs). Then, we introduce the definitions of natural dynamics and artificial dynamics in graphs, and the related works of natural and artificial dynamics about how they boost the aforementioned graph research topics, where we also discuss the current limitation and future opportunities.},
  archive      = {J_FDATA},
  author       = {Fu, Dongqi and He, Jingrui},
  doi          = {10.3389/fdata.2022.1062637},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1062637},
  shortjournal = {Front. Big Data},
  title        = {Natural and artificial dynamics in graphs: Concept, progress, and future},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effects of long COVID-19, its severity, and the need for
immediate attention: Analysis of clinical trials and twitter data.
<em>FDATA</em>, <em>5</em>, 1051386. (<a
href="https://doi.org/10.3389/fdata.2022.1051386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundThe coronavirus disease 2019 (COVID-19) has been declared a pandemic since March 2020 by the World Health Organization; identifying the disease progression, predicting patient outcomes early, the possibility of long-term adverse events through effective modeling, and the use of real-world data are of immense importance to effective treatment, resource allocation, and prevention of severe adverse events of grade 4 or 5.MethodsFirst, we raise awareness about the different clinical trials on long COVID-19. The trials were selected with the search term “long COVID-19” available in ClinicalTrials.gov. Second, we curated the recent tweets on long-haul COVID-19 and gave an overview of the sentiments of the people. The tweets obtained with the query term #long COVID-19 consisted of 8,436 tweets between 28 August 2022 and 06 September 2022. We utilized the National Research Council (NRC) Emotion Lexicon method for sentiment analysis. Finally, we analyze the retweet and favorite counts are associated with the sentiments of the tweeters via a negative binomial regression model.ResultsOur results find that there are two types of clinical trials being conducted: observational and interventional. The retweet counts and favorite counts are associated with the sentiments and emotions, such as disgust, joy, sadness, surprise, trust, negative, and positive.ConclusionWe need resources and further research in the area of long COVID-19.},
  archive      = {J_FDATA},
  author       = {Bhattacharyya, Arinjita and Seth, Anand and Rai, Shesh},
  doi          = {10.3389/fdata.2022.1051386},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1051386},
  shortjournal = {Front. Big Data},
  title        = {The effects of long COVID-19, its severity, and the need for immediate attention: Analysis of clinical trials and twitter data},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resolving responsibility gaps for lethal autonomous weapon
systems. <em>FDATA</em>, <em>5</em>, 1038507. (<a
href="https://doi.org/10.3389/fdata.2022.1038507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper offers a novel understanding of collective responsibility for AI outcomes that can help resolve the “problem of many hands” and “responsibility gaps” when it comes to AI failure, especially in the context of lethal autonomous weapon systems.},
  archive      = {J_FDATA},
  author       = {Smith, Patrick Taylor},
  doi          = {10.3389/fdata.2022.1038507},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1038507},
  shortjournal = {Front. Big Data},
  title        = {Resolving responsibility gaps for lethal autonomous weapon systems},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Worldwide impact of lifestyle predictors of dementia
prevalence: An eXplainable artificial intelligence analysis.
<em>FDATA</em>, <em>5</em>, 1027783. (<a
href="https://doi.org/10.3389/fdata.2022.1027783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDementia is an umbrella term indicating a group of diseases that affect the cognitive sphere. Dementia is not a mere individual health issue, since its interference with the ability to carry out daily activities entails a series of collateral problems, comprising exclusion of patients from civil rights and welfare, unpaid caregiving work, mostly performed by women, and an additional burden on the public healthcare systems. Thus, gender and wealth inequalities (both among individuals and among countries) tend to amplify the social impact of such a disease. Since at present there is no cure for dementia but only drug treatments to slow down its progress and mitigate the symptoms, it is essential to work on prevention and early diagnosis, identifying the risk factors that increase the probability of its onset. The complex and multifactorial etiology of dementia, resulting from an interplay between genetics and environmental factors, can benefit from a multidisciplinary approach that follows the “One Health” guidelines of the World Health Organization.MethodsIn this work, we apply methods of Artificial Intelligence and complex systems physics to investigate the possibility to predict dementia prevalence throughout world countries from a set of variables concerning individual health, food consumption, substance use and abuse, healthcare system efficiency. The analysis uses publicly available indicator values at a country level, referred to a time window of 26 years.ResultsEmploying methods based on eXplainable Artificial Intelligence (XAI) and complex networks, we identify a group of lifestyle factors, mostly concerning nutrition, that contribute the most to dementia incidence prediction.DiscussionThe proposed approach provides a methodological basis to develop quantitative tools for action patterns against such a disease, which involves issues deeply related with sustainable, such as good health and resposible food consumption.},
  archive      = {J_FDATA},
  author       = {Bellantuono, Loredana and Monaco, Alfonso and Amoroso, Nicola and Lacalamita, Antonio and Pantaleo, Ester and Tangaro, Sabina and Bellotti, Roberto},
  doi          = {10.3389/fdata.2022.1027783},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1027783},
  shortjournal = {Front. Big Data},
  title        = {Worldwide impact of lifestyle predictors of dementia prevalence: An eXplainable artificial intelligence analysis},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-scale governance and data for sustainable development.
<em>FDATA</em>, <em>5</em>, 1025256. (<a
href="https://doi.org/10.3389/fdata.2022.1025256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future societal systems will be characterized by heterogeneous human behaviors and data-driven collective action. Complexity will arise as a consequence of the 5th Industrial Revolution and 2nd Data Revolution possible, thanks to a new generation of digital systems and the Metaverse. These technologies will enable new computational methods to tackle inequality while preserving individual rights and self-development. In this context, we do not only need data innovation and computational science, but also new forms of digital policy and governance. The emerging fragility or robustness of the system will depend on how complexity and governance are developed. Through data, humanity has been able to study a number of multi-scale systems from biological to migratory. Multi-scale governance is the new paradigm that feeds the Data Revolution in a world that would be highly digitalized. In the social dimension, we will encounter meta-populations sharing economy and human values. In the temporal dimension, we still need to make all real-time response, evaluation, and mitigation systems a standard integrated system into policy and governance to build up a resilient digital society. Top-down governance is not sufficient to manage all the complexities and exploit all the data available. Coordinating top-down agencies with bottom-up digital platforms will be the design principle. Digital platforms have to be built on top of data innovation and implement Artificial Intelligence (AI)-driven systems to connect, compute, collaborate, and curate data to implement data-driven policy for sustainable development based on Collective Intelligence.},
  archive      = {J_FDATA},
  author       = {Pastor-Escuredo, David and Gardeazabal, Andrea and Koo, Jawoo and Imai, Asuka and Treleaven, Philip},
  doi          = {10.3389/fdata.2022.1025256},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1025256},
  shortjournal = {Front. Big Data},
  title        = {Multi-scale governance and data for sustainable development},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A data-driven spatial approach to characterize the flood
hazard. <em>FDATA</em>, <em>5</em>, 1022900. (<a
href="https://doi.org/10.3389/fdata.2022.1022900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model output of localized flood grids are useful in characterizing flood hazards for properties located in the Special Flood Hazard Area (SFHA—areas expected to experience a 1% or greater annual chance of flooding). However, due to the unavailability of higher return-period [i.e., recurrence interval, or the reciprocal of the annual exceedance probability (AEP)] flood grids, the flood risk of properties located outside the SFHA cannot be quantified. Here, we present a method to estimate flood hazards that are located both inside and outside the SFHA using existing AEP surfaces. Flood hazards are characterized by the Gumbel extreme value distribution to project extreme flood event elevations for which an entire area is assumed to be submerged. Spatial interpolation techniques impute flood elevation values and are used to estimate flood hazards for areas outside the SFHA. The proposed method has the potential to improve the assessment of flood risk for properties located both inside and outside the SFHA and therefore to improve the decision-making process regarding flood insurance purchases, mitigation strategies, and long-term planning for enhanced resilience to one of the world&#39;s most ubiquitous natural hazards.},
  archive      = {J_FDATA},
  author       = {Mostafiz, Rubayet Bin and Rahim, Md Adilur and Friedland, Carol J. and Rohli, Robert V. and Bushra, Nazla and Orooji, Fatemeh},
  doi          = {10.3389/fdata.2022.1022900},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {1022900},
  shortjournal = {Front. Big Data},
  title        = {A data-driven spatial approach to characterize the flood hazard},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring the use of topological data analysis to
automatically detect data quality faults. <em>FDATA</em>, <em>5</em>,
931398. (<a href="https://doi.org/10.3389/fdata.2022.931398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quality problems may occur in various forms in structured and semi-structured data sources. This paper details an unsupervised method of analyzing data quality that is agnostic to the semantics of the data, the format of the encoding, or the internal structure of the dataset. A distance function is used to transform each record of a dataset into an n-dimensional vector of real numbers, which effectively transforms the original data into a high-dimensional point cloud. The shape of the point cloud is then efficiently examined via topological data analysis to find high-dimensional anomalies that may signal quality issues. The specific quality faults examined in this paper are the detection of records that, while not exactly the same, refer to the same entity. Our algorithm, based on topological data analysis, provides similar accuracy for both higher and lower quality data and performs better than a baseline approach for data with poor quality.},
  archive      = {J_FDATA},
  author       = {Tudoreanu, M. Eduard},
  doi          = {10.3389/fdata.2022.931398},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {931398},
  shortjournal = {Front. Big Data},
  title        = {Exploring the use of topological data analysis to automatically detect data quality faults},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pharmacogenomics driven decision support prototype with
machine learning: A framework for improving patient care.
<em>FDATA</em>, <em>5</em>, 1059088. (<a
href="https://doi.org/10.3389/fdata.2022.1059088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionA growing number of healthcare providers make complex treatment decisions guided by electronic health record (EHR) software interfaces. Many interfaces integrate multiple sources of data (e.g., labs, pharmacy, diagnoses) successfully, though relatively few have incorporated genetic data.MethodThis study utilizes informatics methods with predictive modeling to create and validate algorithms to enable informed pharmacogenomic decision-making at the point of care in near real-time. The proposed framework integrates EHR and genetic data relevant to the patient&#39;s current medications including decision support mechanisms based on predictive modeling. We created a prototype with EHR and linked genetic data from the Department of Veterans Affairs (VA), the largest integrated healthcare system in the US. The EHR data included diagnoses, medication fills, and outpatient clinic visits for 2,600 people with HIV and matched uninfected controls linked to prototypic genetic data (variations in single or multiple positions in the DNA sequence). We then mapped the medications that patients were prescribed to medications defined in the drug-gene interaction mapping of the Clinical Pharmacogenomics Implementation Consortium&#39;s (CPIC) level A (i.e., sufficient evidence for at least one prescribing action) guidelines that predict adverse events. CPIC is a National Institute of Health funded group of experts who develop evidence based pharmacogenomic guidelines. Preventable adverse events (PAE) can be defined as a harmful outcome from an intervention that could have been prevented. For this study, we focused on potential PAEs resulting from a medication-gene interaction.ResultsThe final model showed AUC scores of 0.972 with an F1 score of 0.97 with genetic data as compared to 0.766 and 0.73 respectively, without genetic data integration.DiscussionOver 98% of people in the cohort were on at least one medication with CPIC level a guideline in their lifetime. We compared predictive power of machine learning models to detect a PAE between five modeling methods: Random Forest, Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), K Nearest neighbors (KNN), and Decision Tree. We found that XGBoost performed best for the prototype when genetic data was added to the framework and improved prediction of PAE. We compared area under the curve (AUC) between the models in the testing dataset.},
  archive      = {J_FDATA},
  author       = {Kidwai-Khan, Farah and Rentsch, Christopher T. and Pulk, Rebecca and Alcorn, Charles and Brandt, Cynthia A. and Justice, Amy C.},
  doi          = {10.3389/fdata.2022.1059088},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1059088},
  shortjournal = {Front. Big Data},
  title        = {Pharmacogenomics driven decision support prototype with machine learning: A framework for improving patient care},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic transfer learning with progressive meta-task
scheduler. <em>FDATA</em>, <em>5</em>, 1052972. (<a
href="https://doi.org/10.3389/fdata.2022.1052972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic transfer learning refers to the knowledge transfer from a static source task with adequate label information to a dynamic target task with little or no label information. However, most existing theoretical studies and practical algorithms of dynamic transfer learning assume that the target task is continuously evolving over time. This strong assumption is often violated in real world applications, e.g., the target distribution is suddenly changing at some time stamp. To solve this problem, in this paper, we propose a novel meta-learning framework L2S based on a progressive meta-task scheduler for dynamic transfer learning. The crucial idea of L2S is to incrementally learn to schedule the meta-pairs of tasks and then learn the optimal model initialization from those meta-pairs of tasks for fast adaptation to the newest target task. The effectiveness of our L2S framework is verified both theoretically and empirically.},
  archive      = {J_FDATA},
  author       = {Wu, Jun and He, Jingrui},
  doi          = {10.3389/fdata.2022.1052972},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1052972},
  shortjournal = {Front. Big Data},
  title        = {Dynamic transfer learning with progressive meta-task scheduler},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial data analysis for intelligent buildings: Awareness
of context and data uncertainty. <em>FDATA</em>, <em>5</em>, 1049198.
(<a href="https://doi.org/10.3389/fdata.2022.1049198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent buildings are among the most active Internet-of-Things (IoT) verticals, encompassing various IoT-enabled devices and sensing technologies for digital transformation. Analysis of spatial data, a very common type of data collected in intelligent buildings, offers a lot of insights for many purposes such as facilitating space management and enhancing the utilization efficiency of buildings. In this paper, we recognize two major challenges in spatial data analysis for intelligent buildings (SDAIB): (1) the complicated analytical contexts that are related to the building space and internal entities and (2) the uncertainty of spatial data due to the limitations of positioning and other sensing technologies. To address these challenges, we identify and categorize different kinds of analytical contexts and spatial data uncertainties in SDAIB, and propose a unified modeling framework for handling them. Furthermore, we showcase how the proposed framework and the associated modeling techniques are used to enable context-aware and uncertainty-aware SDAIB, in the tasks of hotspot discovery, path planning, semantic trajectory generation, and distance monitoring. Finally, we offer several research directions of SDAIB, in line with the emerging trends of the IoT.},
  archive      = {J_FDATA},
  author       = {Li, Huan and Liu, Tiantian and Chan, Harry Kai-Ho and Lu, Hua},
  doi          = {10.3389/fdata.2022.1049198},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1049198},
  shortjournal = {Front. Big Data},
  title        = {Spatial data analysis for intelligent buildings: Awareness of context and data uncertainty},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TELS: Evolution patterns of research keywords from the
evidence of PNAS social sciences topics. <em>FDATA</em>, <em>5</em>,
1045513. (<a href="https://doi.org/10.3389/fdata.2022.1045513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By reviewing scientific literature, researchers may obtain a comprehensive understanding of field developments, keeping abreast of the current research status and hotspot shifts. The evolution pattern of keywords is supposed to be an efficient indicator in revealing the shifting and sustainability configuration of scientific concepts, ideas, and research hotspots. Here we take an extensive investigation of the evolution of keywords among all publications in PNAS Social Sciences from 1990 to 2021. Statistical tests show the keyword mention time series always accompanied by the emergence of a log-normal distribution. Additionally, we introduce a novel schema of four patterns (TELS), which are Transient impact type, Explosive impact type, Large impact type, and Small impact type, respectively, to illustrate the evolution of keywords. The TELS schema can be used to capture the whole life circle feature of any proposed keyword, from a pool of candidates. By dividing the entire time into four periods, we also introduce the concept of elite keywords to reveal the temporal feature of social sciences focus. An explicit transition from anthropology research to neuroscience and social problems research can be observed from the evolution diagram. We argue that the proposed method is of general sense and might be applicable to other fields of science.},
  archive      = {J_FDATA},
  author       = {Liu, Bing and Shi, Mengfan and Kuang, Yi and Jiang, Xin},
  doi          = {10.3389/fdata.2022.1045513},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1045513},
  shortjournal = {Front. Big Data},
  title        = {TELS: Evolution patterns of research keywords from the evidence of PNAS social sciences topics},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modern hopfield networks for graph embedding.
<em>FDATA</em>, <em>5</em>, 1044709. (<a
href="https://doi.org/10.3389/fdata.2022.1044709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network embedding task is to represent a node in a network as a low-dimensional vector while incorporating the topological and structural information. Most existing approaches solve this problem by factorizing a proximity matrix, either directly or implicitly. In this work, we introduce a network embedding method from a new perspective, which leverages Modern Hopfield Networks (MHN) for associative learning. Our network learns associations between the content of each node and that node&#39;s neighbors. These associations serve as memories in the MHN. The recurrent dynamics of the network make it possible to recover the masked node, given that node&#39;s neighbors. Our proposed method is evaluated on different benchmark datasets for downstream tasks such as node classification, link prediction, and graph coarsening. The results show competitive performance compared to the common matrix factorization techniques and deep learning based methods.},
  archive      = {J_FDATA},
  author       = {Liang, Yuchen and Krotov, Dmitry and Zaki, Mohammed J.},
  doi          = {10.3389/fdata.2022.1044709},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1044709},
  shortjournal = {Front. Big Data},
  title        = {Modern hopfield networks for graph embedding},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Methods for detecting probable COVID-19 cases from
large-scale survey data also reveal probable sex differences in symptom
profiles. <em>FDATA</em>, <em>5</em>, 1043704. (<a
href="https://doi.org/10.3389/fdata.2022.1043704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundDaily symptom reporting collected via web-based symptom survey tools holds the potential to improve disease monitoring. Such screening tools might be able to not only discriminate between states of acute illness and non-illness, but also make use of additional demographic information so as to identify how illnesses may differ across groups, such as biological sex. These capabilities may play an important role in the context of future disease outbreaks.ObjectiveUse data collected via a daily web-based symptom survey tool to develop a Bayesian model that could differentiate between COVID-19 and other illnesses and refine this model to identify illness profiles that differ by biological sex.MethodsWe used daily symptom profiles to plot symptom progressions for COVID-19, influenza (flu), and the common cold. We then built a Bayesian network to discriminate between these three illnesses based on daily symptom reports. We further separated out the COVID-19 cohort into self-reported female and male subgroups to observe any differences in symptoms relating to sex. We identified key symptoms that contributed to a COVID-19 prediction in both males and females using a logistic regression model.ResultsAlthough the Bayesian model performed only moderately well in identifying a COVID-19 diagnosis (71.6% true positive rate), the model showed promise in being able to differentiate between COVID-19, flu, and the common cold, as well as periods of acute illness vs. non-illness. Additionally, COVID-19 symptoms differed between the biological sexes; specifically, fever was a more important symptom in identifying subsequent COVID-19 infection among males than among females.ConclusionWeb-based symptom survey tools hold promise as tools to identify illness and may help with coordinated disease outbreak responses. Incorporating demographic factors such as biological sex into predictive models may elucidate important differences in symptom profiles that hold implications for disease detection.},
  archive      = {J_FDATA},
  author       = {Klein, Amit and Puldon, Karena and Dilchert, Stephan and Hartogensis, Wendy and Chowdhary, Anoushka and Anglo, Claudine and Pandya, Leena S. and Hecht, Frederick M. and Mason, Ashley E. and Smarr, Benjamin L.},
  doi          = {10.3389/fdata.2022.1043704},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1043704},
  shortjournal = {Front. Big Data},
  title        = {Methods for detecting probable COVID-19 cases from large-scale survey data also reveal probable sex differences in symptom profiles},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using facebook advertising data to describe the
socio-economic situation of syrian refugees in lebanon. <em>FDATA</em>,
<em>5</em>, 1033530. (<a
href="https://doi.org/10.3389/fdata.2022.1033530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the fighting in the Syrian civil war has mostly stopped, an estimated 5.6 million Syrians remain living in neighboring countries1. Of these, an estimated 1.5 million are sheltering in Lebanon. Ongoing efforts by organizations such as UNHCR to support the refugee population are often ineffective in reaching those most in need. According to UNHCR&#39;s 2019 Vulnerability Assessment of Syrian Refugees Report (VASyR), only 44% of the Syrian refugee families eligible for multipurpose cash assistance were provided with help, as the others were not captured in the data. In this project, we are investigating the use of non-traditional data, derived from Facebook advertising data, for population level vulnerability assessment. In a nutshell, Facebook provides advertisers with an estimate of how many of its users match certain targeting criteria, e.g., how many Facebook users currently living in Beirut are “living abroad,” aged 18–34, speak Arabic, and primarily use an iOS device. We evaluate the use of such audience estimates to describe the spatial variation in the socioeconomic situation of Syrian refugees across Lebanon. Using data from VASyR as ground truth, we find that iOS device usage explains 90% of the out-of-sample variance in poverty across the Lebanese governorates. However, evaluating predictions at a smaller spatial resolution also indicate limits related to sparsity, as Facebook, for privacy reasons, does not provide audience estimates for fewer than 1,000 users. Furthermore, comparing the population distribution by age and gender of Facebook users with that of the Syrian refugees from VASyR suggests an under-representation of Syrian women on the social media platform. This work adds to growing body of literature demonstrating the value of anonymous and aggregate Facebook advertising data for analysing large-scale humanitarian crises and migration events.},
  archive      = {J_FDATA},
  author       = {Fatehkia, Masoomali and del Villar, Zinnya and Koebe, Till and Letouzé, Emmanuel and Lozano, Andres and Al Feel, Roaa and Mrad, Fouad and Weber, Ingmar},
  doi          = {10.3389/fdata.2022.1033530},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1033530},
  shortjournal = {Front. Big Data},
  title        = {Using facebook advertising data to describe the socio-economic situation of syrian refugees in lebanon},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Auto-GNN: Neural architecture search of graph neural
networks. <em>FDATA</em>, <em>5</em>, 1029307. (<a
href="https://doi.org/10.3389/fdata.2022.1029307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely used in various graph analysis tasks. As the graph characteristics vary significantly in real-world systems, given a specific scenario, the architecture parameters need to be tuned carefully to identify a suitable GNN. Neural architecture search (NAS) has shown its potential in discovering the effective architectures for the learning tasks in image and language modeling. However, the existing NAS algorithms cannot be applied efficiently to GNN search problem because of two facts. First, the large-step exploration in the traditional controller fails to learn the sensitive performance variations with slight architecture modifications in GNNs. Second, the search space is composed of heterogeneous GNNs, which prevents the direct adoption of parameter sharing among them to accelerate the search progress. To tackle the challenges, we propose an automated graph neural networks (AGNN) framework, which aims to find the optimal GNN architecture efficiently. Specifically, a reinforced conservative controller is designed to explore the architecture space with small steps. To accelerate the validation, a novel constrained parameter sharing strategy is presented to regularize the weight transferring among GNNs. It avoids training from scratch and saves the computation time. Experimental results on the benchmark datasets demonstrate that the architecture identified by AGNN achieves the best performance and search efficiency, comparing with existing human-invented models and the traditional search methods.},
  archive      = {J_FDATA},
  author       = {Zhou, Kaixiong and Huang, Xiao and Song, Qingquan and Chen, Rui and Hu, Xia},
  doi          = {10.3389/fdata.2022.1029307},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1029307},
  shortjournal = {Front. Big Data},
  title        = {Auto-GNN: Neural architecture search of graph neural networks},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representing bacteria with unique genomic signatures.
<em>FDATA</em>, <em>5</em>, 1018356. (<a
href="https://doi.org/10.3389/fdata.2022.1018356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying or identifying bacteria in metagenomic samples is an important problem in the analysis of metagenomic data. This task can be computationally expensive since microbial communities usually consist of hundreds to thousands of environmental microbial species. We proposed a new method for representing bacteria in a microbial community using genomic signatures of those bacteria. With respect to the microbial community, the genomic signatures of each bacterium are unique to that bacterium; they do not exist in other bacteria in the community. Further, since the genomic signatures of a bacterium are much smaller than its genome size, the approach allows for a compressed representation of the microbial community. This approach uses a modified Bloom filter to store short k-mers with hash values that are unique to each bacterium. We show that most bacteria in many microbiomes can be represented uniquely using the proposed genomic signatures. This approach paves the way toward new methods for classifying bacteria in metagenomic samples.},
  archive      = {J_FDATA},
  author       = {Pham, Diem-Trang and Phan, Vinhthuy},
  doi          = {10.3389/fdata.2022.1018356},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1018356},
  shortjournal = {Front. Big Data},
  title        = {Representing bacteria with unique genomic signatures},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WINNER: A network biology tool for biomolecular
characterization and prioritization. <em>FDATA</em>, <em>5</em>,
1016606. (<a href="https://doi.org/10.3389/fdata.2022.1016606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and contributionIn network biology, molecular functions can be characterized by network-based inference, or “guilt-by-associations.” PageRank-like tools have been applied in the study of biomolecular interaction networks to obtain further the relative significance of all molecules in the network. However, there is a great deal of inherent noise in widely accessible data sets for gene-to-gene associations or protein-protein interactions. How to develop robust tests to expand, filter, and rank molecular entities in disease-specific networks remains an ad hoc data analysis process.ResultsWe describe a new biomolecular characterization and prioritization tool called Weighted In-Network Node Expansion and Ranking (WINNER). It takes the input of any molecular interaction network data and generates an optionally expanded network with all the nodes ranked according to their relevance to one another in the network. To help users assess the robustness of results, WINNER provides two different types of statistics. The first type is a node-expansion p-value, which helps evaluate the statistical significance of adding “non-seed” molecules to the original biomolecular interaction network consisting of “seed” molecules and molecular interactions. The second type is a node-ranking p-value, which helps evaluate the relative statistical significance of the contribution of each node to the overall network architecture. We validated the robustness of WINNER in ranking top molecules by spiking noises in several network permutation experiments. We have found that node degree–preservation randomization of the gene network produced normally distributed ranking scores, which outperform those made with other gene network randomization techniques. Furthermore, we validated that a more significant proportion of the WINNER-ranked genes was associated with disease biology than existing methods such as PageRank. We demonstrated the performance of WINNER with a few case studies, including Alzheimer&#39;s disease, breast cancer, myocardial infarctions, and Triple negative breast cancer (TNBC). In all these case studies, the expanded and top-ranked genes identified by WINNER reveal disease biology more significantly than those identified by other gene prioritizing software tools, including Ingenuity Pathway Analysis (IPA) and DiAMOND.ConclusionWINNER ranking strongly correlates to other ranking methods when the network covers sufficient node and edge information, indicating a high network quality. WINNER users can use this new tool to robustly evaluate a list of candidate genes, proteins, or metabolites produced from high-throughput biology experiments, as long as there is available gene/protein/metabolic network information.},
  archive      = {J_FDATA},
  author       = {Nguyen, Thanh and Yue, Zongliang and Slominski, Radomir and Welner, Robert and Zhang, Jianyi and Chen, Jake Y.},
  doi          = {10.3389/fdata.2022.1016606},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1016606},
  shortjournal = {Front. Big Data},
  title        = {WINNER: A network biology tool for biomolecular characterization and prioritization},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of the high-level trigger for detecting long-lived
particles at LHCb. <em>FDATA</em>, <em>5</em>, 1008737. (<a
href="https://doi.org/10.3389/fdata.2022.1008737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-lived particles (LLPs) show up in many extensions of the Standard Model, but they are challenging to search for with current detectors, due to their very displaced vertices. This study evaluated the ability of the trigger algorithms used in the Large Hadron Collider beauty (LHCb) experiment to detect long-lived particles and attempted to adapt them to enhance the sensitivity of this experiment to undiscovered long-lived particles. A model with a Higgs portal to a dark sector is tested, and the sensitivity reach is discussed. In the LHCb tracking system, the farthest tracking station from the collision point is the scintillating fiber tracker, the SciFi detector. One of the challenges in the track reconstruction is to deal with the large amount of and combinatorics of hits in the LHCb detector. A dedicated algorithm has been developed to cope with the large data output. When fully implemented, this algorithm would greatly increase the available statistics for any long-lived particle search in the forward region and would additionally improve the sensitivity of analyses dealing with Standard Model particles of large lifetime, such as &lt;mml:math id=&quot;M1&quot; xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mi&gt;K&lt;/mml:mi&gt;&lt;mml:mtext&gt;S&lt;/mml:mtext&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt; or Λ0 hadrons.},
  archive      = {J_FDATA},
  author       = {Calefice, Lukas and Hennequin, Arthur and Henry, Louis and Jashal, Brij and Mendoza, Diego and Oyanguren, Arantza and Sanderswood, Izaac and Sierra, Carlos Vázquez and Zhuo, Jiahui and , Part of LHCb-RTA Collaboration},
  doi          = {10.3389/fdata.2022.1008737},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1008737},
  shortjournal = {Front. Big Data},
  title        = {Effect of the high-level trigger for detecting long-lived particles at LHCb},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping urban socioeconomic inequalities in developing
countries through facebook advertising data. <em>FDATA</em>, <em>5</em>,
1006352. (<a href="https://doi.org/10.3389/fdata.2022.1006352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ending poverty in all its forms everywhere is the number one Sustainable Development Goal of the UN 2030 Agenda. To monitor the progress toward such an ambitious target, reliable, up-to-date and fine-grained measurements of socioeconomic indicators are necessary. When it comes to socioeconomic development, novel digital traces can provide a complementary data source to overcome the limits of traditional data collection methods, which are often not regularly updated and lack adequate spatial resolution. In this study, we collect publicly available and anonymous advertising audience estimates from Facebook to predict socioeconomic conditions of urban residents, at a fine spatial granularity, in four large urban areas: Atlanta (USA), Bogotá (Colombia), Santiago (Chile), and Casablanca (Morocco). We find that behavioral attributes inferred from the Facebook marketing platform can accurately map the socioeconomic status of residential areas within cities, and that predictive performance is comparable in both high and low-resource settings. Our work provides additional evidence of the value of social advertising media data to measure human development and it also shows the limitations in generalizing the use of these data to make predictions across countries.},
  archive      = {J_FDATA},
  author       = {Piaggesi, Simone and Giurgola, Serena and Karsai, Márton and Mejova, Yelena and Panisson, André and Tizzoni, Michele},
  doi          = {10.3389/fdata.2022.1006352},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {1006352},
  shortjournal = {Front. Big Data},
  title        = {Mapping urban socioeconomic inequalities in developing countries through facebook advertising data},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive granularity in tensors: A quest for interpretable
structure. <em>FDATA</em>, <em>5</em>, 929511. (<a
href="https://doi.org/10.3389/fdata.2022.929511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected at very frequent intervals is usually extremely sparse and has no structure that is exploitable by modern tensor decomposition algorithms. Thus, the utility of such tensors is low, in terms of the amount of interpretable and exploitable structure that one can extract from them. In this paper, we introduce the problem of finding a tensor of adaptive aggregated granularity that can be decomposed to reveal meaningful latent concepts (structures) from datasets that, in their original form, are not amenable to tensor analysis. Such datasets fall under the broad category of sparse point processes that evolve over space and/or time. To the best of our knowledge, this is the first work that explores adaptive granularity aggregation in tensors. Furthermore, we formally define the problem and discuss different definitions of “good structure” that are in practice and show that the optimal solution is of prohibitive combinatorial complexity. Subsequently, we propose an efficient and effective greedy algorithm called ICEBREAKER, which follows a number of intuitive decision criteria that locally maximize the “goodness of structure,” resulting in high-quality tensors. We evaluate our method on synthetic, semi-synthetic, and real datasets. In all the cases, our proposed method constructs tensors that have a very high structure quality.},
  archive      = {J_FDATA},
  author       = {Pasricha, Ravdeep S. and Gujral, Ekta and Papalexakis, Evangelos E.},
  doi          = {10.3389/fdata.2022.929511},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {929511},
  shortjournal = {Front. Big Data},
  title        = {Adaptive granularity in tensors: A quest for interpretable structure},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Apparent age prediction from faces: A survey of modern
approaches. <em>FDATA</em>, <em>5</em>, 1025806. (<a
href="https://doi.org/10.3389/fdata.2022.1025806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apparent age estimation via human face image has attracted increased attention due to its numerous real-world applications. Predicting the apparent age has been quite difficult for machines and humans. However, researchers have focused on machine estimation of “age as perceived” to a high level of accuracy. To further improve the performance of apparent age estimation from the facial image, researchers continue to examine different methods to enhance its results further. This paper presents a critical review of the modern approaches and techniques for the apparent age estimation task. We also present a comparative analysis of the performance of some of those approaches on the apparent facial aging benchmark. The study also highlights the strengths and weaknesses of each approach used for apparent age estimation to guide in choosing the appropriate algorithms for future work in the field. The work focuses on the most popular algorithms and those that appear to have been the most successful for apparent age estimation to improve on the existing state-of-the-art results. We based our evaluations on three facial aging datasets, including looking at people (LAP)-2015, LAP-2016, and APPA-REAL, the most popular and publicly available datasets benchmark for apparent age estimation.},
  archive      = {J_FDATA},
  author       = {Agbo-Ajala, Olatunbosun and Viriri, Serestina and Oloko-Oba, Mustapha and Ekundayo, Olufisayo and Heymann, Reolyn},
  doi          = {10.3389/fdata.2022.1025806},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1025806},
  shortjournal = {Front. Big Data},
  title        = {Apparent age prediction from faces: A survey of modern approaches},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-world data mining meets clinical practice: Research
challenges and perspective. <em>FDATA</em>, <em>5</em>, 1021621. (<a
href="https://doi.org/10.3389/fdata.2022.1021621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Big Data Analysis meets healthcare applications, domain-specific challenges and opportunities materialize in all aspects of data science. Advanced statistical methods and Artificial Intelligence (AI) on Electronic Health Records (EHRs) are used both for knowledge discovery purposes and clinical decision support. Such techniques enable the emerging Predictive, Preventative, Personalized, and Participatory Medicine (P4M) paradigm. Working with the Infectious Disease Clinic of the University Hospital of Modena, Italy, we have developed a range of Data–Driven (DD) approaches to solve critical clinical applications using statistics, Machine Learning (ML) and Big Data Analytics on real-world EHR. Here, we describe our perspective on the challenges we encountered. Some are connected to medical data and their sparse, scarce, and unbalanced nature. Others are bound to the application environment, as medical AI tools can affect people&#39;s health and life. For each of these problems, we report some available techniques to tackle them, present examples drawn from our experience, and propose which approaches, in our opinion, could lead to successful real-world, end-to-end implementations.DESY report numberDESY-22-153.},
  archive      = {J_FDATA},
  author       = {Mandreoli, Federica and Ferrari, Davide and Guidetti, Veronica and Motta, Federico and Missier, Paolo},
  doi          = {10.3389/fdata.2022.1021621},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1021621},
  shortjournal = {Front. Big Data},
  title        = {Real-world data mining meets clinical practice: Research challenges and perspective},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Imbalanced ECG signal-based heart disease classification
using ensemble machine learning technique. <em>FDATA</em>, <em>5</em>,
1021518. (<a href="https://doi.org/10.3389/fdata.2022.1021518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine learning (ML)-based classification models are widely utilized for the automated detection of heart diseases (HDs) using various physiological signals such as electrocardiogram (ECG), magnetocardiography (MCG), heart sound (HS), and impedance cardiography (ICG) signals. However, ECG-based HD identification is the most common one used by clinicians. In the current investigation, the ECG records or subjects have been sampled and are used as inputs to the classification model to distinguish between normal and abnormal patients. The study has employed an imbalanced number of ECG samples for training the various classification models. Few ML methods such as support vector machine (SVM), logistic regression (LR), and adaptive boosting (AdaBoost) which have been rarely used for HD detection have been selected. The performance of the developed model has been evaluated in terms of accuracy, F1-score, and area under curve (AUC) values using ECG signals of subjects given in publicly available (PTB-ECG, MIT-BIH) datasets. Ranking of the models has been assigned based on these performance metrics and it is found that the AdaBoost and LR classifiers stand in first and second positions. These two models have been ensembled based on the majority voting principle and the performance measure of this ensemble model has also been determined. It is, in general, observed that the proposed ensemble model demonstrates the best HD detection performance of 0.946, 0.949, and 0.951 for the PTB-ECG dataset and 0.921, 0.926, and 0.950 for the MIT-BIH dataset in terms of accuracy, F1-score, and AUC, respectively. The proposed methodology can also be employed for the classification of HD using ICG, MCG, and HS signals as inputs. Further, the proposed methodology can also be applied to the detection of other diseases.},
  archive      = {J_FDATA},
  author       = {Rath, Adyasha and Mishra, Debahuti and Panda, Ganapati},
  doi          = {10.3389/fdata.2022.1021518},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {1021518},
  shortjournal = {Front. Big Data},
  title        = {Imbalanced ECG signal-based heart disease classification using ensemble machine learning technique},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lethal autonomous weapons systems, revulsion, and respect.
<em>FDATA</em>, <em>5</em>, 991459. (<a
href="https://doi.org/10.3389/fdata.2022.991459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential for the use of artificial intelligence in developing lethal autonomous weapons systems (LAWS) has received a good deal of attention from ethicists. Lines of argument in favor of and against developing and deploying LAWS have already become hardened. In this paper, I examine one strategy for skirting these familiar positions, namely to base an anti-LAWS argument not on claims that LAWS inevitably fail to respect human dignity, but on a different kind of respect, namely respect for public opinion and conventional attitudes (which Robert Sparrow claims are strongly anti-LAWS). My conclusion is that this sort of respect for conventional attitudes does provide some reason for actions and policies, but that it is actually a fairly weak form of respect, that is often override by more direct concerns about respect for humanity or dignity. By doing this, I explain the intuitive force of the claim that one should not disregard public attitudes, but also justify assigning a relatively weak role when other kinds of respect are involved.},
  archive      = {J_FDATA},
  author       = {Dean, Richard},
  doi          = {10.3389/fdata.2022.991459},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {991459},
  shortjournal = {Front. Big Data},
  title        = {Lethal autonomous weapons systems, revulsion, and respect},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The PRC considers military AI ethics: Can autonomy be
trusted? <em>FDATA</em>, <em>5</em>, 991392. (<a
href="https://doi.org/10.3389/fdata.2022.991392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {China&#39;s People&#39;s Liberation Army (PLA) is currently wrestling with the benefits and challenges of using artificial intelligence (AI) to enhance their capabilities. Like many other militaries, a key factor in their analysis is identifying and dealing with the ethical implications of employing AI-enabled systems. Unlike other militaries, however, as the PLA is directly controlled by the Chinese Communist Party (CCP—“the Party”), such considerations are conspicuously influenced by a definition of military ethics that is fundamentally political. This Mini-Review briefly discusses key tenets of PLA military ethics and then investigates how the challenges of military AI ethics are being addressed in publicly-available government and PLA publications. Analysis indicates that, while the PLA is considering AI ethical challenges that are common to all militaries (e.g., accountability), their overriding challenge that they face is “squaring the circle” of benefitting from autonomous AI capabilities while providing the CCP with the absolute control of the PLA that it demands—which is, from the CCP&#39;s perspective, a military ethics consideration. All Chinese translations are my own.},
  archive      = {J_FDATA},
  author       = {Metcalf, Mark},
  doi          = {10.3389/fdata.2022.991392},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {991392},
  shortjournal = {Front. Big Data},
  title        = {The PRC considers military AI ethics: Can autonomy be trusted?},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MedGraph: A semantic biomedical information retrieval
framework using knowledge graph embedding for PubMed. <em>FDATA</em>,
<em>5</em>, 965619. (<a
href="https://doi.org/10.3389/fdata.2022.965619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here we study the semantic search and retrieval problem in biomedical digital libraries. First, we introduce MedGraph, a knowledge graph embedding-based method that provides semantic relevance retrieval and ranking for the biomedical literature indexed in PubMed. Second, we evaluate our approach using PubMed&#39;s Best Match algorithm. Moreover, we compare our method MedGraph to a traditional TF-IDF-based algorithm. Third, we use a dataset extracted from PubMed, including 30 million articles&#39; metadata such as abstracts, author information, citation information, and extracted biological entity mentions. We pull a subset of the dataset to evaluate MedGraph using predefined queries with ground truth ranked results. To our knowledge, this technique has not been explored before in biomedical information retrieval. In addition, our results provide some evidence that semantic approaches to search and relevance in biomedical digital libraries that rely on knowledge graph modeling offer better search relevance results when compared with traditional methods in terms of objective metrics.},
  archive      = {J_FDATA},
  author       = {Ebeid, Islam Akef},
  doi          = {10.3389/fdata.2022.965619},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {965619},
  shortjournal = {Front. Big Data},
  title        = {MedGraph: A semantic biomedical information retrieval framework using knowledge graph embedding for PubMed},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The determinants of investment fraud: A machine learning and
artificial intelligence approach. <em>FDATA</em>, <em>5</em>, 961039.
(<a href="https://doi.org/10.3389/fdata.2022.961039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investment fraud continues to be a severe problem in the Canadian securities industry. This paper aims to employ machine learning algorithms and artificial neural networks (ANN) to predict investment in Canada. Data for this study comes from cases heard by the Investment Industry Regulatory Organization of Canada (IIROC) between June 2008 and December 2019. In total, 406 cases were collected and coded for further analysis. After data cleaning and pre-processing, a total of 385 cases were coded for further analysis. The machine learning algorithms and artificial neural networks were able to predict investment fraud with very good results. In terms of standardized coefficient, the top five features in predicting fraud are offender experience, retired investors, the amount of money lost, the amount of money invested, and the investors&#39; net worth. Machine learning and artificial intelligence have a pivotal role in regulation because they can identify the risks associated with fraud by learning from the data they ingest to survey past practices and come up with the best possible responses to predict fraud. If used correctly, machine learning in the form of regulatory technology can equip regulators with the tools to take corrective actions and make compliance more efficient to safeguard the markets and protect investors from unethical investment advisors.},
  archive      = {J_FDATA},
  author       = {Lokanan, Mark},
  doi          = {10.3389/fdata.2022.961039},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {961039},
  shortjournal = {Front. Big Data},
  title        = {The determinants of investment fraud: A machine learning and artificial intelligence approach},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of convex hull analysis for the evaluation of
data heterogeneity between patient populations of different origin and
implications of hospital bias in downstream machine-learning-based data
processing: A comparison of 4 critical-care patient datasets.
<em>FDATA</em>, <em>5</em>, 603429. (<a
href="https://doi.org/10.3389/fdata.2022.603429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models are developed on a learning dataset covering only a small part of the data of interest. If model predictions are accurate for the learning dataset but fail for unseen data then generalization error is considered high. This problem manifests itself within all major sub-fields of ML but is especially relevant in medical applications. Clinical data structures, patient cohorts, and clinical protocols may be highly biased among hospitals such that sampling of representative learning datasets to learn ML models remains a challenge. As ML models exhibit poor predictive performance over data ranges sparsely or not covered by the learning dataset, in this study, we propose a novel method to assess their generalization capability among different hospitals based on the convex hull (CH) overlap between multivariate datasets. To reduce dimensionality effects, we used a two-step approach. First, CH analysis was applied to find mean CH coverage between each of the two datasets, resulting in an upper bound of the prediction range. Second, 4 types of ML models were trained to classify the origin of a dataset (i.e., from which hospital) and to estimate differences in datasets with respect to underlying distributions. To demonstrate the applicability of our method, we used 4 critical-care patient datasets from different hospitals in Germany and USA. We estimated the similarity of these populations and investigated whether ML models developed on one dataset can be reliably applied to another one. We show that the strongest drop in performance was associated with the poor intersection of convex hulls in the corresponding hospitals&#39; datasets and with a high performance of ML methods for dataset discrimination. Hence, we suggest the application of our pipeline as a first tool to assess the transferability of trained models. We emphasize that datasets from different hospitals represent heterogeneous data sources, and the transfer from one database to another should be performed with utmost care to avoid implications during real-world applications of the developed models. Further research is needed to develop methods for the adaptation of ML models to new hospitals. In addition, more work should be aimed at the creation of gold-standard datasets that are large and diverse with data from varied application sites.},
  archive      = {J_FDATA},
  author       = {Sharafutdinov, Konstantin and Bhat, Jayesh S. and Fritsch, Sebastian Johannes and Nikulina, Kateryna and E. Samadi, Moein and Polzin, Richard and Mayer, Hannah and Marx, Gernot and Bickenbach, Johannes and Schuppert, Andreas},
  doi          = {10.3389/fdata.2022.603429},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {603429},
  shortjournal = {Front. Big Data},
  title        = {Application of convex hull analysis for the evaluation of data heterogeneity between patient populations of different origin and implications of hospital bias in downstream machine-learning-based data processing: A comparison of 4 critical-care patient datasets},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lethal autonomous weapon systems and respect for human
dignity. <em>FDATA</em>, <em>5</em>, 999293. (<a
href="https://doi.org/10.3389/fdata.2022.999293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of the literature concerning the ethics of lethal autonomous weapons systems (LAWS) has focused on the idea of human dignity. The lion&#39;s share of that literature has been devoted to arguing that the use of LAWS is inconsistent with human dignity, so their use should be prohibited. Call this position “Prohibitionism.” Prohibitionists face several major obstacles. First, the concept of human dignity is itself a source of contention and difficult to operationalize. Second, Prohibitionists have struggled to form a consensus about a property P such that (i) all and only instances of LAWS have P and (ii) P is always inconsistent with human dignity. Third, an absolute ban on the use of LAWS seems implausible when they can be used on a limited basis for a good cause. Nevertheless, my main purpose here is to outline an alternative to Prohibitionism and recognize some of its advantages. This alternative, which I will call “Restrictionism,” recognizes the basic intuition at the heart of Prohibitionism - namely, that the use of LAWS raises a concern about human dignity. Moreover, it understands this concern to be rooted in the idea that LAWS can make determinations without human involvement about whom to target for lethal action. However, Restrictionism differs from Prohibitionism in several ways. First, it stipulates a basic standard for respecting human dignity. This basic standard is met by an action in a just war if and only if the action conforms with the following requirements: (i) the action is militarily necessary, (ii) the action involves a distinction between combatants and non-combatants, (iii) noncombatants are not targeted for harm, and (iv) any and all incidental harm to non-combatants is minimized. In short, the use of LAWS meets the standard of basic respect for human dignity if and only if it acts in a way that is functionally isomorphic with how a responsible combatant would act. This approach leaves open the question of whether and under what conditions LAWS can meet the standard of basic respect for human dignity.},
  archive      = {J_FDATA},
  author       = {Kahn, Leonard},
  doi          = {10.3389/fdata.2022.999293},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {999293},
  shortjournal = {Front. Big Data},
  title        = {Lethal autonomous weapon systems and respect for human dignity},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MetaOntology: Toward developing an ontology for the
metaverse. <em>FDATA</em>, <em>5</em>, 998648. (<a
href="https://doi.org/10.3389/fdata.2022.998648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaverse is now perceived as a celebrated future version of the internet. In this new anticipated virtual universe, interconnected digital platforms leveraged by augmented, extended, and virtual realities will elevate users&#39; immersive experiences through multidimensional interactions. In particular, users will be offered a broad spectrum of digital activities within a newly immersive setting mediated by technology. This study aims to design a domain ontology (MetaOntology) for the metaverse to provide an explicit specification of relevant state-of-the-art technologies and infrastructure. A four-step methodological approach is followed to construct the designated ontology. Due to the immaturity of the metaverse, MetaOntology is not intended to furnish a complete outlook on the domain, rather it aims to establish a cornerstone so as to facilitate future efforts in building extant versions of this ontology considering the evolvement of relevant technologies.},
  archive      = {J_FDATA},
  author       = {Abu-Salih, Bilal},
  doi          = {10.3389/fdata.2022.998648},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {998648},
  shortjournal = {Front. Big Data},
  title        = {MetaOntology: Toward developing an ontology for the metaverse},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The role of gender in providing expert advice on cyber
conflict and artificial intelligence for military personnel.
<em>FDATA</em>, <em>5</em>, 992620. (<a
href="https://doi.org/10.3389/fdata.2022.992620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article draws upon original qualitative interview data with Norwegian male and female cyberengineer cadets at the Norwegian Cyber Defense Academy, who could in the future be working with AI-enabled systems in a variety of positions throughout the Norwegian military. The interviews explored how these cadets feel they as cyberengineers will be perceived in their future positions in the military, what challenges they feel they may face, and how gender may play a role in this. Different cyberengineers expressed concern about being able to communicate the cyber domain to their non-technology specialist colleagues due to the increasing complexity of new technologies. Gender appeared to be playing a role in this concern as the women interviewed expressed specific concerns that they feel as women, that they do not fit the stereotype of who is a cyberengineer, while some of the men felt that as cyberengineers they were seen as embodying a nerd masculinity, and that these gendered perceptions has implications for how they feel others perceive their competence levels. The findings from this article highlights gendered hierarchies in the military and the need for military institutions to focus on developing communication skills among those working with cyber operations. As the role of cyber is expected to grow in military operations, cyberengineers will need to find ways of communicating effectively with non-specialists—especially as complex AI-enabled systems are introduced. Finally, this paper argues the need for military institutions to take gender into account for this training and need for gender-sensitive policies.},
  archive      = {J_FDATA},
  author       = {Fisher, Kelly},
  doi          = {10.3389/fdata.2022.992620},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {992620},
  shortjournal = {Front. Big Data},
  title        = {The role of gender in providing expert advice on cyber conflict and artificial intelligence for military personnel},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The comparative ethics of artificial-intelligence methods
for military applications. <em>FDATA</em>, <em>5</em>, 991759. (<a
href="https://doi.org/10.3389/fdata.2022.991759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concerns about the ethics of the use of artificial intelligence by militaries have insufficiently addressed the differences between the methods (algorithms) that such software provides. These methods are discussed and key differences are identified that affect their ethical military use, most notably for lethal autonomous systems. Possible mitigations of ethical problems are discussed such as sharing decision-making with humans, better testing of the software, providing explanations of what is being done, looking for biases, and putting explicit ethics into the software. The best mitigation in many cases is explaining reasoning and calculations to aid transparency.},
  archive      = {J_FDATA},
  author       = {Rowe, Neil C.},
  doi          = {10.3389/fdata.2022.991759},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {991759},
  shortjournal = {Front. Big Data},
  title        = {The comparative ethics of artificial-intelligence methods for military applications},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The ABC recommendations for validation of supervised machine
learning results in biomedical sciences. <em>FDATA</em>, <em>5</em>,
979465. (<a href="https://doi.org/10.3389/fdata.2022.979465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Chicco, Davide and Jurman, Giuseppe},
  doi          = {10.3389/fdata.2022.979465},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {979465},
  shortjournal = {Front. Big Data},
  title        = {The ABC recommendations for validation of supervised machine learning results in biomedical sciences},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using machine learning to improve neutron identification in
water cherenkov detectors. <em>FDATA</em>, <em>5</em>, 978857. (<a
href="https://doi.org/10.3389/fdata.2022.978857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water Cherenkov detectors like Super-Kamiokande, and the next generation Hyper-Kamiokande are adding gadolinium to their water to improve the detection of neutrons. By detecting neutrons in addition to the leptons in neutrino interactions, an improved separation between neutrino and anti-neutrinos, and reduced backgrounds for proton decay searches can be expected. The neutron signal itself is still small and can be confused with muon spallation and other background sources. In this paper, machine learning techniques are employed to optimize the neutron capture detection capability in the new intermediate water Cherenkov detector (IWCD) for Hyper-K. In particular, boosted decision tree (XGBoost), graph convolutional network (GCN), and dynamic graph convolutional neural network (DGCNN) models are developed and benchmarked against a statistical likelihood-based approach, achieving up to a 10% increase in classification accuracy. Characteristic features are also engineered from the datasets and analyzed using SHAP (SHapley Additive exPlanations) to provide insight into the pivotal factors influencing event type outcomes. The dataset used in this research consisted of roughly 1.6 million simulated particle gun events, divided nearly evenly between neutron capture and a background electron source. The current samples used for training are representative only, and more realistic samples will need to be made for the analyses of real data. The current class split is 50/50, but there is expected to be a difference between the classes in the real experiment, and one might consider using resampling techniques to address the issue of serious imbalances in the class distribution in real data if necessary.},
  archive      = {J_FDATA},
  author       = {Jamieson, Blair and Stubbs, Matt and Ramanna, Sheela and Walker, John and Prouse, Nick and Akutsu, Ryosuke and de Perio, Patrick and Fedorko, Wojciech},
  doi          = {10.3389/fdata.2022.978857},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {978857},
  shortjournal = {Front. Big Data},
  title        = {Using machine learning to improve neutron identification in water cherenkov detectors},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The ethics of AI-assisted warfighter enhancement research
and experimentation: Historical perspectives and ethical challenges.
<em>FDATA</em>, <em>5</em>, 978734. (<a
href="https://doi.org/10.3389/fdata.2022.978734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The military applications of AI raise myriad ethical challenges. Critical among them is how AI integrates with human decision making to enhance cognitive performance on the battlefield. AI applications range from augmented reality devices to assist learning and improve training to implantable Brain-Computer Interfaces (BCI) to create bionic “super soldiers.” As these technologies mature, AI-wired warfighters face potential affronts to cognitive liberty, psychological and physiological health risks and obstacles to integrating into military and civil society during their service and upon discharge. Before coming online and operational, however, AI-assisted technologies and neural interfaces require extensive research and human experimentation. Each endeavor raises additional ethical concerns that have been historically ignored thereby leaving military and medical scientists without a cogent ethics protocol for sustainable research. In this way, this paper is a “prequel” to the current debate over enhancement which largely considers neuro-technologies once they are already out the door and operational. To lay the ethics foundation for AI-assisted warfighter enhancement research, we present an historical overview of its technological development followed by a presentation of salient ethics research issues (ICRC, 2006). We begin with a historical survey of AI neuro-enhancement research highlighting the ethics lacunae of its development. We demonstrate the unique ethical problems posed by the convergence of several technologies in the military research setting. Then we address these deficiencies by emphasizing how AI-assisted warfighter enhancement research must pay particular attention to military necessity, and the medical and military cost-benefit tradeoffs of emerging technologies, all attending to the unique status of warfighters as experimental subjects. Finally, our focus is the enhancement of friendly or compatriot warfighters and not, as others have focused, enhancements intended to pacify enemy warfighters.},
  archive      = {J_FDATA},
  author       = {Moreno, Jonathan and Gross, Michael L. and Becker, Jack and Hereth, Blake and Shortland, Neil D. and Evans, Nicholas G.},
  doi          = {10.3389/fdata.2022.978734},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {978734},
  shortjournal = {Front. Big Data},
  title        = {The ethics of AI-assisted warfighter enhancement research and experimentation: Historical perspectives and ethical challenges},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time recommendations for energy-efficient appliance
usage in households. <em>FDATA</em>, <em>5</em>, 972206. (<a
href="https://doi.org/10.3389/fdata.2022.972206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to several studies, the most influencing factor in a household&#39;s energy consumption is user behavior. Changing user behavior to improve energy usage leads to efficient energy consumption, saving money for the consumer and being more friendly for the environment. In this work we propose a framework that aims at assisting households in improving their energy usage by providing real-time recommendations for efficient appliance use. The framework allows for the creation of household-specific and appliance-specific energy consumption profiles by analyzing appliance usage patterns. Based on the household profile and the actual electricity use, real-time recommendations notify users on the appliances that can be switched off in order to reduce consumption. For instance, if a consumer forgets their A/C on at a time that it is usually off (e.g., when there is no one at home), the system will detect this as an outlier and notify the consumer. In the ideal scenario, a household has a smart meter monitoring system installed, that records energy consumption at the appliance level. This is also reflected in the datasets available for evaluating such systems. However, in the general case, the household may only have one main meter reading. In this case, non-intrusive load monitoring (NILM) techniques, which monitor a house&#39;s energy consumption using only one meter, and data mining algorithms that disaggregate the consumption into appliance level, can be employed. In this paper, we propose an end-to-end solution to this problem, starting with the energy disaggregation process, and the creation of user profiles that are then fed to the pattern mining and recommendation process, that through an intuitive UI allows users to further refine their energy consumption preferences and set goals. We employ the UK-DALE (UK Domestic Appliance-Level Electricity) dataset for our experimental evaluations and the proof-of-concept implementation. The results show that the proposed framework accurately captures the energy consumption profiles of each household and thus the generated recommendations are matching the actual household energy habits and can help reduce their energy consumption by 2–17%.},
  archive      = {J_FDATA},
  author       = {Eirinaki, Magdalini and Varlamis, Iraklis and Dahihande, Janhavi and Jaiswal, Akshay and Pagar, Akshay Anil and Thakare, Ajinkya},
  doi          = {10.3389/fdata.2022.972206},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {972206},
  shortjournal = {Front. Big Data},
  title        = {Real-time recommendations for energy-efficient appliance usage in households},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Observation-based assessment of secondary water effects on
seasonal vegetation decay across africa. <em>FDATA</em>, <em>5</em>,
967477. (<a href="https://doi.org/10.3389/fdata.2022.967477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local studies and modeling experiments suggest that shallow groundwater and lateral redistribution of soil moisture, together with soil properties, can be highly important secondary water sources for vegetation in water-limited ecosystems. However, there is a lack of observation-based studies of these terrain-associated secondary water effects on vegetation over large spatial domains. Here, we quantify the role of terrain properties on the spatial variations of dry season vegetation decay rate across Africa obtained from geostationary satellite acquisitions to assess the large-scale relevance of secondary water effects. We use machine learning based attribution to identify where and under which conditions terrain properties related to topography, water table depth, and soil hydraulic properties influence the rate of vegetation decay. Over the study domain, the machine learning model attributes about one-third of the spatial variations of vegetation decay rates to terrain properties, which is roughly equally split between direct terrain effects and interaction effects with climate and vegetation variables. The importance of secondary water effects increases with increasing topographic variability, shallower groundwater levels, and the propensity to capillary rise given by soil properties. In regions with favorable terrain properties, more than 60% of the variations in the decay rate of vegetation are attributed to terrain properties, highlighting the importance of secondary water effects on vegetation in Africa. Our findings provide an empirical assessment of the importance of local-scale secondary water effects on vegetation over Africa and help to improve hydrological and vegetation models for the challenge of bridging processes across spatial scales.},
  archive      = {J_FDATA},
  author       = {Küçük, Çağlar and Koirala, Sujan and Carvalhais, Nuno and Miralles, Diego G. and Reichstein, Markus and Jung, Martin},
  doi          = {10.3389/fdata.2022.967477},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {967477},
  shortjournal = {Front. Big Data},
  title        = {Observation-based assessment of secondary water effects on seasonal vegetation decay across africa},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective cluster based bidding algorithm for
e-commerce search engine marketing system. <em>FDATA</em>, <em>5</em>,
966982. (<a href="https://doi.org/10.3389/fdata.2022.966982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search engine marketing (SEM) is an important channel for the success of e-commerce. With the increasing scale of catalog items, designing an efficient modern industrial-level bidding system usually requires overcoming the following hurdles: 1. the relevant bidding features are of high sparsity, preventing an accurate prediction of the performances of many ads. 2. the large volume of bidding requests induces a significant computation burden to offline and online serving. In this article, we introduce an end-to-end structure of a multi-objective bidding system for search engine marketing for Walmart e-commerce, which successfully handles tens of millions of bids each day. The system deals with multiple business demands by constructing an optimization model targeting a mixture of metrics. Moreover, the system extracts the vector representations of ads via the Transformer model. It leverages their geometric relation to building collaborative bidding predictions via clustering to address performance features&#39; sparsity issues. We provide theoretical and numerical analyzes to discuss how we find the proposed system as a production-efficient solution.},
  archive      = {J_FDATA},
  author       = {Jie, Cheng and Wang, Zigeng and Xu, Da and Shen, Wei},
  doi          = {10.3389/fdata.2022.966982},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {966982},
  shortjournal = {Front. Big Data},
  title        = {Multi-objective cluster based bidding algorithm for E-commerce search engine marketing system},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A concentric circles view of health data relations
facilitates understanding of sociotechnical challenges for learning
health systems and the role of federated data networks. <em>FDATA</em>,
<em>5</em>, 945739. (<a
href="https://doi.org/10.3389/fdata.2022.945739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to use clinical and research data at scale is central to hopes for data-driven medicine. However, in using such data researchers often encounter hurdles–both technical, such as differing data security requirements, and social, such as the terms of informed consent, legal requirements and patient and public trust. Federated or distributed data networks have been proposed and adopted in response to these hurdles. However, to date there has been little consideration of how FDNs respond to both technical and social constraints on data use. In this Perspective we propose an approach to thinking about data in terms that make it easier to navigate the health data space and understand the value of differing approaches to data collection, storage and sharing. We set out a socio-technical model of data systems that we call the “Concentric Circles View” (CCV) of data-relationships. The aim is to enable a consistent understanding of the fit between the local relationships within which data are produced and the extended socio-technical systems that enable their use. The paper suggests this model can help understand and tackle challenges associated with the use of real-world data in the health setting. We use the model to understand not only how but why federated networks may be well placed to address emerging issues and adapt to the evolving needs of health research for patient benefit. We conclude that the CCV provides a useful model with broader application in mapping, understanding, and tackling the major challenges associated with using real world data in the health setting.},
  archive      = {J_FDATA},
  author       = {Milne, Richard and Sheehan, Mark and Barnes, Brendan and Kapper, Janek and Lea, Nathan and N&#39;Dow, James and Singh, Gurparkash and Martín-Uranga, Amelia and Hughes, Nigel},
  doi          = {10.3389/fdata.2022.945739},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {945739},
  shortjournal = {Front. Big Data},
  title        = {A concentric circles view of health data relations facilitates understanding of sociotechnical challenges for learning health systems and the role of federated data networks},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel interpretable machine learning algorithm to identify
optimal parameter space for cancer growth. <em>FDATA</em>, <em>5</em>,
941451. (<a href="https://doi.org/10.3389/fdata.2022.941451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen an increase in the application of machine learning to the analysis of physical and biological systems, including cancer progression. A fundamental downside to these tools is that their complexity and nonlinearity makes it almost impossible to establish a deterministic, a priori relationship between their input and output, and thus their predictions are not wholly accountable. We begin with a series of proofs establishing that this holds even for the simplest possible model of a neural network; the effects of specific loss functions are explored more fully in Appendices. We return to first principles and consider how to construct a physics-inspired model of tumor growth without resorting to stochastic gradient descent or artificial nonlinearities. We derive an algorithm which explores the space of possible parameters in a model of tumor growth and identifies candidate equations much faster than a simulated annealing approach. We test this algorithm on synthetic tumor-growth trajectories and show that it can efficiently and reliably narrow down the area of parameter space where the correct values are located. This approach has the potential to greatly improve the speed and reliability with which patient-specific models of cancer growth can be identified in a clinical setting.},
  archive      = {J_FDATA},
  author       = {Coggan, Helena and Andres Terre, Helena and Liò, Pietro},
  doi          = {10.3389/fdata.2022.941451},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {941451},
  shortjournal = {Front. Big Data},
  title        = {A novel interpretable machine learning algorithm to identify optimal parameter space for cancer growth},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Broadcast your gender.” A comparison of four text-based
classification methods of german YouTube channels. <em>FDATA</em>,
<em>5</em>, 908636. (<a
href="https://doi.org/10.3389/fdata.2022.908636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms provide a large array of behavioral data relevant to social scientific research. However, key information such as sociodemographic characteristics of agents are often missing. This paper aims to compare four methods of classifying social attributes from text. Specifically, we are interested in estimating the gender of German social media creators. By using the example of a random sample of 200 YouTube channels, we compare several classification methods, namely (1) a survey among university staff, (2) a name dictionary method with the World Gender Name Dictionary as a reference list, (3) an algorithmic approach using the website gender-api.com, and (4) a Multinomial Naïve Bayes (MNB) machine learning technique. These different methods identify gender attributes based on YouTube channel names and descriptions in German but are adaptable to other languages. Our contribution will evaluate the share of identifiable channels, accuracy and meaningfulness of classification, as well as limits and benefits of each approach. We aim to address methodological challenges connected to classifying gender attributes for YouTube channels as well as related to reinforcing stereotypes and ethical implications.},
  archive      = {J_FDATA},
  author       = {Seewann, Lena and Verwiebe, Roland and Buder, Claudia and Fritsch, Nina-Sophie},
  doi          = {10.3389/fdata.2022.908636},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {908636},
  shortjournal = {Front. Big Data},
  title        = {“Broadcast your gender.” a comparison of four text-based classification methods of german YouTube channels},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of multiple testing procedures for identifying
relevant comorbidities, from a large set, in traumatic brain injury for
research applications utilizing big health-administrative data.
<em>FDATA</em>, <em>5</em>, 793606. (<a
href="https://doi.org/10.3389/fdata.2022.793606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundMultiple testing procedures (MTP) are gaining increasing popularity in various fields of biostatistics, especially in statistical genetics. However, in injury surveillance research utilizing the growing amount and complexity of health-administrative data encoded in the International Statistical Classification of Diseases and Related Health Problems 10th Revision (ICD-10), few studies involve MTP and discuss their applications and challenges.ObjectiveWe aimed to apply MTP in the population-wide context of comorbidity preceding traumatic brain injury (TBI), one of the most disabling injuries, to find a subset of comorbidity that can be targeted in primary injury prevention.MethodsIn total, 2,600 ICD-10 codes were used to assess the associations between TBI and comorbidity, with 235,003 TBI patients, on a matched data set of patients without TBI. McNemar tests were conducted on each 2,600 ICD-10 code, and appropriate multiple testing adjustments were applied using the Benjamini-Yekutieli procedure. To study the magnitude and direction of associations, odds ratios with 95% confidence intervals were constructed.ResultsBenjamini-Yekutieli procedure captured 684 ICD-10 codes, out of 2,600, as codes positively associated with a TBI event, reducing the effective number of codes for subsequent analysis and comprehension.ConclusionOur results illustrate the utility of MTP for data mining and dimension reduction in TBI research utilizing big health-administrative data to support injury surveillance research and generate ideas for injury prevention.},
  archive      = {J_FDATA},
  author       = {Jana, Sayantee and Sutton, Mitchell and Mollayeva, Tatyana and Chan, Vincy and Colantonio, Angela and Escobar, Michael David},
  doi          = {10.3389/fdata.2022.793606},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {793606},
  shortjournal = {Front. Big Data},
  title        = {Application of multiple testing procedures for identifying relevant comorbidities, from a large set, in traumatic brain injury for research applications utilizing big health-administrative data},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Past efforts in determining suitable normalization methods
for multi-criteria decision-making: A short survey. <em>FDATA</em>,
<em>5</em>, 990699. (<a
href="https://doi.org/10.3389/fdata.2022.990699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of a multi-criteria decision-making (MCDM) technique mostly begins with normalizing the incommensurable data values in the decision matrix. Numerous normalization methods are available in the literature and applying different normalization methods to an MCDM technique is proven to deliver varying results. As such, selecting suitable normalization methods for an MCDM technique has emerged as an intriguing research topic, especially with the advent of big data. Several efforts have been made to compare the suitability of various normalization methods, but regrettably, no paper provides an updated review of these crucial efforts. This study, therefore, aimed to trace articles reporting such efforts and review them based on the following three perspectives: (1) the normalization methods considered, (2) the MCDM methods considered, and (3) the comparison metrics used to determine the suitable normalization methods. The relevant articles were extracted with the aid of Google Scholar using the keywords of “normalization” and “MCDM,” and Tableau software was used to analyze further the data gathered through the articles. A total of five limitations were uncovered based on the current state of literature, and potential future works to address those limitations were offered. This paper is the first to compile and review the previous investigations that compared and determined the ideal normalization methods for an MCDM technique.},
  archive      = {J_FDATA},
  author       = {Krishnan, Anath Rau},
  doi          = {10.3389/fdata.2022.990699},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {990699},
  shortjournal = {Front. Big Data},
  title        = {Past efforts in determining suitable normalization methods for multi-criteria decision-making: A short survey},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lessons learned: A neuroimaging research center’s transition
to open and reproducible science. <em>FDATA</em>, <em>5</em>, 988084.
(<a href="https://doi.org/10.3389/fdata.2022.988084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human functional neuroimaging has evolved dramatically in recent years, driven by increased technical complexity and emerging evidence that functional neuroimaging findings are not generally reproducible. In response to these trends, neuroimaging scientists have developed principles, practices, and tools to both manage this complexity as well as to enhance the rigor and reproducibility of neuroimaging science. We group these best practices under four categories: experiment pre-registration, FAIR data principles, reproducible neuroimaging analyses, and open science. While there is growing recognition of the need to implement these best practices there exists little practical guidance of how to accomplish this goal. In this work, we describe lessons learned from efforts to adopt these best practices within the Brain Imaging Research Center at the University of Arkansas for Medical Sciences over 4 years (July 2018–May 2022). We provide a brief summary of the four categories of best practices. We then describe our center&#39;s scientific workflow (from hypothesis formulation to result reporting) and detail how each element of this workflow maps onto these four categories. We also provide specific examples of practices or tools that support this mapping process. Finally, we offer a roadmap for the stepwise adoption of these practices, providing recommendations of why and what to do as well as a summary of cost-benefit tradeoffs for each step of the transition.},
  archive      = {J_FDATA},
  author       = {Bush, Keith A. and Calvert, Maegan L. and Kilts, Clinton D.},
  doi          = {10.3389/fdata.2022.988084},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {988084},
  shortjournal = {Front. Big Data},
  title        = {Lessons learned: A neuroimaging research center&#39;s transition to open and reproducible science},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multimodal clinical data resource for personalized risk
assessment of sudden unexpected death in epilepsy. <em>FDATA</em>,
<em>5</em>, 965715. (<a
href="https://doi.org/10.3389/fdata.2022.965715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy affects ~2–3 million individuals in the United States, a third of whom have uncontrolled seizures. Sudden unexpected death in epilepsy (SUDEP) is a catastrophic and fatal complication of poorly controlled epilepsy and is the primary cause of mortality in such patients. Despite its huge public health impact, with a ~1/1,000 incidence rate in persons with epilepsy, it is an uncommon enough phenomenon to require multi-center efforts for well-powered studies. We developed the Multimodal SUDEP Data Resource (MSDR), a comprehensive system for sharing multimodal epilepsy data in the NIH funded Center for SUDEP Research. The MSDR aims at accelerating research to address critical questions about personalized risk assessment of SUDEP. We used a metadata-guided approach, with a set of common epilepsy-specific terms enforcing uniform semantic interpretation of data elements across three main components: (1) multi-site annotated datasets; (2) user interfaces for capturing, managing, and accessing data; and (3) computational approaches for the analysis of multimodal clinical data. We incorporated the process for managing dataset-specific data use agreements, evidence of Institutional Review Board review, and the corresponding access control in the MSDR web portal. The metadata-guided approach facilitates structural and semantic interoperability, ultimately leading to enhanced data reusability and scientific rigor. MSDR prospectively integrated and curated epilepsy patient data from seven institutions, and it currently contains data on 2,739 subjects and 10,685 multimodal clinical data files with different data formats. In total, 55 users registered in the current MSDR data repository, and 6 projects have been funded to apply MSDR in epilepsy research, including three R01 projects and three R21 projects.},
  archive      = {J_FDATA},
  author       = {Li, Xiaojin and Tao, Shiqiang and Lhatoo, Samden D. and Cui, Licong and Huang, Yan and Hampson, Johnson P. and Zhang, Guo-Qiang},
  doi          = {10.3389/fdata.2022.965715},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {965715},
  shortjournal = {Front. Big Data},
  title        = {A multimodal clinical data resource for personalized risk assessment of sudden unexpected death in epilepsy},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward data lakes as central building blocks for data
management and analysis. <em>FDATA</em>, <em>5</em>, 945720. (<a
href="https://doi.org/10.3389/fdata.2022.945720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data lakes are a fundamental building block for many industrial data analysis solutions and becoming increasingly popular in research. Often associated with big data use cases, data lakes are, for example, used as central data management systems of research institutions or as the core entity of machine learning pipelines. The basic underlying idea of retaining data in its native format within a data lake facilitates a large range of use cases and improves data reusability, especially when compared to the schema-on-write approach applied in data warehouses, where data is transformed prior to the actual storage to fit a predefined schema. Storing such massive amounts of raw data, however, has its very own challenges, spanning from the general data modeling, and indexing for concise querying to the integration of suitable and scalable compute capabilities. In this contribution, influential papers of the last decade have been selected to provide a comprehensive overview of developments and obtained results. The papers are analyzed with regard to the applicability of their input to data lakes that serve as central data management systems of research institutions. To achieve this, contributions to data lake architectures, metadata models, data provenance, workflow support, and FAIR principles are investigated. Last, but not least, these capabilities are mapped onto the requirements of two common research personae to identify open challenges. With that, potential research topics are determined, which have to be tackled toward the applicability of data lakes as central building blocks for research data management.},
  archive      = {J_FDATA},
  author       = {Wieder, Philipp and Nolte, Hendrik},
  doi          = {10.3389/fdata.2022.945720},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {945720},
  shortjournal = {Front. Big Data},
  title        = {Toward data lakes as central building blocks for data management and analysis},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Zero party data between hype and hope. <em>FDATA</em>,
<em>5</em>, 943372. (<a
href="https://doi.org/10.3389/fdata.2022.943372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero Party Data (ZPD) is a hot topic in the context of privacy-aware personalization, as the exponential growth of consumer data collected by retailers has made safeguarding data privacy a key priority. Articles arguing for the value of ZPD to improve personalization and engender consumer trust have appeared in the popular press, in business magazines as well as in academic journals. Advocates of ZDP argue that instead of inferring what customers want, retailers can simply ask them. Provided that the value exchange is clear, customers will willingly share data such as purchase intentions and preferences to improve personalization and help retailers create a picture of who they are. While the rise of ZPD is a welcome development, this paper takes issue with the claim that ZPD is necessarily accurate as it comes directly from the customer. This view is at odds with established conclusions from decades of research in the social and cognitive sciences, showing that self reports can be influenced by the instrument and that people have limited insight into the factors underlying their behavior. This paper argues that while ZDP disclosures are an important tool for retailers, it is critical to carefully understand their limitations as well. The paper also provides a catalog of biases for identifying potential problems in survey design to help practitioners collect more accurate data.},
  archive      = {J_FDATA},
  author       = {Polonioli, Andrea},
  doi          = {10.3389/fdata.2022.943372},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {943372},
  shortjournal = {Front. Big Data},
  title        = {Zero party data between hype and hope},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do we behave differently on twitter and facebook: Multi-view
social network user personality profiling for content recommendation.
<em>FDATA</em>, <em>5</em>, 931206. (<a
href="https://doi.org/10.3389/fdata.2022.931206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human personality traits are key drivers behind our decision making, influencing our lives on a daily basis. Inference of personality traits, such as the Myers-Briggs personality type, as well as an understanding of dependencies between personality traits and user behavior on various social media platforms, is of crucial importance to modern research and industry applications such as recommender systems. The emergence of diverse and cross-purpose social media avenues makes it possible to perform user personality profiling automatically and efficiently based on data represented across multiple data modalities. However, research efforts on personality profiling from multi-source multi-modal social media data are relatively sparse; the impact of different social network data on profiling performance and of personality traits on applications such as recommender systems is yet to be evaluated. Furthermore, large-scale datasets are also lacking in the research community. To fill these gaps, in this work we develop a novel multi-view fusion framework PERS that infers Myers-Briggs personality type indicators. We evaluate the results not just across data modalities but also across different social networks, and also evaluate the impact of inferred personality traits on recommender systems. Our experimental results demonstrate that PERS is able to learn from multi-view data for personality profiling by efficiently leveraging highly varied data from diverse social multimedia sources. Furthermore, we demonstrate that inferred personality traits can be beneficial to other industry applications. Among other results, we show that people tend to reveal multiple facets of their personality in different social media avenues. We also release a social multimedia dataset in order to facilitate further research on this direction.},
  archive      = {J_FDATA},
  author       = {Yang, Qi and Farseev, Aleksandr and Nikolenko, Sergey and Filchenkov, Andrey},
  doi          = {10.3389/fdata.2022.931206},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {931206},
  shortjournal = {Front. Big Data},
  title        = {Do we behave differently on twitter and facebook: Multi-view social network user personality profiling for content recommendation},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Repeatable enhancement of healthcare data with social
determinants of health. <em>FDATA</em>, <em>5</em>, 894598. (<a
href="https://doi.org/10.3389/fdata.2022.894598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundSocial and behavioral aspects of our lives significantly impact our health, yet minimal social determinants of health (SDOH) data elements are collected in the healthcare system.MethodsIn this proof-of-concept study we developed a repeatable SDOH enrichment and integration process to incorporate dynamically evolving SDOH domain concepts from consumers into clinical data. This process included SDOH mapping, linking compiled consumer data to patient records in Electronic Health Records, data quality analysis and preprocessing, and storage.ResultsConsumer compilers data coverage ranged from ~90 to ~54% and the percentage match rate between compilers was between ~21 and 64%. Our preliminary analysis showed that apart from demographic factors, several SDOH factors like home-ownership, marital-status, presence of children, number of members per household, economic stability and education were significantly different between the COVID-19 positive and negative patient groups while estimated family-income and home market-value were not.ConclusionOur preliminary analysis shows commercial consumer data can be a viable source of SDOH factor at an individual-level for clinical data thus providing a path for clinicians to improve patient treatment and care.},
  archive      = {J_FDATA},
  author       = {Greer, Melody L. and Zayas, Cilia E. and Bhattacharyya, Sudeepa},
  doi          = {10.3389/fdata.2022.894598},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {894598},
  shortjournal = {Front. Big Data},
  title        = {Repeatable enhancement of healthcare data with social determinants of health},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The semi-automatic classification of an open-ended question
on panel survey motivation and its application in attrition analysis.
<em>FDATA</em>, <em>5</em>, 880554. (<a
href="https://doi.org/10.3389/fdata.2022.880554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we demonstrate how supervised learning can extract interpretable survey motivation measurements from a large number of responses to an open-ended question. We manually coded a subsample of 5,000 responses to an open-ended question on survey motivation from the GESIS Panel (25,000 responses in total); we utilized supervised machine learning to classify the remaining responses. We can demonstrate that the responses on survey motivation in the GESIS Panel are particularly well suited for automated classification, since they are mostly one-dimensional. The evaluation of the test set also indicates very good overall performance. We present the pre-processing steps and methods we used for our data, and by discussing other popular options that might be more suitable in other cases, we also generalize beyond our use case. We also discuss various minor problems, such as a necessary spelling correction. Finally, we can showcase the analytic potential of the resulting categorization of panelists&#39; motivation through an event history analysis of panel dropout. The analytical results allow a close look at respondents&#39; motivations: they span a wide range, from the urge to help to interest in questions or the incentive and the wish to influence those in power through their participation. We conclude our paper by discussing the re-usability of the hand-coded responses for other surveys, including similar open questions to the GESIS Panel question.},
  archive      = {J_FDATA},
  author       = {Haensch, Anna-Carolina and Weiß, Bernd and Steins, Patricia and Chyrva, Priscilla and Bitz, Katja},
  doi          = {10.3389/fdata.2022.880554},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {880554},
  shortjournal = {Front. Big Data},
  title        = {The semi-automatic classification of an open-ended question on panel survey motivation and its application in attrition analysis},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SemNet: Learning semantic attributes for human activity
recognition with deep belief networks. <em>FDATA</em>, <em>5</em>,
879389. (<a href="https://doi.org/10.3389/fdata.2022.879389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) is a prominent application in mobile computing and Internet of Things (IoT) that aims to detect human activities based on multimodal sensor signals generated as a result of diverse body movements. Human physical activities are typically composed of simple actions (such as “arm up”, “arm down”, “arm curl”, etc.), referred to as semantic features. Such abstract semantic features, in contrast to high-level activities (“walking”, “sitting”, etc.) and low-level signals (raw sensor readings), can be developed manually to assist activity recognition. Although effective, this manual approach relies heavily on human domain expertise and is not scalable. In this paper, we address this limitation by proposing a machine learning method, SemNet, based on deep belief networks. SemNet automatically constructs semantic features representative of the axial bodily movements. Experimental results show that SemNet outperforms baseline approaches and is capable of learning features that highly correlate with manually defined semantic attributes. Furthermore, our experiments using a different model, namely deep convolutional LSTM, on household activities illustrate the broader applicability of semantic attribute interpretation to diverse deep neural network approaches. These empirical results not only demonstrate that such a deep learning technique is semantically meaningful and superior to its handcrafted counterpart, but also provides a better understanding of the deep learning methods that are used for Human Activity Recognition.},
  archive      = {J_FDATA},
  author       = {Venkatachalam, Shanmuga and Nair, Harideep and Zeng, Ming and Tan, Cathy Shunwen and Mengshoel, Ole J. and Shen, John Paul},
  doi          = {10.3389/fdata.2022.879389},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {879389},
  shortjournal = {Front. Big Data},
  title        = {SemNet: Learning semantic attributes for human activity recognition with deep belief networks},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Scalable network generation &amp; analysis.
<em>FDATA</em>, <em>5</em>, 984256. (<a
href="https://doi.org/10.3389/fdata.2022.984256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Giabbanelli, Philippe J. and Swarup, Samarth and Lambiotte, Renaud and Mangioni, Giuseppe},
  doi          = {10.3389/fdata.2022.984256},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {984256},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Scalable network generation &amp;amp; analysis},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Big data and machine learning in cancer
theranostics. <em>FDATA</em>, <em>5</em>, 972726. (<a
href="https://doi.org/10.3389/fdata.2022.972726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Silva, Fabricio Alves Barbosa da and Tuszynski, Jack Adam and Nakaya, Helder and Paller, Channing J.},
  doi          = {10.3389/fdata.2022.972726},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {972726},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Big data and machine learning in cancer theranostics},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple chronic diseases associated with tooth loss among
the US adult population. <em>FDATA</em>, <em>5</em>, 932618. (<a
href="https://doi.org/10.3389/fdata.2022.932618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundHalf of US adults aged 20–64 years have lost at least one permanent tooth; one in six adults aged 65 and over in the USA is edentulous. Tooth loss and edentulism interfere with nutritional intake and quality of life. Although selected chronic diseases (e.g., diabetes) have been identified as possible risk factors for tooth loss, data on multiple chronic diseases and on having two or more concurrent chronic diseases (multimorbidity) in relation to tooth loss are lacking. Therefore, this study aimed to assess the association between multiple chronic diseases, multimorbidity, and tooth loss in US adults.MethodsWe performed a secondary data analysis using the US 2012 Behavioral Risk Factor Surveillance System (BRFSS), a national cross-sectional telephone survey studying health conditions and health behaviors among US adults (≥18 years) who are non-institutionalized residents. Variables were derived from the BRFSS Standard Core Questionnaire. Descriptive analysis including means, standard deviations (SDs), and percentages was calculated. Sample weights were applied. The stepwise multinomial logistic regression method was used to examine the relationship between several chronic diseases and tooth loss. Separate multinomial logistic regression models were used to examine the relationship between multimorbidity and tooth loss among all adults aged more than 18 years, adults aged 18–64 years, and adults aged more than 65 years, respectively.ResultsAmong the samples (n = 471,107, mean age 55 years, 60% female), 55% reported losing no tooth loss, 30% reported losing one to five teeth, 10% reported losing six or more but not all teeth, and 5% reported losing all teeth. After adjusting for demographic characteristics, socioeconomic status, smoking, BMI, and dental care, chronic diseases that were associated with edentulism were chronic obstructive pulmonary disease (COPD) [adjusted risk ratio (adj. RR) 2.18, 95% confidence interval (CI) 2.08–2.29]; diabetes (adj. RR 1.49, 95% CI 1.44–1.56); arthritis (adj. RR 1.49, 95% CI 1.44–1.54); cardiovascular disease (adj. RR 1.38, 95% CI 1.30–1.45); stroke (adj. RR 1.31, 95% CI 1.24–1.40); kidney disease (adj. RR 1.16, 95% CI 1.08–1.25); cancer (adj. RR 1.05, 95% CI 1.01–1.11); and asthma (adj. RR 1.07, 95% CI 1.02–1.12). For those who reported losing six or more teeth, the association remained significant for all the chronic diseases mentioned, albeit the magnitude of association appeared to be comparative or smaller. In addition, adults with multimorbidity were more likely to have tooth loss (loss of one to five teeth: adj. RR 1.17, 95% CI 1.14–1.19; loss of six or more teeth: adj. RR 1.78, 95% CI 1.73–1.82; edentulous: adj. RR 2.03, 95% CI 1.96–2.10).ConclusionsMultiple chronic diseases were associated with edentulism and tooth loss. People with multimorbidity are more likely to be edentulous than those with one or no chronic disease. The findings from this study will help to identify populations at increased risk for oral problems and nutritional deficits, thus the assessment of oral health should be evaluated further as an important component of chronic illness care.},
  archive      = {J_FDATA},
  author       = {Zhang, Yuqing and Leveille, Suzanne G. and Shi, Ling},
  doi          = {10.3389/fdata.2022.932618},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {932618},
  shortjournal = {Front. Big Data},
  title        = {Multiple chronic diseases associated with tooth loss among the US adult population},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fairness in music recommender systems: A
stakeholder-centered mini review. <em>FDATA</em>, <em>5</em>, 913608.
(<a href="https://doi.org/10.3389/fdata.2022.913608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of recommender systems highly impacts both music streaming platform users and the artists providing music. As fairness is a fundamental value of human life, there is increasing pressure for these algorithmic decision-making processes to be fair as well. However, many factors make recommender systems prone to biases, resulting in unfair outcomes. Furthermore, several stakeholders are involved, who may all have distinct needs requiring different fairness considerations. While there is an increasing interest in research on recommender system fairness in general, the music domain has received relatively little attention. This mini review, therefore, outlines current literature on music recommender system fairness from the perspective of each relevant stakeholder and the stakeholders combined. For instance, various works address gender fairness: one line of research compares differences in recommendation quality across user gender groups, and another line focuses on the imbalanced representation of artist gender in the recommendations. In addition to gender, popularity bias is frequently addressed; yet, primarily from the user perspective and rarely addressing how it impacts the representation of artists. Overall, this narrative literature review shows that the large majority of works analyze the current situation of fairness in music recommender systems, whereas only a few works propose approaches to improve it. This is, thus, a promising direction for future research.},
  archive      = {J_FDATA},
  author       = {Dinnissen, Karlijn and Bauer, Christine},
  doi          = {10.3389/fdata.2022.913608},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {913608},
  shortjournal = {Front. Big Data},
  title        = {Fairness in music recommender systems: A stakeholder-centered mini review},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Two-stage voice application recommender system for unhandled
utterances in intelligent personal assistant. <em>FDATA</em>,
<em>5</em>, 898050. (<a
href="https://doi.org/10.3389/fdata.2022.898050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent personal assistants (IPA) enable voice applications that facilitate people&#39;s daily tasks. However, due to the complexity and ambiguity of voice requests, some requests may not be handled properly by the standard natural language understanding (NLU) component. In such cases, a simple reply like “Sorry, I don&#39;t know” hurts the user&#39;s experience and limits the functionality of IPA. In this paper, we propose a two-stage shortlister-reranker recommender system to match third-party voice applications (skills) to unhandled utterances. In this approach, a skill shortlister is proposed to retrieve candidate skills from the skill catalog by calculating both lexical and semantic similarity between skills and user requests. We also illustrate how to build a new system by using observed data collected from a baseline rule-based system, and how the exposure biases can generate discrepancy between offline and human metrics. Lastly, we present two relabeling methods that can handle the incomplete ground truth, and mitigate exposure bias. We demonstrate the effectiveness of our proposed system through extensive offline experiments. Furthermore, we present online A/B testing results that show a significant boost on user experience satisfaction.},
  archive      = {J_FDATA},
  author       = {Xiao, Wei and Hu, Qian and Mohamed, Thahir and Gao, Zheng and Gao, Xibin and Arava, Radhika and AbdelHady, Mohamed},
  doi          = {10.3389/fdata.2022.898050},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {898050},
  shortjournal = {Front. Big Data},
  title        = {Two-stage voice application recommender system for unhandled utterances in intelligent personal assistant},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network-informed constrained divisive pooled testing
assignments. <em>FDATA</em>, <em>5</em>, 893760. (<a
href="https://doi.org/10.3389/fdata.2022.893760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent universal testing in a finite population is an effective approach to preventing large infectious disease outbreaks. Yet when the target group has many constituents, this strategy can be cost prohibitive. One approach to alleviate the resource burden is to group multiple individual tests into one unit in order to determine if further tests at the individual level are necessary. This approach, referred to as a group testing or pooled testing, has received much attention in finding the minimum cost pooling strategy. Existing approaches, however, assume either independence or very simple dependence structures between individuals. This assumption ignores the fact that in the context of infectious diseases there is an underlying transmission network that connects individuals. We develop a constrained divisive hierarchical clustering algorithm that assigns individuals to pools based on the contact patterns between individuals. In a simulation study based on real networks, we show the benefits of using our proposed approach compared to random assignments even when the network is imperfectly measured and there is a high degree of missingness in the data.},
  archive      = {J_FDATA},
  author       = {Sewell, Daniel K.},
  doi          = {10.3389/fdata.2022.893760},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {893760},
  shortjournal = {Front. Big Data},
  title        = {Network-informed constrained divisive pooled testing assignments},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying and addressing data asymmetries so as to enable
(better) science. <em>FDATA</em>, <em>5</em>, 888384. (<a
href="https://doi.org/10.3389/fdata.2022.888384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a society, we need to become more sophisticated in assessing and addressing data asymmetries—and their resulting political and economic power inequalities—particularly in the realm of open science, research, and development. This article seeks to start filling the analytical gap regarding data asymmetries globally, with a specific focus on the asymmetrical availability of privately-held data for open science, and a look at current efforts to address these data asymmetries. It provides a taxonomy of asymmetries, as well as both their societal and institutional impacts. Moreover, this contribution outlines a set of solutions that could provide a toolbox for open science practitioners and data demand-side actors that stand to benefit from increased access to data. The concept of data liquidity (and portability) is explored at length in connection with efforts to generate an ecosystem of responsible data exchanges. We also examine how data holders and demand-side actors are experimenting with new and emerging operational models and governance frameworks for purpose-driven, cross-sector data collaboratives that connect previously siloed datasets. Key solutions discussed include professionalizing and re-imagining data steward roles and functions (i.e., individuals or groups who are tasked with managing data and their ethical and responsible reuse within organizations). We present these solutions through case studies on notable efforts to address science data asymmetries. We examine these cases using a repurposable analytical framework that could inform future research. We conclude with recommended actions that could support the creation of an evidence base on work to address data asymmetries and unlock the public value of greater science data liquidity and responsible reuse.},
  archive      = {J_FDATA},
  author       = {Verhulst, Stefaan and Young, Andrew},
  doi          = {10.3389/fdata.2022.888384},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {888384},
  shortjournal = {Front. Big Data},
  title        = {Identifying and addressing data asymmetries so as to enable (better) science},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ethics of AI in radiology: A review of ethical and societal
implications. <em>FDATA</em>, <em>5</em>, 850383. (<a
href="https://doi.org/10.3389/fdata.2022.850383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is being applied in medicine to improve healthcare and advance health equity. The application of AI-based technologies in radiology is expected to improve diagnostic performance by increasing accuracy and simplifying personalized decision-making. While this technology has the potential to improve health services, many ethical and societal implications need to be carefully considered to avoid harmful consequences for individuals and groups, especially for the most vulnerable populations. Therefore, several questions are raised, including (1) what types of ethical issues are raised by the use of AI in medicine and biomedical research, and (2) how are these issues being tackled in radiology, especially in the case of breast cancer? To answer these questions, a systematic review of the academic literature was conducted. Searches were performed in five electronic databases to identify peer-reviewed articles published since 2017 on the topic of the ethics of AI in radiology. The review results show that the discourse has mainly addressed expectations and challenges associated with medical AI, and in particular bias and black box issues, and that various guiding principles have been suggested to ensure ethical AI. We found that several ethical and societal implications of AI use remain underexplored, and more attention needs to be paid to addressing potential discriminatory effects and injustices. We conclude with a critical reflection on these issues and the identified gaps in the discourse from a philosophical and STS perspective, underlining the need to integrate a social science perspective in AI developments in radiology in the future.},
  archive      = {J_FDATA},
  author       = {Goisauf, Melanie and Cano Abadía, Mónica},
  doi          = {10.3389/fdata.2022.850383},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {850383},
  shortjournal = {Front. Big Data},
  title        = {Ethics of AI in radiology: A review of ethical and societal implications},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Boosting face presentation attack detection in
multi-spectral videos through score fusion of wavelet partition images.
<em>FDATA</em>, <em>5</em>, 836749. (<a
href="https://doi.org/10.3389/fdata.2022.836749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presentation attack detection (PAD) algorithms have become an integral requirement for the secure usage of face recognition systems. As face recognition algorithms and applications increase from constrained to unconstrained environments and in multispectral scenarios, presentation attack detection algorithms must also increase their scope and effectiveness. It is important to realize that the PAD algorithms are not only effective for one environment or condition but rather be generalizable to a multitude of variabilities that are presented to a face recognition algorithm. With this motivation, as the first contribution, the article presents a unified PAD algorithm for different kinds of attacks such as printed photos, a replay of video, 3D masks, silicone masks, and wax faces. The proposed algorithm utilizes a combination of wavelet decomposed raw input images from sensor and face region data to detect whether the input image is bonafide or attacked. The second contribution of the article is the collection of a large presentation attack database in the NIR spectrum, containing images from individuals of two ethnicities. The database contains 500 print attack videos which comprise approximately 1,00,000 frames collectively in the NIR spectrum. Extensive evaluation of the algorithm on NIR images as well as visible spectrum images obtained from existing benchmark databases shows that the proposed algorithm yields state-of-the-art results and surpassed several complex and state-of-the-art algorithms. For instance, on benchmark datasets, namely CASIA-FASD, Replay-Attack, and MSU-MFSD, the proposed algorithm achieves a maximum error of 0.92% which is significantly lower than state-of-the-art attack detection algorithms.},
  archive      = {J_FDATA},
  author       = {Agarwal, Akshay and Singh, Richa and Vatsa, Mayank and Noore, Afzel},
  doi          = {10.3389/fdata.2022.836749},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {836749},
  shortjournal = {Front. Big Data},
  title        = {Boosting face presentation attack detection in multi-spectral videos through score fusion of wavelet partition images},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blockchain for electronic vaccine certificates: More cons
than pros? <em>FDATA</em>, <em>5</em>, 833196. (<a
href="https://doi.org/10.3389/fdata.2022.833196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic vaccine certificates (EVC) for COVID-19 vaccination are likely to become widespread. Blockchain (BC) is an electronic immutable distributed ledger and is one of the more common proposed EVC platform options. However, the principles of blockchain are not widely understood by public health and medical professionals. We attempt to describe, in an accessible style, how BC works and the potential benefits and drawbacks in its use for EVCs. Our assessment is BC technology is not well suited to be used for EVCs. Overall, blockchain technology is based on two key principles: the use of cryptography, and a distributed immutable ledger in the format of blockchains. While the use of cryptography can provide ease of sharing vaccination records while maintaining privacy, EVCs require some amount of contribution from a centralized authority to confirm vaccine status; this is partly because these authorities are responsible for the distribution and often the administration of the vaccine. Having the data distributed makes the role of a centralized authority less effective. We concluded there are alternative ways to use cryptography outside of a BC that allow a centralized authority to better participate, which seems necessary for an EVC platform to be of practical use.},
  archive      = {J_FDATA},
  author       = {Toubiana, Raphaëlle and Macdonald, Millie and Rajananda, Sivananda and Lokvenec, Tale and Kingsley, Thomas C. and Romero-Brufau, Santiago},
  doi          = {10.3389/fdata.2022.833196},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {833196},
  shortjournal = {Front. Big Data},
  title        = {Blockchain for electronic vaccine certificates: More cons than pros?},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deploying machine learning with messy, real world data in
low- and middle-income countries: Developing a global health use case.
<em>FDATA</em>, <em>5</em>, 553673. (<a
href="https://doi.org/10.3389/fdata.2022.553673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid emergence of machine learning in the form of large-scale computational statistics and accumulation of data offers global health implementing partners an opportunity to adopt, adapt, and apply these techniques and technologies to low- and middle-income country (LMIC) contexts where we work. These benefits reside just out of the reach of many implementing partners because they lack the experience and specific skills to use them. Yet the growth of available analytical systems and exponential growth of data require the global digital health community to become conversant in this technology to continue to make contributions to help fulfill our missions. In this community case study, we describe the approach we took at IntraHealth International to inform the use case for machine learning in global health and development. We found that the data needed to take advantage of machine learning were plentiful and that an international, interdisciplinary team can be formed to collect, clean, and analyze the data at hand using cloud-based (e.g., Dropbox, Google Drive) and open source tools (e.g., R). We organized our work as a “sprint” lasting roughly 10 weeks in length so that we could rapidly prototype these approaches in order to achieve institutional buy in. Our initial sprint resulted in two requests in subsequent workplans for analytics using the data we compiled and directly impacted program implementation.},
  archive      = {J_FDATA},
  author       = {Finnegan, Amy and Potenziani, David D. and Karutu, Caroline and Wanyana, Irene and Matsiko, Nicholas and Elahi, Cyrus and Mijumbi, Nobert and Stanley, Richard and Vota, Wayan},
  doi          = {10.3389/fdata.2022.553673},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {553673},
  shortjournal = {Front. Big Data},
  title        = {Deploying machine learning with messy, real world data in low- and middle-income countries: Developing a global health use case},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Human-interpretable machine learning.
<em>FDATA</em>, <em>5</em>, 956625. (<a
href="https://doi.org/10.3389/fdata.2022.956625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Tolomei, Gabriele and Pinelli, Fabio and Silvestri, Fabrizio},
  doi          = {10.3389/fdata.2022.956625},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {956625},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Human-interpretable machine learning},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Bayesian inference and AI. <em>FDATA</em>,
<em>5</em>, 934362. (<a
href="https://doi.org/10.3389/fdata.2022.934362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Tang, Niansheng and Liu, Catherine and Shi, Jian Qing and Huang, Yangxin},
  doi          = {10.3389/fdata.2022.934362},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {934362},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Bayesian inference and AI},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The oral microbiome and its role in systemic autoimmune
diseases: A systematic review of big data analysis. <em>FDATA</em>,
<em>5</em>, 927520. (<a
href="https://doi.org/10.3389/fdata.2022.927520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDespite decades of research, systemic autoimmune diseases (SADs) continue to be a major global health concern and the etiology of these diseases is still not clear. To date, with the development of high-throughput techniques, increasing evidence indicated a key role of oral microbiome in the pathogenesis of SADs, and the alterations of oral microbiome may contribute to the disease emergence or evolution. This review is to present the latest knowledge on the relationship between the oral microbiome and SADs, focusing on the multiomics data generated from a large set of samples.MethodologyBy searching the PubMed and Embase databases, studies that investigated the oral microbiome of SADs, including systemic lupus erythematosus (SLE), rheumatoid arthritis (RA), and Sjögren&#39;s syndrome (SS), were systematically reviewed according to the PRISMA guidelines.ResultsOne thousand and thirty-eight studies were found, and 25 studies were included: three referred to SLE, 12 referred to RA, nine referred to SS, and one to both SLE and SS. The 16S rRNA sequencing was the most frequent technique used. HOMD was the most common database aligned to and QIIME was the most popular pipeline for downstream analysis. Alterations in bacterial composition and population have been found in the oral samples of patients with SAD compared with the healthy controls. Results regarding candidate pathogens were not always in accordance, but Selenomonas and Veillonella were found significantly increased in three SADs, and Streptococcus was significantly decreased in the SADs compared with controls.ConclusionA large amount of sequencing data was collected from patients with SAD and controls in this systematic review. Oral microbial dysbiosis had been identified in these SADs, although the dysbiosis features were different among studies. There was a lack of standardized study methodology for each study from the inclusion criteria, sample type, sequencing platform, and referred database to downstream analysis pipeline and cutoff. Besides the genomics, transcriptomics, proteomics, and metabolomics technology should be used to investigate the oral microbiome of patients with SADs and also the at-risk individuals of disease development, which may provide us with a better understanding of the etiology of SADs and promote the development of the novel therapies.},
  archive      = {J_FDATA},
  author       = {Gao, Lu and Cheng, Zijian and Zhu, Fudong and Bi, Chunsheng and Shi, Qiongling and Chen, Xiaoyan},
  doi          = {10.3389/fdata.2022.927520},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {927520},
  shortjournal = {Front. Big Data},
  title        = {The oral microbiome and its role in systemic autoimmune diseases: A systematic review of big data analysis},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Offline evaluation of recommender systems in a user
interface with multiple carousels. <em>FDATA</em>, <em>5</em>, 910030.
(<a href="https://doi.org/10.3389/fdata.2022.910030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many video-on-demand and music streaming services provide the user with a page consisting of several recommendation lists, i.e., widgets or swipeable carousels, each built with specific criteria (e.g., most recent, TV series, etc.). Finding efficient strategies to select which carousels to display is an active research topic of great industrial interest. In this setting, the overall quality of the recommendations of a new algorithm cannot be assessed by measuring solely its individual recommendation quality. Rather, it should be evaluated in a context where other recommendation lists are already available, to account for how they complement each other. The traditional offline evaluation protocol however does not take this into account. To address this limitation, we propose an offline evaluation protocol for a carousel setting in which the recommendation quality of a model is measured by how much it improves upon that of an already available set of carousels. We also propose to extend ranking metrics to the two-dimensional carousel setting in order to account for a known position bias, i.e., users will not explore the lists sequentially, but rather concentrate on the top-left corner of the screen. Finally, we describe and evaluate two strategies for the ranking of carousels in a scenario where the technique used to generate the two-dimensional layout is agnostic on the algorithms used to generate each carousel. We report experiments on publicly available datasets in the movie domain to show how the relative effectiveness of several recommendation models compares. Our results indicate that under a carousel setting the ranking of the algorithms changes sometimes significantly. Furthermore, when selecting the optimal carousel layout accounting for the two dimensional layout of the user interface leads to very different selections.},
  archive      = {J_FDATA},
  author       = {Ferrari Dacrema, Maurizio and Felicioni, Nicolò and Cremonesi, Paolo},
  doi          = {10.3389/fdata.2022.910030},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {910030},
  shortjournal = {Front. Big Data},
  title        = {Offline evaluation of recommender systems in a user interface with multiple carousels},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A methodology for evaluating operator usage of machine
learning recommendations for power grid contingency analysis.
<em>FDATA</em>, <em>5</em>, 897295. (<a
href="https://doi.org/10.3389/fdata.2022.897295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents the application of a methodology to measure domain expert trust and workload, elicit feedback, and understand the technological usability and impact when a machine learning assistant is introduced into contingency analysis for real-time power grid simulation. The goal of this framework is to rapidly collect and analyze a broad variety of human factors data in order to accelerate the development and evaluation loop for deploying machine learning applications. We describe our methodology and analysis, and we discuss insights gained from a pilot participant about the current usability state of an early technology readiness level (TRL) artificial neural network (ANN) recommender.},
  archive      = {J_FDATA},
  author       = {Wenskovitch, John and Jefferson, Brett and Anderson, Alexander and Baweja, Jessica and Ciesielski, Danielle and Fallon, Corey},
  doi          = {10.3389/fdata.2022.897295},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {897295},
  shortjournal = {Front. Big Data},
  title        = {A methodology for evaluating operator usage of machine learning recommendations for power grid contingency analysis},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Significant subgraph detection in multi-omics networks for
disease pathway identification. <em>FDATA</em>, <em>5</em>, 894632. (<a
href="https://doi.org/10.3389/fdata.2022.894632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic obstructive pulmonary disease (COPD) is one of the leading causes of death in the United States. COPD represents one of many areas of research where identifying complex pathways and networks of interacting biomarkers is an important avenue toward studying disease progression and potentially discovering cures. Recently, sparse multiple canonical correlation network analysis (SmCCNet) was developed to identify complex relationships between omics associated with a disease phenotype, such as lung function. SmCCNet uses two sets of omics datasets and an associated output phenotypes to generate a multi-omics graph, which can then be used to explore relationships between omics in the context of a disease. Detecting significant subgraphs within this multi-omics network, i.e., subgraphs which exhibit high correlation to a disease phenotype and high inter-connectivity, can help clinicians identify complex biological relationships involved in disease progression. The current approach to identifying significant subgraphs relies on hierarchical clustering, which can be used to inform clinicians about important pathways involved in the disease or phenotype of interest. The reliance on a hierarchical clustering approach can hinder subgraph quality by biasing toward finding more compact subgraphs and removing larger significant subgraphs. This study aims to introduce new significant subgraph detection techniques. In particular, we introduce two subgraph detection methods, dubbed Correlated PageRank and Correlated Louvain, by extending the Personalized PageRank Clustering and Louvain algorithms, as well as a hybrid approach combining the two proposed methods, and compare them to the hierarchical method currently in use. The proposed methods show significant improvement in the quality of the subgraphs produced when compared to the current state of the art.},
  archive      = {J_FDATA},
  author       = {Abdel-Hafiz, Mohamed and Najafi, Mesbah and Helmi, Shahab and Pratte, Katherine A. and Zhuang, Yonghua and Liu, Weixuan and Kechris, Katerina J. and Bowler, Russell P. and Lange, Leslie and Banaei-Kashani, Farnoush},
  doi          = {10.3389/fdata.2022.894632},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {894632},
  shortjournal = {Front. Big Data},
  title        = {Significant subgraph detection in multi-omics networks for disease pathway identification},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal inference in the presence of interference in
sponsored search advertising. <em>FDATA</em>, <em>5</em>, 888592. (<a
href="https://doi.org/10.3389/fdata.2022.888592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classical causal inference, inferring cause-effect relations from data relies on the assumption that units are independent and identically distributed. This assumption is violated in settings where units are related through a network of dependencies. An example of such a setting is ad placement in sponsored search advertising, where the likelihood of a user clicking on a particular ad is potentially influenced by where it is placed and where other ads are placed on the search result page. In such scenarios, confounding arises due to not only the individual ad-level covariates but also the placements and covariates of other ads in the system. In this paper, we leverage the language of causal inference in the presence of interference to model interactions among the ads. Quantification of such interactions allows us to better understand the click behavior of users, which in turn impacts the revenue of the host search engine and enhances user satisfaction. We illustrate the utility of our formalization through experiments carried out on the ad placement system of the Bing search engine.},
  archive      = {J_FDATA},
  author       = {Nabi, Razieh and Pfeiffer, Joel and Charles, Denis and Kıcıman, Emre},
  doi          = {10.3389/fdata.2022.888592},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {888592},
  shortjournal = {Front. Big Data},
  title        = {Causal inference in the presence of interference in sponsored search advertising},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum likelihood reconstruction of water cherenkov events
with deep generative neural networks. <em>FDATA</em>, <em>5</em>,
868333. (<a href="https://doi.org/10.3389/fdata.2022.868333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large water Cherenkov detectors have shaped our current knowledge of neutrino physics and nucleon decay, and will continue to do so in the foreseeable future. These highly capable detectors allow for directional and topological, as well as calorimetric information to be extracted from signals on their photosensors. The current state-of-the-art approach to water Cherenkov reconstruction relies on maximum-likelihood estimation, with several simplifying assumptions employed to make the problem tractable. In this paper, we describe neural networks that produce probability density functions for the signals at each photosensor, given a set of inputs that characterizes a particle in the detector. The neural networks we propose allow for likelihood-based approaches to event reconstruction with significantly fewer assumptions compared to traditional methods, and are thus expected to improve on the current performance of water Cherenkov detectors.},
  archive      = {J_FDATA},
  author       = {Jia, Mo and Kumar, Karan and Mackey, Liam S. and Putra, Alexander and Vilela, Cristovao and Wilking, Michael J. and Xia, Junjie and Yanagisawa, Chiaki and Yang, Karan},
  doi          = {10.3389/fdata.2022.868333},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {868333},
  shortjournal = {Front. Big Data},
  title        = {Maximum likelihood reconstruction of water cherenkov events with deep generative neural networks},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The influence of personal health data on the health coaching
process. <em>FDATA</em>, <em>5</em>, 678061. (<a
href="https://doi.org/10.3389/fdata.2022.678061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking health data, for example, through wearable devices or health apps, is increasingly commonplace. Consequently, health coaches (e.g., personal trainers, dieticians) are facing growing numbers of clients who bring their data to the clinic. These data potentially add value to the coaching process, for example, by showing more objective and specific information on clients&#39; behaviors. However, in practice, it turns out to be hard to effectively utilize health data in a coaching setting, and it is not yet fully understood how data affect the coaching process and the coach-client communication. We organized a workshop (12 coaches, 3 clients) and a field study (5 coaches, 6 clients), where we observed coach-client interactions enriched with data. By including both familiar and unfamiliar coach-client pairs, as well as alternating the timing of the data presented (i.e., at the beginning, or halfway through the session), we acquired a variety of data-driven coaching interactions and analyzed this using a mixture of qualitative and quantitative methods. Our results show that data are not “plug-and-play.” There is an extensive process of interpreting and contextualizing data, in which the client has a key role, which is essential to gain relevant and actionable insights from the data useful to the coaching process. We also observed that data affect the coach-client communication on both content and relationship levels. We will reflect on these insights in terms of design recommendations for wearable tracking devices and e-health technology to effectively support health coaches and their interactions with their clients.},
  archive      = {J_FDATA},
  author       = {Rutjes, Heleen and Willemsen, Martijn C. and Feijt, Milou A. and IJsselsteijn, Wijnand A.},
  doi          = {10.3389/fdata.2022.678061},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {678061},
  shortjournal = {Front. Big Data},
  title        = {The influence of personal health data on the health coaching process},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: User modeling and recommendations.
<em>FDATA</em>, <em>5</em>, 923397. (<a
href="https://doi.org/10.3389/fdata.2022.923397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Helic, Denis and Gadiraju, Ujwal and Tkalcic, Marko},
  doi          = {10.3389/fdata.2022.923397},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {923397},
  shortjournal = {Front. Big Data},
  title        = {Editorial: User modeling and recommendations},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Statistical learning for predicting air quality.
<em>FDATA</em>, <em>5</em>, 898643. (<a
href="https://doi.org/10.3389/fdata.2022.898643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Rybarczyk, Yves Philippe and Zalakeviciute, Rasa},
  doi          = {10.3389/fdata.2022.898643},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {898643},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Statistical learning for predicting air quality},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of feature-based explanation and its output modality
on user satisfaction with service recommender systems. <em>FDATA</em>,
<em>5</em>, 897381. (<a
href="https://doi.org/10.3389/fdata.2022.897381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in natural language based virtual assistants have attracted more researches on application of recommender systems (RS) into the service product domain (e.g., looking for a restaurant or a hotel), given that RS can assist users in more effectively obtaining information. However, though there is emerging study on how the presentation of recommendation (vocal vs. visual) would affect user experiences with RS, little attention has been paid to how the output modality of its explanation (i.e., explaining why a particular item is recommended) interacts with the explanation content to influence user satisfaction. In this work, we particularly consider feature-based explanation, a popular type of explanation that aims to reveal how relevant a recommendation is to the user in terms of its features (e.g., a restaurant&#39;s food quality, service, distance, or price), for which we have concretely examined three content design factors as summarized from the literature survey: feature type, contextual relevance, and number of features. Results of our user studies show that, for explanation presented in different modalities (text and voice), the effects of those design factors on user satisfaction with RS are different. Specifically, for text explanations, the number of features and contextual relevance influenced users&#39; satisfaction with the recommender system, but the feature type did not; while for voice explanations, we found no factors influenced user satisfaction. We finally discuss the practical implications of those findings and possible directions for future research.},
  archive      = {J_FDATA},
  author       = {Zhang, Zhirun and Chen, Li and Jiang, Tonglin and Li, Yutong and Li, Lei},
  doi          = {10.3389/fdata.2022.897381},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {897381},
  shortjournal = {Front. Big Data},
  title        = {Effects of feature-based explanation and its output modality on user satisfaction with service recommender systems},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Examining sentiment in complex texts. A comparison of
different computational approaches. <em>FDATA</em>, <em>5</em>, 886362.
(<a href="https://doi.org/10.3389/fdata.2022.886362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we rely on computational methods to accurately analyze complex texts? To answer this question, we compared different dictionary and scaling methods used in predicting the sentiment of German literature reviews to the “gold standard” of human-coded sentiments. Literature reviews constitute a challenging text corpus for computational analysis as they not only contain different text levels—for example, a summary of the work and the reviewer&#39;s appraisal—but are also characterized by subtle and ambiguous language elements. To take the nuanced sentiments of literature reviews into account, we worked with a metric rather than a dichotomous scale for sentiment analysis. The results of our analyses show that the predicted sentiments of prefabricated dictionaries, which are computationally efficient and require minimal adaption, have a low to medium correlation with the human-coded sentiments (r between 0.32 and 0.39). The accuracy of self-created dictionaries using word embeddings (both pre-trained and self-trained) was considerably lower (r between 0.10 and 0.28). Given the high coding intensity and contingency on seed selection as well as the degree of data pre-processing of word embeddings that we found with our data, we would not recommend them for complex texts without further adaptation. While fully automated approaches appear not to work in accurately predicting text sentiments with complex texts such as ours, we found relatively high correlations with a semiautomated approach (r of around 0.6)—which, however, requires intensive human coding efforts for the training dataset. In addition to illustrating the benefits and limits of computational approaches in analyzing complex text corpora and the potential of metric rather than binary scales of text sentiment, we also provide a practical guide for researchers to select an appropriate method and degree of pre-processing when working with complex texts.},
  archive      = {J_FDATA},
  author       = {Munnes, Stefan and Harsch, Corinna and Knobloch, Marcel and Vogel, Johannes S. and Hipp, Lena and Schilling, Erik},
  doi          = {10.3389/fdata.2022.886362},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {886362},
  shortjournal = {Front. Big Data},
  title        = {Examining sentiment in complex texts. a comparison of different computational approaches},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FAIR digital twins for data-intensive research.
<em>FDATA</em>, <em>5</em>, 883341. (<a
href="https://doi.org/10.3389/fdata.2022.883341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although all the technical components supporting fully orchestrated Digital Twins (DT) currently exist, what remains missing is a conceptual clarification and analysis of a more generalized concept of a DT that is made FAIR, that is, universally machine actionable. This methodological overview is a first step toward this clarification. We present a review of previously developed semantic artifacts and how they may be used to compose a higher-order data model referred to here as a FAIR Digital Twin (FDT). We propose an architectural design to compose, store and reuse FDTs supporting data intensive research, with emphasis on privacy by design and their use in GDPR compliant open science.},
  archive      = {J_FDATA},
  author       = {Schultes, Erik and Roos, Marco and Bonino da Silva Santos, Luiz Olavo and Guizzardi, Giancarlo and Bouwman, Jildau and Hankemeier, Thomas and Baak, Arie and Mons, Barend},
  doi          = {10.3389/fdata.2022.883341},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {883341},
  shortjournal = {Front. Big Data},
  title        = {FAIR digital twins for data-intensive research},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Wasserstein uncertainty estimation for adversarial domain
matching. <em>FDATA</em>, <em>5</em>, 878716. (<a
href="https://doi.org/10.3389/fdata.2022.878716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims at reducing the domain shift between a labeled source domain and an unlabeled target domain, so that the source model can be generalized to target domains without fine tuning. In this paper, we propose to evaluate the cross-domain transferability between source and target samples by domain prediction uncertainty, which is quantified via Wasserstein gradient flows. Further, we exploit it for reweighting the training samples to alleviate the issue of domain shift. The proposed mechanism provides a meaningful curriculum for cross-domain transfer and adaptively rules out samples that contain too much domain specific information during domain adaptation. Experiments on several benchmark datasets demonstrate that our reweighting mechanism can achieve improved results in both balanced and partial domain adaptation.},
  archive      = {J_FDATA},
  author       = {Wang, Rui and Zhang, Ruiyi and Henao, Ricardo},
  doi          = {10.3389/fdata.2022.878716},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {878716},
  shortjournal = {Front. Big Data},
  title        = {Wasserstein uncertainty estimation for adversarial domain matching},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accessibility and essential travel: Public transport
reliance among senior citizens during the COVID-19 pandemic.
<em>FDATA</em>, <em>5</em>, 867085. (<a
href="https://doi.org/10.3389/fdata.2022.867085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using smart card travel data, we compare demand for bus services by passengers of age 65 or older prior to and during the COVID-19 pandemic to identify public transport-reliant users residing in more car-dependent environments—i.e., people who rely on public transport services to carry out essential activities, such as daily shopping and live in areas with low public transport accessibility. Viewing lockdowns as natural experiments, we use spatial analysis combined with multilevel logistic regressions to characterize the demographic and geographic context of those passengers who continued to use public transport services in these areas during lockdown periods, or quickly returned to public transport when restrictions were eased. We find that this particular type of public transport reliance is significantly associated with socio-demographic characteristics alongside urban residential conditions. Specifically, we identify suburban geographies of public transport reliance, which are at risk of being overlooked in approaches that view public transport dependence mainly as an outcome of deprivation. Our research demonstrates once again that inclusive, healthy and sustainable mobility can only be achieved if all areas of metropolitan regions are well and reliably served by public transport.},
  archive      = {J_FDATA},
  author       = {Carney, Ffion and Long, Alfie and Kandt, Jens},
  doi          = {10.3389/fdata.2022.867085},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {867085},
  shortjournal = {Front. Big Data},
  title        = {Accessibility and essential travel: Public transport reliance among senior citizens during the COVID-19 pandemic},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topic modeling for interpretable text classification from
EHRs. <em>FDATA</em>, <em>5</em>, 846930. (<a
href="https://doi.org/10.3389/fdata.2022.846930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clinical notes in electronic health records have many possibilities for predictive tasks in text classification. The interpretability of these classification models for the clinical domain is critical for decision making. Using topic models for text classification of electronic health records for a predictive task allows for the use of topics as features, thus making the text classification more interpretable. However, selecting the most effective topic model is not trivial. In this work, we propose considerations for selecting a suitable topic model based on the predictive performance and interpretability measure for text classification. We compare 17 different topic models in terms of both interpretability and predictive performance in an inpatient violence prediction task using clinical notes. We find no correlation between interpretability and predictive performance. In addition, our results show that although no model outperforms the other models on both variables, our proposed fuzzy topic modeling algorithm (FLSA-W) performs best in most settings for interpretability, whereas two state-of-the-art methods (ProdLDA and LSI) achieve the best predictive performance.},
  archive      = {J_FDATA},
  author       = {Rijcken, Emil and Kaymak, Uzay and Scheepers, Floortje and Mosteiro, Pablo and Zervanou, Kalliopi and Spruit, Marco},
  doi          = {10.3389/fdata.2022.846930},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {846930},
  shortjournal = {Front. Big Data},
  title        = {Topic modeling for interpretable text classification from EHRs},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-linear embedding methods for identifying similar brain
activity in 1 million iEEG records captured from 256 RNS system
patients. <em>FDATA</em>, <em>5</em>, 840508. (<a
href="https://doi.org/10.3389/fdata.2022.840508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding electrophysiological features that are similar across patients with epilepsy may facilitate identifying treatment options for one patient that worked in patients with similar brain activity patterns. Three non-linear iEEG (intracranial electroencephalogram) embedding methods of finding similar cross-patient iEEG records in a large iEEG dataset were developed and compared. About 1 million iEEG records from 256 patients with drug-resistant focal onset seizures who were treated in prospective trials of the RNS System were used for analyses. Data from 200, 25, and 31 patients were randomly selected to be in the train, validation, and test datasets. In method 1, ResNet50 convolutional neural network (CNN) model pre-trained on the ImageNet dataset was used for extracting feature maps from spectrogram images (ImageNet-ResNet) of iEEG records. In method 2, ResNet50 custom trained on an iEEG classification task using ~138,000 manually labeled iEEG records was used as the feature extractor (ESC-ResNet). Feature maps were passed through dimensionality reduction and k nearest neighbors were found in the reduced feature space. In method 3, a 256 dimensional iEEG embedding space was learned via contrastive learning by training a ResNet50 model with triplet training sets generated using within-patient iEEG clustering (CL-ResNet). All three methods had comparable performance when identifying iEEG records from the search dataset similar to test iEEG records of baseline (non-seizure) and interictal spiking activity. Epileptic interictal spikes are represented by vertical (broadband) edges in spectrogram images, and hence even generic features extracted using models trained on everyday images appear to be sufficient to represent iEEG records with similar levels of interictal spiking activity in close proximity. In the case of electrographic seizures, however, the ESC-ResNet model, identified cross-patient iEEG records with electrographic seizure morphology features that were most similar to the test iEEG records. For nuanced electrographic seizure iEEG representation learning, domain specific model training with manually generated labels had the advantage. Finally, representative iEEG records were selected from every patient using an unsupervised clustering method which effectively reduced the number of iEEG records in the search dataset from ~750,000 to 2,148, thus substantially reducing the time required for finding similar cross-patient iEEG records.},
  archive      = {J_FDATA},
  author       = {Arcot Desai, Sharanya and Tcheng, Thomas and Morrell, Martha},
  doi          = {10.3389/fdata.2022.840508},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {840508},
  shortjournal = {Front. Big Data},
  title        = {Non-linear embedding methods for identifying similar brain activity in 1 million iEEG records captured from 256 RNS system patients},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial robustness of deep reinforcement learning based
dynamic recommender systems. <em>FDATA</em>, <em>5</em>, 822783. (<a
href="https://doi.org/10.3389/fdata.2022.822783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks, e.g., adversarial perturbations of the input and adversarial samples, pose significant challenges to machine learning and deep learning techniques, including interactive recommendation systems. The latent embedding space of those techniques makes adversarial attacks challenging to detect at an early stage. Recent advance in causality shows that counterfactual can also be considered one of the ways to generate the adversarial samples drawn from different distribution as the training samples. We propose to explore adversarial examples and attack agnostic detection on reinforcement learning (RL)-based interactive recommendation systems. We first craft different types of adversarial examples by adding perturbations to the input and intervening on the casual factors. Then, we augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our white-box detector trained with one crafting method has the generalization ability over several other crafting methods.},
  archive      = {J_FDATA},
  author       = {Wang, Siyu and Cao, Yuanjiang and Chen, Xiaocong and Yao, Lina and Wang, Xianzhi and Sheng, Quan Z.},
  doi          = {10.3389/fdata.2022.822783},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {822783},
  shortjournal = {Front. Big Data},
  title        = {Adversarial robustness of deep reinforcement learning based dynamic recommender systems},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Application of a machine learning algorithm in generating an
evapotranspiration data product from coupled thermal infrared and
microwave satellite observations. <em>FDATA</em>, <em>5</em>, 768676.
(<a href="https://doi.org/10.3389/fdata.2022.768676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land surface evapotranspiration (ET) is one of the main energy sources for atmospheric dynamics and a critical component of the local, regional, and global water cycles. Consequently, accurate measurement or estimation of ET is one of the most active topics in hydro-climatology research. With massive and spatially distributed observational data sets of land surface properties and environmental conditions being collected from the ground, airborne or space-borne platforms daily over the past few decades, many research teams have started to use big data science to advance the ET estimation methods. The Geostationary satellite Evapotranspiration and Drought (GET-D) product system was developed at the National Oceanic and Atmospheric Administration (NOAA) in 2016 to generate daily ET and drought maps operationally. The primary inputs of the current GET-D system are the thermal infrared (TIR) observations from NOAA GOES satellite series. Because of the cloud contamination to the TIR observations, the spatial coverage of the daily GET-D ET product has been severely impacted. Based on the most recent advances, we have tested a machine learning algorithm to estimate all-weather land surface temperature (LST) from TIR and microwave (MW) combined satellite observations. With the regression tree machine learning approach, we can combine the high accuracy and high spatial resolution of GOES TIR data with the better spatial coverage of passive microwave observations and LST simulations from a land surface model (LSM). The regression tree model combines the three LST data sources for both clear and cloudy days, which enables the GET-D system to derive an all-weather ET product. This paper reports how the all-weather LST and ET are generated in the upgraded GET-D system and provides an evaluation of these LST and ET estimates with ground measurements. The results demonstrate that the regression tree machine learning method is feasible and effective for generating daily ET under all weather conditions with satisfactory accuracy from the big volume of satellite observations.},
  archive      = {J_FDATA},
  author       = {Fang, Li and Zhan, Xiwu and Kalluri, Satya and Yu, Peng and Hain, Chris and Anderson, Martha and Laszlo, Istvan},
  doi          = {10.3389/fdata.2022.768676},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {768676},
  shortjournal = {Front. Big Data},
  title        = {Application of a machine learning algorithm in generating an evapotranspiration data product from coupled thermal infrared and microwave satellite observations},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tuning of elasticsearch configuration: Parameter
optimization through simultaneous perturbation stochastic approximation.
<em>FDATA</em>, <em>5</em>, 686416. (<a
href="https://doi.org/10.3389/fdata.2022.686416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elasticsearch is currently the most popular search engine for full-text database management systems. By default, its configuration does not change while it receives data. However, when Elasticsearch stores a large amount of data over time, the default configuration becomes an obstacle to improving performance. In addition, the servers that host Elasticsearch may have limited resources, such as internal memory and CPU. A general solution to these problems is to dynamically tune the configuration parameters of Elasticsearch in order to improve its performance. The sheer number of parameters involved in this configuration makes it a complex task. In this work, we apply the Simultaneous Perturbation Stochastic Approximation method for optimizing Elasticsearch with multiple unknown parameters. Using this algorithm, our implementation optimizes the Elasticsearch configuration parameters by observing the performance and automatically changing the configuration to improve performance. The proposed solution makes it possible to change the configuration parameters of Elasticsearch automatically without having to restart the currently running instance of Elasticsearch. The results show a higher than 40% improvement in the combined data insertion capacity and the system&#39;s response time.},
  archive      = {J_FDATA},
  author       = {Haugerud, Hårek and Sobhie, Mohamad and Yazidi, Anis},
  doi          = {10.3389/fdata.2022.686416},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {686416},
  shortjournal = {Front. Big Data},
  title        = {Tuning of elasticsearch configuration: Parameter optimization through simultaneous perturbation stochastic approximation},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Computational behavioral modeling for big user
data. <em>FDATA</em>, <em>5</em>, 893216. (<a
href="https://doi.org/10.3389/fdata.2022.893216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Jiang, Meng and Zhang, Chuxu and Zhang, Xiangliang and Shah, Neil},
  doi          = {10.3389/fdata.2022.893216},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {893216},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Computational behavioral modeling for big user data},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The causal fairness field guide: Perspectives from social
and formal sciences. <em>FDATA</em>, <em>5</em>, 892837. (<a
href="https://doi.org/10.3389/fdata.2022.892837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past several years, multiple different methods to measure the causal fairness of machine learning models have been proposed. However, despite the growing number of publications and implementations, there is still a critical lack of literature that explains the interplay of causality-based fairness notions with the social sciences of philosophy, sociology, and law. We hope to remedy this issue by accumulating and expounding upon the thoughts and discussions of causality-based fairness notions produced by both social and formal (specifically machine learning) sciences in this field guide. In addition to giving the mathematical backgrounds of several popular causality-based fair machine learning notions, we explain their connection to and interplay with the fields of philosophy and law. Further, we explore several criticisms of the current approaches to causality-based fair machine learning from a sociological viewpoint as well as from a technical standpoint. It is our hope that this field guide will help fair machine learning practitioners better understand how their causality-based fairness notions align with important humanistic values (such as fairness) and how we can, as a field, design methods and metrics to better serve oppressed and marginalized populaces.},
  archive      = {J_FDATA},
  author       = {Carey, Alycia N. and Wu, Xintao},
  doi          = {10.3389/fdata.2022.892837},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {892837},
  shortjournal = {Front. Big Data},
  title        = {The causal fairness field guide: Perspectives from social and formal sciences},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving data quality in clinical research informatics
tools. <em>FDATA</em>, <em>5</em>, 871897. (<a
href="https://doi.org/10.3389/fdata.2022.871897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining data quality is a fundamental requirement for any successful and long-term data management. Providing high-quality, reliable, and statistically sound data is a primary goal for clinical research informatics. In addition, effective data governance and management are essential to ensuring accurate data counts, reports, and validation. As a crucial step of the clinical research process, it is important to establish and maintain organization-wide standards for data quality management to ensure consistency across all systems designed primarily for cohort identification, allowing users to perform an enterprise-wide search on a clinical research data repository to determine the existence of a set of patients meeting certain inclusion or exclusion criteria. Some of the clinical research tools are referred to as de-identified data tools. Assessing and improving the quality of data used by clinical research informatics tools are both important and difficult tasks. For an increasing number of users who rely on information as one of their most important assets, enforcing high data quality levels represents a strategic investment to preserve the value of the data. In clinical research informatics, better data quality translates into better research results and better patient care. However, achieving high-quality data standards is a major task because of the variety of ways that errors might be introduced in a system and the difficulty of correcting them systematically. Problems with data quality tend to fall into two categories. The first category is related to inconsistency among data resources such as format, syntax, and semantic inconsistencies. The second category is related to poor ETL and data mapping processes. In this paper, we describe a real-life case study on assessing and improving the data quality at one of healthcare organizations. This paper compares between the results obtained from two de-identified data systems i2b2, and Epic Slicedicer, and discuss the data quality dimensions&#39; specific to the clinical research informatics context, and the possible data quality issues between the de-identified systems. This work in paper aims to propose steps/rules for maintaining the data quality among different systems to help data managers, information systems teams, and informaticists at any health care organization to monitor and sustain data quality as part of their business intelligence, data governance, and data democratization processes.},
  archive      = {J_FDATA},
  author       = {AbuHalimeh, Ahmed},
  doi          = {10.3389/fdata.2022.871897},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {871897},
  shortjournal = {Front. Big Data},
  title        = {Improving data quality in clinical research informatics tools},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ethical views on sharing digital data for public health
surveillance: Analysis of survey data among patients. <em>FDATA</em>,
<em>5</em>, 871236. (<a
href="https://doi.org/10.3389/fdata.2022.871236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital data, including social media, wearable device data, electronic health records, and internet search data, are increasingly being integrated into public health research and policy. Because of the current issues around public distrust of science and other ethical issues in public health research, it is essential that researchers conduct ongoing studies assessing people&#39;s perceptions around and willingness to share digital data. This study aims to examine participants&#39; social media use and comfort sharing their data with health researchers. One hundred and sixty-one participants with medical conditions were recruited through social media paid advertisements and referral from a website, and invited to complete surveys on social media use and ethical perspectives on data sharing. Eligibility criteria were adults 18 years old or older, living in the US, self-reported having been diagnosed by a physician with a medical condition, belonging to at least one social media platform, using social media at least twice a week, and owning a smartphone. Study participants were mostly female, White, and with a mean age of 36.31 years. More than one third of participants reported being very comfortable sharing electronic health data and social media data for personalized healthcare and to help others. Findings suggest that participants are very uncomfortable sharing their location and text message data with researchers, with primary concerns centered around loss of privacy, disclosing private information, and that friends, family, and others may find out that they shared text messages with researchers. We discuss the implications of this research before and after the COVID-19 pandemic, along with its potential implications for future collection of digital data for public health.},
  archive      = {J_FDATA},
  author       = {Garett, Renee and Young, Sean D.},
  doi          = {10.3389/fdata.2022.871236},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {871236},
  shortjournal = {Front. Big Data},
  title        = {Ethical views on sharing digital data for public health surveillance: Analysis of survey data among patients},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FROST: Fallback voice apps recommendation for unhandled
voice commands in intelligent personal assistants. <em>FDATA</em>,
<em>5</em>, 867251. (<a
href="https://doi.org/10.3389/fdata.2022.867251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent personal assistants (IPAs) such as Amazon Alexa, Google Assistant and Apple Siri extend their built-in capabilities by supporting voice apps developed by third-party developers. Sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). There are many reasons including automatic speech recognition (ASR) error, natural language understanding (NLU) error, routing utterances to an irrelevant voice app, or simply that the user is asking for a capability that is not supported yet. The failure to handle a voice command leads to customer frustration. In this article, we introduce a fallback skill recommendation system (FROST) to suggest a voice app to a customer for an unhandled voice command. There are several practical issues when developing a skill recommender system for IPAs, i.e., partial observation, hard and noisy utterances. To solve the partial observation problem, we propose collaborative data relabeling (CDR) method. To mitigate hard and noisy utterance issues, we propose a rephrase-based relabeling technique. We evaluate the proposed system in both offline and online settings. The offline evaluation results show that the FROST system outperforms the baseline rule-based system. The online A/B testing results show a significant gain of customer experience metrics.},
  archive      = {J_FDATA},
  author       = {Hu, Qian and Mohamed, Thahir and Xiao, Wei and Ma, Xiyao and Gao, Xibin and Gao, Zheng and Arava, Radhika and AbdelHady, Mohamed},
  doi          = {10.3389/fdata.2022.867251},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {867251},
  shortjournal = {Front. Big Data},
  title        = {FROST: Fallback voice apps recommendation for unhandled voice commands in intelligent personal assistants},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BurnoutEnsemble: Augmented intelligence to detect
indications for burnout in clinical psychology. <em>FDATA</em>,
<em>5</em>, 863100. (<a
href="https://doi.org/10.3389/fdata.2022.863100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burnout, a state of emotional, physical, and mental exhaustion caused by excessive and prolonged stress, is a growing concern. It is known to occur when an individual feels overwhelmed, emotionally exhausted, and unable to meet the constant demands imposed upon them. Detecting burnout is not an easy task, in large part because symptoms can overlap with those of other illnesses or syndromes. The use of natural language processing (NLP) methods has the potential to mitigate the limitations of typical burnout detection via inventories. In this article, the performance of NLP methods on anonymized free text data samples collected from the online forum/social media platform Reddit was analyzed. A dataset consisting of 13,568 samples describing first-hand experiences, of which 352 are related to burnout and 979 to depression, was compiled. This work demonstrates the effectiveness of NLP and machine learning methods in detecting indicators for burnout. Finally, it improves upon standard baseline classifiers by building and training an ensemble classifier using two methods (subreddit and random batching). The best ensemble models attain a balanced accuracy of 0.93, test F1 score of 0.43, and test recall of 0.93. Both the subreddit and random batching ensembles outperform the single classifier baselines in the experimental setup.},
  archive      = {J_FDATA},
  author       = {Merhbene, Ghofrane and Nath, Sukanya and Puttick, Alexandre R. and Kurpicz-Briki, Mascha},
  doi          = {10.3389/fdata.2022.863100},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {863100},
  shortjournal = {Front. Big Data},
  title        = {BurnoutEnsemble: Augmented intelligence to detect indications for burnout in clinical psychology},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning approach for assessing air quality during
COVID-19 lockdown in quito. <em>FDATA</em>, <em>5</em>, 842455. (<a
href="https://doi.org/10.3389/fdata.2022.842455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather Normalized Models (WNMs) are modeling methods used for assessing air contaminants under a business-as-usual (BAU) assumption. Therefore, WNMs are used to assess the impact of many events on urban pollution. Recently, different approaches have been implemented to develop WNMs and quantify the lockdown effects of COVID-19 on air quality, including Machine Learning (ML). However, more advanced methods, such as Deep Learning (DL), have never been applied for developing WNMs. In this study, we proposed WNMs based on DL algorithms, aiming to test five DL architectures and compare their performances to a recent ML approach, namely Gradient Boosting Machine (GBM). The concentrations of five air pollutants (CO, NO2, PM2.5, SO2, and O3) are studied in the city of Quito, Ecuador. The results show that Long-Short Term Memory (LSTM) and Bidirectional Recurrent Neural Network (BiRNN) outperform the other algorithms and, consequently, are recommended as appropriate WNMs to quantify the effects of the lockdowns on air pollution. Furthermore, examining the variable importance in the LSTM and BiRNN models, we identify that the most relevant temporal and meteorological features for predicting air quality are Hours (time of day), Index (1 is the first collected data and increases by one after each instance), Julian Day (day of the year), Relative Humidity, Wind Speed, and Solar Radiation. During the full lockdown, the concentration of most pollutants has decreased drastically: −48.75%, for CO, −45.76%, for SO2, −42.17%, for PM2.5, and −63.98%, for NO2. The reduction of this latter gas has induced an increase of O3 by +26.54%.},
  archive      = {J_FDATA},
  author       = {Chau, Phuong N. and Zalakeviciute, Rasa and Thomas, Ilias and Rybarczyk, Yves},
  doi          = {10.3389/fdata.2022.842455},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {842455},
  shortjournal = {Front. Big Data},
  title        = {Deep learning approach for assessing air quality during COVID-19 lockdown in quito},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Public service and public happiness: Inferences from big
weibo datasets for 31 chinese provincial governments. <em>FDATA</em>,
<em>5</em>, 833703. (<a
href="https://doi.org/10.3389/fdata.2022.833703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The change of public service has usually been considered to affect public happiness. However, since the publication of the Easterlin Paradox, the causal relationship between public service and public happiness has been furiously questioned by public affairs researchers. It has been documented through resolving the four causal factors of public happiness within public administration, new public administration, new public management, and governance that public-service-driven public happiness may be attributed to four happiness dimensions: Objective Reality, Subjective Reality, Inter-Subjective Reality, and Virtual Reality. This article reports the results of significance tests of the relationship between public service and public happiness from analyses of large datasets collected from Weibo systems in 31 Chinese provincial governments from 2010 to 2020. The analyses show that the public service change during this period has not yet led to satisfactory improvement in all four happiness dimensions. Finally, we propose strategies for governments to modify public services to enhance public happiness.},
  archive      = {J_FDATA},
  author       = {Ding, Yi Zhou},
  doi          = {10.3389/fdata.2022.833703},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {833703},
  shortjournal = {Front. Big Data},
  title        = {Public service and public happiness: Inferences from big weibo datasets for 31 chinese provincial governments},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A comparative study of data-driven models for travel
destination characterization. <em>FDATA</em>, <em>5</em>, 829939. (<a
href="https://doi.org/10.3389/fdata.2022.829939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing items for content-based recommender systems is a challenging task in complex domains such as travel and tourism. In the case of destination recommendation, no feature set can be readily used as a similarity ground truth, which makes it hard to evaluate the quality of destination characterization approaches. Furthermore, the process should scale well for many items, be cost-efficient, and most importantly correct. To evaluate which data sources are most suitable, we investigate 18 characterization methods that fall into three categories: venue data, textual data, and factual data. We make these data models comparable using rank agreement metrics and reveal which data sources capture similar underlying concepts. To support choosing more suitable data models, we capture a desired concept using an expert survey and evaluate our characterization methods toward it. We find that the textual models to characterize cities perform best overall, with data models based on factual and venue data being less competitive. However, we show that data models with explicit features can be optimized by learning weights for their features.},
  archive      = {J_FDATA},
  author       = {Dietz, Linus W. and Sertkan, Mete and Myftija, Saadi and Thimbiri Palage, Sameera and Neidhardt, Julia and Wörndl, Wolfgang},
  doi          = {10.3389/fdata.2022.829939},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {829939},
  shortjournal = {Front. Big Data},
  title        = {A comparative study of data-driven models for travel destination characterization},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian joint modeling of multivariate longitudinal and
survival data with an application to diabetes study. <em>FDATA</em>,
<em>5</em>, 812725. (<a
href="https://doi.org/10.3389/fdata.2022.812725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models of longitudinal and time-to-event data have received a lot of attention in epidemiological and clinical research under a linear mixed-effects model with the normal assumption for a single longitudinal outcome and Cox proportional hazards model. However, those model-based analyses may not provide robust inference when longitudinal measurements exhibit skewness and/or heavy tails. In addition, the data collected are often featured by multivariate longitudinal outcomes which are significantly correlated, and ignoring their correlation may lead to biased estimation. Under the umbrella of Bayesian inference, this article introduces multivariate joint (MVJ) models with a skewed distribution for multiple longitudinal exposures in an attempt to cope with correlated multiple longitudinal outcomes, adjust departures from normality, and tailor linkage in specifying a time-to-event process. We develop a Bayesian joint modeling approach to MVJ models that couples a multivariate linear mixed-effects (MLME) model with the skew-normal (SN) distribution and a Cox proportional hazards model. Our proposed models and method are evaluated by simulation studies and are applied to a real example from a diabetes study.},
  archive      = {J_FDATA},
  author       = {Huang, Yangxin and Chen, Jiaqing and Xu, Lan and Tang, Nian-Sheng},
  doi          = {10.3389/fdata.2022.812725},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {812725},
  shortjournal = {Front. Big Data},
  title        = {Bayesian joint modeling of multivariate longitudinal and survival data with an application to diabetes study},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When patients recover from COVID-19: Data-driven insights
from wearable technologies. <em>FDATA</em>, <em>5</em>, 801998. (<a
href="https://doi.org/10.3389/fdata.2022.801998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) is known as a contagious disease and caused an overwhelming of hospital resources worldwide. Therefore, deciding on hospitalizing COVID-19 patients or quarantining them at home becomes a crucial solution to manage an extremely big number of patients in a short time. This paper proposes a model which combines Long-short Term Memory (LSTM) and Deep Neural Network (DNN) to early and accurately classify disease stages of the patients to address the problem at a low cost. In this model, the LSTM component will exploit temporal features while the DNN component extracts attributed features to enhance the model&#39;s classification performance. Our experimental results demonstrate that the proposed model achieves substantially better prediction accuracy than existing state-of-art methods. Moreover, we explore the importance of different vital indicators to help patients and doctors identify the critical factors at different COVID-19 stages. Finally, we create case studies demonstrating the differences between severe and mild patients and show the signs of recovery from COVID-19 disease by extracting shape patterns based on temporal features of patients. In summary, by identifying the disease stages, this research will help patients understand their current disease situation. Furthermore, it will also help doctors to provide patients with an immediate treatment plan remotely that addresses their specific disease stages, thus optimizing their usage of limited medical resources.},
  archive      = {J_FDATA},
  author       = {Guo, Muzhe and Nguyen, Long and Du, Hongfei and Jin, Fang},
  doi          = {10.3389/fdata.2022.801998},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {801998},
  shortjournal = {Front. Big Data},
  title        = {When patients recover from COVID-19: Data-driven insights from wearable technologies},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Applications and techniques for fast machine learning in
science. <em>FDATA</em>, <em>5</em>, 787421. (<a
href="https://doi.org/10.3389/fdata.2022.787421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this community review report, we discuss applications and techniques for fast machine learning (ML) in science—the concept of integrating powerful ML methods into the real-time experimental data processing loop to accelerate scientific discovery. The material for the report builds on two workshops held by the Fast ML for Science community and covers three main areas: applications for fast ML across a number of scientific domains; techniques for training and implementing performant and resource-efficient ML algorithms; and computing architectures, platforms, and technologies for deploying these algorithms. We also present overlapping challenges across the multiple scientific domains where common solutions can be found. This community report is intended to give plenty of examples and inspiration for scientific discovery through integrated and accelerated ML solutions. This is followed by a high-level overview and organization of technical advances, including an abundance of pointers to source material, which can enable these breakthroughs.},
  archive      = {J_FDATA},
  author       = {Deiana, Allison McCarn and Tran, Nhan and Agar, Joshua and Blott, Michaela and Di Guglielmo, Giuseppe and Duarte, Javier and Harris, Philip and Hauck, Scott and Liu, Mia and Neubauer, Mark S. and Ngadiuba, Jennifer and Ogrenci-Memik, Seda and Pierini, Maurizio and Aarrestad, Thea and Bähr, Steffen and Becker, Jürgen and Berthold, Anne-Sophie and Bonventre, Richard J. and Müller Bravo, Tomás E. and Diefenthaler, Markus and Dong, Zhen and Fritzsche, Nick and Gholami, Amir and Govorkova, Ekaterina and Guo, Dongning and Hazelwood, Kyle J. and Herwig, Christian and Khan, Babar and Kim, Sehoon and Klijnsma, Thomas and Liu, Yaling and Lo, Kin Ho and Nguyen, Tri and Pezzullo, Gianantonio and Rasoulinezhad, Seyedramin and Rivera, Ryan A. and Scholberg, Kate and Selig, Justin and Sen, Sougata and Strukov, Dmitri and Tang, William and Thais, Savannah and Unger, Kai Lukas and Vilalta, Ricardo and von Krosigk, Belina and Wang, Shen and Warburton, Thomas K.},
  doi          = {10.3389/fdata.2022.787421},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {787421},
  shortjournal = {Front. Big Data},
  title        = {Applications and techniques for fast machine learning in science},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on spatial-temporal spread and risk profile of the
COVID-19 epidemic based on mobile phone trajectory data. <em>FDATA</em>,
<em>5</em>, 705698. (<a
href="https://doi.org/10.3389/fdata.2022.705698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 epidemic poses a significant challenge to the operation of society and the resumption of work and production. How to quickly track the resident location and activity trajectory of the population, and identify the spread risk of the COVID-19 in geospatial space has important theoretical and practical significance for controlling the spread of the virus on a large scale. In this study, we take the geographical community as the research object, and use the mobile phone trajectory data to construct the spatiotemporal profile of the potential high-risk population. First, by using the spatiotemporal data collision method, identify, and recover the trajectories of the people who were in the same area with the confirmed patients during the same time. Then, based on the range of activities of both cohorts (the confirmed cases and the potentially infected groups), we analyze the risk level of the relevant places and evaluate the scale of potential spread. Finally, we calculate the probability of infection for different communities and construct the spatiotemporal profile for the transmission to help guide the distribution of preventive materials and human resources. The proposed method is verified using survey data of 10 confirmed cases and statistical data of 96 high-risk neighborhoods in Chengdu, China, between 15 January 2020 and 15 February 2020. The analysis finds that the method accurately simulates the spatiotemporal spread of the epidemic in Chengdu and measures the risk level in specific areas, which provides an objective basis for the government and relevant parties to plan and manage the prevention and control of the epidemic.},
  archive      = {J_FDATA},
  author       = {Zuo, Qi and Du, Jiaman and Di, Baofeng and Zhou, Junrong and Zhang, Lixia and Liu, Hongxia and Hou, Xiaoyu},
  doi          = {10.3389/fdata.2022.705698},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {705698},
  shortjournal = {Front. Big Data},
  title        = {Research on spatial-temporal spread and risk profile of the COVID-19 epidemic based on mobile phone trajectory data},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contagion dynamics for manifold learning. <em>FDATA</em>,
<em>5</em>, 668356. (<a
href="https://doi.org/10.3389/fdata.2022.668356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contagion maps exploit activation times in threshold contagions to assign vectors in high-dimensional Euclidean space to the nodes of a network. A point cloud that is the image of a contagion map reflects both the structure underlying the network and the spreading behavior of the contagion on it. Intuitively, such a point cloud exhibits features of the network&#39;s underlying structure if the contagion spreads along that structure, an observation which suggests contagion maps as a viable manifold-learning technique. We test contagion maps and variants thereof as a manifold-learning tool on a number of different synthetic and real-world data sets, and we compare their performance to that of Isomap, one of the most well-known manifold-learning algorithms. We find that, under certain conditions, contagion maps are able to reliably detect underlying manifold structure in noisy data, while Isomap fails due to noise-induced error. This consolidates contagion maps as a technique for manifold learning. We also demonstrate that processing distance estimates between data points before performing methods to determine geometry, topology and dimensionality of a data set leads to clearer results for both Isomap and contagion maps.},
  archive      = {J_FDATA},
  author       = {Mahler, Barbara I.},
  doi          = {10.3389/fdata.2022.668356},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {668356},
  shortjournal = {Front. Big Data},
  title        = {Contagion dynamics for manifold learning},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scaling graph propagation kernels for predictive learning.
<em>FDATA</em>, <em>5</em>, 616617. (<a
href="https://doi.org/10.3389/fdata.2022.616617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world applications deal with data that have an underlying graph structure associated with it. To perform downstream analysis on such data, it is crucial to capture relational information of nodes over their expanded neighborhood efficiently. Herein, we focus on the problem of Collective Classification (CC) for assigning labels to unlabeled nodes. Most deep learning models for CC heavily rely on differentiable variants of Weisfeiler-Lehman (WL) kernels. However, due to current computing architectures&#39; limitations, WL kernels and their differentiable variants are limited in their ability to capture useful relational information only over a small expanded neighborhood of a node. To address this concern, we propose the framework, I-HOP, that couples differentiable kernels with an iterative inference mechanism to scale to larger neighborhoods. I-HOP scales differentiable graph kernels to capture and summarize information from a larger neighborhood in each iteration by leveraging a historical neighborhood summary obtained in the previous iteration. This recursive nature of I-HOP provides an exponential reduction in time and space complexity over straightforward differentiable graph kernels. Additionally, we point out a limitation of WL kernels where the node&#39;s original information is decayed exponentially with an increase in neighborhood size and provide a solution to address it. Finally, extensive evaluation across 11 datasets showcases the improved results and robustness of our proposed iterative framework, I-HOP.},
  archive      = {J_FDATA},
  author       = {Vijayan, Priyesh and Chandak, Yash and Khapra, Mitesh M. and Parthasarathy, Srinivasan and Ravindran, Balaraman},
  doi          = {10.3389/fdata.2022.616617},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {616617},
  shortjournal = {Front. Big Data},
  title        = {Scaling graph propagation kernels for predictive learning},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Advanced deep learning methods for biomedical
information analysis (ADLMBIA). <em>FDATA</em>, <em>5</em>, 863060. (<a
href="https://doi.org/10.3389/fdata.2022.863060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Zhang, Yu-Dong and Li, Shuai and Cattani, Carlo and Wang, Shui-Hua},
  doi          = {10.3389/fdata.2022.863060},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {863060},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Advanced deep learning methods for biomedical information analysis (ADLMBIA)},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey of data quality measurement and monitoring tools.
<em>FDATA</em>, <em>5</em>, 850611. (<a
href="https://doi.org/10.3389/fdata.2022.850611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality data is key to interpretable and trustworthy data analytics and the basis for meaningful data-driven decisions. In practical scenarios, data quality is typically associated with data preprocessing, profiling, and cleansing for subsequent tasks like data integration or data analytics. However, from a scientific perspective, a lot of research has been published about the measurement (i.e., the detection) of data quality issues and different generally applicable data quality dimensions and metrics have been discussed. In this work, we close the gap between data quality research and practical implementations with a detailed investigation on how data quality measurement and monitoring concepts are implemented in state-of-the-art tools. For the first time and in contrast to all existing data quality tool surveys, we conducted a systematic search, in which we identified 667 software tools dedicated to “data quality.” To evaluate the tools, we compiled a requirements catalog with three functionality areas: (1) data profiling, (2) data quality measurement in terms of metrics, and (3) automated data quality monitoring. Using a set of predefined exclusion criteria, we selected 13 tools (8 commercial and 5 open-source tools) that provide the investigated features and are not limited to a specific domain for detailed investigation. On the one hand, this survey allows a critical discussion of concepts that are widely accepted in research, but hardly implemented in any tool observed, for example, generally applicable data quality metrics. On the other hand, it reveals potential for functional enhancement of data quality tools and supports practitioners in the selection of appropriate tools for a given use case.},
  archive      = {J_FDATA},
  author       = {Ehrlinger, Lisa and Wöß, Wolfram},
  doi          = {10.3389/fdata.2022.850611},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {850611},
  shortjournal = {Front. Big Data},
  title        = {A survey of data quality measurement and monitoring tools},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Untangling classification methods for melanoma skin cancer.
<em>FDATA</em>, <em>5</em>, 848614. (<a
href="https://doi.org/10.3389/fdata.2022.848614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is the most common cancer in the USA, and it is a leading cause of death worldwide. Every year, more than five million patients are newly diagnosed in the USA. The deadliest and most serious form of skin cancer is called melanoma. Skin cancer can affect anyone, regardless of skin color, race, gender, and age. The diagnosis of melanoma has been done by visual examination and manual techniques by skilled doctors. It is a time-consuming process and highly prone to error. The skin images captured by dermoscopy eliminate the surface reflection of skin and give a better visualization of deeper levels of the skin. However, the existence of many artifacts and noise such as hair, veins, and water residue make the lesion images very complex. Due to the complexity of images, the border detection, feature extraction, and classification process are challenging. Without a proper mechanism, it is hard to identify and predict melanoma at an early stage. Therefore, there is a need to provide precise details, identify early skin cancer, and classify skin cancer with appropriate sensitivity and precision. This article aims to review and analyze two deep neural network-based classification algorithms (convolutional neural network, CNN; recurrent neural network, RNN) and a decision tree-based algorithm (XG-Boost) on skin lesion images (ISIC dataset) and find which of these provides the best classification performance metric. Also, the performance of algorithms is compared using six different metrics—loss, accuracy, precision, recall, F1 score, and ROC.},
  archive      = {J_FDATA},
  author       = {Kumar, Ayushi and Vatsa, Avimanyou},
  doi          = {10.3389/fdata.2022.848614},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {848614},
  shortjournal = {Front. Big Data},
  title        = {Untangling classification methods for melanoma skin cancer},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: The computational analysis of cultural conflicts.
<em>FDATA</em>, <em>5</em>, 840584. (<a
href="https://doi.org/10.3389/fdata.2022.840584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Törnberg, Petter and Olbrich, Eckehard and Uitermark, Justus},
  doi          = {10.3389/fdata.2022.840584},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {840584},
  shortjournal = {Front. Big Data},
  title        = {Editorial: The computational analysis of cultural conflicts},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AutoLoc: Autonomous sensor location configuration via cross
modal sensing. <em>FDATA</em>, <em>5</em>, 835949. (<a
href="https://doi.org/10.3389/fdata.2022.835949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-Things (IoT) systems have become pervasive for smart homes. In recent years, many of these IoT sensing systems are developed to enable in-home long-term monitoring applications, such as personalized services in smart homes, elderly/patient monitoring, etc. However, these systems often require complicated and expensive installation processes, which are some of the main concerns affecting users&#39; adoption of smart home systems. In this work, we focus on floor vibration-based occupant monitoring systems, which enables non-intrusive in-home continuous occupant monitoring, such as patient step tracking and gait analysis. However, to enable these applications, the system would require known locations of vibration sensors placed in the environment. Current practice relies on manually input of location, which makes the installation labor-intensive, time consuming, and expensive. On the other hand, without known location of vibration sensors, the output of the system does not have intuitive physical meaning and is incomprehensive to users, which limits the systems&#39; usability. We present AutoLoc, a scheme to estimate the location of the vibration sensors in a two-dimensional space in the view of a nearby camera, which has spatial physical meaning. AutoLoc utilizes occupants&#39; walking events captured by both vibration sensors and the co-located camera to estimate the vibration sensors&#39; location in the camera view. First, AutoLoc detects and localizes the occupant&#39;s footsteps in the vision data. Then, it associates the time and location of the event to the floor vibration data. Next, the extracted vibration data of the given event from multiple vibration sensors are used to estimate the sensors&#39; locations in the camera view coordinates. We conducted real-world experiments and achieved up to 0.07 meters localization accuracy.},
  archive      = {J_FDATA},
  author       = {Rohal, Shubham and Zhang, Yue and Ruiz, Carlos and Pan, Shijia},
  doi          = {10.3389/fdata.2022.835949},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {835949},
  shortjournal = {Front. Big Data},
  title        = {AutoLoc: Autonomous sensor location configuration via cross modal sensing},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph neural networks for charged particle tracking on
FPGAs. <em>FDATA</em>, <em>5</em>, 828666. (<a
href="https://doi.org/10.3389/fdata.2022.828666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of charged particle trajectories in collisions at the CERN Large Hadron Collider (LHC) is an important but challenging problem, especially in the high interaction density conditions expected during the future high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a type of geometric deep learning algorithm that has successfully been applied to this task by embedding tracker data as a graph—nodes represent hits, while edges represent possible track segments—and classifying the edges as true or fake track segments. However, their study in hardware- or software-based trigger applications has been limited due to their large computational cost. In this paper, we introduce an automated translation workflow, integrated into a broader tool called hls4ml, for converting GNNs into firmware for field-programmable gate arrays (FPGAs). We use this translation tool to implement GNNs for charged particle tracking, trained using the TrackML challenge dataset, on FPGAs with designs targeting different graph sizes, task complexites, and latency/throughput requirements. This work could enable the inclusion of charged particle tracking GNNs at the trigger level for HL-LHC experiments.},
  archive      = {J_FDATA},
  author       = {Elabd, Abdelrahman and Razavimaleki, Vesal and Huang, Shi-Yu and Duarte, Javier and Atkinson, Markus and DeZoort, Gage and Elmer, Peter and Hauck, Scott and Hu, Jin-Xuan and Hsu, Shih-Chieh and Lai, Bo-Cheng and Neubauer, Mark and Ojalvo, Isobel and Thais, Savannah and Trahms, Matthew},
  doi          = {10.3389/fdata.2022.828666},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {828666},
  shortjournal = {Front. Big Data},
  title        = {Graph neural networks for charged particle tracking on FPGAs},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Air quality forecast by statistical methods: Application to
portugal and macao. <em>FDATA</em>, <em>5</em>, 826517. (<a
href="https://doi.org/10.3389/fdata.2022.826517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution is a major concern issue for most countries in the world. In Portugal and Macao, the values of nitrogen dioxide (NO2), particulate matter (PM) and ozone (O3) are frequently above the concentration thresholds accepted as “good air quality.” Portugal follows the European Union (EU) legislation (Directive 2008/50/EC) on air quality and Macao the air quality guidelines (AQG) from the WHO. Air quality forecasts are very important mitigation tools because of their ability to anticipate pollution events, and issue early warnings, allowing to take preventive measures and reduce impacts, by avoiding exposure. The work presented here refers to the statistical forecast of air pollutants for three regions: Greater Lisbon Area, Madeira Autonomous Region (both located in Portugal), and Macao Special Administrative Region (in Southern China). The presented statistical approach combines Classification and Regression Tree (CART) and multiple regression (MR) analysis to obtain optimized regression models. This consolidated methodology is now in operation for more than a decade in Portugal, and is subject to regular updates that reflect the ongoing research and the changes in the air quality monitoring network. Recently, the same methodology was applied to Macao in collaboration with the Macao Meteorological and Geophysical Bureau (SMG). Here, a statistical approach for air quality forecasting is described that has been proven to be successful, being able to forecast PM10, PM2.5, NO2, and O3 concentrations, for the next day, with a good performance. In general, all the models have shown a good agreement between the observed and forecasted concentrations (with R2 from 0.50 to 0.89), and were able to follow the concentration evolution trend. For some cases, there is a slight delay in the prediction trend. Moreover, the results obtained for pollution episodes have proven that statistical forecast can be an effective way of protecting public health.},
  archive      = {J_FDATA},
  author       = {Mendes, Luísa and Monjardino, Joana and Ferreira, Francisco},
  doi          = {10.3389/fdata.2022.826517},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {826517},
  shortjournal = {Front. Big Data},
  title        = {Air quality forecast by statistical methods: Application to portugal and macao},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven framework for understanding and predicting air
quality in urban areas. <em>FDATA</em>, <em>5</em>, 822573. (<a
href="https://doi.org/10.3389/fdata.2022.822573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring, predicting, and controlling the air quality in urban areas is one of the effective solutions for tackling the climate change problem. Leveraging the availability of big data in different domains like pollutant concentration, urban traffic, aerial imagery of terrains and vegetation, and weather conditions can aid in understanding the interactions between these factors and building a reliable air quality prediction model. This research proposes a novel cost-effective and efficient air quality modeling framework including all these factors employing state-of-the-art artificial intelligence techniques. The framework also includes a novel deep learning-based vegetation detection system using aerial images. The pilot study conducted in the UK city of Cambridge using the proposed framework investigates various predictive models ranging from statistical to machine learning and deep recurrent neural network models. This framework opens up possibilities of broadening air quality modeling and prediction to other domains like vegetation or green space planning or green traffic routing for sustainable urban cities. The research is mainly focused on extracting strong pieces of evidence which could be useful in proposing better policies around climate change.},
  archive      = {J_FDATA},
  author       = {Babu Saheer, Lakshmi and Bhasy, Ajay and Maktabdar, Mahdi and Zarrin, Javad},
  doi          = {10.3389/fdata.2022.822573},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {822573},
  shortjournal = {Front. Big Data},
  title        = {Data-driven framework for understanding and predicting air quality in urban areas},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A balanced scorecard for maximizing data performance.
<em>FDATA</em>, <em>5</em>, 821103. (<a
href="https://doi.org/10.3389/fdata.2022.821103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A good performance monitoring system is crucial to knowing whether an organization&#39;s efforts are making their data capabilities better, the same, or worse. However, comprehensive performance measurements are costly. Organizations need to expend time, resources, and personnel to design the metrics, to gather evidence for the metrics, to assess the metrics&#39; value, and to determine if any actions should be taken as a result of those metrics. Consequently organizations need to be strategic in selecting their portfolio of performance indicators for evaluating how well their data initiatives are producing value to the organization. This paper proposes a balanced scorecard approach to aid organizations in designing a set of meaningful and coordinated metrics for maximizing the potential of their data assets. This paper also discusses implementation challenges and the need for further research in this area.},
  archive      = {J_FDATA},
  author       = {Pierce, Elizabeth},
  doi          = {10.3389/fdata.2022.821103},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {821103},
  shortjournal = {Front. Big Data},
  title        = {A balanced scorecard for maximizing data performance},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Statistical emulation of neural simulators: Application to
neocortical l2/3 large basket cells. <em>FDATA</em>, <em>5</em>, 789962.
(<a href="https://doi.org/10.3389/fdata.2022.789962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific systems are studied using computer codes that simulate the phenomena of interest. Computer simulation enables scientists to study a broad range of possible conditions, generating large quantities of data at a faster rate than the laboratory. Computer models are widespread in neuroscience, where they are used to mimic brain function at different levels. These models offer a variety of new possibilities for the neuroscientist, but also numerous challenges, such as: where to sample the input space for the simulator, how to make sense of the data that is generated, and how to estimate unknown parameters in the model. Statistical emulation can be a valuable complement to simulator-based research. Emulators are able to mimic the simulator, often with a much smaller computational burden and they are especially valuable for parameter estimation, which may require many simulator evaluations. This work compares different statistical models that address these challenges, and applies them to simulations of neocortical L2/3 large basket cells, created and run with the NEURON simulator in the context of the European Human Brain Project. The novelty of our approach is the use of fast empirical emulators, which have the ability to accelerate the optimization process for the simulator and to identify which inputs (in this case, different membrane ion channels) are most influential in affecting simulated features. These contributions are complementary, as knowledge of the important features can further improve the optimization process. Subsequent research, conducted after the process is completed, will gain efficiency by focusing on these inputs.},
  archive      = {J_FDATA},
  author       = {Shapira, Gilad and Marcus-Kalish, Mira and Amsalem, Oren and Van Geit, Werner and Segev, Idan and Steinberg, David M.},
  doi          = {10.3389/fdata.2022.789962},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {789962},
  shortjournal = {Front. Big Data},
  title        = {Statistical emulation of neural simulators: Application to neocortical l2/3 large basket cells},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Human-centric security and privacy.
<em>FDATA</em>, <em>5</em>, 848058. (<a
href="https://doi.org/10.3389/fdata.2022.848058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Nepal, Surya and Ko, Ryan K. L. and Grobler, Marthie and Camp, L. Jean},
  doi          = {10.3389/fdata.2022.848058},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {848058},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Human-centric security and privacy},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Towards exascale solutions for big data
computing. <em>FDATA</em>, <em>5</em>, 838097. (<a
href="https://doi.org/10.3389/fdata.2022.838097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Talia, Domenico and Trunfio, Paolo and Carretero, Jesus and Garcia-Blas, Javier},
  doi          = {10.3389/fdata.2022.838097},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {838097},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Towards exascale solutions for big data computing},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mobility signatures: A tool for characterizing cities using
intercity mobility flows. <em>FDATA</em>, <em>5</em>, 822889. (<a
href="https://doi.org/10.3389/fdata.2022.822889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the patterns of human mobility between cities has various applications from transport engineering to spatial modeling of the spreading of contagious diseases. We adopt a city-centric, data-driven perspective to quantify such patterns and introduce the mobility signature as a tool for understanding how a city (or a region) is embedded in the wider mobility network. We demonstrate the potential of the mobility signature approach through two applications that build on mobile-phone-based data from Finland. First, we use mobility signatures to show that the well-known radiation model is more accurate for mobility flows associated with larger Finnish cities, while the traditional gravity model appears a better fit for less populated areas. Second, we illustrate how the SARS-CoV-2 pandemic disrupted the mobility patterns in Finland in the spring of 2020. These two cases demonstrate the ability of the mobility signatures to quickly capture features of mobility flows that are harder to extract using more traditional methods.},
  archive      = {J_FDATA},
  author       = {Astero, Maryam and Huang, Zhiren and Saramäki, Jari},
  doi          = {10.3389/fdata.2022.822889},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {822889},
  shortjournal = {Front. Big Data},
  title        = {Mobility signatures: A tool for characterizing cities using intercity mobility flows},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balancing gender bias in job advertisements with text-level
bias mitigation. <em>FDATA</em>, <em>5</em>, 805713. (<a
href="https://doi.org/10.3389/fdata.2022.805713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite progress toward gender equality in the labor market over the past few decades, gender segregation in labor force composition and labor market outcomes persists. Evidence has shown that job advertisements may express gender preferences, which may selectively attract potential job candidates to apply for a given post and thus reinforce gendered labor force composition and outcomes. Removing gender-explicit words from job advertisements does not fully solve the problem as certain implicit traits are more closely associated with men, such as ambitiousness, while others are more closely associated with women, such as considerateness. However, it is not always possible to find neutral alternatives for these traits, making it hard to search for candidates with desired characteristics without entailing gender discrimination. Existing algorithms mainly focus on the detection of the presence of gender biases in job advertisements without providing a solution to how the text should be (re)worded. To address this problem, we propose an algorithm that evaluates gender bias in the input text and provides guidance on how the text should be debiased by offering alternative wording that is closely related to the original input. Our proposed method promises broad application in the human resources process, ranging from the development of job advertisements to algorithm-assisted screening of job applications.},
  archive      = {J_FDATA},
  author       = {Hu, Shenggang and Al-Ani, Jabir Alshehabi and Hughes, Karen D. and Denier, Nicole and Konnikov, Alla and Ding, Lei and Xie, Jinhan and Hu, Yang and Tarafdar, Monideepa and Jiang, Bei and Kong, Linglong and Dai, Hongsheng},
  doi          = {10.3389/fdata.2022.805713},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {805713},
  shortjournal = {Front. Big Data},
  title        = {Balancing gender bias in job advertisements with text-level bias mitigation},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving variational autoencoders for new physics detection
at the LHC with normalizing flows. <em>FDATA</em>, <em>5</em>, 803685.
(<a href="https://doi.org/10.3389/fdata.2022.803685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how to improve new physics detection strategies exploiting variational autoencoders and normalizing flows for anomaly detection at the Large Hadron Collider. As a working example, we consider the DarkMachines challenge dataset. We show how different design choices (e.g., event representations, anomaly score definitions, network architectures) affect the result on specific benchmark new physics models. Once a baseline is established, we discuss how to improve the anomaly detection accuracy by exploiting normalizing flow layers in the latent space of the variational autoencoder.},
  archive      = {J_FDATA},
  author       = {Jawahar, Pratik and Aarrestad, Thea and Chernyavskaya, Nadezda and Pierini, Maurizio and Wozniak, Kinga A. and Ngadiuba, Jennifer and Duarte, Javier and Tsan, Steven},
  doi          = {10.3389/fdata.2022.803685},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {803685},
  shortjournal = {Front. Big Data},
  title        = {Improving variational autoencoders for new physics detection at the LHC with normalizing flows},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental study on the scalability of recent node
centrality metrics in sparse complex networks. <em>FDATA</em>,
<em>5</em>, 797584. (<a
href="https://doi.org/10.3389/fdata.2022.797584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node centrality measures are among the most commonly used analytical techniques for networks. They have long helped analysts to identify “important” nodes that hold power in a social context, where damages could have dire consequences for transportation applications, or who should be a focus for prevention in epidemiology. Given the ubiquity of network data, new measures have been proposed, occasionally motivated by emerging applications or by the ability to interpolate existing measures. Before analysts use these measures and interpret results, the fundamental question is: are these measures likely to complete within the time window allotted to the analysis? In this paper, we comprehensively examine how the time necessary to run 18 new measures (introduced from 2005 to 2020) scales as a function of the number of nodes in the network. Our focus is on giving analysts a simple and practical estimate for sparse networks. As the time consumption depends on the properties in the network, we nuance our analysis by considering whether the network is scale-free, small-world, or random. Our results identify that several metrics run in the order of O(nlogn) and could scale to large networks, whereas others can require O(n2) or O(n3) and may become prime targets in future works for approximation algorithms or distributed implementations.},
  archive      = {J_FDATA},
  author       = {Freund, Alexander J. and Giabbanelli, Philippe J.},
  doi          = {10.3389/fdata.2022.797584},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {797584},
  shortjournal = {Front. Big Data},
  title        = {An experimental study on the scalability of recent node centrality metrics in sparse complex networks},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Network models and simulation analytics for multi-scale
dynamics of biological invasions. <em>FDATA</em>, <em>5</em>, 796897.
(<a href="https://doi.org/10.3389/fdata.2022.796897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globalization and climate change facilitate the spread and establishment of invasive species throughout the world via multiple pathways. These spread mechanisms can be effectively represented as diffusion processes on multi-scale, spatial networks. Such network-based modeling and simulation approaches are being increasingly applied in this domain. However, these works tend to be largely domain-specific, lacking any graph theoretic formalisms, and do not take advantage of more recent developments in network science. This work is aimed toward filling some of these gaps. We develop a generic multi-scale spatial network framework that is applicable to a wide range of models developed in the literature on biological invasions. A key question we address is the following: how do individual pathways and their combinations influence the rate and pattern of spread? The analytical complexity arises more from the multi-scale nature and complex functional components of the networks rather than from the sizes of the networks. We present theoretical bounds on the spectral radius and the diameter of multi-scale networks. These two structural graph parameters have established connections to diffusion processes. Specifically, we study how network properties, such as spectral radius and diameter are influenced by model parameters. Further, we analyze a multi-pathway diffusion model from the literature by conducting simulations on synthetic and real-world networks and then use regression tree analysis to identify the important network and diffusion model parameters that influence the dynamics.},
  archive      = {J_FDATA},
  author       = {Adiga, Abhijin and Palmer, Nicholas and Baek, Young Yun and Mortveit, Henning and Ravi, S. S.},
  doi          = {10.3389/fdata.2022.796897},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {796897},
  shortjournal = {Front. Big Data},
  title        = {Network models and simulation analytics for multi-scale dynamics of biological invasions},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a machine learning approach for local-scale
ozone forecasting: Application to kennewick, WA. <em>FDATA</em>,
<em>5</em>, 781309. (<a
href="https://doi.org/10.3389/fdata.2022.781309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chemical transport models (CTMs) are widely used for air quality forecasts, but these models require large computational resources and often suffer from a systematic bias that leads to missed poor air pollution events. For example, a CTM-based operational forecasting system for air quality over the Pacific Northwest, called AIRPACT, uses over 100 processors for several hours to provide 48-h forecasts daily, but struggles to capture unhealthy O3 episodes during the summer and early fall, especially over Kennewick, WA. This research developed machine learning (ML) based O3 forecasts for Kennewick, WA to demonstrate an improved forecast capability. We used the 2017–2020 simulated meteorology and O3 observation data from Kennewick as training datasets. The meteorology datasets are from the Weather Research and Forecasting (WRF) meteorological model forecasts produced daily by the University of Washington. Our ozone forecasting system consists of two ML models, ML1 and ML2, to improve predictability: ML1 uses the random forest (RF) classifier and multiple linear regression (MLR) models, and ML2 uses a two-phase RF regression model with best-fit weighting factors. To avoid overfitting, we evaluate the ML forecasting system with the 10-time, 10-fold, and walk-forward cross-validation analysis. Compared to AIRPACT, ML1 improved forecast skill for high-O3 events and captured 5 out of 10 unhealthy O3 events, while AIRPACT and ML2 missed all the unhealthy events. ML2 showed better forecast skill for less elevated-O3 events. Based on this result, we set up our ML modeling framework to use ML1 for high-O3 events and ML2 for less elevated O3 events. Since May 2019, the ML modeling framework has been used to produce daily 72-h O3 forecasts and has provided forecasts via the web for clean air agency and public use: http://ozonematters.com/. Compared to the testing period, the operational forecasting period has not had unhealthy O3 events. Nevertheless, the ML modeling framework demonstrated a reliable forecasting capability at a selected location with much less computational resources. The ML system uses a single processor for minutes compared to the CTM-based forecasting system using more than 100 processors for hours.},
  archive      = {J_FDATA},
  author       = {Fan, Kai and Dhammapala, Ranil and Harrington, Kyle and Lamastro, Ryan and Lamb, Brian and Lee, Yunha},
  doi          = {10.3389/fdata.2022.781309},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {781309},
  shortjournal = {Front. Big Data},
  title        = {Development of a machine learning approach for local-scale ozone forecasting: Application to kennewick, WA},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated detection of vaping-related tweets on twitter
during the 2019 EVALI outbreak using machine learning classification.
<em>FDATA</em>, <em>5</em>, 770585. (<a
href="https://doi.org/10.3389/fdata.2022.770585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are increasingly strict regulations surrounding the purchase and use of combustible tobacco products (i.e., cigarettes); simultaneously, the use of other tobacco products, including e-cigarettes (i.e., vaping products), has dramatically increased. However, public attitudes toward vaping vary widely, and the health effects of vaping are still largely unknown. As a popular social media, Twitter contains rich information shared by users about their behaviors and experiences, including opinions on vaping. It is very challenging to identify vaping-related tweets to source useful information manually. In the current study, we proposed to develop a detection model to accurately identify vaping-related tweets using machine learning and deep learning methods. Specifically, we applied seven popular machine learning and deep learning algorithms, including Naïve Bayes, Support Vector Machine, Random Forest, XGBoost, Multilayer Perception, Transformer Neural Network, and stacking and voting ensemble models to build our customized classification model. We extracted a set of sample tweets during an outbreak of e-cigarette or vaping-related lung injury (EVALI) in 2019 and created an annotated corpus to train and evaluate these models. After comparing the performance of each model, we found that the stacking ensemble learning achieved the highest performance with an F1-score of 0.97. All models could achieve 0.90 or higher after tuning hyperparameters. The ensemble learning model has the best average performance. Our study findings provide informative guidelines and practical implications for the automated detection of themed social media data for public opinions and health surveillance purposes.},
  archive      = {J_FDATA},
  author       = {Ren, Yang and Wu, Dezhi and Singh, Avineet and Kasson, Erin and Huang, Ming and Cavazos-Rehg, Patricia},
  doi          = {10.3389/fdata.2022.770585},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {770585},
  shortjournal = {Front. Big Data},
  title        = {Automated detection of vaping-related tweets on twitter during the 2019 EVALI outbreak using machine learning classification},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Defense against explanation manipulation. <em>FDATA</em>,
<em>5</em>, 704203. (<a
href="https://doi.org/10.3389/fdata.2022.704203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable machine learning attracts increasing attention as it improves the transparency of models, which is helpful for machine learning to be trusted in real applications. However, explanation methods have recently been demonstrated to be vulnerable to manipulation, where we can easily change a model&#39;s explanation while keeping its prediction constant. To tackle this problem, some efforts have been paid to use more stable explanation methods or to change model configurations. In this work, we tackle the problem from the training perspective, and propose a new training scheme called Adversarial Training on EXplanations (ATEX) to improve the internal explanation stability of a model regardless of the specific explanation method being applied. Instead of directly specifying explanation values over data instances, ATEX only puts constraints on model predictions which avoids involving second-order derivatives in optimization. As a further discussion, we also find that explanation stability is closely related to another property of the model, i.e., the risk of being exposed to adversarial attack. Through experiments, besides showing that ATEX improves model robustness against manipulation targeting explanation, it also brings additional benefits including smoothing explanations and improving the efficacy of adversarial training if applied to the model.},
  archive      = {J_FDATA},
  author       = {Tang, Ruixiang and Liu, Ninghao and Yang, Fan and Zou, Na and Hu, Xia},
  doi          = {10.3389/fdata.2022.704203},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {704203},
  shortjournal = {Front. Big Data},
  title        = {Defense against explanation manipulation},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regression and classification with spline-based separable
expansions. <em>FDATA</em>, <em>5</em>, 688496. (<a
href="https://doi.org/10.3389/fdata.2022.688496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a supervised learning framework for target functions that are well approximated by a sum of (few) separable terms. The framework proposes to approximate each component function by a B-spline, resulting in an approximant where the underlying coefficient tensor of the tensor product expansion has a low-rank polyadic decomposition parametrization. By exploiting the multilinear structure, as well as the sparsity pattern of the compactly supported B-spline basis terms, we demonstrate how such an approximant is well-suited for regression and classification tasks by using the Gauss–Newton algorithm to train the parameters. Various numerical examples are provided analyzing the effectiveness of the approach.},
  archive      = {J_FDATA},
  author       = {Govindarajan, Nithin and Vervliet, Nico and De Lathauwer, Lieven},
  doi          = {10.3389/fdata.2022.688496},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {688496},
  shortjournal = {Front. Big Data},
  title        = {Regression and classification with spline-based separable expansions},
  volume       = {5},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MODIT: MOtif DIscovery in temporal networks. <em>FDATA</em>,
<em>4</em>, 806014. (<a
href="https://doi.org/10.3389/fdata.2021.806014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal networks are graphs where each edge is linked with a timestamp, denoting when an interaction between two nodes happens. According to the most recently proposed definitions of the problem, motif search in temporal networks consists in finding and counting all connected temporal graphs Q (called motifs) occurring in a larger temporal network T, such that matched target edges follow the same chronological order imposed by edges in Q. In the last few years, several algorithms have been proposed to solve motif search, but most of them are limited to very small or specific motifs due to the computational complexity of the problem. In this paper, we present MODIT (MOtif DIscovery in Temporal Networks), an algorithm for counting motifs of any size in temporal networks, inspired by a very recent algorithm for subgraph isomorphism in temporal networks, called TemporalRI. Experiments show that for big motifs (more than 3 nodes and 3 edges) MODIT can efficiently retrieve them in reasonable time (up to few hours) in many networks of medium and large size and outperforms state-of-the art algorithms.},
  archive      = {J_FDATA},
  author       = {Grasso, Roberto and Micale, Giovanni and Ferro, Alfredo and Pulvirenti, Alfredo},
  doi          = {10.3389/fdata.2021.806014},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {806014},
  shortjournal = {Front. Big Data},
  title        = {MODIT: MOtif DIscovery in temporal networks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Survival analysis of COPD patients in a 13-year nationwide
cohort study of the brazilian national health system. <em>FDATA</em>,
<em>4</em>, 788268. (<a
href="https://doi.org/10.3389/fdata.2021.788268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundChronic obstructive pulmonary disease (COPD) has an appreciable socioeconomical impact in low- and middle-income countries, but most epidemiological data originate from high-income countries. For this reason, it is especially important to understand survival and factors associated with survival in COPD patients in these countries.ObjectiveTo assess survival of COPD patients in Brazil, to identify risk factors associated with overall survival, including treatment options funded by the Brazilian National Health System (SUS).MethodologyWe built a retrospective cohort study of patients dispensed COPD treatment in SUS, from 2003 to 2015 using a National Database created from the record linkage of administrative databases. We further matched patients 1:1 based on sex, age and year of entry to assess the effect of the medicines on patient survival. We used the Kaplan-Meier method to estimate overall survival of patients, and Cox&#39;s model of proportional risks to assess risk factors.ResultThirty seven thousand and nine hundred and thirty eight patients were included. Patient&#39;s survival rates at 1 and 10 years were 97.6% (CI 95% 97.4–97.8) and 83.1% (CI 95% 81.9–84.3), respectively. The multivariate analysis showed that male patients, over 65 years old and underweight had an increased risk of death. Therapeutic regimens containing a bronchodilator in a free dose along with a fixed-dose combination of corticosteroid and bronchodilator seem to be a protective factor when compared to other regimens.ConclusionOur findings contribute to the knowledge of COPD patients&#39; profile, survival rate and related risk factors, providing new evidence that supports the debate about pharmacological therapy and healthcare of these patients.},
  archive      = {J_FDATA},
  author       = {Gargano, Ludmila Peres and Zuppo, Isabella de Figueiredo and Nascimento, Mariana Martins Gonzaga do and Augusto, Valéria Maria and Godman, Brian and Costa, Juliana de Oliveira and Acúrcio, Francisco Assis and Álvares-Teodoro, Juliana and Guerra, Augusto Afonso},
  doi          = {10.3389/fdata.2021.788268},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {788268},
  shortjournal = {Front. Big Data},
  title        = {Survival analysis of COPD patients in a 13-year nationwide cohort study of the brazilian national health system},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Daily spatial complete soil moisture mapping over southeast
china using CYGNSS and MODIS data. <em>FDATA</em>, <em>4</em>, 777336.
(<a href="https://doi.org/10.3389/fdata.2021.777336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily spatial complete soil moisture (SM) mapping is important for climatic, hydrological, and agricultural applications. The Cyclone Global Navigation Satellite System (CYGNSS) is the first constellation that utilizes the L band signal transmitted by the Global Navigation Satellite System (GNSS) satellites to measure SM. Since the CYGNSS points are discontinuously distributed with a relativity low density, limiting it to map continuous SM distributions with high accuracy. The Moderate-Resolution Imaging Spectroradiometer (MODIS) product (i.e., vegetation index [VI] and land surface temperature [LST]) provides more surface SM information than other optical remote sensing data with a relatively high spatial resolution. This study proposes a point-surface fusion method to fuse the CYGNSS and MODIS data for daily spatial complete SM retrieval. First, for CYGNSS data, the surface reflectivity (SR) is proposed as a proxy to evaluate its ability to estimate daily SM. Second, the LST output from the China Meteorological Administration Land Data Assimilation System (CLDAS, 0.0625° × 0.0625°) and MODIS LST (1 × 1 km) are fused to generate spatial complete and temporally continuous LST maps. An Enhanced Normalized Vegetation Supply Water Index (E-NVSWI) model is proposed to estimate SM derived from MODIS data at high spatial resolution. Finally, the final SM estimation model is constructed from the back-propagation artificial neural network (BP-ANN) fusing the CYGNSS point, E-NWSVI data, and ancillary data, and applied to get the daily continuous SM result over southeast China. The results show that the estimation SM are comparable and promising (R = 0.723, root mean squared error [RMSE] = 0.062 m3 m−3, and MAE = 0.040 m3 m−3 vs. in situ, R = 0.714, RMSE = 0.057 m3 m−3, and MAE = 0.039 m3 m−3 vs. CLDAS). The proposed algorithm contributes from two aspects: (1) validates the CYGNSS derived SM by taking advantage of the dense in situ networks over Southeast China; (2) provides a point-surface fusion model to combine the usage of CYGNSS and MODIS to generate the temporal and spatial complete SM. The proposed approach reveals significant potential to map daily spatial complete SM using CYGNSS and MODIS data at a regional scale.},
  archive      = {J_FDATA},
  author       = {Yang, Ting and Sun, Zhigang and Wang, Jundong and Li, Sen},
  doi          = {10.3389/fdata.2021.777336},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {777336},
  shortjournal = {Front. Big Data},
  title        = {Daily spatial complete soil moisture mapping over southeast china using CYGNSS and MODIS data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coming together of bayesian inference and skew spherical
data. <em>FDATA</em>, <em>4</em>, 769726. (<a
href="https://doi.org/10.3389/fdata.2021.769726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Bayesian directional data modeling via the skew-rotationally-symmetric Fisher-von Mises-Langevin (FvML) distribution. The prior distributions for the parameters are a pivotal building block in Bayesian analysis, therefore, the impact of the proposed priors will be quantified using the Wasserstein Impact Measure (WIM) to guide the practitioner in the implementation process. For the computation of the posterior, modifications of Gibbs and slice samplings are applied for generating samples. We demonstrate the applicability of our contribution via synthetic and real data analyses. Our investigation paves the way for Bayesian analysis of skew circular and spherical data.},
  archive      = {J_FDATA},
  author       = {Nakhaei Rad, Najmeh and Bekker, Andriette and Arashi, Mohammad and Ley, Christophe},
  doi          = {10.3389/fdata.2021.769726},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {769726},
  shortjournal = {Front. Big Data},
  title        = {Coming together of bayesian inference and skew spherical data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HPTMT parallel operators for high performance data science
and data engineering. <em>FDATA</em>, <em>4</em>, 756041. (<a
href="https://doi.org/10.3389/fdata.2021.756041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-intensive applications are becoming commonplace in all science disciplines. They are comprised of a rich set of sub-domains such as data engineering, deep learning, and machine learning. These applications are built around efficient data abstractions and operators that suit the applications of different domains. Often lack of a clear definition of data structures and operators in the field has led to other implementations that do not work well together. The HPTMT architecture that we proposed recently, identifies a set of data structures, operators, and an execution model for creating rich data applications that links all aspects of data engineering and data science together efficiently. This paper elaborates and illustrates this architecture using an end-to-end application with deep learning and data engineering parts working together. Our analysis show that the proposed system architecture is better suited for high performance computing environments compared to the current big data processing systems. Furthermore our proposed system emphasizes the importance of efficient compact data structures such as Apache Arrow tabular data representation defined for high performance. Thus the system integration we proposed scales a sequential computation to a distributed computation retaining optimum performance along with highly usable application programming interface.},
  archive      = {J_FDATA},
  author       = {Abeykoon, Vibhatha and Kamburugamuve, Supun and Widanage, Chathura and Perera, Niranda and Uyar, Ahmet and Kanewala, Thejaka Amila and von Laszewski, Gregor and Fox, Geoffrey},
  doi          = {10.3389/fdata.2021.756041},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {756041},
  shortjournal = {Front. Big Data},
  title        = {HPTMT parallel operators for high performance data science and data engineering},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Runoff forecasting using machine-learning methods: Case
study in the middle reaches of xijiang river. <em>FDATA</em>,
<em>4</em>, 752406. (<a
href="https://doi.org/10.3389/fdata.2021.752406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Runoff forecasting is useful for flood early warning and water resource management. In this study, backpropagation (BP) neural network, generalized regression neural network (GRNN), extreme learning machine (ELM), and wavelet neural network (WNN) models were employed, and a high-accuracy runoff forecasting model was developed at Wuzhou station in the middle reaches of Xijiang River. The GRNN model was selected as the optimal runoff forecasting model and was also used to predict the streamflow and water level by considering the flood propagation time. Results show that (1) the GRNN presents the best performance in the 7-day lead time of streamflow; (2) the WNN model shows the highest accuracy in the 7-day lead time of water level; (3) the GRNN model performs well in runoff forecasting by considering flood propagation time, increasing the Qualification Rate (QR) of mean streamflow and water level forecast to 98.36 and 82.74%, respectively, and illustrates scientifically of the peak underestimation in streamflow and water level. This research proposes a high-accuracy runoff forecasting model using machine learning, which would improve the early warning capabilities of floods and droughts, the results also lay an important foundation for the mid-long-term runoff forecasting.},
  archive      = {J_FDATA},
  author       = {Xiao, Lu and Zhong, Ming and Zha, Dawei},
  doi          = {10.3389/fdata.2021.752406},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {752406},
  shortjournal = {Front. Big Data},
  title        = {Runoff forecasting using machine-learning methods: Case study in the middle reaches of xijiang river},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Big data analytics for precision health and
prevention. <em>FDATA</em>, <em>4</em>, 835353. (<a
href="https://doi.org/10.3389/fdata.2021.835353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Capobianco, Enrico and Deng, Jun},
  doi          = {10.3389/fdata.2021.835353},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {835353},
  shortjournal = {Front. Big Data},
  title        = {Editorial: Big data analytics for precision health and prevention},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Corrigendum: The rise of populism and the reconfiguration of
the german political space. <em>FDATA</em>, <em>4</em>, 833037. (<a
href="https://doi.org/10.3389/fdata.2021.833037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Olbrich, Eckehard and Banisch, Sven},
  doi          = {10.3389/fdata.2021.833037},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {833037},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: The rise of populism and the reconfiguration of the german political space},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Educational anomaly analytics: Features, methods, and
challenges. <em>FDATA</em>, <em>4</em>, 811840. (<a
href="https://doi.org/10.3389/fdata.2021.811840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies in education affect the personal careers of students and universities&#39; retention rates. Understanding the laws behind educational anomalies promotes the development of individual students and improves the overall quality of education. However, the inaccessibility of educational data hinders the development of the field. Previous research in this field used questionnaires, which are time- and cost-consuming and hardly applicable to large-scale student cohorts. With the popularity of educational management systems and the rise of online education during the prevalence of COVID-19, a large amount of educational data is available online and offline, providing an unprecedented opportunity to explore educational anomalies from a data-driven perspective. As an emerging field, educational anomaly analytics rapidly attracts scholars from a variety of fields, including education, psychology, sociology, and computer science. This paper intends to provide a comprehensive review of data-driven analytics of educational anomalies from a methodological standpoint. We focus on the following five types of research that received the most attention: course failure prediction, dropout prediction, mental health problems detection, prediction of difficulty in graduation, and prediction of difficulty in employment. Then, we discuss the challenges of current related research. This study aims to provide references for educational policymaking while promoting the development of educational anomaly analytics as a growing field.},
  archive      = {J_FDATA},
  author       = {Guo, Teng and Bai, Xiaomei and Tian, Xue and Firmin, Selena and Xia, Feng},
  doi          = {10.3389/fdata.2021.811840},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {811840},
  shortjournal = {Front. Big Data},
  title        = {Educational anomaly analytics: Features, methods, and challenges},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Correlation-based discovery of disease patterns for
syndromic surveillance. <em>FDATA</em>, <em>4</em>, 784159. (<a
href="https://doi.org/10.3389/fdata.2021.784159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early outbreak detection is a key aspect in the containment of infectious diseases, as it enables the identification and isolation of infected individuals before the disease can spread to a larger population. Instead of detecting unexpected increases of infections by monitoring confirmed cases, syndromic surveillance aims at the detection of cases with early symptoms, which allows a more timely disclosure of outbreaks. However, the definition of these disease patterns is often challenging, as early symptoms are usually shared among many diseases and a particular disease can have several clinical pictures in the early phase of an infection. As a first step toward the goal to support epidemiologists in the process of defining reliable disease patterns, we present a novel, data-driven approach to discover such patterns in historic data. The key idea is to take into account the correlation between indicators in a health-related data source and the reported number of infections in the respective geographic region. In an preliminary experimental study, we use data from several emergency departments to discover disease patterns for three infectious diseases. Our results show the potential of the proposed approach to find patterns that correlate with the reported infections and to identify indicators that are related to the respective diseases. It also motivates the need for additional measures to overcome practical limitations, such as the requirement to deal with noisy and unbalanced data, and demonstrates the importance of incorporating feedback of domain experts into the learning procedure.},
  archive      = {J_FDATA},
  author       = {Rapp, Michael and Kulessa, Moritz and Loza Mencía, Eneldo and Fürnkranz, Johannes},
  doi          = {10.3389/fdata.2021.784159},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {784159},
  shortjournal = {Front. Big Data},
  title        = {Correlation-based discovery of disease patterns for syndromic surveillance},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Top-down machine learning-based architecture for
cyberattacks identification and classification in IoT communication
networks. <em>FDATA</em>, <em>4</em>, 782902. (<a
href="https://doi.org/10.3389/fdata.2021.782902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prompt revolution and emergence of smart, self-reliant, and low-power devices, Internet of Things (IoT) has inconceivably expanded and impacted almost every real-life application. Nowadays, for example, machines and devices are now fully reliant on computer control and, instead, they have their own programmable interfaces, such as cars, unmanned aerial vehicles (UAVs), and medical devices. With this increased use of IoT, attack capabilities have increased in response, which became imperative that new methods for securing these systems be developed to detect attacks launched against IoT devices and gateways. These attacks are usually aimed at accessing, changing, or destroying sensitive information; extorting money from users; or interrupting normal business processes. In this research, we present new efficient and generic top-down architecture for intrusion detection, and classification in IoT networks using non-traditional machine learning is proposed in this article. The proposed architecture can be customized and used for intrusion detection/classification incorporating any IoT cyber-attack datasets, such as CICIDS Dataset, MQTT dataset, and others. Specifically, the proposed system is composed of three subsystems: feature engineering (FE) subsystem, feature learning (FL) subsystem, and detection and classification (DC) subsystem. All subsystems have been thoroughly described and analyzed in this article. Accordingly, the proposed architecture employs deep learning models to enable the detection of slightly mutated attacks of IoT networking with high detection/classification accuracy for the IoT traffic obtained from either real-time system or a pre-collected dataset. Since this work employs the system engineering (SE) techniques, the machine learning technology, the cybersecurity of IoT systems field, and the collective corporation of the three fields have successfully yielded a systematic engineered system that can be implemented with high-performance trajectories.},
  archive      = {J_FDATA},
  author       = {Abu Al-Haija, Qasem},
  doi          = {10.3389/fdata.2021.782902},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {782902},
  shortjournal = {Front. Big Data},
  title        = {Top-down machine learning-based architecture for cyberattacks identification and classification in IoT communication networks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recipe recommendation with hierarchical graph attention
network. <em>FDATA</em>, <em>4</em>, 778417. (<a
href="https://doi.org/10.3389/fdata.2021.778417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recipe recommendation systems play an important role in helping people find recipes that are of their interest and fit their eating habits. Unlike what has been developed for recommending recipes using content-based or collaborative filtering approaches, the relational information among users, recipes, and food items is less explored. In this paper, we leverage the relational information into recipe recommendation and propose a graph learning approach to solve it. In particular, we propose HGAT, a novel hierarchical graph attention network for recipe recommendation. The proposed model can capture user history behavior, recipe content, and relational information through several neural network modules, including type-specific transformation, node-level attention, and relation-level attention. We further introduce a ranking-based objective function to optimize the model. Thorough experiments demonstrate that HGAT outperforms numerous baseline methods.},
  archive      = {J_FDATA},
  author       = {Tian, Yijun and Zhang, Chuxu and Metoyer, Ronald and Chawla, Nitesh V.},
  doi          = {10.3389/fdata.2021.778417},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {778417},
  shortjournal = {Front. Big Data},
  title        = {Recipe recommendation with hierarchical graph attention network},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The effects of gender signals and performance in online
product reviews. <em>FDATA</em>, <em>4</em>, 771404. (<a
href="https://doi.org/10.3389/fdata.2021.771404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work quantifies the effects of signaling gender through gender specific user names, on the success of reviews written on the popular amazon.com shopping platform. Highly rated reviews play an important role in e-commerce since they are prominently displayed next to products. Differences in reviews, perceived—consciously or unconsciously—with respect to gender signals, can lead to crucial biases in determining what content and perspectives are represented among top reviews. To investigate this, we extract signals of author gender from user names to select reviews where the author’s likely gender can be inferred. Using reviews authored by these gender-signaling authors, we train a deep learning classifier to quantify the gendered writing style (i.e., gendered performance) of reviews written by authors who do not send clear gender signals via their user name. We contrast the effects of gender signaling and performance on the review helpfulness ratings using matching experiments. This is aimed at understanding if an advantage is to be gained by (not) signaling one’s gender when posting reviews. While we find no general trend that gendered signals or performances influence overall review success, we find strong context-specific effects. For example, reviews in product categories such as Electronics or Computers are perceived as less helpful when authors signal that they are likely woman, but are received as more helpful in categories such as Beauty or Clothing. In addition to these interesting findings, we believe this general chain of tools could be deployed across various social media platforms.},
  archive      = {J_FDATA},
  author       = {Sikdar, Sandipan and Sachdeva, Rachneet and Wachs, Johannes and Lemmerich, Florian and Strohmaier, Markus},
  doi          = {10.3389/fdata.2021.771404},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {771404},
  shortjournal = {Front. Big Data},
  title        = {The effects of gender signals and performance in online product reviews},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The bayes estimators of the variance and scale parameters of
the normal model with a known mean for the conjugate and noninformative
priors under stein’s loss. <em>FDATA</em>, <em>4</em>, 763925. (<a
href="https://doi.org/10.3389/fdata.2021.763925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the normal model with a known mean, the Bayes estimation of the variance parameter under the conjugate prior is studied in Lehmann and Casella (1998) and Mao and Tang (2012). However, they only calculate the Bayes estimator with respect to a conjugate prior under the squared error loss function. Zhang (2017) calculates the Bayes estimator of the variance parameter of the normal model with a known mean with respect to the conjugate prior under Stein’s loss function which penalizes gross overestimation and gross underestimation equally, and the corresponding Posterior Expected Stein’s Loss (PESL). Motivated by their works, we have calculated the Bayes estimators of the variance parameter with respect to the noninformative (Jeffreys’s, reference, and matching) priors under Stein’s loss function, and the corresponding PESLs. Moreover, we have calculated the Bayes estimators of the scale parameter with respect to the conjugate and noninformative priors under Stein’s loss function, and the corresponding PESLs. The quantities (prior, posterior, three posterior expectations, two Bayes estimators, and two PESLs) and expressions of the variance and scale parameters of the model for the conjugate and noninformative priors are summarized in two tables. After that, the numerical simulations are carried out to exemplify the theoretical findings. Finally, we calculate the Bayes estimators and the PESLs of the variance and scale parameters of the S&amp;amp;P 500 monthly simple returns for the conjugate and noninformative priors.},
  archive      = {J_FDATA},
  author       = {Zhang, Ying-Ying and Rong, Teng-Zhong and Li, Man-Man},
  doi          = {10.3389/fdata.2021.763925},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {763925},
  shortjournal = {Front. Big Data},
  title        = {The bayes estimators of the variance and scale parameters of the normal model with a known mean for the conjugate and noninformative priors under stein’s loss},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preparing distributed computing operations for the HL-LHC
era with operational intelligence. <em>FDATA</em>, <em>4</em>, 753409.
(<a href="https://doi.org/10.3389/fdata.2021.753409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a joint effort from various communities involved in the Worldwide LHC Computing Grid, the Operational Intelligence project aims at increasing the level of automation in computing operations and reducing human interventions. The distributed computing systems currently deployed by the LHC experiments have proven to be mature and capable of meeting the experimental goals, by allowing timely delivery of scientific results. However, a substantial number of interventions from software developers, shifters, and operational teams is needed to efficiently manage such heterogenous infrastructures. Under the scope of the Operational Intelligence project, experts from several areas have gathered to propose and work on “smart” solutions. Machine learning, data mining, log analysis, and anomaly detection are only some of the tools we have evaluated for our use cases. In this community study contribution, we report on the development of a suite of operational intelligence services to cover various use cases: workload management, data management, and site operations.},
  archive      = {J_FDATA},
  author       = {Di Girolamo, Alessandro and Legger, Federica and Paparrigopoulos, Panos and Schovancová, Jaroslava and Beermann, Thomas and Boehler, Michael and Bonacorsi, Daniele and Clissa, Luca and Decker de Sousa, Leticia and Diotalevi, Tommaso and Giommi, Luca and Grigorieva, Maria and Giordano, Domenico and Hohn, David and Javůrek, Tomáš and Jezequel, Stephane and Kuznetsov, Valentin and Lassnig, Mario and Mageirakos, Vasilis and Olocco, Micol and Padolski, Siarhei and Paltenghi, Matteo and Rinaldi, Lorenzo and Sharma, Mayank and Tisbeni, Simone Rossi and Tuckus, Nikodemas},
  doi          = {10.3389/fdata.2021.753409},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {753409},
  shortjournal = {Front. Big Data},
  title        = {Preparing distributed computing operations for the HL-LHC era with operational intelligence},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Artificial intelligence models do not ground negation,
humans do. GuessWhat?! Dialogues as a case study. <em>FDATA</em>,
<em>4</em>, 736709. (<a
href="https://doi.org/10.3389/fdata.2021.736709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negation is widely present in human communication, yet it is largely neglected in the research on conversational agents based on neural network architectures. Cognitive studies show that a supportive visual context makes the processing of negation easier. We take GuessWhat?!, a referential visually grounded guessing game, as test-bed and evaluate to which extent guessers based on pre-trained language models profit from negatively answered polar questions. Moreover, to get a better grasp of models&#39; results, we select a controlled sample of games and run a crowdsourcing experiment with subjects. We evaluate models and humans against the same settings and use the comparison to better interpret the models&#39; results. We show that while humans profit from negatively answered questions to solve the task, models struggle in grounding negation, and some of them barely use it; however, when the language signal is poorly informative, visual features help encoding the negative information. Finally, the experiments with human subjects put us in the position of comparing humans and models&#39; predictions and get a grasp about which models make errors that are more human-like and as such more plausible.},
  archive      = {J_FDATA},
  author       = {Testoni, Alberto and Greco, Claudio and Bernardi, Raffaella},
  doi          = {10.3389/fdata.2021.736709},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {736709},
  shortjournal = {Front. Big Data},
  title        = {Artificial intelligence models do not ground negation, humans do. GuessWhat?! dialogues as a case study},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BigFiRSt: A software program using big data technique for
mining simple sequence repeats from large-scale sequencing data.
<em>FDATA</em>, <em>4</em>, 727216. (<a
href="https://doi.org/10.3389/fdata.2021.727216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BackgroundSimple Sequence Repeats (SSRs) are short tandem repeats of nucleotide sequences. It has been shown that SSRs are associated with human diseases and are of medical relevance. Accordingly, a variety of computational methods have been proposed to mine SSRs from genomes. Conventional methods rely on a high-quality complete genome to identify SSRs. However, the sequenced genome often misses several highly repetitive regions. Moreover, many non-model species have no entire genomes. With the recent advances of next-generation sequencing (NGS) techniques, large-scale sequence reads for any species can be rapidly generated using NGS. In this context, a number of methods have been proposed to identify thousands of SSR loci within large amounts of reads for non-model species. While the most commonly used NGS platforms (e.g., Illumina platform) on the market generally provide short paired-end reads, merging overlapping paired-end reads has become a common way prior to the identification of SSR loci. This has posed a big data analysis challenge for traditional stand-alone tools to merge short read pairs and identify SSRs from large-scale data.ResultsIn this study, we present a new Hadoop-based software program, termed BigFiRSt, to address this problem using cutting-edge big data technology. BigFiRSt consists of two major modules, BigFLASH and BigPERF, implemented based on two state-of-the-art stand-alone tools, FLASH and PERF, respectively. BigFLASH and BigPERF address the problem of merging short read pairs and mining SSRs in the big data manner, respectively. Comprehensive benchmarking experiments show that BigFiRSt can dramatically reduce the execution times of fast read pairs merging and SSRs mining from very large-scale DNA sequence data.ConclusionsThe excellent performance of BigFiRSt mainly resorts to the Big Data Hadoop technology to merge read pairs and mine SSRs in parallel and distributed computing on clusters. We anticipate BigFiRSt will be a valuable tool in the coming biological Big Data era.},
  archive      = {J_FDATA},
  author       = {Chen, Jinxiang and Li, Fuyi and Wang, Miao and Li, Junlong and Marquez-Lago, Tatiana T. and Leier, André and Revote, Jerico and Li, Shuqin and Liu, Quanzhong and Song, Jiangning},
  doi          = {10.3389/fdata.2021.727216},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {727216},
  shortjournal = {Front. Big Data},
  title        = {BigFiRSt: A software program using big data technique for mining simple sequence repeats from large-scale sequencing data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Representation of jews and anti-jewish bias in 19th century
french public discourse: Distant and close reading. <em>FDATA</em>,
<em>4</em>, 723043. (<a
href="https://doi.org/10.3389/fdata.2021.723043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore through the lens of distant reading the evolution of discourse on Jews in France during the XIX century. We analyze a large textual corpus including heterogeneous sources—literary works, periodicals, songs, essays, historical narratives—to trace how Jews are associated to different semantic domains, and how such associations shift over time. Our analysis deals with three key aspects of such changes: the overall transformation of embedding spaces, the trajectories of word associations, and the comparative projection of different religious groups over different, historically relevant semantic dimensions or streams of discourse. This allows to show changes in the association between words and semantic domains (referring e.g. to economic and moral behaviors), the evolution of stereotypes, and the dynamics of bias over a long time span characterized by major historical transformations. We suggest that the analysis of large textual corpora can be fruitfully used in a dialogue with more traditional close reading approaches—by pointing to opportunities of in-depth analyses that mobilize more qualitative approaches and a detailed inspection of the sources that distant reading inevitably tends to aggregate. We offer a short example of such a dialogue between different approaches in our discussion of the Second Empire transformations, where we mobilize the historian’s tools to start disentangling the complex interactions between changes in French society, the nature of sources, and representations of Jews. While our example is limited in scope, we foresee large potential payoffs in the cooperative interaction between distant and close reading.},
  archive      = {J_FDATA},
  author       = {Levis Sullam, Simon and Minello, Giorgia and Tripodi, Rocco and Warglien, Massimo},
  doi          = {10.3389/fdata.2021.723043},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {723043},
  shortjournal = {Front. Big Data},
  title        = {Representation of jews and anti-jewish bias in 19th century french public discourse: Distant and close reading},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
