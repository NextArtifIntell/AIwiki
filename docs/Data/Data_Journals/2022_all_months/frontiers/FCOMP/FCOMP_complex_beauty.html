<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FCOMP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fcomp---178">FCOMP - 178</h2>
<ul>
<li><details>
<summary>
(2022). Corrigendum: Visualizing uncertainty for non-expert end
users: The challenge of the deterministic construal error.
<em>FCOMP</em>, <em>4</em>, 1093379. (<a
href="https://doi.org/10.3389/fcomp.2022.1093379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Joslyn, Susan and Savelli, Sonia},
  doi          = {10.3389/fcomp.2022.1093379},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1093379},
  shortjournal = {Front. Comput. Sci.},
  title        = {Corrigendum: visualizing uncertainty for non-expert end users: the challenge of the deterministic construal error},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Computational commensality. <em>FCOMP</em>,
<em>4</em>, 1086841. (<a
href="https://doi.org/10.3389/fcomp.2022.1086841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Mancini, Maurizio and Cavazza, Nicoletta and Higgs, Suzanne and Huisman, Gijs and Van Den Boer, Janet and Niewiadomski, Radoslaw},
  doi          = {10.3389/fcomp.2022.1086841},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1086841},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Computational commensality},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data and model bias in artificial intelligence for
healthcare applications in new zealand. <em>FCOMP</em>, <em>4</em>,
1070493. (<a href="https://doi.org/10.3389/fcomp.2022.1070493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionDevelopments in Artificial Intelligence (AI) are adopted widely in healthcare. However, the introduction and use of AI may come with biases and disparities, resulting in concerns about healthcare access and outcomes for underrepresented indigenous populations. In New Zealand, Māori experience significant inequities in health compared to the non-Indigenous population. This research explores equity concepts and fairness measures concerning AI for healthcare in New Zealand.MethodsThis research considers data and model bias in NZ-based electronic health records (EHRs). Two very distinct NZ datasets are used in this research, one obtained from one hospital and another from multiple GP practices, where clinicians obtain both datasets. To ensure research equality and fair inclusion of Māori, we combine expertise in Artificial Intelligence (AI), New Zealand clinical context, and te ao Māori. The mitigation of inequity needs to be addressed in data collection, model development, and model deployment. In this paper, we analyze data and algorithmic bias concerning data collection and model development, training and testing using health data collected by experts. We use fairness measures such as disparate impact scores, equal opportunities and equalized odds to analyze tabular data. Furthermore, token frequencies, statistical significance testing and fairness measures for word embeddings, such as WEAT and WEFE frameworks, are used to analyze bias in free-form medical text. The AI model predictions are also explained using SHAP and LIME.ResultsThis research analyzed fairness metrics for NZ EHRs while considering data and algorithmic bias. We show evidence of bias due to the changes made in algorithmic design. Furthermore, we observe unintentional bias due to the underlying pre-trained models used to represent text data. This research addresses some vital issues while opening up the need and opportunity for future research.DiscussionsThis research takes early steps toward developing a model of socially responsible and fair AI for New Zealand&#39;s population. We provided an overview of reproducible concepts that can be adopted toward any NZ population data. Furthermore, we discuss the gaps and future research avenues that will enable more focused development of fairness measures suitable for the New Zealand population&#39;s needs and social structure. One of the primary focuses of this research was ensuring fair inclusions. As such, we combine expertise in AI, clinical knowledge, and the representation of indigenous populations. This inclusion of experts will be vital moving forward, proving a stepping stone toward the integration of AI for better outcomes in healthcare.},
  archive      = {J_FCOMP},
  author       = {Yogarajan, Vithya and Dobbie, Gillian and Leitch, Sharon and Keegan, Te Taka and Bensemann, Joshua and Witbrock, Michael and Asrani, Varsha and Reith, David},
  doi          = {10.3389/fcomp.2022.1070493},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1070493},
  shortjournal = {Front. Comput. Sci.},
  title        = {Data and model bias in artificial intelligence for healthcare applications in new zealand},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An intrinsically motivated learning algorithm based on
bayesian surprise for cognitive radar in autonomous vehicles.
<em>FCOMP</em>, <em>4</em>, 1066422. (<a
href="https://doi.org/10.3389/fcomp.2022.1066422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionThis paper proposes a Bayesian surprise learning algorithm that internally motivates the cognitive radar to estimate a target&#39;s state (i.e., velocity, distance) from noisy measurements and make decisions to reduce the estimation error gradually. The work exhibits how the sensor learns from experiences, anticipates future responses, and adjusts its waveform parameters to achieve informative measurements based on the Bayesian surprise.MethodsFor a simple vehicle-following scenario where the radar measurements are generated from linear Gaussian state-space models, the article adopts the Kalman filter to carry out state estimation. According to the information within the filter&#39;s estimate, the sensor intrinsically assigns a surprise-based reward value to the immediate past action and updates the value-to-go function. Through a series of hypothetical steps, the cognitive radar considers the impact of future transmissions for a prescribed set of waveforms–available from the sensor profile library–to improve the estimation process.Results and discussionNumerous experiments investigate the performance of the proposed design for various surprise-based reward expressions. The robustness of the proposed method is compared to the state-of-the-art for practical and risky driving situations. Results show that the reward functions inspired by estimation credibility measures outperform their competitors when one-step planning is considered. Simulation results also indicate that multiple-step planning does not necessarily lead to lower error, particularly when the environment changes abruptly.},
  archive      = {J_FCOMP},
  author       = {Zamiri-Jafarian, Yeganeh and Hou, Ming and Plataniotis, Konstantinos N.},
  doi          = {10.3389/fcomp.2022.1066422},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1066422},
  shortjournal = {Front. Comput. Sci.},
  title        = {An intrinsically motivated learning algorithm based on bayesian surprise for cognitive radar in autonomous vehicles},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Invisible clinical labor driving the successful integration
of AI in healthcare. <em>FCOMP</em>, <em>4</em>, 1045704. (<a
href="https://doi.org/10.3389/fcomp.2022.1045704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence and Machine Learning (AI/ML) tools are changing the landscape of healthcare decision-making. Vast amounts of data can lead to efficient triage and diagnosis of patients with the assistance of ML methodologies. However, more research has focused on the technological challenges of developing AI, rather than the system integration. As a result, clinical teams&#39; role in developing and deploying these tools has been overlooked. We look to three case studies from our research to describe the often invisible work that clinical teams do in driving the successful integration of clinical AI tools. Namely, clinical teams support data labeling, identifying algorithmic errors and accounting for workflow exceptions, translating algorithmic output to clinical next steps in care, and developing team awareness of how the tool is used once deployed. We call for detailed and extensive documentation strategies (of clinical labor, workflows, and team structures) to ensure this labor is valued and to promote sharing of sociotechnical implementation strategies.},
  archive      = {J_FCOMP},
  author       = {Ulloa, Mara and Rothrock, Blaine and Ahmad, Faraz S. and Jacobs, Maia},
  doi          = {10.3389/fcomp.2022.1045704},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1045704},
  shortjournal = {Front. Comput. Sci.},
  title        = {Invisible clinical labor driving the successful integration of AI in healthcare},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preserving conceptual model semantics in the forward
engineering of relational schemas. <em>FCOMP</em>, <em>4</em>, 1020168.
(<a href="https://doi.org/10.3389/fcomp.2022.1020168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forward engineering relational schemas based on conceptual models (in languages such as UML and ER) is an established practice, with several automated transformation approaches discussed in the literature and implemented in production tools. These transformations must bridge the gap between the primitives offered by conceptual modeling languages on the one hand and the relational model on the other. As a result, it is often the case that some of the semantics of the source conceptual model is lost in the transformation process. In this paper, we address this problem by forward engineering additional constraints along with the transformed schema (ultimately implemented as triggers). We formulate our approach in terms of the operations of “flattening” and “lifting” of classes to make our approach largely independent of the particular transformation strategy (one table per hierarchy, one table per class, one table per concrete class, one table per leaf class, etc.). An automated transformation tool is provided that traces the cumulative consequences of the operations as they are applied throughout the transformation process. We report on tests of this tool using models published in an open model repository.},
  archive      = {J_FCOMP},
  author       = {Guidoni, Gustavo L. and Almeida, João Paulo A. and Guizzardi, Giancarlo},
  doi          = {10.3389/fcomp.2022.1020168},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1020168},
  shortjournal = {Front. Comput. Sci.},
  title        = {Preserving conceptual model semantics in the forward engineering of relational schemas},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The role of gender in the international conference on
pervasive computing and communications. <em>FCOMP</em>, <em>4</em>,
1008552. (<a href="https://doi.org/10.3389/fcomp.2022.1008552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The International Conference on Pervasive Computing and Communications (IEEE PerCom) is a CORE 2021 A* conference (top 7% of ranked venues) that aims to present scientific advances in a broad spectrum of technologies and topics in ubiquitous/pervasive computing, including wireless networking, mobile and distributed computing, sensor systems, ambient intelligence, and smart devices. During the last couple of years, the PerCom organization committee has successfully included many prestigious female researchers to submit, participate, and organize the conference. However, there is still work to do and to help the progress, this article analyses the history of the conference from a gender perspective. This article goes through accepted articles of the last 20 years of the PerCom conferences, showing that even if the role of female authors, in general, has increased, more first and leading female researchers should still be welcomed in the community. Through this analysis, this article aims to highlight the role of gender in the conference program and seeks to find trends and possible improvements to achieve a broader gender balance in pervasive computing.},
  archive      = {J_FCOMP},
  author       = {Peltonen, Ella},
  doi          = {10.3389/fcomp.2022.1008552},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {1008552},
  shortjournal = {Front. Comput. Sci.},
  title        = {The role of gender in the international conference on pervasive computing and communications},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Commentary: A review on the role of affective stimuli in
event-related frontal alpha asymmetry. <em>FCOMP</em>, <em>4</em>,
994071. (<a href="https://doi.org/10.3389/fcomp.2022.994071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Schöne, Benjamin},
  doi          = {10.3389/fcomp.2022.994071},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {994071},
  shortjournal = {Front. Comput. Sci.},
  title        = {Commentary: A review on the role of affective stimuli in event-related frontal alpha asymmetry},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Student experiences in a university preparatory programming
course. <em>FCOMP</em>, <em>4</em>, 983237. (<a
href="https://doi.org/10.3389/fcomp.2022.983237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though computer science is currently being integrated into primary and secondary education worldwide, we cannot yet make assumptions about our student&#39;s prior knowledge of computing. Every student might have different conceptions about the field of study they are about to enter. Students start at computer science programs with different prior experiences with programming, ranging from no experience to a high degree of proficiency. At the Department of Computer Science, University of Copenhagen, we have designed a 2-week voluntary summer kickstart course in programming to help students in this transition into our three computer science programs. To evaluate the course, we followed three groups of students. Group one with no/limited programming experience attended the kickstart course. The second group with no/little programming experience did not participate in the course, and the third group of students with programming experience did not participate in the class. We observed the kickstart course and conducted interviews. We followed up about 3 weeks after the start of the semester and then again at the end of the semester in December. Our findings suggest that the course reduces the gap in programming experiences and strengthens students&#39; self-efficacy and sense of belonging. However, the approach creates a social gap between students who have not attended the course with no/limited experience at the beginning of the semester. Even though the students in December do not experience any difference between students who have attended the course and those who have not, it is important to consider this social gap at the beginning of the semester when designing and planning a preparatory course like the kickstart course.},
  archive      = {J_FCOMP},
  author       = {Spikol, Daniel and Dybdal, Martin and Elmeskov, Dorte C.},
  doi          = {10.3389/fcomp.2022.983237},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {983237},
  shortjournal = {Front. Comput. Sci.},
  title        = {Student experiences in a university preparatory programming course},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The potential of remote XR experimentation: Defining
benefits and limitations through expert survey and case study.
<em>FCOMP</em>, <em>4</em>, 952996. (<a
href="https://doi.org/10.3389/fcomp.2022.952996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimentation using extended reality (XR) technology is predominantly conducted in-lab with a co-present researcher. Remote XR experiments, without co-present researchers, have been less common, despite the success of remote approaches for non-XR investigations. In order to understand why remote XR experiments are atypical, this article outlines the perceived limitations, as well as potential benefits, of conducting remote XR experiments, through a thematic analysis of responses to a 30-item survey of 46 XR researchers. These are synthesized into five core research questions for the XR community, and concern types of participant, recruitment processes, potential impacts of remote setup and settings, the data-capture affordances of XR hardware and how remote XR experiment development can be optimized to reduce demands on the researcher. It then explores these questions by running two experiments in a fully “encapsulated” remote XR case study, in which the recruitment and experiment processes is distributed and conducted unsupervised. It discusses the design, experiment, and results from this case study in the context of these core questions.},
  archive      = {J_FCOMP},
  author       = {Ratcliffe, Jack and Tokarchuk, Laurissa},
  doi          = {10.3389/fcomp.2022.952996},
  journal      = {Frontiers in Computer Science},
  month        = {12},
  pages        = {952996},
  shortjournal = {Front. Comput. Sci.},
  title        = {The potential of remote XR experimentation: Defining benefits and limitations through expert survey and case study},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Serious games—volume II. <em>FCOMP</em>,
<em>4</em>, 1088284. (<a
href="https://doi.org/10.3389/fcomp.2022.1088284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Vaz de Carvalho, Carlos and González González, Carina S. and Popescu, Elvira and Rugelj, Jože},
  doi          = {10.3389/fcomp.2022.1088284},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1088284},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Serious games—Volume II},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Governance AI ethics. <em>FCOMP</em>, <em>4</em>,
1081147. (<a href="https://doi.org/10.3389/fcomp.2022.1081147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Rousi, Rebekah and Saariluoma, Pertti and Nieminen, Mika},
  doi          = {10.3389/fcomp.2022.1081147},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1081147},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Governance AI ethics},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Challenges and best practices in corporate AI governance:
Lessons from the biopharmaceutical industry. <em>FCOMP</em>, <em>4</em>,
1068361. (<a href="https://doi.org/10.3389/fcomp.2022.1068361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the use of artificial intelligence (AI) systems promises to bring significant economic and social benefits, it is also coupled with ethical, legal, and technical challenges. Business leaders thus face the question of how to best reap the benefits of automation whilst managing the associated risks. As a first step, many companies have committed themselves to various sets of ethics principles aimed at guiding the design and use of AI systems. So far so good. But how can well-intentioned ethical principles be translated into effective practice? And what challenges await companies that attempt to operationalize AI governance? In this article, we address these questions by drawing on our first-hand experience of shaping and driving the roll-out of AI governance within AstraZeneca, a biopharmaceutical company. The examples we discuss highlight challenges that any organization attempting to operationalize AI governance will have to face. These include questions concerning how to define the material scope of AI governance, how to harmonize standards across decentralized organizations, and how to measure the impact of specific AI governance initiatives. By showcasing how AstraZeneca managed these operational questions, we hope to provide project managers, CIOs, AI practitioners, and data privacy officers responsible for designing and implementing AI governance frameworks within other organizations with generalizable best practices. In essence, companies seeking to operationalize AI governance are encouraged to build on existing policies and governance structures, use pragmatic and action-oriented terminology, focus on risk management in development and procurement, and empower employees through continuous education and change management.},
  archive      = {J_FCOMP},
  author       = {Mökander, Jakob and Sheth, Margi and Gersbro-Sundler, Mimmi and Blomgren, Peder and Floridi, Luciano},
  doi          = {10.3389/fcomp.2022.1068361},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1068361},
  shortjournal = {Front. Comput. Sci.},
  title        = {Challenges and best practices in corporate AI governance: Lessons from the biopharmaceutical industry},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time multiple target segmentation with multimodal
few-shot learning. <em>FCOMP</em>, <em>4</em>, 1062792. (<a
href="https://doi.org/10.3389/fcomp.2022.1062792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based target segmentation requires a big training dataset to achieve good results. In this regard, few-shot learning a model that quickly adapts to new targets with a few labeled support samples is proposed to tackle this issue. In this study, we introduce a new multimodal few-shot learning [e.g., red-green-blue (RGB), thermal, and depth] for real-time multiple target segmentation in a real-world application with a few examples based on a new squeeze-and-attentions mechanism for multiscale and multiple target segmentation. Compared to the state-of-the-art methods (HSNet, CANet, and PFENet), the proposed method demonstrates significantly better performance on the PST900 dataset with 32 time-series sets in both Hand-Drill, and Survivor classes.},
  archive      = {J_FCOMP},
  author       = {Khoshboresh-Masouleh, Mehdi and Shah-Hosseini, Reza},
  doi          = {10.3389/fcomp.2022.1062792},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1062792},
  shortjournal = {Front. Comput. Sci.},
  title        = {Real-time multiple target segmentation with multimodal few-shot learning},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: 2021 editors’ pick: Computer science.
<em>FCOMP</em>, <em>4</em>, 1062066. (<a
href="https://doi.org/10.3389/fcomp.2022.1062066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Lukowicz, Paul and Nijholt, Anton and Siddiqi, Kaleem and Pelillo, Marcello and Laerhoven, Kristof Van and Viganò, Luca and Zannone, Nicola},
  doi          = {10.3389/fcomp.2022.1062066},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1062066},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: 2021 editors&#39; pick: computer science},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A coarse-to-fine and automatic algorithm for breast
diagnosis on multi-series MRI images. <em>FCOMP</em>, <em>4</em>,
1054158. (<a href="https://doi.org/10.3389/fcomp.2022.1054158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionEarly breast carcinomas can be effectively diagnosed and controlled. However, it demands extra work and radiologist in China often suffer from overtime working due to too many patients, even experienced ones could make mistakes after overloaded work. To improve the efficiency and reduce the rate of misdiagnosis, automatic breast diagnosis on Magnetic Resonance Imaging (MRI) images is vital yet challenging for breast disease screening and successful treatment planning. There are some obstacles that hinder the development of automatic approaches, such as class-imbalance of samples, hard mimics of lesions, etc. In this paper, we propose a coarse-to-fine algorithm to address those problems of automatic breast diagnosis on multi-series MRI images. The algorithm utilizes deep learning techniques to provide breast segmentation, tumor segmentation and tumor classification functions, thus supporting doctors&#39; decisions in clinical practice.MethodsIn proposed algorithm, a DenseUNet is firstly employed to extract breast-related regions by removing irrelevant parts in the thoracic cavity. Then, by taking advantage of the attention mechanism and the focal loss, a novel network named Attention Dense UNet (ADUNet) is designed for the tumor segmentation. Particularly, the focal loss in ADUNet addresses class-imbalance and model overwhelmed problems. Finally, a customized network is developed for the tumor classification. Besides, while most approaches only consider one or two series, the proposed algorithm takes in account multiple series of MRI images.ResultsExtensive experiments are carried out to evaluate its performance on 435 multi-series MRI volumes from 87 patients collected from Tongji Hospital. In the dataset, all cases are with benign, malignant, or both type of tumors, the category of which covers carcinoma, fibroadenoma, cyst and abscess. The ground truths of tumors are labeled by two radiologists with 3 years of experience on breast MRI reporting by drawing contours of tumor slice by slice. ADUNet is compared with other prevalent deep-learning methods on the tumor segmentation and quantitative results, and achieves the best performance on both Case Dice Score and Global Dice Score by 0.748 and 0.801 respectively. Moreover, the customized classification network outperforms two CNN-M based models and achieves tumor-level and case-level AUC by 0.831 and 0.918 respectively.DiscussionAll data in this paper are collected from the same MRI device, thus it is reasonable to assume that they are from the same domain and independent identically distributed. Whether the proposed algorithm is robust enough in a multi-source case still remains an open question. Each stage of the proposed algorithm is trained separately, which makes each stage more robust and converge faster. Such training strategy considers each stage as a separate task and does not take into account the relationships between tasks.},
  archive      = {J_FCOMP},
  author       = {Xue, Hongwei and Qian, Guangwu and Wu, Xiaofen and Gao, Yan and Yang, Hongguang and Liu, Mingqian and Wang, Lei and Chen, Renfei and Wang, Peijun},
  doi          = {10.3389/fcomp.2022.1054158},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1054158},
  shortjournal = {Front. Comput. Sci.},
  title        = {A coarse-to-fine and automatic algorithm for breast diagnosis on multi-series MRI images},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Landmark selection for route instructions: At which corner
of an intersection is the preferred landmark located? <em>FCOMP</em>,
<em>4</em>, 1044151. (<a
href="https://doi.org/10.3389/fcomp.2022.1044151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive studies showed that good landmarks–salient objects in the environment–make it easier for recipients of route instructions to find their way to the destination. Adding landmarks to route instructions also improves mobile navigation systems for pedestrians. But, which landmarks do people consider most helpful when giving route instructions? Four experiments explored this question. In the first experiment, the environment, including the route and landmarks, was presented on a map. The landmarks were located at the four corners of a right-angled intersection. Participants had to select those landmark-based route instructions they considered most helpful. In all other experiments, the environment was presented from an egocentric perspective, either in a video or as a sequence of pictures of intersections. Participants had to select those landmarks they would use in a route instruction. All landmarks had the same visual and semantic salience. The positions of the participants at the intersection were varied. Results show that participants consistently selected landmarks at the side of the road into which they had to turn. Moreover, the participants&#39; position at the intersection affected whether they selected landmarks before or behind the decision point. These results have consequences for human spatial cognition research and for the automatic selection of landmarks in mobile pedestrian navigation systems.},
  archive      = {J_FCOMP},
  author       = {Hamburger, Kai and Röser, Florian and Knauff, Markus},
  doi          = {10.3389/fcomp.2022.1044151},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1044151},
  shortjournal = {Front. Comput. Sci.},
  title        = {Landmark selection for route instructions: At which corner of an intersection is the preferred landmark located?},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Is it enough to optimize CNN architectures on ImageNet?
<em>FCOMP</em>, <em>4</em>, 1041703. (<a
href="https://doi.org/10.3389/fcomp.2022.1041703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification performance based on ImageNet is the de-facto standard metric for CNN development. In this work we challenge the notion that CNN architecture design solely based on ImageNet leads to generally effective convolutional neural network (CNN) architectures that perform well on a diverse set of datasets and application domains. To this end, we investigate and ultimately improve ImageNet as a basis for deriving such architectures. We conduct an extensive empirical study for which we train 500 CNN architectures, sampled from the broad AnyNetX design space, on ImageNet as well as 8 additional well-known image classification benchmark datasets from a diverse array of application domains. We observe that the performances of the architectures are highly dataset dependent. Some datasets even exhibit a negative error correlation with ImageNet across all architectures. We show how to significantly increase these correlations by utilizing ImageNet subsets restricted to fewer classes. These contributions can have a profound impact on the way we design future CNN architectures and help alleviate the tilt we see currently in our community with respect to over-reliance on one dataset.},
  archive      = {J_FCOMP},
  author       = {Tuggener, Lukas and Schmidhuber, Jürgen and Stadelmann, Thilo},
  doi          = {10.3389/fcomp.2022.1041703},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1041703},
  shortjournal = {Front. Comput. Sci.},
  title        = {Is it enough to optimize CNN architectures on ImageNet?},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward extreme face super-resolution in the wild: A
self-supervised learning approach. <em>FCOMP</em>, <em>4</em>, 1037435.
(<a href="https://doi.org/10.3389/fcomp.2022.1037435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme face super-resolution (FSR), that is, improving the resolution of face images by an extreme scaling factor (often greater than ×8) has remained underexplored in the literature of low-level vision. Extreme FSR in the wild must address the challenges of both unpaired training data and unknown degradation factors. Inspired by the latest advances in image super-resolution (SR) and self-supervised learning (SSL), we propose a novel two-step approach to FSR by introducing a mid-resolution (MR) image as the stepping stone. In the first step, we leverage ideas from SSL-based SR reconstruction of medical images (e.g., MRI and ultrasound) to modeling the realistic degradation process of face images in the real world; in the second step, we extract the latent codes from MR images and interpolate them in a self-supervised manner to facilitate artifact-suppressed image reconstruction. Our two-step extreme FSR can be interpreted as the combination of existing self-supervised CycleGAN (step 1) and StyleGAN (step 2) that overcomes the barrier of critical resolution in face recognition. Extensive experimental results have shown that our two-step approach can significantly outperform existing state-of-the-art FSR techniques, including FSRGAN, Bulat&#39;s method, and PULSE, especially for large scaling factors such as 64.},
  archive      = {J_FCOMP},
  author       = {Cheikh Sidiya, Ahmed and Li, Xin},
  doi          = {10.3389/fcomp.2022.1037435},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1037435},
  shortjournal = {Front. Comput. Sci.},
  title        = {Toward extreme face super-resolution in the wild: A self-supervised learning approach},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed-UNet: Refined class activation mapping for
weakly-supervised semantic segmentation with multi-scale inference.
<em>FCOMP</em>, <em>4</em>, 1036934. (<a
href="https://doi.org/10.3389/fcomp.2022.1036934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have shown great potential in medical image processing, particularly through accurate and reliable image segmentation on magnetic resonance imaging (MRI) scans or computed tomography (CT) scans, which allow the localization and diagnosis of lesions. However, training these segmentation models requires a large number of manually annotated pixel-level labels, which are time-consuming and labor-intensive, in contrast to image-level labels that are easier to obtain. It is imperative to resolve this problem through weakly-supervised semantic segmentation models using image-level labels as supervision since it can significantly reduce human annotation efforts. Most of the advanced solutions exploit class activation mapping (CAM). However, the original CAMs rarely capture the precise boundaries of lesions. In this study, we propose the strategy of multi-scale inference to refine CAMs by reducing the detail loss in single-scale reasoning. For segmentation, we develop a novel model named Mixed-UNet, which has two parallel branches in the decoding phase. The results can be obtained after fusing the extracted features from two branches. We evaluate the designed Mixed-UNet against several prevalent deep learning-based segmentation approaches on our dataset collected from the local hospital and public datasets. The validation results demonstrate that our model surpasses available methods under the same supervision level in the segmentation of various lesions from brain imaging.},
  archive      = {J_FCOMP},
  author       = {Liu, Yang and Lian, Lijin and Zhang, Ersi and Xu, Lulu and Xiao, Chufan and Zhong, Xiaoyun and Li, Fang and Jiang, Bin and Dong, Yuhan and Ma, Lan and Huang, Qiming and Xu, Ming and Zhang, Yongbing and Yu, Dongmei and Yan, Chenggang and Qin, Peiwu},
  doi          = {10.3389/fcomp.2022.1036934},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1036934},
  shortjournal = {Front. Comput. Sci.},
  title        = {Mixed-UNet: Refined class activation mapping for weakly-supervised semantic segmentation with multi-scale inference},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extended reality for mental health: Current trends and
future challenges. <em>FCOMP</em>, <em>4</em>, 1034307. (<a
href="https://doi.org/10.3389/fcomp.2022.1034307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual and augmented reality have been used to diagnose and treat several mental health disorders for decades. Technological advances in these fields have facilitated the availability of commercial solutions for end customers and practitioners. However, there are still some barriers and limitations that prevent these technologies from being widely used by professionals on a daily basis. In addition, the COVID-19 pandemic has exposed a variety of new scenarios in which these technologies could play an essential role, like providing remote treatment. Disorders that traditionally had received less attention are also getting in the spotlight, such as depression or obsessive-compulsive disorder. Improvements in equipment and hardware, like Mixed Reality Head Mounted Displays, could help open new opportunities in the mental health field. Extended reality (XR) is an umbrella term meant to comprise Virtual reality (VR), mixed reality (MR), and augmented reality (AR). While XR applications are eminently visual, other senses are being explored in literature around multisensory interactions, such as auditory, olfactory, or haptic feedback. Applying such stimuli within XR experiences around mental disorders is still under-explored and could greatly enrich the therapeutic experience. This manuscript reviews recent research regarding the use of XR for mental health scenarios, highlighting trends, and potential applications as well as areas for improvement. It also discusses future challenges and research areas in upcoming topics such as the use of wearables, multisensory, and multimodal interaction. The main goal of this paper is to unpack how these technologies could be applied to XR scenarios for mental health to exploit their full potential and follow the path of other health technologies by promoting personalized medicine.},
  archive      = {J_FCOMP},
  author       = {Pons, Patricia and Navas-Medrano, Samuel and Soler-Dominguez, Jose L.},
  doi          = {10.3389/fcomp.2022.1034307},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1034307},
  shortjournal = {Front. Comput. Sci.},
  title        = {Extended reality for mental health: Current trends and future challenges},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight and anchor-free frame detection strategy based
on improved CenterNet for multiscale ships in SAR images.
<em>FCOMP</em>, <em>4</em>, 1012755. (<a
href="https://doi.org/10.3389/fcomp.2022.1012755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection using synthetic aperture radar (SAR) images has important applications in military and civilian fields, but the different sizes of the ship downgrade the detection accuracy of multiscale ships. Aiming at the problem of the poor accuracy and low efficiency of multiscale ship detection in complex scenes, this paper proposes a lightweight and anchor-free frame detection strategy for multiscale ships in SAR images. First, to deal with the problems of limited training samples, different sizes, attitudes, and angles of the ships in SAR images, a data augmentation strategy suitable for SAR images is adopted to expand the training space, followed by multiscale training to enhance the model generalization ability for multiscale ship detection. Second, a lightweight and anchor-free ship detection model based on the improved CenterNet is proposed, which abandons the dense anchor frame generation and extracts the key point of the ships for detection and positioning. Compared with the anchor frame-based detection method, this proposed detection model does not need to use the post-processing method to remove redundant anchor frames, and can accurately locate the center point of the ships with a better detection performance. Third, to reduce the model size and simplify the model parameters, a more lightweight network design is adopted in combination with the characteristics of SAR images. Hence, a residual network (ResNet) with fewer convolutional layers is constructed as the backbone network, and the cross-stage partial network (CSPNet) and spatial pyramid pooling (SPP) network are designed as the bottleneck network. The shallow ResNet can fully extract the SAR image features and reduce the training overfitting, and CSPNet and SPP can effectively combine the low-level image features to obtain the high-level features, reducing the model computation while at the same time enhancing the feature extraction ability. Finally, the evaluation index of the common objects in the context dataset is introduced, which can provide higher-quality evaluation results for ship detection accuracy and provide comprehensive evaluation indicators for multiscale ship detection. Experimental results show that the proposed strategy has the advantages of high detection efficiency, strong detection ability, and good generalization performance, which can achieve real-time and high-precision detection of the multiscale ship in complex SAR images.},
  archive      = {J_FCOMP},
  author       = {Xie, Hongtu and Jiang, Xinqiao and Zhang, Jian and Chen, Jiaxing and Wang, Guoqian and Xie, Kai},
  doi          = {10.3389/fcomp.2022.1012755},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {1012755},
  shortjournal = {Front. Comput. Sci.},
  title        = {Lightweight and anchor-free frame detection strategy based on improved CenterNet for multiscale ships in SAR images},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging a sensory gap between deaf and hearing people–a
plea for a situated design approach to sensory augmentation.
<em>FCOMP</em>, <em>4</em>, 991180. (<a
href="https://doi.org/10.3389/fcomp.2022.991180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deaf and hearing people can encounter challenges when communicating with one another in everyday situations. Although problems in verbal communication are often seen as the main cause, such challenges may also result from sensory differences between deaf and hearing people and their impact on individual understandings of the world. That is, challenges arising from a sensory gap. Proposals for innovative communication technologies to address this have been met with criticism by the deaf community. They are mostly designed to enhance deaf people&#39;s understanding of the verbal cues that hearing people rely on, but omit many critical sensory signals that deaf people rely on to understand (others in) their environment and to which hearing people are not tuned to. In this perspective paper, sensory augmentation, i.e., technologically extending people&#39;s sensory capabilities, is put forward as a way to bridge this sensory gap: (1) by tuning to the signals deaf people rely on more strongly but are commonly missed by hearing people, and vice versa, and (2) by sensory augmentations that enable deaf and hearing people to sense signals that neither person is able to normally sense. Usability and user-acceptance challenges, however, lie ahead of realizing the alleged potential of sensory augmentation for bridging the sensory gap between deaf and hearing people. Addressing these requires a novel approach to how such technologies are designed. We contend this requires a situated design approach.},
  archive      = {J_FCOMP},
  author       = {Witter, Michel and de Rooij, Alwin and van Dartel, Michel and Krahmer, Emiel},
  doi          = {10.3389/fcomp.2022.991180},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {991180},
  shortjournal = {Front. Comput. Sci.},
  title        = {Bridging a sensory gap between deaf and hearing people–A plea for a situated design approach to sensory augmentation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Imagine this smart speaker to have a body”: An analysis of
the external appearances and the characteristics that people associate
with voice assistants. <em>FCOMP</em>, <em>4</em>, 981435. (<a
href="https://doi.org/10.3389/fcomp.2022.981435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionModern digital devices, such as conversational agents, simulate human–human interactions to an increasing extent. However, their outward appearance remains distinctly technological. While research revealed that mental representations of technology shape users&#39; expectations and experiences, research on technology sending ambiguous cues is rare.MethodsTo bridge this gap, this study analyzes drawings of the outward appearance participants associate with voice assistants (Amazon Echo or Google Home).ResultsHuman beings and (humanoid) robots were the most frequent associations, which were rated to be rather trustworthy, conscientious, agreeable, and intelligent. Drawings of the Amazon Echos and Google Homes differed marginally, but “human,” “robotic,” and “other” associations differed with respect to the ascribed humanness, consciousness, intellect, affinity to technology, and innovation ability.DiscussionThis study aims to further elaborate on the rather unconscious cognitive and emotional processes elicited by technology and discusses the implications of this perspective for developers, users, and researchers.},
  archive      = {J_FCOMP},
  author       = {Carolus, Astrid and Wienrich, Carolin},
  doi          = {10.3389/fcomp.2022.981435},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {981435},
  shortjournal = {Front. Comput. Sci.},
  title        = {“Imagine this smart speaker to have a body”: An analysis of the external appearances and the characteristics that people associate with voice assistants},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An algebra for local histograms. <em>FCOMP</em>, <em>4</em>,
939563. (<a href="https://doi.org/10.3389/fcomp.2022.939563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider local overlapping histograms of functions between discrete domains and codomains. We develop a simple algebra for local histograms. Based on a separation of overlapping domains into non-overlapping domains, we (1) show how these can be used to enumerate the size of the set of possible histograms given the local histogram domains, and (2) enumerate the number of functions, which share a specific choice of a set of local histograms. Finally, we present a decoding algorithm, which given a set of overlapping histograms, and calculate the set of functions, which share these histograms.},
  archive      = {J_FCOMP},
  author       = {Sporring, Jon and Darkner, Sune},
  doi          = {10.3389/fcomp.2022.939563},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {939563},
  shortjournal = {Front. Comput. Sci.},
  title        = {An algebra for local histograms},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive human bodies and adaptive built environments for
enriching futures. <em>FCOMP</em>, <em>4</em>, 931973. (<a
href="https://doi.org/10.3389/fcomp.2022.931973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As humans, we spend most of our lives inside human-built environments, such as homes, offices, and schools. In these built environments, humans co-create and share a collective awareness, social practices, and knowledge while computing machinery is designed to maintain the built environment and support our interactions. The effects of these technologically enriched built environments on humans are how our bodies adapt to the practices they promote and how these practices, in return, affect the built environment and the natural environment. This perspective paper uses inbodied interaction to frame the constant adaption of our bodies to our surrounding environment as an opportunity to inform the design of technology and its practices and offer a vision where humans, the built environment, and the natural environment coexist in a mutually beneficial relationship.},
  archive      = {J_FCOMP},
  author       = {Andres, Josh},
  doi          = {10.3389/fcomp.2022.931973},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {931973},
  shortjournal = {Front. Comput. Sci.},
  title        = {Adaptive human bodies and adaptive built environments for enriching futures},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing heart rate variability biofeedback and simple
paced breathing to inform the design of guided breathing technologies.
<em>FCOMP</em>, <em>4</em>, 926649. (<a
href="https://doi.org/10.3389/fcomp.2022.926649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IntroductionA goal of inbodied interaction is to explore how tools can be designed to provide external interactions that support our internal processes. One process that often suffers from our external interactions with modern computing technology is our breathing. Because of the ergonomics and low-grade-but-frequent stress associated with computer work, many people adopt a short, shallow breathing pattern that is known to have a negative effect on other parts of our physiology. Breathing guides are tools that help people match their breathing patterns to an external (most often visual) cue to practice healthy breathing exercises.However, there are two leading protocols for how breathing cues are offered by breathing guides used in non-clinical settings: simple paced breathing (SPB) and Heart Rate Variability Biofeedback (HRV-b). Although these protocols have separately been demonstrated to be effective, they differ substantially in their complexity and design. Paced breathing is a simpler protocol where a user is asked to match their breathing pattern with a cue paced at a predetermined rate and is simple enough to be completed as a secondary task during other activities. HRV-b, on the other hand, provides adaptive, real-time guidance derived from heart rate variability, a physiological signal that can be sensed through a wearable device. Although the benefits of these two protocols have been well established in clinical contexts, designers of guided breathing technology have little information about whether one is better than the other for non-clinical use.MethodsTo address this important gap in knowledge, we conducted the first comparative study of these two leading protocols in the context of end-user applications. In our N=28 between-subject design, participants were trained in either SPB or HRV-b and then completed a 10-minute session following their training protocol. Breathing rates and heart rate variability scores were recorded and compared between groups.Results and discussionOur findings indicate that the exercises did not significantly differ in their immediate outcomes – both resulted in significantly slower breathing rates than their baseline and both provided similar relative increases in HRV. Therefore, there were no observed differences in the acute physiological effects when using either SPB or HRV-b. Our paper contributes new findings suggesting that simple paced breathing – a straightforward, intuitive, and easy-to-design breathing exercise – provides the same immediate benefits as HRV-b, but without its added design complexities.},
  archive      = {J_FCOMP},
  author       = {Tabor, Aaron and Bateman, Scott and Scheme, Erik J. and schraefel, m.c.},
  doi          = {10.3389/fcomp.2022.926649},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {926649},
  shortjournal = {Front. Comput. Sci.},
  title        = {Comparing heart rate variability biofeedback and simple paced breathing to inform the design of guided breathing technologies},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncovering terra incognita in the AHD design space: A review
of affective haptic devices. <em>FCOMP</em>, <em>4</em>, 795772. (<a
href="https://doi.org/10.3389/fcomp.2022.795772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective haptic devices (AHDs) have been developed with the aim of communicating touch acts, symbolic messages, emotions, and/or providing a sense of social awareness. Within AHDs, three categories can be distinguished: mediated social touch (MST), symbolic communication systems, and awareness systems. For each of these categories, prototypes have been developed and discussed in the literature. Each such prototype, however, describes but a small part of the design space of AHDs. What is lacking is a description of the design space itself—of all choices that can be considered during the design process. Such a description will allow for a more systematic exploration of AHD designs and provides a means of combining insights gained from individual point solutions (i.e., existing prototypes). Therefore, in this article, we provide a systematic description of the design space of AHDs and its underlying dimensions based on general (e.g., revisability or synchronicity) and AHD-specific (e.g., actuation type) communication system characteristics. This resulted in 17 design dimensions, each consisting of two or more categories (the design characteristics). Based on a systematic literature search from devices up to 2019, 89 AHD prototypes were identified, and each was classified on the design dimensions. The empirical analysis of where these AHDs are located in the design space revealed, first, that potentially interesting characteristics from mediated communication, such as revisability and reviewability, are underexplored in AHDs. Second, MST devices were found to often lack those system characteristics, such as real-time modalities, that seem crucial for providing the affordances needed to simulate social touch. In particular, when comparing symbolic and awareness devices to MST devices, we found the latter to more frequently lack some of the key characteristics of face-to-face communication (i.e., bi-directional and symmetric communication). Limitations and implications are discussed.},
  archive      = {J_FCOMP},
  author       = {Ipakchian Askari, Sima and Haans, Antal and IJsselsteijn, Wijnand A.},
  doi          = {10.3389/fcomp.2022.795772},
  journal      = {Frontiers in Computer Science},
  month        = {11},
  pages        = {795772},
  shortjournal = {Front. Comput. Sci.},
  title        = {Uncovering terra incognita in the AHD design space: A review of affective haptic devices},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simple interactive robot to promote computational
thinking. <em>FCOMP</em>, <em>4</em>, 1022778. (<a
href="https://doi.org/10.3389/fcomp.2022.1022778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper described the functionalities of a simple robot adopted in the classroom to promote computational thinking (CT) in the context of the project PeCOT—computational thinking with tangible objects. This robot, similar to some commercial educational robots, was developed to be used in primary education classroom contexts (second, third, and fourth grades) by children between 8 and 10 years old. PeCOT is a pedagogical intervention project and research project that will run over the next 2 years. The aim of this paper is to present the robot, how it works, and some activities that can be implemented in the educational context to promote CT skills, including learning in different subject areas, such as mathematics or natural science. Thus, we begin by presenting a general description of the robot. Next, we identify activities and formalize the programming steps for each of the activities. Finally, we discuss the potential that the robot and the proposed activities may have to promote the development of CT skills.},
  archive      = {J_FCOMP},
  author       = {Funk, Matthias and Cascalho, José and Santos, Ana Isabel and Pedro, Francisco and Medeiros, Paulo and Amaral, Bárbara and Domingos, Manuel and Ramos, Alberto and Mendes, Armando},
  doi          = {10.3389/fcomp.2022.1022778},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {1022778},
  shortjournal = {Front. Comput. Sci.},
  title        = {A simple interactive robot to promote computational thinking},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review on the teaching of programming and computational
thinking in the world. <em>FCOMP</em>, <em>4</em>, 997222. (<a
href="https://doi.org/10.3389/fcomp.2022.997222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies suggest that computational thinking, composed of the skills of abstraction, decomposition, algorithmization, debugging, and problem-solving, is the fundamental skill for scientific, technological, and economic development for the twenty-first century. However, this diagnosis that is unveiled in rich countries remains nebulous for poor countries. The problem is that education in computational thinking is fundamental for countries to insert themselves in the international arena in an advantageous way and thus achieve the welfare goals for the population of each country. The objective of this research was to make a bibliographic review that shows the state of the art in the teaching of computer programming and computational thinking in the 5 continents. In the review, the advances in the countries of Europe, North America, Oceania, and Asia were observed, whereas in Latin America and Africa, the advances are still basic in some countries and non-existent in others. This review is based on Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). The main search terms were “Computational thinking” and “Teaching computer programming.” The search was performed in the ACM, Conference on Computational Thinking Education (Hong-Kong), Google Scholar, WOS, and SCOPUS databases, from October until December 2020, whose publication year was from 2016 onward. One of the main results found is that the teaching of computational thinking in England was implemented in schools in 2014; in Germany, it has been implemented since 2016 at a transversal level in universities; in South Korea, China, and Taiwan, it has been implemented since 2016. However, in Latin America and Africa governments, the subject is still not considered.},
  archive      = {J_FCOMP},
  author       = {Belmar, Héctor},
  doi          = {10.3389/fcomp.2022.997222},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {997222},
  shortjournal = {Front. Comput. Sci.},
  title        = {Review on the teaching of programming and computational thinking in the world},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theory and applications of InfraRed and thermal image
analysis in ergonomics research. <em>FCOMP</em>, <em>4</em>, 990290. (<a
href="https://doi.org/10.3389/fcomp.2022.990290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing products and services to fit human needs, wants and lifestyle require meaningful data. With Industry 4.0 and the internet of things, we have many ways to capture data using sensors and other means. InfraRed (IR) cameras are quite ubiquitous, especially for screening illness and wellness. They can provide a wealth of data on different objects and even people. However, their use has been limited due to processing complexities. With reducing cost and increasing accuracy of IR cameras, access to thermal data is becoming quite widespread, especially in medicine and people-related applications. These cameras have software to help process the data, with a focus on qualitative analyses and rather primitive quantitative analyses. In ergonomics, data from multiple users are essential to make reasonable predictions for a given population. In this study, using 4 simple experiments, several quantitative analysis techniques such as simple statistics, multivariate statistics, geometric modeling, and Fourier series modeling are applied to IR images and videos to extract essential user and population data. Results show that IR data can be useful to provide user and population data that are important for design. More research in modeling IR data and application software is needed for the increased application of IR information in ergonomics applications.},
  archive      = {J_FCOMP},
  author       = {Luximon, Ameersing and Chao, Huang and Goonetilleke, Ravindra S. and Luximon, Yan},
  doi          = {10.3389/fcomp.2022.990290},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {990290},
  shortjournal = {Front. Comput. Sci.},
  title        = {Theory and applications of InfraRed and thermal image analysis in ergonomics research},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unfreezing autonomous vehicles with game theory, proxemics,
and trust. <em>FCOMP</em>, <em>4</em>, 969194. (<a
href="https://doi.org/10.3389/fcomp.2022.969194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid deployment of robotic systems in public places such as roads, pavements, workplaces and care homes. Robot navigation in environments with static objects is largely solved, but navigating around humans in dynamic environments remains an active research question for autonomous vehicles (AVs). To navigate in human social spaces, self-driving cars and other robots must also show social intelligence. This involves predicting and planning around pedestrians, understanding their personal space, and establishing trust with them. Most current AVs, for legal and safety reasons, consider pedestrians to be obstacles, so these AVs always stop for or replan to drive around them. But this highly safe nature may lead pedestrians to take advantage over them and slow their progress, even to a complete halt. We provide a review of our recent research on predicting and controlling human–AV interactions, which combines game theory, proxemics and trust, and unifies these fields via quantitative, probabilistic models and robot controllers, to solve this “freezing robot” problem.},
  archive      = {J_FCOMP},
  author       = {Camara, Fanta and Fox, Charles},
  doi          = {10.3389/fcomp.2022.969194},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {969194},
  shortjournal = {Front. Comput. Sci.},
  title        = {Unfreezing autonomous vehicles with game theory, proxemics, and trust},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross-vendor programming abstraction for diverse
heterogeneous platforms. <em>FCOMP</em>, <em>4</em>, 945652. (<a
href="https://doi.org/10.3389/fcomp.2022.945652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware specialization is a well-known means to significantly improve the performance and energy efficiency of various application domains. Modern computing systems consist of multiple specialized processing devices which need to collaborate with each other to execute common tasks. New heterogeneous programming abstractions have been created to program heterogeneous systems. Even though many of these abstractions are open vendor-independent standards, cross-vendor interoperability between different implementations is limited since the vendors typically do not have commercial motivations to invest in it. Therefore, getting good performance from vendor-independent heterogeneous programming standards has proven difficult for systems with multiple different device types. In order to help unify the field of heterogeneous programming APIs for platforms with hardware accelerators from multiple vendors, we propose a new software abstraction for hardware-accelerated tasks based on the open OpenCL programming standard. In the proposed abstraction, we rely on the built-in kernel feature of the OpenCL specification to define a portability layer that stores enough information for automated accelerator utilization. This enables the portability of high-level applications to a diverse set of accelerator devices with minimal programmer effort. The abstraction enables a layered software architecture that provides for an efficient combination of application phases to a single asynchronous application description from multiple domain-specific input languages. As proofs of the abstraction layer serving its purpose for the layers above and below it, we show how a domain-specific input description ONNX can be implemented on top of this portability abstraction, and how it also allows driving fixed function and FPGA-based hardware accelerators below in the hardware-specific backend. We also provide an example implementation of the abstraction to show that the abstraction layer does not seem to incur significant execution time overhead.},
  archive      = {J_FCOMP},
  author       = {Leppänen, Topi and Lotvonen, Atro and Jääskeläinen, Pekka},
  doi          = {10.3389/fcomp.2022.945652},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {945652},
  shortjournal = {Front. Comput. Sci.},
  title        = {Cross-vendor programming abstraction for diverse heterogeneous platforms},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gamifying rehabilitation: MILORD platform as an upper limb
motion rehabilitation service. <em>FCOMP</em>, <em>4</em>, 932342. (<a
href="https://doi.org/10.3389/fcomp.2022.932342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor learning is based on the correct repetition of specific movements for their permanent storage in the central nervous system (CNS). Rehabilitation relies heavily on the repetition of specific movements, and game scenarios are ideal environments to build routines of repetitive exercises that have entertaining characteristics. In this respect, the gamification of the rehabilitation program, through the introduction of game-specific techniques and design concepts, has gained attention as a complementary or alternative to routine rehabilitation programs. A gamified rehabilitation program promises to gain the patient&#39;s attention, to reduce the monotony of the process and preserve motivation to attend, and to create virtual incentives through the game, toward maintaining compliance to the “prescribed” program. This is often achieved through goal-oriented tasks and real-time feedback in the form of points and other in-game rewards. This paper describes MILORD rehabilitation platform, an affordable technological solution, which aims to support health professionals and enable remote rehabilitation, while maintaining health service characteristics and monitoring. MILORD is an end-to-end platform that consists of an interactive computer game, utilizing a leap motion sensor, a centralized user management system, an analysis platform that processes the data generated by the game, and an analysis dashboard presenting a set of meaningful features that describe upper limb movement. Our solution facilitates the monitoring of the patients&#39; progress and provides an alternative way to analyze hand movement. The system was tested with normal subjects and patients and experts to record user&#39;s experience, receive feedback, identify any problems, and understand the system&#39;s value in monitoring and support motion defect and progress. This small-scale study indicated the capacity of the analysis to quantify the movement in a meaningful way and express the differences between normal and pathological movement, and the user experience was positive with both patients and normal subjects.},
  archive      = {J_FCOMP},
  author       = {Fotopoulos, Dimitris and Ladakis, Ioannis and Kilintzis, Vassilis and Chytas, Achilleas and Koutsiana, Elisavet and Loizidis, Theodoros and Chouvarda, Ioanna},
  doi          = {10.3389/fcomp.2022.932342},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {932342},
  shortjournal = {Front. Comput. Sci.},
  title        = {Gamifying rehabilitation: MILORD platform as an upper limb motion rehabilitation service},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). P3 problem and magnolia language: Specializing array
computations for emerging architectures. <em>FCOMP</em>, <em>4</em>,
931312. (<a href="https://doi.org/10.3389/fcomp.2022.931312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of producing portable high-performance computing (HPC) software that is cheap to develop and maintain is called the P3 (performance, portability, productivity) problem. Good solutions to the P3 problem have been achieved when the performance profiles of the target machines have been similar. The variety of HPC architectures is, however, large and can be expected to grow larger. Software for HPC therefore needs to be highly adaptable, and there is a pressing need to provide developers with tools to produce software that can target machines with vastly different profiles. Multi-dimensional array manipulation constitutes a core component of numerous numerical methods, such as finite difference solvers of Partial Differential Equations (PDEs). The efficiency of these computations is tightly connected to traversing and distributing array data in a hardware-friendly way. The Mathematics of Arrays (MoA) allows for formally reasoning about array computations and enables systematic transformations of array-based programs, e.g., to use data layouts that fit to a specific architecture. This paper presents a programming methodology aimed for tackling the P3 problem in domains that are well-explored using Magnolia, a language designed to embody generic programming. The Magnolia programmer can restrict the semantic properties of abstract generic types and operations by defining so-called axioms. Axioms can be used to produce tests for concrete implementations of specifications, for formal verification, or to perform semantics-preserving program transformations. We leverage Magnolia&#39;s semantic specification facilities to extend the Magnolia compiler with a term rewriting system. We implement MoA&#39;s transformation rules in Magnolia, and demonstrate through a case study on a finite difference solver of PDEs how our rewriting system allows exploring the space of possible optimizations.},
  archive      = {J_FCOMP},
  author       = {Chetioui, Benjamin and Larnøy, Marius Kleppe and Järvi, Jaakko and Haveraaen, Magne and Mullin, Lenore},
  doi          = {10.3389/fcomp.2022.931312},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {931312},
  shortjournal = {Front. Comput. Sci.},
  title        = {P3 problem and magnolia language: Specializing array computations for emerging architectures},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mitigation strategies for participant non-attendance in VR
remote collaborative experiments. <em>FCOMP</em>, <em>4</em>, 928269.
(<a href="https://doi.org/10.3389/fcomp.2022.928269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 led to the temporary closure of many HCI research facilities disrupting many ongoing user studies. While some studies could easily move online, this has proven problematic for virtual reality (VR) studies. The main challenge of remote VR study is the recruitment of participants who have access to specialized hardware such as head-mounted displays. This challenge is exacerbated in collaborative VR studies, where multiple participants need to be available and remotely connect to the study simultaneously. We identify the latter as the worst-case scenario regarding resource wastage and frustration. Across two collaborative user studies, we identified the personal connection between the experimenter and the participant as a critical factor in reducing non-attendance. We compare three recruitment strategies that we have iteratively developed based on our recent experiences. We introduce a metric to quantify the cost for each recruitment strategy, and we show that our final strategy achieves the best metric score. Our work is valuable for HCI researchers recruiting participants for collaborative VR remote studies, but it can be easily extended to every remote experiment scenario.},
  archive      = {J_FCOMP},
  author       = {Bovo, Riccardo and Giunchi, Daniele and Costanza, Enrico and Steed, Anthony and Heinis, Thomas},
  doi          = {10.3389/fcomp.2022.928269},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {928269},
  shortjournal = {Front. Comput. Sci.},
  title        = {Mitigation strategies for participant non-attendance in VR remote collaborative experiments},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unconscious load changer: Designing method to subtly
influence load perception by simply presenting modified myoelectricity
sensor information. <em>FCOMP</em>, <em>4</em>, 914525. (<a
href="https://doi.org/10.3389/fcomp.2022.914525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systems of presenting myoelectricity sensor information allow users to understand the body&#39;s load for various purposes, such as medical rehabilitation and sports training. If there is a method to create the psychological phenomenon of unconsciously increasing or decreasing a user&#39;s load perception simply by changing how to present the myoelectricity sensor values, it will help design a more effective system. Therefore, we propose a method to manipulate load perception by presenting modified myoelectricity sensor information. The proposed method aims to induce higher or lower load perception by modifying the actual myoelectric value to a higher or lower value. We implemented a prototype system and evaluated our method for the two types of load perception of weight perception and fatigue perception when handling objects. The result showed that most subjects unconsciously increased or decreased their load perception to match the presented myoelectric value, while the minority subjects got the opposite response from the majority subjects. This result indicates the feasibility of user assistance systems that use this phenomenon for a good purpose, such as systems that slightly reduce the load perception during physical activity. On the other hand, this result also indicates the feasibility of systems that use this phenomenon for a bad purpose, such as systems that increase user&#39;s fatigue to harm user&#39;s activity. This study provides helpful findings for designing and using sensor information presentation systems considering the psychological phenomenon.},
  archive      = {J_FCOMP},
  author       = {Futami, Kyosuke and Seki, Tomoya and Murao, Kazuya},
  doi          = {10.3389/fcomp.2022.914525},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {914525},
  shortjournal = {Front. Comput. Sci.},
  title        = {Unconscious load changer: Designing method to subtly influence load perception by simply presenting modified myoelectricity sensor information},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up deep neural architecture search for wearable
activity recognition with early prediction of converged performance.
<em>FCOMP</em>, <em>4</em>, 914330. (<a
href="https://doi.org/10.3389/fcomp.2022.914330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has the potential to uncover more performant networks for human activity recognition from wearable sensor data. However, a naive evaluation of the search space is computationally expensive. We introduce neural regression methods for predicting the converged performance of a deep neural network (DNN) using validation performance in early epochs and topological and computational statistics. Our approach shows a significant improvement in predicting converged testing performance over a naive approach taking the ranking of the DNNs at an early epoch as an indication of their ranking on convergence. We apply this to the optimization of the convolutional feature extractor of an LSTM recurrent network using NAS with deep Q-learning, optimizing the kernel size, number of kernels, number of layers, and the connections between layers, allowing for arbitrary skip connections and dimensionality reduction with pooling layers. We find architectures which achieve up to 4% better F1 score on the recognition of gestures in the Opportunity dataset than our implementation of DeepConvLSTM and 0.8% better F1 score than our implementation of state-of-the-art model Attend and Discriminate, while reducing the search time by more than 90% over a random search. This opens the way to rapidly search for well-performing dataset-specific architectures. We describe the computational implementation of the system (software frameworks, computing resources) to enable replication of this work. Finally, we lay out several future research directions for NAS which the community may pursue to address ongoing challenges in human activity recognition, such as optimizing architectures to minimize power, minimize sensor usage, or minimize training data needs.},
  archive      = {J_FCOMP},
  author       = {Pellatt, Lloyd and Roggen, Daniel},
  doi          = {10.3389/fcomp.2022.914330},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {914330},
  shortjournal = {Front. Comput. Sci.},
  title        = {Speeding up deep neural architecture search for wearable activity recognition with early prediction of converged performance},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Embodiment enables non-predictive ways of coping with
self-caused sensory stimuli. <em>FCOMP</em>, <em>4</em>, 896465. (<a
href="https://doi.org/10.3389/fcomp.2022.896465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Living systems process sensory data to facilitate adaptive behavior. A given sensor can be stimulated as the result of internally driven activity, or by purely external (environmental) sources. It is clear that these inputs are processed differently—have you ever tried tickling yourself? Self-caused stimuli have been shown to be attenuated compared to externally caused stimuli. A classical explanation of this effect is that when the brain sends a signal that would result in motor activity, it uses a copy of that signal to predict the sensory consequences of the resulting motor activity. The predicted sensory input is then subtracted from the actual sensory input, resulting in attenuation of the stimuli. To critically evaluate the utility of this predictive approach for coping with self-caused stimuli, and investigate when non-predictive solutions may be viable, we implement a computational model of a simple embodied system with self-caused sensorimotor dynamics, and use a genetic algorithm to explore the solutions possible in this model. We find that in this simple system the solutions that emerge modify their behavior to shape or avoid self-caused sensory inputs, rather than predicting these self-caused inputs and filtering them out. In some cases, solutions take advantage of the presence of these self-caused inputs. The existence of these non-predictive solutions demonstrates that embodiment provides possibilities for coping with self-caused sensory interference without the need for an internal, predictive model.},
  archive      = {J_FCOMP},
  author       = {Garner, James and Egbert, Matthew D.},
  doi          = {10.3389/fcomp.2022.896465},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {896465},
  shortjournal = {Front. Comput. Sci.},
  title        = {Embodiment enables non-predictive ways of coping with self-caused sensory stimuli},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Skeletons, object shape, statistics. <em>FCOMP</em>,
<em>4</em>, 842637. (<a
href="https://doi.org/10.3389/fcomp.2022.842637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects and object complexes in 3D, as well as those in 2D, have many possible representations. Among them skeletal representations have special advantages and some limitations. For the special form of skeletal representation called “s-reps,” these advantages include strong suitability for representing slabular object populations and statistical applications on these populations. Accomplishing these statistical applications is best if one recognizes that s-reps live on a curved shape space. Here we will lay out the definition of s-reps, their advantages and limitations, their mathematical properties, methods for fitting s-reps to single- and multi-object boundaries, methods for measuring the statistics of these object and multi-object representations, and examples of such applications involving statistics. While the basic theory, ideas, and programs for the methods are described in this paper and while many applications with evaluations have been produced, there remain many interesting open opportunities for research on comparisons to other shape representations, new areas of application and further methodological developments, many of which are explicitly discussed here.},
  archive      = {J_FCOMP},
  author       = {Pizer, Stephen M. and Marron, J. S. and Damon, James N. and Vicory, Jared and Krishna, Akash and Liu, Zhiyuan and Taheri, Mohsen},
  doi          = {10.3389/fcomp.2022.842637},
  journal      = {Frontiers in Computer Science},
  month        = {10},
  pages        = {842637},
  shortjournal = {Front. Comput. Sci.},
  title        = {Skeletons, object shape, statistics},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Exploring the technological needs of older
adults: Advances in design, functionality, user experience, and
age-related cognitive and sensory aids to facilitate adoption.
<em>FCOMP</em>, <em>4</em>, 1043652. (<a
href="https://doi.org/10.3389/fcomp.2022.1043652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Cachero, Cristina and Chaparro, Alex and Wood, Joanne M.},
  doi          = {10.3389/fcomp.2022.1043652},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1043652},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: exploring the technological needs of older adults: advances in design, functionality, user experience, and age-related cognitive and sensory aids to facilitate adoption},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Group analysis and multi-agent
interaction—bridging the gap between social investigations and
computational analysis. <em>FCOMP</em>, <em>4</em>, 1030966. (<a
href="https://doi.org/10.3389/fcomp.2022.1030966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Böck, Ronald and Allen, Joseph A.},
  doi          = {10.3389/fcomp.2022.1030966},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1030966},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Group analysis and multi-agent interaction—Bridging the gap between social investigations and computational analysis},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A survey on peer-to-peer energy trading for local
communities: Challenges, applications, and enabling technologies.
<em>FCOMP</em>, <em>4</em>, 1008504. (<a
href="https://doi.org/10.3389/fcomp.2022.1008504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity is still a highly demanding resource in developing countries, especifically in rural areas. Local communities in developing countries are now focusing on generating their own electricity using solar and trading the energy which is in access to the supplier. Energy trading at the peer-to-peer level is a novel paradigm. It allows customers to contribute energy to the grid and power generation and supply companies. In this paper, we investigate the energy trading enabling technologies, their applications, challenges, and benefits to the local communities from various aspects. We analyse how blockchain and IoT technologies can help energy trading at the customer level from recent research proposals. We also present our analysis of recent research on the implementation of energy trading technologies and future directions.},
  archive      = {J_FCOMP},
  author       = {Nadeem, Adnan},
  doi          = {10.3389/fcomp.2022.1008504},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {1008504},
  shortjournal = {Front. Comput. Sci.},
  title        = {A survey on peer-to-peer energy trading for local communities: Challenges, applications, and enabling technologies},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The privacy and control paradoxes in the context of
smartphone apps. <em>FCOMP</em>, <em>4</em>, 986138. (<a
href="https://doi.org/10.3389/fcomp.2022.986138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines how various factors, such as the degree of e-privacy concerns and control over data access permissions, can influence a user&#39;s intention to install a smartphone app. We conducted two survey-based experiments with 441 participants. In each experiment, we manipulated the degree of control over the number and type of data access permissions granted to different fictional apps. In Study 1, participants were informed about the set of permissions the apps required. In Study 2, participants indicated which individual permissions they were willing to grant to the apps. In both experiments, we assessed the level of e-privacy concerns, perceived app importance, and the intention to install the apps. The results suggest that the type of app plays a central role in determining both the perceived benefit of installing the app and the level of e-privacy concerns. The intention to install an app is more strongly associated with perceived app importance than with e-privacy concerns (especially when app importance is high, and users have explicit control over which specific data access permissions they want to grant). The implications of these results are discussed regarding psychological factors involved in app installation decision-making process and the importance of promoting data protection by design.},
  archive      = {J_FCOMP},
  author       = {Ayres-Pereira, Vanessa and Pirrone, Angelo and Korbmacher, Max and Tjostheim, Ingvar and Böhm, Gisela},
  doi          = {10.3389/fcomp.2022.986138},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {986138},
  shortjournal = {Front. Comput. Sci.},
  title        = {The privacy and control paradoxes in the context of smartphone apps},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How do participants collaborate during an online hackathon?
An empirical, quantitative study of communication traces.
<em>FCOMP</em>, <em>4</em>, 983164. (<a
href="https://doi.org/10.3389/fcomp.2022.983164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Starting as niche programming events, hackathons have since become a popular form of collaboration. Events are organized in various domains across the globe, aiming to foster innovation and learning, create and expand communities and tackle civic and environmental issues. While research around such events has grown in recent years, most studies are based on observations of a few individuals during an event and on post-hoc interviews during which participants report their experiences. Such studies are helpful but somewhat limited in that they do not allow us to study how individuals communicate at scale using technology. To address this gap, we conducted an archival analysis of communication traces of teams during a 48-h event. Our findings indicate that teams scaffold their communication around the design of an event, influenced by milestones set by the organizers. The officially selected communication platform&#39;s main use was to organize the event and the teams and to facilitate contact between participants and hackathon officials. We further investigated the balance of intra-team communication on the given platform and the potential use of additional communication tools.},
  archive      = {J_FCOMP},
  author       = {Schulten, Cleo and Nolte, Alexander and Spikol, Daniel and Chounta, Irene-Angelica},
  doi          = {10.3389/fcomp.2022.983164},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {983164},
  shortjournal = {Front. Comput. Sci.},
  title        = {How do participants collaborate during an online hackathon? an empirical, quantitative study of communication traces},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparing consumer grade sleep trackers for research
purposes: A field study. <em>FCOMP</em>, <em>4</em>, 971793. (<a
href="https://doi.org/10.3389/fcomp.2022.971793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep tracking has been rapidly developing alongside wearable technologies and digital trackers are increasingly being used in research, replacing diaries and other more laborious methods. In this work, we describe the user expectations and experiences of four different sleep tracking devices used simultaneously during week-long field deployment. The sensor-based data collection was supplemented with qualitative data from a 2-week long daily questionnaire period which overlapped with device usage for a period of 1 week. We compare the sleep data on each of the tracking nights between all four devices, and showcase that while each device has been validated with the polysomnography (PSG) gold standard, the devices show highly varying results in everyday use. Differences between devices for measuring sleep duration or sleep stages on a single night can be up to an average of 1 h 36 min. Study participants provided their expectations and experiences with the devices, and provided qualitative insights into their usage throughout the daily questionnaires. The participants assessed each device according to ease of use, functionality and reliability, and comfortability and effect on sleep disturbances. We conclude the work with lessons learned and recommendations for researchers who wish to conduct field studies using digital sleep trackers, and how to mitigate potential challenges and problems that might arise regarding data validity and technical issues.},
  archive      = {J_FCOMP},
  author       = {Kuosmanen, Elina and Visuri, Aku and Risto, Roosa and Hosio, Simo},
  doi          = {10.3389/fcomp.2022.971793},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {971793},
  shortjournal = {Front. Comput. Sci.},
  title        = {Comparing consumer grade sleep trackers for research purposes: A field study},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How (and why) to think that the brain is literally a
computer. <em>FCOMP</em>, <em>4</em>, 970396. (<a
href="https://doi.org/10.3389/fcomp.2022.970396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between brains and computers is often taken to be merely metaphorical. However, genuine computational systems can be implemented in virtually any media; thus, one can take seriously the view that brains literally compute. But without empirical criteria for what makes a physical system genuinely a computational one, computation remains a matter of perspective, especially for natural systems (e.g., brains) that were not explicitly designed and engineered to be computers. Considerations from real examples of physical computers—both analog and digital, contemporary and historical—make clear what those empirical criteria must be. Finally, applying those criteria to the brain shows how we can view the brain as a computer (probably an analog one at that), which, in turn, illuminates how that claim is both informative and falsifiable.},
  archive      = {J_FCOMP},
  author       = {Maley, Corey J.},
  doi          = {10.3389/fcomp.2022.970396},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {970396},
  shortjournal = {Front. Comput. Sci.},
  title        = {How (and why) to think that the brain is literally a computer},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meaningful, gamified training of reading fluency.
<em>FCOMP</em>, <em>4</em>, 968137. (<a
href="https://doi.org/10.3389/fcomp.2022.968137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving learners&#39; reading is of importance. The digital world is centred on the written word, and today&#39;s labour market requires high literacy levels. Furthermore, the development of school and foreign language skills among learners, especially those of weaker learners, is crucial as the effects of globalisation allow for increased work mobility and the necessity for lifelong learning. GameLet provides effective training scenarios for reading fluency, a prerequisite for reading comprehension, in schools with gamified, self-guided, personalised, media-based individual and collaborative learning, thereby allowing educators to intensify and extend learning activities to out-of-school settings. At the core of GameLet lies the production of a podcast by the learners that encourages them to read repeatedly, hence improving their reading fluency, and to successfully record their role in a digital Recording Studio. Increasing reading fluency is targeted by means of media-supported individual and cooperative learning phases with various training methods. Furthermore, GameLet implements meaningful digital media-based Gamification mechanisms for the purpose of increasing student motivation. The software is web-based and was developed with a focus on user-centred design and an agile and design-based approach. Prototype development followed an iterative and participative process, in which students and teachers of three participating partner countries interacted with the developed materials. Artefacts were tested in both face-to-face and online settings. GameLet exemplifies the successful application of Gamification for improving and extending classical learning scenarios at school as well as the design of effective learning technologies that are meaningful, gamified, effective and usable.},
  archive      = {J_FCOMP},
  author       = {Massler, Ute and Müller, Wolfgang and Iurgel, Ido and Haake, Susanne and Gantikow, Alexander and Hadzilacos, Thanasis},
  doi          = {10.3389/fcomp.2022.968137},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {968137},
  shortjournal = {Front. Comput. Sci.},
  title        = {Meaningful, gamified training of reading fluency},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Materializing the abstract: Understanding AI by game
jamming. <em>FCOMP</em>, <em>4</em>, 959351. (<a
href="https://doi.org/10.3389/fcomp.2022.959351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we argue that game jam formats are uniquely suited to engage participants in learning about artificial intelligence (AI) as a design material because of four factors which are characteristic of game jams: 1) Game jams provide an opportunity for hands-on, interactive prototyping, 2) Game jams encourage playful participation, 3) Game jams encourage creative combinations of AI and game development, and 4) Game jams offer understandable goals and evaluation metrics for AI. We support the argument with an interview study conducted with three AI experts who had all organized game jams with a focus on using AI in game development. Based on a thematic analysis of the expert interviews and a theoretical background of Schön&#39;s work on educating the reflective practitioner, we identified the four abovementioned factors as well as four recommendations for structuring and planning an AI-focused game jam: 1) Aligning repertoires, 2) Supporting playful participation, 3) Supporting ideation, and 4) Facilitating evaluation and reflection. Our contribution is motivated by the recent discourse on general challenges and recommendations of teaching AI identified by related literature, here under the long and intertwined history of games and AI in general. The article presents an initial discussion of the value of game jam formats for learning about AI and which factors need to be considered in regard to this specific learning goal.},
  archive      = {J_FCOMP},
  author       = {Falk, Jeanette and Inie, Nanna},
  doi          = {10.3389/fcomp.2022.959351},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {959351},
  shortjournal = {Front. Comput. Sci.},
  title        = {Materializing the abstract: Understanding AI by game jamming},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blazing fire or breezy wind? A story-driven playful
experience for annotating dance movement. <em>FCOMP</em>, <em>4</em>,
957274. (<a href="https://doi.org/10.3389/fcomp.2022.957274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The annotation of animated motion-captured segments is a challenging, interdisciplinary task, especially when it comes to characterizing movement qualitatively. The lack of intuitive, easy-to-learn-and-use frameworks is considered to be one of the biggest challenges in this process; another is the lack of approaches able to motivate a wide audience of users, from the broader public to dance experts, researchers and performers, to contribute with annotations. In this paper we present Motion Hollow, a story-driven playful experience that uses metaphors based on Laban Movement Analysis, an established framework for movement analysis and annotation, to familiarize novice users with the process of qualitative characterization of dance moves. This work proposes a first step into introducing movement annotation to non-expert users, and as such, its main goal is to explore the implications and potential of such an approach. The evaluation of the experience confirms its potential to transform the annotation of dance movement segments into an engaging and enjoyable experience as well as to foster a deeper understanding of movement annotation both as a concept and process.},
  archive      = {J_FCOMP},
  author       = {Kougioumtzian, Lori and El Raheb, Katerina and Katifori, Akrivi and Roussou, Maria},
  doi          = {10.3389/fcomp.2022.957274},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {957274},
  shortjournal = {Front. Comput. Sci.},
  title        = {Blazing fire or breezy wind? a story-driven playful experience for annotating dance movement},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The reality of remote extended reality research: Practical
case studies and taxonomy. <em>FCOMP</em>, <em>4</em>, 954038. (<a
href="https://doi.org/10.3389/fcomp.2022.954038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote user studies—those where the experimenter and participant are not physically located together—offer challenges and opportunities in HCI research in general, and extended reality (XR) research specifically. The COVID-19 pandemic has forced this form of research to overcome a long period of unprecedented circumstances. However, this experience has produced a lot of lessons learned that should be shared. We propose guidelines based on findings from a set of six remote virtual reality studies, by analyzing participants&#39; and researchers&#39; feedback. These studies ranged from one-session types to longitudinal ones and spanned a variety of subjects such as cybersickness, selection tasks, and visual search. In this paper, we offer a big-picture summary of how we conducted these studies, our research design considerations, our findings in these case studies, and what worked well and what did not in different scenarios. Additionally, we propose a taxonomy for devising such studies in a systematic and easy-to-follow manner. We argue that the XR community should move from theoretical proposals and thought pieces to testing and sharing practical data-informed proposals and guidelines.},
  archive      = {J_FCOMP},
  author       = {Kroma, Assem and Grinyer, Kristen and Scavarelli, Anthony and Samimi, Elaheh and Kyian, Stanislav and Teather, Robert J.},
  doi          = {10.3389/fcomp.2022.954038},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {954038},
  shortjournal = {Front. Comput. Sci.},
  title        = {The reality of remote extended reality research: Practical case studies and taxonomy},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smaller progress measures and separating automata for parity
games. <em>FCOMP</em>, <em>4</em>, 936903. (<a
href="https://doi.org/10.3389/fcomp.2022.936903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calude et al. have recently shown that parity games can be solved in quasi-polynomial time, a landmark result that has led to several approaches with quasi-polynomial complexity. Jurdzinski and Lazic have further improved the precise complexity of parity games, especially when the number of priorities is low (logarithmic in the number of positions). Both of these algorithms belong to a class of game solving techniques now often called separating automata: deterministic automata that can be used as witness automata to decide the winner in parity games up to a given number of states and colors. We suggest several adjustments to the approach of Calude et al. that lead to smaller statespaces. These include and improve over those earlier introduced by Fearnley et al. We identify two of them that, together, lead to a statespace of exactly the same size Jurdzinski and Lazic&#39;s concise progress measures, which currently hold the crown as the smallest statespace. The remaining improvements, hence, lead to a further reduction in the size of the statespace, making our approach the most succinct progress measure available for parity games.},
  archive      = {J_FCOMP},
  author       = {Dell&#39;Erba, Daniele and Schewe, Sven},
  doi          = {10.3389/fcomp.2022.936903},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {936903},
  shortjournal = {Front. Comput. Sci.},
  title        = {Smaller progress measures and separating automata for parity games},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Working with robotic process automation: User experience
after 18 months of adoption. <em>FCOMP</em>, <em>4</em>, 936146. (<a
href="https://doi.org/10.3389/fcomp.2022.936146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports a study of User Experience (UX) with Robotic Process Automation (RPA), in the perspective of workers of EdP Brazil, a large electric utility company that operates in Brazil. RPA are software solutions for automating business processes that find increased interest of companies because they are inserted in workgroups as a co-worker, emulating human workers operating on GUI interfaces. Although the technology promises to drive a new wave of productivity in service companies, its impact on co-workers&#39; experience is still unexplored. Based on projective interviews using the AXE (Anticipated eXperience Evaluation) protocol, after the first 18 months of RPA operation, the analysis of workers&#39; collaboration with the robots has evidenced multiple facets of UX, technology acceptance and innovation adoption. For this case, RPA has provided an overall positive user experience mainly due to the perceived utility of the spared time, the upgrade in career opportunities and the pride for actively participating in the innovation adoption. Negative experience comes mainly from the lack of visibility that hinders robot management for efficiency and improvement. The methodology used in the study was successful in capturing the multifaceted workers&#39; experience and is potentially useful to support user research in new expansion RPA projects.},
  archive      = {J_FCOMP},
  author       = {Filgueiras, Lucia Vilela Leite and Corrêa, Pedro Luiz Pizzigatti and Alves-Souza, Solange N. and Teodoro, Sigmar Monroe and Silva, Mariana Savarezze Pereira da and Encinas Quille, Rosa Virginia and Demuner, Vanessa Rafaela de Souza},
  doi          = {10.3389/fcomp.2022.936146},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {936146},
  shortjournal = {Front. Comput. Sci.},
  title        = {Working with robotic process automation: User experience after 18 months of adoption},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing a play-anywhere handheld AR storytelling app
using remote data collection. <em>FCOMP</em>, <em>4</em>, 927177. (<a
href="https://doi.org/10.3389/fcomp.2022.927177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive story experiences like immersive theater productions and escape rooms have grown in popularity in recent years, offering the audience a more active role in the events portrayed. However, many of these activities were forced to close at the start of the COVID-19 pandemic, arising from restrictions placed on group activities and travel. This created an opportunity for a story experience that users could take part in around their local neighborhoods. Five mobile applications (apps) were developed toward this goal, aiming to make effective use of available local map data, alongside virtual content overlaid on users&#39; surroundings through Augmented Reality (AR), to offer additional story features not present in the real environment. The first two apps investigated the feasibility of such an approach, including the remote field testing of the apps, where participants used their own devices across a variety of locations. Two follow-up apps further aimed to offer an improved user experience, also adopting a more standardized testing procedure, to better ensure each app was completed in an intended manner by those participating remotely. Participants rated their experience through immersion and engagement questionnaire factors that tested for their appropriateness to rate such experiences, in addition to providing their feedback. A final app applied the same AR story implementation to a curated site-specific study, once pandemic restrictions had eased. This combination of remote studies and subsequent curated study offered a reverse methodology to much previous research in this field, but was found to offer advantages in corroborating the results of the remote studies, and also in offering new insights to further improve such an AR story app, that is designed to be used at an outdoor location of the user&#39;s choosing. Such an app offers benefits to those who may prefer the opportunity to take part in such an activity solo or close to home, as well as for storytellers to develop an outside story for use at a variety of locations, making it available to a larger audience, without the challenges and costs in migrating it to different locations.},
  archive      = {J_FCOMP},
  author       = {Raeburn, Gideon and Welton, Martin and Tokarchuk, Laurissa},
  doi          = {10.3389/fcomp.2022.927177},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {927177},
  shortjournal = {Front. Comput. Sci.},
  title        = {Developing a play-anywhere handheld AR storytelling app using remote data collection},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Investigating (re)current state-of-the-art in human activity
recognition datasets. <em>FCOMP</em>, <em>4</em>, 924954. (<a
href="https://doi.org/10.3389/fcomp.2022.924954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many human activities consist of physical gestures that tend to be performed in certain sequences. Wearable inertial sensor data have as a consequence been employed to automatically detect human activities, lately predominantly with deep learning methods. This article focuses on the necessity of recurrent layers—more specifically Long Short-Term Memory (LSTM) layers—in common Deep Learning architectures for Human Activity Recognition (HAR). Our experimental pipeline investigates the effects of employing none, one, or two LSTM layers, as well as different layers&#39; sizes, within the popular DeepConvLSTM architecture. We evaluate the architecture&#39;s performance on five well-known activity recognition datasets and provide an in-depth analysis of the per-class results, showing trends which type of activities or datasets profit the most from the removal of LSTM layers. For 4 out of 5 datasets, an altered architecture with one LSTM layer produces the best prediction results. In our previous work we already investigated the impact of a 2-layered LSTM when dealing with sequential activity data. Extending upon this, we now propose a metric, rGP, which aims to measure the effectiveness of learned temporal patterns for a dataset and can be used as a decision metric whether to include recurrent layers into a network at all. Even for datasets including activities without explicit temporal processes, the rGP can be high, suggesting that temporal patterns were learned, and consequently convolutional networks are being outperformed by networks including recurrent layers. We conclude this article by putting forward the question to what degree popular HAR datasets contain unwanted temporal dependencies, which if not taken care of, can benefit networks in achieving high benchmark scores and give a false sense of overall generability to a real-world setting.},
  archive      = {J_FCOMP},
  author       = {Bock, Marius and Hoelzemann, Alexander and Moeller, Michael and Van Laerhoven, Kristof},
  doi          = {10.3389/fcomp.2022.924954},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {924954},
  shortjournal = {Front. Comput. Sci.},
  title        = {Investigating (re)current state-of-the-art in human activity recognition datasets},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IBSync: Intra-body synchronization and implicit
contextualization of wearable devices using artificial ECG landmarks.
<em>FCOMP</em>, <em>4</em>, 915448. (<a
href="https://doi.org/10.3389/fcomp.2022.915448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a smaller form factor and a larger set of applications, body-worn devices have evolved into a collection of simultaneously deployed hardware units, rather than into a single all-round wearable. The sensor data, logged by such devices across the user&#39;s body, contains a wealth of information but is often difficult to synchronize. Especially the application of machine learning techniques, e.g., for activity recognition, suffers from the inaccuracy of the devices&#39; internal clocks. In recent years, intra-body communication emerged as a promising alternative to the traditional wired and wireless communication techniques. Distributed wearable systems will notably benefit from its advantages, such as a superior energy efficiency. However, due to the absence of commercially available platforms, applications using this innovative technique remain rare and underinvestigated. With IBSync, we present a novel concept in which artificial landmark signals are received by body-worn devices on touching, approaching, or passing certain areas, surfaces, or objects with embedded transmitter beacons. The landmark signals enable both the wearables&#39; intentional or incidental synchronization as well as the implicit contextualization using supplementary information about the beacons&#39; situational context. For the detection of the landmarks, we propose to repurpose the on-board ECG sensor front-end available in recent commercial wearable devices. Evaluated on a total of 215 min of recordings from two devices, we demonstrate the concept&#39;s feasibility and a promising synchronization error of 0.80±1.79 samples or 6.25±14.00 ms at a device&#39;s sampling rate of 128 Hz.},
  archive      = {J_FCOMP},
  author       = {Wolling, Florian and Van Laerhoven, Kristof},
  doi          = {10.3389/fcomp.2022.915448},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {915448},
  shortjournal = {Front. Comput. Sci.},
  title        = {IBSync: Intra-body synchronization and implicit contextualization of wearable devices using artificial ECG landmarks},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Face beautification: Beyond makeup transfer. <em>FCOMP</em>,
<em>4</em>, 910233. (<a
href="https://doi.org/10.3389/fcomp.2022.910233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial appearance plays an important role in our social lives. Subjective perception of women&#39;s beauty depends on various face-related (e.g., skin, shape, hair) and environmental (e.g., makeup, lighting, angle) factors. Similarly to cosmetic surgery in the physical world, virtual face beautification is an emerging field with many open issues to be addressed. Inspired by the latest advances in style-based synthesis and face beauty prediction, we propose a novel framework for face beautification. For a given reference face with a high beauty score, our GAN-based architecture is capable of translating an inquiry face into a sequence of beautified face images with the referenced beauty style and the target beauty score values. To achieve this objective, we propose to integrate both style-based beauty representation (extracted from the reference face) and beauty score prediction (trained on the SCUT-FBP database) into the beautification process. Unlike makeup transfer, our approach targets many-to-many (instead of one-to-one) translation, where multiple outputs can be defined by different references with various beauty scores. Extensive experimental results are reported to demonstrate the effectiveness and flexibility of the proposed face beautification framework. To support reproducible research, the source codes accompanying this work will be made publicly available on GitHub.},
  archive      = {J_FCOMP},
  author       = {Liu, Xudong and Wang, Ruizhe and Peng, Hao and Yin, Minglei and Chen, Chih-Fan and Li, Xin},
  doi          = {10.3389/fcomp.2022.910233},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {910233},
  shortjournal = {Front. Comput. Sci.},
  title        = {Face beautification: Beyond makeup transfer},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Thoughts on the usage of audible smiling in speech synthesis
applications. <em>FCOMP</em>, <em>4</em>, 885657. (<a
href="https://doi.org/10.3389/fcomp.2022.885657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this perspective paper we explore the question how audible smiling can be integrated in speech synthesis applications. In human-human communication, smiling can serve various functions, such as signaling politeness or as a marker of trustworthiness and other aspects that raise and maintain the social likeability of a speaker. However, in human-machine communication, audible smiling is nearly unexplored, but could be an advantage in different applications such as dialog systems. The rather limited knowledge of the details of audible smiling and their exploitation for speech synthesis applications is a great challenge. This is also true for modeling smiling in spoken dialogs and testing it with users. Thus, this paper argues to fill the research gaps in identifying factors that constitute and affect audible smiling in order to incorporate it in speech synthesis applications. The major claim is to focus on the dynamics of audible smiling on various levels.},
  archive      = {J_FCOMP},
  author       = {Trouvain, Jürgen and Weiss, Benjamin},
  doi          = {10.3389/fcomp.2022.885657},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {885657},
  shortjournal = {Front. Comput. Sci.},
  title        = {Thoughts on the usage of audible smiling in speech synthesis applications},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ani-GIFs: A benchmark dataset for domain generalization of
action recognition from GIFs. <em>FCOMP</em>, <em>4</em>, 876846. (<a
href="https://doi.org/10.3389/fcomp.2022.876846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models perform remarkably well for the same task under the assumption that data is always coming from the same distribution. However, this is generally violated in practice, mainly due to the differences in data acquisition techniques and the lack of information about the underlying source of new data. Domain generalization targets the ability to generalize to test data of an unseen domain; while this problem is well-studied for images, such studies are significantly lacking in spatiotemporal visual content—videos and GIFs. This is due to (1) the challenging nature of misalignment of temporal features and the varying appearance/motion of actors and actions in different domains, and (2) spatiotemporal datasets being laborious to collect and annotate for multiple domains. We collect and present the first synthetic video dataset of Animated GIFs for domain generalization, Ani-GIFs, that is used to study the domain gap of videos vs. GIFs, and animated vs. real GIFs, for the task of action recognition. We provide a training and testing setting for Ani-GIFs, and extend two domain generalization baseline approaches, based on data augmentation and explainability, to the spatiotemporal domain to catalyze research in this direction.},
  archive      = {J_FCOMP},
  author       = {Majumdar, Shoumik Sovan and Jain, Shubhangi and Tourni, Isidora Chara and Mustafin, Arsenii and Lteif, Diala and Sclaroff, Stan and Saenko, Kate and Bargal, Sarah Adel},
  doi          = {10.3389/fcomp.2022.876846},
  journal      = {Frontiers in Computer Science},
  month        = {9},
  pages        = {876846},
  shortjournal = {Front. Comput. Sci.},
  title        = {Ani-GIFs: A benchmark dataset for domain generalization of action recognition from GIFs},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Recognizing the state of emotion, cognition and
action from physiological and behavioral signals. <em>FCOMP</em>,
<em>4</em>, 998416. (<a
href="https://doi.org/10.3389/fcomp.2022.998416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Chen, Siyuan and Cho, Youngjun and Yu, Kun and Ferrari, Laura M. and Bremond, Francois},
  doi          = {10.3389/fcomp.2022.998416},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {998416},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Recognizing the state of emotion, cognition and action from physiological and behavioral signals},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The contribution of different face parts to deep face
recognition. <em>FCOMP</em>, <em>4</em>, 958629. (<a
href="https://doi.org/10.3389/fcomp.2022.958629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of face recognition improvements still lacks knowledge on what parts of the face are important. In this article, the authors present face parts analysis to obtain important recognition information in a certain area of the face, more than just the eye or eyebrow, from the black box perspective. In addition, the authors propose a more advanced way to select parts without introducing artifacts using the average face and morphing. Furthermore, multiple face recognition systems are used to analyze the face component contribution. Finally, the results show that the four deep face recognition systems produce a different behavior for each experiment. However, the eyebrows are still the most important part of deep face recognition systems. In addition, the face texture played an important role deeper than the face shape.},
  archive      = {J_FCOMP},
  author       = {Lestriandoko, Nova Hadi and Veldhuis, Raymond and Spreeuwers, Luuk},
  doi          = {10.3389/fcomp.2022.958629},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {958629},
  shortjournal = {Front. Comput. Sci.},
  title        = {The contribution of different face parts to deep face recognition},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Augmented humans. <em>FCOMP</em>, <em>4</em>,
957927. (<a href="https://doi.org/10.3389/fcomp.2022.957927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Kosch, Thomas and Abdelrahman, Yomna and Zhou, Bo},
  doi          = {10.3389/fcomp.2022.957927},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {957927},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Augmented humans},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Remote evaluation of augmented reality interaction with
personal health information. <em>FCOMP</em>, <em>4</em>, 934694. (<a
href="https://doi.org/10.3389/fcomp.2022.934694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses novel research methods used to examine how Augmented Reality (AR) can be utilized to present “omic” (i.e., genomes, microbiomes, pathogens, allergens) information to non-expert users. While existing research shows the potential of AR as a tool for personal health, methodological challenges pose a barrier to the ways in which AR research can be conducted. There is a growing need for new evaluation methods for AR systems, especially as remote testing becomes increasingly popular. In this article, we present two AR studies adapted for remote research environments in the context of personal health. The first study (n = 355) is a non-moderated remote study conducted using an AR web application to explore the effect of layering abstracted pathogens and mitigative behaviors on a user, on perceived risk perceptions, negative affect, and behavioral intentions. This study introduces methods that address participant precursor requirements, diversity of platforms for delivering the AR intervention, unsupervised setups, and verification of participation as instructed. The second study (n = 9) presents the design and moderated remote evaluation of a technology probe, a prototype of a novel AR tool that overlays simulated timely and actionable environmental omic data in participants&#39; living environment, which helps users to contextualize and make sense of the data. Overall, the two studies contribute to the understanding of investigating AR as a tool for health behavior and interventions for remote, at-home, empirical studies.},
  archive      = {J_FCOMP},
  author       = {Shaer, Orit and Otiono, Jennifer and Qian, Ziyue and Seals, Ayanna and Nov, Oded},
  doi          = {10.3389/fcomp.2022.934694},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {934694},
  shortjournal = {Front. Comput. Sci.},
  title        = {Remote evaluation of augmented reality interaction with personal health information},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lumen: A machine learning framework to expose influence cues
in texts. <em>FCOMP</em>, <em>4</em>, 929515. (<a
href="https://doi.org/10.3389/fcomp.2022.929515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phishing and disinformation are popular social engineering attacks with attackers invariably applying influence cues in texts to make them more appealing to users. We introduce Lumen, a learning-based framework that exposes influence cues in text: (i) persuasion, (ii) framing, (iii) emotion, (iv) objectivity/subjectivity, (v) guilt/blame, and (vi) use of emphasis. Lumen was trained with a newly developed dataset of 3K texts comprised of disinformation, phishing, hyperpartisan news, and mainstream news. Evaluation of Lumen in comparison to other learning models showed that Lumen and LSTM presented the best F1-micro score, but Lumen yielded better interpretability. Our results highlight the promise of ML to expose influence cues in text, toward the goal of application in automatic labeling tools to improve the accuracy of human-based detection and reduce the likelihood of users falling for deceptive online content.},
  archive      = {J_FCOMP},
  author       = {Shi, Hanyu and Silva, Mirela and Giovanini, Luiz and Capecci, Daniel and Czech, Lauren and Fernandes, Juliana and Oliveira, Daniela},
  doi          = {10.3389/fcomp.2022.929515},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {929515},
  shortjournal = {Front. Comput. Sci.},
  title        = {Lumen: A machine learning framework to expose influence cues in texts},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolutional kernel function algebra. <em>FCOMP</em>,
<em>4</em>, 921454. (<a
href="https://doi.org/10.3389/fcomp.2022.921454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many systems for image manipulation, signal analysis, machine learning, and scientific computing make use of discrete convolutional filters that are known before computation begins. These contexts benefit from common sub-expression elimination to reduce the number of calculations required, both multiplications and additions. We present an algebra for describing convolutional kernels and filters at a sufficient level of abstraction to enable intuitive common sub-expression based optimizations through decomposing filters into smaller, repeated, kernels. This enables the creation of an enormous search space of potential implementations of filters via algebraic manipulation. We demonstrate how integral image and sliding window optimizations can be expressed in the context of common sub-expression elimination as well as show the direct use case for this algebra in massively SIMD multiply-free contexts such as in cellular processor arrays. We then show that this algebra is general enough to express and optimize kernels that use non-standard semi-rings to enable shortest path algorithms.},
  archive      = {J_FCOMP},
  author       = {Stow, Edward and Kelly, Paul H. J.},
  doi          = {10.3389/fcomp.2022.921454},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {921454},
  shortjournal = {Front. Comput. Sci.},
  title        = {Convolutional kernel function algebra},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social robots as eating companions. <em>FCOMP</em>,
<em>4</em>, 909844. (<a
href="https://doi.org/10.3389/fcomp.2022.909844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research shows that eating together (i.e., commensality) impacts food choice, time spent eating, and enjoyment. Conversely, eating alone is considered a possible cause of unhappiness. In this paper, we conceptually explore how interactive technology might allow for the creation of artificial commensal companions: embodied agents providing company to humans during meals (e.g., a person living in isolation due to health reasons). We operationalize this with the design of our commensal companion: a system based on the MyKeepon robot, paired with a Kinect sensor, able to track the human commensal&#39;s activity (i.e., food picking and intake) and able to perform predefined nonverbal behavior in response. In this preliminary study with 10 participants, we investigate whether this autonomous social robot-based system can positively establish an interaction that humans perceive and whether it can influence their food choices. In this study, the participants are asked to taste some chocolates with and without the presence of an artificial commensal companion. The participants are made to believe that the study targets the food experience, whilst the presence of a robot is accidental. Next, we analyze their food choices and feedback regarding the role and social presence of the artificial commensal during the task performance. We conclude the paper by discussing the lessons we learned about the first interactions we observed between a human and a social robot in a commensality setting and by proposing future steps and more complex applications for this novel kind of technology.},
  archive      = {J_FCOMP},
  author       = {Niewiadomski, Radoslaw and Bruijnes, Merijn and Huisman, Gijs and Gallagher, Conor Patrick and Mancini, Maurizio},
  doi          = {10.3389/fcomp.2022.909844},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {909844},
  shortjournal = {Front. Comput. Sci.},
  title        = {Social robots as eating companions},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The emotions effect on a virtual characters design–a student
perspective analysis. <em>FCOMP</em>, <em>4</em>, 892597. (<a
href="https://doi.org/10.3389/fcomp.2022.892597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interaction between people and virtual characters through digital and electronic devices is a reality. In this context, the design of virtual characters must incorporate emotional expression at a nonverbal level looking for effective communication with the user. This exploratory study investigates the design features of an avatar functioning as a virtual assistant in educational contexts. From a multidisciplinary approach, the user&#39;s research was elaborated by a semi-open questionnaire of self-perception of emotional characteristics: likeability, attractiveness, and applicability of a set of six 2D and 3D characters. The results extracted from a sample of 69 university students provide a relevant information on design features and open new lines for future research. Aspects such as Ekman&#39;s basic emotion discrimination and the design of facial expression are analyzed. The incorporation of other body parts, their spatial orientation and contextual elements, seems to contribute to effective emotional communication. The results also highlight how the design of a virtual character should take into consideration the complexity involved in facial gestures and changes in relation to the vertical axis and planes of movement. Finally, this article discusses the complexity involved in expressing a given emotion in a virtual character.},
  archive      = {J_FCOMP},
  author       = {del Valle-Canencia, Marta and Moreno Martínez, Carlos and Rodríguez-Jiménez, Rosa-María and Corrales-Paredes, Ana},
  doi          = {10.3389/fcomp.2022.892597},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {892597},
  shortjournal = {Front. Comput. Sci.},
  title        = {The emotions effect on a virtual characters design–A student perspective analysis},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human factors affecting ventilation in australian classrooms
during the COVID-19 pandemic: Toward insourcing occupants’ proficiency
in ventilation management. <em>FCOMP</em>, <em>4</em>, 888688. (<a
href="https://doi.org/10.3389/fcomp.2022.888688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underventilation in classrooms is associated with poorer academic performance and greater transmission risk of COVID-19. In a study involving data from CO2 logging in 67 classrooms in Brisbane, Australia, it was found that more than half of the classrooms monitored experienced between 5 and 50 separate instances of CO2 concentrations exceeding 1,800 ppm, a level at which cognitive performance reductions have been recorded and which is considered high risk for COVID-19 transmission. The research identifies a number of human-related factors affecting ventilation in certain classrooms, including the disabling of window operation to minimize the potential for student interference, keeping windows closed in naturally ventilated buildings to improve energy efficiency, difficult to reach switches for exhaust fans and perceptions of the likelihood of remedial action being taken. Identifying Inbodied Interaction as a useful lens to enable users themselves to better identify and remedy instances of poor IAQ, the paper contributes: (1) Insight into the CO2 concentrations experienced in Australian classrooms during the COVID pandemic; (2) Identification of human-factors contributing to the ventilation—and underventilation—of the rooms monitored; and (3) Suggestions for how to foster greater awareness of ventilation among classroom occupants and translate awareness into more active, informed, and healthier ventilation behaviors from occupants, using principles of Inbodied Interaction.},
  archive      = {J_FCOMP},
  author       = {Snow, Stephen and Danam, Royce and Leardini, Paola and Glencross, Mashhuda and Beeson, Brett and Ottenhaus, Lisa-Mareike and Boden, Marie},
  doi          = {10.3389/fcomp.2022.888688},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {888688},
  shortjournal = {Front. Comput. Sci.},
  title        = {Human factors affecting ventilation in australian classrooms during the COVID-19 pandemic: Toward insourcing occupants&#39; proficiency in ventilation management},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Utilization of wearable smartwatch and its application among
saudi population. <em>FCOMP</em>, <em>4</em>, 874841. (<a
href="https://doi.org/10.3389/fcomp.2022.874841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Study purposeIt is important to understand the users&#39; perceptions toward the use of smartwatches and the various factors that affect the adoption of smart watches. These findings would contribute to the literature in understanding users&#39; usage, preferences, needs, and expectations about smartwatches that would aid smartwatches designers and also decision-makers in integrating the smartwatch technology in various service-enabled areas such a healthcare, e-learning etc. To address this gap, the objective of this study was formulated to understand the actual consumers&#39; perceptions of toward the use of smartwatches and explore the critical factors affecting the adoption and intention to use smartwatches in the Kingdom of Saudi Arabia.MethodsA cross-sectional study was designed to assess the actual perception of use smartwatches and to investigate the influencing factors that affect the utilization among Saudi population using the survey technique. Questionnaire design was based on the domains of Technology Acceptance Model (TAM) to determine the factors affecting smartwatches utilization. The sample composed of Saudi Arabian residents aged 18 years and above. The response rate for the online questionnaire that was distributed through the social media applications was 58.61% representing 135 participants. The data was collected in November 2020 and analyzed using the Statistical Packages for Software Sciences (SPSS).ResultsThe prevalence of participants who had knowledge about smartwatch was 94.1%. When comparing the demographic characteristics between those owning and not owning a smartwatch, it was found that educational level (X2 = 9.365; p = 0.025) and knowledge about smartwatch (X2 = 7.897; p = 0.005) had significant relationship with owning a smartwatch. When comparing between design aesthetic, perceived usefulness, ease of use, enjoyment and healthology in relation to the socio demographic characteristics, it was found that respondents in the older age group (≥45 years) (F = 11.797; p &amp;lt;0.001) and those with master degree (F = 3.449; p = 0.002) observed to have significantly lower mean score in design aesthetic while females exhibited significantly higher score in perceived enjoyment and healthology (T = −3.629; p = 0.001) as well as design aesthetic (T = −2.070; p = 0.043).ConclusionFactors such as age, education, gender, income can significantly affect the adoption of wearable devices in Saudi Arabia.},
  archive      = {J_FCOMP},
  author       = {Alaskari, Entesar and Alanzi, Turki and Alrayes, Saja and Aljabri, Duaa and Almulla, Salma and Alsalman, Demah and Algumzi, Areej and Alameri, Rana and Alakrawi, Zahraa and Alnaim, Norah and Almusfar, Latifa and Alotaibi, Leyan and Saraireh, Linah and Attar, Razaz and Bakhshwain, Amal and Almuhanna, Afnan and AlSanad, Duha and Alenazi, Fahad and Mushcab, Hayat and Alanezi, Nouf and Alenazi, Naif},
  doi          = {10.3389/fcomp.2022.874841},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {874841},
  shortjournal = {Front. Comput. Sci.},
  title        = {Utilization of wearable smartwatch and its application among saudi population},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method for sperm activity analysis based on feature point
detection network in deep learning. <em>FCOMP</em>, <em>4</em>, 861495.
(<a href="https://doi.org/10.3389/fcomp.2022.861495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sperm motility is an important index to evaluate semen quality. Computer-assisted sperm analysis (CASA) is based on the sperm image, through the image-processing algorithm to detect the position of the sperm target and track tracking, so as to judge the sperm activity. Because of the small and dense sperm targets in sperm images, traditional image-processing algorithms take a long time to detect sperm targets, while target-detection algorithms based on the deep learning have a lot of missed detection problems in the process of sperm target detection. In order to accurately and efficiently analyze sperm activity in the sperm image sequence, this article proposes a sperm activity analysis method based on the deep learning. First, the sperm position is detected through the deep learning feature point detection network based on the improved SuperPoint, then the multi-sperm target tracking is carried out through SORT and the sperm motion trajectory is drawn, and at last the sperm survival is judged through the sperm trajectory to realize the analysis of sperm activity. The experimental results show that this method can effectively analyze the sperm activity in the sperm image sequence. At the same time, the average detection speed of the sperm target detection method in the detection process is 65fps, and the detection accuracy is 92%.},
  archive      = {J_FCOMP},
  author       = {Chen, Zhong and Yang, Jinkun and Luo, Chen and Zhang, Changheng},
  doi          = {10.3389/fcomp.2022.861495},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {861495},
  shortjournal = {Front. Comput. Sci.},
  title        = {A method for sperm activity analysis based on feature point detection network in deep learning},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving classification results on a small medical dataset
using a GAN; an outlook for dealing with rare disease datasets.
<em>FCOMP</em>, <em>4</em>, 858874. (<a
href="https://doi.org/10.3389/fcomp.2022.858874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For clinical decision support systems, automated classification algorithms on medical image data have become more important in the past. For such computer vision problems, deep convolutional neural networks (DCNNs) have made breakthroughs. These often require large, annotated, and privacy-cleared datasets as a prerequisite for gaining high-quality results. This proves to be difficult with rare diseases due to limited incidences. Therefore, it is hard to sensitize clinical decision support systems to identify these diseases at an early stage. It has been shown several times, that synthetic data can improve the results of clinical decision support systems. At the same time, the greatest problem for the generation of these synthetic images is the data basis. In this paper, we present four different methods to generate synthetic data from a small dataset. The images are from 2D magnetic resonance tomography of the spine. The annotation resulted in 540 healthy, 47 conspicuously non-pathological, and 106 conspicuously pathological vertebrae. Four methods are presented to obtain optimal generation results in each of these classes. The obtained generation results are then evaluated with a classification net. With this procedure, we showed that adding synthetic annotated data has a positive impact on the classification results of the original data. In addition, one of our methods is appropriate to generate synthetic image data from &amp;lt;50 images. Thus, we found a general approach for dealing with small datasets in rare diseases, which can be used to build sensitized clinical decision support systems to detect and treat these diseases at an early stage.},
  archive      = {J_FCOMP},
  author       = {Röglin, Julia and Ziegeler, Katharina and Kube, Jana and König, Franziska and Hermann, Kay-Geert and Ortmann, Steffen},
  doi          = {10.3389/fcomp.2022.858874},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {858874},
  shortjournal = {Front. Comput. Sci.},
  title        = {Improving classification results on a small medical dataset using a GAN; an outlook for dealing with rare disease datasets},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploring views on affective haptic devices in times of
COVID-19. <em>FCOMP</em>, <em>4</em>, 795927. (<a
href="https://doi.org/10.3389/fcomp.2022.795927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective haptic devices (AHDs) are communication technologies utilizing the sense of touch, and include mediated social touch (MST), symbolic haptic messaging, and awareness systems that, for example, let one feel another person&#39;s heartbeat. The COVID-19 pandemic and consequent social distancing measures have led to a reemphasis of the importance of social touch, and many people have experienced firsthand what it is like to miss touching loved ones. This offers an excellent opportunity to study people&#39;s intention to use AHDs. For this purpose, a survey study (n = 277) was conducted combining qualitative and quantitative data analysis methods. Touch deprivation, resulting from not being able to touch a loved one, was associated with intention to use AHDs: the more deprived an individual, the higher his or her intention to use AHDs. Technology readiness and touch aversion did not affect intention to use AHDs. AHDs for symbolic messaging gained higher interest than MST and awareness devices, and long-distance relationships were seen as the most likely scenario for using AHDs. Bi-directionality, synchronicity, and symmetry were regarded as important features for providing shared meaning and a sense of connectedness. Reviewability, multimodality, and actuation type were also deemed important. Limitations of the study and implications for the design of AHDs are discussed.},
  archive      = {J_FCOMP},
  author       = {Ipakchian Askari, Sima and Huisman, Gijs and Haans, Antal and IJsselsteijn, Wijnand A.},
  doi          = {10.3389/fcomp.2022.795927},
  journal      = {Frontiers in Computer Science},
  month        = {8},
  pages        = {795927},
  shortjournal = {Front. Comput. Sci.},
  title        = {Exploring views on affective haptic devices in times of COVID-19},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trusted execution environments: Applications and
organizational challenges. <em>FCOMP</em>, <em>4</em>, 930741. (<a
href="https://doi.org/10.3389/fcomp.2022.930741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lack of trust in the providers is still a major barrier to cloud computing adoption – especially when sensitive data is involved. While current privacy-enhancing technologies, such as homomorphic encryption, can increase security, they come with a considerable performance overhead. As an alternative Trusted Executing Environment (TEE) provides trust guarantees for code execution in the cloud similar to transport layer security for data transport or advanced encryption standard algorithms for data storage. Cloud infrastructure providers like Amazon, Google, and Microsoft introduced TEEs as part of their infrastructure offerings. This review will shed light on the different technological options of TEEs, as well as give insight into organizational issues regarding their usage.},
  archive      = {J_FCOMP},
  author       = {Geppert, Tim and Deml, Stefan and Sturzenegger, David and Ebert, Nico},
  doi          = {10.3389/fcomp.2022.930741},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {930741},
  shortjournal = {Front. Comput. Sci.},
  title        = {Trusted execution environments: Applications and organizational challenges},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot and weakly supervised repetition counting with
body-worn accelerometers. <em>FCOMP</em>, <em>4</em>, 925108. (<a
href="https://doi.org/10.3389/fcomp.2022.925108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates few-shot weakly supervised repetition counting of a human action such as workout using a wearable inertial sensor. We present WeakCounterF that leverages few weakly labeled segments containing occurrences of a target action from a target user to achieve precise repetition counting. Here, a weak label is defined to specify only the number of repetitions of an action included in an input data segment in this study, facilitating preparation of datasets for repetition counting. First, WeakCounterF leverages data augmentation and label diversification techniques to generate augmented diverse training data from weakly labeled data from users other than a target user, i.e., source users. Then, WeakCounterF generates diverse weakly labeled training data from few weakly labeled training data from the target user. Finally, WeakCounterF trains its repetition counting model composed of an attention mechanism on the augmented diversified data from the source users, and then fine-tunes the model on the diversified data from the target user.},
  archive      = {J_FCOMP},
  author       = {Nishino, Yuuki and Maekawa, Takuya and Hara, Takahiro},
  doi          = {10.3389/fcomp.2022.925108},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {925108},
  shortjournal = {Front. Comput. Sci.},
  title        = {Few-shot and weakly supervised repetition counting with body-worn accelerometers},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gastrointestinal tract-based implicit measures for
cognition, emotion and behavior. <em>FCOMP</em>, <em>4</em>, 899507. (<a
href="https://doi.org/10.3389/fcomp.2022.899507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit physiological measures such as heart rate and skin conductance convey information about someone&#39;s cognitive or affective state. Currently, gastrointestinal (GI) tract-based markers are not yet considered while both the organs involved as well as the microbiota populating the GI tract are bidirectionally connected to the brain and have a relation to emotion, cognition and behavior. This makes GI tract-based measures relevant and interesting, especially because the relation may be causal, and because they have a different timescale than current physiological measures. This perspective paper (1) presents the (mechanistic) involvement of the GI tract and its microbiota in emotion, cognition and behavior; (2) explores the added value of microbiome-based implicit measures as complementary to existing measures; and (3) sets the priorities to move forward. Five potential measures are proposed and discussed in more detail: bowel movement, short-chain fatty acids, tyrosine and tryptophan, GI tract flora composition, and cytokine levels. We conclude (1) that the involvement of the GI tract in emotion, cognition and behavior is undisputed, (2) that GI tract-based implicit measures are still in a conceptual phase of development but show potential and (3) that the first step to bring this field forward is to start validation studies in healthy humans and that are designed in the context of implicit measurements.},
  archive      = {J_FCOMP},
  author       = {van Erp, Jan B. F.},
  doi          = {10.3389/fcomp.2022.899507},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {899507},
  shortjournal = {Front. Comput. Sci.},
  title        = {Gastrointestinal tract-based implicit measures for cognition, emotion and behavior},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward privacy-aware federated analytics of cohorts for
smart mobility. <em>FCOMP</em>, <em>4</em>, 891206. (<a
href="https://doi.org/10.3389/fcomp.2022.891206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based Behavioral Analytics (LBA) holds a great potential for improving the services available in smart cities. Naively implemented, such an approach would track the movements of every citizen and share their location traces with the various smart service providers—similar to today&#39;s Web analytics systems that track visitors across the web sites they visit. This study presents a novel privacy-aware approach to location-based federated analytics that removes the need for individuals to share their location traces with a central server. The general approach is to model the behavior of cohorts instead of modeling specific users. Using a federated approach, location data is processed locally on user devices and only shared in anonymized fashion with a server. The server aggregates the data using Secure Multiparty Computation (SMPC) into service-defined cohorts, whose data is then used to provide cohort analytics (e.g., demographics) for the various smart service providers. The approach was evaluated on three real-life datasets with varying dropout rates, i.e., clients not being able to participate in the SMPC rounds. The results show that our approach can privately estimate various cohort demographics (e.g., percentages of male and female visitors) with an error between 0 and 8 percentage points relative to the actual cohort percentages. Furthermore, we experimented with predictive models for estimating these cohort percentages 1-week ahead. Across all three datasets, the best-performing predictive model achieved a Pearson&#39;s correlation coefficient above 0.8 (strong correlation), and a Mean Absolute Error (MAE) between 0 and 10 (0 is the minimum and 100 is the maximum). We conclude that privacy-aware LBA can be achieved using existing mobile technologies and federated analytics.},
  archive      = {J_FCOMP},
  author       = {Gjoreski, Martin and Laporte, Matías and Langheinrich, Marc},
  doi          = {10.3389/fcomp.2022.891206},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {891206},
  shortjournal = {Front. Comput. Sci.},
  title        = {Toward privacy-aware federated analytics of cohorts for smart mobility},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PERF: Performant, explicit radiance fields. <em>FCOMP</em>,
<em>4</em>, 871808. (<a
href="https://doi.org/10.3389/fcomp.2022.871808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel way of approaching image-based 3D reconstruction based on radiance fields. The problem of volumetric reconstruction is formulated as a non-linear least-squares problem and solved explicitly without the use of neural networks. This enables the use of solvers with a higher rate of convergence than what is typically used for neural networks, and fewer iterations are required until convergence. The volume is represented using a grid of voxels, with the scene surrounded by a hierarchy of environment maps. This makes it possible to get clean reconstructions of 360° scenes where the foreground and background is separated. A number of synthetic and real scenes from well-known benchmark-suites are successfully reconstructed with quality on par with state-of-the-art methods, but at significantly reduced reconstruction times.},
  archive      = {J_FCOMP},
  author       = {Rasmuson, Sverker and Sintorn, Erik and Assarsson, Ulf},
  doi          = {10.3389/fcomp.2022.871808},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {871808},
  shortjournal = {Front. Comput. Sci.},
  title        = {PERF: Performant, explicit radiance fields},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A review on the role of affective stimuli in event-related
frontal alpha asymmetry. <em>FCOMP</em>, <em>4</em>, 869123. (<a
href="https://doi.org/10.3389/fcomp.2022.869123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frontal alpha asymmetry refers to the difference between the right and left alpha activity over the frontal brain region. Increased activity in the left hemisphere has been linked to approach motivation and increased activity in the right hemisphere has been linked to avoidance or withdrawal. However, research on alpha asymmetry is diverse and has shown mixed results, which may partly be explained by the potency of the used stimuli to emotionally and motivationally engage participants. This review gives an overview of the types of affective stimuli utilized with the aim to identify which stimuli elicit a strong approach-avoidance effect in an affective context. We hope this contributes to better understanding of what is reflected by alpha asymmetry, and in what circumstances it may be an informative marker of emotional state. We systematically searched the literature for studies exploring event-related frontal alpha asymmetry in affective contexts. The search resulted in 61 papers, which were categorized in five stimulus categories that were expected to differ in their potency to engage participants: images &amp;amp; sounds, videos, real cues, games and other tasks. Studies were viewed with respect to the potency of the stimuli to evoke significant approach-avoidance effects on their own and in interaction with participant characteristics or condition. As expected, passively perceived stimuli that are multimodal or realistic, seem more potent to elicit alpha asymmetry than unimodal stimuli. Games, and other stimuli with a strong task-based component were expected to be relatively engaging but approach-avoidance effects did not seem to be much clearer than the studies using perception of videos and real cues. While multiple factors besides stimulus characteristics determine alpha asymmetry, and we did not identify a type of affective stimulus that induces alpha asymmetry highly consistently, our results indicate that strongly engaging, salient and/or personally relevant stimuli are important to induce an approach-avoidance effect.},
  archive      = {J_FCOMP},
  author       = {Sabu, Priya and Stuldreher, Ivo V. and Kaneko, Daisuke and Brouwer, Anne-Marie},
  doi          = {10.3389/fcomp.2022.869123},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {869123},
  shortjournal = {Front. Comput. Sci.},
  title        = {A review on the role of affective stimuli in event-related frontal alpha asymmetry},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Acoustic-based automatic addressee detection for technical
systems: A review. <em>FCOMP</em>, <em>4</em>, 831784. (<a
href="https://doi.org/10.3389/fcomp.2022.831784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ObjectiveAcoustic addressee detection is a challenge that arises in human group interactions, as well as in interactions with technical systems. The research domain is relatively new, and no structured review is available. Especially due to the recent growth of usage of voice assistants, this topic received increased attention. To allow a natural interaction on the same level as human interactions, many studies focused on the acoustic analyses of speech. The aim of this survey is to give an overview on the different studies and compare them in terms of utilized features, datasets, as well as classification architectures, which has so far been not conducted.MethodsThe survey followed the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) guidelines. We included all studies which were analyzing acoustic and/or acoustic characteristics of speech utterances to automatically detect the addressee. For each study, we describe the used dataset, feature set, classification architecture, performance, and other relevant findings.Results1,581 studies were screened, of which 23 studies met the inclusion criteria. The majority of studies utilized German or English speech corpora. Twenty-six percent of the studies were tested on in-house datasets, where only limited information is available. Nearly 40% of the studies employed hand-crafted feature sets, the other studies mostly rely on Interspeech ComParE 2013 feature set or Log-FilterBank Energy and Log Energy of Short-Time Fourier Transform features. 12 out of 23 studies used deep-learning approaches, the other 11 studies used classical machine learning methods. Nine out of 23 studies furthermore employed a classifier fusion.ConclusionSpeech-based automatic addressee detection is a relatively new research domain. Especially by using vast amounts of material or sophisticated models, device-directed speech is distinguished from non-device-directed speech. Furthermore, a clear distinction between in-house datasets and pre-existing ones can be drawn and a clear trend toward pre-defined larger feature sets (with partly used feature selection methods) is apparent.},
  archive      = {J_FCOMP},
  author       = {Siegert, Ingo and Weißkirchen, Norman and Wendemuth, Andreas},
  doi          = {10.3389/fcomp.2022.831784},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {831784},
  shortjournal = {Front. Comput. Sci.},
  title        = {Acoustic-based automatic addressee detection for technical systems: A review},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gameful design of an application for patients in
rehabilitation. <em>FCOMP</em>, <em>4</em>, 822167. (<a
href="https://doi.org/10.3389/fcomp.2022.822167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design process of any interactive application is an important part of its lifecycle, since it largely defines its structure, means of interaction with the users and its actual content. In the case of applications related to medical uses and self-help, it is even more important, given the aims of the application, the diversity of target users and the urgent need for increased retention. In this article, we present a gameful design process for a mobile application targeted toward patients in rehabilitation, implementing concepts related to increasing user rapport and motivation through gamification, and means to offer guidance and personalized services to improve user experience. Both gamification and personalization build on narrative concepts, by putting patients in the place of a “hero”, offering them the opportunity to overcome “challenges” and receive a clear view of their progress (a.k.a. a “hero&#39;s journey”), both in terms of physical and mental condition. Finally, we discuss measurable indicators used to evaluate the application in terms of the progress that patients showed, their motivation and interest, and degree of adherence to the exercise plans.},
  archive      = {J_FCOMP},
  author       = {Menychtas, Andreas and Galliakis, Michael and Pardos, Antonis and Panagopoulos, Christos and Karpouzis, Kostas and Maglogiannis, Ilias},
  doi          = {10.3389/fcomp.2022.822167},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {822167},
  shortjournal = {Front. Comput. Sci.},
  title        = {Gameful design of an application for patients in rehabilitation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncovering inclusivity gaps in design pedagogy through the
digital design marginalization framework. <em>FCOMP</em>, <em>4</em>,
822090. (<a href="https://doi.org/10.3389/fcomp.2022.822090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designers play a key role in the design of inclusive and socially conscious interfaces. Thus, it is imperative for designers to be thoughtful of the ethical and social implications of design. However, gaps in the foundational training that designers receive (e.g., as university students) can negatively impact their ability to consider the social implications of their design practice. This can result in consequences such as digital marginalization, which, as defined by the Digital Design Marginalization (DDM) framework, is the “pushing away”, whether intentional or not, of a defined group of users from a digital or online service or system, where the exclusion has additional, indirect, and long-lasting social consequences on that particular user group. Designers can contribute, even unintentionally, to digital marginalization through their design practices and the design choices they make. We argue that our role as educators includes ensuring not only that our design pedagogy is inclusive, but that the designers we train now are prepared to conduct their future design practice in a manner that is inclusive to all users. As such, we propose to use the Digital Design Marginalization as a lens to guide a reflection-based approach to identify gaps in our pedagogy that may lead to designers becoming ill-equipped to identify how their designs may lead to digital marginalization. Through seven case studies from our own teaching practice, we demonstrate the use of the DDM framework to guide marginalization-focused introspective reflections of curricula. These reflections through the DDM lens revealed gaps in our pedagogy with respect to providing future designers with training that enables them to consider the broader societal and individual implications of the design choices they will make in future practice. Based on our experience using the DDM framework, we then discuss in greater depth how reflection of social consequences of design pedagogy can be operationalized within institutions to reduce educational gaps that may be associated with design-mediated digital marginalization. Finally, we comment on avenues for further development of pedagogical reflection using DDM.},
  archive      = {J_FCOMP},
  author       = {Sin, Jaisie and Munteanu, Cosmin and Nixon, Michael and Pandeliev, Velian and Tigwell, Garreth W. and Shinohara, Kristen and Tang, Anthony and Szigeti, Steve},
  doi          = {10.3389/fcomp.2022.822090},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {822090},
  shortjournal = {Front. Comput. Sci.},
  title        = {Uncovering inclusivity gaps in design pedagogy through the digital design marginalization framework},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantics for combinatory logic with intersection types.
<em>FCOMP</em>, <em>4</em>, 792570. (<a
href="https://doi.org/10.3389/fcomp.2022.792570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a plethora of semantics of computational models, nevertheless, the semantics of combinatory logic are among the less investigated ones. In this paper, we propose semantics for the computational system of combinatory logic with intersection types. We define extensional applicative structures endowed with special elements corresponding to primitive combinators. We prove two soundness and completeness results. First, the equational theory of untyped combinatory logic is proven to be sound and complete with respect to the proposed semantics. Second, the system of the combinatory logic with intersection types is proven to be sound and complete with respect to the proposed semantics. The usual approach to the semantics for calculi with types that can be found in the literature is based on models for the untyped calculus endowed with a valuation of type variables which enables the interpretation of types to be defined inductively. We propose, however, a different approach. In the semantics we propose, the interpretation of types is represented as a family of subsets that satisfies certain properties, whereas for a given valuation of term variables, the interpretation of terms is defined inductively. Due to the wide applicability of semantics of computational models, the presented approach could be further developed to other computational models and beyond—to current and foreseen application of semantics to large distributed systems and new challenging technologies.},
  archive      = {J_FCOMP},
  author       = {Ghilezan, Silvia and Kašterović, Simona},
  doi          = {10.3389/fcomp.2022.792570},
  journal      = {Frontiers in Computer Science},
  month        = {7},
  pages        = {792570},
  shortjournal = {Front. Comput. Sci.},
  title        = {Semantics for combinatory logic with intersection types},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Towards omnipresent and smart speech assistants.
<em>FCOMP</em>, <em>4</em>, 966163. (<a
href="https://doi.org/10.3389/fcomp.2022.966163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Siegert, Ingo and Hillmann, Stefan and Weiss, Benjamin and Szczuka, Jessica M. and Karpov, Alexey},
  doi          = {10.3389/fcomp.2022.966163},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {966163},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Towards omnipresent and smart speech assistants},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Enhancing quality of life in ambient spaces.
<em>FCOMP</em>, <em>4</em>, 947056. (<a
href="https://doi.org/10.3389/fcomp.2022.947056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Waterworth, John and Chignell, Mark and Wiberg, Mikael},
  doi          = {10.3389/fcomp.2022.947056},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {947056},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Enhancing quality of life in ambient spaces},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Language and vision in robotics: Emerging neural
and on-device approaches. <em>FCOMP</em>, <em>4</em>, 930067. (<a
href="https://doi.org/10.3389/fcomp.2022.930067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Masala, Giovanni Luca and Esposito, Massimo and Maniscalco, Umberto and Calimera, Andrea},
  doi          = {10.3389/fcomp.2022.930067},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {930067},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: language and vision in robotics: emerging neural and on-device approaches},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Remote iVR for nutrition education: From design to
evaluation. <em>FCOMP</em>, <em>4</em>, 927161. (<a
href="https://doi.org/10.3389/fcomp.2022.927161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While different crowdsourcing platforms promote remote data collection, experiments in the immersive Virtual Reality (iVR) research community are predominantly performed in person. The COVID-19 pandemic, however, has forced researchers in different disciplines, including iVR, to seriously consider remote studies. In this paper, we present a remote study using the Immersive Virtual Alimentation and Nutrition (IVAN) application, designed to educate users about food-energy density and portion size control. We report on the results of a remote experiment with 45 users using the IVAN app. In IVAN, users actively construct knowledge about energy density by manipulating virtual food items, and explore the concept of portion size control through hypothesis testing and assembling virtual meals in iVR. To explore the feasibility of conducting remote iVR studies using an interactive health-related application for nutrition education, two conditions were devised (interactive vs. passive). The results demonstrate the feasibility of conducting remote iVR studies using health-related applications. Furthermore, the results also indicate that regardless of level of interactivity learners significantly improved their knowledge about portion size control after using the IVAN (p &amp;lt; 0.0001). Adding interactivity, however, suggests that the perceived learning experience of users could be partially affected. Learners reported significantly higher scores for immediacy of control in the interactive condition compared to those in the passive condition (p &amp;lt; 0.05). This study demonstrates the feasibility of conducting an unsupervised remote iVR experiment using a complex and interactive health-related iVR app.},
  archive      = {J_FCOMP},
  author       = {Sajjadi, Pejman and Edwards, Caitlyn G. and Zhao, Jiayan and Fatemi, Alex and Long, John W. and Klippel, Alexander and Masterson, Travis D.},
  doi          = {10.3389/fcomp.2022.927161},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {927161},
  shortjournal = {Front. Comput. Sci.},
  title        = {Remote iVR for nutrition education: From design to evaluation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Move with the theremin: Body posture and gesture recognition
using the theremin in loose-garment with embedded textile cables as
antennas. <em>FCOMP</em>, <em>4</em>, 915280. (<a
href="https://doi.org/10.3389/fcomp.2022.915280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel intelligent garment design approach for body posture/gesture detection in the form of a loose-fitting blazer prototype, “the MoCaBlazer.” The design is realized by leveraging conductive textile antennas with the capacitive sensing modality, supported by an open-source electronic theremin system (OpenTheremin). The use of soft textile antennas as the sensing element allows flexible garment design and seamless tech-garment integration for the specific structure of different clothes. Our novel approach is evaluated through two experiments involving defined movements (20 arm/torso gestures and eight dance movements). In cross-validation, the classification model yields up to 97.18% average accuracy and 92% f1-score, respectively. We have also explored real-time inference enabled by a radio frequency identification (RFID) synchronization method, yielding an f1-score of 82%. Our approach opens a new paradigm for designing motion-aware smart garments with soft conductive textiles beyond traditional approaches that rely on tight-fitting flexible sensors or rigid motion sensor accessories.},
  archive      = {J_FCOMP},
  author       = {Bello, Hymalai and Zhou, Bo and Suh, Sungho and Sanchez Marin, Luis Alfredo and Lukowicz, Paul},
  doi          = {10.3389/fcomp.2022.915280},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {915280},
  shortjournal = {Front. Comput. Sci.},
  title        = {Move with the theremin: Body posture and gesture recognition using the theremin in loose-garment with embedded textile cables as antennas},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An investigation into the sensitivity of personal
information and implications for disclosure: A UK perspective.
<em>FCOMP</em>, <em>4</em>, 908245. (<a
href="https://doi.org/10.3389/fcomp.2022.908245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perceived sensitivity of information is a crucial factor in both security and privacy concerns and the behaviors of individuals. Furthermore, such perceptions motivate how people disclose and share information with others. We study this topic by using an online questionnaire where a representative sample of 491 British citizens rated the sensitivity of different data items in a variety of scenarios. The sensitivity evaluations revealed in this study are compared to prior results from the US, Brazil and Germany, allowing us to examine the impact of culture. In addition to discovering similarities across cultures, we also identify new factors overlooked in the current research, including concerns about reactions from others, personal safety or mental health and finally, consequences of disclosure on others. We also highlight a difference between the regulatory perspective and the citizen perspective on information sensitivity. We then operationalized this understanding within several example use-cases exploring disclosures in the healthcare and finance industry, two areas where security is paramount. We explored the disclosures being made through two different interaction means: directly to a human or chatbot mediated (given that an increasing amount of personal data is shared with these agents in industry). We also explored the effect of anonymity in these contexts. Participants showed a significant reluctance to disclose information they considered “irrelevant” or “out of context” information disregarding other factors such as interaction means or anonymity. We also observed that chatbots proved detrimental to eliciting sensitive disclosures in the healthcare domain; however, within the finance domain, there was less effect. This article&#39;s findings provide new insights for those developing online systems intended to elicit sensitive personal information from users.},
  archive      = {J_FCOMP},
  author       = {Belen-Saglam, Rahime and Nurse, Jason R. C. and Hodges, Duncan},
  doi          = {10.3389/fcomp.2022.908245},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {908245},
  shortjournal = {Front. Comput. Sci.},
  title        = {An investigation into the sensitivity of personal information and implications for disclosure: A UK perspective},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Commentary: Metaphors we live by. <em>FCOMP</em>,
<em>4</em>, 890531. (<a
href="https://doi.org/10.3389/fcomp.2022.890531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Gomez-Marin, Alex},
  doi          = {10.3389/fcomp.2022.890531},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {890531},
  shortjournal = {Front. Comput. Sci.},
  title        = {Commentary: Metaphors we live by},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the use of efficient projection kernels for motion-based
visual saliency estimation. <em>FCOMP</em>, <em>4</em>, 867289. (<a
href="https://doi.org/10.3389/fcomp.2022.867289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the potential of a family of efficient filters—the Gray-Code Kernels (GCKs)—for addressing visual saliency estimation with a focus on motion information. Our implementation relies on the use of 3D kernels applied to overlapping blocks of frames and is able to gather meaningful spatio-temporal information with a very light computation. We introduce an attention module that reasons the use of pooling strategies, combined in an unsupervised way to derive a saliency map highlighting the presence of motion in the scene. A coarse segmentation map can also be obtained. In the experimental analysis, we evaluate our method on publicly available datasets and show that it is able to effectively and efficiently identify the portion of the image where the motion is occurring, providing tolerance to a variety of scene conditions and complexities.},
  archive      = {J_FCOMP},
  author       = {Nicora, Elena and Noceti, Nicoletta},
  doi          = {10.3389/fcomp.2022.867289},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {867289},
  shortjournal = {Front. Comput. Sci.},
  title        = {On the use of efficient projection kernels for motion-based visual saliency estimation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Go ahead, please!—evaluation of external human—machine
interfaces in a real-world crossing scenario. <em>FCOMP</em>,
<em>4</em>, 863072. (<a
href="https://doi.org/10.3389/fcomp.2022.863072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the future, automated vehicles (AVs) without a human driver will potentially have to manage communication with vulnerable road users, such as pedestrians, in everyday traffic interaction situations. The aim of this work is to investigate pedestrian reactions to external communication concepts in a controlled, but real-world crossing scenario. The focus is to investigate which properties of external human–machine interfaces (eHMIs) promote the comprehensibility of vehicle intention (yielding for the pedestrian) and therefore lead to faster and, at the same time, safer crossing decisions of pedestrians. For this purpose, three different eHMI concepts (intention-based light-band, perception-based light-band, and the combination of light-band and signal lamp) were examined and compared to a baseline (no eHMI). In a Wizard-of-Oz experiment, participants (n = 30) encountered a test vehicle equipped with the eHMIs in a real-world crossing scenario. The crossing initiation time in seconds and the participant&#39;s intention recognition were measured. Furthermore, the influence of the eHMIs on acceptance and perceived safety was evaluated. It was shown that the presence of the intention-based light-band, and the combination of light-band and signal lamp led to an earlier crossing decision compared to baseline with no eHMI. In summary, the results indicate that the intention-based light-band has a positive effect on the comprehensibility of the vehicle&#39;s intention. All concepts were evaluated positively regarding acceptance and perceived safety, and did not differ significantly from each other.},
  archive      = {J_FCOMP},
  author       = {Loew, Alexandra and Graefe, Julia and Heil, Lukas and Guthardt, Anne and Boos, Annika and Dietrich, André and Bengler, Klaus},
  doi          = {10.3389/fcomp.2022.863072},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {863072},
  shortjournal = {Front. Comput. Sci.},
  title        = {Go ahead, Please!—Evaluation of external Human—Machine interfaces in a real-world crossing scenario},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive-attentive geolocalization from few queries: A
hybrid approach. <em>FCOMP</em>, <em>4</em>, 841817. (<a
href="https://doi.org/10.3389/fcomp.2022.841817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the task of cross-domain visual geo-localization, where the goal is to geo-localize a given query image against a database of geo-tagged images, in the case where the query and the database belong to different visual domains. In particular, at training time, we consider having access to only few unlabeled queries from the target domain. To adapt our deep neural network to the database distribution, we rely on a 2-fold domain adaptation technique, based on a hybrid generative-discriminative approach. To further enhance the architecture, and to ensure robustness across domains, we employ a novel attention layer that can easily be plugged into existing architectures. Through a large number of experiments, we show that this adaptive-attentive approach makes the model robust to large domain shifts, such as unseen cities or weather conditions. Finally, we propose a new large-scale dataset for cross-domain visual geo-localization, called SVOX.},
  archive      = {J_FCOMP},
  author       = {Paolicelli, Valerio and Berton, Gabriele and Montagna, Francesco and Masone, Carlo and Caputo, Barbara},
  doi          = {10.3389/fcomp.2022.841817},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {841817},
  shortjournal = {Front. Comput. Sci.},
  title        = {Adaptive-attentive geolocalization from few queries: A hybrid approach},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of meal similarity on interpersonal synchronization
in three-party remote dining. <em>FCOMP</em>, <em>4</em>, 838229. (<a
href="https://doi.org/10.3389/fcomp.2022.838229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, online commensality, such as remote dining, has become a way to connect people in different places. In remote dining, people have drinks, snacks, or meals while chatting with each other via video calls and seek connectedness and belonging. However, many people feel that there is a gap between real-life and digital co-eating and that interaction in current remote dining fails to satisfy the need for companionship. Unlike real-life co-eating, in remote dining, one&#39;s meal may not be similar to that of a partner&#39;s because people usually prepare their own food separately. In this study, we focused on the effects of meal similarity on interpersonal synchronization and subjective feelings. We conducted a laboratory-based remote dining experiment and video analysis to investigate whether eating similar meals in remote conditions has any effect on eating behavior and to explore the relationship between meal similarity, interpersonal synchronization, and subjective feelings. The results showed that participants ate at a faster pace and conducted eating actions more frequently. They were more synchronized with their partners, and the feeling of togetherness was stronger. Thus, we suggest that preparing similar meals or ordering the same dishes can enhance the remote dining experience.},
  archive      = {J_FCOMP},
  author       = {Wang, Jui-Ying and Kubota, Sho and Inoue, Tomoo},
  doi          = {10.3389/fcomp.2022.838229},
  journal      = {Frontiers in Computer Science},
  month        = {6},
  pages        = {838229},
  shortjournal = {Front. Comput. Sci.},
  title        = {Effects of meal similarity on interpersonal synchronization in three-party remote dining},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Machine vision for assistive technologies.
<em>FCOMP</em>, <em>4</em>, 937433. (<a
href="https://doi.org/10.3389/fcomp.2022.937433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Leo, Marco and Farinella, Giovanni Maria and Furnari, Antonino and Medioni, Gerard},
  doi          = {10.3389/fcomp.2022.937433},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {937433},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Machine vision for assistive technologies},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Methods and tools for bioimage analysis.
<em>FCOMP</em>, <em>4</em>, 931939. (<a
href="https://doi.org/10.3389/fcomp.2022.931939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Levet, Florian and Uhlmann, Virginie and Jug, Florian},
  doi          = {10.3389/fcomp.2022.931939},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {931939},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Methods and tools for bioimage analysis},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Designing technology for emotions to improve
mental health and wellbeing. <em>FCOMP</em>, <em>4</em>, 898839. (<a
href="https://doi.org/10.3389/fcomp.2022.898839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Sterling, Leon and Pedell, Sonja and Craig, Claire},
  doi          = {10.3389/fcomp.2022.898839},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {898839},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Designing technology for emotions to improve mental health and wellbeing},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How COVID-19 has changed crowdfunding: Evidence from
GoFundMe. <em>FCOMP</em>, <em>4</em>, 893338. (<a
href="https://doi.org/10.3389/fcomp.2022.893338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the long-term effects of the COVID-19 pandemic have yet to be determined, its immediate impact on crowdfunding is nonetheless significant. This study adopts a computational approach to better understanding this consequence. We aim to gain insight into whether and how the COVID-19 pandemic has changed crowdfunding. Using a unique dataset of all GoFundMe campaigns published over the past 2 years, we explore the factors that have led to successfully funded crowdfunding projects. In particular, we study a corpus of 36,370 projects from November 2018 to December 2020 by analyzing cover images and other attributes commonly found on crowdfunding sites. We first construct a classifier and a regression model to assess the importance of features based on XGBoost. Next, we employ counterfactual analysis to investigate the causality between features and the success of crowdfunding. Furthermore, sentiment analysis and paired sample t-tests are performed to examine differences in crowdfunding campaigns before and after the COVID-19 outbreak in March 2020. Findings suggest a significant racial disparity in crowdfunding success. In addition, sad emotions expressed in a campaign&#39;s description became significant after the COVID-19 outbreak. This study enriches our understanding of the impact of the COVID-19 pandemic on crowdfunding as well as the prevalence of discrimination in crowdfunding.},
  archive      = {J_FCOMP},
  author       = {Wang, Junda and Luo, Jiebo and Zhang, Xupin},
  doi          = {10.3389/fcomp.2022.893338},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {893338},
  shortjournal = {Front. Comput. Sci.},
  title        = {How COVID-19 has changed crowdfunding: Evidence from GoFundMe},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Governance of responsible AI: From ethical guidelines to
cooperative policies. <em>FCOMP</em>, <em>4</em>, 873437. (<a
href="https://doi.org/10.3389/fcomp.2022.873437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly pervasive role of Artificial Intelligence (AI) in our societies is radically changing the way that social interaction takes place within all fields of knowledge. The obvious opportunities in terms of accuracy, speed and originality of research are accompanied by questions about the possible risks and the consequent responsibilities involved in such a disruptive technology. In recent years, this twofold aspect has led to an increase in analyses of the ethical and political implications of AI. As a result, there has been a proliferation of documents that seek to define the strategic objectives of AI together with the ethical precautions required for its acceptable development and deployment. Although the number of documents is certainly significant, doubts remain as to whether they can effectively play a role in safeguarding democratic decision-making processes. Indeed, a common feature of the national strategies and ethical guidelines published in recent years is that they only timidly address how to integrate civil society into the selection of AI objectives. Although scholars are increasingly advocating the necessity to include civil society, it remains unclear which modalities should be selected. If both national strategies and ethics guidelines appear to be neglecting the necessary role of a democratic scrutiny for identifying challenges, objectives, strategies and the appropriate regulatory measures that such a disruptive technology should undergo, the question is then, what measures can we advocate that are able to overcome such limitations? Considering the necessity to operate holistically with AI as a social object, what theoretical framework can we adopt in order to implement a model of governance? What conceptual methodology shall we develop that is able to offer fruitful insights to governance of AI? Drawing on the insights of classical pragmatist scholars, we propose a framework of democratic experimentation based on the method of social inquiry. In this article, we first summarize some of the main points of discussion around the potential societal, ethical and political issues of AI systems. We then identify the main answers and solutions by analyzing current national strategies and ethics guidelines. After showing the theoretical and practical limits of these approaches, we outline an alternative proposal that can help strengthening the active role of society in the discussion about the role and extent of AI systems.},
  archive      = {J_FCOMP},
  author       = {Gianni, Robert and Lehtinen, Santtu and Nieminen, Mika},
  doi          = {10.3389/fcomp.2022.873437},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {873437},
  shortjournal = {Front. Comput. Sci.},
  title        = {Governance of responsible AI: From ethical guidelines to cooperative policies},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing interactions with shared AVs in complex urban
mobility scenarios. <em>FCOMP</em>, <em>4</em>, 866258. (<a
href="https://doi.org/10.3389/fcomp.2022.866258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we report on the design and evaluation of an external human-machine interface (eHMI) for a real autonomous vehicle (AV), developed to operate as a shared transport pod in a pedestrianized urban space. We present insights about our human-centered design process, which included testing initial concepts through a tangible toolkit and evaluating 360-degree recordings of a staged pick-up scenario in virtual reality. Our results indicate that in complex mobility scenarios, participants filter for critical eHMI messages; further, we found that implicit cues (i.e., pick-up manoeuvre and proximity to the rider) influence participants&#39; experience and trust, while at the same time more explicit interaction modes are desired. This highlights the importance of considering interactions with shared AVs as a service more holistically, in order to develop knowledge about AV-pedestrian interactions in complex mobility scenarios that complements more targeted eHMI evaluations.},
  archive      = {J_FCOMP},
  author       = {Hoggenmueller, Marius and Tomitsch, Martin and Worrall, Stewart},
  doi          = {10.3389/fcomp.2022.866258},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {866258},
  shortjournal = {Front. Comput. Sci.},
  title        = {Designing interactions with shared AVs in complex urban mobility scenarios},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The spatial leaky competing accumulator model.
<em>FCOMP</em>, <em>4</em>, 866029. (<a
href="https://doi.org/10.3389/fcomp.2022.866029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Leaky Competing Accumulator model (LCA) of Usher and McClelland is able to simulate the time course of perceptual decision making between an arbitrary number of stimuli. Reaction times, such as saccadic latencies, produce a typical distribution that is skewed toward longer latencies and accumulator models have shown excellent fit to these distributions. We propose a new implementation called the Spatial Leaky Competing Accumulator (SLCA), which can be used to predict the timing of subsequent fixation durations during a visual task. SLCA uses a pre-existing saliency map as input and represents accumulation neurons as a two-dimensional grid to generate predictions in visual space. The SLCA builds on several biologically motivated parameters: leakage, recurrent self-excitation, randomness and non-linearity, and we also test two implementations of lateral inhibition. A global lateral inhibition, as implemented in the original model of Usher and McClelland, is applied to all competing neurons, while a local implementation allows only inhibition of immediate neighbors. We trained and compared versions of the SLCA with both global and local lateral inhibition with use of a genetic algorithm, and compared their performance in simulating human fixation latency distribution in a foraging task. Although both implementations were able to produce a positively skewed latency distribution, only the local SLCA was able to match the human data distribution from the foraging task. Our model is discussed for its potential in models of salience and priority, and its benefits as compared to other models like the Leaky integrate and fire network.},
  archive      = {J_FCOMP},
  author       = {Zemliak, Viktoria and MacInnes, W. Joseph},
  doi          = {10.3389/fcomp.2022.866029},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {866029},
  shortjournal = {Front. Comput. Sci.},
  title        = {The spatial leaky competing accumulator model},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RefRec+: Six degree-of-freedom estimation for smartphone
using floor reflecting light. <em>FCOMP</em>, <em>4</em>, 856942. (<a
href="https://doi.org/10.3389/fcomp.2022.856942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a novel visible light positioning (VLP) system called RefRec+ allowing to estimate the six degree-of-freedom (6DoF) of a smartphone. In most existing VLP systems, their front camera faces multiple light sources installed at different places on a ceiling to detect their direct signals. To overcome the problem of the limited field of views that causes failure to capture required numbers of light sources for positioning and that of the high computational complexity because of image processing to a large-sized pixel data, RefRec+ captures indirect lights from the light sources reflected via a floor. RefRec+ estimates the 2-D position of a point of interest (POI) by calculating the received signal strength of individual light sources using the floor image captured by the camera. Using 2-D positions of multiple POIs and the angle of arrival method, RefRec+ obtains the 6DoF of the smartphone. Several experiments to confirm the performance of RefRec+ were conducted. Experimental results in a room measuring 4.0 m × 4.0 m using nine POIs each of which consists of 32 × 32 pixels in a captured image showed that the absolute errors at the 90th percentile for the 3-D coordinates were 0.2073 m, 0.1713 m, and 0.002464 m along the X, Y, and Z axes, respectively, and for the pitch, roll, and yaw angles were 5.78, 5.69 and 3.96 degrees, respectively.},
  archive      = {J_FCOMP},
  author       = {Sugimoto, Masanori and Shimada, Shota and Hashizume, Hiromichi},
  doi          = {10.3389/fcomp.2022.856942},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {856942},
  shortjournal = {Front. Comput. Sci.},
  title        = {RefRec+: Six degree-of-freedom estimation for smartphone using floor reflecting light},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Micro-HBI: Human-biology interaction with living cells,
viruses, and molecules. <em>FCOMP</em>, <em>4</em>, 849887. (<a
href="https://doi.org/10.3389/fcomp.2022.849887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Biology Interaction (HBI) is a field that aims to provide first-hand experience with living matter and the modern life-sciences to the lay public. Advances in optical, bioengineering, and digital technologies as well as interaction design now also enable real and direct experiences at the microscale, such as with living cells and molecules, motivating the sub-field of “micro-HBI.” This is distinct from simulating any biological processes. There is a significant need for HBI as new educational modalities are required to enable all strata of society to become informed about new technologies and biology in general, as we face challenges like global pandemics, environmental loss, and species extinctions. Here we review this field in order to provide a jump-off point for future work and to bring stakeholder from different disciplines together. By now, the field has explored and demonstrated many such interactive systems, the use of different microorganisms, new interaction design principles, and versatile applications, such as museum exhibits, biotic games, educational cloud labs, citizen science platforms, and hands-on do-it-yourself (DIY) Bio maker activities. We close with key open questions for the field to move forward.},
  archive      = {J_FCOMP},
  author       = {Lee, Seung Ah and Riedel-Kruse, Ingmar H.},
  doi          = {10.3389/fcomp.2022.849887},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {849887},
  shortjournal = {Front. Comput. Sci.},
  title        = {Micro-HBI: Human-biology interaction with living cells, viruses, and molecules},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social neuro AI: Social interaction as the “dark matter” of
AI. <em>FCOMP</em>, <em>4</em>, 846440. (<a
href="https://doi.org/10.3389/fcomp.2022.846440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a three-axis framework indicating how AI can be informed by biological examples of social learning mechanisms. We argue that the complex human cognitive architecture owes a large portion of its expressive power to its ability to engage in social and cultural learning. However, the field of AI has mostly embraced a solipsistic perspective on intelligence. We thus argue that social interactions not only are largely unexplored in this field but also are an essential element of advanced cognitive ability, and therefore constitute metaphorically the “dark matter” of AI. In the first section, we discuss how social learning plays a key role in the development of intelligence. We do so by discussing social and cultural learning theories and empirical findings from social neuroscience. Then, we discuss three lines of research that fall under the umbrella of Social NeuroAI and can contribute to developing socially intelligent embodied agents in complex environments. First, neuroscientific theories of cognitive architecture, such as the global workspace theory and the attention schema theory, can enhance biological plausibility and help us understand how we could bridge individual and social theories of intelligence. Second, intelligence occurs in time as opposed to over time, and this is naturally incorporated by dynamical systems. Third, embodiment has been demonstrated to provide more sophisticated array of communicative signals. To conclude, we discuss the example of active inference, which offers powerful insights for developing agents that possess biological realism, can self-organize in time, and are socially embodied.},
  archive      = {J_FCOMP},
  author       = {Bolotta, Samuele and Dumas, Guillaume},
  doi          = {10.3389/fcomp.2022.846440},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {846440},
  shortjournal = {Front. Comput. Sci.},
  title        = {Social neuro AI: Social interaction as the “Dark matter” of AI},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Older adults and smart technology: Facilitators and barriers
to use. <em>FCOMP</em>, <em>4</em>, 835927. (<a
href="https://doi.org/10.3389/fcomp.2022.835927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart technologies (e.g., smartphones, smart security technologies, digital home assistants) have advanced over the years and will continue to do so. There are various benefits to using these technologies in one&#39;s life, such as an increase in productivity through automation and self-monitoring one&#39;s health. Older adults particularly may benefit from smart technologies to support their everyday activities and compensate for age related changes. In this study, we explored the experiences and attitudes of eighty older adults including those who had prior experience and those who had never used (or perhaps never heard of) smart technologies through an online survey. We assessed their general opinions toward using smart technology and explored what facilitated or hindered their use. Older adults rated the facilitators to use for each smart technology differently, with few commonalities between the order of the most agreed upon facilitators. However, older adults&#39; opinions were consistent across each technology about their ignorance of technological features and cost of the smart technology, which could be potential barriers to use. Among those who had never used one of the smart technologies, privacy was the most commonly endorsed concern. The results from this study support the understanding of key considerations when developing and deploying smart technologies for older adults.},
  archive      = {J_FCOMP},
  author       = {Harris, Maurita T. and Blocker, Kenneth A. and Rogers, Wendy A.},
  doi          = {10.3389/fcomp.2022.835927},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {835927},
  shortjournal = {Front. Comput. Sci.},
  title        = {Older adults and smart technology: Facilitators and barriers to use},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of type-2 diabetes mellitus disease using machine
learning classifiers and techniques. <em>FCOMP</em>, <em>4</em>, 835242.
(<a href="https://doi.org/10.3389/fcomp.2022.835242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technological advancements in today&#39;s healthcare sector have given rise to many innovations for disease prediction. Diabetes mellitus is one of the diseases that has been growing rapidly among people of different age groups; there are various reasons and causes involved. All these reasons are considered as different attributes for this study. To predict type-2 diabetes mellitus disease, various machine learning algorithms can be used. The objective of using the algorithm is to construct a predictive model to critically predict whether a person is affected by diabetes. The classifiers taken are logistic regression, XGBoost, gradient boosting, decision trees, ExtraTrees, random forest, and light gradient boosting machine (LGBM). The dataset used is PIMA Indian Dataset sourced from UC Irvine Repository. The performance of these algorithms is compared in reference to the accuracy obtained. The results obtained from these classifiers show that the LGBM classifier has the highest accuracy of 95.20% in comparison with the other algorithms.},
  archive      = {J_FCOMP},
  author       = {Ahamed, B. Shamreen and Arya, Meenakshi Sumeet and Nancy V, Auxilia Osvin},
  doi          = {10.3389/fcomp.2022.835242},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {835242},
  shortjournal = {Front. Comput. Sci.},
  title        = {Prediction of type-2 diabetes mellitus disease using machine learning classifiers and techniques},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Linking haptic parameters to the emotional space for
mediated social touch. <em>FCOMP</em>, <em>4</em>, 826545. (<a
href="https://doi.org/10.3389/fcomp.2022.826545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social touch is essential for creating and maintaining strong interpersonal bonds amongst humans. However, when distance separates users, they often rely on voice and video communication technologies to stay connected with each other, and the lack of tactile interactions between users lowers the quality of the social interactions. In this research, we investigated haptic patterns to communicate five tactile messages comprising of four types of social touch (high five, handshake, caress, and asking for attention) and one physiological signal (the pulse of a heartbeat), delivered on the hand through a haptic glove. Since social interactions are highly dependent on their context, we conceived two interaction scenarios for each of the five tactile messages, conveying distinct emotions being spread across the circumplex model of emotions. We conducted two user studies: in the first one participants tuned the parameters of haptic patterns to convey tactile messages in each scenario, and a follow up study tested naïve participants to assess the validity of these patterns. Our results show that all haptic patterns were recognized above chance level, and the well-defined parameter clusters had a higher recognition rate, reinforcing the hypothesis that some social touches have more universal patterns than others. We also observed parallels between the parameters&#39; levels and the type of emotions they conveyed based on their mapping in the circumplex model of emotions.},
  archive      = {J_FCOMP},
  author       = {Rognon, Carine and Stephens-Fripp, Benjamin and Hartcher-O&#39;Brien, Jess and Rost, Bob and Israr, Ali},
  doi          = {10.3389/fcomp.2022.826545},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {826545},
  shortjournal = {Front. Comput. Sci.},
  title        = {Linking haptic parameters to the emotional space for mediated social touch},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Turning eight family homes into interactive, pervasive
playgrounds during the COVID-19 pandemic lockdown. <em>FCOMP</em>,
<em>4</em>, 822337. (<a
href="https://doi.org/10.3389/fcomp.2022.822337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an evaluation study of how eighth families adopted, played and experienced a movement-based game system of analog and digital technologies in their homes during a pandemic lockdown. The COVID-19 pandemic locked down many countries and grounded people in their homes with social and physical implications. A game system consisting of simple, tangible technologies with modular components was designed to meet these needs. The game system was developed for the players to set up in their homes easily and, therefore, should not depend on screens or extensive physical installations. The game system comprises simple, tangible technologies such as light and music cubes, a simple mobile robot, card game challenges, and a suite of mini-games combining the elements in a variety of playful experiences. Using the technology probes methodology, the game system was packed into a suitcase and evaluated by eight families that played the game in their homes, video-recorded their sessions, wrote a final report and were (informally) interviewed afterwards. The data set presents how the families turned their ordinary everyday spaces into interactive, pervasive playgrounds encouraging social and bodily exploration and play. Furthermore, the study shows how bodily movement and social play can be promoted through different technologies that stimulate various bodily senses and incorporate them through the different game and play structures into their everyday living environments. The findings resulted in four design implications to aid designers and researchers in future work on movement-based game systems and interactive, pervasive playground design. These design implications accommodate social and bodily activities in ordinary places otherwise not pre-allocated for play or game activities.},
  archive      = {J_FCOMP},
  author       = {Matjeka, Louise Petersen and Svanæs, Dag and Wang, Alf Inge},
  doi          = {10.3389/fcomp.2022.822337},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {822337},
  shortjournal = {Front. Comput. Sci.},
  title        = {Turning eight family homes into interactive, pervasive playgrounds during the COVID-19 pandemic lockdown},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An open-ended blended approach to teaching interaction
designers to code. <em>FCOMP</em>, <em>4</em>, 813889. (<a
href="https://doi.org/10.3389/fcomp.2022.813889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reports on a three and a half year design-led project investigating the use of open-ended learning to teach programming to students of interaction design. Our hypothesis is that a more open-ended approach to teaching programming, characterized by both creativity and self-reflection, would improve learning outcomes among our cohort of aspiring HCI practitioners. The objective of our design-led action research was to determine how to effectively embed open-endedness, student-led teaching, and self-reflection into an online programming class. Each of these notions has been studied separately before, but there is a dearth of published work into their actual design and implementation in practice. In service of that objective we present our contribution in two parts: a qualitatively-derived understanding of student attitudes toward open-ended blended learning, as well as a matching set of design principles for future open-ended HCI education. The project was motivated by a search for better educational outcomes, both in terms of student coding self-efficacy and quantitative metrics of cohort performance (e.g., failure rates). The first year programming course within our interaction design-focussed Bachelors program has had the highest failure rate of any core unit for over a decade. Unfortunately, the COVID-19 pandemic confounded any year-to-year quantitative comparison of the learning efficacy of our successive prototypes. There is simply no way to fairly compare the experiences of pre-pandemic and pandemic-affected student cohorts. However, the experience of teaching this material in face-to-face, fully online, and hybrid modalities throughout the pandemic has aided our qualitative exploration of why open-ended learning helps some students but seems to harm others. Through three sets of student interviews, platform data, and insights gained from both the instructional and platform design process, we show that open-ended learning can empower students, but can also exacerbate fears and anxieties around inadequacy and failure. Through seven semesters of iterating on our designs, interviewing students and reflecting on our interventions, we&#39;ve developed a set of classroom-validated design principles for teaching programming to HCI students without strong computational backgrounds.},
  archive      = {J_FCOMP},
  author       = {Grace, Kazjon and Klaassens, Brittany and Bray, Liam and Elton-Pym, Alex},
  doi          = {10.3389/fcomp.2022.813889},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {813889},
  shortjournal = {Front. Comput. Sci.},
  title        = {An open-ended blended approach to teaching interaction designers to code},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designerly ways of knowing in HCI education: A case study of
a peer community-based studio. <em>FCOMP</em>, <em>4</em>, 793968. (<a
href="https://doi.org/10.3389/fcomp.2022.793968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design methods and approaches are common within Human-Computer Interaction. And while design is recognized as a discipline with its own epistemology and pedagogy outside of HCI, there is a lot of work to be done in incorporating, facilitating, and developing designerly knowledge in HCI education. The abrupt shift toward distance education caused by COVID-19 surfaced the necessity for course design to purposely support online informal learning environments and facilitating tacit knowledge as previously prevalent in the design studio environment. Firstly, we present theory on design epistemology, related to “designerly ways of knowing” and the role of the studio in the learning process. Secondly, a case study presents the set up of a digital studio for a course in Designing User Experiences, with an emphasis on supporting a community-based studio. The empirical material includes an overview of the course set up and a thorough qualitative analysis of the feedback provided by a cohort of 48 students with diverse backgrounds. The course was conducted online and heavily based on the use of software such as Zoom and Miro. We conclude by offering a set of themes in three categories to be considered when designing community-based “designerly” courses within HCI. As future work, we suggest the Community-Based Designerly Scale to be used, adapted, and developed by teachers and students as a tool in their educational practice.},
  archive      = {J_FCOMP},
  author       = {Gamboa, Mafalda and Ljungblad, Sara},
  doi          = {10.3389/fcomp.2022.793968},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {793968},
  shortjournal = {Front. Comput. Sci.},
  title        = {Designerly ways of knowing in HCI education: A case study of a peer community-based studio},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MultiSoma: Motor and gaze analysis on distributed embodiment
with synchronized behavior and perception. <em>FCOMP</em>, <em>4</em>,
788014. (<a href="https://doi.org/10.3389/fcomp.2022.788014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human behavior and perception are optimized for a single body. Yet, the human brain has plasticity, which allows us to extend our body schema. By utilizing technology like robotics or virtual reality (VR), we can modify our body parts or even add a new body to our own while retaining control over these parts. However, the update of body cognition when controlling multiple bodies has not been well examined. In this study, we explore the task performance and body cognition of humans when they have multiple full bodies as an extended embodiment. Our experimental system allows a participant to control up to four bodies at the same time and perceive sensory information from them. The participant experiences synchronizing behavior and vision perception in a virtual environment. We set up three tasks for multiple bodies and evaluated the cognition of these bodies with their gazing information, task performances, and subjective ratings. We found that humans can have the sense of body ownership and agency for each body when controlling multiple bodies simultaneously. Furthermore, it was observed that people manipulate multiple bodies by actively switching their attention in a static environment and passively switching their attention in a dynamic environment. Distributed embodiment has the potential to extend human behavior in cooperative work, parallel work, group behavior, and so on.},
  archive      = {J_FCOMP},
  author       = {Miura, Reiji and Kasahara, Shunichi and Kitazaki, Michiteru and Verhulst, Adrien and Inami, Masahiko and Sugimoto, Maki},
  doi          = {10.3389/fcomp.2022.788014},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {788014},
  shortjournal = {Front. Comput. Sci.},
  title        = {MultiSoma: Motor and gaze analysis on distributed embodiment with synchronized behavior and perception},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speech assistant system with local client and server devices
to guarantee data privacy. <em>FCOMP</em>, <em>4</em>, 778367. (<a
href="https://doi.org/10.3389/fcomp.2022.778367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users of speech assistant systems have reservations about the distributed approach of these systems. They have concerns that people might get access to the transmitted speech data or that somebody is able to access their microphone from outside. Therefore, we investigate the concept of a setup with local client and server systems. This comes along with the requirement of cost-efficient realizations of client and server. We examined a number of different cost-efficient server solutions depending on the required recognition capability of specific applications. A fairly cost-efficient solution is the use of a small computing device for recognizing a few dozens of words with a GMM-HMM based recognition. To perform a DNN-HMM based recognition, we looked at small computing devices with an integrated additional graphical processor unit (GPU). Furthermore, we investigated the use of low-cost PCs for implementing real-time versions of the Kaldi framework to allow the recognition of large vocabularies. We investigated the control of a smart home by speech as an exemplary application. For this, we designed compact client systems that can be integrated at certain places inside a room, e.g., in a standard outlet socket. Besides activating a client by a sensor that detects approaching people, the recognition of a spoken wake-up word is the usual way for activation. We developed a keyword recognition algorithm that can be implemented in the client despite its limited computing resources. The control of the whole dialogue has been integrated in our client, so that no further server is needed. In a separate study, we examined the approach of an extremely energy-efficient realization of the client system without the need of an external power supply. The approach is based on using a special microphone with an additional low-power operating mode detecting the exceeding of a preset sound level threshold only. This detection can be used to wake up the client&#39;s microcontroller and to make the microphone switch to normal operating mode. In the listening mode, the energy consumption of the microphone is so low that a client system can be active for months with an energy supply from standard batteries only.},
  archive      = {J_FCOMP},
  author       = {Hirsch, Hans-Günter},
  doi          = {10.3389/fcomp.2022.778367},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {778367},
  shortjournal = {Front. Comput. Sci.},
  title        = {Speech assistant system with local client and server devices to guarantee data privacy},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of spatial directional guidance using cheek
haptic stimulation in a virtual environment. <em>FCOMP</em>, <em>4</em>,
733844. (<a href="https://doi.org/10.3389/fcomp.2022.733844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial cues play an important role in navigating people in both physical and virtual spaces. In spatial navigation, visual information with additional cues, such as haptic cues, enables effective guidance. Most haptic devices are applied to various body parts to make mechanical stimuli, while few devices stimulate a head despite the excellent sensitivity. This article presents Virtual Whiskers, a spatial directional guidance technique by cheek haptic stimulation using tiny robot arms attached to a Head-Mounted Display (HMD). The tip of the robotic arm has photo reflective sensors to detect the distance between the tip and the cheek surface. Using the robot arms, we stimulate a point on the cheek obtained by calculating an intersection between the cheek surface and the target direction. In the directional guidance experiment, we investigated how accurately participants identify the target direction provided by our guidance method. We evaluated an error between the actual target direction and the participant&#39;s pointed direction. The experimental result shows that our method achieves the average absolute directional error of 2.54° in the azimuthal plane and 6.54° in the elevation plane. We also conducted a spatial guidance experiment to evaluate task performance in a target search task. We compared the condition of visual information, visual and audio information, and visual information and cheek haptics for task completion time, System Usability Scale (SUS) score, NASA-TLX score. The averages of task completion time were M = 6.39 s, SD = 3.34 s, and M = 5.62 s, SD = 3.12 s, and M = 4.35 s, SD = 2.26 s, in visual-only condition, visual+audio condition, and visual+haptic condition, respectively. In terms of the SUS score, visual condition, visual+audio condition, and visual+haptic condition achieved M = 55.83, SD = 20.40, and M = 47.78, SD = 20.09, and M = 80.42, SD = 10.99, respectively. As for NASA-TLX score, visual condition, visual+audio condition, and visual+haptic condition resulted in M = 75.81, SD = 16.89, and M = 67.57, SD = 14.96, and M = 38.83, SD = 18.52, respectively. Statistical tests revealed significant differences in task completion time, SUS score, and NASA-TLX score between the visual and the visual+haptic condition and the visual+audio and the visual+haptic condition.},
  archive      = {J_FCOMP},
  author       = {Nakamura, Fumihiko and Verhulst, Adrien and Sakurada, Kuniharu and Fukuoka, Masaaki and Sugimoto, Maki},
  doi          = {10.3389/fcomp.2022.733844},
  journal      = {Frontiers in Computer Science},
  month        = {5},
  pages        = {733844},
  shortjournal = {Front. Comput. Sci.},
  title        = {Evaluation of spatial directional guidance using cheek haptic stimulation in a virtual environment},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Software engineering and filmmaking: A literature review.
<em>FCOMP</em>, <em>4</em>, 884533. (<a
href="https://doi.org/10.3389/fcomp.2022.884533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development is a complex process that requires skills in mathematics and physics. Moreover, it usually includes collaboration with other people. To get a precise understanding of the way such a process is organized, we need to understand its essence. Technical knowledge is crucially important for any developer; however, another important characteristic of any software engineer is creativity. In this article, we look at one particular artistic practice [filmmaking] that involves both these latter characteristics to determine whether insights from such a practice can be applied in the IT industry and vice versa.},
  archive      = {J_FCOMP},
  author       = {Farina, Mirko and Fedorovskaya, Arina and Polivtsev, Egor and Succi, Giancarlo},
  doi          = {10.3389/fcomp.2022.884533},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {884533},
  shortjournal = {Front. Comput. Sci.},
  title        = {Software engineering and filmmaking: A literature review},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machine learning solutions applied to amyotrophic lateral
sclerosis prognosis: A review. <em>FCOMP</em>, <em>4</em>, 869140. (<a
href="https://doi.org/10.3389/fcomp.2022.869140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prognosis of Amyotrophic Lateral Sclerosis (ALS), a complex and rare disease, represents a challenging and essential task to better comprehend its progression and improve patients&#39; quality of life. The use of Machine Learning (ML) techniques in healthcare has produced valuable contributions to the prognosis field. This article presents a systematic and critical review of primary studies that used ML applied to the ALS prognosis, searching for databases, relevant predictor biomarkers, the ML algorithms and techniques, and their outcomes. We focused on studies that analyzed biomarkers commonly present in the ALS disease clinical practice, such as demographic, clinical, laboratory, and imaging data. Hence, we investigate studies to provide an overview of solutions that can be applied to develop decision support systems and be used by a higher number of ALS clinical settings. The studies were retrieved from PubMed, Science Direct, IEEEXplore, and Web of Science databases. After completing the searching and screening process, 10 articles were selected to be analyzed and summarized. The studies evaluated and used different ML algorithms, techniques, datasets, sample sizes, biomarkers, and performance metrics. Based on the results, three distinct types of prediction were identified: Disease Progression, Survival Time, and Need for Support. The biomarkers identified as relevant in more than one study were the ALSFRS/ALSFRS-R, disease duration, Forced Vital Capacity, Body Mass Index, age at onset, and Creatinine. In general, the studies presented promissory results that can be applied in developing decision support systems. Besides, we discussed the open challenges, the limitations identified, and future research opportunities.},
  archive      = {J_FCOMP},
  author       = {Papaiz, Fabiano and Dourado, Mario Emílio Teixeira and Valentim, Ricardo Alexsandro de Medeiros and de Morais, Antonio Higor Freire and Arrais, Joel Perdiz},
  doi          = {10.3389/fcomp.2022.869140},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {869140},
  shortjournal = {Front. Comput. Sci.},
  title        = {Machine learning solutions applied to amyotrophic lateral sclerosis prognosis: A review},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). What makes interactive art engaging? <em>FCOMP</em>,
<em>4</em>, 859496. (<a
href="https://doi.org/10.3389/fcomp.2022.859496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive art requires people to engage with it, and some works of interactive art are more intrinsically engaging than others. This article asks what properties of a work of interactive art promote engagement. More specifically, it examines four properties: (1) the number of controllable parameters in the interaction, (2) the use of fantasy in the work, (3) the timescale on which the work responds, and (4) the amount agency ascribed to the work. Each of these is hypothesized to promote engagement, and each hypothesis is tested with a controlled user study in an ecologically valid setting on the Internet. In these studies, we found that more controllable parameters increases engagement; the use of fantasy increases engagement for some users and not others; the timescale surprisingly has no significant on engagement but may relate to the style of interaction; and more ascribed agency is correlated with greater engagement although the direction of causation is not known. This is not intended to be an exhaustive list of all properties that may promote engagement, but rather a starting point for more studies of this kind.},
  archive      = {J_FCOMP},
  author       = {Krzyzaniak, Michael and Erdem, Çağri and Glette, Kyrre},
  doi          = {10.3389/fcomp.2022.859496},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {859496},
  shortjournal = {Front. Comput. Sci.},
  title        = {What makes interactive art engaging?},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). With clear intention—an ethical responsibility model for
robot governance. <em>FCOMP</em>, <em>4</em>, 852528. (<a
href="https://doi.org/10.3389/fcomp.2022.852528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is much discussion about super artificial intelligence (AI) and autonomous machine learning (ML) systems, or learning machines (LM). Yet, the reality of thinking robotics still seems far on the horizon. It is one thing to define AI in light of human intelligence, citing the remoteness between ML and human intelligence, but another to understand issues of ethics, responsibility, and accountability in relation to the behavior of autonomous robotic systems within a human society. Due to the apparent gap between a society in which autonomous robots are a reality and present-day reality, many of the efforts placed on establishing robotic governance, and indeed, robot law fall outside the fields of valid scientific research. Work within this area has concentrated on manifestos, special interest groups and popular culture. This article takes a cognitive scientific perspective toward characterizing the nature of what true LMs would entail—i.e., intentionality and consciousness. It then proposes the Ethical Responsibility Model for Robot Governance (ER-RoboGov) as an initial platform or first iteration of a model for robot governance that takes the standpoint of LMs being conscious entities. The article utilizes past AI governance model research to map out the key factors of governance from the perspective of autonomous machine learning systems.},
  archive      = {J_FCOMP},
  author       = {Rousi, Rebekah},
  doi          = {10.3389/fcomp.2022.852528},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {852528},
  shortjournal = {Front. Comput. Sci.},
  title        = {With clear Intention—An ethical responsibility model for robot governance},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uniform polylogarithmic space completeness. <em>FCOMP</em>,
<em>4</em>, 845990. (<a
href="https://doi.org/10.3389/fcomp.2022.845990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-known that polylogarithmic space (PolyL for short) does not have complete problems under logarithmic space many-one reductions. Thus, we propose an alternative notion of completeness inspired by the concept of uniformity studied in circuit complexity theory. We then prove the existence of a uniformly complete problem for PolyL under this new notion. Moreover, we provide evidence that uniformly complete problems can help us to understand the still unclear relationship between complexity classes such as PolyL and polynomial time.},
  archive      = {J_FCOMP},
  author       = {Ferrarotti, Flavio and González, Senén and Schewe, Klaus-Dieter and Turull-Torres, José María},
  doi          = {10.3389/fcomp.2022.845990},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {845990},
  shortjournal = {Front. Comput. Sci.},
  title        = {Uniform polylogarithmic space completeness},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Take it to the curb: Scalable communication between
autonomous cars and vulnerable road users through curbstone displays.
<em>FCOMP</em>, <em>4</em>, 844245. (<a
href="https://doi.org/10.3389/fcomp.2022.844245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated driving will require new approaches to the communication between vehicles and vulnerable road users (VRUs) such as pedestrians, e.g., through external human–machine interfaces (eHMIs). However, the majority of eHMI concepts are neither scalable (i.e., take into account complex traffic scenarios with multiple vehicles and VRUs), nor do they optimize traffic flow. Speculating on the upgrade of traffic infrastructure in the automated city, we propose Smart Curbs, a scalable communication concept integrated into the curbstone. Using a combination of immersive and non-immersive prototypes, we evaluated the suitability of our concept for complex urban environments in a user study (N = 18). Comparing the approach to a projection-based eHMI, our findings reveal that Smart Curbs are safer to use, as our participants spent less time on the road when crossing. Based on our findings, we discuss the potential of Smart Curbs to mitigate the scalability problem in AV-pedestrian communication and simultaneously enhance traffic flow.},
  archive      = {J_FCOMP},
  author       = {Holländer, Kai and Hoggenmüller, Marius and Gruber, Romy and Völkel, Sarah Theres and Butz, Andreas},
  doi          = {10.3389/fcomp.2022.844245},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {844245},
  shortjournal = {Front. Comput. Sci.},
  title        = {Take it to the curb: Scalable communication between autonomous cars and vulnerable road users through curbstone displays},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Curricula design &amp; pedagogy for sketching within HCI
&amp; UX education. <em>FCOMP</em>, <em>4</em>, 826445. (<a
href="https://doi.org/10.3389/fcomp.2022.826445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketching is recognised as an important tool in the journey of research and practical processes of Human Computer Interaction (HCI) and User Experience Design (UX). However, it is not always included in higher education curriculum, in which HCI and UX is often a single module in one year group amongst more “traditional” approaches in computer science. The benefits of sketching and visualisation practice can be used by students across the board in computing degrees, but especially so within HCI and UX, where novel approaches and ideation are valued and practiced. By the time learners leave higher education, they may or may not have engaged with this valuable skill. HCI has a lot in common with UX, and the two are commonly conflated to be the same thing, though despite this, there is not a focus on practical sketching and visualisation skills. In comparison, within the UX workplace environment, sketching is part of design thinking and vital for the structuring of ideas, storyboards, user journey maps and more. We focus on the incorporation and exploration of sketching as an educational tool, technique and output within HCI, and how this learning is given and received over a number of contexts. This paper outlines case studies where sketching has been included in both formal and informal learning with both undergraduate, postgraduate, and post education populations, and how this knowledge exchange has been both enhanced and changed by the recent compulsory move to online teaching during the COVID-19 pandemic. We discuss practice and learning in the context of four case studies: Data-Sketching in a First Year Minor; Sketching in a 2nd Year HCI Cohort; Sketching as a Foundational Tool for MSc User Experience Design; and, Sketching in HCI for Peer-to-Peer Learning. Further, we make recommendations for incorporating sketching practice and theory into both undergraduate and postgraduate university programs, as well as for peer-to-peer learning in both public and private contexts.},
  archive      = {J_FCOMP},
  author       = {Lewis, Makayla and Sturdee, Miriam},
  doi          = {10.3389/fcomp.2022.826445},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {826445},
  shortjournal = {Front. Comput. Sci.},
  title        = {Curricula design &amp;amp; pedagogy for sketching within HCI &amp;amp; UX education},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microbial content generation for natural terrains in
computer games. <em>FCOMP</em>, <em>4</em>, 826412. (<a
href="https://doi.org/10.3389/fcomp.2022.826412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural content generation (PCG) has been applied since several decades to fulfill various game-related design needs. Besides bio-inspired methods, living (non-human) organisms were used in computer games for various purposes, such as behavior generation, data gathering, and player education. No living organisms were used for the generation of virtual terrains in games. Such an approach to terrain generation could benefit from morphological similarity between natural terrains and colonies of microbial organisms, real-time development of terrains over time, and educational opportunities. We successfully executed an experiment in which we used growing bacterial and fungal cultures for generating naturally appearing virtual terrains in real-time. Concludingly, we confirm the feasibility of using living organisms in real-time non-behavioral PCG and reflect on its potential impact.},
  archive      = {J_FCOMP},
  author       = {Lamers, Maarten H. and van Eck, Wim J. O. M.},
  doi          = {10.3389/fcomp.2022.826412},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {826412},
  shortjournal = {Front. Comput. Sci.},
  title        = {Microbial content generation for natural terrains in computer games},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI ethics as applied ethics. <em>FCOMP</em>, <em>4</em>,
776837. (<a href="https://doi.org/10.3389/fcomp.2022.776837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to design and develop artificial intelligence (AI) in a sustainable manner has motivated researchers, institutions, and organizations to formulate suggestions for AI ethics. Although these suggestions cover various topics and address diverse audiences, they share the presupposition that AI ethics provides a generalizable basis for designers that is applicable to their work. We propose that one of the reasons the influence of current ethical codes has remained modest, may be the conception of the applied ethics that they represent. We discuss bioethics as a point of reference for weighing the metaethical and methodological approaches adopted in AI ethics, and propose that AI ethics could be made more methodologically solid and substantively more influential if the resources were enriched by adopting tools from fields of study created to improve the quality of human action and safeguard its desired outcomes. The approaches we consider to be useful for this purpose are the systems theory, safety research, impact assessment approach, and theory of change.},
  archive      = {J_FCOMP},
  author       = {Hallamaa, Jaana and Kalliokoski, Taina},
  doi          = {10.3389/fcomp.2022.776837},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {776837},
  shortjournal = {Front. Comput. Sci.},
  title        = {AI ethics as applied ethics},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing dysarthria diversity for automatic speech
recognition: A tutorial from the clinical perspective. <em>FCOMP</em>,
<em>4</em>, 770210. (<a
href="https://doi.org/10.3389/fcomp.2022.770210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in automatic speech recognition (ASR) technology, even the best performing ASR systems are inadequate for speakers with impaired speech. This inadequacy may be, in part, due to the challenges associated with acquiring a sufficiently diverse training sample of disordered speech. Speakers with dysarthria, which refers to a group of divergent speech disorders secondary to neurologic injury, exhibit highly variable speech patterns both within and across individuals. This diversity is currently poorly characterized and, consequently, difficult to adequately represent in disordered speech ASR corpora. In this article, we consider the variable expressions of dysarthria within the context of established clinical taxonomies (e.g., Darley, Aronson, and Brown dysarthria subtypes). We also briefly consider past and recent efforts to capture this diversity quantitatively using speech analytics. Understanding dysarthria diversity from the clinical perspective and how this diversity may impact ASR performance could aid in (1) optimizing data collection strategies for minimizing bias; (2) ensuring representative ASR training sets; and (3) improving generalization of ASR for difficult-to-recognize speakers. Our overarching goal is to facilitate the development of robust ASR systems for dysarthric speech using clinical knowledge.},
  archive      = {J_FCOMP},
  author       = {Rowe, Hannah P. and Gutz, Sarah E. and Maffei, Marc F. and Tomanek, Katrin and Green, Jordan R.},
  doi          = {10.3389/fcomp.2022.770210},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {770210},
  shortjournal = {Front. Comput. Sci.},
  title        = {Characterizing dysarthria diversity for automatic speech recognition: A tutorial from the clinical perspective},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving mobile device security by embodying and
co-adapting a behavioral biometric interface. <em>FCOMP</em>,
<em>4</em>, 754716. (<a
href="https://doi.org/10.3389/fcomp.2022.754716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, interfaces between users and smart devices such as smart phones rely primarily on passwords. This has allowed for the intrusion and perturbation of the interface between the user and the device and has compromised security. Recently, Frank et al. have suggested that security could be improved by having an interface with biometric features of finger swiping. This approach has been termed touchalytics, in maintaining cybersecurity. The number of features of finger swiping have been large (32) and have been made available as a public database, which we utilize in our study. However, it has not been shown which of these features uniquely identify a particular user. In this paper, we study whether a subset of features that embody human cognitive motor features can be used to identify a particular user. We consider how the security might be made more efficient embodying Principal Component Analysis (PCA) into the interface, which has the potential of reducing the features utilized in the identification of intruders. We compare the accuracy and performance of the reduced feature space to that of having all the features. Embodying a robust continuous authentication system will give users an extra layer of security and an increased sense of peace of mind if their devices are lost or stolen. Consequently, such improvements may prevent access to sensitive information and thus will save businesses money. Consequently, such improvements may prevent access to sensitive information and thus will save businesses money. If continuous authentication models become successful and easily implementable, embodiment and co-adaptation of user authentication would inhibit the growing problem of mobile device theft.},
  archive      = {J_FCOMP},
  author       = {Jairam, Avinash and Halevi, Tzipora and Raphan, Theodore},
  doi          = {10.3389/fcomp.2022.754716},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {754716},
  shortjournal = {Front. Comput. Sci.},
  title        = {Improving mobile device security by embodying and co-adapting a behavioral biometric interface},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal behavioral cues analysis of the sense of presence
and social presence during a social interaction with a virtual patient.
<em>FCOMP</em>, <em>4</em>, 746804. (<a
href="https://doi.org/10.3389/fcomp.2022.746804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User&#39;s experience evaluation is a key challenge when studying human-agent interaction. Besides user&#39;s satisfaction, this question is addressed in virtual reality through the sense of presence and social presence, generally assessed thanks to subjective post-experience questionnaires. We propose in this article a novel approach making it possible to evaluate automatically these notions by correlating objective multimodal cues produced by users to their subjective sense of presence and social presence. This study is based on a multimodal human-agent interaction corpus collected in a task-oriented context: a virtual environment aiming at training doctors to break bad news to a patient played by a virtual agent. Based on a corpus study, we applied machine learning approaches to build a model predicting the user&#39;s sense of presence and social presence thanks to specific multimodal behavioral cues. We explore different classification algorithms and machine learning techniques (oversampling and clustering) to cope with the dimensionality of the dataset and to optimize the prediction performance. We obtain models to automatically and accurately predict the level of presence and social presence. The results highlight the relevance of a multimodal model, based both on verbal and non-verbal cues as objective measures of (social) presence. The main contribution of the article is two-fold: 1/ proposing the first presence and social prediction presence models offering a way to automatically provide a user&#39;s experience evaluation and 2/ showing the importance of multimodal information for describing these notions.},
  archive      = {J_FCOMP},
  author       = {Ochs, Magalie and Bousquet, Jérémie and Pergandi, Jean-Marie and Blache, Philippe},
  doi          = {10.3389/fcomp.2022.746804},
  journal      = {Frontiers in Computer Science},
  month        = {4},
  pages        = {746804},
  shortjournal = {Front. Comput. Sci.},
  title        = {Multimodal behavioral cues analysis of the sense of presence and social presence during a social interaction with a virtual patient},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Digital/online networks in everyday life during
pandemics. <em>FCOMP</em>, <em>4</em>, 887604. (<a
href="https://doi.org/10.3389/fcomp.2022.887604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Lacasa, Pilar and Zagalo, Nelson and Emmer, Martin},
  doi          = {10.3389/fcomp.2022.887604},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {887604},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Digital/Online networks in everyday life during pandemics},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing wearable augmented reality concepts to support
scalability in autonomous vehicle-pedestrian interaction.
<em>FCOMP</em>, <em>4</em>, 866516. (<a
href="https://doi.org/10.3389/fcomp.2022.866516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable augmented reality (AR) offers new ways for supporting the interaction between autonomous vehicles (AVs) and pedestrians due to its ability to integrate timely and contextually relevant data into the user&#39;s field of view. This article presents novel wearable AR concepts that assist crossing pedestrians in multi-vehicle scenarios where several AVs frequent the road from both directions. Three concepts with different communication approaches for signaling responses from multiple AVs to a crossing request, as well as a conventional pedestrian push button, were simulated and tested within a virtual reality environment. The results showed that wearable AR is a promising way to reduce crossing pedestrians&#39; cognitive load when the design offers both individual AV responses and a clear signal to cross. The willingness of pedestrians to adopt a wearable AR solution, however, is subject to different factors, including costs, data privacy, technical defects, liability risks, maintenance duties, and form factors. We further found that all participants favored sending a crossing request to AVs rather than waiting for the vehicles to detect their intentions—pointing to an important gap and opportunity in the current AV-pedestrian interaction literature.},
  archive      = {J_FCOMP},
  author       = {Tran, Tram Thi Minh and Parker, Callum and Wang, Yiyuan and Tomitsch, Martin},
  doi          = {10.3389/fcomp.2022.866516},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {866516},
  shortjournal = {Front. Comput. Sci.},
  title        = {Designing wearable augmented reality concepts to support scalability in autonomous vehicle-pedestrian interaction},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: The role of real world evidence (RWE) for digital
health. <em>FCOMP</em>, <em>4</em>, 862712. (<a
href="https://doi.org/10.3389/fcomp.2022.862712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Kyriazakos, Sofoklis},
  doi          = {10.3389/fcomp.2022.862712},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {862712},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: The role of real world evidence (RWE) for digital health},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Personalized digital health and patient-centric
services. <em>FCOMP</em>, <em>4</em>, 862358. (<a
href="https://doi.org/10.3389/fcomp.2022.862358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Hägglund, Maria and Cajander, Åsa and Rexhepi, Hanife and Kane, Bridget},
  doi          = {10.3389/fcomp.2022.862358},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {862358},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Personalized digital health and patient-centric services},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social modeling of virtual healthy food intake.
<em>FCOMP</em>, <em>4</em>, 832996. (<a
href="https://doi.org/10.3389/fcomp.2022.832996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People tend to adapt the amount of their food intake to that of others around them. This so-called social modeling of eating has been extensively studied over the past decades. The current study complements these experiments and aims to investigate social modeling of healthy food intake using a video paradigm in which a virtual model consumed either a small or a large portion of apple. In addition, it was tested whether modeling effects of a virtual female confederate were equally strong in male and female participants. Thirty participants (13 female, 17 male) completed the low norm condition in which a virtual model consumed 30 g of apple. Another 30 participants (17 female, 13 male) were allocated to the high norm condition in which a virtual model consumed 100 g of apple. Participants completed an irrelevant task, after which their own intake of apple was measured. Average intake in the low norm condition was 3 g, average intake in the high norm condition was 26 g (p &amp;lt; 0.001). In conclusion, participants adapted their intake to that of the virtual model. This effect was found irrespective of gender; female and male participants equally adapted their intake to that of a female virtual model. Stimulating food intake via a virtual model in people who have difficulty to meet dietary requirements or inhibiting food intake in people who tend to consume too much would be interesting new steps in the development of healthcare applications.},
  archive      = {J_FCOMP},
  author       = {Bulsing, Patricia Johanna and Salmon, Stefanie Johanna},
  doi          = {10.3389/fcomp.2022.832996},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {832996},
  shortjournal = {Front. Comput. Sci.},
  title        = {Social modeling of virtual healthy food intake},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Raaz: A transdisciplinary exploration at the intersection of
bioart, HCI, and community engagement. <em>FCOMP</em>, <em>4</em>,
830959. (<a href="https://doi.org/10.3389/fcomp.2022.830959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Living organisms and their biological properties, including the capacity for transformation and representation of information, offer exciting and inspiring opportunities for transdisciplinary art and design explorations. While an emerging body of work is increasingly investigating the possibilities at the intersection of interactive computing, biology, and art, more work is needed to investigate the potential of these approaches for supporting community and public engagement and participation in art, science, and technology. In this project, we describe a multimedia transdisciplinary bioart installation and hands-on agar art activity that we presented to members of the public in a community biology lab setting. Using short interviews, observations, and questionaries, we investigated attendees&#39; reactions and impressions of the experience and found that the event generated transdisciplinary reflections, invited participants to bring their previous knowledge and experience to bear in engaging with different aspects of the work, and that the audience benefited from contextualization by artists.},
  archive      = {J_FCOMP},
  author       = {Stamato, Lydia and Higgins, Erin and Prottoy, Hasan Mahmud and Asgarali-Hoffman, S. Nisa and Scheifele, Lisa and Dusman, Linda and deCarvalho, Tagide and Ascencao, Teresa and Hamidi, Foad},
  doi          = {10.3389/fcomp.2022.830959},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {830959},
  shortjournal = {Front. Comput. Sci.},
  title        = {Raaz: A transdisciplinary exploration at the intersection of bioart, HCI, and community engagement},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding the effect of speed on human emotion
perception in mediated social touch using voice coil actuators.
<em>FCOMP</em>, <em>4</em>, 826637. (<a
href="https://doi.org/10.3389/fcomp.2022.826637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Touch as a modality in social communication has been getting more attention with recent developments in wearable technology and an increase in awareness of how limited physical contact can lead to touch starvation and feelings of depression. Although several mediated touch methods have been developed for conveying emotional support, the transfer of emotion through mediated touch has not been widely studied. This work addresses this need by exploring emotional communication through a novel wearable haptic system. The system records physical touch patterns through an array of force sensors, processes the recordings using novel gesture-based algorithms to create actuator control signals, and generates mediated social touch through an array of voice coil actuators. We conducted a human subject study (N = 20) to understand the perception and emotional components of this mediated social touch for common social touch gestures, including poking, patting, massaging, squeezing, and stroking. Our results show that the speed of the virtual gesture significantly alters the participants&#39; ratings of valence, arousal, realism, and comfort of these gestures with increased speed producing negative emotions and decreased realism. The findings from the study will allow us to better recognize generic patterns from human mediated touch perception and determine how mediated social touch can be used to convey emotion. Our system design, signal processing methods, and results can provide guidance in future mediated social touch design.},
  archive      = {J_FCOMP},
  author       = {Zhu, Xin and Feng, Tiantian and Culbertson, Heather},
  doi          = {10.3389/fcomp.2022.826637},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {826637},
  shortjournal = {Front. Comput. Sci.},
  title        = {Understanding the effect of speed on human emotion perception in mediated social touch using voice coil actuators},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Solo dining at home in the company of ICT devices.
<em>FCOMP</em>, <em>4</em>, 818650. (<a
href="https://doi.org/10.3389/fcomp.2022.818650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consumption of a solo meal is often subject to negative associations. Studies indicate that solo diners use information and communication technology (ICT) devices such as smartphones, to mitigate negative experiences such as boredom and loneliness, especially when dining in a public context. However, we know less about the motivation to use such devices and consequent meal experiences in a private context. For this exploratory qualitative study, we asked participants to fill out a cultural probe kit to capture their dining experience and use of ICT devices over a period of seven days. Once completed, the content was discussed with participants during a semi-structured interview. Data was analyzed using thematic analysis in a deductive and inductive form leading to four themes: (1) The experience of eating with others; (2) The use of electronic devices while eating; (3) The meaning of food; and (4) Relaxing features and influences. Participants indicated that eating alone can be a pleasurable experience that people enjoy and perceive as relaxing. ICT devices were named to play an essential part in the dining experience. The entertainment that devices provide can mitigate feelings of loneliness and uncomfortable silence when eating by oneself. We reflect on the findings and point out potential design avenues for future studies.},
  archive      = {J_FCOMP},
  author       = {Nicolau i Torra, Núria and Lemke, Mailin and Huisman, Gijs},
  doi          = {10.3389/fcomp.2022.818650},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {818650},
  shortjournal = {Front. Comput. Sci.},
  title        = {Solo dining at home in the company of ICT devices},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling japanese praising behavior by analyzing audio and
visual behaviors. <em>FCOMP</em>, <em>4</em>, 815128. (<a
href="https://doi.org/10.3389/fcomp.2022.815128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Praising behavior is considered to be verbal and nonverbal behaviors that expresses praise the behavior and character of the target. However, how one should use verbal and nonverbal behaviors to successfully praise a target has not been clarified. Therefore, we focus on attempts to analyze praising behavior in Japanese dialogue using verbal and nonverbal behaviors. In this study, we attempted to analyze the relationship between praising skills and human behaviors in Japanese dialogue by focusing on voice, head, and face behaviors. First, we created a new dialogue corpus in Japanese containing voice, head, and face behaviors from individuals giving praise (praiser) and receiving praise (receiver), as well as the degree of success of praising (praising score). Second, we developed machine learning models that uses features related to voice, head, and face behaviors to estimate praising skills to clarify which features of the praiser and receiver are important for estimating praising skills. Evaluation resulte showed that some audio features of the praiser are particularly important for estimation of praising skills. Our analysis results demonstrated the importance of features related to the zero-crossing rate, MFCCs of the praiser. Analyzing the features of high importance revealed that the praiser should praise with specific words that mean amazing or great in Japanese and the voice quality of the praiser is considered to be important for praising successfully.},
  archive      = {J_FCOMP},
  author       = {Onishi, Toshiki and Yamauchi, Arisa and Ogushi, Asahi and Ishii, Ryo and Fukayama, Atsushi and Nakamura, Takao and Miyata, Akihiro},
  doi          = {10.3389/fcomp.2022.815128},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {815128},
  shortjournal = {Front. Comput. Sci.},
  title        = {Modeling japanese praising behavior by analyzing audio and visual behaviors},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experiential learning to teach user experience in higher
education in past 20 years: A scoping review. <em>FCOMP</em>,
<em>4</em>, 812907. (<a
href="https://doi.org/10.3389/fcomp.2022.812907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiential learning is an effective method to teach User Experience (UX) to Human-Computer Interaction (HCI) students. Despite its popularity, there seems to be no comprehensive overview on (1) the current use of experiential learning in UX education at universities and (2) student learning outcomes and benefits resulting from the use of experiential learning. Hence, we conducted a scoping review to provide such overview. We analyzed 45 articles published from 2000 to 2021 and we found 12 types of experiential learning employed by HCI educators: applied research project, industry/community research project, hands-on activity, role-play, interactive workshops, guest speakers, in-house work placement, internship, flipped classroom, field project, lab, and design hackathon, from most to least frequent. Twenty-six articles reported student learning outcomes and benefits: (1) enhanced UX technical knowledge, (2) applied textbook knowledge into practice, (3) acquired soft skills, (4) student satisfaction, (5) increased awareness of user diversity, and (6) increased job marketability. Overall, we advance current HCI teaching practices by providing HCI educators with a list of experiential learning types that they can adopt in their classes to teach UX.},
  archive      = {J_FCOMP},
  author       = {Kang, Jin and Roestel, Noemi M. E. and Girouard, Audrey},
  doi          = {10.3389/fcomp.2022.812907},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {812907},
  shortjournal = {Front. Comput. Sci.},
  title        = {Experiential learning to teach user experience in higher education in past 20 years: A scoping review},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting the nature of repetitive actions for their
effective and efficient recognition. <em>FCOMP</em>, <em>4</em>, 806027.
(<a href="https://doi.org/10.3389/fcomp.2022.806027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of human action recognition (HAR), the recognition of actions with large duration is hindered by the memorization capacity limitations of the standard probabilistic and recurrent neural network (R-NN) approaches that are used for temporal sequence modeling. The simplest remedy is to employ methods that reduce the input sequence length, by performing window sampling, pooling, or key-frame extraction. However, due to the nature of the frame selection criteria or the employed pooling operations, the majority of these approaches do not guarantee that the useful, discriminative information is preserved. In this work, we focus on the case of repetitive actions. In such actions, a discriminative, core execution motif is maintained throughout each repetition, with slight variations in execution style and duration. Additionally, scene appearance may change as a consequence of the action. We exploit those two key observations on the nature of repetitive actions to build a compact and efficient representation of long actions by maintaining the discriminative sample information and removing redundant information which is due to task repetitiveness. We show that by partitioning an input sequence based on repetition and by treating each repetition as a discrete sample, HAR models can achieve an increase of up to 4% in action recognition accuracy. Additionally, we investigate the relation between the dataset and action set attributes with this strategy and explore the conditions under which the utilization of repetitiveness for input sequence sampling, is a useful preprocessing step in HAR. Finally, we suggest deep NN design directions that enable the effective exploitation of the distinctive action-related information found in repetitiveness, and evaluate them with a simple deep architecture that follows these principles.},
  archive      = {J_FCOMP},
  author       = {Bacharidis, Konstantinos and Argyros, Antonis},
  doi          = {10.3389/fcomp.2022.806027},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {806027},
  shortjournal = {Front. Comput. Sci.},
  title        = {Exploiting the nature of repetitive actions for their effective and efficient recognition},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From shallow to deep: Exploiting feature-based classifiers
for domain adaptation in semantic segmentation. <em>FCOMP</em>,
<em>4</em>, 805166. (<a
href="https://doi.org/10.3389/fcomp.2022.805166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable performance of Convolutional Neural Networks on image segmentation tasks comes at the cost of a large amount of pixelwise annotated images that have to be segmented for training. In contrast, feature-based learning methods, such as the Random Forest, require little training data, but rarely reach the segmentation accuracy of CNNs. This work bridges the two approaches in a transfer learning setting. We show that a CNN can be trained to correct the errors of the Random Forest in the source domain and then be applied to correct such errors in the target domain without retraining, as the domain shift between the Random Forest predictions is much smaller than between the raw data. By leveraging a few brushstrokes as annotations in the target domain, the method can deliver segmentations that are sufficiently accurate to act as pseudo-labels for target-domain CNN training. We demonstrate the performance of the method on several datasets with the challenging tasks of mitochondria, membrane and nuclear segmentation. It yields excellent performance compared to microscopy domain adaptation baselines, especially when a significant domain shift is involved.},
  archive      = {J_FCOMP},
  author       = {Matskevych, Alex and Wolny, Adrian and Pape, Constantin and Kreshuk, Anna},
  doi          = {10.3389/fcomp.2022.805166},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {805166},
  shortjournal = {Front. Comput. Sci.},
  title        = {From shallow to deep: Exploiting feature-based classifiers for domain adaptation in semantic segmentation},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Outer product-based fusion of smartwatch sensor data for
human activity recognition. <em>FCOMP</em>, <em>4</em>, 796866. (<a
href="https://doi.org/10.3389/fcomp.2022.796866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of IoT devices in combination with Human Activity Recognition (HAR) technologies can contribute to battle with sedentariness by continuously monitoring the users&#39; daily activities. With this information, autonomous systems could detect users&#39; physical weaknesses and plan personalized training routines to improve them. This work investigates the multimodal fusion of smartwatch sensor data for HAR. Specifically, we exploit pedometer, heart rate, and accelerometer information to train unimodal and multimodal models for the task at hand. The models are trained end-to-end, and we compare the performance of dedicated Recurrent Neural Network-based (RNN) and Convolutional Neural Network-based (CNN) architectures to extract deep learnt representations from the input modalities. To fuse the embedded representations when training the multimodal models, we investigate a concatenation-based and an outer product-based approach. This work explores the harAGE dataset, a new dataset for HAR collected using a Garmin Vivoactive 3 device with more than 17 h of data. Our best models obtain an Unweighted Average Recall (UAR) of 95.6, 69.5, and 60.8% when tackling the task as a 2-class, 7-class, and 10-class classification problem, respectively. These performances are obtained using multimodal models that fuse the embedded representations extracted with dedicated CNN-based architectures from the pedometer, heart rate, and accelerometer modalities. The concatenation-based fusion scores the highest UAR in the 2-class classification problem, while the outer product-based fusion obtains the best performances in the 7-class and the 10-class classification problems.},
  archive      = {J_FCOMP},
  author       = {Mallol-Ragolta, Adria and Semertzidou, Anastasia and Pateraki, Maria and Schuller, Björn},
  doi          = {10.3389/fcomp.2022.796866},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {796866},
  shortjournal = {Front. Comput. Sci.},
  title        = {Outer product-based fusion of smartwatch sensor data for human activity recognition},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The user experience design program: Applying situated and
embodied cognition together with reflective teaching. <em>FCOMP</em>,
<em>4</em>, 794400. (<a
href="https://doi.org/10.3389/fcomp.2022.794400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The education of students to become competent user experience designers is a delicate matter as students need to obtain a multitude of knowledge, skills, and judgmental abilities. In this paper, our effort to manage this multiplicity in a bachelor&#39;s program in user experience design is shared along with our experiences and teaching practices influenced by theories of situated and embodied cognition together with reflective teaching. The program was followed up through interviews with eight alumni and a company representative that employs user experience designers. The results show that the program overall works well, although some of the identified issues need to be addressed in the future. The interpretation is that our program curricula and teaching practices are fruitful, which hopefully can contribute to thoughts and discussions for other teachers in the field of user experience design and human-computer interaction.},
  archive      = {J_FCOMP},
  author       = {Alenljung, Beatrice and Nalin, Kajsa and Rambusch, Jana},
  doi          = {10.3389/fcomp.2022.794400},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {794400},
  shortjournal = {Front. Comput. Sci.},
  title        = {The user experience design program: Applying situated and embodied cognition together with reflective teaching},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Teaching user experience design ethics to engineering
students: Lessons learned. <em>FCOMP</em>, <em>4</em>, 793879. (<a
href="https://doi.org/10.3389/fcomp.2022.793879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary dilemmas about the role and impact of digital technologies in society have motivated the inclusion of topics of computing ethics in university programmes. Many past works have investigated how different pedagogical approaches and tools can support learning and teaching such a subject. This brief research report contributes to these efforts by describing a pilot study examining how engineering students learn from and apply ethical principles when making design decisions for an introductory User Experience (UX) design project. After a short lecture, students were asked to design and evaluate the ethical implications of digital health intervention prototypes. This approach was evaluated through the thematic analysis of semi-instructed interviews conducted with 12 students, focused on the benefits and limitations of teaching ethics this way. Findings indicate that it can be very challenging to convey the importance of ethics to unaware and uninterested students, an observation that calls for a much stronger emphasis on moral philosophy education throughout engineering degrees. This paper finishes with a reflection on the hardships and possible ways forward for teaching and putting UX design ethics into practice. The lessons learned and described in this report aim to contribute to future pedagogical efforts to enable ethical thinking in computing education.},
  archive      = {J_FCOMP},
  author       = {Vilaza, Giovanna Nunes and Bækgaard, Per},
  doi          = {10.3389/fcomp.2022.793879},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {793879},
  shortjournal = {Front. Comput. Sci.},
  title        = {Teaching user experience design ethics to engineering students: Lessons learned},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal EEG and eye tracking feature fusion approaches
for attention classification in hybrid BCIs. <em>FCOMP</em>, <em>4</em>,
780580. (<a href="https://doi.org/10.3389/fcomp.2022.780580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often, various modalities capture distinct aspects of particular mental states or activities. While machine learning algorithms can reliably predict numerous aspects of human cognition and behavior using a single modality, they can benefit from the combination of multiple modalities. This is why hybrid BCIs are gaining popularity. However, it is not always straightforward to combine features from a multimodal dataset. Along with the method for generating the features, one must decide when the modalities should be combined during the classification process. We compare unimodal EEG and eye tracking classification of internally and externally directed attention to multimodal approaches for early, middle, and late fusion in this study. On a binary dataset with a chance level of 0.5, late fusion of the data achieves the highest classification accuracy of 0.609–0.675 (95%-confidence interval). In general, the results indicate that for these modalities, middle or late fusion approaches are better suited than early fusion approaches. Additional validation of the observed trend will require the use of additional datasets, alternative feature generation mechanisms, decision rules, and neural network designs. We conclude with a set of premises that need to be considered when deciding on a multimodal attentional state classification approach.},
  archive      = {J_FCOMP},
  author       = {Vortmann, Lisa-Marie and Ceh, Simon and Putze, Felix},
  doi          = {10.3389/fcomp.2022.780580},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {780580},
  shortjournal = {Front. Comput. Sci.},
  title        = {Multimodal EEG and eye tracking feature fusion approaches for attention classification in hybrid BCIs},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Tonga”: A novel toolbox for straightforward bioimage
analysis. <em>FCOMP</em>, <em>4</em>, 777458. (<a
href="https://doi.org/10.3389/fcomp.2022.777458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Techniques to acquire and analyze biological images are central to life science. However, the workflow downstream of imaging can be complex and involve several tools, leading to creation of very specialized scripts and pipelines that are difficult to reproduce by other users. Although many commercial and open-source software are available, non-expert users are often challenged by a knowledge gap in setting up analysis pipelines and selecting correct tools for extracting data from images. Moreover, a significant share of everyday image analysis requires simple tools, such as precise segmentation, cell counting, and recording of fluorescent intensities. Hence, there is a need for user-friendly platforms for everyday image analysis that do not require extensive prior knowledge on bioimage analysis or coding. We set out to create a bioimage analysis software that has a straightforward interface and covers common analysis tasks such as object segmentation and analysis, in a practical, reproducible, and modular fashion. We envision our software being useful for analysis of cultured cells, histological sections, and high-content data.},
  archive      = {J_FCOMP},
  author       = {Ritchie, Alexandra and Laitinen, Suvi and Katajisto, Pekka and Englund, Johanna I.},
  doi          = {10.3389/fcomp.2022.777458},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {777458},
  shortjournal = {Front. Comput. Sci.},
  title        = {“Tonga”: A novel toolbox for straightforward bioimage analysis},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Device classification for industrial control systems using
predicted traffic features. <em>FCOMP</em>, <em>4</em>, 777089. (<a
href="https://doi.org/10.3389/fcomp.2022.777089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve a secure interconnected Industrial Control System (ICS) architecture, security practitioners depend on accurate identification of network host behavior. However, accurate machine learning based host identification methods depends on the availability of significant quantities of network traffic data, which can be difficult to obtain due to system constraints such as network security, data confidentiality, and physical location. In this work, we propose a network traffic feature prediction method based on a generative model, which achieves high host identification accuracy. Furthermore, we develop a joint training algorithm to improve host identification performance compared to separate training of the generative model and the classifier responsible for host identification.},
  archive      = {J_FCOMP},
  author       = {Chakraborty, Indrasis and Kelley, Brian M. and Gallagher, Brian},
  doi          = {10.3389/fcomp.2022.777089},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {777089},
  shortjournal = {Front. Comput. Sci.},
  title        = {Device classification for industrial control systems using predicted traffic features},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An estimation of online video user engagement from features
of time- and value-continuous, dimensional emotions. <em>FCOMP</em>,
<em>4</em>, 773154. (<a
href="https://doi.org/10.3389/fcomp.2022.773154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portraying emotion and trustworthiness is known to increase the appeal of video content. However, the causal relationship between these signals and online user engagement is not well understood. This limited understanding is partly due to a scarcity in emotionally annotated data and the varied modalities which express user engagement online. In this contribution, we utilize a large dataset of YouTube review videos which includes ca. 600 h of dimensional arousal, valence and trustworthiness annotations. We investigate features extracted from these signals against various user engagement indicators including views, like/dislike ratio, as well as the sentiment of comments. In doing so, we identify the positive and negative influences which single features have, as well as interpretable patterns in each dimension which relate to user engagement. Our results demonstrate that smaller boundary ranges and fluctuations for arousal lead to an increase in user engagement. Furthermore, the extracted time-series features reveal significant (p &amp;lt; 0.05) correlations for each dimension, such as, count below signal mean (arousal), number of peaks (valence), and absolute energy (trustworthiness). From this, an effective combination of features is outlined for approaches aiming to automatically predict several user engagement indicators. In a user engagement prediction paradigm we compare all features against semi-automatic (cross-task), and automatic (task-specific) feature selection methods. These selected feature sets appear to outperform the usage of all features, e.g., using all features achieves 1.55 likes per day (Lp/d) mean absolute error from valence; this improves through semi-automatic and automatic selection to 1.33 and 1.23 Lp/d, respectively (data mean 9.72 Lp/d with a std. 28.75 Lp/d).},
  archive      = {J_FCOMP},
  author       = {Stappen, Lukas and Baird, Alice and Lienhart, Michelle and Bätz, Annalena and Schuller, Björn},
  doi          = {10.3389/fcomp.2022.773154},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {773154},
  shortjournal = {Front. Comput. Sci.},
  title        = {An estimation of online video user engagement from features of time- and value-continuous, dimensional emotions},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a cushion-shaped device to induce respiratory
rhythm and depth for enhanced relaxation and improved cognition.
<em>FCOMP</em>, <em>4</em>, 770701. (<a
href="https://doi.org/10.3389/fcomp.2022.770701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workplace stress is a pertinent problem in today&#39;s world. Preventing and overcoming stress is critical for a healthy lifestyle because it is linked to various health problems and can lead to poor work performance. Controlling your breathing is one of the most effective ways to promote relaxation. However, regulating one&#39;s breathing necessitates some training and is not something that everyone can do easily. As a result, we concentrated on the relaxing effect of breathing and developed a cushion-shaped device that displays the desired respiratory motion. We used the effect of inducing one&#39;s respiratory movements by watching others&#39; respiratory movements. When the user hugged the device, it changed the user&#39;s respiratory rhythm and depth. We conducted a user study with this device, which revealed that presenting respiratory motion can induce the user&#39;s respiratory rhythm and depth without any pre-training. Furthermore, subjective evaluation and ECG data suggested that using this device during task breaks can improve the relaxation effect and thus task performance after the break.},
  archive      = {J_FCOMP},
  author       = {Ban, Yuki and Karasawa, Hiroyuki and Fukui, Rui and Warisawa, Shin&#39;ichi},
  doi          = {10.3389/fcomp.2022.770701},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {770701},
  shortjournal = {Front. Comput. Sci.},
  title        = {Development of a cushion-shaped device to induce respiratory rhythm and depth for enhanced relaxation and improved cognition},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A cross-corpus speech-based analysis of escalating negative
interactions. <em>FCOMP</em>, <em>4</em>, 749804. (<a
href="https://doi.org/10.3389/fcomp.2022.749804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The monitoring of an escalating negative interaction has several benefits, particularly in security, (mental) health, and group management. The speech signal is particularly suited to this, as aspects of escalation, including emotional arousal, are proven to easily be captured by the audio signal. A challenge of applying trained systems in real-life applications is their strong dependence on the training material and limited generalization abilities. For this reason, in this contribution, we perform an extensive analysis of three corpora in the Dutch language. All three corpora are high in escalation behavior content and are annotated on alternative dimensions related to escalation. A process of label mapping resulted in two possible ground truth estimations for the three datasets as low, medium, and high escalation levels. To observe class behavior and inter-corpus differences more closely, we perform acoustic analysis of the audio samples, finding that derived labels perform similarly across each corpus, with escalation interaction increasing in pitch (F0) and intensity (dB). We explore the suitability of different speech features, data augmentation, merging corpora for training, and testing on actor and non-actor speech through our experiments. We find that the extent to which merging corpora is successful depends greatly on the similarities between label definitions before label mapping. Finally, we see that the escalation recognition task can be performed in a cross-corpus setup with hand-crafted speech features, obtaining up to 63.8% unweighted average recall (UAR) at best for a cross-corpus analysis, an increase from the inter-corpus results of 59.4% UAR.},
  archive      = {J_FCOMP},
  author       = {Lefter, Iulia and Baird, Alice and Stappen, Lukas and Schuller, Björn W.},
  doi          = {10.3389/fcomp.2022.749804},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {749804},
  shortjournal = {Front. Comput. Sci.},
  title        = {A cross-corpus speech-based analysis of escalating negative interactions},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling feedback in interaction with conversational
agents—a review. <em>FCOMP</em>, <em>4</em>, 744574. (<a
href="https://doi.org/10.3389/fcomp.2022.744574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent agents interacting with humans through conversation (such as a robot, embodied conversational agent, or chatbot) need to receive feedback from the human to make sure that its communicative acts have the intended consequences. At the same time, the human interacting with the agent will also seek feedback, in order to ensure that her communicative acts have the intended consequences. In this review article, we give an overview of past and current research on how intelligent agents should be able to both give meaningful feedback toward humans, as well as understanding feedback given by the users. The review covers feedback across different modalities (e.g., speech, head gestures, gaze, and facial expression), different forms of feedback (e.g., backchannels, clarification requests), and models for allowing the agent to assess the user&#39;s level of understanding and adapt its behavior accordingly. Finally, we analyse some shortcomings of current approaches to modeling feedback, and identify important directions for future research.},
  archive      = {J_FCOMP},
  author       = {Axelsson, Agnes and Buschmeier, Hendrik and Skantze, Gabriel},
  doi          = {10.3389/fcomp.2022.744574},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {744574},
  shortjournal = {Front. Comput. Sci.},
  title        = {Modeling feedback in interaction with conversational Agents—A review},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Brain-computer interfaces for non-clinical (home,
sports, art, entertainment, education, well-being) applications.
<em>FCOMP</em>, <em>4</em>, 860619. (<a
href="https://doi.org/10.3389/fcomp.2022.860619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Nijholt, Anton and Contreras-Vidal, Jose Luis and Jeunet, Camille and Väljamäe, Aleksander},
  doi          = {10.3389/fcomp.2022.860619},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {860619},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: Brain-computer interfaces for non-clinical (Home, sports, art, entertainment, education, well-being) applications},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluating the impact of voice activity detection on speech
emotion recognition for autistic children. <em>FCOMP</em>, <em>4</em>,
837269. (<a href="https://doi.org/10.3389/fcomp.2022.837269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals with autism are known to face challenges with emotion regulation, and express their affective states in a variety of ways. With this in mind, an increasing amount of research on automatic affect recognition from speech and other modalities has recently been presented to assist and provide support, as well as to improve understanding of autistic individuals&#39; behaviours. As well as the emotion expressed from the voice, for autistic children the dynamics of verbal speech can be inconsistent and vary greatly amongst individuals. The current contribution outlines a voice activity detection (VAD) system specifically adapted to autistic children&#39;s vocalisations. The presented VAD system is a recurrent neural network (RNN) with long short-term memory (LSTM) cells. It is trained on 130 acoustic Low-Level Descriptors (LLDs) extracted from more than 17 h of audio recordings, which were richly annotated by experts in terms of perceived emotion as well as occurrence and type of vocalisations. The data consist of 25 English-speaking autistic children undertaking a structured, partly robot-assisted emotion-training activity and was collected as part of the DE-ENIGMA project. The VAD system is further utilised as a preprocessing step for a continuous speech emotion recognition (SER) task aiming to minimise the effects of potential confounding information, such as noise, silence, or non-child vocalisation. Its impact on the SER performance is compared to the impact of other VAD systems, including a general VAD system trained from the same data set, an out-of-the-box Web Real-Time Communication (WebRTC) VAD system, as well as the expert annotations. Our experiments show that the child VAD system achieves a lower performance than our general VAD system, trained under identical conditions, as we obtain receiver operating characteristic area under the curve (ROC-AUC) metrics of 0.662 and 0.850, respectively. The SER results show varying performances across valence and arousal depending on the utilised VAD system with a maximum concordance correlation coefficient (CCC) of 0.263 and a minimum root mean square error (RMSE) of 0.107. Although the performance of the SER models is generally low, the child VAD system can lead to slightly improved results compared to other VAD systems and in particular the VAD-less baseline, supporting the hypothesised importance of child VAD systems in the discussed context.},
  archive      = {J_FCOMP},
  author       = {Milling, Manuel and Baird, Alice and Bartl-Pokorny, Katrin D. and Liu, Shuo and Alcorn, Alyssa M. and Shen, Jie and Tavassoli, Teresa and Ainger, Eloise and Pellicano, Elizabeth and Pantic, Maja and Cummins, Nicholas and Schuller, Björn W.},
  doi          = {10.3389/fcomp.2022.837269},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {837269},
  shortjournal = {Front. Comput. Sci.},
  title        = {Evaluating the impact of voice activity detection on speech emotion recognition for autistic children},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Teaching for values in human–computer interaction.
<em>FCOMP</em>, <em>4</em>, 830736. (<a
href="https://doi.org/10.3389/fcomp.2022.830736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing awareness of the importance of considering values in the design of technology. There are several research approaches focused on this, such as e.g., value-sensitive design, value-centred human–computer interaction (HCI), and value-led participatory design, just to mention a few. However, less attention has been given to developing educational materials for the role that values play in HCI, why hands-on teaching activities are insufficient, and especially teaching activities that cover the full design process. In this article, we claim that teaching for ethics and values in HCI is not only important in some parts of the design and development process, but equally important all through. We will demonstrate this by a unique collection of 28 challenges identified throughout the design process, accompanied by inspirational suggestions for teaching activities to tackle these challenges. The article is based on results from applying a modified pedagogical design pattern approach in the iterative development of an open educational resource containing teaching and assessment activities and pedagogical framework, and from pilot testing. Preliminary results from pilots of parts of the teaching activities indicate that student participants experience achieving knowledge about how to understand and act ethically on human values in design, and teachers experience an increased capacity to teach for values in design in relevant and innovative ways. Hopefully, this overview of challenges and inspirational teaching activities focused on values in the design of technology can be one way to provide teachers with inspiration to sensitize their students and make them better prepared to become responsible designers by learning how to address and work with values in HCI.},
  archive      = {J_FCOMP},
  author       = {Eriksson, Eva and Nilsson, Elisabet M. and Hansen, Anne-Marie and Bekker, Tilde},
  doi          = {10.3389/fcomp.2022.830736},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {830736},
  shortjournal = {Front. Comput. Sci.},
  title        = {Teaching for values in Human–Computer interaction},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Game design, gender and personalities in programming
education. <em>FCOMP</em>, <em>4</em>, 824995. (<a
href="https://doi.org/10.3389/fcomp.2022.824995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a changing world programming learning is becoming more and more essential in education. And, there are many programming environments and teaching approaches that address the learning needs of students in CS education. A single programming tool or a method do not fit all students. Research has focused on gender differences and there is high interest in increasing female participation. Games and especially game-design tend to impact perceived usefulness of programming environments. Moreover, personality traits like cognitive style and emotional intelligence (EQ) seem to correlate with technology and achievement in programming. In this study, the effects of three different programming environments were investigated, in high school settings, by creating games and taking into account gender and personality characteristics. Three groups were formed, group A created games with Scratch, group B used App Inventor and made games for mobile devices, while group C created interactive stories-games with Alice 3D. This study was seeking to find possible biases based on gender, learning perception, usage and students&#39; personalities between the three experimental conditions. One hundred and sixty three students aged 14–15 years old participated in the study, and data were collected through pre activity and post activity questionnaires. Results show different gender preferences for the three programming tools and, in some cases, different personalities (cognitive styles and EQ) have different learning preferences. Moreover, all programming environments had different emotional effects on the students. The study concludes with guidelines for programming learning environments that respect individual learning preferences and aim to maximize learning effectiveness.},
  archive      = {J_FCOMP},
  author       = {Theodoropoulos, Anastasios and Lepouras, George},
  doi          = {10.3389/fcomp.2022.824995},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {824995},
  shortjournal = {Front. Comput. Sci.},
  title        = {Game design, gender and personalities in programming education},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The brain-computer metaphor debate is useless: A matter of
semantics. <em>FCOMP</em>, <em>4</em>, 810358. (<a
href="https://doi.org/10.3389/fcomp.2022.810358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is commonly assumed that usage of the word “computer” in the brain sciences reflects a metaphor. However, there is no single definition of the word “computer” in use. In fact, based on the usage of the word “computer” in computer science, a computer is merely some physical machinery that can in theory compute any computable function. According to this definition the brain is literally a computer; there is no metaphor. But, this deviates from how the word “computer” is used in other academic disciplines. According to the definition used outside of computer science, “computers” are human-made devices that engage in sequential processing of inputs to produce outputs. According to this definition, brains are not computers, and arguably, computers serve as a weak metaphor for brains. Thus, we argue that the recurring brain-computer metaphor debate is actually just a semantic disagreement, because brains are either literally computers or clearly not very much like computers at all, depending on one&#39;s definitions. We propose that the best path forward is simply to put the debate to rest, and instead, have researchers be clear about which definition they are using in their work. In some circumstances, one can use the definition from computer science and simply ask, what type of computer is the brain? In other circumstances, it is important to use the other definition, and to clarify the ways in which our brains are radically different from the laptops, smartphones, and servers that surround us in modern life.},
  archive      = {J_FCOMP},
  author       = {Richards, Blake A. and Lillicrap, Timothy P.},
  doi          = {10.3389/fcomp.2022.810358},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {810358},
  shortjournal = {Front. Comput. Sci.},
  title        = {The brain-computer metaphor debate is useless: A matter of semantics},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LABKIT: Labeling and segmentation toolkit for big image
data. <em>FCOMP</em>, <em>4</em>, 777728. (<a
href="https://doi.org/10.3389/fcomp.2022.777728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present LABKIT, a user-friendly Fiji plugin for the segmentation of microscopy image data. It offers easy to use manual and automated image segmentation routines that can be rapidly applied to single- and multi-channel images as well as to timelapse movies in 2D or 3D. LABKIT is specifically designed to work efficiently on big image data and enables users of consumer laptops to conveniently work with multiple-terabyte images. This efficiency is achieved by using ImgLib2 and BigDataViewer as well as a memory efficient and fast implementation of the random forest based pixel classification algorithm as the foundation of our software. Optionally we harness the power of graphics processing units (GPU) to gain additional runtime performance. LABKIT is easy to install on virtually all laptops and workstations. Additionally, LABKIT is compatible with high performance computing (HPC) clusters for distributed processing of big image data. The ability to use pixel classifiers trained in LABKIT via the ImageJ macro language enables our users to integrate this functionality as a processing step in automated image processing workflows. Finally, LABKIT comes with rich online resources such as tutorials and examples that will help users to familiarize themselves with available features and how to best use LABKIT in a number of practical real-world use-cases.},
  archive      = {J_FCOMP},
  author       = {Arzt, Matthias and Deschamps, Joran and Schmied, Christopher and Pietzsch, Tobias and Schmidt, Deborah and Tomancak, Pavel and Haase, Robert and Jug, Florian},
  doi          = {10.3389/fcomp.2022.777728},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {777728},
  shortjournal = {Front. Comput. Sci.},
  title        = {LABKIT: Labeling and segmentation toolkit for big image data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). μMatch: 3D shape correspondence for biological image data.
<em>FCOMP</em>, <em>4</em>, 777615. (<a
href="https://doi.org/10.3389/fcomp.2022.777615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern microscopy technologies allow imaging biological objects in 3D over a wide range of spatial and temporal scales, opening the way for a quantitative assessment of morphology. However, establishing a correspondence between objects to be compared, a first necessary step of most shape analysis workflows, remains challenging for soft-tissue objects without striking features allowing them to be landmarked. To address this issue, we introduce the μMatch 3D shape correspondence pipeline. μMatch implements a state-of-the-art correspondence algorithm initially developed for computer graphics and packages it in a streamlined pipeline including tools to carry out all steps from input data pre-processing to classical shape analysis routines. Importantly, μMatch does not require any landmarks on the object surface and establishes correspondence in a fully automated manner. Our open-source method is implemented in Python and can be used to process collections of objects described as triangular meshes. We quantitatively assess the validity of μMatch relying on a well-known benchmark dataset and further demonstrate its reliability by reproducing published results previously obtained through manual landmarking.},
  archive      = {J_FCOMP},
  author       = {Klatzow, James and Dalmasso, Giovanni and Martínez-Abadías, Neus and Sharpe, James and Uhlmann, Virginie},
  doi          = {10.3389/fcomp.2022.777615},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {777615},
  shortjournal = {Front. Comput. Sci.},
  title        = {μMatch: 3D shape correspondence for biological image data},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic thoughts and facial expressions in cognitive
restructuring with virtual agents. <em>FCOMP</em>, <em>4</em>, 762424.
(<a href="https://doi.org/10.3389/fcomp.2022.762424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive restructuring is a well-established mental health technique for amending automatic thoughts, which are distorted and biased beliefs about a situation, into objective and balanced thoughts. Since virtual agents can be used anytime and anywhere, they are expected to perform cognitive restructuring without being influenced by medical infrastructure or patients&#39; stigma toward mental illness. Unfortunately, since the quantitative analysis of human-agent interaction is still insufficient, the effect on the user&#39;s cognitive state remains unclear. We collected interaction data between virtual agents and users to observe the mood improvements associated with changes in automatic thoughts that occur in user cognition and addressed the following two points: (1) implementation of a virtual agent that helps a user identify and evaluate automatic thoughts; (2) identification of the relationship between a user&#39;s facial expressions and the extent of the mood improvement subjectively felt by users during the human-agent interaction. We focus on these points because cognitive restructuring by a human therapist starts by identifying automatic thoughts and seeking sufficient evidence to find balanced thoughts (evaluation of automatic thoughts). Therapists also use such non-verbal behaviors as facial expressions to detect changes in a user&#39;s mood, which is an important indicator for guidance. Based on the results of this analysis, we provide a technical guidance framework that fully automates the identification and evaluation of automatic thoughts to achieve a virtual agent that can interact with users by taking into account their verbal and non-verbal behaviors in face-to-face situations. This research supports the possibility of improving the effectiveness of mental health care in cognitive restructuring using virtual agents.},
  archive      = {J_FCOMP},
  author       = {Shidara, Kazuhiro and Tanaka, Hiroki and Adachi, Hiroyoshi and Kanayama, Daisuke and Sakagami, Yukako and Kudo, Takashi and Nakamura, Satoshi},
  doi          = {10.3389/fcomp.2022.762424},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {762424},
  shortjournal = {Front. Comput. Sci.},
  title        = {Automatic thoughts and facial expressions in cognitive restructuring with virtual agents},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmenting human appearance through technological design
layers. <em>FCOMP</em>, <em>4</em>, 755451. (<a
href="https://doi.org/10.3389/fcomp.2022.755451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmenting human appearance with the means of technology can focus on different layers attached to or around the body. In this article, we present a categorization of human appearance and expression, with augmenting skin and its appendages, clothing and textile, accessories, body parts, and digital aura around the body. We report a non-systematic review of related works in each category and discuss their means in expressing functional, hedonic, and social aspects. In conclusion, our study contributes design perspectives on augmenting human appearances, as well as reveals challenges and opportunities.},
  archive      = {J_FCOMP},
  author       = {Genç, Çağlar and Raudanjoki, Özge and Colley, Ashley and Häkkilä, Jonna},
  doi          = {10.3389/fcomp.2022.755451},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {755451},
  shortjournal = {Front. Comput. Sci.},
  title        = {Augmenting human appearance through technological design layers},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “Alexa, you’re really stupid”: A longitudinal field study on
communication breakdowns between family members and a voice assistant.
<em>FCOMP</em>, <em>4</em>, 791704. (<a
href="https://doi.org/10.3389/fcomp.2022.791704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present the results of our long-term study on use of a voice assistant (Amazon Alexa via Amazon Echo Dot) in nine families with children and no previous experience with this technology. The study was conducted over the course of 5 weeks during which the families could interact with the device freely. Three house visits were made to collect empirical data from the adult participants in form of questionnaires. Additionally, conversational data from log files of the voice assistant were obtained. These data were annotated and analyzed with a focus on communication breakdowns during human-assistant interaction. We investigate user behavior for both adults and children in such situations, its reasons and consequences for user satisfaction. This article provides qualitative analysis of three particularly interesting breakdown cases, as well as statistical analysis along several hypotheses and research questions combining empirical and conversational data. Described cases of communication breakdown illustrate findings from existing literature on the topic. The statistical analysis paints a mixed picture, however, it helped us identify further avenues for research, some of which can be explored with our data set in the future. We found a significant negative effect of the number of abandoned failed requests on user satisfaction, contrary to the number of successfully repaired requests that had no influence on user satisfaction. We discovered that users are more inclined to use reformulation as repair strategy when they do not perceive the emergence of miscommunication as their fault. We could not identify a significant effect of internal reasons for the choice of other strategies, so we suggest that situational clues such as the immediate response of the voice assistant are more important for the choice of repair strategy. Our results also hint that users distinguish between repair strategies differently, as the self-perceived frequency of repetitions and abortions of requests were found to be positive predictors for the use of reformulation-based strategies. With regards to the long-term aspect of the study, use of repetition as a repair strategy by both children and adults significantly decreased with time, no other changes were found for other strategies. Additionally, no significant impact of age on the choice of repair strategy was found, as well as no interaction effect between age and time.},
  archive      = {J_FCOMP},
  author       = {Mavrina, Lina and Szczuka, Jessica and Strathmann, Clara and Bohnenkamp, Lisa Michelle and Krämer, Nicole and Kopp, Stefan},
  doi          = {10.3389/fcomp.2022.791704},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {791704},
  shortjournal = {Front. Comput. Sci.},
  title        = {“Alexa, you&#39;re really stupid”: A longitudinal field study on communication breakdowns between family members and a voice assistant},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Emotion recognition in a multi-componential framework: The
role of physiology. <em>FCOMP</em>, <em>4</em>, 773256. (<a
href="https://doi.org/10.3389/fcomp.2022.773256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Component Process Model is a well-established framework describing an emotion as a dynamic process with five highly interrelated components: cognitive appraisal, expression, motivation, physiology and feeling. Yet, few empirical studies have systematically investigated discrete emotions through this full multi-componential view. We therefore elicited various emotions during movie watching and measured their manifestations across these components. Our goal was to investigate the relationship between physiological measures and the theoretically defined components, as well as to determine whether discrete emotions could be predicted from the multicomponent response patterns. By deploying a data-driven computational approach based on multivariate pattern classification, our results suggest that physiological features are encoded within each component, supporting the hypothesis of a synchronized recruitment during an emotion episode. Overall, while emotion prediction was higher when classifiers were trained with all five components, a model without physiology features did not significantly reduce the performance. The findings therefore support a description of emotion as a multicomponent process, in which emotion recognition requires the integration of all the components. However, they also indicate that physiology per se is the least significant predictor for emotion classification among these five components.},
  archive      = {J_FCOMP},
  author       = {Menétrey, Maëlan Q. and Mohammadi, Gelareh and Leitão, Joana and Vuilleumier, Patrik},
  doi          = {10.3389/fcomp.2022.773256},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {773256},
  shortjournal = {Front. Comput. Sci.},
  title        = {Emotion recognition in a multi-componential framework: The role of physiology},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unifying physical interaction, linguistic communication, and
language acquisition of cognitive agents by minimalist grammars.
<em>FCOMP</em>, <em>4</em>, 733596. (<a
href="https://doi.org/10.3389/fcomp.2022.733596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive agents that act independently and solve problems in their environment on behalf of a user are referred to as autonomous. In order to increase the degree of autonomy, advanced cognitive architectures also contain higher-level psychological modules with which needs and motives of the agent are also taken into account and with which the behavior of the agent can be controlled. Regardless of the level of autonomy, successful behavior is based on interacting with the environment and being able to communicate with other agents or users. The agent can use these skills to learn a truthful knowledge model of the environment and thus predict the consequences of its own actions. For this purpose, the symbolic information received during the interaction and communication must be converted into representational data structures so that they can be stored in the knowledge model, processed logically and retrieved from there. Here, we firstly outline a grammar-based transformation mechanism that unifies the description of physical interaction and linguistic communication and on which the language acquisition is based. Specifically, we use minimalist grammar (MG) for this aim, which is a recent computational implementation of generative linguistics. In order to develop proper cognitive information and communication technologies, we are using utterance meaning transducers (UMT) that are based on semantic parsers and a mental lexicon, comprising syntactic and semantic features of the language under consideration. This lexicon must be acquired by a cognitive agent during interaction with its users. To this aim we outline a reinforcement learning algorithm for the acquisition of syntax and semantics of English utterances. English declarative sentences are presented to the agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as formulas of predicate logic. Since MG codifies universal linguistic competence through inference rules, thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach unifies generative grammar and reinforcement learning, hence potentially resolving the still pending Chomsky-Skinner controversy.},
  archive      = {J_FCOMP},
  author       = {Römer, Ronald and beim Graben, Peter and Huber-Liebl, Markus and Wolff, Matthias},
  doi          = {10.3389/fcomp.2022.733596},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {733596},
  shortjournal = {Front. Comput. Sci.},
  title        = {Unifying physical interaction, linguistic communication, and language acquisition of cognitive agents by minimalist grammars},
  volume       = {4},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the frontiers of software science and software
engineering. <em>FCOMP</em>, <em>3</em>, 766053. (<a
href="https://doi.org/10.3389/fcomp.2021.766053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in software engineering, software science, computational intelligence, and intelligent mathematics have led to the establishment of Frontiers in Computer Science—Software (FCSS). FCSS aims to promote transdisciplinary research on software science and engineering (SSE), autonomous systems, and computational intelligence. FCSS covers not only classical empirical software engineering and industrial processes, but also contemporary topics of software science, intelligent programming languages, autonomous code generation, mathematical foundations of software, and programming knowledge bases. FCSS reports empirical studies and emerging topics in software engineering including tools, development platforms, industrial processes, management infrastructures, quality assurance schemes, big data systems, and software migrations across languages and platforms.},
  archive      = {J_FCOMP},
  author       = {Wang, Yingxu},
  doi          = {10.3389/fcomp.2021.766053},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {766053},
  shortjournal = {Front. Comput. Sci.},
  title        = {On the frontiers of software science and software engineering},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ten questions for a theory of vision. <em>FCOMP</em>,
<em>3</em>, 701248. (<a
href="https://doi.org/10.3389/fcomp.2021.701248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By and large, the remarkable progress in visual object recognition in the last few years has been fueled by the availability of huge amounts of labelled data paired with powerful, bespoke computational resources. This has opened the doors to the massive use of deep learning, which has led to remarkable improvements on new challenging benchmarks. While acknowledging this point of view, in this paper I claim that the time has come to begin working towards a deeper understanding of visual computational processes that, instead of being regarded as applications of general purpose machine learning algorithms, are likely to require tailored learning schemes. A major claim of in this paper is that current approaches to object recognition lead to facing a problem that is significantly more difficult than the one offered by nature. This is because of learning algorithms that work on images in isolation, while neglecting the crucial role of temporal coherence. Starting from this remark, this paper raises ten questions concerning visual computational processes that might contribute to better solutions to a number of challenging computer vision tasks. While this paper is far from being able to provide answers to those questions, it contains some insights that might stimulate an in-depth re-thinking in object perception, while suggesting research directions in the control of object-directed action.},
  archive      = {J_FCOMP},
  author       = {Gori, Marco},
  doi          = {10.3389/fcomp.2021.701248},
  journal      = {Frontiers in Computer Science},
  month        = {3},
  pages        = {701248},
  shortjournal = {Front. Comput. Sci.},
  title        = {Ten questions for a theory of vision},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated video monitoring of unmarked and marked honey bees
at the hive entrance. <em>FCOMP</em>, <em>3</em>, 769338. (<a
href="https://doi.org/10.3389/fcomp.2021.769338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel system for the automatic video monitoring of honey bee foraging activity at the hive entrance. This monitoring system is built upon convolutional neural networks that perform multiple animal pose estimation without the need for marking. This precise detection of honey bee body parts is a key element of the system to provide detection of entrance and exit events at the entrance of the hive including accurate pollen detection. A detailed evaluation of the quality of the detection and a study of the effect of the parameters are presented. The complete system also integrates identification of barcode marked bees, which enables the monitoring at both aggregate and individual levels. The results obtained on multiple days of video recordings show the applicability of the approach for large-scale deployment. This is an important step forward for the understanding of complex behaviors exhibited by honey bees and the automatic assessment of colony health.},
  archive      = {J_FCOMP},
  author       = {Rodriguez, Iván F. and Chan, Jeffrey and Alvarez Rios, Manuel and Branson, Kristin and Agosto-Rivera, José L. and Giray, Tugrul and Mégret, Rémi},
  doi          = {10.3389/fcomp.2021.769338},
  journal      = {Frontiers in Computer Science},
  month        = {2},
  pages        = {769338},
  shortjournal = {Front. Comput. Sci.},
  title        = {Automated video monitoring of unmarked and marked honey bees at the hive entrance},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit estimation of paragraph relevance from eye
movements. <em>FCOMP</em>, <em>3</em>, 808507. (<a
href="https://doi.org/10.3389/fcomp.2021.808507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye movements were shown to be an effective source of implicit relevance feedback in constrained search and decision-making tasks. Recent research suggests that gaze-based features, extracted from scanpaths over short news articles (g-REL), can reveal the perceived relevance of read text with respect to a previously shown trigger question. In this work, we aim to confirm this finding and we investigate whether it generalizes to multi-paragraph documents from Wikipedia (Google Natural Questions) that require readers to scroll down to read the whole text. We conduct a user study (n = 24) in which participants read single- and multi-paragraph articles and rate their relevance at the paragraph level with respect to a trigger question. We model the perceived document relevance using machine learning and features from the literature as input. Our results confirm that eye movements can be used to effectively model the relevance of short news articles, in particular if we exclude difficult cases: documents which are on topic of the trigger questions but irrelevant. However, our results do not clearly show that the modeling approach generalizes to multi-paragraph document settings. We publish our dataset and our code for feature extraction under an open source license to enable future research in the field of gaze-based implicit relevance feedback.},
  archive      = {J_FCOMP},
  author       = {Barz, Michael and Bhatti, Omair Shahzad and Sonntag, Daniel},
  doi          = {10.3389/fcomp.2021.808507},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {808507},
  shortjournal = {Front. Comput. Sci.},
  title        = {Implicit estimation of paragraph relevance from eye movements},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: Urban play and the playable city: A critical
perspective. <em>FCOMP</em>, <em>3</em>, 806494. (<a
href="https://doi.org/10.3389/fcomp.2021.806494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FCOMP},
  author       = {Chisik, Yoram and Nijholt, Anton and Schouten, Ben and Thibault, Mattia},
  doi          = {10.3389/fcomp.2021.806494},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {806494},
  shortjournal = {Front. Comput. Sci.},
  title        = {Editorial: urban play and the playable city: a critical perspective},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). STEP-UP: Enabling low-cost IMU sensors to predict the type
of dementia during everyday stair climbing. <em>FCOMP</em>, <em>3</em>,
804917. (<a href="https://doi.org/10.3389/fcomp.2021.804917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posterior Cortical Atrophy is a rare but significant form of dementia which affects people&#39;s visual ability before their memory. This is often misdiagnosed as an eyesight rather than brain sight problem. This paper aims to address the frequent, initial misdiagnosis of this disease as a vision problem through the use of an intelligent, cost-effective, wearable system, alongside diagnosis of the more typical Alzheimer&#39;s Disease. We propose low-level features constructed from the IMU data gathered from 35 participants, while they performed a stair climbing and descending task in a real-world simulated environment. We demonstrate that with these features the machine learning models predict dementia with 87.02% accuracy. Furthermore, we investigate how system parameters, such as number of sensors, affect the prediction accuracy. This lays the groundwork for a simple clinical test to enable detection of dementia which can be carried out in the wild.},
  archive      = {J_FCOMP},
  author       = {Holloway, Catherine and Bhot, William and Yong, Keir X. X. and McCarthy, Ian and Suzuki, Tatsuto and Carton, Amelia and Yang, Biao and Serougne, Robin and Boampong, Derrick and Tyler, Nick and Crutch, Sebastian J. and Berthouze, Nadia and Cho, Youngjun},
  doi          = {10.3389/fcomp.2021.804917},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {804917},
  shortjournal = {Front. Comput. Sci.},
  title        = {STEP-UP: Enabling low-cost IMU sensors to predict the type of dementia during everyday stair climbing},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How older adults learn ICT—guided and self-regulated
learning in individuals with and without disabilities. <em>FCOMP</em>,
<em>3</em>, 803740. (<a
href="https://doi.org/10.3389/fcomp.2021.803740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to use information and communication technologies (ICT) may be more difficult for older people due to decreases in fluid intelligence, generational effects, and other age-related effects. Especially older people with intellectual disabilities (ID) are at a high risk of digital exclusion. To enable all older adults to use ICT, individualized technology training may be provided. However, little is known about the ICT learning preferences among older people with ID. Based on semi-structured interviews with older adults (n = 7, mean age = 76.6 years) and older adults with ID (n = 14, mean age = 62.4 years), this paper analyzes learning strategies, preferences, and learning settings. The results from content analysis show that guided learning with personal explanations in a one-to-one setting is the most preferred learning format in both groups of older adults. While many older adults without ID additionally favor self-regulated learning (i.e., learning with manuals or videos), older adults with ID mostly rely on guided learning with personal assistance. The differences can be explained by different abilities (e.g., reading skills) and social networks (e.g., living situation, having children). Not all older adults have a family or an institutional support network to help them learn ICT and community organizations may provide additional support. Researchers and practitioners should be aware of the diverse knowledge backgrounds and competencies in the group of older adults. ICT training in old age should be ideally composed in a modular way embedding self-regulated learning formats into guided learning modules.},
  archive      = {J_FCOMP},
  author       = {Schlomann, Anna and Even, Christiane and Hammann, Torsten},
  doi          = {10.3389/fcomp.2021.803740},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {803740},
  shortjournal = {Front. Comput. Sci.},
  title        = {How older adults learn ICT—Guided and self-regulated learning in individuals with and without disabilities},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ZELDA: A 3D image segmentation and parent-child relation
plugin for microscopy image analysis in napari. <em>FCOMP</em>,
<em>3</em>, 796117. (<a
href="https://doi.org/10.3389/fcomp.2021.796117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bioimage analysis workflows allow the measurement of sample properties such as fluorescence intensity and polarization, cell number, and vesicles distribution, but often require the integration of multiple software tools. Furthermore, it is increasingly appreciated that to overcome the limitations of the 2D-view-based image analysis approaches and to correctly understand and interpret biological processes, a 3D segmentation of microscopy data sets becomes imperative. Despite the availability of numerous algorithms for the 2D and 3D segmentation, the latter still offers some challenges for the end-users, who often do not have either an extensive knowledge of the existing software or coding skills to link the output of multiple tools. While several commercial packages are available on the market, fewer are the open-source solutions able to execute a complete 3D analysis workflow. Here we present ZELDA, a new napari plugin that easily integrates the cutting-edge solutions offered by python ecosystem, such as scikit-image for image segmentation, matplotlib for data visualization, and napari multi-dimensional image viewer for 3D rendering. This plugin aims to provide interactive and zero-scripting customizable workflows for cell segmentation, vesicles counting, parent-child relation between objects, signal quantification, and results presentation; all included in the same open-source napari viewer, and “few clicks away”.},
  archive      = {J_FCOMP},
  author       = {D’Antuono, Rocco and Pisignano, Giuseppina},
  doi          = {10.3389/fcomp.2021.796117},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {796117},
  shortjournal = {Front. Comput. Sci.},
  title        = {ZELDA: A 3D image segmentation and parent-child relation plugin for microscopy image analysis in napari},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communicating photograph content through tactile images to
people with visual impairments. <em>FCOMP</em>, <em>3</em>, 787735. (<a
href="https://doi.org/10.3389/fcomp.2021.787735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of people with a visual impairment across the world are denied access to visual images. They are unable to enjoy the simple pleasures of viewing family photographs, those in textbooks or tourist brochures and the pictorial embellishment of news stories etc. We propose a simple, inexpensive but effective approach, to make content accessible via touch. We use state-of-the-art algorithms to automatically process an input photograph into a collage of icons, that depict the most important semantic aspects of a scene. This collage is then printed onto swell paper. Our experiments show that people can recognise content with an accuracy exceeding 70% and create plausible narratives to explain it. This means that people can understand image content via touch. Communicating scene foreground is a step forward, but there are many other steps needed to provide the visually impaired with the fullest possible access to visual content.},
  archive      = {J_FCOMP},
  author       = {Pakėnaitė, Karolina and Nedelev, Petar and Kamperou, Eirini and Proulx, Michael J. and Hall, Peter M.},
  doi          = {10.3389/fcomp.2021.787735},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {787735},
  shortjournal = {Front. Comput. Sci.},
  title        = {Communicating photograph content through tactile images to people with visual impairments},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An open-source whole slide image registration workflow at
cellular precision using fiji, QuPath and elastix. <em>FCOMP</em>,
<em>3</em>, 780026. (<a
href="https://doi.org/10.3389/fcomp.2021.780026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image analysis workflows for Histology increasingly require the correlation and combination of measurements across several whole slide images. Indeed, for multiplexing, as well as multimodal imaging, it is indispensable that the same sample is imaged multiple times, either through various systems for multimodal imaging, or using the same system but throughout rounds of sample manipulation (e.g. multiple staining sessions). In both cases slight deformations from one image to another are unavoidable, leading to an imperfect superimposition Redundant and thus a loss of accuracy making it difficult to link measurements, in particular at the cellular level. Using pre-existing software components and developing missing ones, we propose a user-friendly workflow which facilitates the nonlinear registration of whole slide images in order to reach sub-cellular resolution level. The set of whole slide images to register and analyze is at first defined as a QuPath project. Fiji is then used to open the QuPath project and perform the registrations. Each registration is automated by using an elastix backend, or semi-automated by using BigWarp in order to interactively correct the results of the automated registration. These transformations can then be retrieved in QuPath to transfer any regions of interest from an image to the corresponding registered images. In addition, the transformations can be applied in QuPath to produce on-the-fly transformed images that can be displayed on top of the reference image. Thus, relevant data can be combined and analyzed throughout all registered slides, facilitating the analysis of correlative results for multiplexed and multimodal imaging.},
  archive      = {J_FCOMP},
  author       = {Chiaruttini, Nicolas and Burri, Olivier and Haub, Peter and Guiet, Romain and Sordet-Dessimoz, Jessica and Seitz, Arne},
  doi          = {10.3389/fcomp.2021.780026},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {780026},
  shortjournal = {Front. Comput. Sci.},
  title        = {An open-source whole slide image registration workflow at cellular precision using fiji, QuPath and elastix},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AI applications and regulation: Mapping the regulatory
strata. <em>FCOMP</em>, <em>3</em>, 779957. (<a
href="https://doi.org/10.3389/fcomp.2021.779957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many accounts suggest that artificial intelligence (AI) law is still in its infancy with few statutes and other regulatory instruments regulating AI development and use. In this paper, we argue that such accounts are misguided. AI applications exist in a rich regulatory landscape, subject to multiple rules. To demonstrate our claim, we conduct two semi-fictional case studies under Finnish law. In the first case study, we chart the rules that currently would govern and impact AI tool use in recruitment. In the second case study, we map the legal framework for the Finnish COVID-19 contact tracing app. The article makes three contributions to the literature. First, the case studies provide ample evidence that the prevailing orthodoxy misstates the state of AI law. There is AI law on the books and existing laws have a profound impact on AI application design. Second, the mappings provide building material for developing a grounded theory framework for categorizing AI law and its types and modalities, allowing us to formulate a heuristic for understanding AI regulation. We argue that developers and AI application stakeholders should construe AI law as a complex stratigraphy consisting of five layers: data rules that regulate data use, application-specific AI rules that target specific AI applications or application domains, general AI rules that apply to a wide range of AI applications, application-specific non-AI rules that apply to specific activities but not to AI specifically and general non-AI rules that apply generically and across domains. Third, we provide guidance for practitioners for structuring AI compliance processes. We argue that practitioners should keep in mind that the rules and standards differ in their scopes, targets, certainty, and regulatory modalities. Consequently, understanding the AI regulatory landscape requires developing an understanding of multiple rule complexes, their dynamics, and regulatory modalities.},
  archive      = {J_FCOMP},
  author       = {Viljanen, Mika and Parviainen, Henni},
  doi          = {10.3389/fcomp.2021.779957},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {779957},
  shortjournal = {Front. Comput. Sci.},
  title        = {AI applications and regulation: Mapping the regulatory strata},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relevant physiological indicators for assessing workload in
conditionally automated driving, through three-class classification and
regression. <em>FCOMP</em>, <em>3</em>, 775282. (<a
href="https://doi.org/10.3389/fcomp.2021.775282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In future conditionally automated driving, drivers may be asked to take over control of the car while it is driving autonomously. Performing a non-driving-related task could degrade their takeover performance, which could be detected by continuous assessment of drivers&#39; mental load. In this regard, three physiological signals from 80 subjects were collected during 1 h of conditionally automated driving in a simulator. Participants were asked to perform a non-driving cognitive task (N-back) for 90 s, 15 times during driving. The modality and difficulty of the task were experimentally manipulated. The experiment yielded a dataset of drivers&#39; physiological indicators during the task sequences, which was used to predict drivers&#39; workload. This was done by classifying task difficulty (three classes) and regressing participants&#39; reported level of subjective workload after each task (on a 0–20 scale). Classification of task modality was also studied. For each task, the effect of sensor fusion and task performance were studied. The implemented pipeline consisted of a repeated cross validation approach with grid search applied to three machine learning algorithms. The results showed that three different levels of mental load could be classified with a f1-score of 0.713 using the skin conductance and respiration signals as inputs of a random forest classifier. The best regression model predicted the subjective level of workload with a mean absolute error of 3.195 using the three signals. The accuracy of the model increased with participants&#39; task performance. However, classification of task modality (visual or auditory) was not successful. Some physiological indicators such as estimates of respiratory sinus arrhythmia, respiratory amplitude, and temporal indices of heart rate variability were found to be relevant measures of mental workload. Their use should be preferred for ongoing assessment of driver workload in automated driving.},
  archive      = {J_FCOMP},
  author       = {Meteier, Quentin and De Salis, Emmanuel and Capallera, Marine and Widmer, Marino and Angelini, Leonardo and Abou Khaled, Omar and Sonderegger, Andreas and Mugellini, Elena},
  doi          = {10.3389/fcomp.2021.775282},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {775282},
  shortjournal = {Front. Comput. Sci.},
  title        = {Relevant physiological indicators for assessing workload in conditionally automated driving, through three-class classification and regression},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting activation liking of people with dementia.
<em>FCOMP</em>, <em>3</em>, 770492. (<a
href="https://doi.org/10.3389/fcomp.2021.770492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical, social and cognitive activation is an important cornerstone in non-pharmacological therapy for People with Dementia (PwD). To support long-term motivation and well-being, activation contents first need to be perceived positively. Prompting for explicit feedback, however, is intrusive and interrupts the activation flow. Automated analyses of verbal and non-verbal signals could provide an unobtrusive means of recommending suitable contents based on implicit feedback. In this study, we investigate the correlation between engagement responses and self-reported activation ratings. Subsequently, we predict ratings of PwD based on verbal and non-verbal signals in an unconstrained care setting. Applying Long-Short-Term-Memory (LSTM) networks, we can show that our classifier outperforms chance level. We further investigate which features are the most promising indicators for the prediction of activation ratings of PwD.},
  archive      = {J_FCOMP},
  author       = {Steinert, Lars and Putze, Felix and Küster, Dennis and Schultz, Tanja},
  doi          = {10.3389/fcomp.2021.770492},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {770492},
  shortjournal = {Front. Comput. Sci.},
  title        = {Predicting activation liking of people with dementia},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal user feedback during adaptive robot-human
presentations. <em>FCOMP</em>, <em>3</em>, 741148. (<a
href="https://doi.org/10.3389/fcomp.2021.741148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feedback is an essential part of all communication, and agents communicating with humans must be able to both give and receive feedback in order to ensure mutual understanding. In this paper, we analyse multimodal feedback given by humans towards a robot that is presenting a piece of art in a shared environment, similar to a museum setting. The data analysed contains both video and audio recordings of 28 participants, and the data has been richly annotated both in terms of multimodal cues (speech, gaze, head gestures, facial expressions, and body pose), as well as the polarity of any feedback (negative, positive, or neutral). We train statistical and machine learning models on the dataset, and find that random forest models and multinomial regression models perform well on predicting the polarity of the participants&#39; reactions. An analysis of the different modalities shows that most information is found in the participants&#39; speech and head gestures, while much less information is found in their facial expressions, body pose and gaze. An analysis of the timing of the feedback shows that most feedback is given when the robot makes pauses (and thereby invites feedback), but that the more exact timing of the feedback does not affect its meaning.},
  archive      = {J_FCOMP},
  author       = {Axelsson, Agnes and Skantze, Gabriel},
  doi          = {10.3389/fcomp.2021.741148},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {741148},
  shortjournal = {Front. Comput. Sci.},
  title        = {Multimodal user feedback during adaptive robot-human presentations},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bio-inspired multimodal imaging in reduced visibility.
<em>FCOMP</em>, <em>3</em>, 737144. (<a
href="https://doi.org/10.3389/fcomp.2021.737144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual systems found in nature rely on capturing light under different modalities, in terms of spectral sensitivities and polarization sensitivities. Numerous imaging techniques are inspired by this variety, among which, the most famous is color imaging inspired by the trichromacy theory of the human visual system. We investigate the spectral and polarimetric properties of biological imaging systems that will lead to the best performance on scene imaging through haze, i.e., dehazing. We design a benchmark experiment based on modalities inspired by several visual systems, and adapt state-of-the-art image reconstruction algorithms to those modalities. We show the difference in performance of each studied systems and discuss it in front of our methodology and the statistical relevance of our data.},
  archive      = {J_FCOMP},
  author       = {Lapray, Pierre-Jean and Thomas, Jean-Baptiste and Farup, Ivar},
  doi          = {10.3389/fcomp.2021.737144},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {737144},
  shortjournal = {Front. Comput. Sci.},
  title        = {Bio-inspired multimodal imaging in reduced visibility},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Eye movement and pupil measures: A review. <em>FCOMP</em>,
<em>3</em>, 733531. (<a
href="https://doi.org/10.3389/fcomp.2021.733531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our subjective visual experiences involve complex interaction between our eyes, our brain, and the surrounding world. It gives us the sense of sight, color, stereopsis, distance, pattern recognition, motor coordination, and more. The increasing ubiquity of gaze-aware technology brings with it the ability to track gaze and pupil measures with varying degrees of fidelity. With this in mind, a review that considers the various gaze measures becomes increasingly relevant, especially considering our ability to make sense of these signals given different spatio-temporal sampling capacities. In this paper, we selectively review prior work on eye movements and pupil measures. We first describe the main oculomotor events studied in the literature, and their characteristics exploited by different measures. Next, we review various eye movement and pupil measures from prior literature. Finally, we discuss our observations based on applications of these measures, the benefits and practical challenges involving these measures, and our recommendations on future eye-tracking research directions.},
  archive      = {J_FCOMP},
  author       = {Mahanama, Bhanuka and Jayawardana, Yasith and Rengarajan, Sundararaman and Jayawardena, Gavindya and Chukoskie, Leanne and Snider, Joseph and Jayarathna, Sampath},
  doi          = {10.3389/fcomp.2021.733531},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {733531},
  shortjournal = {Front. Comput. Sci.},
  title        = {Eye movement and pupil measures: A review},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Quantum-inspired uncertainty quantification. <em>FCOMP</em>,
<em>3</em>, 662632. (<a
href="https://doi.org/10.3389/fcomp.2021.662632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasonable quantification of uncertainty is a major issue of cognitive infocommunications, and logic is a backbone for successful communication. Here, an axiomatic approach to quantum logic, which highlights similarity to and differences to classical logic, is presented. The axiomatic method ensures that applications are not restricted to quantum physics. Based on this, algorithms are developed that assign to an incoming signal a similarity measure to a pattern generated by a set of training signals.},
  archive      = {J_FCOMP},
  author       = {Wirsching, Günther},
  doi          = {10.3389/fcomp.2021.662632},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {662632},
  shortjournal = {Front. Comput. Sci.},
  title        = {Quantum-inspired uncertainty quantification},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detecting fear of heights response to a virtual reality
environment using functional near-infrared spectroscopy. <em>FCOMP</em>,
<em>3</em>, 652550. (<a
href="https://doi.org/10.3389/fcomp.2021.652550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable virtual reality exposure therapy (VRET) that treats anxiety disorders by gradually exposing the patient to fear using virtual reality (VR), it is important to monitor the patient&#39;s fear levels during the exposure. Despite the evidence of a fear circuit in the brain as reflected by functional near-infrared spectroscopy (fNIRS), the measurement of fear response in highly immersive VR using fNIRS is limited, especially in combination with a head-mounted display (HMD). In particular, it is unclear to what extent fNIRS can differentiate users with and without anxiety disorders and detect fear response in a highly ecological setting using an HMD. In this study, we investigated fNIRS signals captured from participants with and without a fear of height response. To examine the extent to which fNIRS signals of both groups differ, we conducted an experiment during which participants with moderate fear of heights and participants without it were exposed to VR scenarios involving heights and no heights. The between-group statistical analysis shows that the fNIRS data of the control group and the experimental group are significantly different only in the channel located close to right frontotemporal lobe, where the grand average oxygenated hemoglobin Δ[HbO] contrast signal of the experimental group exceeds that of the control group. The within-group statistical analysis shows significant differences between the grand average Δ[HbO] contrast values during fear responses and those during no-fear responses, where the Δ[HbO] contrast values of the fear responses were significantly higher than those of the no-fear responses in the channels located towards the frontal part of the prefrontal cortex. Also, the channel located close to frontocentral lobe was found to show significant difference for the grand average deoxygenated hemoglobin contrast signals. Support vector machine-based classifier could detect fear responses at an accuracy up to 70% and 74% in subject-dependent and subject-independent classifications, respectively. The results demonstrate that cortical hemodynamic responses of a control group and an experimental group are different to a considerable extent, exhibiting the feasibility and ecological validity of the combination of VR-HMD and fNIRS to elicit and detect fear responses. This research thus paves a way toward the a brain-computer interface to effectively manipulate and control VRET.},
  archive      = {J_FCOMP},
  author       = {de With, Luciënne A. and Thammasan, Nattapong and Poel, Mannes},
  doi          = {10.3389/fcomp.2021.652550},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {652550},
  shortjournal = {Front. Comput. Sci.},
  title        = {Detecting fear of heights response to a virtual reality environment using functional near-infrared spectroscopy},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data checkers: A grid-based UI for managing
patient-generated data sharing to support collaborative self-care.
<em>FCOMP</em>, <em>3</em>, 639748. (<a
href="https://doi.org/10.3389/fcomp.2021.639748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic health conditions are becoming increasingly prevalent. As part of chronic care, sharing patient-generated health data (PGHD) is likely to play a prominent role. Sharing PGHD is increasingly recognized as potentially useful for not only monitoring health conditions but for informing and supporting collaboration with caregivers and healthcare providers. In this paper, we describe a new design for the fine-grained control over sharing one&#39;s PGHD to support collaborative self-care, one that centers on giving people with health conditions control over their own data. The system, Data Checkers (DC), uses a grid-based interface and a preview feature to provide users with the ability to control data access and dissemination. DC is of particular use in the case of severe chronic conditions, such as spinal cord injuries and disorders (SCI/D), that require not just intermittent involvement of healthcare providers but daily support and assistance from caregivers. In this paper, after providing relevant background information, we articulate our steps for developing this innovative system for sharing PGHD including (a) use of a co-design process; (b) identification of design requirements; and (c) creation of the DC System. We then present a qualitative evaluation of DC to show how DC satisfied these design requirements in a way that provided advantages for care. Our work extends existing research in the areas of Human-Computer Interaction (HCI), Computer-Supported Cooperative Work (CSCW), Ubiquitous Computing (Ubicomp), and Health Informatics about sharing data and PGHD.},
  archive      = {J_FCOMP},
  author       = {Hung, Pei-Yao and Canada, Drew and Meade, Michelle A. and Ackerman, Mark S.},
  doi          = {10.3389/fcomp.2021.639748},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {639748},
  shortjournal = {Front. Comput. Sci.},
  title        = {Data checkers: A grid-based UI for managing patient-generated data sharing to support collaborative self-care},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A universal screening tool for dyslexia by a web-game and
machine learning. <em>FCOMP</em>, <em>3</em>, 628634. (<a
href="https://doi.org/10.3389/fcomp.2021.628634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children with dyslexia have difficulties learning how to read and write. They are often diagnosed after they fail school even if dyslexia is not related to general intelligence. Early screening of dyslexia can prevent the negative side effects of late detection and enables early intervention. In this context, we present an approach for universal screening of dyslexia using machine learning models with data gathered from a web-based language-independent game. We designed the game content taking into consideration the analysis of mistakes of people with dyslexia in different languages and other parameters related to dyslexia like auditory perception as well as visual perception. We did a user study with 313 children (116 with dyslexia) and train predictive machine learning models with the collected data. Our method yields an accuracy of 0.74 for German and 0.69 for Spanish as well as a F1-score of 0.75 for German and 0.75 for Spanish, using Random Forests and Extra Trees, respectively. We also present the game content design, potential new auditory input, and knowledge about the design approach for future research to explore Universal screening of dyslexia. universal screening with language-independent content can be used for the screening of pre-readers who do not have any language skills, facilitating a potential early intervention.},
  archive      = {J_FCOMP},
  author       = {Rauschenberger, Maria and Baeza-Yates, Ricardo and Rello, Luz},
  doi          = {10.3389/fcomp.2021.628634},
  journal      = {Frontiers in Computer Science},
  month        = {1},
  pages        = {628634},
  shortjournal = {Front. Comput. Sci.},
  title        = {A universal screening tool for dyslexia by a web-game and machine learning},
  volume       = {3},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
