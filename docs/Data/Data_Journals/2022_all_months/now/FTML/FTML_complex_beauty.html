<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FTML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ftml---4">FTML - 4</h2>
<ul>
<li><details>
<summary>
(2022). Divided differences, falling factorials, and discrete
splines: Another look at trend filtering and related problems.
<em>FTML</em>, <em>15</em>(6), 694–846. (<a
href="https://doi.org/10.1561/2200000099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This monograph reviews a class of univariate piecewise polynomial functions known as , which share properties analogous to the better-known class of spline functions, but where continuity in derivatives is replaced by (a suitable notion of) continuity in . As it happens, discrete splines bear connections to a wide array of developments in applied mathematics and statistics, from divided differences and Newton interpolation (dating back to over 300 years ago) to trend filtering (from the last 15 years). We survey these connections, and contribute some new perspectives and new results along the way.},
  archive      = {J_FTML},
  author       = {Ryan J. Tibshirani},
  doi          = {10.1561/2200000099},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {6},
  pages        = {694-846},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Divided differences, falling factorials, and discrete splines: Another look at trend filtering and related problems},
  volume       = {15},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Risk-sensitive reinforcement learning via policy gradient
search. <em>FTML</em>, <em>15</em>(5), 537–693. (<a
href="https://doi.org/10.1561/2200000091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective in a traditional reinforcement learning (RL) problem is to find a policy that optimizes the expected value of a performance metric such as the infinite-horizon cumulative discounted or long-run average cost/reward. In practice, optimizing the expected value alone may not be satisfactory, in that it may be desirable to incorporate the notion of risk into the optimization problem formulation, either in the objective or as a constraint. Various risk measures have been proposed in the literature, e.g., exponential utility, variance, percentile performance, chance constraints, value at risk (quantile), conditional value-at-risk, prospect theory and its later enhancement, cumulative prospect theory.In this monograph, we consider risk-sensitive RL in two settings: one where the goal is to find a policy that optimizes the usual expected value objective while ensuring that a risk constraint is satisfied, and the other where the risk measure is the objective. We survey some of the recent work in this area specifically where policy gradient search is the solution approach. In the first risk-sensitive RL setting, we cover popular risk measures based on variance, conditional valueat- risk, and chance constraints, and present a template for policy gradient-based risk-sensitive RL algorithms using a Lagrangian formulation. For the setting where risk is incorporated directly into the objective function, we consider an exponential utility formulation, cumulative prospect theory, and coherent risk measures. This non-exhaustive survey aims to give a flavor of the challenges involved in solving risk-sensitive RL problems using policy gradient methods, as well as outlining some potential future research directions.},
  archive      = {J_FTML},
  author       = {Prashanth L. A. and Michael C. Fu},
  doi          = {10.1561/2200000091},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {5},
  pages        = {537-693},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Risk-sensitive reinforcement learning via policy gradient search},
  volume       = {15},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A unifying tutorial on approximate message passing.
<em>FTML</em>, <em>15</em>(4), 335–536. (<a
href="https://doi.org/10.1561/2200000092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade or so, Approximate Message Passing (AMP) algorithms have become extremely popular in various structured high-dimensional statistical problems. Although the origins of these techniques can be traced back to notions of belief propagation in the statistical physics literature, our goals in this work are to present the main ideas of AMP from a statistical perspective and to illustrate the power and flexibility of the AMP framework. Along the way, we strengthen and unify many of the results in the existing literature.},
  archive      = {J_FTML},
  author       = {Oliver Y. Feng and Ramji Venkataramanan and Cynthia Rush and Richard J. Samworth},
  doi          = {10.1561/2200000092},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {4},
  pages        = {335-536},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {A unifying tutorial on approximate message passing},
  volume       = {15},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning in repeated auctions. <em>FTML</em>,
<em>15</em>(3), 176–334. (<a
href="https://doi.org/10.1561/2200000077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online auctions are one of the most fundamental facets of the modern economy and power an industry generating hundreds of billions of dollars a year in revenue. Auction theory has historically focused on the question of designing the best way to sell a single item to potential buyers, with the concurrent objectives of maximizing revenue generated or welfare created. Theoretical results in this area have typically relied on some prior Bayesian knowledge agents were assumed to have on each other. This assumption is no longer satisfied in new markets such as online advertising: similar items are sold repeatedly, and agents are unaware of each other or might try to manipulate each other. On the other hand, statistical learning theory now provides tools to supplement those missing pieces of information given enough data, as agents can learn from their environment to improve their strategies.This monograph covers recent advances in learning in repeated auctions, starting from the traditional economic study of optimal one-shot auctions with a Bayesian prior. We then focus on the question of learning optimal mechanisms from a dataset of bidders’ past values. The sample complexity as well as the computational efficiency of different methods will be studied. We will also investigate online variants where gathering data has a cost to be accounted for, either by sellers or buyers (“earning while learning”). Later in the monograph, we will further assume that bidders are also adaptive to the mechanism as they interact repeatedly with the same seller. We will show how strategic agents can actually manipulate repeated auctions, to their own advantage. A particularly interesting example is that of reserve price improvements for strategic buyers in second price auctions.All the questions discussed in this monograph are grounded in real-world applications and many of the ideas and algorithms we describe are used every day to power the Internet economy.},
  archive      = {J_FTML},
  author       = {Thomas Nedelec and Clément Calauzènes and Noureddine El Karoui and Vianney Perchet},
  doi          = {10.1561/2200000077},
  journal      = {Foundations and Trends® in Machine Learning},
  number       = {3},
  pages        = {176-334},
  shortjournal = {Found. Trends Mach. Learn.},
  title        = {Learning in repeated auctions},
  volume       = {15},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
