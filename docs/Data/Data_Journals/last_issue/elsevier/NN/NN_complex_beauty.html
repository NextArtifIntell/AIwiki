<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nn---93">NN - 93</h2>
<ul>
<li><details>
<summary>
(2025). A novel self-supervised graph clustering method with
reliable semi-supervision. <em>NN</em>, <em>187</em>, 107418. (<a
href="https://doi.org/10.1016/j.neunet.2025.107418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis, as a core technique in unsupervised learning, has widespread applications. With the increasing complexity of data, deep clustering, which integrates the advantages of deep learning and traditional clustering algorithms, demonstrates outstanding performance in processing high-dimensional and complex data. However, when applied to graph data, deep clustering faces two major challenges: noise and sparsity. Noise introduces misleading connections, while sparsity makes it difficult to accurately capture relationships between nodes. These two issues not only increase the difficulty of feature extraction but also significantly affect clustering performance. To address these problems, we propose a novel Self-Supervised Graph Clustering model based on Reliable Semi-Supervision (SSGC-RSS). This model innovates through upstream and downstream components. The upstream component employs a dual-decoder graph autoencoder with joint clustering optimization, preserving latent information of features and graph structure, and alleviates the sparsity problem by generating cluster centers and pseudo-labels. The downstream component utilizes a semi-supervised graph attention encoding network based on highly reliable samples and their pseudo-labels to select reliable samples for training, thereby effectively reducing the interference of noise. Experimental results on multiple graph datasets demonstrate that, compared to existing methods, SSGC-RSS achieves significant performance improvements, with accuracy improvements of 0.9%, 2.0%, and 5.6% on Cora, Citeseer, and Pubmed datasets respectively, proving its effectiveness and superiority in complex graph data clustering tasks.},
  archive      = {J_NN},
  author       = {Weijia Lu and Min Wang and Yun Yu and Liang Ma and Yaxiang Shi and Zhongqiu Huang and Ming Gong},
  doi          = {10.1016/j.neunet.2025.107418},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107418},
  shortjournal = {Neural Netw.},
  title        = {A novel self-supervised graph clustering method with reliable semi-supervision},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LUNETR: Language-infused UNETR for precise pancreatic tumor
segmentation in 3D medical image. <em>NN</em>, <em>187</em>, 107414. (<a
href="https://doi.org/10.1016/j.neunet.2025.107414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of early micro-lesions and adjacent blood vessels in CT scans plays a pivotal role in the clinical diagnosis of pancreatic cancer, considering its aggressive nature and high fatality rate. Despite the widespread application of deep learning methods for this task, several challenges persist: (1) the complex background environment in abdominal CT scans complicates the accurate localization of potential micro-tumors; (2) the subtle contrast between micro-lesions within pancreatic tissue and the surrounding tissues makes it challenging for models to capture these features accurately; and (3) tumors that invade adjacent blood vessels pose significant barriers to surgical procedures. To address these challenges, we propose LUNETR (Language-Infused UNETR), an advanced multimodal encoder model that combines textual and image information for precise medical image segmentation. The integration of an autoencoding language model with cross-attention enabling our model to effectively leverage semantic associations between textual and image data, thereby facilitating precise localization of potential pancreatic micro-tumors. Additionally, we designed a Multi-scale Aggregation Attention (MSAA) module to comprehensively capture both spatial and channel characteristics of global multi-scale image data, enhancing the model&#39;s capacity to extract features from micro-lesions embedded within pancreatic tissue. Furthermore, in order to facilitate precise segmentation of pancreatic tumors and nearby blood vessels and address the scarcity of multimodal medical datasets, we collaborated with Zhuzhou Central Hospital to construct a multimodal dataset comprising CT images and corresponding pathology reports from 135 pancreatic cancer patients. Our experimental results surpass current state-of-the-art models, with the incorporation of the semantic encoder improving the average Dice score for pancreatic tumor segmentation by 2.23 %. For the Medical Segmentation Decathlon (MSD) liver and lung cancer datasets, our model achieved an average Dice score improvement of 4.31 % and 3.67 %, respectively, demonstrating the efficacy of the LUNETR.},
  archive      = {J_NN},
  author       = {Ziyang Shi and Ruopeng Zhang and Xiajun Wei and Cheng Yu and Haojie Xie and Zhen Hu and Xili Chen and Yongzhong Zhang and Bin Xie and Zhengmao Luo and Wanxiang Peng and Xiaochun Xie and Fang Li and Xiaoli Long and Lin Li and Linan Hu},
  doi          = {10.1016/j.neunet.2025.107414},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107414},
  shortjournal = {Neural Netw.},
  title        = {LUNETR: Language-infused UNETR for precise pancreatic tumor segmentation in 3D medical image},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-resistant predefined-time convergent ZNN models for
dynamic least squares and multi-agent systems. <em>NN</em>,
<em>187</em>, 107412. (<a
href="https://doi.org/10.1016/j.neunet.2025.107412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zeroing neural networks (ZNNs) are commonly used for dynamic matrix equations, but their performance under numerically unstable conditions has not been thoroughly explored, especially in situations involving unequal row-column matrices. The challenge is further aggravated by noise, particularly in dynamic least squares (DLS) problems. To address these issues, we propose the QR decomposition-driven noise-resistant ZNN (QRDN-ZNN) model, specifically designed for DLS problems. By integrating QR decomposition into the ZNN framework, QRDN-ZNN enhances numerical stability and guarantees both precise and rapid convergence through a novel activation function (N-Af). As validated by theoretical analysis and experiments, the model can effectively counter disturbances and enhance solution accuracy in dynamic environments. Experimental results show that, in terms of noise resistance, the QRDN-ZNN model outperforms existing mainstream ZNN models, including the original ZNN, integral-enhanced ZNN, double-integral enhanced ZNN, and super-twisting ZNN. Furthermore, the N-Af offers higher accuracy and faster convergence than other state-of-the-art activation functions. To demonstrate the practical utility of the method, We develop a new noise-resistant consensus protocol inspired by QRDN-ZNN, which enables multi-agent systems to reach consensus even in noisy conditions.},
  archive      = {J_NN},
  author       = {Yiwei Li and Jiaxin Liu and Lei Jia and Liangze Yin and Xingpei Li and Yong Zhang},
  doi          = {10.1016/j.neunet.2025.107412},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107412},
  shortjournal = {Neural Netw.},
  title        = {Noise-resistant predefined-time convergent ZNN models for dynamic least squares and multi-agent systems},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Restarted multiple kernel algorithms with self-guiding for
large-scale multi-view clustering. <em>NN</em>, <em>187</em>, 107409.
(<a href="https://doi.org/10.1016/j.neunet.2025.107409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering is a powerful approach for discovering underlying structures hidden behind diverse views of datasets. Most existing multi-view spectral clustering methods use fixed similarity matrices or alternately updated ones. However, the former often fall short in adaptively capturing relationships among different views, while the latter are often time-consuming and even impractical for large-scale datasets. To the best of our knowledge, there are no multi-view spectral clustering methods can both construct multi-view similarity matrices inexpensively and preserve the valuable clustering insights from previous cycles at the same time. To fill in this gap, we present a Sum-Ratio Multi-view Ncut model that share a common representation embedding for multi-view data. Based on this model, we propose a restarted multi-view multiple kernel clustering framework with self-guiding. To release the overhead, we use similarity matrices with strict block diagonal representation, and present an efficient multiple kernel selection technique. Comprehensive experiments on benchmark multi-view datasets demonstrate that, even using randomly generated initial guesses, the restarted algorithms can improve the clustering performances by 5–10 times for some popular multi-view clustering methods. Specifically, our framework offers a potential boosting effect for most of the state-of-the-art multi-view clustering algorithms at very little cost, especially for those with poor performances.},
  archive      = {J_NN},
  author       = {Yongyan Guo and Gang Wu},
  doi          = {10.1016/j.neunet.2025.107409},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107409},
  shortjournal = {Neural Netw.},
  title        = {Restarted multiple kernel algorithms with self-guiding for large-scale multi-view clustering},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motif and supernode-enhanced gated graph neural networks for
session-based recommendation. <em>NN</em>, <em>187</em>, 107406. (<a
href="https://doi.org/10.1016/j.neunet.2025.107406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation systems aim to predict users’ next interactions based on short-lived, anonymous sessions, a challenging yet vital task due to the sparsity and dynamic nature of user behavior. Existing Graph Neural Network (GNN)-based methods primarily focus on the session graphs while overlooking the influence of micro-structures and user behavior patterns. To address these limitations, we propose a Motif and Supernode-Enhanced Session-based Recommender System (MSERS), which constructs a global session graph, identifies and encodes motifs as supernodes, and reintegrates them into the global graph to enrich its topology and better represent item dependencies. By employing supernode-enhanced Gated Graph Neural Networks (GGNN), MSERS captures both long-term and latent item dependencies, significantly improving session representations. Extensive experiments on two real-world datasets demonstrate the superiority of MSERS over baseline methods, providing robust insights into the role of micro-structures in session-based recommendations.},
  archive      = {J_NN},
  author       = {Ronghua Lin and Chang Liu and Hao Zhong and Chengzhe Yuan and Guohua Chen and Yuncheng Jiang and Yong Tang},
  doi          = {10.1016/j.neunet.2025.107406},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107406},
  shortjournal = {Neural Netw.},
  title        = {Motif and supernode-enhanced gated graph neural networks for session-based recommendation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical expression exploration with graph
representation and generative graph neural network. <em>NN</em>,
<em>187</em>, 107405. (<a
href="https://doi.org/10.1016/j.neunet.2025.107405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic Regression (SR) methods in tree representations have exhibited commendable outcomes across Genetic Programming (GP) and deep learning search paradigms. Nonetheless, the tree representation of mathematical expressions occasionally embodies redundant substructures. Representing expressions as computation graphs is more succinct and intuitive through graph representation. Despite its adoption in evolutionary strategies within SR, deep learning paradigms remain under-explored. Acknowledging the profound advancements of deep learning in tree-centric SR approaches, we advocate for addressing SR tasks using the Directed Acyclic Graph (DAG) representation of mathematical expressions, complemented by a generative graph neural network. We name the proposed method as Graph -based D eep S ymbolic R egression (GraphDSR) . We vectorize node types and employ an adjacent matrix to delineate connections. The graph neural networks craft the DAG incrementally, sampling node types and graph connections conditioned on previous DAG at every step. During each sample step, the valid check is implemented to avoid meaningless sampling, and four domain-agnostic constraints are adopted to further streamline the search. This process culminates once a coherent expression emerges. Constants undergo optimization by SGD and BFGS algorithms, and rewards refine the graph neural network through reinforcement learning. A comprehensive evaluation across 110 benchmarks underscores the potency of our approach.},
  archive      = {J_NN},
  author       = {Jingyi Liu and Weijun Li and Lina Yu and Min Wu and Wenqiang Li and Yanjie Li and Meilan Hao},
  doi          = {10.1016/j.neunet.2025.107405},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107405},
  shortjournal = {Neural Netw.},
  title        = {Mathematical expression exploration with graph representation and generative graph neural network},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). More signals matter to detection: Integrating language
knowledge and frequency representations for boosting fine-grained
aircraft recognition. <em>NN</em>, <em>187</em>, 107402. (<a
href="https://doi.org/10.1016/j.neunet.2025.107402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As object detection tasks progress rapidly, fine-grained detection flourishes as a promising extension. Fine-grained recognition naturally demands high-quality detail signals; however, existing fine-grained detectors, built upon the mainstream detection paradigm, struggle to simultaneously address the challenges of insufficient original signals and the loss of critical signals, resulting in inferior performance. We argue that language signals with advanced semantic knowledge can provide valuable information for fine-grained objects, as well as the frequency domain exhibits greater flexibility in suppressing and enhancing signals; then, we propose a fine-grained aircraft detector by integrating language knowledge and frequency representations into the one-stage detection paradigm. Concretely, by considering both original signals and deep feature signals, we develop three components, including an adaptive frequency augmentation branch (AFAB), a content-aware global features intensifier (CGFI), and a fine-grained text–image interactive feeder (FTIF), to facilitate perceiving and retaining critical signals throughout pivotal detection stages. The AFAB adaptively processes image patches according to their frequency characteristics in the Fourier domain, thus thoroughly mining critical visual content in the data space; the CGFI employs content-aware frequency filtering to enhance global features, allowing for generating an information-rich feature space; the FTIF introduces text knowledge to describe visual differences among fine-grained categories, conveying robust semantic priors from language signals to visual spaces via multimodal interaction for information supplement. Extensive experiments conducted on optical and SAR images demonstrate the superior performance of the proposed fine-grained detector, especially the FTIF, which can be plugged into most existing one-stage detectors to boost their fine-grained recognition performance significantly.},
  archive      = {J_NN},
  author       = {Xueru Xu and Zhong Chen and Yuxin Hu and Guoyou Wang},
  doi          = {10.1016/j.neunet.2025.107402},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107402},
  shortjournal = {Neural Netw.},
  title        = {More signals matter to detection: Integrating language knowledge and frequency representations for boosting fine-grained aircraft recognition},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep transfer learning method based on explainable
feature extraction and domain reconstruction. <em>NN</em>, <em>187</em>,
107401. (<a href="https://doi.org/10.1016/j.neunet.2025.107401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep transfer learning has made significant progress, its “black-box” nature and unstable feature adaptation remain key obstacles. This study proposes a multi-stage deep transfer learning method, called XDTL, which combines explainable feature extraction and domain reconstruction to enhance the performance of target models. Specifically, the study first divides features into key and regular features through cross-validation and explainability analysis, then reconstructs the target domain using a seed replacement method based on key target samples, ultimately achieving deep transfer. Experimental results show that, compared to other methods, XDTL achieves an average improvement of 27.43 % in effectiveness, demonstrating superior performance and stronger explainability. This method offers new insights into addressing the explainability challenges in transfer learning and highlights its potential for broader applications across various tasks.},
  archive      = {J_NN},
  author       = {Li Wang and Lucong Zhang and Ling Feng and Tianyu Chen and Hongwu Qin},
  doi          = {10.1016/j.neunet.2025.107401},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107401},
  shortjournal = {Neural Netw.},
  title        = {A novel deep transfer learning method based on explainable feature extraction and domain reconstruction},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight all-MLP time–frequency anomaly detection for
IIoT time series. <em>NN</em>, <em>187</em>, 107400. (<a
href="https://doi.org/10.1016/j.neunet.2025.107400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in the Industrial Internet of Things (IIoT) aims at identifying abnormal sensor signals to ensure industrial production safety. However, most existing models only focus on high accuracy by building a bulky neural network with deep structures and huge parameters. In this case, these models usually exhibit poor timeliness and high resource consumption, which makes these models unsuitable for resource-limited edge industrial scenarios. To solve this problem, a lightweight All-MLP time–frequency anomaly detection model is proposed for IIoT time series, namely LTFAD. Firstly , unlike traditional deep and bulky solutions, a shallow and lightweight All-MLP architecture is designed to achieve high timeliness and low resource consumption. Secondly , based on the lightweight architecture, a dual-branch network is constructed to improve model accuracy by simultaneously learning “global to local” and “local to global” reconstruction. Finally , time–frequency joint learning is employed in each reconstruction branch to further enhance accuracy. To the best of our knowledge, this is the first work to develop a time–frequency anomaly detection model based only on the shallow All-MLP architecture. Extensive experiments demonstrate that LTFAD can quickly and accurately identify anomalies on resource-limited edge devices, such as the Raspberry Pi 4b and Jetson Xavier NX. The source code for LTFAD is available at https://github.com/infogroup502/LTFAD .},
  archive      = {J_NN},
  author       = {Lei Chen and Xinzhe Cao and Tingqin He and Yepeng Xu and Xuxin Liu and Bowen hu},
  doi          = {10.1016/j.neunet.2025.107400},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107400},
  shortjournal = {Neural Netw.},
  title        = {A lightweight all-MLP time–frequency anomaly detection for IIoT time series},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposition-based multi-scale transformer framework for
time series anomaly detection. <em>NN</em>, <em>187</em>, 107399. (<a
href="https://doi.org/10.1016/j.neunet.2025.107399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series anomaly detection is crucial for maintaining stable systems. Existing methods face two main challenges. First, it is difficult to directly model the dependencies of diverse and complex patterns within the sequences. Second, many methods that optimize parameters using mean squared error struggle with noise in the time series, leading to performance deterioration. To address these challenges, we propose a transformer-based framework built on decomposition (TransDe) for multivariate time series anomaly detection. The key idea is to combine the strengths of time series decomposition and transformers to effectively learn the complex patterns in normal time series data. A multi-scale patch-based transformer architecture is proposed to exploit the representative dependencies of each decomposed component of the time series. Furthermore, a contrastive learn paradigm based on patch operation is proposed, which leverages KL divergence to align the positive pairs, namely the pure representations of normal patterns between different patch-level views. A novel asynchronous loss function with a stop-gradient strategy is further introduced to enhance the performance of TransDe effectively. It can avoid time-consuming and labor-intensive computation costs in the optimization process. Extensive experiments on five public datasets are conducted and TransDe shows superiority compared with twelve baselines in terms of F1 score. Our code is available at https://github.com/shaieesss/TransDe .},
  archive      = {J_NN},
  author       = {Wenxin Zhang and Cuicui Luo},
  doi          = {10.1016/j.neunet.2025.107399},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107399},
  shortjournal = {Neural Netw.},
  title        = {Decomposition-based multi-scale transformer framework for time series anomaly detection},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot 3D anomaly detection via online voter mechanism.
<em>NN</em>, <em>187</em>, 107398. (<a
href="https://doi.org/10.1016/j.neunet.2025.107398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D anomaly detection aims to solve the problem that image anomaly detection is greatly affected by lighting conditions. As commercial confidentiality and personal privacy become increasingly paramount, access to training samples is often restricted. To address these challenges, we propose a zero-shot 3D anomaly detection method. Unlike previous CLIP-based methods, the proposed method does not require any prompt and is capable of detecting anomalies on the depth modality. Furthermore, we also propose a pre-trained structural rerouting strategy, which modifies the transformer without retraining or fine-tuning for the anomaly detection task. Most importantly, this paper proposes an online voter mechanism that registers voters and performs majority voter scoring in a one-stage, zero-start and growth-oriented manner, enabling direct anomaly detection on unlabeled test sets. Finally, we also propose a confirmatory judge credibility assessment mechanism, which provides an efficient adaptation for possible few-shot conditions. Results on datasets such as MVTec3D-AD demonstrate that the proposed method can achieve superior zero-shot 3D anomaly detection performance, indicating its pioneering contributions within the pertinent domain.},
  archive      = {J_NN},
  author       = {Wukun Zheng and Xiao Ke and Wenzhong Guo},
  doi          = {10.1016/j.neunet.2025.107398},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107398},
  shortjournal = {Neural Netw.},
  title        = {Zero-shot 3D anomaly detection via online voter mechanism},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SympGNNs: Symplectic graph neural networks for identifying
high-dimensional hamiltonian systems and node classification.
<em>NN</em>, <em>187</em>, 107397. (<a
href="https://doi.org/10.1016/j.neunet.2025.107397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.},
  archive      = {J_NN},
  author       = {Alan John Varghese and Zhen Zhang and George Em Karniadakis},
  doi          = {10.1016/j.neunet.2025.107397},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107397},
  shortjournal = {Neural Netw.},
  title        = {SympGNNs: Symplectic graph neural networks for identifying high-dimensional hamiltonian systems and node classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expert guidance and partially-labeled data collaboration for
multi-organ segmentation. <em>NN</em>, <em>187</em>, 107396. (<a
href="https://doi.org/10.1016/j.neunet.2025.107396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abdominal multi-organ segmentation in computed tomography (CT) scans has exhibited successful applications in numerous real clinical scenarios. Nevertheless, prevailing methods for multi-organ segmentation often necessitate either a substantial volume of datasets derived from a single healthcare institution or the centralized storage of patient data obtained from diverse healthcare institutions. This prevailing approach significantly burdens data labeling and collection, thereby exacerbating the associated challenges. Compared to multi organ annotation labels, single organ annotation labels are extremely easy to obtain and have low costs. Therefor, this work establishes an effective collaborative mechanism between multi organ labels and single organ labels, and proposes an expert guided and partially-labeled data collaboration framework for multi organ segmentation, named EGPD-Seg. Firstly, a reward penalty loss function is proposed under the setting of partial labels to make the model more focused on the targets in single organ labels, while suppressing the influence of unlabeled organs on segmentation results. Then, an expert guided module is proposed to enable the model to learn prior knowledge, thereby enabling the model to obtain the ability to segment unlabeled organs on a single organ labeled dataset. The two modules interact with each other and jointly promote the multi organ segmentation performance of the model under label partial settings. This work has been effectively validated on five publicly available abdominal multi organ segmentation datasets, including internal datasets and invisible external datasets. Code: https://github.com/LiLiXJTU/EGPDC-Seg .},
  archive      = {J_NN},
  author       = {Li Li and Jianyi Liu and Hanguang Xiao and Guanqun Zhou and Qiyuan Liu and Zhicheng Zhang},
  doi          = {10.1016/j.neunet.2025.107396},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107396},
  shortjournal = {Neural Netw.},
  title        = {Expert guidance and partially-labeled data collaboration for multi-organ segmentation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling scale and rotation invariance in convolutional
neural networks with retina like transformation. <em>NN</em>,
<em>187</em>, 107395. (<a
href="https://doi.org/10.1016/j.neunet.2025.107395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional convolutional neural networks (CNNs) struggle with scale and rotation transformations, resulting in reduced performance on transformed images. Previous research focused on designing specific CNN modules to extract transformation-invariant features. However, these methods lack versatility and are not adaptable to a wide range of scenarios. Drawing inspiration from human visual invariance, we propose a novel brain-inspired approach to tackle the invariance problem in CNNs. If we consider a CNN as the visual cortex, we have the potential to design an “eye” that exhibits transformation invariance, allowing CNNs to perceive the world consistently. Therefore, we propose a retina module and then integrate it into CNNs to create transformation-invariant CNNs (TICNN), achieving scale and rotation invariance. The retina module comprises a retina-like transformation and a transformation-aware neural network (TANN). The retina-like transformation supports flexible image transformations, while the TANN regulates these transformations for scaling and rotation. Specifically, we propose a reference-based training method (RBTM) where the retina module learns to align input images with a reference scale and rotation, thereby achieving invariance. Furthermore, we provide mathematical substantiation for the retina module to confirm its feasibility. Experimental results also demonstrate that our method outperforms existing methods in recognizing images with scale and rotation variations. The code will be released at https://github.com/JiaHongZ/TICNN .},
  archive      = {J_NN},
  author       = {Jiahong Zhang and Guoqi Li and Qiaoyi Su and Lihong Cao and Yonghong Tian and Bo Xu},
  doi          = {10.1016/j.neunet.2025.107395},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107395},
  shortjournal = {Neural Netw.},
  title        = {Enabling scale and rotation invariance in convolutional neural networks with retina like transformation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-tuning hierarchical transformer via token
communication and sample aggregation constraint for object
re-identification. <em>NN</em>, <em>187</em>, 107394. (<a
href="https://doi.org/10.1016/j.neunet.2025.107394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transformer-based methods have shown remarkable success in object re-identification. However, most works directly embed off-the-shelf transformer backbones for feature extraction. These methods treat all patch tokens equally, ignoring the difference of distinct patch tokens for feature representation. To solve this issue, this paper designs a feature-tuning mechanism for transformer backbones to emphasize important patches and attenuate unimportant patches. Specifically, a Feature-tuning Hierarchical Transformer (FHTrans) for object re-identification is proposed. First, we propose a plug-and-play Feature-tuning module via Token Communication (TCF) deployed within transformer encoder blocks. This module regards the class token as a pivot to achieve communication between patch tokens. Important patch tokens are emphasized, while unimportant patch tokens are attenuated, focusing more precisely on the discriminative features related to object distinction. Then, we construct a FHTrans based on the designed feature-tuning module. The encoder blocks are divided into three hierarchies considering the correlation between feature representativeness and transformer depth. As the hierarchy deepens, the communication between tokens becomes tighter. This enables the model to capture more crucial feature information. Finally, we propose a Sample Aggregation (SA) loss to impose more effective constraints on statistical characteristics among samples, thereby enhancing intra-class aggregation and guiding FHTrans to learn more discriminative features. Experiments on object re-identification benchmarks demonstrate that our method can achieve state-of-the-art performance.},
  archive      = {J_NN},
  author       = {Zhi Yu and Zhiyong Huang and Mingyang Hou and Jiaming Pei and Yan Yan and Yushi Liu and Daming Sun},
  doi          = {10.1016/j.neunet.2025.107394},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107394},
  shortjournal = {Neural Netw.},
  title        = {Feature-tuning hierarchical transformer via token communication and sample aggregation constraint for object re-identification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive node-level weighted learning for directed graph
neural network. <em>NN</em>, <em>187</em>, 107393. (<a
href="https://doi.org/10.1016/j.neunet.2025.107393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed graph neural networks (DGNNs) have garnered increasing interest, yet few studies have focused on node-level representation in directed graphs. In this paper, we argue that different nodes rely on neighbor information from different directions. Furthermore, the commonly used mean aggregation for in-neighbor sets and out-neighbor sets may lose expressive power for certain nodes. To achieve this, first, we estimate the homophily of each node to neighbors in different directions by extending the Dirichlet energy. This approach allows us to assign larger weights to neighbors in directions exhibiting higher homophilic ratios for any node. Second, we introduce out-degree and in-degree information in the learning of weights to avoid the problem of weak expressive power ability of mean aggregation. Moreover, we theoretically demonstrate that our method enhances the expressive ability of directed graphs. Extensive experiments on seven real-world datasets demonstrate that our method outperforms state-of-the-art approaches in both node classification and link prediction tasks.},
  archive      = {J_NN},
  author       = {Jincheng Huang and Xiaofeng Zhu},
  doi          = {10.1016/j.neunet.2025.107393},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107393},
  shortjournal = {Neural Netw.},
  title        = {Adaptive node-level weighted learning for directed graph neural network},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Central loss guides coordinated transformer for reliable
anatomical landmark detection. <em>NN</em>, <em>187</em>, 107391. (<a
href="https://doi.org/10.1016/j.neunet.2025.107391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heatmap-based anatomical landmark detection is still facing two unresolved challenges: (1) inability to accurately evaluate the distribution of heatmap; (2) inability to effectively exploit global spatial structure information. To address the computational inability challenge, we propose a novel position-aware and sample-aware central loss. Specifically, our central loss can absorb position information, enabling accurate evaluation of the heatmap distribution. More advanced is that our central loss is sample-aware, which can adaptively distinguish easy and hard samples and make the model more focused on hard samples while solving the challenge of extreme imbalance between landmarks and non-landmarks. To address the challenge of ignoring structure information, a Coordinated Transformer, called CoorTransformer, is proposed, which establishes long-range dependencies under the guidance of landmark coordinate information, making the attention more focused on the sparse landmarks while taking advantage of global spatial structure. Furthermore, CoorTransformer can speed up convergence, effectively avoiding the defect that Transformers have difficulty converging in sparse representation learning. Using the advanced CoorTransformer and central loss, we propose a generalized detection model that can handle various scenarios, inherently exploiting the underlying relationship between landmarks and incorporating rich structural knowledge around the target landmarks. We analyzed and evaluated CoorTransformer and central loss on three challenging landmark detection tasks. The experimental results show that our CoorTransformer outperforms state-of-the-art methods, and the central loss significantly improves the model’s performance with p -values &lt; 0 . 05 . The source code of this work is available at the GitHub repository .},
  archive      = {J_NN},
  author       = {Qikui Zhu and Yihui Bi and Jie Chen and Xiangpeng Chu and Danxin Wang and Yanqing Wang},
  doi          = {10.1016/j.neunet.2025.107391},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107391},
  shortjournal = {Neural Netw.},
  title        = {Central loss guides coordinated transformer for reliable anatomical landmark detection},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level semantic-aware transformer for image captioning.
<em>NN</em>, <em>187</em>, 107390. (<a
href="https://doi.org/10.1016/j.neunet.2025.107390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective visual representation is crucial for image captioning task. Among the existing methods, the grid-based visual encoding methods take fragmented features extracted from the entire image as input, lacking the fine-grained semantic information focused on salient objects. To address this issue, we propose an effective method, namely Multi-Level Semantic-Aware Transformer (MLSAT) for image captioning, to simultaneously focus on contextual details and high-level semantic information centered on salient objects. First, to model the spatial correlations of grids and the semantic interactions of salient objects, we propose the Visual Content Guided Attention (VCGA), which adaptively embeds the relative position relationships of the grids into the visual features based on their visual content and is used as the attention layer of the encoder. Then, in order to enhance the visual representation, we propose the Multi-Level Semantic-Aware (MLSA) module which further models the fine-grained semantic information centered on salient objects. In this module, the primary semantic information is first extracted from the encoder by using the Semantic Information Extractor (SIE), then refined by the Semantic Refiner (SR) and adaptively integrated into the visual representation by the Visual-Semantic Fusion Block (V-SFB). Our MLSAT is extensively evaluated on the MS-COCO dataset and outperforms the state-of-the-art models, with 135.1% CIDEr (c40) on the official online testing server. The source code is available at https://github.com/XvZhao147/MLSAT},
  archive      = {J_NN},
  author       = {Qin Xu and Shan Song and Qihang Wu and Bo Jiang and Bin Luo and Jinhui Tang},
  doi          = {10.1016/j.neunet.2025.107390},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107390},
  shortjournal = {Neural Netw.},
  title        = {Multi-level semantic-aware transformer for image captioning},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A shape composition method for named entity recognition.
<em>NN</em>, <em>187</em>, 107389. (<a
href="https://doi.org/10.1016/j.neunet.2025.107389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) roughly encode a sentence into a dense representation (a vector), which mixes up the semantic expression of all named entities within a sentence. So the decoding process is easily overwhelmed by sentence-specific information learned during the pre-training process. It results in seriously performance degeneration in recognizing named entities, especially annotated with nested structures. In contrast to LLMs condensing a sentence into a single vector, our model adopts a discriminative language model to map each sentence into a high-order semantic space. In this space, named entities are decomposed into entity body and entity edge. The decomposition is effective to decode complex semantic structures of named entities. In this paper, a shape composition method is proposed for recognizing named entities. This approach leverages a multi-objective learning neural architecture to simultaneously detect entity bodies and classify entity edges. During training, the dual objectives for body and edge learning guide the deep network to encode more task-relevant semantic information. Our method is evaluated on eight widely used public datasets and demonstrated competitive performance. Analytical experiments show that the strategy of let semantic expressions take its course aligns with the entity recognition task. This approach yields finer-grained semantic representations, which enhance not only NER but also other NLP tasks.},
  archive      = {J_NN},
  author       = {Ying Hu and Yanping Chen and Yong Xu},
  doi          = {10.1016/j.neunet.2025.107389},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107389},
  shortjournal = {Neural Netw.},
  title        = {A shape composition method for named entity recognition},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semantic enhancement-based multimodal network model for
extracting information from evidence lists. <em>NN</em>, <em>187</em>,
107387. (<a href="https://doi.org/10.1016/j.neunet.2025.107387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Courts require the extraction of crucial information about various cases from heterogeneous evidence lists for knowledge-driven decision-making. However, traditional manual screening is complex and inaccurate when confronted with massive evidence lists and cannot meet the demands of legal judgment. Therefore, we propose a semantic enhancement-based multimodal network model (SEBM) to accurately extract critical information from evidence lists. First, we construct the entity semantic graph based on the differences among entity categories in the text content. Subsequently, we extract the features of multiple modalities within the document by employing distinct methods and guide the fusion of features within each modality to enhance the semantic association among them based on the constructed entity semantic graphs. Furthermore, the improved multimodal self-attention mechanism is employed to enhance the interactions between the various modal features, and the loss function combining Taylor polynomials and supervised contrast learning is utilized to reduce the information loss. Finally, SEBM is evaluated using the authentic Chinese evidence list dataset, which includes extensive entity details from diverse case types across multiple law firms. Results from experiments conducted on the authentic evidence list dataset demonstrate that our model performs better than other high-performing models.},
  archive      = {J_NN},
  author       = {Shun Luo and Juan Yu},
  doi          = {10.1016/j.neunet.2025.107387},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107387},
  shortjournal = {Neural Netw.},
  title        = {A semantic enhancement-based multimodal network model for extracting information from evidence lists},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memristive circuit of emotion with negative feedback based
on three primary color model. <em>NN</em>, <em>187</em>, 107385. (<a
href="https://doi.org/10.1016/j.neunet.2025.107385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many memristive circuits tend to oversimplify the process of emotion generation as a linear event, disregarding crucial factors such as negative feedback and other regulatory mechanisms. In this paper, a memristive circuit of emotion with negative feedback based on three primary color model is proposed to solve the above problems. The designed circuit is composed of perception modules, synapse modules, central nervous system modules and overt behavior module. It realizes emotion generation, emotion evolution and long-term memory functions based on the neural network circuit with behavioral homeostatic negative feedback function. Meanwhile, the three primary color model of basic emotions is discussed and realized. Any two basic emotions can be mixed to produce a higher order emotion. The memristive circuit, based on the three primary color model as a theoretical foundation, offers valuable insights for the further advancement of neural networks.},
  archive      = {J_NN},
  author       = {Juntao Han and Gang Liu and Zhang Zhang},
  doi          = {10.1016/j.neunet.2025.107385},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107385},
  shortjournal = {Neural Netw.},
  title        = {Memristive circuit of emotion with negative feedback based on three primary color model},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arch-net: Model conversion and quantization for architecture
agnostic model deployment. <em>NN</em>, <em>187</em>, 107384. (<a
href="https://doi.org/10.1016/j.neunet.2025.107384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant computational demands of Deep Neural Networks (DNNs) present a major challenge for their practical application. Recently, many Application-Specific Integrated Circuit (ASIC) chips have incorporated dedicated hardware support for neural network acceleration. However, the lengthy development cycle of ASIC chips means they often lag behind the latest advances in neural architecture research. For instance, Layer Normalization is not well-supported on many popular chips, and the efficiency of 7 × 7 convolution is significantly lower than the equivalent three 3 × 3 convolution. Therefore, in this paper, we introduce Arch-Net, a neural network framework comprised exclusively of a select few common operators, namely 3 × 3 Convolution, 2 × 2 Max-pooling, Batch Normalization, Fully Connected layers, and Concatenation, which are efficiently supported across the majority of ASIC architectures. To facilitate the conversion of disparate network architectures into Arch-Net, we propose the Arch-Distillation methodology, which incorporates strategies such as Residual Feature Adaptation and Teacher Attention Mechanism. These mechanisms enable effective conversion between different network structures alongside efficient model quantization. The resultant Arch-Net eliminates unconventional network constructs while maintaining robust performance even under sub-8-bit quantization, thereby enhancing compatibility and deployment efficiency. Empirical results from image classification and machine translation tasks demonstrate that using only a few types of operators in Arch-Net can achieve results comparable to those obtained with complex architectures. This provides a new insight for deploying structure-agnostic neural networks on various ASIC chips.},
  archive      = {J_NN},
  author       = {Shuangkang Fang and Weixin Xu and Zipeng Feng and Song Yuan and Yufeng Wang and Yi Yang and Wenrui Ding and Shuchang Zhou},
  doi          = {10.1016/j.neunet.2025.107384},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107384},
  shortjournal = {Neural Netw.},
  title        = {Arch-net: Model conversion and quantization for architecture agnostic model deployment},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-based distributed cooperative neural learning control
for nonlinear multiagent systems with time-varying output constraints.
<em>NN</em>, <em>187</em>, 107383. (<a
href="https://doi.org/10.1016/j.neunet.2025.107383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical engineering, many systems are required to operate under different constraint conditions due to considerations of system security. Violating these constraints conditions during operation may lead to performance degradation. Additionally, communication among agents is highly dependent on the network, which inevitably imposes a network burden on the control systems. To address these issues, this paper investigates the switching event-triggered distributed cooperative learning control issue for nonlinear multiagent systems with time-vary output constraints. An improved output-dependent universal barrier function with adjustable constraint boundaries is proposed, which can uniformly handle symmetric or asymmetric output constraints without changing the controller structure. Meanwhile, an improved switching event-triggered condition is designed based on neural networks (NNs) weight, which can allow the system to adaptively adjust the NNs weight update frequency according to the performance of the system, thereby saving communication resources. Furthermore, the Padé approximation technique is employed to address the input delay issue and simplify the controller design process. Using Lyapunov stability theory, it is proved that the outputs of all followers converge to a neighborhood around the leader output without violating output constraints, and all signals in the closed-loop system remain ultimately bounded. At last, the availability of the presented approach can be verified through some simulation results.},
  archive      = {J_NN},
  author       = {Congyan Lv and Guangliang Liu and Yingnan Pan and Zhijian Hu and Yan Lei},
  doi          = {10.1016/j.neunet.2025.107383},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107383},
  shortjournal = {Neural Netw.},
  title        = {Event-based distributed cooperative neural learning control for nonlinear multiagent systems with time-varying output constraints},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilinear spatiotemporal fusion network: An efficient
approach for traffic flow prediction. <em>NN</em>, <em>187</em>, 107382.
(<a href="https://doi.org/10.1016/j.neunet.2025.107382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow forecasting is critical for intelligent transportation systems, yet increasing model complexity in spatiotemporal graph neural networks does not always yield proportional gains. In this paper, we present a Bilinear Spatiotemporal Fusion Network (BLSTF) tailored for stable, periodic traffic scenarios. First, a temporal enhancement module is introduced to mitigate multi-step error accumulation. Second, predefined graph priors with linear feedback leverage known road topologies for straightforward yet effective spatial modeling. Finally, a bilinear fusion mechanism seamlessly integrates refined temporal and spatial features with minimal computational overhead. Extensive experiments on four real-world datasets show that BLSTF outperforms state-of-the-art methods, achieving MAE and MAPE of 14.05 and 13.90% on PEMS03, 17.93 and 12.12% on PEMS04, 18.87 and 7.86% on PEMS07, and 13.49 and 8.71% on PEMS08, demonstrating BLSTF’s potential to deliver accurate, efficient, and interpretable traffic flow forecasts.},
  archive      = {J_NN},
  author       = {Jing Chen and Shixiang Pan and Weimin Peng and Wenqiang Xu},
  doi          = {10.1016/j.neunet.2025.107382},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107382},
  shortjournal = {Neural Netw.},
  title        = {Bilinear spatiotemporal fusion network: An efficient approach for traffic flow prediction},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving generalization of neural vehicle routing problem
solvers through the lens of model architecture. <em>NN</em>,
<em>187</em>, 107380. (<a
href="https://doi.org/10.1016/j.neunet.2025.107380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural models produce promising results when solving Vehicle Routing Problems (VRPs), but may often fall short in generalization. Recent attempts to enhance model generalization often incur unnecessarily large training cost or cannot be directly applied to other models solving different VRP variants. To address these issues, we take a novel perspective on model architecture in this study. Specifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and a Distribution-Specific (DS) decoder to enhance the size and distribution generalization, respectively. ESF adjusts the attention weight pattern of the model towards familiar ones discovered during training when solving VRPs of varying sizes. The DS decoder explicitly models VRPs of multiple training distribution patterns through multiple auxiliary light decoders, expanding the model representation space to encompass a broader range of distributional scenarios. We conduct extensive experiments on both synthetic and widely recognized real-world benchmarking datasets and compare the performance with seven baseline models. The results demonstrate the effectiveness of using ESF and DS decoder to obtain a more generalizable model and showcase their applicability to solve different VRP variants, i.e., traveling salesman problem and capacitated VRP. Notably, our proposed generic components require minimal computational resources, and can be effortlessly integrated into conventional generalization strategies to further elevate model generalization.},
  archive      = {J_NN},
  author       = {Yubin Xiao and Di Wang and Xuan Wu and Yuesong Wu and Boyang Li and Wei Du and Liupu Wang and You Zhou},
  doi          = {10.1016/j.neunet.2025.107380},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107380},
  shortjournal = {Neural Netw.},
  title        = {Improving generalization of neural vehicle routing problem solvers through the lens of model architecture},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Turbulence control in memristive neural network via adaptive
magnetic flux based on DLS-ADMM technique. <em>NN</em>, <em>187</em>,
107379. (<a href="https://doi.org/10.1016/j.neunet.2025.107379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-voltage defibrillation for eliminating cardiac spiral waves has significant side effects, necessitating the pursuit of low-energy alternatives for a long time. Adaptive optimization techniques and machine learning methods provide promising solutions for adaptive control of cardiac wave propagation. In this paper, the control of spiral waves and turbulence, as well as 2D and 3D heterogeneity in memristive neural network by using adaptive magnetic flux (AMF) is investigated based on dynamic learning of synchronization - alternating direction method of multipliers (DLS-ADMM). The results show that AMF can achieve global electrical synchronization under multiple complex conditions. There is a trade-off between AMF accuracy and computational speed, lowering the resolution of AMF requires a higher flux of magnetic fields to achieve the network synchronization, resulting in an increase in average Hamiltonian energy, which implies greater energy consumption. The AMF method is more energy efficient than existing DC and AC methods, but it relies on adequate resolution. The ADMM constraints can enhance the synchronization robustness and energy efficiency of DLS techniques, albeit at the cost of increased the computational complexity. The adaptive elimination of spiral waves and turbulence using AMF presented in this paper may provide a novel approach for the low-energy defibrillation studies, and its practical application and performance enhancement deserve further research.},
  archive      = {J_NN},
  author       = {Qianming Ding and Yong Wu and Ying Xie and Yipeng Hu and Weifang Huang and Ya Jia},
  doi          = {10.1016/j.neunet.2025.107379},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107379},
  shortjournal = {Neural Netw.},
  title        = {Turbulence control in memristive neural network via adaptive magnetic flux based on DLS-ADMM technique},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformation-invariant neural network and its applications in
distorted image restoration and analysis. <em>NN</em>, <em>187</em>,
107378. (<a href="https://doi.org/10.1016/j.neunet.2025.107378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconformality of the output deformation map. By controlling the Beltrami coefficient, the local geometric distortion under the quasiconformal mapping can be controlled. The QCTN is lightweight and simple, which can be readily integrated into other existing deep neural networks to enhance their performance. Leveraging our framework, we have developed an image classification network that achieves accurate classification of distorted images. Our proposed framework has been applied to restore geometrically distorted images by atmospheric turbulence and water turbulence. DINN outperforms existing GAN-based restoration methods under these scenarios, demonstrating the effectiveness of the proposed framework. Additionally, we apply our proposed framework to the 1-1 verification of human face images under atmospheric turbulence and achieve satisfactory performance, further demonstrating the efficacy of our approach.},
  archive      = {J_NN},
  author       = {Han Zhang and Qiguang Chen and Lok Ming Lui},
  doi          = {10.1016/j.neunet.2025.107378},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107378},
  shortjournal = {Neural Netw.},
  title        = {Deformation-invariant neural network and its applications in distorted image restoration and analysis},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing text-centric fake news detection via external
knowledge distillation from LLMs. <em>NN</em>, <em>187</em>, 107377. (<a
href="https://doi.org/10.1016/j.neunet.2025.107377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news poses a significant threat to society, making the automatic and accurate detection of fake news an urgent task. Various detection cues have been explored in extensive research, with news text content shown to be indispensable as it directly reflects the creator’s intent. Existing paradigms for developing text-centric methods, i.e., small language model (SLM)-based, external knowledge-enhanced, and large language model (LLM)-based approaches, have achieved remarkable improvements. However, each of these paradigms still faces the following challenges: (1) the low generalization ability of SLM-based methods, due to their training on limited and specific knowledge; (2) the extensive retrieval operations required by external knowledge-enhanced methods, both during training and at the inference stage, leading to increased computational costs; and (3) LLMs are prone to hallucinations and less suited for factual reasoning. To address these challenges, we propose LEKD, which combines the strengths of SLMs, external knowledge, and LLMs to enhance text-centric fake news detection. Specifically, LEKD leverages the LLM to generate external knowledge as supplementary information for the training set only and introduces a graph-based semantic-aware feature alignment module to resolve knowledge contradictions, as well as an information bottleneck-based knowledge distillation module to ensure the implicit generation of these features during inference. Extensive experiments conducted on two datasets demonstrate the advantages of LEKD over the baselines.},
  archive      = {J_NN},
  author       = {Xueqin Chen and Xiaoyu Huang and Qiang Gao and Li Huang and Guisong Liu},
  doi          = {10.1016/j.neunet.2025.107377},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107377},
  shortjournal = {Neural Netw.},
  title        = {Enhancing text-centric fake news detection via external knowledge distillation from LLMs},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic semantic-geometric guidance and structure transfer
network for cross-scene hyperspectral image classification. <em>NN</em>,
<em>187</em>, 107374. (<a
href="https://doi.org/10.1016/j.neunet.2025.107374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, cross-scene hyperspectral image classification(HSIC) via domain adaptation is drawing increasing attention. However, most existing methods either directly align the source domain and target domain without fully mining of SD information, or perform the domain adaptation from semantic and structure aspects with simply characterization method which is sensitive to noise, resulting in the negative transfer and performance decline. To address these issues, in this paper, we propose a novel Dynamic Semantic-Geometric Guidance and Structure Transfer (DSGG-ST) network for cross-scene hyperspectral image classification task. The main aspects of DSGG-ST are twofold. On the one hand, the dynamic semantic-geometric guidance (DSGG) module is designed which consists of the semantic guidance component and geometric guidance component. The proposed DSGG module can align source and target domains under the dynamical guidance of the domain-invariance learning from the semantic and geometric perspectives. On the other hand, the graph attention learning-matching (GALM) module is developed for effectively transferring the structure information between the source domain and target domain. In this module, the graph attention network is adopted to encode the underlying complex structures, and the SeedGNN is exploited for efficient graph matching and alignment. Extensive experiments on three commonly used cross-scene HSI datasets demonstrate that the proposed DSGG-ST obtains a new SOTA performance on cross-scene HSIC, verifying the effectiveness of the proposed DSGG-ST.},
  archive      = {J_NN},
  author       = {Qin Xu and Shuke Wang and Jie Wei and Bo Jiang and Zhifu Tao and Bin Luo},
  doi          = {10.1016/j.neunet.2025.107374},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107374},
  shortjournal = {Neural Netw.},
  title        = {Dynamic semantic-geometric guidance and structure transfer network for cross-scene hyperspectral image classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved artificial protozoa optimizer for CNN
architecture optimization. <em>NN</em>, <em>187</em>, 107368. (<a
href="https://doi.org/10.1016/j.neunet.2025.107368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel neural architecture search (NAS) method called MAPOCNN, which leverages an enhanced version of the Artificial Protozoa Optimizer (APO) to optimize the architecture of Convolutional Neural Networks (CNNs). The APO is known for its rapid convergence, high stability, and minimal parameter involvement. To further improve its performance, we introduce MAPO (Modified Artificial Protozoa Optimizer), which incorporates the phototaxis behavior of protozoa. This addition helps mitigate the risk of premature convergence, allowing the algorithm to explore a broader range of possible CNN architectures and ultimately identify more optimal solutions. Through rigorous experimentation on benchmark datasets, including Rectangle and Mnist-random, we demonstrate that MAPOCNN not only achieves faster convergence times but also performs competitively when compared to other state-of-the-art NAS algorithms. The results highlight the effectiveness of MAPOCNN in efficiently discovering CNN architectures that outperform existing methods in terms of both speed and accuracy. This work presents a promising direction for optimizing deep learning architectures using biologically inspired optimization techniques.},
  archive      = {J_NN},
  author       = {Xiaofeng Xie and Yuelin Gao and Yuming Zhang},
  doi          = {10.1016/j.neunet.2025.107368},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107368},
  shortjournal = {Neural Netw.},
  title        = {An improved artificial protozoa optimizer for CNN architecture optimization},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive graph auto-encoder for graph embedding.
<em>NN</em>, <em>187</em>, 107367. (<a
href="https://doi.org/10.1016/j.neunet.2025.107367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding aims to embed the information of graph data into low-dimensional representation space. Prior methods generally suffer from an imbalance of preserving structural information and node features due to their pre-defined inductive biases, leading to unsatisfactory generalization performance. In order to preserve the maximal information, graph contrastive learning (GCL) has become a prominent technique for learning discriminative embeddings. However, in contrast with graph-level embeddings, existing GCL methods generally learn less discriminative node embeddings in a self-supervised way. In this paper, we ascribe above problem to two challenges: (1) graph data augmentations, which are designed for generating contrastive representations, hurt the original semantic information for nodes. (2) the nodes within the same cluster are selected as negative samples. To alleviate these challenges, we propose C ontrastive G raph A uto- E ncoder (CGAE) and C ontrastive V ariational G raph A uto- E ncoder (CVGAE). Specifically, we first propose two distribution-dependent regularizations to guide the paralleled encoders to generate contrastive representations following similar distribution, followed by theoretical derivations to verify the equivalence of the above regularizations. Then, we utilize truncated triplet loss, which only selects top-k nodes as negative samples, to avoid over-separate nodes affiliated to the same cluster. Furthermore, we give theoretical analysis of the effectiveness of our models. Experiments on several real-world datasets show that our models advanced performance over all baselines in link prediction, node clustering, and graph visualization tasks.},
  archive      = {J_NN},
  author       = {Shuaishuai Zu and Li Li and Jun Shen and Weitao Tang},
  doi          = {10.1016/j.neunet.2025.107367},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107367},
  shortjournal = {Neural Netw.},
  title        = {Contrastive graph auto-encoder for graph embedding},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic information-based attention mapping network for
few-shot knowledge graph completion. <em>NN</em>, <em>187</em>, 107366.
(<a href="https://doi.org/10.1016/j.neunet.2025.107366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Knowledge Graph Completion (FKGC), an emerging technology capable of inferring new triples using only a few reference relation triples, has gained significant attention in recent years. However, existing FKGC methods primarily focus on structural information while failing to effectively utilize the textual semantic information inherent in triples. To address this limitation, we propose an innovative Semantic Information-based Attention Mapping Network (SI-AMN). This novel model significantly enhances knowledge graph completion accuracy through a unique dual-information fusion mechanism that effectively integrates both structural and textual semantic information. The core innovation of SI-AMN lies in its two key components: a semantic encoder for extracting high-quality textual features and an attention mapping network that learns semantic interactions between entity and relation types. Experimental results on benchmark datasets demonstrate SI-AMN’s superior performance, achieving a 40% improvement in prediction accuracy compared to state-of-the-art methods. Ablation studies further validate the effectiveness of each component in our proposed model. This research not only provides a novel solution for knowledge graph completion but also reveals the crucial value of semantic information in graph completion tasks, paving the way for future research directions in this field.},
  archive      = {J_NN},
  author       = {Fan Guo and Xiangmao Chang and Yunqi Guo and Guoliang Xing and Yunlong Zhao},
  doi          = {10.1016/j.neunet.2025.107366},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107366},
  shortjournal = {Neural Netw.},
  title        = {Semantic information-based attention mapping network for few-shot knowledge graph completion},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2AF: An action framework to self-check the understanding
self-consistency of large language models. <em>NN</em>, <em>187</em>,
107365. (<a href="https://doi.org/10.1016/j.neunet.2025.107365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), which are trained on massive text data, have demonstrated remarkable advancements in language understanding capabilities. Nevertheless, it remains unclear to what extent LLMs have effectively captured and utilized the implicit relationships inherent in the text. This study introduces ‘Understanding Self-Consistency’ , a new perspective that reflects LLMs’ ability to grasp in-depth knowledge relationships through their consistency performance. Specifically, Understanding Self-Consistency refers to the model’s capacity to maintain logical and contextual consistency between inputs and responses. Inspired by human cognitive behavior, we design a self-check action framework named S 2 A F . Wherein, a self-question and answering mechanism is emphasized and forms a logically closed loop including four classes of actions, allowing our S 2 A F to generate, question, answer, and evaluate autonomously. Experimental results on six LLMs across two datasets show that LLMs exhibit objective ability values of the understanding self-consistency and demonstrate their differentiated grasp of knowledge relationships across different reasoning paradigms. Moreover, our findings reveal that LLMs’ performance can be improved with their own outputs (which we call ‘self-enhanced Feedforward’). Notably, S 2 A F merely relies on factual logical relationships, showcasing its potential to advance the development of embodied artificial intelligence (EAI).},
  archive      = {J_NN},
  author       = {Huihui Shao and Fanyu Wang and Zhenping Xie},
  doi          = {10.1016/j.neunet.2025.107365},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107365},
  shortjournal = {Neural Netw.},
  title        = {S2AF: An action framework to self-check the understanding self-consistency of large language models},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep huber quantile regression networks. <em>NN</em>,
<em>187</em>, 107364. (<a
href="https://doi.org/10.1016/j.neunet.2025.107364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical machine learning regression applications aim to report the mean or the median of the predictive probability distribution, via training with a squared or an absolute error scoring function. The importance of issuing predictions of more functionals of the predictive probability distribution (quantiles and expectiles) has been recognized as a means to quantify the uncertainty of the prediction. In deep learning (DL) applications, that is possible through quantile and expectile regression neural networks (QRNN and ERNN respectively). Here we introduce deep Huber quantile regression networks (DHQRN) that nest QRNN and ERNN as edge cases. DHQRN can predict Huber quantiles, which are more general functionals in the sense that they nest quantiles and expectiles as limiting cases. The main idea is to train a DL algorithm with the Huber quantile scoring function, which is consistent for the Huber quantile functional. As a proof of concept, DHQRN are applied to predict house prices in Melbourne, Australia and Boston, United States (US). In this context, predictive performances of three DL architectures are discussed along with evidential interpretation of results from two economic case studies. Additional simulation experiments and applications to real-world case studies using open datasets demonstrate a satisfactory absolute performance of DHQRN.},
  archive      = {J_NN},
  author       = {Hristos Tyralis and Georgia Papacharalampous and Nilay Dogulu and Kwok P. Chun},
  doi          = {10.1016/j.neunet.2025.107364},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107364},
  shortjournal = {Neural Netw.},
  title        = {Deep huber quantile regression networks},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware graph contrastive fusion network for
multimodal physiological signal emotion recognition. <em>NN</em>,
<em>187</em>, 107363. (<a
href="https://doi.org/10.1016/j.neunet.2025.107363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been widely adopted to mine topological patterns contained in physiological signals for emotion recognition. However, since physiological signals are non-stationary and susceptible to various noises, there exists inter-sensor connectivity uncertainty in each modality. Such intra-modal connectivity uncertainty may further lead to inter-modal semantic gap uncertainty, which will cause the unimodal bias problem and greatly affect the fusion effectiveness. While, such issue has never been fully considered in existing multimodal fusion models. To this end, we proposed an Uncertainty-Aware Graph Contrastive Fusion Network (UAGCFNet) to fuse multimodal physiological signals effectively for emotion recognition. Firstly, a probabilistic model-based Uncertainty-Aware Graph Convolutional Network (UAGCN), which can estimate and quantify the inter-sensor connectivity uncertainty, is constructed for each modality to extract its uncertainty-aware graph representation. Secondly, a Transitive Contrastive Fusion (TCF) module, which combines the Criss-Cross Attention (CCA)-based fusion mechanism and Transitive Contrastive Learning (TCL)-based calibration strategy organically, is designed to achieve effective fusion of multimodal graph representations by eliminating the unimodal bias problem resulting from the inter-modal semantic gap uncertainty. Extensive experimental results on DEAP, DREAMER, and MPED datasets under both subject-dependent and subject-independent scenarios demonstrate that (i) the proposed model outperforms State-Of-The-Art (SOTA) multimodal fusion models with fewer parameters and lower computational complexity; (ii) each key module and loss function contributes significantly to the performance enhancement of the proposed model; (iii) the proposed model can eliminate the unimodal bias problem effectively.},
  archive      = {J_NN},
  author       = {Guangqiang Li and Ning Chen and Hongqing Zhu and Jing Li and Zhangyong Xu and Zhiying Zhu},
  doi          = {10.1016/j.neunet.2025.107363},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107363},
  shortjournal = {Neural Netw.},
  title        = {Uncertainty-aware graph contrastive fusion network for multimodal physiological signal emotion recognition},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural approach to the turing test: The role of emotions.
<em>NN</em>, <em>187</em>, 107362. (<a
href="https://doi.org/10.1016/j.neunet.2025.107362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As is well known, the Turing Test proposes the possibility of distinguishing the behavior of a machine from that of a human being through an experimental session. The Turing Test assesses whether a person asking questions to two different entities, can tell from their answers which of them is the human being and which is the machine. With the progress of Artificial Intelligence, the number of contexts in which the capacities of response of a machine will be indistinguishable from those of a human being is expected to increase rapidly. In order to configure a Turing Test in which it is possible to distinguish human behavior from machine behavior independently from the advances of Artificial Intelligence, at least in the short-medium term, it would be important to base it not on the differences between man and machine in terms of performance and dialogue capacity, but on some specific characteristic of the human mind that cannot be reproduced by the machine even in principle. We studied a new kind of test based on the hypothesis that such characteristic of the human mind exists and can be made experimentally evident. This peculiar characteristic is the emotional content of human cognition and, more specifically, its link with memory enhancement. To validate this hypothesis we recorded the EEG signals of 39 subjects that underwent a specific test and analyzed their signals with a neural network able to label similar signal patterns with similar binary codes. The results showed that, with a statistically significant difference, the test participants more easily recognized images associated in the past with an emotional reaction than those not associated with such a reaction. This distinction in our view is not accessible to a software system, even AI-based, and a Turing Test based on this feature of the mind may make distinguishable human versus machine responses.},
  archive      = {J_NN},
  author       = {Rita Pizzi and Hao Quan and Matteo Matteucci and Simone Mentasti and Roberto Sassi},
  doi          = {10.1016/j.neunet.2025.107362},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107362},
  shortjournal = {Neural Netw.},
  title        = {A neural approach to the turing test: The role of emotions},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial and frequency information fusion transformer for
image super-resolution. <em>NN</em>, <em>187</em>, 107351. (<a
href="https://doi.org/10.1016/j.neunet.2025.107351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works have indicated that Transformer-based models bring impressive image reconstruction performance in single image super-resolution (SISR). However, existing Transformer-based approaches utilize self-attention within non-overlapping windows. This restriction hinders the network’s ability to adopt large receptive fields, which are essential for capturing global information and establishing long-distance dependencies, especially in the early layers. To fully leverage global information and activate more pixels during the image reconstruction process, we have developed a Spatial and Frequency Information Fusion Transformer (SFFT) with an expansive receptive field. SFFT concurrently combines spatial and frequency domain information to comprehensively leverage their complementary strengths, capturing both local and global image features while integrating low and high-frequency information. Additionally, we utilize the overlapping cross-attention block (OCAB) to facilitate pixel transmission between adjacent windows, enhancing network performance. During the training stage, we incorporate the Fast Fourier Transform (FFT) loss, thereby fully leveraging the capabilities of our proposed modules and further tapping into the model’s potential. Extensive quantitative and qualitative evaluations on benchmark datasets indicate that the proposed algorithm surpasses state-of-the-art methods in terms of accuracy. Specifically, our method achieves a PSNR score of 32.67 dB on the Manga109 dataset, surpassing SwinIR by 0.64 dB and HAT by 0.19 dB, respectively. The source code and pre-trained models are available at https://github.com/Xufujie/SFFT},
  archive      = {J_NN},
  author       = {Yan Zhang and Fujie Xu and Yemei Sun and Jiao Wang},
  doi          = {10.1016/j.neunet.2025.107351},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107351},
  shortjournal = {Neural Netw.},
  title        = {Spatial and frequency information fusion transformer for image super-resolution},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spatial–spectral fusion convolutional transformer network
with contextual multi-head self-attention for hyperspectral image
classification. <em>NN</em>, <em>187</em>, 107350. (<a
href="https://doi.org/10.1016/j.neunet.2025.107350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) can effectively extract local features, while Vision Transformer excels at capturing global features. Combining these two networks to enhance the classification performance of hyperspectral images (HSI) has garnered significant attention. However, most existing fusion methods introduce inductive biases for the Transformer by directly connecting convolutional modules and Transformer encoders for feature extraction but rarely enhance the Transformer’s ability to extract local contextual information through convolutional embedding. In this paper, we propose a spatial–spectral fusion convolutional Transformer method (SSFCT) with contextual multi-head self-attention (CMHSA) for HSI classification. Specifically, we first designed a local feature aggregation (LFA) module that utilizes a three-branch convolution architecture and attention layers to extract and enhance local spatial–spectral fusion features. Then, a novel CMHSA is built to extract interaction information of local contextual features through integrating static and dynamic local contextual representations from 3D convolution and attention mechanisms, and the CMHSA is integrated into the devised dual-branch spatial–spectral convolutional transformer (DSSCT) module to simultaneously capture global–local associations in both spatial and spectral domains. Finally, the attention feature fusion (AFF) module is proposed to fully obtain global–local spatial–spectral comprehensive features. Extensive experiments on five HSI datasets — Indian Pines, Salinas Valley, Houston2013, Botswana, and Yellow River Delta — outperform state-of-the-art methods, achieving overall accuracies of 98.03%, 99.68%, 98.65%, 97.97%, and 89.43%, respectively, showcasing its effectiveness for HSI classification.},
  archive      = {J_NN},
  author       = {Wuli Wang and Qi Sun and Li Zhang and Peng Ren and Jianbu Wang and Guangbo Ren and Baodi Liu},
  doi          = {10.1016/j.neunet.2025.107350},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107350},
  shortjournal = {Neural Netw.},
  title        = {A spatial–spectral fusion convolutional transformer network with contextual multi-head self-attention for hyperspectral image classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-augmented cross-view imputation network for partial
multi-view incomplete multi-label classification. <em>NN</em>,
<em>187</em>, 107349. (<a
href="https://doi.org/10.1016/j.neunet.2025.107349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, multi-view multi-label learning often encounters the challenge of incomplete training data due to limitations in data collection and unreliable annotation processes. The absence of multi-view features impairs the comprehensive understanding of samples, omitting crucial details essential for classification. To address this issue, we present a task-augmented cross-view imputation network (TACVI-Net) for the purpose of handling partial multi-view incomplete multi-label classification. Specifically, we employ a two-stage network to derive highly task-relevant features to recover the missing views. In the first stage, we leverage the information bottleneck theory to obtain a discriminative representation of each view by extracting task-relevant information through a view-specific encoder-classifier architecture. In the second stage, an autoencoder based multi-view reconstruction network is utilized to extract high-level semantic representation of the augmented features and recover the missing data, thereby aiding the final classification task. Extensive experiments on five datasets demonstrate that our TACVI-Net outperforms other state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Lian Zhao and Jie Wen and Xiaohuan Lu and Wai Keung Wong and Jiang Long and Wulin Xie},
  doi          = {10.1016/j.neunet.2025.107349},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107349},
  shortjournal = {Neural Netw.},
  title        = {Task-augmented cross-view imputation network for partial multi-view incomplete multi-label classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards zero-shot human–object interaction detection via
vision–language integration. <em>NN</em>, <em>187</em>, 107348. (<a
href="https://doi.org/10.1016/j.neunet.2025.107348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–object interaction (HOI) detection aims to locate human–object pairs and identify their interaction categories in images. Most existing methods primarily focus on supervised learning, which relies on extensive manual HOI annotations. Such heavy reliance on closed-set supervised learning limits their generalization capabilities to unseen object categories. Inspired by the remarkable zero-shot capabilities of VLM, we propose a novel framework, termed Knowledge Integration to HOI (KI2HOI), that effectively integrates the knowledge of the visual–language model to improve zero-shot HOI detection. Specifically, we propose a ho-pair encoder to supplement contextual and interaction-specific semantic representation decoder into our model. Additionally, we propose two fusion strategies to facilitate prior knowledge transfer of VLM. One is visual-level fusion, producing more global context interaction features; another is language-level fusion, further enhancing the capability of VLM for HOI detection. Extensive experiments conducted on the mainstream HICO-DET and V-COCO datasets demonstrate that our model outperforms the previous methods in various zero-shot and full-supervised settings. The source code is available in https://github.com/xwyscut/K2HOI .},
  archive      = {J_NN},
  author       = {Weiying Xue and Qi Liu and Yuxiao Wang and Zhenao Wei and Xiaofen Xing and Xiangmin Xu},
  doi          = {10.1016/j.neunet.2025.107348},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107348},
  shortjournal = {Neural Netw.},
  title        = {Towards zero-shot human–object interaction detection via vision–language integration},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CFI-former: Efficient lane detection by multi-granularity
perceptual query attention transformer. <em>NN</em>, <em>187</em>,
107347. (<a href="https://doi.org/10.1016/j.neunet.2025.107347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the booming development of Transformer methods, the performance of lane detection tasks has been rapidly improved. However, due to the influence of inaccurate lane line shape constraints, the query sequences of existing transformer-based lane line detection methods contain a large number of repetitive and invalid information regions, which leads to redundant information in the detection region and makes the processing of information on localized feature details of the lanes biased. In this paper, a multi-granularity perceptual query attention transformer lane detection method, CFI-Former, is proposed to achieve more accurate lane detection. Specifically, a multi-granularity perceptual query attention (GQA) module is designed to extract lane local detail information. By a two-stage query from coarse to fine, redundant key–value pairs with low information relevance are first filtered out, and then fine-grained token-to-token attention is executed on the remaining candidate regions. This module emphasizes the multi-granularity nuances of lane features from global to local, leading to more effective models based on lane line shape constraints. In addition, weighted adaptive LIoU loss ( L φ − L I oU ) is proposed to improve lane detection in more challenging scenarios by adaptively increasing the relative gradient of high IoU lane objects and the weight of the loss. Extensive experiments show that CFI-Former outperforms the baseline on two popular lane detection benchmark datasets.},
  archive      = {J_NN},
  author       = {Rong Gao and Siqi Hu and Lingyu Yan and Lefei Zhang and Jia Wu},
  doi          = {10.1016/j.neunet.2025.107347},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107347},
  shortjournal = {Neural Netw.},
  title        = {CFI-former: Efficient lane detection by multi-granularity perceptual query attention transformer},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive decoupling-fusion in siamese network for image
classification. <em>NN</em>, <em>187</em>, 107346. (<a
href="https://doi.org/10.1016/j.neunet.2025.107346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are highly regarded for their ability to extract semantic information from visual inputs. However, this capability often leads to the inadvertent loss of important visual details. In this paper, we introduce an Adaptive Decoupling Fusion (ADF) designed to preserve these valuable visual details and integrate seamlessly with existing hierarchical models. Our approach emphasizes retaining and leveraging appearance information from the network’s shallow layers to enhance semantic understanding. We first decouple the appearance information from one branch of a Siamese Network and embed it into the deep feature space of the other branch. This facilitates a synergistic interaction: one branch supplies appearance information that benefits semantic understanding, while the other integrates this information into the semantic space. Traditional Siamese Networks typically use shared weights, which constrains the diversity of features that can be learned. To address this, we propose a differentiated collaborative learning where both branches receive the same input but are trained with cross-entropy loss, allowing them to have distinct weights. This enhances the network’s adaptability to specific tasks. To further optimize the decoupling and fusion, we introduce a Mapper module featuring depthwise separable convolution and a gated fusion mechanism. This module regulates the information flow between branches, balancing appearance and semantic information. Under fully self-supervised conditions, utilizing only minimal data augmentation, we achieve a top-1 accuracy of 81.11% on the ImageNet-1k dataset using ADF-ResNeXt-101.},
  archive      = {J_NN},
  author       = {Xi Yang and Pai Peng and Danyang Li and Yinghao Ye and Xiaohuan Lu},
  doi          = {10.1016/j.neunet.2025.107346},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107346},
  shortjournal = {Neural Netw.},
  title        = {Adaptive decoupling-fusion in siamese network for image classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-attention fusion and adaptive continual updating for
multimodal federated learning with heterogeneous data. <em>NN</em>,
<em>187</em>, 107345. (<a
href="https://doi.org/10.1016/j.neunet.2025.107345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables collaborative model training without direct data sharing, facilitating knowledge exchange while ensuring data privacy. Multimodal federated learning (MFL) is particularly advantageous for decentralized multimodal data, effectively managing heterogeneous information across modalities. However, the diversity in environments and data collection methods among participating devices introduces substantial challenges due to non-independent and identically distributed (non-IID) data. Our experiments reveal that, despite the theoretical benefits of multimodal data, MFL under non-IID conditions often exhibits poor performance, even trailing traditional unimodal FL approaches. Additionally, MFL frequently encounter missing modality issues, further complicating the training process. To address these challenges, we propose several improvements: the federated self-attention multimodal (FSM) feature fusion method and the multimodal federated learning adaptive continual update (FedMAC) algorithm. Moreover, we utilize a Stable Diffusion model to mitigate the impact of missing image modality. Extensive experimental results demonstrate that our proposed methods outperform other state-of-the-art FL algorithms, enhancing both accuracy and robustness in MFL.},
  archive      = {J_NN},
  author       = {Kangning Yin and Zhen Ding and Xinhui Ji and Zhiguo Wang},
  doi          = {10.1016/j.neunet.2025.107345},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107345},
  shortjournal = {Neural Netw.},
  title        = {Self-attention fusion and adaptive continual updating for multimodal federated learning with heterogeneous data},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel one-layer neural network for solving quadratic
programming problems. <em>NN</em>, <em>187</em>, 107344. (<a
href="https://doi.org/10.1016/j.neunet.2025.107344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel one-layer neural network to solve quadratic programming problems in real time by using a control parameter and transforming the optimality conditions into a system of projection equations. The proposed network includes two existing dual networks as its special cases, and an existing model can be derived from it. In particular, another new model for linear and quadratic programming problems can be obtained from the proposed network. Meanwhile, a new Lyapunov function is constructed to ensure that the proposed network is Lyapunov stable and can converge to an optimal solution of the concerned problem under mild conditions. In contrast with the existing models for quadratic programming, the proposed network requires the least neurons while maintaining weaker stability conditions. The effectiveness and characteristics of the proposed model are demonstrated by the limited simulation results.},
  archive      = {J_NN},
  author       = {Xingbao Gao and Lili Du},
  doi          = {10.1016/j.neunet.2025.107344},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107344},
  shortjournal = {Neural Netw.},
  title        = {A novel one-layer neural network for solving quadratic programming problems},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inspired by pathogenic mechanisms: A novel gradual
multi-modal fusion framework for mild cognitive impairment diagnosis.
<em>NN</em>, <em>187</em>, 107343. (<a
href="https://doi.org/10.1016/j.neunet.2025.107343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is a precursor to Alzheimer’s disease (AD), and its progression involves complex pathogenic mechanisms. Specifically, disturbed by gene variants, the regulation of gene expression ultimately changes brain structure, resulting in the progression of brain diseases. However, the existing works rarely take these mechanisms into account when designing their diagnosis methods. Therefore, we propose a novel gradual multi-modal fusion framework to fuse representative data from each stage of disease progression in hybrid feature space, including single nucleotide polymorphism (SNP), gene expression (GE), and magnetic resonance imaging (MRI). Specifically, to integrate genetic sequence and expression data, we design a SNP-GE fusion module, which performs multi-modal fusion to obtain genetic embedding by considering the relation between SNP and GE. Compared with SNP-GE fusion, representation of genetic embedding and MRI have more obvious heterogeneity, especially correlation with disease. Therefore, we propose to align the manifold of genetic and imaging representations, which can explore the high-order relationship between imaging and genetic data in the presence of modal heterogeneity. Our proposed framework was validated using the Alzheimer’s Disease Neuroimaging Initiative dataset, and achieved diagnosis accuracy of 76.88%, 72.84%, 87.72%, and 95.00% for distinguishing MCI from control normal, lately MCI from early MCI, MCI from AD, and AD from control normal, respectively. Additionally, our proposed framework helps to identify some multi-modal biomarkers related to MCI progression. In summary, our proposed framework is effective not only for MCI diagnosis but also for guiding the further development of genetic and imaging-based brain studies. Our code is published at https://github.com/tianxu8822/workflow_MCI/tree/main/ .},
  archive      = {J_NN},
  author       = {Xu Tian and Hong-Dong Li and Hanhe Lin and Chao Li and Yu-Ping Wang and Harrison X. Bai and Wei Lan and Jin Liu},
  doi          = {10.1016/j.neunet.2025.107343},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107343},
  shortjournal = {Neural Netw.},
  title        = {Inspired by pathogenic mechanisms: A novel gradual multi-modal fusion framework for mild cognitive impairment diagnosis},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking exploration–exploitation trade-off in
reinforcement learning via cognitive consistency. <em>NN</em>,
<em>187</em>, 107342. (<a
href="https://doi.org/10.1016/j.neunet.2025.107342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploration–exploitation dilemma is one of the fundamental challenges in deep reinforcement learning (RL). Agents must strike a trade-off between making decisions based on current beliefs or gathering more information. Prior work mostly prefers devising sophisticated exploration methods to ensure accurate target Q-values or learn rewards and actions association, which may not be intelligent enough for sample efficiency. In this paper, we propose to rethink the trade-off between exploration and exploitation from the perspective of cognitive consistency: humans tend to think and behave in line with their existing knowledge structures (maintaining cognitive consistency), yielding satisfactory results within a brief timeframe. We argue that maintaining consistency, specifically through pessimistic exploration, within the context of optimal policy-oriented cognition, can improve efficiency without compromising performance. To this end, we propose a Cognitive Consistency (CoCo) framework. CoCo first leverages a self-imitating distribution correction approach to pursue cognition oriented toward the optimal policy. Then, it conservatively implements pessimistic exploration by extracting novel inconsistency-minimization objectives inspired by label distribution learning. We validate our framework across various standard off-policy RL tasks and show that maintaining cognitive consistency improves sample efficiency and performance. Code is available at https://github.com/DkING-lv6/CoCo .},
  archive      = {J_NN},
  author       = {Da Wang and Wei Wei and Lin Li and Xin Wang and Jiye Liang},
  doi          = {10.1016/j.neunet.2025.107342},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107342},
  shortjournal = {Neural Netw.},
  title        = {Rethinking exploration–exploitation trade-off in reinforcement learning via cognitive consistency},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving transferability of adversarial examples via
statistical attribution-based attacks. <em>NN</em>, <em>187</em>,
107341. (<a href="https://doi.org/10.1016/j.neunet.2025.107341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks are significant in uncovering vulnerabilities and assessing the robustness of deep neural networks (DNNs), offering profound insights into their internal mechanisms. Feature-level attacks, a potent approach, craft adversarial examples by extensively corrupting the intermediate-layer features of the source model during each iteration. However, it often has imprecise metrics to assess the significance of features and may impose constraints on the transferability of adversarial examples. To address these issues, this paper introduces the Statistical Attribution-based Attack (SAA) method, which emphasizes finding feature importance representations and refining optimization objectives, thereby achieving stronger attack performance. To calculate the Comprehensive Gradient for more accurate feature representation, we introduce the Region-wise Feature Disturbance and Gradient Information Aggregation, which can effectively disrupt the model’s attention focus areas. Subsequently, a statistical attribution-based approach is employed, leveraging the average feature information across layers to provide a more advantageous optimization objective. Experiments have validated the superiority of this method. Specifically, SAA improves the attack success rate by 9.3% compared with the second-best method. When combined with input transformation methods, it achieves an average success rate of 79.2% against eight leading defense models.},
  archive      = {J_NN},
  author       = {Hegui Zhu and Yanmeng Jia and Yue Yan and Ze Yang},
  doi          = {10.1016/j.neunet.2025.107341},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107341},
  shortjournal = {Neural Netw.},
  title        = {Improving transferability of adversarial examples via statistical attribution-based attacks},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised non-negative matrix factorization with
structure preserving for image clustering. <em>NN</em>, <em>187</em>,
107340. (<a href="https://doi.org/10.1016/j.neunet.2025.107340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning methods have wide applications thanks to the reasonable utilization for a part of label information of data. In recent years, non-negative matrix factorization (NMF) has received considerable attention because of its interpretability and practicality. Based on the advantages of semi-supervised learning and NMF, many semi-supervised NMF methods have been presented. However, these existing semi-supervised NMF methods construct a label matrix only containing elements 1 and 0 to represent the labeled data and further construct a label regularization, which neglects an intrinsic structure of NMF. To address the deficiency, in this paper, we propose a novel semi-supervised NMF method with structure preserving. Specifically, we first construct a new label matrix with weights and further construct a label constraint regularizer to both utilize the label information and maintain the intrinsic structure of NMF. Then, based on the label constraint regularizer, the basis images of labeled data are extracted for monitoring and modifying the basis images learning of all data by establishing a basis regularizer. Finally, incorporating the label constraint regularizer and the basis regularizer into NMF, we propose a new semi-supervised NMF method. To solve the optimization problem, a multiplicative updating algorithm is developed. The proposed method is applied to image clustering to test its performance. Experimental results on eight data sets demonstrate the effectiveness of the proposed method in contrast with state-of-the-art unsupervised and semi-supervised algorithms.},
  archive      = {J_NN},
  author       = {Wenjing Jing and Linzhang Lu and Weihua Ou},
  doi          = {10.1016/j.neunet.2025.107340},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107340},
  shortjournal = {Neural Netw.},
  title        = {Semi-supervised non-negative matrix factorization with structure preserving for image clustering},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing few-shot image classification through learnable
multi-scale embedding and attention mechanisms. <em>NN</em>,
<em>187</em>, 107339. (<a
href="https://doi.org/10.1016/j.neunet.2025.107339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of few-shot classification, the goal is to train a classifier using a limited number of samples while maintaining satisfactory performance. However, traditional metric-based methods exhibit certain limitations in achieving this objective. These methods typically rely on a single distance value between the query feature and support feature, thereby overlooking the contribution of shallow features. To overcome this challenge, we propose a novel approach in this paper. Our approach involves utilizing a multi-output embedding network that maps samples into distinct feature spaces. The proposed method extracts feature vectors at different stages, enabling the model to capture both global and abstract features. By utilizing these diverse feature spaces, our model enhances its performance. Moreover, employing a self-attention mechanism improves the refinement of features at each stage, leading to even more robust representations and improved overall performance. Furthermore, assigning learnable weights to each stage significantly improved performance and results. We conducted comprehensive evaluations on the MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way 5-shot scenarios. Additionally, we performed cross-domain tasks across eight benchmark datasets, achieving high accuracy in the testing domains. These evaluations demonstrate the efficacy of our proposed method in comparison to state-of-the-art approaches. https://github.com/FatemehAskari/MSENet},
  archive      = {J_NN},
  author       = {Fatemeh Askari and Amirreza Fateh and Mohammad Reza Mohammadi},
  doi          = {10.1016/j.neunet.2025.107339},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107339},
  shortjournal = {Neural Netw.},
  title        = {Enhancing few-shot image classification through learnable multi-scale embedding and attention mechanisms},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks with coarse- and fine-grained division
for mitigating label noise and sparsity. <em>NN</em>, <em>187</em>,
107338. (<a href="https://doi.org/10.1016/j.neunet.2025.107338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have gained considerable prominence in semi-supervised learning tasks in processing graph-structured data, primarily owing to their message-passing mechanism, which largely relies on the availability of clean labels. However, in real-world scenarios, labels on nodes of graphs are inevitably noisy and sparsely labeled, significantly degrading the performance of GNNs. Exploring robust GNNs for semi-supervised node classification in the presence of noisy and sparse labels remains a critical challenge. Therefore, we propose a novel G raph N eural N etwork with C oarse- and F ine- G rained D ivision for mitigating label sparsity and noise, namely GNN-CFGD. The key idea of GNN-CFGD is reducing the negative impact of noisy labels via coarse- and fine-grained division, along with graph reconstruction. Specifically, we first investigate the effectiveness of linking unlabeled nodes to cleanly labeled nodes, demonstrating that this approach is more effective in combating labeling noise than linking to potentially noisy labeled nodes. Based on this observation, we introduce a Gaussian Mixture Model (GMM) based on the memory effect to perform a coarse-grained division of the given labels into clean and noisy labels. Next, we propose a clean labels oriented link that connects unlabeled nodes to cleanly labeled nodes, aimed at mitigating label sparsity and promoting supervision propagation. Furthermore, to provide refined supervision for noisy labeled nodes and additional supervision for unlabeled nodes, we fine-grain the noisy labeled and unlabeled nodes into two candidate sets based on confidence, respectively. Extensive experiments on various datasets demonstrate the superior effectiveness and robustness of GNN-CFGD.},
  archive      = {J_NN},
  author       = {Shuangjie Li and Baoming Zhang and Jianqing Song and Gaoli Ruan and Chongjun Wang and Junyuan Xie},
  doi          = {10.1016/j.neunet.2025.107338},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107338},
  shortjournal = {Neural Netw.},
  title        = {Graph neural networks with coarse- and fine-grained division for mitigating label noise and sparsity},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-training EEG discrimination model with weakly
supervised sample construction: An age-based perspective on ASD
evaluation. <em>NN</em>, <em>187</em>, 107337. (<a
href="https://doi.org/10.1016/j.neunet.2025.107337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning for Electroencephalography (EEG) has become dominant in the tasks of discrimination and evaluation of brain disorders. However, despite its significant successes, this approach has long been facing challenges due to the limited availability of labeled samples and the individuality of subjects, particularly in complex scenarios such as Autism Spectrum Disorders (ASD). To facilitate the efficient optimization of EEG discrimination models in the face of these limitations, this study has developed a framework called STEM (Self-Training EEG Model). STEM accomplishes this by self-training the model, which involves initializing it with limited labeled samples and optimizing it with self-constructed samples. (1) Model initialization with multi-task learning: A multi-task model (MAC) comprising an AutoEncoder and a classifier offers guidance for subsequent pseudo-labeling. This guidance includes task-related latent EEG representations and prediction probabilities of unlabeled samples. The AutoEncoder, which consists of depth-separable convolutions and BiGRUs, is responsible for learning comprehensive EEG representations through the EEG reconstruction task. Meanwhile, the classifier, trained using limited labeled samples through supervised learning, directs the model’s attention towards capturing task-related features. (2) Model optimization aided by pseudo-labeled samples construction: Next, trustworthy pseudo-labels are assigned to the unlabeled samples, and this approach (PLASC) combines the sample’s distance relationship in the feature space mapped by the encoder with the sample’s predicted probability, using the initial MAC model as a reference. The constructed pseudo-labeled samples then support the self-training of MAC to learn individual information from new subjects, potentially enhancing the adaptation of the optimized model to samples from new subjects. The STEM framework has undergone an extensive evaluation, comparing it to state-of-the-art counterparts, using resting-state EEG data collected from 175 ASD-suspicious children spanning different age groups. The observed results indicate the following: (1) STEM achieves the best performance, with an accuracy of 88.33% and an F1-score of 87.24%, and (2) STEM’s multi-task learning capability outperforms supervised methods when labeled data is limited. More importantly, the use of PLASC improves the model’s performance in ASD discrimination across different age groups, resulting in an increase in accuracy (3%–8%) and F1-scores (4%–10%). These increments are approximately 6% higher than those achieved by the comparison methods.},
  archive      = {J_NN},
  author       = {Tengfei Gao and Dan Chen and Meiqi Zhou and Yaodong Wang and Yiping Zuo and Weiping Tu and Xiaoli Li and Jingying Chen},
  doi          = {10.1016/j.neunet.2025.107337},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107337},
  shortjournal = {Neural Netw.},
  title        = {Self-training EEG discrimination model with weakly supervised sample construction: An age-based perspective on ASD evaluation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse teacher–students for deep safe semi-supervised
learning under class mismatch. <em>NN</em>, <em>187</em>, 107336. (<a
href="https://doi.org/10.1016/j.neunet.2025.107336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning can significantly boost model performance by leveraging unlabeled data, particularly when labeled data is scarce. However, real-world unlabeled data often contain unseen-class samples, which can hinder the classification of seen classes. To address this issue, mainstream safe SSL methods suggest detecting and discarding unseen-class samples from unlabeled data. Nevertheless, these methods typically employ a single-model strategy to simultaneously tackle both the classification of seen classes and the detection of unseen classes. Our research indicates that such an approach may lead to conflicts during training, resulting in suboptimal model optimization. Inspired by this, we introduce a novel framework named Diverse Teacher–Students ( DTS ), which uniquely utilizes dual teacher–student models to individually and effectively handle these two tasks. DTS employs a novel uncertainty score to softly separate unseen-class and seen-class data from the unlabeled set, and intelligently creates an additional ( K +1)th class supervisory signal for training. By training both teacher–student models with all unlabeled samples, DTS can enhance the classification of seen classes while simultaneously improving the detection of unseen classes. Comprehensive experiments demonstrate that DTS surpasses baseline methods across a variety of datasets and configurations. Our code and models can be publicly accessible on the link https://github.com/Zhanlo/DTS .},
  archive      = {J_NN},
  author       = {Qikai Wang and Rundong He and Yongshun Gong and Chunxiao Ren and Haoliang Sun and Xiaoshui Huang and Yilong Yin},
  doi          = {10.1016/j.neunet.2025.107336},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107336},
  shortjournal = {Neural Netw.},
  title        = {Diverse Teacher–Students for deep safe semi-supervised learning under class mismatch},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive evaluation of pipelines for classification of
psychiatric disorders using multi-site resting-state fMRI datasets.
<em>NN</em>, <em>187</em>, 107335. (<a
href="https://doi.org/10.1016/j.neunet.2025.107335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective classification biomarkers that are developed using resting-state functional magnetic resonance imaging (rs-fMRI) data are expected to contribute to more effective treatment for psychiatric disorders. Unfortunately, no widely accepted biomarkers are available at present, partially because of the large variety of analysis pipelines for their development. In this study, we comprehensively evaluated analysis pipelines using a large-scale, multi-site fMRI dataset for major depressive disorder (MDD). We explored combinations of options in four sub-processes of the analysis pipelines: six types of brain parcellation, four types of functional connectivity (FC) estimations, three types of site-difference harmonization, and five types of machine-learning methods. A total of 360 different MDD classification biomarkers were constructed using the SRPBS dataset acquired with unified protocols (713 participants from four sites) as the discovery dataset, and datasets from other projects acquired with heterogeneous protocols (449 participants from four sites) were used for independent validation. We repeated the procedure after swapping the roles of the two datasets to identify superior pipelines, regardless of the discovery dataset. The classification results of the top 10 biomarkers showed high similarity, and weight similarity was observed between eight of the biomarkers, except for two that used both data-driven parcellation and FC computation. We applied the top 10 pipelines to the datasets of other psychiatric disorders (autism spectrum disorder and schizophrenia), and eight of the biomarkers exhibited sufficient classification performance for both disorders. Our results will be useful for establishing a standardized pipeline for classification biomarkers.},
  archive      = {J_NN},
  author       = {Yuji Takahara and Yuto Kashiwagi and Tomoki Tokuda and Junichiro Yoshimoto and Yuki Sakai and Ayumu Yamashita and Toshinori Yoshioka and Hidehiko Takahashi and Hiroto Mizuta and Kiyoto Kasai and Akira Kunimitsu and Naohiro Okada and Eri Itai and Hotaka Shinzato and Satoshi Yokoyama and Yoshikazu Masuda and Yuki Mitsuyama and Go Okada and Yasumasa Okamoto and Takashi Itahashi and Okito Yamashita},
  doi          = {10.1016/j.neunet.2025.107335},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107335},
  shortjournal = {Neural Netw.},
  title        = {Comprehensive evaluation of pipelines for classification of psychiatric disorders using multi-site resting-state fMRI datasets},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SHFormer: Dynamic spectral filtering convolutional neural
network and high-pass kernel generation transformer for adaptive MRI
reconstruction. <em>NN</em>, <em>187</em>, 107334. (<a
href="https://doi.org/10.1016/j.neunet.2025.107334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention Mechanism (AM) selectively focuses on essential information for imaging tasks and captures relationships between regions from distant pixel neighborhoods to compute feature representations. Accelerated magnetic resonance image (MRI) reconstruction can benefit from AM, as the imaging process involves acquiring Fourier domain measurements that influence the image representation in a non-local manner. However, AM-based models are more adept at capturing low-frequency information and have limited capacity in constructing high-frequency representations, restricting the models to smooth reconstruction. Secondly, AM-based models need mode-specific retraining for multimodal MRI data as their knowledge is restricted to local contextual variations within modes that might be inadequate to capture the diverse transferable features across heterogeneous data domains. To address these challenges, we propose a neuromodulation-based discriminative multi-spectral AM for scalable MRI reconstruction, that can (i) propagate the context-aware high-frequency details for high-quality image reconstruction, and (ii) capture features reusable to deviated unseen domains in multimodal MRI, to offer high practical value for the healthcare industry and researchers. The proposed network consists of a spectral filtering convolutional neural network to capture mode-specific transferable features to generalize to deviated MRI data domains and a dynamic high-pass kernel generation transformer that focuses on high-frequency details for improved reconstruction. We have evaluated our model on various aspects, such as comparative studies in supervised and self-supervised learning, diffusion model-based training, closed-set and open-set generalization under heterogeneous MRI data, and interpretation-based analysis. Our results show that the proposed method offers scalable and high-quality reconstruction with best improvement margins of ∼ 1 dB in PSNR and ∼ 0.01 in SSIM under unseen scenarios. Our code is available at https://github.com/sriprabhar/SHFormer .},
  archive      = {J_NN},
  author       = {Sriprabha Ramanarayanan and Rahul G.S. and Mohammad Al Fahim and Keerthi Ram and Ramesh Venkatesan and Mohanasankar Sivaprakasam},
  doi          = {10.1016/j.neunet.2025.107334},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107334},
  shortjournal = {Neural Netw.},
  title        = {SHFormer: Dynamic spectral filtering convolutional neural network and high-pass kernel generation transformer for adaptive MRI reconstruction},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ternary spike-based neuromorphic signal processing system.
<em>NN</em>, <em>187</em>, 107333. (<a
href="https://doi.org/10.1016/j.neunet.2025.107333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have been successfully implemented across various signal processing fields, resulting in significant enhancements in performance. However, DNNs generally require substantial computational resources, leading to significant economic costs and posing challenges for their deployment on resource-constrained edge devices. In this study, we take advantage of spiking neural networks (SNNs) and quantization technologies to develop an energy-efficient and lightweight neuromorphic signal processing system. Our system is characterized by two principal innovations: a threshold-adaptive encoding (TAE) method and a quantized ternary SNN (QT-SNN). The TAE method can efficiently encode time-varying analog signals into sparse ternary spike trains, thereby reducing energy and memory demands for signal processing. QT-SNN, compatible with ternary spike trains from the TAE method, quantifies both membrane potentials and synaptic weights to reduce memory requirements while maintaining performance. Extensive experiments are conducted on two typical signal-processing tasks: speech and electroencephalogram recognition. The results demonstrate that our neuromorphic signal processing system achieves state-of-the-art (SOTA) performance with a 94% reduced memory requirement. Furthermore, through theoretical energy consumption analysis, our system shows 7 . 5 × energy saving compared to other SNN works. The efficiency and efficacy of the proposed system highlight its potential as a promising avenue for energy-efficient signal processing.},
  archive      = {J_NN},
  author       = {Shuai Wang and Dehao Zhang and Ammar Belatreche and Yichen Xiao and Hongyu Qing and Wenjie Wei and Malu Zhang and Yang Yang},
  doi          = {10.1016/j.neunet.2025.107333},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107333},
  shortjournal = {Neural Netw.},
  title        = {Ternary spike-based neuromorphic signal processing system},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-guided feature fusion network with
knowledge-inspired attention mechanism for multi-source remote sensing
classification. <em>NN</em>, <em>187</em>, 107332. (<a
href="https://doi.org/10.1016/j.neunet.2025.107332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land use and land cover (LULC) classification is a popular research area in remote sensing. The information of single-modal data is insufficient for accurate classification, especially in complex scenes, while the complementarity of multi-modal data such as hyperspectral images (HSIs) and light detection and ranging (LiDAR) data could effectively improve classification performance. The attention mechanism has recently been widely used in multi-modal LULC classification methods to achieve better feature representation. However, the knowledge of data is insufficiently considered in these methods, such as spectral mixture in HSIs and inconsistent spatial scales of different categories in LiDAR data. Moreover, multi-modal features contain different physical attributes, HSI features can represent spectral information of several channels while LiDAR features focus on elevation information at the spatial dimension. Ignoring these attributes, feature fusion may introduce redundant information and effect detrimentally on classification. In this paper, we propose an attribute-guided feature fusion network with knowledge-inspired attention mechanisms, named AFNKA. Focusing on the spectral characteristics of HSI and elevation information of LiDAR data, we design the knowledge-inspired attention mechanism to explore enhanced features. Especially, a novel adaptive cosine estimator (ACE) based attention module is presented to learn features with more discriminability, which adequately utilizes the spatial–spectral correlation of HSI mixed pixels. In the fusion stage, two novel attribute-guided fusion modules are developed to selectively aggregate multi-modal features, which sufficiently exploit the correlations between the spatial–spectral property of HSI features and the spatial-elevation property of LiDAR features. Experimental results on several multi-source datasets quantitatively indicate that the proposed AFNKA significantly outperforms the state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Xiao Pan and Changzhe Jiao and Bo Yang and Hao Zhu and Jinjian Wu},
  doi          = {10.1016/j.neunet.2025.107332},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107332},
  shortjournal = {Neural Netw.},
  title        = {Attribute-guided feature fusion network with knowledge-inspired attention mechanism for multi-source remote sensing classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential stability of infinite-dimensional impulsive
stochastic systems with poisson jumps under aperiodically intermittent
control. <em>NN</em>, <em>187</em>, 107331. (<a
href="https://doi.org/10.1016/j.neunet.2025.107331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of mean square exponential stability (ES) for a class of impulsive stochastic infinite-dimensional systems with Poisson jumps (ISIDSP) using aperiodically intermittent control (AIC). It provides a detailed analysis of impulsive disturbances, and the related inequalities are given for the two cases when the impulse perturbation occurs at the start time points of the control and rest intervals or non-startpoints, respectively. Additionally, in virtue of Yosida approximating systems, combining with the Lyapunov method, graph theory and the above inequalities, criteria for ES of the above impulsive stochastic infinite-dimensional systems are established under AIC for these two perturbation scenarios. These criteria elucidate the effects of the impulsive perturbation strength, the ratio of control period, to rest period, and network topology on ES. Finally, the theoretical results are applied to a class of neural networks with reaction–diffusion processes, and the effectiveness of the findings is validated through numerical simulations.},
  archive      = {J_NN},
  author       = {Yiqun Liu and Lili Chen and Yanfeng Zhao and Zhen Wang},
  doi          = {10.1016/j.neunet.2025.107331},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107331},
  shortjournal = {Neural Netw.},
  title        = {Exponential stability of infinite-dimensional impulsive stochastic systems with poisson jumps under aperiodically intermittent control},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PanoGen++: Domain-adapted text-guided panoramic environment
generation for vision-and-language navigation. <em>NN</em>,
<em>187</em>, 107320. (<a
href="https://doi.org/10.1016/j.neunet.2025.107320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-language navigation (VLN) tasks require agents to navigate three-dimensional environments guided by natural language instructions, offering substantial potential for diverse applications. However, the scarcity of training data impedes progress in this field. This paper introduces PanoGen++, a novel framework that addresses this limitation by generating varied and pertinent panoramic environments for VLN tasks. PanoGen++ incorporates pre-trained diffusion models with domain-specific fine-tuning, employing parameter-efficient techniques such as low-rank adaptation to minimize computational costs. We investigate two settings for environment generation: masked image inpainting and recursive image outpainting. The former maximizes novel environment creation by inpainting masked regions based on textual descriptions, while the latter facilitates agents’ learning of spatial relationships within panoramas. Empirical evaluations on room-to-room (R2R), room-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN) datasets reveal significant performance enhancements: a 2.44% increase in success rate on the R2R test leaderboard, a 0.63% improvement on the R4R validation unseen set, and a 0.75-meter enhancement in goal progress on the CVDN validation unseen set. PanoGen++ augments the diversity and relevance of training environments, resulting in improved generalization and efficacy in VLN tasks.},
  archive      = {J_NN},
  author       = {Sen Wang and Dongliang Zhou and Liang Xie and Chao Xu and Ye Yan and Erwei Yin},
  doi          = {10.1016/j.neunet.2025.107320},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107320},
  shortjournal = {Neural Netw.},
  title        = {PanoGen++: Domain-adapted text-guided panoramic environment generation for vision-and-language navigation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learn the global prompt in the low-rank tensor space for
heterogeneous federated learning. <em>NN</em>, <em>187</em>, 107319. (<a
href="https://doi.org/10.1016/j.neunet.2025.107319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning collaborates with multiple clients to train a global model, enhancing the model generalization while allowing the local data transmission-free and security. However, federated learning currently faces three intractable challenges: (1) The large number of model parameters result in an excessive communication burden. (2) The non-independently and identically distributed local data induces the degradation of global model. (3) The model heterogeneity renders traditional federated aggregation infeasible. To dissipate the three difficulties, we propose to learn the global prompt in the low-rank tensor space (FedGPT) for heterogeneous federated learning. Specifically, we employ the prompts rather than the model parameters as the carrier of local knowledge to achieve the information interaction between multiple clients. Since the prompts only have a very small number of variables, the communication volume is greatly reduced. To cope with the data heterogeneity, the prompts from different clients are stacked into the third-order tensors, on which the tensor singular value decomposition is performed to extract the global information. Furthermore, the proposed FedGPT possesses the ability to handle the model heterogeneity, the local models of different sizes can transfer the knowledge with the help of the prompts to improve the performance. Extensive experiments on three real-world datasets are conducted. Overall, FedGPT outperforms other state-of-the-art compared methods by up to 13.21%, and achieves less than 3% of communication volume of FedAvg, demonstrating the superiority of the proposed FedGPT.},
  archive      = {J_NN},
  author       = {Lele Fu and Sheng Huang and Yuecheng Li and Chuan Chen and Chuanfu Zhang and Zibin Zheng},
  doi          = {10.1016/j.neunet.2025.107319},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107319},
  shortjournal = {Neural Netw.},
  title        = {Learn the global prompt in the low-rank tensor space for heterogeneous federated learning},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCPI-HRL: Human causal perception and inference-driven
hierarchical reinforcement learning. <em>NN</em>, <em>187</em>, 107318.
(<a href="https://doi.org/10.1016/j.neunet.2025.107318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dependency on extensive expert knowledge for defining subgoals in hierarchical reinforcement learning (HRL) restricts the training efficiency and adaptability of HRL agents in complex, dynamic environments. Inspired by human-guided causal discovery skills, we proposed a novel method, Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning (HCPI-HRL), designed to infer diverse, effective subgoal structures as intrinsic rewards and incorporate critical objects from dynamic environmental states using stable causal relationships. The HCPI-HRL method is supposed to guide an agent’s exploration direction and promote the reuse of learned subgoal structures across different tasks. Our designed HCPI-HRL comprises two levels: the top level operates as a meta controller, assigning subgoals discovered based on human-driven causal critical object perception and causal structure inference; the bottom level employs the Proximal Policy Optimisation (PPO) algorithm to accomplish the assigned subgoals. Experiments conducted across discrete and continuous control environments demonstrated that HCPI-HRL outperforms benchmark methods such as hierarchical and adjacency PPO in terms of training efficiency, exploration capability, and transferability. Our research extends the potential of HRL methods incorporating human-guided causal modelling to infer the effective relationships across subgoals, enhancing the agent’s capability to learn efficient policies in dynamic environments with sparse reward signals.},
  archive      = {J_NN},
  author       = {Bin Chen and Zehong Cao and Wolfgang Mayer and Markus Stumptner and Ryszard Kowalczyk},
  doi          = {10.1016/j.neunet.2025.107318},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107318},
  shortjournal = {Neural Netw.},
  title        = {HCPI-HRL: Human causal perception and inference-driven hierarchical reinforcement learning},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing user preferences by social networks: A
condition-guided social recommendation model for mitigating popularity
bias. <em>NN</em>, <em>187</em>, 107317. (<a
href="https://doi.org/10.1016/j.neunet.2025.107317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social recommendation models weave social interactions into their design to provide uniquely personalized recommendation results for users. However, social networks not only amplify the popularity bias in recommendation models, resulting in more frequent recommendation of hot items and fewer long-tail items, but also include a substantial amount of redundant information that is essentially meaningless for the model’s performance. Existing social recommendation models often integrate the entire social network directly, with little effort to filter or adjust social information to mitigate popularity bias introduced by the social network. In this paper, we propose a Condition-Guided Social Recommendation Model (named CGSoRec) to mitigate the model’s popularity bias by denoising the social network and adjusting the weights of user’s social preferences. More specifically, CGSoRec first includes a Condition-Guided Social Denoising Model (CSD) to remove redundant social relations in the social network for capturing users’ social preferences with items more precisely. Then, CGSoRec calculates users’ social preferences based on denoised social network and adjusts the weights in users’ social preferences to make them can counteract the popularity bias present in the recommendation model. At last, CGSoRec includes a Condition-Guided Diffusion Recommendation Model (CGD) to introduce the adjusted social preferences as conditions to control the recommendation results for a debiased direction. Comprehensive experiments on three real-world datasets demonstrate the effectiveness of our proposed method. The anonymous code is in: https://anonymous.4open.science/r/CGSoRec-2B72 .},
  archive      = {J_NN},
  author       = {Xin He and Wenqi Fan and Ruobing Wang and Yili Wang and Ying Wang and Shirui Pan and Xin Wang},
  doi          = {10.1016/j.neunet.2025.107317},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107317},
  shortjournal = {Neural Netw.},
  title        = {Balancing user preferences by social networks: A condition-guided social recommendation model for mitigating popularity bias},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADAMT: Adaptive distributed multi-task learning for
efficient image recognition in mobile ad-hoc networks. <em>NN</em>,
<em>187</em>, 107316. (<a
href="https://doi.org/10.1016/j.neunet.2025.107316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning in mobile adhoc networks faces significant challenges due to the limited computational resources of devices, non-IID data distribution, and dynamic network topology. Existing approaches often rely on centralized coordination and stable network conditions, which may not be feasible in practice. To address these issues, we propose an adaptive distributed multi-task learning framework called ADAMT for efficient image recognition in resource-constrained mobile ad hoc networks. ADAMT introduces three key innovations: (1) a feature expansion mechanism that enhances the expressiveness of local models by leveraging task-specific information; (2) a deep hashing technique that enables efficient on-device retrieval and multi-task fusion; and (3) an adaptive communication strategy that dynamically adjusts the model updating process based on network conditions and node reliability. The proposed framework allows each device to perform personalized model training on its local dataset while collaboratively updating the shared parameters with neighboring nodes. Extensive experiments on the ImageNet dataset demonstrate the superiority of ADAMT over state-of-the-art methods. ADAMT achieves a top-1 accuracy of 0.867, outperforming existing distributed learning approaches. Moreover, ADAMT significantly reduces the communication overhead and accelerates the convergence speed by 2.69 times compared to traditional distributed SGD. The adaptive communication strategy effectively balances the trade-off between model performance and resource consumption, making ADAMT particularly suitable for resource-constrained environments. Our work sheds light on the design of efficient and robust distributed learning algorithms for mobile adhoc networks and paves the way for deploying advanced machine learning applications on edge devices.},
  archive      = {J_NN},
  author       = {Jia Zhao and Wei Zhao and Yunan Zhai and Liyuan Zhang and Yan Ding},
  doi          = {10.1016/j.neunet.2025.107316},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107316},
  shortjournal = {Neural Netw.},
  title        = {ADAMT: Adaptive distributed multi-task learning for efficient image recognition in mobile ad-hoc networks},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FingerPoseNet: A finger-level multitask learning network
with residual feature sharing for 3D hand pose estimation. <em>NN</em>,
<em>187</em>, 107315. (<a
href="https://doi.org/10.1016/j.neunet.2025.107315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand pose estimation approaches commonly rely on shared hand feature maps to regress the 3D locations of all hand joints. Subsequently, they struggle to enhance finger-level features which are invaluable in capturing joint-to-finger associations and articulations. To address this limitation, we propose a finger-level multitask learning network with residual feature sharing, named FingerPoseNet, for accurate 3D hand pose estimation from a depth image. FingerPoseNet comprises three stages: (a) a shared base feature map extraction backbone based on pre-trained ResNet-50; (b) a finger-level multitask learning stage that extracts and enhances feature maps for each finger and the palm; and (c) a multitask fusion layer for consolidating the estimation results obtained by each subtask. We exploit multitask learning by decoupling the hand pose estimation task into six subtasks dedicated to each finger and palm. Each subtask is responsible for subtask-specific feature extraction, enhancement, and 3D keypoint regression. To enhance subtask-specific features, we propose a residual feature-sharing approach scaled up to mine supplementary information from all subtasks. Experiments performed on five challenging public hand pose datasets, including ICVL, NYU, MSRA, Hands-2019-Task1, and HO3D-v3 demonstrate significant improvements in accuracy compared with state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {Tekie Tsegay Tewolde and Ali Asghar Manjotho and Prodip Kumar Sarker and Zhendong Niu},
  doi          = {10.1016/j.neunet.2025.107315},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107315},
  shortjournal = {Neural Netw.},
  title        = {FingerPoseNet: A finger-level multitask learning network with residual feature sharing for 3D hand pose estimation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general debiasing framework with counterfactual reasoning
for multimodal public speaking anxiety detection. <em>NN</em>,
<em>187</em>, 107314. (<a
href="https://doi.org/10.1016/j.neunet.2025.107314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Public Speaking Anxiety Detection (MPSAD), which aims to identify the anxiety states of learners, has attracted widespread attention. Unfortunately, the current MPSAD task inevitably suffers from the impact of latent different types of multimodal hybrid biases, such as context bias, label bias and keyword bias. Models may rely on these biases as shortcuts, preventing them from fully utilizing all three modalities to learn multimodal knowledge. Existing methods primarily focus on addressing specific types of biases, but anticipating bias types when designing these methods is challenging, as we cannot foresee all possible biases. To tackle this issue, we propose a General Multimodal Counterfactual Reasoning debiasing framework (GMCR), which eliminates multimodal hybrid biases from a unified causal perspective. Specifically, this plug-and-play debiasing framework removes multimodal hybrid biases by disentangling causal and biased features and capturing adverse effects via a counterfactual branch. It then subtracts spurious correlations during inference for unbiased predictions. Due to the challenge of collecting speech video data, there are currently limited high-quality datasets available for the MPSAD task. To overcome this scarcity, we create a new large-scale fine-grained Multimodal English Public Speaking Anxiety (ME-PSA) dataset. Extensive experiments on our ME-PSA and two benchmarks demonstrate the superiority of our proposed framework, with improvements of over 2.00% in accuracy and 4.00% in F1 score compared to the vanilla SOTA baselines. 1},
  archive      = {J_NN},
  author       = {Tingting Zhang and Yangfu Zhu and Bin Wu and Chunping Zheng and Jiachen Tan and Zihua Xiong},
  doi          = {10.1016/j.neunet.2025.107314},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107314},
  shortjournal = {Neural Netw.},
  title        = {A general debiasing framework with counterfactual reasoning for multimodal public speaking anxiety detection},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph neural network with adaptive relation
reconstruction. <em>NN</em>, <em>187</em>, 107313. (<a
href="https://doi.org/10.1016/j.neunet.2025.107313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological structures of real-world graphs often exhibit heterogeneity involving diverse nodes and relation types. In recent years, heterogeneous graph learning methods utilizing meta-paths to capture composite relations and guide neighbor selection have garnered considerable attention. However, meta-path based approaches may establish connections between nodes of different categories while overlooking relations between nodes of the same category, decreasing the quality of node embeddings. In light of this, this paper proposes a Heterogeneous Graph Neural Network with Adaptive Relation Reconstruction (HGNN-AR 2 ) that adaptively adjusts the relations to alleviate connection deficiencies and heteromorphic issues. HGNN-AR 2 is grounded on distinct connections derived from multiple meta-paths. By examining the homomorphic correlations of latent features from each meta-path, we reshape the cross-node connections to explore the pertinent latent relations. Through the relation reconstruction, we unveil unique connections reflected by each meta-path and incorporate them into graph convolutional networks for more comprehensive representations. The proposed model is evaluated on various benchmark heterogeneous graph datasets, demonstrating superior performance compared to state-of-the-art competitors.},
  archive      = {J_NN},
  author       = {Weihong Lin and Zhaoliang Chen and Yuhong Chen and Shiping Wang},
  doi          = {10.1016/j.neunet.2025.107313},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107313},
  shortjournal = {Neural Netw.},
  title        = {Heterogeneous graph neural network with adaptive relation reconstruction},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ABVS breast tumour segmentation via integrating CNN with
dilated sampling self-attention and feature interaction transformer.
<em>NN</em>, <em>187</em>, 107312. (<a
href="https://doi.org/10.1016/j.neunet.2025.107312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the rapid increase in breast cancer incidence, the Automated Breast Volume Scanner (ABVS) is developed to screen breast tumours efficiently and accurately. However, reviewing ABVS images is a challenging task owing to the significant variations in sizes and shapes of breast tumours. We propose a novel 3D segmentation network (i.e., DST-C) that combines a convolutional neural network (CNN) with a dilated sampling self-attention Transformer (DST). In our network, the global features extracted from the DST branch are guided by the detailed local information provided by the CNN branch, which adapts to the diversity of tumour size and morphology. For medical images, especially ABVS images, the scarcity of annotation leads to difficulty in model training. Therefore, a self-supervised learning method based on a dual-path approach for mask image modelling is introduced to generate valuable representations of images. In addition, a unique postprocessing method is proposed to reduce the false-positive rate and improve the sensitivity simultaneously. The experimental results demonstrate that our model has achieved promising 3D segmentation and detection performance using our in-house dataset. Our code is available at: https://github.com/magnetliu/dstc-net .},
  archive      = {J_NN},
  author       = {Yiyao Liu and Jinyao Li and Yi Yang and Cheng Zhao and Yongtao Zhang and Peng Yang and Lei Dong and Xiaofei Deng and Ting Zhu and Tianfu Wang and Wei Jiang and Baiying Lei},
  doi          = {10.1016/j.neunet.2025.107312},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107312},
  shortjournal = {Neural Netw.},
  title        = {ABVS breast tumour segmentation via integrating CNN with dilated sampling self-attention and feature interaction transformer},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual selective fusion transformer network for hyperspectral
image classification. <em>NN</em>, <em>187</em>, 107311. (<a
href="https://doi.org/10.1016/j.neunet.2025.107311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer has achieved satisfactory results in the field of hyperspectral image (HSI) classification. However, existing Transformer models face two key challenges when dealing with HSI scenes characterized by diverse land cover types and rich spectral information: (1) A fixed receptive field overlooks the effective contextual scales required by various HSI objects; (2) invalid self-attention features in context fusion affect model performance. To address these limitations, we propose a novel Dual Selective Fusion Transformer Network (DSFormer) for HSI classification. DSFormer achieves joint spatial and spectral contextual modeling by flexibly selecting and fusing features across different receptive fields, effectively reducing unnecessary information interference by focusing on the most relevant spatial–spectral tokens. Specifically, we design a Kernel Selective Fusion Transformer Block (KSFTB) to learn an optimal receptive field by adaptively fusing spatial and spectral features across different scales, enhancing the model’s ability to accurately identify diverse HSI objects. Additionally, we introduce a Token Selective Fusion Transformer Block (TSFTB), which strategically selects and combines essential tokens during the spatial–spectral self-attention fusion process to capture the most crucial contexts. Extensive experiments conducted on four benchmark HSI datasets demonstrate that the proposed DSFormer significantly improves land cover classification accuracy, outperforming existing state-of-the-art methods. Specifically, DSFormer achieves overall accuracies of 96.59%, 97.66%, 95.17%, and 94.59% in the Pavia University, Houston, Indian Pines, and Whu-HongHu datasets, respectively, reflecting improvements of 3.19%, 1.14%, 0.91%, and 2.80% over the previous model. The code will be available online at https://github.com/YichuXu/DSFormer .},
  archive      = {J_NN},
  author       = {Yichu Xu and Di Wang and Lefei Zhang and Liangpei Zhang},
  doi          = {10.1016/j.neunet.2025.107311},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107311},
  shortjournal = {Neural Netw.},
  title        = {Dual selective fusion transformer network for hyperspectral image classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting face forgery detection towards generalization.
<em>NN</em>, <em>187</em>, 107310. (<a
href="https://doi.org/10.1016/j.neunet.2025.107310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery detection aims to distinguish AI generated fake faces with real faces. With the rapid development of face forgery creation algorithms, a large number of generative models have been proposed, which gradually reduce the local distortion phenomenon or the specific frequency traces in these models. At the same time, in the process of face data compression and transmission, distortion phenomenon and specific frequency cues could be eliminated, which brings severe challenges to the performance and generalization ability of face forgery detection. To promote the progress on face forgery detection research towards generalization, we present the first comprehensive overview and in-depth analysis of the generalizable face forgery detection methods. We categorize the target of generalizable face forgery detection into the robustness on novel and unknown forged images, and robustness on damaged low-quality images. We discuss representative generalization strategies including the aspects of data augmentation, multi-source learning, fingerprints detection, feature enhancement, temporal analysis, vision-language detection. We summarize the widely used datasets and the generalization performance of state-of-the-art methods in terms of robustness to novel unknown forgery as well as damaged quality forgery types. Finally, we discuss under-investigated open issues on face forgery detection towards generalization in six directions, including building a new generation of datasets, extracting strong forgery cues, considering identity features in face forgery detection, security and fairness of forgery detectors, the potential of large models in forgery detection and test-time adaptation. Our revisit of face forgery detection towards generalization will help promote the research and application of face forgery detection on real-world unconstrained conditions in the future.},
  archive      = {J_NN},
  author       = {Chunlei Peng and Tao Chen and Decheng Liu and Huiqing Guo and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.neunet.2025.107310},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107310},
  shortjournal = {Neural Netw.},
  title        = {Revisiting face forgery detection towards generalization},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRTN: Dual relation transformer network with feature erasure
and contrastive learning for multi-label image classification.
<em>NN</em>, <em>187</em>, 107309. (<a
href="https://doi.org/10.1016/j.neunet.2025.107309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of multi-label image classification (MLIC) task is to simultaneously identify multiple objects present in an image. Several researchers directly flatten 2D feature maps into 1D grid feature sequences, and utilize Transformer encoder to capture the correlations of grid features to learn object relationships. Although obtaining promising results, these Transformer-based methods lose spatial information. In addition, current attention-based models often focus only on salient feature regions, but ignore other potential useful features that contribute to MLIC task. To tackle these problems, we present a novel D ual R elation T ransformer N etwork ( DRTN ) for MLIC task, which can be trained in an end-to-end manner. Concretely, to compensate for the loss of spatial information of grid features resulting from the flattening operation, we adopt a grid aggregation scheme to generate pseudo-region features, which does not need to make additional expensive annotations to train object detector. Then, a new dual relation enhancement (DRE) module is proposed to capture correlations between objects using two different visual features, thereby complementing the advantages provided by both grid and pseudo-region features. After that, we design a new feature enhancement and erasure (FEE) module to learn discriminative features and mine additional potential valuable features. By using attention mechanism to discover the most salient feature regions and removing them with region-level erasure strategy, our FEE module is able to mine other potential useful features from the remaining parts. Further, we devise a novel contrastive learning (CL) module to encourage the foregrounds of salient and potential features to be closer, while pushing their foregrounds further away from background features. This manner compels our model to learn discriminative and valuable features more comprehensively. Extensive experiments demonstrate that DRTN method surpasses current MLIC models on three challenging benchmarks, i.e. , MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE datasets.},
  archive      = {J_NN},
  author       = {Wei Zhou and Kang Lin and Zhijie Zheng and Dihu Chen and Tao Su and Haifeng Hu},
  doi          = {10.1016/j.neunet.2025.107309},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107309},
  shortjournal = {Neural Netw.},
  title        = {DRTN: Dual relation transformer network with feature erasure and contrastive learning for multi-label image classification},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural network measures reveal the emergence of
heavy-tailed degree distributions in lottery ticket multilayer
perceptrons. <em>NN</em>, <em>187</em>, 107308. (<a
href="https://doi.org/10.1016/j.neunet.2025.107308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) were originally modeled after their biological counterparts, but have since conceptually diverged in many ways. The resulting network architectures are not well understood, and furthermore, we lack the quantitative tools to characterize their structures. Network science provides an ideal mathematical framework with which to characterize systems of interacting components, and has transformed our understanding across many domains, including the mammalian brain. Yet, little has been done to bring network science to ANNs. In this work, we propose tools that leverage and adapt network science methods to measure both global- and local-level characteristics of ANNs. Specifically, we focus on the structures of efficient multilayer perceptrons as a case study, which are sparse and systematically pruned such that they share many characteristics with real-world networks. We use adapted network science metrics to show that the pruning process leads to the emergence of a spanning subnetwork (lottery ticket multilayer perceptrons) with complex architecture. This complex network exhibits global and local characteristics, including heavy-tailed nodal degree distributions and dominant weighted pathways, that mirror patterns observed in human neuronal connectivity. Furthermore, alterations in network metrics precede catastrophic decay in performance as the network is heavily pruned. This network science-driven approach to the analysis of artificial neural networks serves as a valuable tool to establish and improve biological fidelity, increase the interpretability, and assess the performance of artificial neural networks. Significance Statement Artificial neural network architectures have become increasingly complex, often diverging from their biological counterparts in many ways. To design plausible “brain-like” architectures, whether to advance neuroscience research or to improve explainability, it is essential that these networks optimally resemble their biological counterparts. Network science tools offer valuable information about interconnected systems, including the brain, but have not attracted much attention for analyzing artificial neural networks. Here, we present the significance of our work: •We adapt network science tools to analyze the structural characteristics of artificial neural networks. •We demonstrate that organizational patterns similar to those observed in the mammalian brain emerge through the pruning process alone. The convergence on these complex network features in both artificial neural networks and biological brain networks is compelling evidence for their optimality in information processing capabilities. •Our approach is a significant first step towards a network science-based understanding of artificial neural networks, and has the potential to shed light on the biological fidelity of artificial neural networks.},
  archive      = {J_NN},
  author       = {Chris Kang and Jasmine A. Moore and Samuel Robertson and Matthias Wilms and Emma K. Towlson and Nils D. Forkert},
  doi          = {10.1016/j.neunet.2025.107308},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107308},
  shortjournal = {Neural Netw.},
  title        = {Structural network measures reveal the emergence of heavy-tailed degree distributions in lottery ticket multilayer perceptrons},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrivCore: Multiplication-activation co-reduction for
efficient private inference. <em>NN</em>, <em>187</em>, 107307. (<a
href="https://doi.org/10.1016/j.neunet.2025.107307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The marriage of deep neural network (DNN) and secure 2-party computation (2PC) enables private inference (PI) on the encrypted client-side data and server-side models with both privacy and accuracy guarantees, coming at the cost of orders of magnitude communication and latency penalties. Prior works on designing PI-friendly network architectures are confined to mitigating the overheads associated with non-linear (e.g., ReLU) operations, assuming other linear computations are free. Recent works have shown that linear convolutions can no longer be ignored and are responsible for the majority of communication in PI protocols. In this work, we present PrivCore , a framework that jointly optimizes the alternating linear and non-linear DNN operators via a careful co-design of sparse Winograd convolution and fine-grained activation reduction, to improve high-efficiency ciphertext computation without impacting the inference precision. Specifically, being aware of the incompatibility between the spatial pruning and Winograd convolution, we propose a two-tiered Winograd-aware structured pruning method that removes spatial filters and Winograd vectors from coarse to fine-grained for multiplication reduction, both of which are specifically optimized for Winograd convolution in a structured pattern. PrivCore further develops a novel sensitivity-based differentiable activation approximation to automate the selection of ineffectual ReLUs and polynomial options. PrivCore also supports the dynamic determination of coefficient-adaptive polynomial replacement to mitigate the accuracy degradation. Extensive experiments on various models and datasets consistently validate the effectiveness of PrivCore , achieving 2 . 2 × communication reduction with 1.8% higher accuracy compared with SENet (ICLR 2023) on CIFAR-100, and 2 . 0 × total communication reduction with iso-accuracy compared with CoPriv (NeurIPS 2023) on ImageNet.},
  archive      = {J_NN},
  author       = {Zhi Pang and Lina Wang and Fangchao Yu and Kai Zhao and Bo Zeng and Shuwang Xu},
  doi          = {10.1016/j.neunet.2025.107307},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107307},
  shortjournal = {Neural Netw.},
  title        = {PrivCore: Multiplication-activation co-reduction for efficient private inference},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DuPt: Rehearsal-based continual learning with dual prompts.
<em>NN</em>, <em>187</em>, 107306. (<a
href="https://doi.org/10.1016/j.neunet.2025.107306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rehearsal-based continual learning methods usually involve reviewing a small number of representative samples to enable the network to learn new contents while retaining old knowledge. However, existing works overlook two crucial factors: (1) While the network prioritizes learning new data at incremental stages, it exhibits weaker generalization capabilities when trained individually on limited samples from specific categories, in contrast to training on large-scale samples across multiple categories simultaneously. (2) Knowledge distillation of a limited set of old samples can transfer certain existing knowledge, but imposing strong constraints may hinder knowledge transfer and restrict the ability of the network from the current stage to capture fresh knowledge. To alleviate these issues, we propose a rehearsal-based continual learning method with dual prompts, termed DuPt. First, we propose an input-aware prompt, an input-level cue that utilizes an input prior to querying for valid cue information. These hints serve as an additional complement to help the input samples generate more rational and diverse distributions. Second, we introduce a proxy feature prompt, a feature-level hint that bridges the knowledge gap between the teacher and student models to maintain consistency in the feature transfer process, reinforcing feature plasticity and stability. This is because differences in network features between the new and old incremental stages could affect the generalization of their new models if strictly aligned. Our proposed prompt can act as a consistency regularization to avoid feature conflicts caused by the differences between network features. Extensive experiments validate the effectiveness of our method, which can seamlessly integrate with existing methods, leading to performance improvements.},
  archive      = {J_NN},
  author       = {Shengqin Jiang and Daolong Zhang and Fengna Cheng and Xiaobo Lu and Qingshan Liu},
  doi          = {10.1016/j.neunet.2025.107306},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107306},
  shortjournal = {Neural Netw.},
  title        = {DuPt: Rehearsal-based continual learning with dual prompts},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex quantized minimum error entropy with fiducial
points: Theory and application in model regression. <em>NN</em>,
<em>187</em>, 107305. (<a
href="https://doi.org/10.1016/j.neunet.2025.107305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum error entropy with fiducial points (MEEF) has gained significant attention due to its excellent performance in mitigating the adverse effects of non-Gaussian noise in the fields of machine learning and signal processing. However, the original MEEF algorithm suffers from high computational complexity due to the double summation of error samples. The quantized MEEF (QMEEF), proposed by Zheng et al. alleviates this computational burden through strategic quantization techniques, providing a more efficient solution. In this paper, we extend the application of these techniques to the complex domain, introducing complex QMEEF (CQMEEF). We theoretically introduce and prove the fundamental properties and convergence of CQMEEF. Furthermore, we apply this novel method to the training of a range of Linear-in-parameters (LIP) models, demonstrating its broad applicability. Experimental results show that CQMEEF achieves high precision in regression tasks involving various noise-corrupted datasets, exhibiting effectiveness under unfavorable conditions, and surpassing existing methods across critical performance metrics. Consequently, CQMEEF not only offers an efficient computational alternative but also opens up new avenues for dealing with complex data in regression tasks.},
  archive      = {J_NN},
  author       = {Bingqing Lin and Guobing Qian and Zongli Ruan and Junhui Qian and Shiyuan Wang},
  doi          = {10.1016/j.neunet.2025.107305},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107305},
  shortjournal = {Neural Netw.},
  title        = {Complex quantized minimum error entropy with fiducial points: Theory and application in model regression},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RISE-editing: Rotation-invariant neural point fields with
interactive segmentation for fine-grained and efficient editing.
<em>NN</em>, <em>187</em>, 107304. (<a
href="https://doi.org/10.1016/j.neunet.2025.107304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) have shown great potential for synthesizing novel views. Currently, despite the existence of some initial controllable and editable NeRF methods, they remain limited in terms of efficient and fine-grained editing capabilities, hinders the creative editing abilities and potential applications for NeRF. In this paper, we present the rotation-invariant neural point fields with interactive segmentation for fine-grained and efficient editing. Editing the implicit field presents a significant challenge, as varying the orientation of the corresponding explicit scaffold—whether point, mesh, volume, or other representations—may lead to a notable decline in rendering quality. By leveraging the complementary strengths of implicit NeRF-based representations and explicit point-based representations, we introduce a novel rotation-invariant neural point field representation. This representation enables the learning of local contents using Cartesian coordinates, leading to significant improvements in scene rendering quality after fine-grained editing. To achieve this rotation-invariant representation, we carefully design a Rotation-Invariant Neural Inverse Distance Weighting Interpolation (RNIDWI) module to aggregate the neural points. To enable more efficient and flexible cross-scene compositing, we disentangle the traditional NeRF representation into two components: a scene-agnostic rendering module and the scene-specific neural point fields. Furthermore, we present a multi-view ensemble learning strategy to lift the 2D inconsistent zero-shot segmentation results to 3D neural points field in real-time without post retraining. With simple click-based prompts on 2D images, user can efficiently segment the 3D neural point field and manipulate the corresponding neural points, enabling fine-grained editing of the implicit fields. Extensive experimental results demonstrate that our method offers enhanced editing capabilities and simplified editing process for users, delivers photorealistic rendering quality for novel views, and surpasses related methods in terms of the space–time efficiency and the types of editing functions they can achieve. The code is available at https://github.com/yuzewang1998/RISE-Editing .},
  archive      = {J_NN},
  author       = {Yuze Wang and Junyi Wang and Chen Wang and Yue Qi},
  doi          = {10.1016/j.neunet.2025.107304},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107304},
  shortjournal = {Neural Netw.},
  title        = {RISE-editing: Rotation-invariant neural point fields with interactive segmentation for fine-grained and efficient editing},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unambiguous granularity distillation for asymmetric image
retrieval. <em>NN</em>, <em>187</em>, 107303. (<a
href="https://doi.org/10.1016/j.neunet.2025.107303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous asymmetric image retrieval methods based on knowledge distillation have primarily focused on aligning the global features of two networks to transfer global semantic information from the gallery network to the query network. However, these methods often fail to effectively transfer local semantic information, limiting the fine-grained alignment of feature representation spaces between the two networks. To overcome this limitation, we propose a novel approach called Layered-Granularity Localized Distillation (GranDist). GranDist constructs layered feature representations that balance the richness of contextual information with the granularity of local features. As we progress through the layers, the contextual information becomes more detailed, but the semantic gap between networks can widen, complicating the transfer process. To address this challenge, GranDist decouples the feature maps at each layer to capture local features at different granularities and establishes distillation pipelines focused on effectively transferring these contextualized local features. In addition, we introduce an Unambiguous Localized Feature Selection (UnamSel) method, which leverages a well-trained fully connected layer to classify these contextual features as either ambiguous or unambiguous. By discarding the ambiguous features, we prevent the transfer of irrelevant or misleading information, such as background elements that are not pertinent to the retrieval task. Extensive experiments on various benchmark datasets demonstrate that our method outperforms state-of-the-art techniques and significantly enhances the performance of previous asymmetric retrieval approaches.},
  archive      = {J_NN},
  author       = {Hongrui Zhang and Yi Xie and Haoquan Zhang and Cheng Xu and Xuandi Luo and Donglei Chen and Xuemiao Xu and Huaidong Zhang and Pheng Ann Heng and Shengfeng He},
  doi          = {10.1016/j.neunet.2025.107303},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107303},
  shortjournal = {Neural Netw.},
  title        = {Unambiguous granularity distillation for asymmetric image retrieval},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hy-DeFake: Hypergraph neural networks for detecting fake
news in online social networks. <em>NN</em>, <em>187</em>, 107302. (<a
href="https://doi.org/10.1016/j.neunet.2025.107302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays social media is the primary platform for people to obtain news and share information. Combating online fake news has become an urgent task to reduce the damage it causes to society. Existing methods typically improve their fake news detection performances by utilizing textual auxiliary information (such as relevant retweets and comments) or simple structural information ( i.e. , graph construction). However, these methods face two challenges. First, an increasing number of users tend to directly forward the source news without adding comments, resulting in a lack of textual auxiliary information. Second, simple graphs are unable to extract complex relations beyond pairwise association in a social context. Given that real-world social networks are intricate and involve high-order relations, we argue that exploring beyond pairwise relations between news and users is crucial for fake news detection. Therefore, we propose constructing an attributed hypergraph to represent non-textual and high-order relations for user participation in news spreading. We also introduce a hypergraph neural network-based method called Hy-DeFake to tackle the challenges. Our proposed method captures semantic information from news content, credibility information from involved users, and high-order correlations between news and users to learn distinctive embeddings for fake news detection. The superiority of Hy-DeFake is demonstrated through experiments conducted on four widely-used datasets, and it is compared against nine baselines using four evaluation metrics.},
  archive      = {J_NN},
  author       = {Xing Su and Jian Yang and Jia Wu and Zitai Qiu},
  doi          = {10.1016/j.neunet.2025.107302},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107302},
  shortjournal = {Neural Netw.},
  title        = {Hy-DeFake: Hypergraph neural networks for detecting fake news in online social networks},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum federated learning with pole-angle quantum local
training and trainable measurement. <em>NN</em>, <em>187</em>, 107301.
(<a href="https://doi.org/10.1016/j.neunet.2025.107301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, quantum federated learning (QFL) has received significant attention as an innovative paradigm. QFL has remarkable features by employing quantum neural networks (QNNs) instead of conventional neural networks owing to quantum supremacy. In order to enhance the flexibility and reliability of classical QFL frameworks, this paper proposes a novel slimmable QFL (SlimQFL) incorporating QNN-grounded slimmable neural network (QSNN) architectures. This innovative design considers time-varying wireless communication channels and computing resource constraints. This framework ensures higher efficiency by using fewer parameters with no performance loss. Furthermore, the proposed QNN is novel according to the implementation of trainable measurement within QFL. The fundamental concept of our QSNN is designed based on the key characteristics of separated training and the dynamic exploitation of joint angle and pole parameters. Our performance evaluation results verify that using both parameters, our proposed QSNN-based SlimQFL achieves higher classification accuracy than QFL and ensures transmission stability, particularly in poor channel conditions.},
  archive      = {J_NN},
  author       = {Soohyun Park and Hyunsoo Lee and Seok Bin Son and Soyi Jung and Joongheon Kim},
  doi          = {10.1016/j.neunet.2025.107301},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107301},
  shortjournal = {Neural Netw.},
  title        = {Quantum federated learning with pole-angle quantum local training and trainable measurement},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EBM-WGF: Training energy-based models with wasserstein
gradient flow. <em>NN</em>, <em>187</em>, 107300. (<a
href="https://doi.org/10.1016/j.neunet.2025.107300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-based models (EBMs) show their efficiency in density estimation. However, MCMC sampling in traditional EBMs suffers from expensive computation. Although EBMs with minimax game avoid the above drawback, the energy estimation and generator’s optimization are not always stable. We find that the reason for this instability arises from the inaccuracy of minimizing KL divergence between generative and energy distribution along a vanilla gradient flow. In this paper, we leverage the Wasserstein gradient flow (WGF) of the KL divergence to correct the optimization direction of the generator in the minimax game. Different from existing WGF-based models, we pullback the WGF to parameter space and solve it with a variational scheme for bounded solution error. We propose a new EBM with WGF that overcomes the instability of the minimax game and avoids computational MCMC sampling in traditional methods, as we observe that the solution of WGF in our approach is equivalent to Langevin dynamic in EBMs with MCMC sampling. The empirical experiments on toy and natural datasets validate the effectiveness of our approach.},
  archive      = {J_NN},
  author       = {Ben Wan and Cong Geng and Tianyi Zheng and Jia Wang},
  doi          = {10.1016/j.neunet.2025.107300},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107300},
  shortjournal = {Neural Netw.},
  title        = {EBM-WGF: Training energy-based models with wasserstein gradient flow},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic memory auto-encoding network for abnormal
behavior detection in surveillance video. <em>NN</em>, <em>187</em>,
107299. (<a href="https://doi.org/10.1016/j.neunet.2025.107299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal behavior detection in surveillance video, as one of the essential functions in the intelligent surveillance system, plays a vital role in anti-terrorism, maintaining stability, and ensuring social security. Aiming at the problem of extremely imbalance between normal behavior data and abnormal behavior data, the probabilistic memory model-based network is designed to learn from the distribution of normal behaviors and guide the detection of abnormal behavior. An auto-encoding model is employed as the backbone network, and the gap between the predicted future frame and the real frame is used to measure the degree of abnormality. An autoregressive conditional probability estimation model and a normal distribution memory model are employed as auxiliary modules, to achieve the prediction of normal frames. When extracting temporal and spatial features in the backbone network, the causal three-dimensional convolution and time-dimension shared fully connected layers are used to avoid future information leakage and ensure the timing of information. In addition, from the perspective of probability entropy and behavioral modality diversity, autoregressive probability model is proposed to fit the distribution of input normal frame, so the network converges to the low entropy state of the normal behavior distribution. The memory module stores the feature of normal behavior in historical data, and injects the current input data. The memory vector and the encoding vector are concatenated along the time dimension and input to the decoder, realizing normal frame prediction. Using public datasets, ablation and comparison experiments show that the proposed algorithm has significant advantages in anomaly detection.},
  archive      = {J_NN},
  author       = {Jinsheng Xiao and Jingyi Wu and Shurui Wang and Qiuze Yu and Honggang Xie and Yuan-Fang Wang},
  doi          = {10.1016/j.neunet.2025.107299},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107299},
  shortjournal = {Neural Netw.},
  title        = {Probabilistic memory auto-encoding network for abnormal behavior detection in surveillance video},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dataset-free weight-initialization on restricted boltzmann
machine. <em>NN</em>, <em>187</em>, 107297. (<a
href="https://doi.org/10.1016/j.neunet.2025.107297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In feed-forward neural networks, dataset-free weight-initialization methods such as LeCun, Xavier (or Glorot), and He initializations have been developed. These methods randomly determine the initial values of weight parameters based on specific distributions (e.g., Gaussian or uniform distributions) without using training datasets. To the best of the authors’ knowledge, such a dataset-free weight-initialization method is yet to be developed for restricted Boltzmann machines (RBMs), which are probabilistic neural networks consisting of two layers. In this study, we derive a dataset-free weight-initialization method for Bernoulli–Bernoulli RBMs based on statistical mechanical analysis. In the proposed weight-initialization method, the weight parameters are drawn from a Gaussian distribution with zero mean. The standard deviation of the Gaussian distribution is optimized based on our hypothesis that a standard deviation providing a larger layer correlation (LC) between the two layers improves the learning efficiency. The expression of the LC is derived based on a statistical mechanical analysis. The optimal value of the standard deviation corresponds to the maximum point of the LC. The proposed weight-initialization method is identical to Xavier initialization in a specific case (i.e., when the sizes of the two layers are the same, the random variables of the layers are { − 1 , 1 } -binary, and all bias parameters are zero). The validity of the proposed weight-initialization method is demonstrated in numerical experiments using a toy dataset and real-world datasets.},
  archive      = {J_NN},
  author       = {Muneki Yasuda and Ryosuke Maeno and Chako Takahashi},
  doi          = {10.1016/j.neunet.2025.107297},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107297},
  shortjournal = {Neural Netw.},
  title        = {Dataset-free weight-initialization on restricted boltzmann machine},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual learning of conjugated visual representations
through higher-order motion flows. <em>NN</em>, <em>187</em>, 107296.
(<a href="https://doi.org/10.1016/j.neunet.2025.107296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with neural networks from a continuous stream of visual information presents several challenges due to the non-i.i.d. nature of the data. However, it also offers novel opportunities to develop representations that are consistent with the information flow. In this paper we investigate the case of unsupervised continual learning of pixel-wise features subject to multiple motion-induced constraints, therefore named motion-conjugated feature representations . Differently from existing approaches, motion is not a given signal (either ground-truth or estimated by external modules), but is the outcome of a progressive and autonomous learning process, occurring at various levels of the feature hierarchy. Multiple motion flows are estimated with neural networks and characterized by different levels of abstractions, spanning from traditional optical flow to other latent signals originating from higher-level features, hence called higher-order motions. Continuously learning to develop consistent multi-order flows and representations is prone to trivial solutions, which we counteract by introducing a self-supervised contrastive loss, spatially-aware and based on flow-induced similarity. We assess our model on photorealistic synthetic streams and real-world videos, comparing to pre-trained state-of-the art feature extractors (also based on Transformers) and to recent unsupervised learning models, significantly outperforming these alternatives.},
  archive      = {J_NN},
  author       = {Simone Marullo and Matteo Tiezzi and Marco Gori and Stefano Melacci},
  doi          = {10.1016/j.neunet.2025.107296},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107296},
  shortjournal = {Neural Netw.},
  title        = {Continual learning of conjugated visual representations through higher-order motion flows},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting sparse behavior data for user identity linkage
with self-generated by model and mixup-generated samples. <em>NN</em>,
<em>187</em>, 107295. (<a
href="https://doi.org/10.1016/j.neunet.2025.107295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The user identity linkage task aims to associate user accounts belonging to the same individual by utilizing user data. This task is relevant in domains such as recommendation systems, where user-generated content (i.e., behavioral data) serves as the key information for identifying users. However, user identity linkage tasks relying on behavioral data face two primary challenges due to data sparsity: insufficient user behavior data and the presence of low-frequency behavior items. These issues hinder accurate modeling and exacerbate representation errors. To address these challenges, we propose two data augmentation methods: self-generated samples by the model and mixup-generated samples. Collectively, these methods are referred to as SGAMDA (Self-generated by Model and Mixup-generated Samples-based Data Augmentation). The self-generated samples method uses Variational Autoencoders to generate new training data by decoding samples in the representation space. The mixup-generated samples method creates new training data by mixing the behavior data of different user groups, thereby alleviating data sparsity. SGAMDA categorizes user behavior data based on data volume and the proportion of low-frequency behaviors to guide the two data augmentation strategies. We evaluate SGAMDA on the Movies2Books and CDs2Movies datasets for user identity linkage tasks. The results show that SGAMDA significantly improves prediction accuracy, enhancing behavior representation through the proposed data augmentation methods.},
  archive      = {J_NN},
  author       = {Hongren Huang and Jianxin Li and Feihong Lu and Lihong Wang and Qian Li and Qingyun Sun},
  doi          = {10.1016/j.neunet.2025.107295},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107295},
  shortjournal = {Neural Netw.},
  title        = {Augmenting sparse behavior data for user identity linkage with self-generated by model and mixup-generated samples},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral anomaly detection with self-supervised anomaly
prior. <em>NN</em>, <em>187</em>, 107294. (<a
href="https://doi.org/10.1016/j.neunet.2025.107294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral anomaly detection (HAD) can identify and locate the targets without any known information and is widely applied in Earth observation and military fields. The majority of existing HAD methods use the low-rank representation (LRR) model to separate the background and anomaly through mathematical optimization, in which the anomaly is optimized with a handcrafted sparse prior (e.g., ℓ 2 , 1 -norm). However, this may not be ideal since they overlook the spatial structure present in anomalies and make the detection result largely dependent on manually set sparsity. To tackle these problems, we redefine the optimization criterion for the anomaly in the LRR model with a self-supervised network called self-supervised anomaly prior (SAP). This prior is obtained by the pretext task of self-supervised learning, which is customized to learn the characteristics of hyperspectral anomalies. Specifically, this pretext task is a classification task to distinguish the original hyperspectral image (HSI) and the pseudo-anomaly HSI, where the pseudo-anomaly is generated from the original HSI and designed as a prism with arbitrary polygon bases and arbitrary spectral bands. In addition, a dual-purified strategy is proposed to provide a more refined background representation with an enriched background dictionary, facilitating the separation of anomalies from complex backgrounds. Extensive experiments on various hyperspectral datasets demonstrate that the proposed SAP offers a more accurate and interpretable solution than other advanced HAD methods.},
  archive      = {J_NN},
  author       = {Yidan Liu and Kai Jiang and Weiying Xie and Jiaqing Zhang and Yunsong Li and Leyuan Fang},
  doi          = {10.1016/j.neunet.2025.107294},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107294},
  shortjournal = {Neural Netw.},
  title        = {Hyperspectral anomaly detection with self-supervised anomaly prior},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anxiety disorder identification with biomarker detection
through subspace-enhanced hypergraph neural network. <em>NN</em>,
<em>187</em>, 107293. (<a
href="https://doi.org/10.1016/j.neunet.2025.107293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a subspace-enhanced hypergraph neural network (seHGNN) for classifying anxiety disorders (AD), which are prevalent mental illnesses that affect a significant portion of the global population. Our seHGNN model utilizes a learnable incidence matrix to strengthen the influence of hyperedges in graphs and enhance the feature extraction performance of hypergraph neural networks (HGNNs). Then, we integrate multimodal data on the brain limbic system into a hypergraph within an existing binary hypothesis testing framework. Experimental results demonstrate that our seHGNN achieves a remarkable accuracy of 84.46% for AD classification. By employing an ensemble learning strategy, we can further improve its performance, achieving a high accuracy of 94.1%. Our method outperforms other deep-learning-based methods, particularly GNN-based methods. Furthermore, our seHGNN successfully identifies discriminative AD biomarkers that align with existing reports, providing strong evidence supporting the effectiveness and interpretability of our proposed method.},
  archive      = {J_NN},
  author       = {Yibin Tang and Jikang Ding and Ying Chen and Yuan Gao and Aimin Jiang and Chun Wang},
  doi          = {10.1016/j.neunet.2025.107293},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107293},
  shortjournal = {Neural Netw.},
  title        = {Anxiety disorder identification with biomarker detection through subspace-enhanced hypergraph neural network},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory flow-controlled knowledge tracing with three stages.
<em>NN</em>, <em>187</em>, 107292. (<a
href="https://doi.org/10.1016/j.neunet.2025.107292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Tracing (KT), as a pivotal technology in intelligent education systems, analyzes students’ learning data to infer their knowledge acquisition and predict their future performance. Recent advancements in KT recognize the importance of memory laws on knowledge acquisition but neglect modeling the inherent structure of memory, which leads to the inconsistency between explicit student learning and implicit memory transformation. Therefore, to enhance the consistency, we propose a novel memory flow-controlled knowledge tracing with three stages (MFCKT). According to information processing theory, we deconstruct learning into: sensory registration, short-term encoding, and long-term memory retrieval stages. Specifically, to extract sensory memory, MFCKT maximizes the similarity between positive augmentation views of learning sequence representations through contrastive pre-training. Then, to transform sensory memory into short-term memory, MFCKT fuses relational and temporal properties of sensory memory through a dual-channel structure composed of attention and recurrent neural networks. Furthermore, for obtaining long-term memory, MFCKT designs a monotonic gating mechanism to compute weights of hidden memory states, and then performs read-write operations on the memory matrix. Finally, MFCKT combines long-term and short-term memory vectors to retrieve latent knowledge states for future performance prediction. Extensive experimental results on five real-world datasets verify the superiority and interpretability of MFCKT.},
  archive      = {J_NN},
  author       = {Tao Huang and Junjie Hu and Huali Yang and Shengze Hu and Jing Geng and Xinjia Ou},
  doi          = {10.1016/j.neunet.2025.107292},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107292},
  shortjournal = {Neural Netw.},
  title        = {Memory flow-controlled knowledge tracing with three stages},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view graph-of-graph representation learning with graph
transformer for graph-level anomaly detection. <em>NN</em>,
<em>187</em>, 107291. (<a
href="https://doi.org/10.1016/j.neunet.2025.107291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-Level Anomaly Detection (GLAD) endeavors to pinpoint a small subset of anomalous graphs that deviate from the normal data distribution within a given set of graph data. Existing GLAD methods typically rely on Graph Neural Networks (GNNs) to extract graph-level representations, which are then used for the detection task. However, the inherent limited receptive field of GNNs may exclude crucial anomalous information embedded within the graph. Moreover, the inadequate modeling of cross-graph relationships limits the exploration of connections between different graphs, thus restricting the model’s ability to uncover inter-graph anomalous patterns. In this paper, we propose a novel approach called Dual-View Graph-of-Graph Representation Learning Network for unsupervised GLAD, which takes into account both intra-graph and inter-graph perspectives. Firstly, to enhance the capability of mining intra-graph information, we introduce a Graph Transformer that enhances the receptive field of the GNNs by considering both attribute and structural information. This augmentation enables a comprehensive exploration of the information encoded within the graph. Secondly, to explicitly capture the cross-graph dependencies, we devise a Graph-of-Graph-based dual-view representation learning network to explicitly capture cross-graph interdependencies. Attribute and structure-based graph-of-graph representations are induced, facilitating a comprehensive understanding of the relationships between graphs. Finally, we utilize anomaly scores from different perspectives to quantify the extent of anomalies present in each graph. This multi-perspective evaluation provides a more comprehensive assessment of anomalies within the graph data. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness of our proposed method in detecting anomalies within graph data.},
  archive      = {J_NN},
  author       = {Wangyu Jin and Huifang Ma and Yingyue Zhang and Zhixin Li and Liang Chang},
  doi          = {10.1016/j.neunet.2025.107291},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107291},
  shortjournal = {Neural Netw.},
  title        = {Dual-view graph-of-graph representation learning with graph transformer for graph-level anomaly detection},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Episodic memory-double actor–critic twin delayed deep
deterministic policy gradient. <em>NN</em>, <em>187</em>, 107286. (<a
href="https://doi.org/10.1016/j.neunet.2025.107286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep reinforcement learning (DRL) algorithms suffer from the problem of low sample efficiency. Episodic memory allows DRL algorithms to remember and use past experiences with high return, thereby improving sample efficiency. However, due to the high dimensionality of the state–action space in continuous action tasks, previous methods in continuous action tasks often only utilize the information stored in episodic memory, rather than directly employing episodic memory for action selection as done in discrete action tasks. We suppose that episodic memory retains the potential to guide action selection in continuous control tasks. Our objective is to enhance sample efficiency by leveraging episodic memory for action selection in such tasks—either reducing the number of training steps required to achieve comparable performance or enabling the agent to obtain higher rewards within the same number of training steps. To this end, we propose an “Episodic Memory-Double Actor–Critic (EMDAC)” framework, which can use episodic memory for action selection in continuous action tasks. The critics and episodic memory evaluate the value of state–action pairs selected by the two actors to determine the final action. Meanwhile, we design an episodic memory based on a Kalman filter optimizer, which updates using the episodic rewards of collected state–action pairs. The Kalman filter optimizer assigns different weights to experiences collected at different time periods during the memory update process. In our episodic memory, state–action pair clusters are used as indices, recording both the occurrence frequency of these clusters and the value estimates for the corresponding state–action pairs. This enables the estimation of the value of state–action pair clusters by querying the episodic memory. After that, we design intrinsic reward based on the novelty of state–action pairs with episodic memory, defined by the occurrence frequency of state–action pair clusters, to enhance the exploration capability of the agent. Ultimately, we propose an “EMDAC-TD3” algorithm by applying this three modules to Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm within an Actor–Critic framework. Through evaluations in MuJoCo environments within the OpenAI Gym domain, EMDAC-TD3 achieves higher sample efficiency compared to baseline algorithms. EMDAC-TD3 demonstrates superior final performance compared to state-of-the-art episodic control algorithms and advanced Actor–Critic algorithms, by comparing the final rewards, Median, Interquartile Mean, Mean, and Optimality Gap. The final rewards can directly demonstrate the advantages of the algorithms. Based on the final rewards, EMDAC-TD3 achieves an average performance improvement of 11.01% over TD3, surpassing the current state-of-the-art algorithms in the same category.},
  archive      = {J_NN},
  author       = {Man Shu and Shuai Lü and Xiaoyu Gong and Daolong An and Songlin Li},
  doi          = {10.1016/j.neunet.2025.107286},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107286},
  shortjournal = {Neural Netw.},
  title        = {Episodic memory-double Actor–Critic twin delayed deep deterministic policy gradient},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximation by non-symmetric networks for cross-domain
learning. <em>NN</em>, <em>187</em>, 107282. (<a
href="https://doi.org/10.1016/j.neunet.2025.107282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. In particular, we obtain estimates on the accuracy of uniform approximation of functions in a Sobolev class by ReLU r networks when r is not necessarily an integer. Our general results apply to the approximation of functions with small smoothness compared to the dimension of the input space.},
  archive      = {J_NN},
  author       = {H.N. Mhaskar},
  doi          = {10.1016/j.neunet.2025.107282},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107282},
  shortjournal = {Neural Netw.},
  title        = {Approximation by non-symmetric networks for cross-domain learning},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StoCFL: A stochastically clustered federated learning
framework for non-IID data with dynamic client participation.
<em>NN</em>, <em>187</em>, 107278. (<a
href="https://doi.org/10.1016/j.neunet.2025.107278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed learning framework that takes full advantage of private data samples kept on edge devices. In real-world federated learning systems, these data samples are often decentralized and Non-Independently Identically Distributed (Non-IID), causing divergence and performance degradation in the federated learning process. As a new solution, clustered federated learning groups federated clients with similar data distributions to impair the Non-IID effects and train a better model for every cluster. However, existing CFL algorithms are ineffective because they lack an information-sharing mechanism across clusters resulting in low data efficiency and model performance. Meanwhile, their performance is highly subjected to ideal client clustering results which are practically unavailable. This paper proposes StoCFL, a novel clustered federated learning framework for generic Non-IID issues. In detail, StoCFL implements a flexible CFL framework that supports an arbitrary proportion of client participation and newly joined clients for a varying FL system, while maintaining a great improvement in model performance. The intensive experiments are conducted by using four basic Non-IID settings and a real-world dataset. The results show that StoCFL could obtain promising cluster results even when the number of clusters is unknown. Based on the client clustering results, models trained with StoCFL outperform baseline approaches in a variety of scenarios.},
  archive      = {J_NN},
  author       = {Dun Zeng and Xiangjing Hu and Shiyu Liu and Yue Yu and Qifan Wang and Zenglin Xu},
  doi          = {10.1016/j.neunet.2025.107278},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107278},
  shortjournal = {Neural Netw.},
  title        = {StoCFL: A stochastically clustered federated learning framework for non-IID data with dynamic client participation},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural networks trained by weight permutation are universal
approximators. <em>NN</em>, <em>187</em>, 107277. (<a
href="https://doi.org/10.1016/j.neunet.2025.107277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The universal approximation property is fundamental to the success of neural networks, and has traditionally been achieved by training networks without any constraints on their parameters. However, recent experimental research proposed a novel permutation-based training method, which exhibited a desired classification performance without modifying the exact weight values. In this paper, we provide a theoretical guarantee of this permutation training method by proving its ability to guide a ReLU network to approximate one-dimensional continuous functions. Our numerical results further validate this method’s efficiency in regression tasks with various initializations. The notable observations during weight permutation suggest that permutation training can provide an innovative tool for describing network learning behavior.},
  archive      = {J_NN},
  author       = {Yongqiang Cai and Gaohang Chen and Zhonghua Qiao},
  doi          = {10.1016/j.neunet.2025.107277},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107277},
  shortjournal = {Neural Netw.},
  title        = {Neural networks trained by weight permutation are universal approximators},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedELR: When federated learning meets learning with noisy
labels. <em>NN</em>, <em>187</em>, 107275. (<a
href="https://doi.org/10.1016/j.neunet.2025.107275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing research on federated learning (FL) usually assumes that training labels are of high quality for each client, which is impractical in many real-world scenarios (e.g., noisy labels by crowd-sourced annotations), leading to dramatic performance degradation. In this work, we investigate noisy FL through the lens of early-time training phenomenon (ETP). Specifically, a key finding of this paper is that the early training phase varies among different local clients due to the different noisy classes in each client. In addition, we show that such an inconsistency also exists between the local and global models. As a result, local clients would always begin to memorize noisy labels before the global model reaches the optimal, which inevitably leads to the degradation of the quality of service in real-world FL applications (e.g. tumor image classification among different hospitals). Our findings provide new insights into the learning dynamics and shed light on the essence cause of this degradation in noisy FL. To address this problem, we reveal a new principle for noisy FL: it is necessary to align the early training phases across local models. To this end, we propose FedELR, a simple yet effective framework that aims to force local models to stick to their early training phase via an early learning regularization (ELR), so that the learning dynamics of local models can be kept at the same pace. Moreover, this also leverages the ETP in local clients, leading each client to take more training steps in learning a more robust local model for optimal global aggregation. Extensive experiments on various real-world datasets also validate the effectiveness of our proposed methods.},
  archive      = {J_NN},
  author       = {Ruizhi Pu and Lixing Yu and Shaojie Zhan and Gezheng Xu and Fan Zhou and Charles X. Ling and Boyu Wang},
  doi          = {10.1016/j.neunet.2025.107275},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107275},
  shortjournal = {Neural Netw.},
  title        = {FedELR: When federated learning meets learning with noisy labels},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image debanding using cross-scale invertible networks with
banded deformable convolutions. <em>NN</em>, <em>187</em>, 107270. (<a
href="https://doi.org/10.1016/j.neunet.2025.107270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Banding artifacts in images stem from limitations in color bit depth, image compression, or over-editing, significantly degrades image quality, especially in regions with smooth gradients. Image debanding is about eliminating these artifacts while preserving the authenticity of image details. This paper introduces a novel approach to image debanding using a cross-scale invertible neural network (INN). The proposed INN is information-lossless and enhanced by a more effective cross-scale scheme. Additionally, we present a technique called banded deformable convolution, which fully leverages the anisotropic properties of banding artifacts. This technique is more compact, efficient, and exhibits better generalization compared to existing deformable convolution methods. Our proposed INN exhibits superior performance in both quantitative metrics and visual quality, as evidenced by the results of the experiments.},
  archive      = {J_NN},
  author       = {Yuhui Quan and Xuyi He and Ruotao Xu and Yong Xu and Hui Ji},
  doi          = {10.1016/j.neunet.2025.107270},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107270},
  shortjournal = {Neural Netw.},
  title        = {Image debanding using cross-scale invertible networks with banded deformable convolutions},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting instance-label dynamics through reciprocal
anchored contrastive learning for few-shot relation extraction.
<em>NN</em>, <em>187</em>, 107259. (<a
href="https://doi.org/10.1016/j.neunet.2025.107259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of Few-shot Relation Extraction (FSRE), the primary objective is to distill relational facts from limited labeled datasets. This task has recently witnessed significant advancements through the integration of Pre-trained Language Models (PLMs) within a supervised contrastive learning schema, which effectively leverages the dynamics between instance and label information. Despite these advancements, the comprehensive utilization of extensive instance-label pairs, aimed at facilitating the extraction of semantically rich representations within this paradigm, has yet to be fully harnessed. To bridge this gap, we introduce a R eciprocal A nchored C ontrastive L earning framework (RACL) for few-shot relation extraction, which is predicated on the premise that instance-label pairs provide distinct yet inherently complementary insights into textual semantics. Specifically, RACL employs a symmetric contrastive objective that incorporates both instance-level and label-level contrastive losses, promoting a more integrated and unified representational space. This approach is engineered to effectively delineate the nuanced relationships between instance attributes and relational facts, while simultaneously optimizing information sharing across different perspectives within the same relations. Extensive experiments on the FSRE benchmark datasets demonstrate the superiority of our approach as compared to the state-of-the-art baselines. Further ablation studies on Zero-shot and None-of-the-above settings confirm its robustness and adaptability in practical applications.},
  archive      = {J_NN},
  author       = {Yanglei Gan and Qiao Liu and Run Lin and Tian Lan and Yuxiang Cai and Xueyi Liu and Changlin Li and Yan Liu},
  doi          = {10.1016/j.neunet.2025.107259},
  journal      = {Neural Networks},
  month        = {7},
  pages        = {107259},
  shortjournal = {Neural Netw.},
  title        = {Exploiting instance-label dynamics through reciprocal anchored contrastive learning for few-shot relation extraction},
  volume       = {187},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
