<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EJOR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ejor---21">EJOR - 21</h2>
<ul>
<li><details>
<summary>
(2025). Milk adulteration testing and impact of farmers efficiency
heterogeneity: A strategic analysis. <em>EJOR</em>, <em>323</em>(2),
686–700. (<a href="https://doi.org/10.1016/j.ejor.2024.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by economic motives, dairy farmers adulterate milk to increase its perceived quality, posing a serious risk to consumer health. We analyse a milk supply chain in which smallholder dairy farmers can adulterate milk and explore the feasibility of selling it to end consumers through an aggregator. Using a non-cooperative sequential game between the aggregator and farmers, we examine the impact of two testing strategies offered by the aggregator to curb adulteration - (i) individual (testing milk procured from each farmer individually) and (ii) composite (testing the milk after aggregating the portions procured from all the farmers). Our analyses reveal that the aggregator can control milk adulteration by judiciously using testing and penalty mechanisms. We find that a higher market price (aggregation effect) , fetched by the aggregator because of its bargaining power owing to the consolidation of milk supplies, is essential for its operation. It leads to higher revenue for the aggregator and expands the zone in which it is profitable for the aggregator to operate. However, our results show that the efficiency heterogeneity among farmers, which leads to the less efficient farmers free-riding on the more efficient ones, has a detrimental effect on the aggregator operation. We also explore the impact of external uncertainties on the supply chain and observe that the composite testing strategy becomes more profitable for the aggregator when external uncertainties increase. Our results provide important policy recommendations for aggregators adopting optimal testing strategies.},
  archive      = {J_EJOR},
  author       = {Samir Biswas and Preetam Basu and Balram Avittathur},
  doi          = {10.1016/j.ejor.2024.12.001},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {686-700},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Milk adulteration testing and impact of farmers efficiency heterogeneity: A strategic analysis},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from the aggregated optimum: Managing port wine
inventory in the face of climate risks. <em>EJOR</em>, <em>323</em>(2),
671–685. (<a href="https://doi.org/10.1016/j.ejor.2024.11.046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Port wine stocks ameliorate during storage, facilitating product differentiation according to age. This induces a trade-off between immediate revenues and further maturation. Varying climate conditions in the limited supply region lead to stochastic purchase prices for wine grapes. Decision makers must integrate recurring purchasing, production, and issuance decisions. Because stocks from different age classes can be blended to create final products, the solution space increases exponentially in the number of age classes. We model the problem of managing port wine inventory as a Markov decision process, considering decay as an additional source of uncertainty. For small problems, we derive general management strategies from the long-run behavior of the optimal policy. Our solution approach for otherwise intractable large problems, therefore, first aggregates age classes to create a tractable problem representation. We then use machine learning to train tree-based decision rules that reproduce the optimal aggregated policy and the enclosed management strategies. The derived rules are scaled back to solve the original problem. Learning from the aggregated optimum outperforms benchmark rules by 21.4% in annual profits (while leaving a 2.8%-gap to an upper bound). For an industry case, we obtain a 17.4%-improvement over current practices. Our research provides distinct strategies for how producers can mitigate climate risks. The purchasing policy dynamically adapts to climate-dependent price fluctuations. Uncertainties are met with lower production of younger products, whereas strategic surpluses of older stocks ensure high production of older products. Moreover, a wide spread in the age classes used for blending reduces decay risk exposure.},
  archive      = {J_EJOR},
  author       = {Alexander Pahr and Martin Grunow and Pedro Amorim},
  doi          = {10.1016/j.ejor.2024.11.046},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {671-685},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Learning from the aggregated optimum: Managing port wine inventory in the face of climate risks},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible enhanced indexation models through stochastic
dominance and ordered weighted average optimization. <em>EJOR</em>,
<em>323</em>(2), 657–670. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss portfolio selection strategies for Enhanced Indexation (EI), which are based on stochastic dominance relations. The goal is to select portfolios that stochastically dominate a given benchmark but that, at the same time, must generate some excess return with respect to a benchmark index. To achieve this goal, we propose a new methodology that selects portfolios using the ordered weighted average (OWA) operator, which generalizes previous approaches based on minimax selection rules and still leads to solving linear programming models. We also introduce a new type of approximate stochastic dominance rule and show that it implies the almost Second-order Stochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczyński (2012). We prove that our EI model based on OWA selects portfolios that dominate a given benchmark through this new form of stochastic dominance criterion. We test the performance of the obtained portfolios in an extensive empirical analysis based on real-world datasets. The computational results show that our proposed approach outperforms several SSD-based strategies widely used in the literature, as well as the global minimum variance portfolio.},
  archive      = {J_EJOR},
  author       = {Francesco Cesarone and Justo Puerto},
  doi          = {10.1016/j.ejor.2024.11.050},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {657-670},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Flexible enhanced indexation models through stochastic dominance and ordered weighted average optimization},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal fulfillment and replenishment for omnichannel
retailers with standard shipping contracts. <em>EJOR</em>,
<em>323</em>(2), 642–656. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce sales rise exponentially and represent an increasing proportion of global retail. To benefit from this, traditional brick-and-mortar stores enter the e-commerce market and become omnichannel retailers. However, the profitability of omnichannel retailers remains questionable due to high shipment and fulfillment costs. This paper addresses this challenge, focusing on using standard shipping contracts as a potential solution. Such contracts promise delivery within a given number of periods. Once a customer orders, the retailer should set a delivery period. In this way, retailers are flexible in setting exact delivery days, providing an opportunity for jointly optimizing product replenishment and customer fulfillment. We provide a generic model for the use of standard shipping contracts and formulate it as a Markov decision process. We provide optimal solutions using a modified policy iteration algorithm. Our results show that using standard shipping contracts creates a win-win situation: It increases profits and customer service. The observed profit increase is directly linked to maintaining less on-hand inventory. This effect is more pronounced for higher valued products and longer replenishment lead times. Additionally, we propose a heuristic policy that performs within 4% of the optimal policy.},
  archive      = {J_EJOR},
  author       = {Bartu Arslan and Albert H. Schrotenboer and Zümbül Atan},
  doi          = {10.1016/j.ejor.2024.11.051},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {642-656},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal fulfillment and replenishment for omnichannel retailers with standard shipping contracts},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank matrix estimation via nonconvex spectral
regularized methods in errors-in-variables matrix regression.
<em>EJOR</em>, <em>323</em>(2), 626–641. (<a
href="https://doi.org/10.1016/j.ejor.2025.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional matrix regression has been studied in various aspects, such as statistical properties, computational efficiency and application to specific instances including multivariate regression, system identification and matrix compressed sensing. Current studies mainly consider the idealized case that the covariate matrix is obtained without noise, while the more realistic scenario that the covariates may always be corrupted with noise or missing data has received little attention. We consider the general errors-in-variables matrix regression model and proposed a unified framework for low-rank estimation based on nonconvex spectral regularization. Then from the statistical aspect, recovery bounds for any stationary points are provided to achieve statistical consistency. From the computational aspect, the proximal gradient method is applied to solve the nonconvex optimization problem and is proved to converge to a small neighborhood of the global solution in polynomial time. Consequences for concrete models such as matrix compressed sensing models with additive noise and missing data are obtained via verifying corresponding regularity conditions. Finally, the performance of the proposed nonconvex estimation method is illustrated by numerical experiments on both synthetic and real neuroimaging data.},
  archive      = {J_EJOR},
  author       = {Xin Li and Dongya Wu},
  doi          = {10.1016/j.ejor.2025.02.005},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {626-641},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Low-rank matrix estimation via nonconvex spectral regularized methods in errors-in-variables matrix regression},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From collaborative filtering to deep learning: Advancing
recommender systems with longitudinal data in the financial services
industry. <em>EJOR</em>, <em>323</em>(2), 609–625. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) are highly relevant for multiple domains, allowing to construct personalized suggestions for consumers. Previous studies have strongly focused on collaborative filtering approaches, but the inclusion of longitudinal data (LD) has received limited attention. To address this gap, we investigate the impact of incorporating LD for recommendations, comparing traditional collaborative filtering approaches, multi-label classifier (MLC) algorithms, and a deep learning model (DL) in the form of gated recurrent units (GRU). Additional analysis for the best performing model is provided through SHapley Additive exPlanations (SHAP), to uncover relations between the different recommended products and features. Thus, this article contributes to operational research literature by (1) comparing several MLC techniques and RS, including state-of-the-art DL models in a real-life scenario, (2) the comparison of various featurization techniques to assess the impact of incorporating LD on MLC performance, (3) the evaluation of LD as sequential input through the use of DL models, (4) offering interpretable model insights to improve the understanding of RS with LD. The results uncover that DL models are capable of extracting information from longitudinal features for overall higher and statistically significant performance. Further, SHAP values reveal that LD has the higher impact on model output and managerial relevant temporal patterns emerge across product categories.},
  archive      = {J_EJOR},
  author       = {Stephanie Beyer Díaz and Kristof Coussement and Arno De Caigny},
  doi          = {10.1016/j.ejor.2025.01.022},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {609-625},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {From collaborative filtering to deep learning: Advancing recommender systems with longitudinal data in the financial services industry},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On enhancing the explainability and fairness of tree
ensembles. <em>EJOR</em>, <em>323</em>(2), 599–608. (<a
href="https://doi.org/10.1016/j.ejor.2025.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree ensembles are one of the most powerful methodologies in Machine Learning. In this paper, we investigate how to make tree ensembles more flexible to incorporate explainability and fairness in the training process, possibly at the expense of a decrease in accuracy. While explainability helps the user understand the key features that play a role in the classification task, with fairness we ensure that the ensemble does not discriminate against a group of observations that share a sensitive attribute. We propose a Mixed Integer Linear Optimization formulation to train an ensemble of trees that, apart from minimizing the misclassification cost, controls for sparsity as well as the accuracy in the sensitive group. Our formulation is scalable in the number of observations since its number of binary decision variables is independent of the number of observations. In our numerical results, we show that for standard datasets used in the fairness literature, we can dramatically enhance the fairness of the benchmark, namely the popular Random Forest, while using only a few features, all without damaging the misclassification cost.},
  archive      = {J_EJOR},
  author       = {Emilio Carrizosa and Kseniia Kurishchenko and Dolores Romero Morales},
  doi          = {10.1016/j.ejor.2025.01.008},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {599-608},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {On enhancing the explainability and fairness of tree ensembles},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The manufacturer’s resale strategy for trade-ins.
<em>EJOR</em>, <em>323</em>(2), 583–598. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with the ever-increasing number of used cars, many automobile manufacturers now offer trade-in programs whereby they resell used cars to generate revenue. Consumers have the alternative of selling their used cars via an online peer-to-peer (P2P) resale platform, which charges a commission on each transaction. This paper studies a manufacturer’s traded-in resale strategy and assess how the manufacturer’s resale strategy and profits are affected by the presence of online P2P platforms. We find that in the absence of P2P platforms, the manufacturer may opt against implementing a resale program, whereas it will always do so in the presence of P2P platforms. This suggests a notable shift in manufacturers’ optimal choice of trade-in resale strategies due to the emergence of P2P platforms. Furthermore, the study reveals that the introduction of P2P platforms may diminishes the profits of manufacturers who have implemented a resale program. Importantly, the study underscores that manufacturers are not necessarily obliged to adopt a planned obsolescence strategy. When P2P platforms are absent, implementing a resale program allows manufacturers to increase profits by producing products that are either less or more durable. However, in the face of competition from P2P platforms, profitability can only be enhanced by making products more durable. This suggests that a platform’s emergence can alter how the depreciation rate affects a manufacturer’s profit and hence its optimal product design strategies. Understanding these dynamics is crucial for effectively navigating the growing used car market.},
  archive      = {J_EJOR},
  author       = {Shu Hu and Stuart X. Zhu and Ke Fu},
  doi          = {10.1016/j.ejor.2024.12.017},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {583-598},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {The manufacturer’s resale strategy for trade-ins},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connections between multiple-objective programming and
weight restricted data envelopment analysis: The role of the ordering
cone. <em>EJOR</em>, <em>323</em>(2), 571–582. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores some new, important and interesting connections between Multiple-Objective Programming (MOP) and Data Envelopment Analysis (DEA). We show that imposing weight restrictions in DEA corresponds to changing the ordering cone in MOP in a specific way. The new ordering cone is constructed and its properties are proved, providing useful insights about the connections between MOP and DEA. After providing several theoretical results, we illustrate them on a real-world data set. In addition to their theoretical appeal, our results hold significant practical importance for several reasons which are addressed in the paper.},
  archive      = {J_EJOR},
  author       = {Pekka Korhonen and Majid Soleimani-damaneh and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.12.002},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {571-582},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Connections between multiple-objective programming and weight restricted data envelopment analysis: The role of the ordering cone},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An incremental preference elicitation-based approach to
learning potentially non-monotonic preferences in multi-criteria
sorting. <em>EJOR</em>, <em>323</em>(2), 553–570. (<a
href="https://doi.org/10.1016/j.ejor.2024.11.047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging assignment example preference information, to determine the shape of marginal utility functions and category thresholds of the threshold-based multi-criteria sorting (MCS) model, has emerged as a focal point of current research within the realm of MCS. Most studies assume decision makers can provide all assignment example preference information in batch and that their preferences over criteria are monotonic, which may not align with practical MCS problems. This paper introduces a novel incremental preference elicitation-based approach to learning potentially non-monotonic preferences in MCS problems, enabling decision makers to progressively provide assignment example preference information. Specifically, we first construct a max-margin optimization-based model to model potentially non-monotonic preferences and inconsistent assignment example preference information in each iteration of the incremental preference elicitation process. Using the optimal objective function value of the max-margin optimization-based model, we devise information amount measurement methods and question selection strategies to pinpoint the most informative alternative in each iteration within the framework of uncertainty sampling in active learning. Once the termination criterion is satisfied, the sorting result for non-reference alternatives can be determined through the use of two optimization models, i.e., the max-margin optimization-based model and the complexity controlling optimization model. Subsequently, two incremental preference elicitation-based algorithms are developed to learn potentially non-monotonic preferences, considering different termination criteria. Ultimately, we apply the proposed approach to a firm financial state rating problem to elucidate the detailed implementation steps, and perform computational experiments on both artificial and real-world data sets to compare the proposed question selection strategies with several benchmark strategies.},
  archive      = {J_EJOR},
  author       = {Zhuolin Li and Zhen Zhang and Witold Pedrycz},
  doi          = {10.1016/j.ejor.2024.11.047},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {553-570},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An incremental preference elicitation-based approach to learning potentially non-monotonic preferences in multi-criteria sorting},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coevolutionary algorithm for exploiting a large fuzzy
outranking relation. <em>EJOR</em>, <em>323</em>(2), 540–552. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outranking approach in Multiple Criteria Decision Analysis (MCDA) uses ranking procedures to exploit a fuzzy outranking relation, which captures the decision maker&#39;s notion of a ranking. However, as decision problems become more complex and computer performance improves, new ranking procedures are needed to rank complex data sets that decision-makers may not interpret. This paper discusses recent efforts and potential directions for developing ranking procedures that use multiobjective evolutionary algorithms (MOEAs) to exploit a fuzzy outranking relation. After that, based on the cooperative coevolutionary algorithms (CCEA) approach, we suggest some fundamental modifications to extend the RP 2 -NSGA-II+H algorithm that improve the scalability of this MOEA to exploit large-sized fuzzy outranking relations. Empirical results indicate that adjustments improve the RP 2 -NSGA-II+H algorithm for the addressed problem. The proposed ranking procedure outperforms RP 2 -NSGA-II+H in terms of ranking error rates based on the experiments conducted. Our experimental results also demonstrate that the proposed approach can be scaled for instances of the ranking problem of up to one thousand alternatives.},
  archive      = {J_EJOR},
  author       = {Jesús Jaime Solano Noriega and Juan Carlos Leyva López and Carlos Andrés Oñate Ochoa and José Rui Figueira},
  doi          = {10.1016/j.ejor.2024.12.012},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {540-552},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A coevolutionary algorithm for exploiting a large fuzzy outranking relation},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain adoption and coordination strategies for green
supply chains considering consumer privacy concern. <em>EJOR</em>,
<em>323</em>(2), 525–539. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumers’ uncertainty about the value of green products will reduce their willingness to pay, thereby obstructing green product promotion. Blockchain can eliminate this uncertainty but bring privacy concerns. We develop a game theoretical model to study a green supply chain composed of one manufacturer and one retailer, aiming to explore the implications of partial or full blockchain adoption on green product manufacturing. Subsequently, we consider the use of revenue-sharing and cost-sharing contracts as mechanisms to coordinate the supply chain that adopts blockchain technologies. We show that adopting blockchain for some products benefits the manufacturer and the retailer, and consumers’ privacy concerns make it impossible for blockchain to be adopted for all products. Interestingly, partial or full blockchain adoption does not affect the green investment level. Furthermore, we find that revenue-sharing and cost-sharing contracts are always beneficial for the manufacturer. However, it can be beneficial for the retailer only when the revenue-sharing or cost-sharing ratio is small. Surprisingly, the effectiveness of the coordinating contract is not affected by consumers’ privacy concerns. Finally, when comparing the wholesale price contract with two coordination mechanisms, we find that the manufacturer and the retailer can agree on adopting a cost-sharing contract when both revenue- and cost-sharing ratios are low. When the revenue-sharing ratio is moderate and the cost-sharing ratio is low, a revenue-sharing contract is adopted. In all other cases, trading is conducted according to the wholesale price contract. These insights can contribute to optimize the application of blockchain in green supply chains.},
  archive      = {J_EJOR},
  author       = {Changhua Liao and Qihui Lu and Salar Ghamat and Helen Huifen Cai},
  doi          = {10.1016/j.ejor.2024.12.022},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {525-539},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Blockchain adoption and coordination strategies for green supply chains considering consumer privacy concern},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An operation-agnostic stochastic user equilibrium model for
mobility-on-demand networks with congestible capacities. <em>EJOR</em>,
<em>323</em>(2), 504–524. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the impact of privately-owned Mobility-on-Demand (MoD) services is important from a regulatory perspective. There is a need to model multimodal equilibria with MoD to support policymaking. While there exists a large body of literature on MoD services focusing on service design under equilibrium modeling, these studies commonly adopt assumptions of MoD operational policies. However, such policies might not be shared with regulatory agencies due to commercial privacy concerns of private operators. We model multimodal equilibrium with MoD systems in an operation-agnostic manner based on empirical observations of flow and capacity. This is done with a Flow-Capacity Interaction (FC) matrix that captures systematic effect of congestible capacities, a phenomenon in MoD systems where capacities are affected by flows. The FC matrix encapsulates the operation and demand patterns by capturing the empirical equilibrium relationship between flows and capacities. An operation-agnostic logit-based stochastic user equilibrium (SUE) formulation is proposed and proof of equivalence of the SUE formulation is derived. The proof shows that, unlike static capacities, path delays are not just the sum of the Lagrange multipliers of the links on the paths, but dependent on the whole network. We name this phenomenon as “non-separable link delays”. A solution algorithm that finds SUE with a bounded path set is proposed, with a custom Frank-Wolfe algorithm to solve the non-linear SUE formulation. Since the FC matrix cannot be directly observed, an inverse optimization problem is introduced to estimate it with observed flow and capacity data. Two numerical examples are provided with sensitivity tests. An empirical example with yellow taxi data of downtown Manhattan, NY is provided to demonstrate effectiveness of estimating the FC matrix from real data, and for determining the equilibrium that captures the underlying flow-capacity dynamics.},
  archive      = {J_EJOR},
  author       = {Bingqing Liu and David Watling and Joseph Y.J. Chow},
  doi          = {10.1016/j.ejor.2024.12.038},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {504-524},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {An operation-agnostic stochastic user equilibrium model for mobility-on-demand networks with congestible capacities},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product line extensions and distribution channels in
pharmaceutical supply chain. <em>EJOR</em>, <em>323</em>(2), 490–503.
(<a href="https://doi.org/10.1016/j.ejor.2024.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to aggressive generic competition after the original drug’s patent expires, various original firms extend their product lines by introducing an authorized generic drug with both lower quality and cost, either via internal distribution or third-party distribution. In this paper, we develop a game-theoretic model to investigate the product line extension and distribution channel decisions for an original firm that has already sold an original drug and considers introducing an authorized generic drug to compete against the generic firm. We show that product line extension enables the original firm to leverage the value of drug differentiation by price discriminating the patients with heterogeneous preferences for quality, but it also leads to original drug’s profit loss caused by the internal cannibalization. Given an internal distribution channel, when the cost gap is not small for the internal cannibalization to be less aggressive, the original firm will extend the product line, which could surprisingly benefit the generic firm but harm the patients. In contrast, under a third-party distribution channel, the original firm always prefers to extend the product line by setting a low wholesale price, which always reduces the generic firm’s profit but increases the patient surplus. Finally, contrary to the conventional wisdom that a decentralized channel always harms the original firm compared with a centralized one due to the double marginalization, our results suggest that when the original drug has a small cost gap or a large quality gap relative to the generic drug, the original firm is better off with using the third-party distribution to introduce the authorized generic drug than the internal distribution, as it permits higher original drug’s profit due to alleviated internal cannibalization, although at the expense of lower authorized generic drug’s profit.},
  archive      = {J_EJOR},
  author       = {Ran Tao and Yanfei Lan and Ruiqing Zhao and Rong Gao},
  doi          = {10.1016/j.ejor.2024.12.013},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {490-503},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Product line extensions and distribution channels in pharmaceutical supply chain},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated differentiated time slot pricing and order
dispatching with uncertain customer demand in on-demand food delivery.
<em>EJOR</em>, <em>323</em>(2), 471–489. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiated time slot pricing (DTSP) is a promising approach to enhance the efficiency and cost-effectiveness of food delivery platforms by influencing customers’ choices regarding delivery time slots. In this paper, we investigate the integrated problem of DTSP at the tactical level and order dispatching at the operational level, formulating it as a two-stage stochastic programming model. The first-stage model determines the delivery price for each time slot to maximize the system’s expected profit. The second-stage model generates the optimal order dispatching plan to minimize the generalized system cost under each stochastic scenario. To efficiently estimate the order dispatching cost for each scenario, we develop an order consolidation dispatching algorithm (OCDA) to solve the second-stage order dispatching subproblem under each demand scenario. Building on OCDA, we propose a hybrid adaptive large neighborhood search (HALNS) heuristic to solve the integrated problem. Extensive case studies based on real-world data verify the effectiveness of the proposed approach and demonstrate the benefits of DTSP strategy. Our numerical analysis provides important managerial insights for operating food delivery platforms.},
  archive      = {J_EJOR},
  author       = {Bo Zhang and Elkafi Hassini and Yun Zhou and Meng Zhao and Xiangpei Hu},
  doi          = {10.1016/j.ejor.2024.12.011},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {471-489},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Integrated differentiated time slot pricing and order dispatching with uncertain customer demand in on-demand food delivery},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal forecast reconciliation with time series selection.
<em>EJOR</em>, <em>323</em>(2), 455–470. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecast reconciliation ensures forecasts of time series in a hierarchy adhere to aggregation constraints, enabling aligned decision making. While forecast reconciliation can enhance overall accuracy in a hierarchical or grouped structure, it can lead to worse forecasts for certain series, with the greatest gains typically seen in series that originally have poorly performing base forecasts. In practical applications, some series in a structure often produce poor base forecasts due to model misspecification or low forecastability. To mitigate their negative impact, we propose two categories of forecast reconciliation methods that incorporate automatic time series selection based on out-of-sample and in-sample information, respectively. These methods keep “poor” base forecasts unused in forming reconciled forecasts, while adjusting the weights assigned to the remaining series accordingly when generating bottom-level reconciled forecasts. Additionally, our methods ameliorate disparities stemming from varied estimators of the base forecast error covariance matrix, alleviating challenges associated with estimator selection. Empirical evaluations through two simulation studies and applications using Australian labour force and domestic tourism data demonstrate the potential of the proposed methods to exclude series with high scaled forecast errors and show promising results.},
  archive      = {J_EJOR},
  author       = {Xiaoqian Wang and Rob J. Hyndman and Shanika L. Wickramasuriya},
  doi          = {10.1016/j.ejor.2024.12.004},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {455-470},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Optimal forecast reconciliation with time series selection},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the discrete and continuous edge improvement
problems: Models and algorithms. <em>EJOR</em>, <em>323</em>(2),
441–454. (<a href="https://doi.org/10.1016/j.ejor.2024.12.051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the edge improvement problem where the fixed edge traversal time assumption of the traditional network flow problems is relaxed. We consider two variants of the problem: one where improvement decisions are restricted to a discrete set (discrete edge improvement problem), and the other where they can take any value within a specified range (continuous edge improvement problem). We first analyze both problem variants on a tree-shaped network and discuss their computational complexities. For the general case, where the underlying network has no special structure, we provide mixed-integer programming (MIP) formulations for both versions of the problem. To the best of our knowledge, this study is the first to propose and compare different formulations for the discrete edge improvement problem and to present a formulation for the continuous edge improvement problem. Since the developed models do not perform well for medium and large problem instances, we introduce a Benders decomposition algorithm to solve the discrete edge improvement problem. Additionally, we employ it heuristically to find high-quality solution for the continuous edge improvement problem within reasonable times. We also devise an MIP formulation to find lower bounds for the continuous edge improvement problem, leveraging the McCormick envelopes and optimal solution properties. Our experiments demonstrate that the Benders decomposition algorithm outperforms the other formulations for the discrete edge improvement problem, while the heuristic method proposed for the continuous edge improvement problem provides quite well results even for large problem instances.},
  archive      = {J_EJOR},
  author       = {Esra Koca and A. Burak Paç},
  doi          = {10.1016/j.ejor.2024.12.051},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {441-454},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Exploring the discrete and continuous edge improvement problems: Models and algorithms},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and effective breakpoints heuristic algorithm for the
quadratic knapsack problem. <em>EJOR</em>, <em>323</em>(2), 425–440. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Quadratic Knapsack Problem (QKP) involves selecting a subset of elements that maximizes the sum of pairwise and singleton utilities without exceeding a given budget. The pairwise utilities are nonnegative, the singleton utilities may be positive, negative, or zero, and the node costs are nonnegative. We introduce a Breakpoints Algorithm for QKP, named QKBP, which is based on a technique proposed in Hochbaum (2009) for efficiently generating the concave envelope of the solutions to the relaxation of the problem for all values of the budget. Our approach utilizes the fact that breakpoints in the concave envelopes are optimal solutions for their respective budgets. For budgets between breakpoints, a fast greedy heuristic derives high-quality solutions from the optimal solutions of adjacent breakpoints. The QKBP algorithm is a heuristic which is highly scalable due to an efficient parametric cut procedure used to generate the concave envelope. This efficiency is further improved by a newly developed compact problem formulation. Our extensive computational study on both existing and new benchmark instances, with up to 10,000 elements, shows that while some leading algorithms perform well on a few instances, QKBP consistently delivers high-quality solutions regardless of instance size, density, or budget. Moreover, QKBP achieves these results in significantly faster running times than all leading algorithms. The source code of the QKBP algorithm, the benchmark instances, and the detailed results are publicly available on GitHub.},
  archive      = {J_EJOR},
  author       = {D.S. Hochbaum and P. Baumann and O. Goldschmidt and Y. Zhang},
  doi          = {10.1016/j.ejor.2024.12.019},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {425-440},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {A fast and effective breakpoints heuristic algorithm for the quadratic knapsack problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving the multiobjective quasi-clique problem.
<em>EJOR</em>, <em>323</em>(2), 409–424. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a simple undirected graph G , a quasi-clique is a subgraph of G whose density is at least γ ( 0 &lt; γ ≤ 1 ) . Finding a maximum quasi-clique has been addressed from two different perspectives: ( i ) maximizing vertex cardinality for a given edge density; and ( i i ) maximizing edge density for a given vertex cardinality. However, when no a priori preference information about cardinality and density is available, a more natural approach is to consider the problem from a multiobjective perspective. We introduce the Multiobjective Quasi-clique (MOQC) problem, which aims to find a quasi-clique by simultaneously maximizing both vertex cardinality and edge density. To efficiently address this problem, we explore the relationship among MOQC, its single-objective counterpart problems, and a bi-objective optimization problem, along with several properties of the MOQC problem and quasi-cliques. We propose a baseline approach using ɛ -constraint scalarization and introduce a Two-phase strategy, which applies a dichotomic search based on weighted sum scalarization in the first phase and an ɛ -constraint methodology in the second phase. Additionally, we present a Three-phase strategy that combines the dichotomic search used in Two-phase with a vertex-degree-based local search employing novel sufficient conditions to assess quasi-clique efficiency, followed by an ɛ -constraint in a final stage. Experimental results on synthetic and real-world sparse graphs indicate that the integrated use of dichotomic search and local search, together with mechanisms to assess quasi-clique efficiency, makes the Three-phase strategy an effective approach for solving the MOQC problem in sparse graphs in terms of running time and ability to produce new efficient quasi-cliques.},
  archive      = {J_EJOR},
  author       = {Daniela Scherer dos Santos and Kathrin Klamroth and Pedro Martins and Luís Paquete},
  doi          = {10.1016/j.ejor.2024.12.018},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {409-424},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Solving the multiobjective quasi-clique problem},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete optimization: A quantum revolution? <em>EJOR</em>,
<em>323</em>(2), 378–408. (<a
href="https://doi.org/10.1016/j.ejor.2024.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop several quantum procedures and investigate their potential to solve discrete optimization problems. First, we introduce a binary search procedure and illustrate how it can be used to effectively solve the binary knapsack problem. Next, we introduce two other procedures: a hybrid branch-and-bound procedure that allows to exploit the structure of the problem and a random-ascent procedure that can be used to solve problems that have no clear structure and/or are difficult to solve using traditional methods. We explain how to assess the performance of these procedures and perform an elaborate computational experiment. Our results show that we can match the worst-case performance of the best classical algorithms when solving the binary knapsack problem. After improving and generalizing our procedures, we show that they can be used to solve any discrete optimization problem. To illustrate, we show how to solve the quadratic binary knapsack problem. For this problem, our procedures outperform the best classical algorithms. In addition, we demonstrate that our procedures can be used as heuristics to find (near-) optimal solutions in limited time Not only does our work provide the tools required to explore a myriad of future research directions, it also shows that quantum computing has the potential to revolutionize the field of discrete optimization.},
  archive      = {J_EJOR},
  author       = {Stefan Creemers and Luis Fernando Pérez Armas},
  doi          = {10.1016/j.ejor.2024.12.016},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {378-408},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Discrete optimization: A quantum revolution?},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifty years of multiple criteria decision analysis: From
classical methods to robust ordinal regression. <em>EJOR</em>,
<em>323</em>(2), 351–377. (<a
href="https://doi.org/10.1016/j.ejor.2024.07.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Criteria Decision Analysis (MCDA) is a subfield of Operational Research that aims to support Decision-Makers (DMs) in the decision-making process through mathematical models and computational procedures. In this perspective, MCDA employs structured and traceable protocols to identify potential actions and the criteria for evaluating them. MCDA procedures aim to define recommendations consistent with the preferences of DMs for the specific decision problem at hand. These problems are generally formulated in terms of either choosing the best action, classifying actions into pre-defined and ordered decision classes, or ranking actions from best to worst. As the evaluation criteria are generally conflicting, the main challenge is to aggregate them into a mathematical preference model representing the DM value system. We review the development of MCDA over the past fifty years and describe its evolution with examples of distinctive methods. They are distinguished by the type of preference information elicited by DMs, the type of the preference model (criteria aggregation), and the way of converting the preference relation induced by the preference model in the set of potential actions into a decision recommendation. We focus on MCDA methods with a finite set of actions. References to specific application areas will be given. In the conclusion section, some prospective avenues of research will be outlined.},
  archive      = {J_EJOR},
  author       = {Salvatore Greco and Roman Słowiński and Jyrki Wallenius},
  doi          = {10.1016/j.ejor.2024.07.038},
  journal      = {European Journal of Operational Research},
  month        = {6},
  number       = {2},
  pages        = {351-377},
  shortjournal = {Eur. J. Oper. Res.},
  title        = {Fifty years of multiple criteria decision analysis: From classical methods to robust ordinal regression},
  volume       = {323},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
