<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NATMI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="natmi---20">NATMI - 20</h2>
<ul>
<li><details>
<summary>
(2025). Author correction: Kernel approximation using analogue
in-memory computing. <em>NATMI</em>, <em>7</em>(2), 328. (<a
href="https://doi.org/10.1038/s42256-025-00996-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NATMI},
  author       = {Büchel, Julian and Camposampiero, Giacomo and Vasilopoulos, Athanasios and Lammie, Corey and Le Gallo, Manuel and Rahimi, Abbas and Sebastian, Abu},
  doi          = {10.1038/s42256-025-00996-x},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {328},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Author correction: Kernel approximation using analogue in-memory computing},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantitative analysis of knowledge-learning preferences in
large language models in molecular science. <em>NATMI</em>,
<em>7</em>(2), 315–327. (<a
href="https://doi.org/10.1038/s42256-024-00977-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has significantly advanced molecular modelling and design, enabling an efficient understanding and discovery of novel molecules. In particular, large language models introduce a fresh research paradigm to tackle scientific problems from a natural language processing perspective. Large language models significantly enhance our understanding and generation of molecules, often surpassing existing methods with their capabilities to decode and synthesize complex molecular patterns. However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multimodal benchmark, named ChEBI-20-MM, and perform 1,263 experiments to assess the model’s compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our analysis offers an exploration of the learning mechanism and paves the way for advancing large language models in molecular science. Large language models promise substantial advances in molecular modelling and design. A multimodal benchmark is proposed to analyse performance, and 1,263 experiments are conducted to examine the compatibility of a large language model with data modalities and knowledge acquisition.},
  archive      = {J_NATMI},
  author       = {Liu, Pengfei and Tao, Jun and Ren, Zhixiang},
  doi          = {10.1038/s42256-024-00977-6},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {315-327},
  shortjournal = {Nat. Mach. Intell.},
  title        = {A quantitative analysis of knowledge-learning preferences in large language models in molecular science},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering fully semantic representations via centroid- and
orientation-aware feature learning. <em>NATMI</em>, <em>7</em>(2),
307–314. (<a href="https://doi.org/10.1038/s42256-024-00978-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning meaningful representations of images in scientific domains that are robust to variations in centroids and orientations remains an important challenge. Here we introduce centroid- and orientation-aware disentangling autoencoder (CODAE), an encoder–decoder-based neural network that learns meaningful content of objects in a latent space. Specifically, a combination of a translation- and rotation-equivariant encoder, Euler encoding and an image moment loss enables CODAE to extract features invariant to positions and orientations of objects of interest from randomly translated and rotated images. We evaluate this approach on several publicly available scientific datasets, including protein images from life sciences, four-dimensional scanning transmission electron microscopy data from material science and galaxy images from astronomy. The evaluation shows that CODAE learns centroids, orientations and their invariant features and outputs, as well as aligned reconstructions and the exact view reconstructions of the input images with high quality. Cha and colleagues present a translation- and rotation-equivariant autoencoder-based method for robust image recognition, which they demonstrate on diverse tasks from bioinformatics, material science and astronomy.},
  archive      = {J_NATMI},
  author       = {Cha, Jaehoon and Park, Jinhae and Pinilla, Samuel and Morris, Kyle L. and Allen, Christopher S. and Wilkinson, Mark I. and Thiyagalingam, Jeyan},
  doi          = {10.1038/s42256-024-00978-5},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {307-314},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Discovering fully semantic representations via centroid- and orientation-aware feature learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning approach to leveraging electronic health
records for enhanced omics analysis. <em>NATMI</em>, <em>7</em>(2),
293–306. (<a href="https://doi.org/10.1038/s42256-024-00974-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omics studies produce a large number of measurements, enabling the development, validation and interpretation of systems-level biological models. Large cohorts are required to power these complex models; yet, the cohort size remains limited due to clinical and budgetary constraints. We introduce clinical and omics multimodal analysis enhanced with transfer learning (COMET), a machine learning framework that incorporates large, observational electronic health record databases and transfer learning to improve the analysis of small datasets from omics studies. By pretraining on electronic health record data and adaptively blending both early and late fusion strategies, COMET overcomes the limitations of existing multimodal machine learning methods. Using two independent datasets, we showed that COMET improved the predictive modelling performance and biological discovery compared with the analysis of omics data with traditional methods. By incorporating electronic health record data into omics analyses, COMET enables more precise patient classifications, beyond the simplistic binary reduction to cases and controls. This framework can be broadly applied to the analysis of multimodal omics studies and reveals more powerful biological insights from limited cohort sizes. COMET, an artificial intelligence method that improves the analysis of small medical studies using large clinical databases, has been created. COMET can help develop better artificial intelligence tools and identify key biomarkers across many diseases, potentially changing medical research.},
  archive      = {J_NATMI},
  author       = {Mataraso, Samson J. and Espinosa, Camilo A. and Seong, David and Reincke, S. Momsen and Berson, Eloise and Reiss, Jonathan D. and Kim, Yeasul and Ghanem, Marc and Shu, Chi-Hung and James, Tomin and Tan, Yuqi and Shome, Sayane and Stelzer, Ina A. and Feyaerts, Dorien and Wong, Ronald J. and Shaw, Gary M. and Angst, Martin S. and Gaudilliere, Brice and Stevenson, David K. and Aghaeepour, Nima},
  doi          = {10.1038/s42256-024-00974-9},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {293-306},
  shortjournal = {Nat. Mach. Intell.},
  title        = {A machine learning approach to leveraging electronic health records for enhanced omics analysis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified cross-attention model for predicting antigen
binding specificity to both HLA and TCR molecules. <em>NATMI</em>,
<em>7</em>(2), 278–292. (<a
href="https://doi.org/10.1038/s42256-024-00973-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immune checkpoint inhibitors have demonstrated promising clinical efficacy across various tumour types, yet the percentage of patients who benefit from them remains low. The bindings between tumour antigens and human leukocyte antigen class I/T cell receptor molecules determine the antigen presentation and T cell activation, thereby playing an important role in the immunotherapy response. In this paper, we propose UnifyImmun, a unified cross-attention transformer model designed to simultaneously predict the bindings of peptides to both receptors, providing more comprehensive evaluation of antigen immunogenicity. We devise a two-phase strategy using virtual adversarial training that enables these two tasks to reinforce each other mutually, by compelling the encoders to extract more expressive features. Our method demonstrates superior performance in predicting both peptide-HLA and peptide-TCR binding on multiple independent and external test sets. Notably, on a large-scale COVID-19 peptide-TCR binding test set without any seen peptide in the training set, our method outperforms the current state-of-the-art methods by more than 10%. The predicted binding scores significantly correlate with the immunotherapy response and clinical outcomes on two clinical cohorts. Furthermore, the cross-attention scores and integrated gradients reveal the amino acid sites critical for peptide binding to receptors. In essence, our approach marks an essential step towards comprehensive evaluation of antigen immunogenicity. This work proposes a deep learning model based on the cross-attention mechanism to simultaneously predict peptide–HLA and peptide–TCR bindings. Experiments verify that its performance for both prediction tasks on multiple test sets compares favourably with previous methods.},
  archive      = {J_NATMI},
  author       = {Yu, Chenpeng and Fang, Xing and Tian, Shiye and Liu, Hui},
  doi          = {10.1038/s42256-024-00973-w},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {278-292},
  shortjournal = {Nat. Mach. Intell.},
  title        = {A unified cross-attention model for predicting antigen binding specificity to both HLA and TCR molecules},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Battery lifetime prediction across diverse ageing conditions
with inter-cell deep learning. <em>NATMI</em>, <em>7</em>(2), 270–277.
(<a href="https://doi.org/10.1038/s42256-024-00972-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting battery lifetime in early cycles holds tremendous value in real-world applications. However, this task poses significant challenges due to diverse factors influencing complex battery capacity degradation, such as cycling protocols, ambient temperatures and electrode materials. Moreover, cycling under specific conditions is both resource-intensive and time-consuming. Existing predictive models, primarily developed and validated within a restricted set of ageing conditions, thus raise doubts regarding their extensive applicability. Here we introduce BatLiNet, a deep learning framework tailored to predict battery lifetime reliably across a variety of ageing conditions. The distinctive design is integrating an inter-cell learning mechanism to predict the lifetime differences between two battery cells. This mechanism, when combined with conventional single-cell learning, enhances the stability of lifetime predictions for a target cell under varied ageing conditions. Our experimental results, derived from a broad spectrum of ageing conditions, demonstrate BatLiNet’s superior accuracy and robustness compared to existing models. BatLiNet also exhibits transferring capabilities across different battery chemistries, benefitting scenarios with limited resources. We expect this study could promote exploration of cross-cell insights and facilitate battery research across comprehensive ageing factors. Zhang and colleagues introduce an inter-cell learning mechanism to predict battery lifetime in the presence of diverse ageing conditions.},
  archive      = {J_NATMI},
  author       = {Zhang, Han and Li, Yuqi and Zheng, Shun and Lu, Ziheng and Gui, Xiaofan and Xu, Wei and Bian, Jiang},
  doi          = {10.1038/s42256-024-00972-x},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {270-277},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Battery lifetime prediction across diverse ageing conditions with inter-cell deep learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preserving and combining knowledge in robotic lifelong
reinforcement learning. <em>NATMI</em>, <em>7</em>(2), 256–269. (<a
href="https://doi.org/10.1038/s42256-025-00983-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can continually accumulate knowledge and develop increasingly complex behaviours and skills throughout their lives, which is a capability known as ‘lifelong learning’. Although this lifelong learning capability is considered an essential mechanism that makes up general intelligence, recent advancements in artificial intelligence predominantly excel in narrow, specialized domains and generally lack this lifelong learning capability. Here we introduce a robotic lifelong reinforcement learning framework that addresses this gap by developing a knowledge space inspired by the Bayesian non-parametric domain. In addition, we enhance the agent’s semantic understanding of tasks by integrating language embeddings into the framework. Our proposed embodied agent can consistently accumulate knowledge from a continuous stream of one-time feeding tasks. Furthermore, our agent can tackle challenging real-world long-horizon tasks by combining and reapplying its acquired knowledge from the original tasks stream. The proposed framework advances our understanding of the robotic lifelong learning process and may inspire the development of more broadly applicable intelligence. Humans continuously acquire knowledge and develop complex behaviours. Meng, Bing, Yao and colleagues present a robotic lifelong learning framework using a Bayesian non-parametric knowledge space, enabling agents to dynamically preserve and integrate knowledge from sequential tasks, enhancing adaptability.},
  archive      = {J_NATMI},
  author       = {Meng, Yuan and Bing, Zhenshan and Yao, Xiangtong and Chen, Kejia and Huang, Kai and Gao, Yang and Sun, Fuchun and Knoll, Alois},
  doi          = {10.1038/s42256-025-00983-2},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {256-269},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Preserving and combining knowledge in robotic lifelong reinforcement learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-based generation for molecule design with SketchMol.
<em>NATMI</em>, <em>7</em>(2), 244–255. (<a
href="https://doi.org/10.1038/s42256-025-00982-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient molecular design methods are crucial for accelerating early stage drug discovery, potentially saving years of development time and billions of dollars in costs. Current molecular design methods rely on sequence-based or graph-based representations, emphasizing local features such as bonds and atoms but lacking a comprehensive depiction of the overall molecular topology. Here we introduce SketchMol, an image-based molecular generation framework that combines visual understanding with molecular design. SketchMol leverages diffusion models and applies a refinement technique called reinforcement learning from molecular experts to improve the generation of viable molecules. It creates molecules through a painting-like approach that simultaneously depicts local structures and global layout of the molecule. By visualizing molecular structures, various design tasks are unified within a single image-based framework. De novo design becomes sketching new molecular images, whereas editing tasks transform into filling partially drawn images. Through extensive experiments, we demonstrated that SketchMol effectively handles a variety of molecular design tasks. SketchMol is a model that explores the feasibility of incorporating image generation techniques into the field of small-molecule design.},
  archive      = {J_NATMI},
  author       = {Wang, Zixu and Chen, Yangyang and Ma, Pengsen and Yu, Zhou and Wang, Jianmin and Liu, Yuansheng and Ye, Xiucai and Sakurai, Tetsuya and Zeng, Xiangxiang},
  doi          = {10.1038/s42256-025-00982-3},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {244-255},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Image-based generation for molecule design with SketchMol},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning enhances the prediction of HLA class
i-presented CD8+ t cell epitopes in foreign pathogens. <em>NATMI</em>,
<em>7</em>(2), 232–243. (<a
href="https://doi.org/10.1038/s42256-024-00971-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate in silico determination of CD8+ T cell epitopes would greatly enhance T cell-based vaccine development, but current prediction models are not reliably successful. Here, motivated by recent successes applying machine learning to complex biology, we curated a dataset of 651,237 unique human leukocyte antigen class I (HLA-I) ligands and developed MUNIS, a deep learning model that identifies peptides presented by HLA-I alleles. MUNIS shows improved performance compared with existing models in predicting peptide presentation and CD8+ T cell epitope immunodominance hierarchies. Moreover, application of MUNIS to proteins from Epstein–Barr virus led to successful identification of both established and novel HLA-I epitopes which were experimentally validated by in vitro HLA-I-peptide stability and T cell immunogenicity assays. MUNIS performs comparably to an experimental stability assay in terms of immunogenicity prediction, suggesting that deep learning can reduce experimental burden and accelerate identification of CD8+ T cell epitopes for rapid T cell vaccine development. Accurate prediction of immunogenic CD8+ T cell epitopes would greatly accelerate T cell vaccine development. A new deep learning model, MUNIS, can rapidly identify HLA-binding, immunogenic and immunodominant peptides in foreign pathogens.},
  archive      = {J_NATMI},
  author       = {Wohlwend, Jeremy and Nathan, Anusha and Shalon, Nitan and Crain, Charles R. and Tano-Menka, Rhoda and Goldberg, Benjamin and Richards, Emma and Gaiha, Gaurav D. and Barzilay, Regina},
  doi          = {10.1038/s42256-024-00971-y},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {232-243},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Deep learning enhances the prediction of HLA class I-presented CD8+ t cell epitopes in foreign pathogens},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What large language models know and what people think they
know. <em>NATMI</em>, <em>7</em>(2), 221–231. (<a
href="https://doi.org/10.1038/s42256-024-00976-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. Whereas recent work has focused on LLMs’ internal confidence, less is understood about how effectively they convey uncertainty to users. Here we explore the calibration gap, which refers to the difference between human confidence in LLM-generated answers and the models’ actual confidence, and the discrimination gap, which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Moreover, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models’ internal confidence, both the calibration gap and the discrimination gap narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in artificial-intelligence-assisted decision-making environments. Understanding how people perceive and interpret uncertainty from large language models (LLMs) is crucial, as users often overestimate LLM accuracy, especially with default explanations. Steyvers et al. show that aligning LLM explanations with their internal confidence improves user perception.},
  archive      = {J_NATMI},
  author       = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas W. and Smyth, Padhraic},
  doi          = {10.1038/s42256-024-00976-7},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {221-231},
  shortjournal = {Nat. Mach. Intell.},
  title        = {What large language models know and what people think they know},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Goals as reward-producing programs. <em>NATMI</em>,
<em>7</em>(2), 205–220. (<a
href="https://doi.org/10.1038/s42256-025-00981-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People are remarkably capable of generating their own goals, beginning with child’s play and continuing into adulthood. Despite considerable empirical and computational work on goals and goal-oriented behaviour, models are still far from capturing the richness of everyday human goals. Here we bridge this gap by collecting a dataset of human-generated playful goals (in the form of scorable, single-player games), modelling them as reward-producing programs and generating novel human-like goals through program synthesis. Reward-producing programs capture the rich semantics of goals through symbolic operations that compose, add temporal constraints and allow program execution on behavioural traces to evaluate progress. To build a generative model of goals, we learn a fitness function over the infinite set of possible goal programs and sample novel goals with a quality-diversity algorithm. Human evaluators found that model-generated goals, when sampled from partitions of program space occupied by human examples, were indistinguishable from human-created games. We also discovered that our model’s internal fitness scores predict games that are evaluated as more fun to play and more human-like. To enable artificial agents to generate human-like goals, a model must capture the complexity and diversity of human goals. Davidson et al. model playful goals from a naturalistic experiment as reward-producing programs, mapping an agent’s behaviour to goal success. They then develop a computational model to generate diverse human-like goals.},
  archive      = {J_NATMI},
  author       = {Davidson, Guy and Todd, Graham and Togelius, Julian and Gureckis, Todd M. and Lake, Brenden M.},
  doi          = {10.1038/s42256-025-00981-4},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {205-220},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Goals as reward-producing programs},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary optimization of model merging recipes.
<em>NATMI</em>, <em>7</em>(2), 195–204. (<a
href="https://doi.org/10.1038/s42256-024-00975-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. Although model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models such as a Japanese LLM with math reasoning capabilities. Surprisingly, our Japanese math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with substantially more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally aware Japanese vision–language model generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese vision–language models. This work not only contributes new state-of-the-art models back to the open-source community but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development. Akiba et al. developed an evolutionary approach to automatically merge artificial intelligence models, creating powerful hybrid models without extensive training. The method produces models with enhanced mathematical and visual capabilities that outperform larger models.},
  archive      = {J_NATMI},
  author       = {Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
  doi          = {10.1038/s42256-024-00975-8},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {195-204},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Evolutionary optimization of model merging recipes},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking machine unlearning for large language models.
<em>NATMI</em>, <em>7</em>(2), 181–194. (<a
href="https://doi.org/10.1038/s42256-025-00985-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore machine unlearning in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (for example, sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative artificial intelligence that is not only safe, secure and trustworthy but also resource-efficient without the need for full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, for example, unlearning scope, data–model interaction and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction. Machine unlearning techniques remove undesirable data and associated model capabilities while preserving essential knowledge, so that machine learning models can be updated without costly retraining. Liu et al. review recent advances and opportunities in machine unlearning in LLMs, revisiting methodologies and overlooked principles for future improvements and exploring emerging applications in copyright and privacy safeguards and in reducing sociotechnical harms.},
  archive      = {J_NATMI},
  author       = {Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and Varshney, Kush R. and Bansal, Mohit and Koyejo, Sanmi and Liu, Yang},
  doi          = {10.1038/s42256-025-00985-0},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {181-194},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Rethinking machine unlearning for large language models},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the caveats of AI autophagy. <em>NATMI</em>,
<em>7</em>(2), 172–180. (<a
href="https://doi.org/10.1038/s42256-025-00984-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence (AI) technologies and large models are producing realistic outputs across various domains, such as images, text, speech and music. Creating these advanced generative models requires significant resources, particularly large and high-quality datasets. To minimize training expenses, many algorithm developers use data created by the models themselves as a cost-effective training solution. However, not all synthetic data effectively improve model performance, necessitating a strategic balance in the use of real versus synthetic data to optimize outcomes. Currently, the previously well-controlled integration of real and synthetic data is becoming uncontrollable. The widespread and unregulated dissemination of synthetic data online leads to the contamination of datasets traditionally compiled through web scraping, now mixed with unlabelled synthetic data. This trend, known as the AI autophagy phenomenon, suggests a future where generative AI systems may increasingly consume their own outputs without discernment, raising concerns about model performance, reliability and ethical implications. What will happen if generative AI continuously consumes itself without discernment? What measures can we take to mitigate the potential adverse effects? To address these research questions, this Perspective examines the existing literature, delving into the consequences of AI autophagy, analysing the associated risks and exploring strategies to mitigate its impact. Our aim is to provide a comprehensive perspective on this phenomenon advocating for a balanced approach that promotes the sustainable development of generative AI technologies in the era of large models. With widespread generation and availability of synthetic data, AI systems are increasingly trained on their own outputs, leading to various technical and ethical challenges. The authors analyse this development and discuss measures to mitigate the potential adverse effects of ‘AI eating itself’.},
  archive      = {J_NATMI},
  author       = {Xing, Xiaodan and Shi, Fadong and Huang, Jiahao and Wu, Yinzhe and Nan, Yang and Zhang, Sheng and Fang, Yingying and Roberts, Michael and Schönlieb, Carola-Bibiane and Del Ser, Javier and Yang, Guang},
  doi          = {10.1038/s42256-025-00984-1},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {172-180},
  shortjournal = {Nat. Mach. Intell.},
  title        = {On the caveats of AI autophagy},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging peptide presentation and t cell recognition with
multi-task learning. <em>NATMI</em>, <em>7</em>(2), 170–171. (<a
href="https://doi.org/10.1038/s42256-025-01004-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immunogenic binding interactions of antigens are complex and interconnected. A new transformer-based model can simultaneously predict the bindings of antigens to two main receptors.},
  archive      = {J_NATMI},
  author       = {Su, Li and Wang, Duolin and Xu, Dong},
  doi          = {10.1038/s42256-025-01004-y},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {170-171},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Bridging peptide presentation and t cell recognition with multi-task learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On board with COMET to improve omics prediction models.
<em>NATMI</em>, <em>7</em>(2), 168–169. (<a
href="https://doi.org/10.1038/s42256-025-00990-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of omics prediction models can be significantly improved by combining limited patient proteomic data with widely available electronic health records.},
  archive      = {J_NATMI},
  author       = {Fogel, Paul and Luta, George},
  doi          = {10.1038/s42256-025-00990-3},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {168-169},
  shortjournal = {Nat. Mach. Intell.},
  title        = {On board with COMET to improve omics prediction models},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical benchmarks for testing algorithms. <em>NATMI</em>,
<em>7</em>(2), 166–167. (<a
href="https://doi.org/10.1038/s42256-025-00999-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of comprehensive benchmarks to assess the performance of algorithms on causal tasks is an important, emerging area. The introduction of two physical ‘causal chamber’ systems serves as a firm step towards future, more reliable benchmarks in the field.},
  archive      = {J_NATMI},
  author       = {Zeitler, Jakob},
  doi          = {10.1038/s42256-025-00999-8},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {166-167},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Physical benchmarks for testing algorithms},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why the carbon footprint of generative large language models
alone will not help us assess their sustainability. <em>NATMI</em>,
<em>7</em>(2), 164–165. (<a
href="https://doi.org/10.1038/s42256-025-00979-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing awareness of the substantial environmental costs of large language models (LLMs), but discussing the sustainability of LLMs only in terms of CO2 emissions is not enough. This Comment emphasizes the need to take into account the social and ecological costs and benefits of LLMs as well.},
  archive      = {J_NATMI},
  author       = {Bossert, Leonie N. and Loh, Wulf},
  doi          = {10.1038/s42256-025-00979-y},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {164-165},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Why the carbon footprint of generative large language models alone will not help us assess their sustainability},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The promise of generative AI for suicide prevention in
india. <em>NATMI</em>, <em>7</em>(2), 162–163. (<a
href="https://doi.org/10.1038/s42256-025-00992-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NATMI},
  author       = {Chakraborty, Tanmoy and Sinha Deb, Koushik and Kulkarni, Himanshu and Masud, Sarah and Math, Suresh Bada and Oke, Gayatri and Sagar, Rajesh and Sharma, Mona},
  doi          = {10.1038/s42256-025-00992-1},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {162-163},
  shortjournal = {Nat. Mach. Intell.},
  title        = {The promise of generative AI for suicide prevention in india},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seeking visions for sustainable AI. <em>NATMI</em>,
<em>7</em>(2), 161. (<a
href="https://doi.org/10.1038/s42256-025-01008-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As countries around the world heavily invest in artificial intelligence (AI) and related infrastructure, the sustainable development of AI technology needs to be higher on the global agenda.},
  archive      = {J_NATMI},
  doi          = {10.1038/s42256-025-01008-8},
  journal      = {Nature Machine Intelligence},
  month        = {2},
  number       = {2},
  pages        = {161},
  shortjournal = {Nat. Mach. Intell.},
  title        = {Seeking visions for sustainable AI},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
