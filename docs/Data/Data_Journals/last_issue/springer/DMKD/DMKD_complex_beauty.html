<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---6">DMKD - 6</h2>
<ul>
<li><details>
<summary>
(2025). Missing value replacement in strings and applications.
<em>DMKD</em>, <em>39</em>(2), 1–50. (<a
href="https://doi.org/10.1007/s10618-024-01074-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values arise routinely in real-world sequential (string) datasets due to: (1) imprecise data measurements; (2) flexible sequence modeling, such as binding profiles of molecular sequences; or (3) the existence of confidential information in a dataset which has been deleted deliberately for privacy protection. In order to analyze such datasets, it is often important to replace each missing value, with one or more valid letters, in an efficient and effective way. Here we formalize this task as a combinatorial optimization problem: the set of constraints includes the context of the missing value (i.e., its vicinity) as well as a finite set of user-defined forbidden patterns, modeling, for instance, implausible or confidential patterns; and the objective function seeks to minimize the number of new letters we introduce. Algorithmically, our problem translates to finding shortest paths in special graphs that contain forbidden edges representing the forbidden patterns. Our work makes the following contributions: (1) we design a linear-time algorithm to solve this problem for strings over constant-sized alphabets; (2) we show how our algorithm can be effortlessly applied to fully sanitize a private string in the presence of a set of fixed-length forbidden patterns [Bernardini et al. 2021a]; (3) we propose a methodology for sanitizing and clustering a collection of private strings that utilizes our algorithm and an effective and efficiently computable distance measure; and (4) we present extensive experimental results showing that our methodology can efficiently sanitize a collection of private strings while preserving clustering quality, outperforming the state of the art and baselines. To arrive at our theoretical results, we employ techniques from formal languages and combinatorial pattern matching.},
  archive      = {J_DMKD},
  author       = {Bernardini, Giulia and Liu, Chang and Loukides, Grigorios and Marchetti-Spaccamela, Alberto and Pissis, Solon P. and Stougie, Leen and Sweering, Michelle},
  doi          = {10.1007/s10618-024-01074-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-50},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Missing value replacement in strings and applications},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative evaluation of clustering-based outlier
detection. <em>DMKD</em>, <em>39</em>(2), 1–55. (<a
href="https://doi.org/10.1007/s10618-024-01086-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform an extensive experimental evaluation of clustering-based outlier detection methods. These methods offer benefits such as efficiency, the possibility to capitalize on more mature evaluation measures, more developed subspace analysis for high-dimensional data and better explainability, and yet they have so-far been neglected in literature. To our knowledge, our work is the first effort to analytically and empirically study their advantages and disadvantages. Our main goal is to evaluate whether or not clustering-based techniques can compete in efficiency and effectiveness against the most studied state-of-the-art algorithms in the literature. We consider the quality of the results, the resilience against different types of data and variations in parameter configuration, the scalability, and the ability to filter out inappropriate parameter values automatically based on internal measures of clustering quality. It has been recently shown that several classic, simple, unsupervised methods surpass many deep learning approaches and, hence, remain at the state-of-the-art of outlier detection. We therefore study 14 of the best classic unsupervised methods, in particular 11 clustering-based methods and 3 non-clustering-based ones, using a consistent parameterization heuristic to identify the pros and cons of each approach. We consider 46 real and synthetic datasets with up to 125k points and 1.5k dimensions aiming to achieve plausibility with the broadest possible diversity of real-world use cases. Our results indicate that the clustering-based methods are on par with (if not surpass) the non-clustering-based ones, and we argue that clustering-based methods like KMeans−− should be included as baselines in future benchmarking studies, as they often offer a competitive quality at a relatively low run time, besides several other benefits.},
  archive      = {J_DMKD},
  author       = {Sánchez Vinces, Braulio V. and Schubert, Erich and Zimek, Arthur and Cordeiro, Robson L. F.},
  doi          = {10.1007/s10618-024-01086-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-55},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comparative evaluation of clustering-based outlier detection},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-level relation extraction for informative taxonomy
learning. <em>DMKD</em>, <em>39</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10618-024-01088-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the ever-shifting domain of technological advancement, the quest for automatic taxonomy construction is intensifying. This paper confronts the nuanced challenges of distilling synonym and hyponym relationships from diverse, domain-specific scientific literature, treating it as a domain-level relation extraction problem and resulting in the creation of a hierarchical taxonomy through arborescence generation. The proposed Multi-Scale Identity Connection (MSIC) model excels in capturing inter-entity relationships across various scales, demonstrating superior empirical performance compared to existing relation extraction models. To enhance practicality, a two-stage optimization is introduced to improve efficiency without compromising performance. Additionally, the Depth-prioritized Maximum Spanning Arborescence (DMSA) algorithm has been proposed as a highly efficient strategy for generating an informative and well-structured taxonomy tree. We annotated a concise dataset to train and validate the MSIC model, subsequently applying it to a substantial domain-specific dataset for taxonomy induction. The experimental findings indicate that the DMSA efficiently constructs an information-rich taxonomy tree structure by leveraging extensive domain-specific scientific literature. These results not only affirm the efficacy of the approach but also highlight its effectiveness in supporting industrial-grade applications.},
  archive      = {J_DMKD},
  author       = {Hu, Maodi and Song, Donghuan and Qian, Li and Zhang, Zhixiong},
  doi          = {10.1007/s10618-024-01088-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Domain-level relation extraction for informative taxonomy learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximity forest 2.0: A new effective and scalable
similarity-based classifier for time series. <em>DMKD</em>,
<em>39</em>(2), 1–29. (<a
href="https://doi.org/10.1007/s10618-024-01085-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a challenging task due to the diversity of types of features that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms other state-of-the-art methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series similarity measures — (1) computationally efficient early abandoning and pruning to speedup elastic similarity computations; (2) a new elastic similarity measure, Amerced Dynamic Time Warping ( $${{\,\textrm{ADTW}\,}}$$ ); and (3) cost function tuning. It rationalizes the set of similarity measures employed, reducing the eight base measures of the original PF to four and using the first derivative transform with all similarity measures, rather than a limited subset. It also incorporates HYDRA, a dictionary-based transform. We have re-implemented PF 1.0 and implemented PF 2.0 framework in Java, making the PF framework more efficient.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Herrmann, Matthieu and Salehi, Mahsa and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-024-01085-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Proximity forest 2.0: A new effective and scalable similarity-based classifier for time series},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TED: Related party transaction guided tax evasion detection
on heterogeneous graph. <em>DMKD</em>, <em>39</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s10618-025-01091-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tax evasion causes severe losses of government revenues and disturbs the economic order of fair competition. To help alleviate this problem, the latest tax evasion detection solutions utilize expert knowledge to extract features and then train classifiers to determine whether a company is suspected of tax evasion. However, existing solutions mainly focus on the statistical features of the company, but fail to exploit the rich interactive information in tax scenarios, which affect the detection performance. In this paper, we first model the tax scenario as a heterogeneous graph and study the tax evasion detection problem under the heterogeneous graph model. To improve the performance of tax evasion detection, a novel graph neural network model is proposed to extract the comprehensive information of heterogeneous graphs. Specifically, we use heterogeneous and complex related party transaction groups to filter low-level noise information. Moreover, a hierarchical attention mechanism is designed to capture the deeper structure and semantic information hidden in the related party transaction group. We apply our method to the real risk management system of the tax bureau, and evaluate it on two human-labeled real-world tax datasets. The results demonstrate that our method significantly outperforms the state-of-the-art in the tax evasion detection task. The code and data are available at: https://github.com/yimingxu24/TED .},
  archive      = {J_DMKD},
  author       = {Xu, Yiming and Shi, Bin and Dong, Bo and Wang, Jiaxiang and Wei, Hua and Zheng, Qinghua},
  doi          = {10.1007/s10618-025-01091-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TED: Related party transaction guided tax evasion detection on heterogeneous graph},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring tie strength in temporal networks. <em>DMKD</em>,
<em>39</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10618-025-01093-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring tie strengths in social networks is an essential task in social network analysis. Common approaches classify the ties as weak and strong ties based on the strong triadic closure (STC). The STC states that if for three nodes, A, B, and C, there are strong ties between A and B, as well as A and C, there has to be a (weak or strong) tie between B and C. A variant of the STC called STC+ allows adding a few new weak edges to obtain improved solutions. So far, most works discuss the STC or STC+ in static networks. However, modern large-scale social networks are usually highly dynamic, providing user contacts and communications as streams of edge updates. Temporal networks capture these dynamics. To apply the STC to temporal networks, we first generalize the STC and introduce a weighted version such that empirical a priori knowledge given in the form of edge weights is respected by the STC. Similarly, we introduce a generalized weighted version of the STC+. The weighted STC is hard to compute, and our main contribution is an efficient 2-approximation (resp. 3-approximation) streaming algorithm for the weighted STC (resp. STC+) in temporal networks. As a technical contribution, we introduce a fully dynamic k-approximation for the minimum weighted vertex cover problem in hypergraphs with edges of size k, which is a crucial component of our streaming algorithms. An empirical evaluation shows that the weighted STC leads to solutions that better capture the a priori knowledge given by the edge weights than the non-weighted STC. Moreover, we show that our streaming algorithm efficiently approximates the weighted STC in real-world large-scale social networks.},
  archive      = {J_DMKD},
  author       = {Oettershagen, Lutz and Konstantinidis, Athanasios L. and Italiano, Giuseppe F.},
  doi          = {10.1007/s10618-025-01093-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Inferring tie strength in temporal networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
