<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---12">JRTIP - 12</h2>
<ul>
<li><details>
<summary>
(2025). CFP-PSPNet: A lightweight unmanned vessel water segmentation
algorithm. <em>JRTIP</em>, <em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01603-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate water segmentation is the first prerequisite for unmanned vessels to navigate safely and perform other operations. Aiming at the problems of low utilization rate of unmanned boat water image features and low accuracy of contour edge segmentation in complex inland river scenarios, this paper proposes a lightweight water segmentation algorithm: channel feature pyramid-pyramid scene parsing network (CFP-PSPNet), which realizes efficient and accurate segmentation of water area in complex scenarios. First, a cross transformation-channel feature pyramid (CT-CFP) is proposed, which improves the utilization of the original feature information by cross-fertilizing the feature information between different layers and realizes the improvement of the water segmentation accuracy; Secondly, a parallel semantic segmentation network CFP-PSPNet is designed, which extracts the image information by pyramid pooling module (PPM) and CT-CFP dual pyramid, which solves the problem of loss of detail information and edge information, so as to achieve the purpose of improving the accuracy; finally, Mobilenetv2 after the introduction of encoder-context-attention (ECA) is used as a feature extraction network, which reduces the number of parameters and computation of the network without affecting the segmentation accuracy and realizes the lightweight design of the network. Experiments are conducted on the open-source dataset USVInland, and the experimental results show that our CFP-PSPNet algorithm has a significant reduction in the number of parameters, an increase in detection speed by 81FPS, and mean intersection over union (MIoU) and accuracy rates of 97.71% and 98.75%, respectively, which are 1.41% and 0.74% higher than that of the original network. It is superior to other classical semantic segmentation algorithms.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Song, Yijing and He, Lintao and Xue, Hang and Dong, Zhonghua and Zhang, Qingyun},
  doi          = {10.1007/s11554-024-01603-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CFP-PSPNet: A lightweight unmanned vessel water segmentation algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECDet: Efficient oriented object detection on the aerial
image with cross-layer attention. <em>JRTIP</em>, <em>22</em>(1), 1–13.
(<a href="https://doi.org/10.1007/s11554-024-01617-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in horizontal object detection models, oriented object detection models have also made significant strides. Yet existing rotated object detectors often struggle to maintain high accuracy while processing high-resolution remote sensing images in real-time. To address these challenges, we propose a new lightweight model specifically for oriented object detection, named the efficient cross-layer attention detector (ECDet). ECDet integrates several efficient modules, including an efficient reparameterized Transformer-like backbone (ERepViT) to reduce computational costs, and the efficient cross-layer fusion neck (CLF-Neck), a lightweight alternative to traditional pyramid networks for feature fusion with attention mechanism. Additionally, we introduce the lightweight task interaction decoupled (LTID) head, which enhances task-specific performance by providing more detailed, task-aligned information for classification and regression with minimal computational cost. Furthermore, an ensemble loss combined with the phase shifting coder (PSC) mitigates the angle discontinuity issue in regression-based methods. Evaluations on the DOTAv1 and HRSC datasets show that ECDet runs 32% faster than RTMDet-S with higher accuracy, demonstrating its strong potential for practical application. The source code will be release at https://github.com/tianlianghai/ECDet .},
  archive      = {J_JRTIP},
  author       = {Lyu, Xueqiang and Tian, Lianghai and Teng, Shangzhi},
  doi          = {10.1007/s11554-024-01617-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ECDet: Efficient oriented object detection on the aerial image with cross-layer attention},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-level attention based data compression for spike
camera. <em>JRTIP</em>, <em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01618-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of autonomous driving applications, there is an increasing demand for high-speed vision sensors like spike cameras. However, the data transmission and storage requirements are becoming increasingly burdensome due to the high temporal resolution, such as 40,000 Hz. To resolve this problem, we propose a pixel-level attention-based data compression method for the spike camera. First the input spike data are partitioned into two block types by pixel-level attention-based method. Then, the two blocks are condensed using different methods and side information marking is transmitted for spike decoding. Finally, the condensed spike and side information marking are compressed into a binary stream for storage. The experimental results show that our method achieves higher compression efficiency than conventional methods. The decompressed spike can also reconstruct the image with better visual quality.},
  archive      = {J_JRTIP},
  author       = {Li, Yansong and Huang, Xiaofeng and Li, Shangqia and Cui, Yan and Zhou, Yang and Song, Jian and Yin, Haibing},
  doi          = {10.1007/s11554-024-01618-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Pixel-level attention based data compression for spike camera},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rocknet: Lightweight network for real-time segmentation of
martian rocks. <em>JRTIP</em>, <em>22</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01619-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rock segmentation on the Martian is particularly critical for rover navigation, obstacle avoidance, and scientific target detection. We propose a lightweight network for real-time semantic segmentation of Martian rocks (RockNet). First, we propose the cross-dimension channel attention (CDCA) model to replace traditional downsample and upsample operation, which gives more weight to the channels with more useful information by adjusting the weight of each channel. Second, we modify the short-term dense concatenate model, we adopt dilated convolution to learn the feature with a larger receptive field, and through the skip connection structure, the degradation of the network can be reduced. Finally, we propose a feature fusion module (FFM) to fully fuse different levels of features. With only 0.86M parameters, our model gets 82.37% mIoU and 105.7 FPS running speed on the dataset of TWMARS.},
  archive      = {J_JRTIP},
  author       = {Wei, Pengfei and Sun, Zezhou and Tian, He},
  doi          = {10.1007/s11554-024-01619-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rocknet: Lightweight network for real-time segmentation of martian rocks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time underwater target detection based on improved
YOLOv7. <em>JRTIP</em>, <em>22</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-025-01621-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target detection is crucial for ocean exploration, but existing methods struggle to achieve satisfactory results due to the complexity of the underwater environment. To enhance the accuracy and real-time performance of underwater detection models, we propose an improved YOLOv7 model. We introduce a multi-granularity feature attention method based on the Efficient Channel Attention (ECA) to help the model better adapt to the diverse conditions in the underwater environment, reducing focus on redundant information. Utilizing coordinate convolution provides the network with spatial awareness of input image coordinates, enabling more effective localization of target objects and reducing interference from similar background elements. To accommodate the features of small and dense underwater targets, we use normalized Wasserstein distance to measure the similarity of bounding boxes. On the Underwater Robot Picking Contest 2019 (URPC 2019) dataset, the mean Average Precision (mAP) of our improved network has reached 86.19%, which represents a 1.57% increase compared to the original YOLOv7 network. Additionally, the frames per second (fps) has achieved 124, surpassing the performance of the original network. This improvement is significantly superior to conventional target detection models, providing a faster and more accurate advantage for underwater target detection tasks in complex underwater environments.},
  archive      = {J_JRTIP},
  author       = {Wu, Qingqi and Cen, Lihui and Kan, Shichao and Zhai, Yongping and Chen, Xiaofang and Zhang, Hong},
  doi          = {10.1007/s11554-025-01621-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time underwater target detection based on improved YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LKR-DETR: Small object detection in remote sensing images
based on multi-large kernel convolution. <em>JRTIP</em>, <em>22</em>(1),
1–14. (<a href="https://doi.org/10.1007/s11554-025-01622-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in remote sensing imagery remains a challenging problem in computer vision. To address the inherent limitations of aerial imagery, such as densely packed objects with insufficient detail and occlusions caused by complex backgrounds, this study proposes LKR-DETR, an innovative object detection in remote sensing imagery framework based on RT-DETR. We propose a lightweight and efficient feature extraction module with large kernel convolution, which expands the receptive field while reducing parameters and computational costs. Furthermore, we present a novel multi-scale feature fusion structure based on wavelet transform convolution that effectively utilizes low-frequency information from low-level feature maps. Additionally, we introduce a lightweight image restoration module utilizing large kernel convolutions, which effectively recovers previously undetected details of small objects. To improve bounding box regression accuracy, the original GIoU loss is replaced with a Focaler-DIoU loss function. Compared to the benchmark model RT-DETR, the LKR-DETR model achieves a 2.5% improvement in $$mAP_{0.5}$$ and a 2.0% improvement in $$mAP_{0.5:0.95}$$ on the VisDrone2019-DET dataset, a 1.7% and 4.4% improvement on the DOTAv1.5 dataset, and a 3.4% and 2.4% improvement on the HIT-UAV dataset, while also reducing the parameter count and model size. Relative to other cutting-edge models, LKR-DETR attains superior detection accuracy while maintaining relatively low computational complexity, establishing it as an efficient solution for small object detection in remote sensing imagery.},
  archive      = {J_JRTIP},
  author       = {Dong, Ying and Xu, Fucheng and Guo, Jiahao},
  doi          = {10.1007/s11554-025-01622-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LKR-DETR: Small object detection in remote sensing images based on multi-large kernel convolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DHNet: A surface defect detection model utilizing
multi-scale convolutional kernels. <em>JRTIP</em>, <em>22</em>(1), 1–15.
(<a href="https://doi.org/10.1007/s11554-025-01623-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting micro-defects in densely populated printed circuit boards (PCBs) with complex backgrounds is a critical challenge. To address the problem, the DHNet, a small object detection network based on YOLOv8 employing multi-scale convolutional kernels is proposed for feature extraction and fusion. The lightweight VOVGSHet module is designed for feature fusion and a pyramid structure to efficiently leverage feature map relationships while minimizing model complexity and parameters. Otherwise, to optimize the original extraction structure and enhance multi-scale defect detection, convolutional kernels of varying sizes process the same input channels. Additionally, the incorporation of the Wise-IoU loss function improves small defect detection accuracy and efficiency. Moreover, extensive experiments on a custom PCB dataset demonstrate DHNet&#39;s effectiveness, achieving an outstanding mean Average Precision (mAP) of 84.5%, surpassing the original YOLOv8 network by 4.0%, with parameters only of 2.85 M. Model demonstrates a latency of 3.6 ms on NVIDIA 4090. However, YOLOv8n has a latency of 4.4 ms. Validation on public DeepPCB and NEU datasets further confirms DHNet&#39;s superiority, which can reach 99.1% and 79.9% mAP, respectively. Finally, successful deployment on the NVIDIA Jetson Nano platform validates DHNet&#39;s suitability for real-time defect detection in industrial applications.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yingying and Wang, Shuo and Wang, Jinhai and Zhao, Yu and Chen, Zhiwei},
  doi          = {10.1007/s11554-025-01623-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DHNet: A surface defect detection model utilizing multi-scale convolutional kernels},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSBF-YOLO: A lightweight model for tomato ripeness detection
in natural environments. <em>JRTIP</em>, <em>22</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01624-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate tomato ripeness detection is essential for optimizing harvest timing and maximizing yield. Deep learning-based object detection has proven effective in this task. However, many existing algorithms have numerous parameters and substantial computational demands, making them unsuitable for agricultural environments with limited computational resources. Additionally, accurate detection becomes challenging with overlapping fruits, leaf occlusion, or complex backgrounds. To address these issues, this paper proposes a lightweight detection model, GSBF-YOLO. This model designs the GSim module to reduce parameters while maintaining detection accuracy. The C3Ghost module further reduces parameter count by replacing the traditional C3 module. The PANet multi-scale feature fusion network in the neck is replaced with the Bi-directional Feature Pyramid Network (BiFPN), which adjusts weights based on the importance of input features. Lastly, the fine-tuned FocalEIOU Loss function is used to calculate the bounding box regression loss, enhancing the model’s ability to adjust the weights of high-quality anchor boxes for better detection of targets in occlusion scenarios. Experimental results show that GSBF-YOLO reduces parameters and computational load by 42% and 45%, respectively, while mean Average Precision (mAP) increases by 1.9% and 1.6% on two datasets. The model achieves 110 Frames Per Second (FPS), meeting real-time detection requirements, and has fewer parameters and higher accuracy compared to models like YOLOv8. The research indicates that the proposed lightweight model can effectively detect tomato ripeness in natural environments.},
  archive      = {J_JRTIP},
  author       = {Hao, Fengqi and Zhang, Zuyao and Ma, Dexin and Kong, Hoiio},
  doi          = {10.1007/s11554-025-01624-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GSBF-YOLO: A lightweight model for tomato ripeness detection in natural environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time and general method for converting offline
skeleton-based action recognition to online ones. <em>JRTIP</em>,
<em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01625-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many motion sensors can directly acquire human skeletal data, and then extract features on the skeletal data through GCNs (graph convolutional networks) to perform action recognition. However, almost all state-of-the-art (SOTA) methods are offline methods, cannot perform online inference, wasting computational resources. The existing approach to transforming offline action recognition into online action recognition is to reconstruct the network structure of the offline method. This requires developers to have a deep understanding of the algorithm’s network structure and make extensive modifications, which results in slow development. To address the above issue, this paper points out that to convert offline methods to online ones, the key is removing outdated frame features and fusing new frame features. Furthermore, we propose a general and simple model called Encode One Frame (EOF), which achieves feature removal and fusion by a correlation matrix and the guidance of a teacher model. The EOF model has online inference capabilities, requiring only the input of the new frame of the current sample and the features encoded from the old sample. Based on the EOF model, we further propose the You Only Encode One Frame (YOEOF) algorithm to correct the cumulative errors generated during EOF model online inference. By coupling these proposals, YOEOF achieves online inference and outperforms some SOTA methods on public datasets. The deployment at the application level indicates that our method meets the requirements of high accuracy and real-time performance for dangerous action recognition.},
  archive      = {J_JRTIP},
  author       = {Dong, Liheng and He, Guiqing and Zhang, Zhaoxiang and Xu, Yuelei and Hui, Tian and Xu, Xin and Tao, Chengyang and Li, Huafeng},
  doi          = {10.1007/s11554-025-01625-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and general method for converting offline skeleton-based action recognition to online ones},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid architecture for video frame prediction:
Combining convolutional LSTM and 3D CNN. <em>JRTIP</em>, <em>22</em>(1),
1–18. (<a href="https://doi.org/10.1007/s11554-025-01626-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame prediction represents a fundamental challenge in computer vision, necessitating precise modeling of both spatial and temporal dynamics within video sequences. This computational task holds substantial implications across diverse domains, including video compression optimization, robust object tracking systems, and advanced motion forecasting applications. In this investigation, we present a novel hybrid architecture that synthesizes the complementary strengths of Convolutional Long Short-Term Memory (ConvLSTM) networks and three-dimensional Convolutional Neural Networks (3D CNN) for enhanced frame prediction capabilities. Our methodological framework incorporates a ConvLSTM component that fundamentally augments the traditional LSTM architecture through the integration of convolutional operations, thereby facilitating sophisticated modeling of sequential dependencies. Concurrently, the 3D CNN component employs volumetric convolutional layers to extract rich spatio-temporal features from the input sequences. Rigorous empirical evaluation demonstrates the superior performance of the ConvLSTM architecture, which consistently yields reduced validation errors and elevated coefficients of determination. Specifically, the ConvLSTM model achieves a validation Mean Squared Error (MSE) of 0.0237 and an $${\textrm{R}}^{2}$$ value of 0.6951, substantially outperforming the 3D CNN model, which exhibits a validation MSE of 0.0471 and an $${\textrm{R}}^{2}$$ value of 0.3939. These empirical findings substantiate the efficacy of the ConvLSTM architecture in addressing the inherent complexities of video frame prediction, while simultaneously illuminating its considerable potential for deployment across various video processing and predictive modeling applications. The results provide compelling evidence for the advantages of incorporating convolutional operations within recurrent architectures for sequential visual data processing.},
  archive      = {J_JRTIP},
  author       = {Aravinda, C. V. and Al-Shehari, Taher and Alsadhan, Nasser A. and Shetty, Shashank and Padmajadevi, G. and Reddy, K. R. Udaya Kumar},
  doi          = {10.1007/s11554-025-01626-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel hybrid architecture for video frame prediction: Combining convolutional LSTM and 3D CNN},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMAL-YOLOv8: Real-time detection algorithm for insulator
defects based on optimized feature fusion. <em>JRTIP</em>,
<em>22</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11554-025-01629-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of misdetection and missed detection caused by the diversity of insulator types and the complexity of defect textures in power transmission lines, and to meet the demands of collaborative inspection, we propose a real-time detection algorithm for insulator defects based on YOLOv8. First, considering the characteristics of the dataset sample sizes, we designed a lightweight OMAL-Neck structure that optimizes feature fusion, enhancing the utilization of feature information and improving detection performance for medium and large targets. Second, to address the issue of large parameter and computation requirements in the YOLOv8 detection head, we designed a lightweight and efficient detection head. This redesigned detection head incorporates PConv, further accelerating model inference speed. Lastly, to counteract the decline in detection accuracy due to model lightweighting, we integrated the C2f module with DySnakeConv, enhancing the feature extraction capability for tubular structures and complex textures, thereby preventing information loss. Experimental results demonstrate that compared to the baseline YOLOv8s, the proposed model increases FPS from 44 to 78 frames/s, reduces the number of parameters and computational complexity by 27 and 38%, respectively, and improves the mAP by 1.7%. The improved model offers significant advantages in both detection accuracy and real-time performance, enabling rapid and precise identification of insulators and their defects, thereby improving the efficiency of power line inspections.},
  archive      = {J_JRTIP},
  author       = {Ru, Hongfang and Zhang, Wenhao and Wang, Guoxin and Ding, Luyang},
  doi          = {10.1007/s11554-025-01629-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {OMAL-YOLOv8: Real-time detection algorithm for insulator defects based on optimized feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition method for PCB chip targets based on
YOLO-GSG. <em>JRTIP</em>, <em>22</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01616-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial settings, the identification of chips on PCB boards is crucial for quality control and efficiency. However, achieving both speed and accuracy in chip detection remains a significant challenge. To address this issue, we propose the YOLO-GSG deep network model, which incorporates several novel modifications to the standard YOLO architecture. The key innovations include the replacement of the ELAN module with the C3Ghostnet module in the backbone network, improving feature extraction and reducing model complexity, and the introduction of the SE attention mechanism to minimize feature loss. Additionally, the GSnet module and GSConv convolution are integrated into the neck network to enhance feature fusion. The experimental results indicate that the YOLO-GSG algorithm achieves a mAP of 99.014%, with precision and recall improvements of 1.080% and 1.446% over the baseline YOLOv7 model. Additionally, the improved model has 24.478M parameters, 61.4 GFLOPs, and a model size of 50.8 MB. The model achieves an FPS of 231.55, representing a 12.8% speedup over the baseline. These results indicate that the YOLO-GSG model offers a superior balance of speed and accuracy for chip identification in industrial applications. This study contributes to the advancement of deep learning applications in industrial environments, providing a more efficient and effective tool for quality control in PCB manufacturing.},
  archive      = {J_JRTIP},
  author       = {Yue, Zeang and Li, Xun and Zhou, Huilong and Wang, Gaopin and Wang, Wenjie},
  doi          = {10.1007/s11554-024-01616-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition method for PCB chip targets based on YOLO-GSG},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
