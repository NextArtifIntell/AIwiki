<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SOCO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="soco---35">SOCO - 35</h2>
<ul>
<li><details>
<summary>
(2025). Improving recurrent deterministic policy gradient strategy
in autonomous driving. <em>SOCO</em>, <em>29</em>(3), 1931–1946. (<a
href="https://doi.org/10.1007/s00500-025-10442-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though autonomous driving has emerged as a prominent study topic, the conventional control systems for autonomous driving are often rule-based and need to be more adaptable to the flow and conditions of traffic that change over time. Recurrent deterministic policy gradient (RDPG) is a strategy for building autonomous driving control systems. Its performance has been shown to be better than some other methods. Consequently, in this study, we make use of the RDPG algorithm to implement our control strategies as well and further give more comprehensive considerations to the learning procedure to obtain better control performance in the testing procedure, e.g., various punishments to avoid vehicle collisions, different speed limitations to avoid slow-driving or fast-driving, distinct rewards to encourage the ego-vehicle to reach the destination, and so on. On the other hand, we also improve the training performance by focusing solely on the critical events during the training procedure. Namely, our training architecture is more efficient based on the same training time (training steps). The road scene and vehicular simulator, AirSim, has been selected as the experimental platform. The findings indicate that our design achieves more accurate and steady outcomes in control and faster convergence in learning compared to an existing RDPG control strategy for autonomous driving in the literature.},
  archive      = {J_SOCO},
  author       = {Ooi, Yee-Ming and Chang, Che-Cheng},
  doi          = {10.1007/s00500-025-10442-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1931-1946},
  shortjournal = {Soft Comput.},
  title        = {Improving recurrent deterministic policy gradient strategy in autonomous driving},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of discharge coefficient of submerged gates using
a stacking ensemble model. <em>SOCO</em>, <em>29</em>(3), 1911–1929. (<a
href="https://doi.org/10.1007/s00500-025-10518-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the precision of discharge coefficient (Cd) prediction is crucial for effective agricultural water management. However, existing methods for Cd calculation are often complex and dependent on specific assumptions. Therefore, there is a critical need for robust and automated models for Cd estimation. This study introduces a dual-stage ensemble model called EnsembleCNN, for Cd prediction using two distinct gate types under submerged flow conditions. The EnsembleCNN framework uniquely integrates machine learning (ML) models with a recurrent convolutional neural network (CNN) model to capture higher-order interactions and non-linearities. Five base ML models are employed to generate initial predictions. These predictions are subsequently processed by a CNN model embedded with long short-term memory (LSTM) layer, residual connection (RC) and an attention mechanism (ATM). This setup effectively manages the complexity of the combined predictions, seamlessly integrating the outputs from the base models. LSTM is exploited to aggregate the best features for prediction. ATM effectively prioritized high-performing base model outputs, while RC improved the gradient flow, collectively reducing the impact of irrelevant features. The proposed approach strategically weights the contributions of each base model, resulting in accurate Cd estimations. The proposed model achieved root mean square errors of 0.0552 and 0.0173 on vertical sluice gates and radial gates datasets, respectively. Additionally, EnsembleCNN outperformed the base and existing models in terms of prediction accuracy. The proposed system provides a robust tool for optimizing water resource management. Moreover, the adaptability to two field datasets further underscores the practical utility of our model in diverse irrigation scenarios.},
  archive      = {J_SOCO},
  author       = {Hosny, Mohamed and Abdelhaleem, Fahmy S. and Elshenhab, Ahmed M. and Ibrahim, Amir},
  doi          = {10.1007/s00500-025-10518-x},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1911-1929},
  shortjournal = {Soft Comput.},
  title        = {Prediction of discharge coefficient of submerged gates using a stacking ensemble model},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-specific multi-label text classification based on
dynamic graph convolutional networks. <em>SOCO</em>, <em>29</em>(3),
1897–1909. (<a
href="https://doi.org/10.1007/s00500-025-10446-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification is a key task in natural language processing, aiming to assign each text to multiple predefined categories simultaneously. Existing neural network models usually learn the same text representation for different labels, which limits the effectiveness of the models in capturing deep semantics and distinguishing between similar labels; moreover, these models tend to ignore inter-label correlation, leading to loss of information. To overcome these limitations, we propose a novel label-specific dynamic graph convolutional network (LDGCN). This network combines convolutional operations and BiLSTM to model text sequences and obtains label-specific text representations through a label attention mechanism. In addition, LDGCN improves the dynamic graph convolutional network by utilizing statistical label co-occurrence and label reconstruction maps to effectively capture inter-label dependencies and adaptive interactions between label-specific semantic components. Extensive experiments on the RCV1, AAPD, and EUR-Lex datasets show that our model achieves 96.92%, 86.30%, and 81.42% on the P@1 metrics, respectively, and demonstrates a significant advantage in dealing with tail labels.},
  archive      = {J_SOCO},
  author       = {Yan, Yaoyao and Liu, Fang‘ai and Liu, Kenan and Xu, Weizhi and Zhuang, Xuqiang},
  doi          = {10.1007/s00500-025-10446-w},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1897-1909},
  shortjournal = {Soft Comput.},
  title        = {Label-specific multi-label text classification based on dynamic graph convolutional networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Threats to medical diagnosis systems: Analyzing targeted
adversarial attacks in deep learning-based COVID-19 diagnosis.
<em>SOCO</em>, <em>29</em>(3), 1879–1896. (<a
href="https://doi.org/10.1007/s00500-025-10516-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep and machine learning models have become pivotal in medical image analysis, especially for diagnosing COVID-19 using X-rays and CT scans. While these models, including transfer learning-based approaches, have achieved high accuracy, they remain highly vulnerable to adversarial attacks, which can manipulate input data and cause misclassification, posing critical risks in clinical applications. This study introduces a novel approach to addressing this issue by systematically evaluating the impact of adversarial attacks on COVID-19 diagnosis models built with two leading architectures, VGG-16 and DenseNet-121, using the Fast Gradient Sign Method (FGSM). The FGSM attack causes a dramatic drop in accuracy, reducing VGG-16’s accuracy from 95.12 to 9.97% and DenseNet-121’s from 96.51 to 10.13%. To counter these vulnerabilities, we propose a novel defense mechanism that combines adversarial training with Gaussian noise data augmentation, a dynamic approach that generates perturbations across various epsilon values during the training phase. This innovative method significantly enhances model robustness, restoring accuracy to over 92% on adversarial examples. These findings emphasize the need for strong defense mechanisms in deep learning models for COVID-19 diagnosis, ensuring reliability and security against adversarial threats in clinical environments.},
  archive      = {J_SOCO},
  author       = {Haque, Sheikh Burhan Ul and Zafar, Aasim and Haq, Sheikh Riyaz Ul and Haque, Sheikh Moeen Ul and Ahmad, Mohassin and Roshan, Khushnaseeb},
  doi          = {10.1007/s00500-025-10516-z},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1879-1896},
  shortjournal = {Soft Comput.},
  title        = {Threats to medical diagnosis systems: Analyzing targeted adversarial attacks in deep learning-based COVID-19 diagnosis},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive multimodal approach for parkinson’s disease
classification using artificial intelligence: Insights and model
explainability. <em>SOCO</em>, <em>29</em>(3), 1845–1877. (<a
href="https://doi.org/10.1007/s00500-025-10463-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is a debilitating neurodegenerative disorder affecting millions worldwide. Early detection is vital for effective management, yet remains challenging. In this study, we investigated four distinct datasets for PD detection. Through comprehensive experimentation employing ensemble methods and feature selection, we achieved high classification accuracies across the datasets. For the Oxford Parkinson’s Disease Detection Dataset, an accuracy of 95.67%, precision of 97.59%, recall of 84.5%, specificity of 99.32%, and F1-score of 90.57% were achieved. For the Alzheimer Parkinson Diseases 3 Class Dataset, the “Stacking” approach surpasses individual models, reaching an accuracy of 99.85%, precision of 99.81%, recall of 99.81%, specificity of 99.86%, and F1 of 99.81%. For the NewHandPD dataset, Regarding the Spiral category, The “Base-P32-384” model surpasses others with an accuracy of 97.35%, precision of 96.50%, recall of 98.57%, and F1-score of 97.53%. The collective “Stacking” approach proves highly effective regarding the Circle category, achieving 100% across all performance metrics. Regarding the Meander category, the “Base-P16-224” model achieves an accuracy of 97.35%, precision of 99.26%, recall of 95.71%, specificity of 99.19%, and F1 of 97.45%. The Mobile Device Voice Recordings at King’s College London (MDVR-KCL) dataset contains two datasets. Regarding the “SpontaneousDialogue” dataset, accuracy, BAC, precision, recall, specificity, and F1-score were computed, resulting in values of 94.03%, 92.83%, 90.78%, 100.0%, and 85.67%, respectively. Regarding the “ReadText” dataset, accuracy, BAC, precision, recall, specificity, and F1-score were computed, resulting in values of 91.89%, 90.62%, 87.5%, 100.0%, and 81.25%, respectively. Our findings highlight the efficacy of leveraging diverse data sources and advanced machine learning techniques to enhance PD detection accuracy.},
  archive      = {J_SOCO},
  author       = {Balaha, Hossam Magdy and Hassan, Asmaa El-Sayed and Ahmed, Rawan Ayman and Balaha, Magdy Hassan},
  doi          = {10.1007/s00500-025-10463-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1845-1877},
  shortjournal = {Soft Comput.},
  title        = {Comprehensive multimodal approach for parkinson’s disease classification using artificial intelligence: Insights and model explainability},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing neural network predictions with finetuned numeric
embeddings for stock trend forecasting. <em>SOCO</em>, <em>29</em>(3),
1829–1844. (<a
href="https://doi.org/10.1007/s00500-025-10483-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The financial markets, particularly stock trading, offer a variety of profit-generating opportunities based on complex and volatile behaviour. Investors seek strategies to maximise returns, leading to an investigation of inherent market patterns. Converting OHLC (Open, High, Low, Close) data into transformers-based pre-trained language model compatible text is an innovative method for representing numeric data. Extending the language model’s utility to integrate stock market numeric time-series data incorporates its inherent numeracy in embeddings. Raw data are converted into a format compatible with the pre-trained language model through preprocessing and text templates. Using an ensemble of Bidirectional Encoder Representations from Transformers (BERT), FinBERT (BERT finetuned with the financial corpus), FLANG-BERT (BERT finetuned with the financial corpus) and FLANG-ELECTRA (ELECTRA finetuned with the financial corpus) as feature extractor, historical stock market data are utilised to generate an embedding matrix and fused with established neural network architectures, such as Backpropagation Neural Network (BPNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), to predict stock market trends. The simulation results demonstrate that the proposed integrated approach is preferable to previous methodologies. The significance of the findings is confirmed by statistical validation using the Wilcoxon signed-rank test (p value &lt; 0.01). This study offers a promising approach for improving stock market trend prediction by integrating the ensemble of language model-based numeric embeddings with neural networks.},
  archive      = {J_SOCO},
  author       = {Trivedi, Avinash and Sangeetha, S.},
  doi          = {10.1007/s00500-025-10483-5},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1829-1844},
  shortjournal = {Soft Comput.},
  title        = {Enhancing neural network predictions with finetuned numeric embeddings for stock trend forecasting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-theoretic exploration with surplus profit-sharing in
a three-channel supply chain, featuring e-commerce dynamics.
<em>SOCO</em>, <em>29</em>(3), 1811–1827. (<a
href="https://doi.org/10.1007/s00500-025-10453-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a three-channel supply chain, coordination can be challenging especially when a manufacturer has to work with a retailer and an online platform. In such a scenario, sales efforts can be critical to the success of the supply chain. However, there is a risk of free riding behavior by either the retailer or the manufacturer, which can lead to suboptimal sales performance. This article will explore the centralized and the decentralized models by the use of game theory (Nash and Stackelberg) and eventually tries to coordinate the three-channel supply chain with the help of Operational Research (OR) to optimize the decision-making and create a win–win situation. Numerical examples are provided to prove the efficiency of the presented models. Finally, the models are evaluated through sensitivity analysis, and managerial insights are provided to enhance the applicability of the models for coordinating a three-channel supply chain.},
  archive      = {J_SOCO},
  author       = {Vatanara, Maryam and Rabbani, Masoud and Heydari, Jafar},
  doi          = {10.1007/s00500-025-10453-x},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1811-1827},
  shortjournal = {Soft Comput.},
  title        = {A game-theoretic exploration with surplus profit-sharing in a three-channel supply chain, featuring e-commerce dynamics},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forest fire rescue framework to jointly optimize
firefighting force configuration and facility layout: A case study of
digital-twin simulation optimization. <em>SOCO</em>, <em>29</em>(3),
1789–1810. (<a
href="https://doi.org/10.1007/s00500-025-10434-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pre-prevention stage, firefighting force configuration and facility layout play a critical role in reducing fire extinguishing time (FET) during the early-stage forest fire rescue. It is acknowledged that there is a scarcity of quantitative evaluation research establishing a connection between observed forest fire behaviors and pre-prevention research. Therefore, we propose a forest fire rescue framework to jointly optimize firefighting force configuration and facility layout. As an iterative optimization framework based on fire spread and suppression model (FSSM), firefighting force configuration and facility layout methods use differential-evolution-based algorithm and deep neural network to adjust the configuration funds of various firefighting forces and plan the spatial layout of multiple firefighting facilities. With iterations increasing, the proposed method can continue to find better solutions than before. Moreover, through the offensive and defensive procedures in FSSM, the best configuration and layout solution can mirror multi-rescue-resource interactions and mutual restraints. The performance of the proposed framework is validated through various maps and experiments in terms of FET, forest burned area, and uncontrolled fire rate, even under extreme wind-speed pressure conditions. This implies that the proposed framework demonstrates favorable adaptability. Furthermore, the proposed framework can be introduced into the related dynamic interactions and constraints optimization scenarios (e.g., smart factories, smart construction sites, and more), thereby opening the door of digital-twin simulation optimization.},
  archive      = {J_SOCO},
  author       = {Zhang, HongGuang and Ma, ShengWen and Li, Xiang and You, MingCan and Tao, YuXuan},
  doi          = {10.1007/s00500-025-10434-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1789-1810},
  shortjournal = {Soft Comput.},
  title        = {Forest fire rescue framework to jointly optimize firefighting force configuration and facility layout: A case study of digital-twin simulation optimization},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient collocation algorithm for third order
non-linear emden–fowler equation. <em>SOCO</em>, <em>29</em>(3),
1767–1788. (<a
href="https://doi.org/10.1007/s00500-025-10431-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study presents a novel algorithm for solving third-order non-linear equations (Emden–Fowler type), which can be applied to various physical models. The algorithm uses a quintic trigonometric B-spline collocation method and a quasilinearization technique to avoid the non-linearity term in the equation. The study established a comprehensive error analysis for the proposed algorithm and proved that it has fourth order, i.e., $$(\mathscr {O}(h^4))$$ convergent. The algorithm’s ability to handle singular behavior at the point $$x=0$$ and its faster rate of convergence exhibit a promising approach to solving such problems. The study also validates the theoretical results through numerical experiments and shows that the proposed algorithm has a faster rate of convergence in comparison to the existing methods.},
  archive      = {J_SOCO},
  author       = {Alam, Mohammad Prawesh and Khan, Arshad},
  doi          = {10.1007/s00500-025-10431-3},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1767-1788},
  shortjournal = {Soft Comput.},
  title        = {An efficient collocation algorithm for third order non-linear Emden–Fowler equation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature extraction method for rotating machinery fault
diagnosis based on a multiscale entropy fusion strategy and GA-RL-LDA
model. <em>SOCO</em>, <em>29</em>(3), 1747–1765. (<a
href="https://doi.org/10.1007/s00500-025-10484-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of information loss, feature redundancy and unsatisfactory diagnosis accuracy when using traditional multiscale entropy methods and feature reduction methods to diagnose rotating machinery faults, a feature extraction method based on a multiscale entropy fusion strategy and a GA-RL-LDA model is proposed in this paper. Firstly, the multiscale fluctuation dispersion entropy (MFDE), the refined composite multiscale dispersion entropy (RCMDE) and the refined composite multiscale fluctuation dispersion entropy (RCMFDE) of the collected vibration signal are calculated to form an original feature set. Then, based on the ReliefF algorithm and Laplacian score (LS), an RL index is constructed for feature sensitivity evaluation. After that, combing the RL with Linear discriminant analysis (LDA) and using genetic algorithm (GA) to optimize the uncertain parameters, a GA-RL-LDA model is proposed for feature reduction. Finally, the reduced feature subset is input into support vector machine (SVM) for fault classification. The experiment utilized data from Unit 3 of the SK Hydropower Station and bearing data from Case Western Reserve University, achieving diagnostic accuracies of 95.2381% and 97.3333%, respectively. In the 105 test samples from Unit 3 of the SK Hydropower Station, only 5 samples were misclassified, while in the 150 test samples from Case Western Reserve University, only 4 samples were misclassified. Compared with different information entropy and optimization strategies, the results show that the proposed method can more effectively extract fault sensitive features and accurately diagnose rotating machinery faults even with a small number of training samples.},
  archive      = {J_SOCO},
  author       = {Lu, Na and Li, Zhongliang and Liu, Dong and Cao, Chaofan and Jiang, Shuangyun and Chen, Xudong and Wang, Peng},
  doi          = {10.1007/s00500-025-10484-4},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1747-1765},
  shortjournal = {Soft Comput.},
  title        = {A feature extraction method for rotating machinery fault diagnosis based on a multiscale entropy fusion strategy and GA-RL-LDA model},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning approach to analyse stress by using voice
and body posture. <em>SOCO</em>, <em>29</em>(3), 1719–1745. (<a
href="https://doi.org/10.1007/s00500-025-10441-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current scenario, where we can see young people struggling for their careers, they are even fighting a battle with their stress and tension. None of their work is done without stress to complete their task and compete with others. To overcome stress, one should have good emotional intelligence to cope with emotions and any upcoming stress. But at some point, due to lack of guidance, some people don’t know how to analyze the situations and how to handle them without taking the stress and end up with anxiety, depression, disappointment, suicide, heart attack, stroke etc. Due to the advancement of Human–Computer Interaction (HCI), medical science has leveled up to another peak. Machine Learning and Deep Learning played a major role in such interactions and predictions. Many applications have been developed in past years based on machine learning and deep learning. One of those applications is related to psychology and is still in research. These applications can be used for emotion and stress analysis among people, especially youngsters. Research in this field is being conducted using various verbal and non-verbal parameters. This paper addresses the research problem of improving emotion recognition accuracy and robustness to better analyze and manage stress. The primary objective is to develop an advanced Emotion Recognition System (ERS) that leverages deep learning algorithms to analyses both verbal and non-verbal cues—specifically, speech and body posture, including facial expressions. We have further integrated it with the Flask web framework to make an Emotion Recognition System that takes input in the form of video and audio to analyze Emotions and Stress. We have also compared our proposed ERS with existing ones and found that our ERS gives better results.},
  archive      = {J_SOCO},
  author       = {Gupta, Sumita and Gambhir, Sapna and Gambhir, Mohit and Majumdar, Rana and Shrivastava, Avinash K. and Pham, Hoang},
  doi          = {10.1007/s00500-025-10441-1},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1719-1745},
  shortjournal = {Soft Comput.},
  title        = {A deep learning approach to analyse stress by using voice and body posture},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced TOPSIS-CoCoSo framework for multi-attribute
decision-making with triangular fuzzy neutrosophic sets: “Effect
evaluation of intelligent technology empowering physical education
teaching” case. <em>SOCO</em>, <em>29</em>(3), 1703–1717. (<a
href="https://doi.org/10.1007/s00500-025-10411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The history of human education development has proven that there is an interactive relationship between technological development and education and teaching. In the process of promoting education modernization and high-quality development, the widespread application of intelligent technology in the field of education is the trend, and intelligence is driving profound transformation and transformation in the field of education. The effect evaluation of intelligent technology empowering Physical Education teaching could be considered as multiple-attribute decision-making (MADM). Recently, the TOPSIS technique and Combined Compromise Solution (CoCoSo) technique was employed to deal with MADM. The triangular fuzzy neutrosophic sets (TFNSs) are employed as a better tool for expressing uncertain information during the effect evaluation of intelligent technology empowering Physical Education teaching. In this paper, the triangular fuzzy neutrosophic number TOPSIS-CoCoSo (TFNN-TOPSIS-CoCoSo) technique based on the TFNN relative closeness coefficient (TFNNRCC) technique is managed to cope with the MADM under TFNSs. The information entropy technique is employed to manage the weight values based on the TFNNRCC under TFNSs. Finally, a numerical example of effect evaluation of intelligent technology empowering Physical Education teaching is managed and some better comparisons are managed to verify the TFNN-TOPSIS-CoCoSo technique. The main contribution of this paper is outlined: (1)TFNN-TOPSIS-CoCoSo technique based on the TFNNRCC is constructed; (2) Entropy technique is employed to manage weight based on the TFNNRCC under TFNSs. (3) TFNN-TOPSIS-CoCoSo technique is founded to manage the MADM based on the TFNNRCC under TFNSs; (4) numerical example for effect evaluation of intelligent technology empowering Physical Education teaching and some comparative analysis is supplied to verify the proposed TFNN-TOPSIS-CoCoSo technique.},
  archive      = {J_SOCO},
  author       = {Xiao, Jie and Zhang, Yu},
  doi          = {10.1007/s00500-025-10411-7},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1703-1717},
  shortjournal = {Soft Comput.},
  title        = {Enhanced TOPSIS-CoCoSo framework for multi-attribute decision-making with triangular fuzzy neutrosophic sets: “effect evaluation of intelligent technology empowering physical education teaching” case},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lacunary statistical soft convergence in soft topology.
<em>SOCO</em>, <em>29</em>(3), 1691–1701. (<a
href="https://doi.org/10.1007/s00500-025-10479-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical convergence and some related types of convergence are a generalisation of topological convergence. Similarly, soft set theory, introduced by Molodtsov to deal with uncertainty in various scientific fields, is a generalisation of the classical concept of sets. Although both concepts have found extensive applications to various mathematical structures, the investigation of statistical convergence within soft topological spaces has not yet been undertaken. This study examines the lacunary statistical convergence of sequences of soft points in soft topological spaces, employing a density defined by an unbounded modulus function. Basic results and inclusion theorems concerning this convergence are presented.},
  archive      = {J_SOCO},
  author       = {Bayram, Erdal and Dervişoğlu, Melisa},
  doi          = {10.1007/s00500-025-10479-1},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1691-1701},
  shortjournal = {Soft Comput.},
  title        = {Lacunary statistical soft convergence in soft topology},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-InfNode: Ranking top-k influential nodes in complex
networks with random walk. <em>SOCO</em>, <em>29</em>(3), 1677–1690. (<a
href="https://doi.org/10.1007/s00500-025-10471-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex network is a symbolic representation of distinct real-world systems where information propagates through nodes. Its goal is to identify communities that represent the network’s structure. However, locating the influential node with the maximal range among various nodes and the ability to disseminate influence to a wide portion of the network is one of the most essential concerns in such a network. Centrality is a traditional metric for understanding the effect of nodes in a network, with numerous variants such as closeness, betweenness, degree centrality, and so on. The centrality metrics either work locally or globally to identify influential nodes. In this study, a proposed algorithm named k-InfNode, based on the characteristics of community structure, captures the dynamics of nodes. k-InfNode uses a random walk and combines local and global properties to figure out which nodes are important in a complex network. It was inspired by the idea of overlapping nodes that show how nodes and communities interact with each other across the network. In the beginning, the fuzzy c-means algorithm finds the overlapping nodes in the network. Next, the algorithm assigns an initial score to each node based on node and community information, and iteratively scores each node using the Random Walk with Restart (RWR) algorithm. Experiments performed using real and artificial networks have shown that the k-InfNode is effective.},
  archive      = {J_SOCO},
  author       = {Hasan, Ahmadi and Kamal, Ahmad and Kumar, Pawan},
  doi          = {10.1007/s00500-025-10471-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1677-1690},
  shortjournal = {Soft Comput.},
  title        = {K-InfNode: Ranking top-k influential nodes in complex networks with random walk},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A micro-level approach for modeling rumor propagation in
online social networks. <em>SOCO</em>, <em>29</em>(3), 1667–1675. (<a
href="https://doi.org/10.1007/s00500-025-10456-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks have become the major platforms for information dissemination in recent years. However, rapid propagation of rumors in these networks as a special form of information can greatly influences social lives. Hence, work on rumor propagation models and analysis is under great attention by the research communities. Previously, researchers have proposed various models to explore the dynamics of rumor propagation and analyze steady-state. However, most of them did not consider people’s behavior differences in the spreading or opposing rumor. To overcome this limitation, we assume that individuals have different probability of spreading rumor, spreading anti-rumor and stifling. In this paper we introduce a new model for rumor propagation in social networks considering these differences at micro-level. The proposed model which considered both types of rumor and anti-rumor messages on people decision is an agent-based model in terms of probabilistic automata network. To evaluate the proposed model, we conduct a number of Monte-Carlo simulation experiments on Barabasi-Albert model of social networks that show the accuracy of the proposed model. We also conduct interesting sensitivity analysis to see the effects of different model parameters on the dynamics of the rumor propagation.},
  archive      = {J_SOCO},
  author       = {Sahafizadeh, Ebrahim and Talatian Azad, Saeed},
  doi          = {10.1007/s00500-025-10456-8},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1667-1675},
  shortjournal = {Soft Comput.},
  title        = {A micro-level approach for modeling rumor propagation in online social networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Paw decompositions of diamond and some edge cycle graphs.
<em>SOCO</em>, <em>29</em>(3), 1659–1665. (<a
href="https://doi.org/10.1007/s00500-025-10541-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $${C}_{n}, {K}_{n},{W}_{n},{K}_{r,s}$$ denote a cycle, complete graph, wheel graph, complete bipartite graph respectively. An edge cycle graph of a graph $$G$$ is the graph $$G({C}_{k})$$ formed from one copy of $$G$$ and $$|E(G)|$$ copies of $${P}_{k},$$ where t he ends of the $${i}^{th}$$ edge are identified with the ends of $${i}^{th}$$ copy of $${P}_{k}$$ . In this article, we determine the necessary and sufficient conditions for the existence of paw- decompositions of the diamond graph $${Br}_{n}$$ and some edge cycle graphs like $${K}_{n}\left({C}_{3}\right), { W}_{n}\left({C}_{3}\right),{ K}_{r,s}\left({C}_{3}\right), { C}_{n}\circ \stackrel{\leftharpoonup}{{K}_{m}}({C}_{3})$$ and $${P}_{n}\circ \stackrel{\leftharpoonup}{{K}_{m}}({C}_{3})$$ where $$\circ $$ denotes the corona of graphs.},
  archive      = {J_SOCO},
  author       = {Esakkimuthu, Murugan and Rameshbabu, Sivaprakash Gunniya},
  doi          = {10.1007/s00500-025-10541-y},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1659-1665},
  shortjournal = {Soft Comput.},
  title        = {Paw decompositions of diamond and some edge cycle graphs},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On goal programming approach for interval-valued
intuitionistic fuzzy multi-objective transportation problems with an
application to tourism industry. <em>SOCO</em>, <em>29</em>(3),
1627–1657. (<a
href="https://doi.org/10.1007/s00500-025-10420-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation problems are inevitably affected by numerous imprecise factors like weather, fuel expenses, topography, etc. Hence, the use of crisp parameters to model transportation problems appears to be both insufficient and inaccurate. Consequently, transportation problems using fuzzy/ intuitionistic fuzzy (IF) numbers seem more effective. Interval-valued intuitionistic fuzzy (IVIF) numbers are further generalization of IF numbers where membership and non-membership degrees are closed sub-intervals of [0, 1]. This concept of allocating interval values helps in dealing with the hesitancy of decision-maker while assigning fixed values to membership and non-membership degrees. In this article, balanced transportation problems having multiple objectives under the IVIF environment are examined. To overcome inconsistencies in the existing approaches, novel linear as well as non-linear interval-valued membership and non-membership functions have been proposed. Subsequently, an improved IVIF programming approach is developed using these newly defined functions along with theoretical validation. In addition, when goals are associated with objective functions, the proposed approach has been further improvised as IVIF prioritized goal programming. Eventually, a trip planning problem in the tourism industry is exhibited to illustrate the proposed IVIF technique and later, it is amalgamated with prioritized goals to demonstrate the proposed IVIF goal programming approach.},
  archive      = {J_SOCO},
  author       = {Chauhan, Abhishek and Mahajan, Sumati},
  doi          = {10.1007/s00500-025-10420-6},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1627-1657},
  shortjournal = {Soft Comput.},
  title        = {On goal programming approach for interval-valued intuitionistic fuzzy multi-objective transportation problems with an application to tourism industry},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware coverage path planning for a swarm of UAVs
using mobile ground stations for battery-swapping. <em>SOCO</em>,
<em>29</em>(3), 1605–1625. (<a
href="https://doi.org/10.1007/s00500-025-10537-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of swarms of drones is expected to continue growing in the next years, particularly in dangerous scenarios, such as monitoring and rescue missions in hostile and disaster areas. Small-sized Unmanned Aerial Vehicles (UAVs) are highly suitable for use in such scenarios due to their agility and maneuverability. On the other hand, their limited battery capacity poses significant challenges, especially during missions requiring full coverage of large areas in a short time and extreme weather conditions. This work proposed an energy efficiency approach, which makes use of mobile ground-based battery-swapping stations (BSSes), to speed up the UAV’s battery replacement and reduce energy waste in the round trip to the charging station. Specifically, a Context-Aware Coverage Path Planning (CACPP) problem has been formulated to determine the complete coverage path of a large area by a swarm of UAVs, minimizing the path overlapping and UAV battery swapping. The model takes into account the need to continue re-planning the mission, depending on the weather conditions (i.e., temperature and wind), the presence of obstacles, and the residual energy levels of the drones, as well as the relative positions of the drones and mobile BSSes. To solve the CACPP problem, an iterative approach leveraging two synchronized optimization models for planning UAV paths and BSS routes has been presented. As the CACPP problem is NP-hard, a heuristic procedure for solving it has also been evaluated. Experimental results show that it can be appropriate for large instances of the problem.},
  archive      = {J_SOCO},
  author       = {Porcelli, Lorenzo and Ficco, Massimo and D’Angelo, Gianni and Palmieri, Francesco},
  doi          = {10.1007/s00500-025-10537-8},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1605-1625},
  shortjournal = {Soft Comput.},
  title        = {Context-aware coverage path planning for a swarm of UAVs using mobile ground stations for battery-swapping},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using deep forest regression and multi-layer state
transition algorithm to soft measuring modeling with small sample data.
<em>SOCO</em>, <em>29</em>(3), 1587–1603. (<a
href="https://doi.org/10.1007/s00500-025-10527-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In actual industrial operation process, some key performance indicators (KPIs) are tricky to detect online due to the characteristics of the detection equipment and the nature of the parameters. Moreover, these KPIs usually present small sample attributes. In this article, a stable and efficient soft measuring model for the KPIs of industrial processes is proposed using deep forest regression (DFR) and multi-layer state transition algorithm (STA). First, DFR is used to build soft measuring models for KPIs with random initial hyperparameters. Second, an improved dynamic STA (DSTA) is developed to optimize the DFR’s hyperparameters. Furthermore, the probability parameters of the DSTA structure are optimally selected using a STA. Finally, gradient refinement is utilized to fine-tune the state factor, which achieves a more accurate optimization process during the internal iteration process. The proposed algorithm is evaluated on the benchmark function, dataset, and an actual industrial problem. Results prove that the use of our method in soft measuring modeling can be effective.},
  archive      = {J_SOCO},
  author       = {Xia, Heng and Tang, Jian and Yu, Wen},
  doi          = {10.1007/s00500-025-10527-w},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1587-1603},
  shortjournal = {Soft Comput.},
  title        = {Using deep forest regression and multi-layer state transition algorithm to soft measuring modeling with small sample data},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A community-based simulated annealing approach with a new
structure-based neighborhood search to identify influential nodes in
social networks. <em>SOCO</em>, <em>29</em>(3), 1567–1585. (<a
href="https://doi.org/10.1007/s00500-025-10490-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying influential nodes has attracted the attention of many researchers in recent years. Because of the weak tradeoff between accuracy and running time, and ignoring the community structure by the proposed algorithms in the past research studies, further studies in this area are required. In this paper, we consider communities and also use a novel structure-based neighborhood search to improve exploration strategy of the simulated annealing (SA) algorithm. Moreover, we use the k-shell method for generating a better initial solution instead of random generation. In the proposed algorithm called Ckshell-SA, first, the communities are detected, then the k-shell method is used in each community to find initial candidate nodes locally. Finally, SA algorithm is applied with a neighborhood search that considers the structural properties of the network, and three centralities to find the influential nodes globally. A derivative of the Ckshell-SA method called kshell-SA is also introduced in this paper to examine the impact of considering communities. Unlike the Ckshell-SA, the community structure is neglected, and the k-shell is performed on the whole network in kshell-SA algorithm. Extensive experiments are conducted on eight real-world networks under Independent Cascade Model (IC) and Weighted Independent Cascade Model (WC). The results show that the Ckshell-SA and kshell-SA algorithms outperform the state-of-the-art algorithms concerning influence spread. Furthermore, the results show that Ckshell-SA is more efficient in networks like Facebook with a high Power Law exponent and higher modularity. On the contrary, kshell-SA is more successful in networks like Slashdot or Epinions with lower modularity.},
  archive      = {J_SOCO},
  author       = {Abyaneh, Farzaneh Rajaee and Charkari, Nasrollah Moghadam and Roayaei, Mehdy},
  doi          = {10.1007/s00500-025-10490-6},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1567-1585},
  shortjournal = {Soft Comput.},
  title        = {A community-based simulated annealing approach with a new structure-based neighborhood search to identify influential nodes in social networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid genetic search based approach for the generalized
vehicle routing problem. <em>SOCO</em>, <em>29</em>(3), 1553–1566. (<a
href="https://doi.org/10.1007/s00500-025-10507-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel meta-heuristic for addressing a variant of the classical Capacitated Vehicle Routing Problem (CVRP) known as the Generalized Vehicle Routing Problem (GVRP). In the GVRP, nodes are organized into clusters, with the constraint that only one node from each cluster must be visited. The proposed meta-heuristic is a Hybrid Genetic Search (HGS) that leverages recent advancements in CVRP methodologies, adapting successful strategies and techniques from CVRP to the GVRP context. To evaluate the performance of the HGS meta-heuristic, we perform an extensive computational analysis on numerous benchmark instances ranging from small to large sizes. To thoroughly analyze the algorithm’s average behavior, convergence profiles over time are reported for the considered instances. Results show that the proposed algorithm achieves 174 new best solutions out of the 498 instances considered. In only six instances out of 498, the algorithm is unable to reach or improve upon the best-known solution in the literature. These results suggest that the proposed meta-heuristic has significant potential in addressing real-world generalized vehicle routing challenges. Code available at: https://github.com/vlatorre847/HGSGVRP .},
  archive      = {J_SOCO},
  author       = {Latorre, Vittorio},
  doi          = {10.1007/s00500-025-10507-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1553-1566},
  shortjournal = {Soft Comput.},
  title        = {A hybrid genetic search based approach for the generalized vehicle routing problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient reconfigurable architecture to extract image
features for face recognition using local binary pattern. <em>SOCO</em>,
<em>29</em>(3), 1541–1552. (<a
href="https://doi.org/10.1007/s00500-025-10415-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of the face is a widely used method to detect human features. In various scenarios the face recognition speed becomes significant which necessitates to improve the critical delay of the architecture. In this paper, we propose Efficient FPGA architecture to extract image features using Local Binary pattern (LBP) for Face Recognition. The face image is converted into standard size (256 × 256) as pre-processing and the Gaussian filter is used to remove the high frequency components. These image is then applied to optimized LBP block to obtain the LBP features for both database sample and test sample are further compared to make the decision for face recognition. The proposed LBP architecture is designed using simple counter and comparators which leads to minimum complexity in turn improving the critical delay and hardware utilizations of the entire system. The simulation is performed for Olivetti Research Laboratory (ORL) dataset using MATLAB by showing False Acceptance Rate (FAR), False Rejection Rate (FRR) and Total Success Rate (TSR) values. The thresholding is performed based on Weighted Mean Square Difference and is varied for Total Success Rate (TSR) calculations tested for different combinations of Person in Database (PID) and Person Out of database (POD). Finally, the proposed architecture is synthesized on Spartan 6-xc651 × 4c-3csg432 Digilent FPGA board. It is observed that the recognition time of our architecture in hardware (FPGA) is 1.05 µS which is better compared to existing methods.},
  archive      = {J_SOCO},
  author       = {Bhavikatti, Sumangala and Bhairannawar, Satish},
  doi          = {10.1007/s00500-025-10415-3},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1541-1552},
  shortjournal = {Soft Comput.},
  title        = {Efficient reconfigurable architecture to extract image features for face recognition using local binary pattern},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposition matheuristics for last mile delivery using
public transportation systems. <em>SOCO</em>, <em>29</em>(3), 1511–1539.
(<a href="https://doi.org/10.1007/s00500-025-10513-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the potential of using public transportation systems for freight delivery, where we intend to utilize the spare capacities of public vehicles like buses, trams, metros, and trains, particularly during off-peak hours, to transport packages within the city instead of using dedicated delivery vehicles. The study contributes to the growing literature on innovative strategies for performing sustainable last mile deliveries. We study an operational level problem called the Three-Tier Delivery Problem on Public Transportation, where packages are first transported from the Consolidation and Distribution Center (CDC) to nearby public vehicle stations by delivery trucks, comprising the first tier of the problem. In the second tier, the public vehicles pick them up from the stops and transport them into the city area. The last leg, or the third tier of the delivery, is performed to deliver the packages to their respective customers using green vehicles or eco-friendly systems. We propose mixed-integer linear programming formulations to study the transport of packages from the CDC to the customers and employ decomposition-based matheuristics to solve them. We have three decomposition approaches based on the order of solving the tiers, resulting from the tier we start solving the problem from. We use a heuristic methodology to link the tiers by coordinating the flow of packages between them, and utilize CPLEX to solve the individual tiers. We provide numerical experiments to demonstrate the efficiency and effectiveness of the system. Our results show that this system has the potential to reduce the length of trips performed by traditional delivery trucks by 85.91%, thereby reducing the negative social and environmental impacts of existing last mile delivery systems.},
  archive      = {J_SOCO},
  author       = {Mandal, Minakshi Punam and Archetti, Claudia},
  doi          = {10.1007/s00500-025-10513-2},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1511-1539},
  shortjournal = {Soft Comput.},
  title        = {Decomposition matheuristics for last mile delivery using public transportation systems},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure quantum homomorphic encryption ciphertext retrieval
scheme. <em>SOCO</em>, <em>29</em>(3), 1497–1509. (<a
href="https://doi.org/10.1007/s00500-025-10454-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent paper (Gong et al. Quantum Inf Process 19:3, 2020), a novel ciphertext retrieval scheme based on the Grover algorithm and quantum homomorphic encryption was presented. In this scheme, when the server performs the operation of marking the solution on the user’s encrypted state in the Grover iteration, it needs to remove many gate-errors generated in the homomorphic evaluation of the T gate. And the server could judge this specific solution from the quantum circuit of marking the solution. It makes this scheme unable to achieve the low-cost and secure ciphertext retrieval. Therefore, we improve the Gong et al.’s scheme and propose a secure quantum homomorphic encryption ciphertext retrieval scheme. In our scheme, the trusted third party is introduced to cooperate with the server to execute the Grover algorithm. In each Grover iteration, the trusted third party can quickly mark the solution on the plaintext state, encrypt the marked state, and transmit it to the server. Then the server performs the remaining operations of this Grover iteration on the encrypted state. The trusted third party finally decrypts the iterated state. This cooperative approach ensures that the number of auxiliary qubits required and extra quantum gates executed in our scheme are lower than the Gong et al.’s scheme. By analyzing the security of our scheme, we confirm that the server and the trusted third party will not be informed of this solution. Thus, our scheme realizes the secure ciphertext retrieval with low computational overhead. We utilize IBM’s Qiskit framework to simulate our scheme, and the experimental result shows that our scheme is correct. It is worth noting that the low-cost and secure ciphertext retrieval will play a crucial role in modern information security and privacy protection.},
  archive      = {J_SOCO},
  author       = {Cheng, Zhen-Wen and Chen, Xiu-Bo and Xu, Gang and Chang, Yan and Miao, Li-Hua and Yang, Yi-Xian and Wang, Ya-Lan},
  doi          = {10.1007/s00500-025-10454-w},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1497-1509},
  shortjournal = {Soft Comput.},
  title        = {A secure quantum homomorphic encryption ciphertext retrieval scheme},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing signature scheme: The enhanced edward
elgamal extreme performance accumulate signature approach for IoT and
blockchain applications. <em>SOCO</em>, <em>29</em>(3), 1473–1496. (<a
href="https://doi.org/10.1007/s00500-025-10426-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital signatures, essential for establishing trust in the digital realm, have evolved in their application and importance alongside emerging technologies such as the Internet of Things (IoT), Blockchain, and cryptocurrency. These advancements necessitate improvements in performance, security, and efficiency. This article examines and compares the Elliptic Curve Digital Signature Algorithm with the Hyper Elliptic Curve Digital Signature Algorithm and the Edwards Curve Digital Signature Algorithm. We highlight its superior capabilities for blockchain and IoT applications and advocate for its potential to deliver immediate enhancements in security and performance. Our study introduces a novel digital signature scheme specifically designed to enhance non-repudiation in blockchain ecosystems. Utilizing the Optimized Extreme Performance Edwards Curve Accumulated Signature scheme, our approach significantly reduces signing and verification times by 10% and 13%, respectively, compared to traditional signatures. Additionally, it offers a 10% boost in transaction throughput and block validation efficiency. Experiments conducted within various blockchain-integrated IoT setups demonstrate the scheme&#39;s effectiveness, consistently achieving improvements across diverse IoT sensor data. This highlights the innovative contribution of our scheme to the efficiency and security of blockchain technology.},
  archive      = {J_SOCO},
  author       = {Anusha, R. and Saravanan, R.},
  doi          = {10.1007/s00500-025-10426-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1473-1496},
  shortjournal = {Soft Comput.},
  title        = {Revolutionizing signature scheme: The enhanced edward elgamal extreme performance accumulate signature approach for IoT and blockchain applications},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank decomposition optimization and its application in
fabric defects. <em>SOCO</em>, <em>29</em>(3), 1453–1472. (<a
href="https://doi.org/10.1007/s00500-025-10399-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank decomposition model is frequently employed in defect detection. It separates the target matrix into a low-rank component and a sparse component using the nuclear norm and the $$l_1$$ -norm, which aids in extracting the background and defects. However, the nuclear norm, derived from singular value decomposition, often fails to effectively extract the background of fabrics. This paper introduces a novel matrix norm, defined by integrating several key elementary functions, enhancing the separation of the low-rank and sparse matrices. The Alternating Direction Method of Multipliers (ADMM) typically solves the low-rank decomposition model with a fixed step size penalty factor. This study dynamically adjusts the penalty factor based on defect detection characteristics, thus enhancing the algorithm’s computational efficiency. Additionally, the convergence of the proposed algorithm is validated. Experimental results demonstrate that this new model not only precisely distinguishes the sparse matrix but also achieves higher computational efficiency, surpassing other existing methods in both accuracy and efficiency.},
  archive      = {J_SOCO},
  author       = {Chen, Zhixiang and Shi, Wenya and Liang, Jiuzhen and Liu, Hao},
  doi          = {10.1007/s00500-025-10399-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1453-1472},
  shortjournal = {Soft Comput.},
  title        = {Low-rank decomposition optimization and its application in fabric defects},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient COVID-19 detection using data mining algorithms: A
comparison of basic and hybrid approaches. <em>SOCO</em>,
<em>29</em>(3), 1437–1451. (<a
href="https://doi.org/10.1007/s00500-025-10538-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient diagnosis of COVID-19 remains a significant challenge due to the limitations of current detection methods, such as blood tests and chest scans, which can be time-consuming and error-prone. This study aims to compare the performance of basic and hybrid data mining algorithms in diagnosing COVID-19, using blood test results and clinical information to identify the most effective approach. A dataset of 200 records from suspected and infected COVID-19 patients, with 23 characteristics and one diagnostic class, was analysed. Nine data mining algorithms were tested: four basic algorithms (Naive Bayes, Support Vector Machine, Decision Tree, K-Nearest Neighbor) and five hybrid algorithms (Random Forest, AdaBoost, Majority Voting, XGBoost, Bagging). The study also integrated Response Surface Methodology (RSM) and Adaptive-Network-based Fuzzy Inference System (ANFIS) to enhance model performance. The Bagging algorithm demonstrated superior performance with an accuracy of 88%, sensitivity of 74%, and F-criterion of 78%. The integration of RSM and ANFIS further showed that a smart model could be developed for efficient pandemic crisis management, achieving up to 100% accuracy when considering key factors like AST, Albumin, and CRP. The findings suggest that Bagging and hybrid data mining algorithms can significantly improve COVID-19 detection, reducing time and errors in identifying exposed individuals. The study highlights the potential of combining machine learning techniques with RSM-ANFIS models for effective pandemic management and decision-making in medical settings.},
  archive      = {J_SOCO},
  author       = {Saidi, Mohammad and Gheibi, Mohammad and Ghazikhani, Adel and Lotfata, Aynaz and Chahkandi, Benyamin and Familsamavati, Sajad and Behzadian, Kourosh},
  doi          = {10.1007/s00500-025-10538-7},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1437-1451},
  shortjournal = {Soft Comput.},
  title        = {Efficient COVID-19 detection using data mining algorithms: A comparison of basic and hybrid approaches},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging variant of CAE with sparse convolutional
embedding and two-stage application-driven data augmentation for image
clustering. <em>SOCO</em>, <em>29</em>(3), 1419–1435. (<a
href="https://doi.org/10.1007/s00500-025-10500-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep clustering approaches often struggle with redundant feature learning, which limits their effectiveness. The primary goal of this study is to address these issues by developing a more robust deep clustering method. To achieve this, we propose a variant of the convolutional autoencoder (CAE) called SCDAC, which incorporates sparse convolutional embedding and a two-stage application-driven data augmentation approach. The proposed model operates in two main stages: pretraining and finetuning. In the pretraining stage, we employ application-driven data augmentation to train the CAE variant, focusing on learning robust features and constructing a foundational feature space using sparse convolutional embedding. During the finetuning stage, the model performs joint feature learning and cluster assignment. The feature learning task utilizes an augmented framework to control the input of both original and augmented data, preserving the local structure of images in the feature space. For cluster assignment, the framework controls the input of original data and uses the sparse convolutional embedding layer to obtain low-dimensional representations for soft cluster assignment. Experimental evaluations on six publicly available datasets demonstrate the effectiveness of the proposed model, with significant improvements in accuracy, particularly increases of $$3\%$$ and $$10.3\%$$ on the COIL20 and ORL datasets, respectively. In conclusion, our findings underscore the significance of the SCDAC approach in enhancing deep image clustering performance, offering a viable solution to the limitations of existing methods.},
  archive      = {J_SOCO},
  author       = {Liu, Yanming and Liu, Jinglei},
  doi          = {10.1007/s00500-025-10500-7},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1419-1435},
  shortjournal = {Soft Comput.},
  title        = {Leveraging variant of CAE with sparse convolutional embedding and two-stage application-driven data augmentation for image clustering},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatgpt and operations research: Evaluation on the shortest
path problem. <em>SOCO</em>, <em>29</em>(3), 1407–1418. (<a
href="https://doi.org/10.1007/s00500-025-10505-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ChatGPT tool, the large language model developed by OpenAI, is having a great impact among users, experts and scholars for its capabilities of answering questions and retrieving solutions automatically. Despite the short time since its release, it has already been employed in several application domains. However, to the best of our knowledge, it has not been studied in the field of operation research (OR). In this paper, we use ChatGPT to define solution strategies for addressing several variants of the shortest path problem. The results obtained by executing the solution approaches returned by the tool are compared, in terms of correctness and efficiency, to reference codes. They indicate that the proper utilization of this tool could represent a good aid for domain experts. In particular, the outputs provided by ChatGPT could represent not only a good base for more complex implementations, but also they represent a way to facilitate some tasks in order to reduce times to do certain activities, which in any case must involve human control, adaptation and supervision.},
  archive      = {J_SOCO},
  author       = {Luzzi, Martina and Guerriero, Francesca and Maratea, Marco and Greco, Gianluigi and Garofalo, Marco},
  doi          = {10.1007/s00500-025-10505-2},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1407-1418},
  shortjournal = {Soft Comput.},
  title        = {Chatgpt and operations research: Evaluation on the shortest path problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using past sample means in exponential ratio and regression
type estimators under a simple random sampling. <em>SOCO</em>,
<em>29</em>(3), 1389–1406. (<a
href="https://doi.org/10.1007/s00500-025-10408-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical sampling commonly employs auxiliary variables for the selection and estimation phases to improve efficiency of the estimators. However, existing estimators like ratio and product types display limitations under specific conditions. Regression-type estimators, known for their unbiasedness and efficiency, rely solely on current sample information. This highlights the need for more effective estimators capable of leveraging both past and current sample means to improve accuracy and applicability across diverse datasets. In this study, we introduce two novel memory-type estimators, drawing inspiration from Noor-ul-Amin&#39;s (2020) approach, which integrates past and current sample information using Hybrid Exponentially Weighted Moving Averages (HEWMA), particularly effective for time-based surveys. Through simulation studies and real data examples, we evaluate the performance of our estimators and identify crucial shortcomings in previous memory-type estimator studies. Furthermore, we highlight significant deficits in previous studies, particularly concerning the impact of sample sizes based on past means, correlation, number of past means, weight parameters and initial values of EWMA and HEWMA algorithms, and the distribution shape of the data on estimator efficiency. Our findings underscore the importance of parameter selection in HEWMA, a greater number of past means, and the significance of past sample sizes for optimizing the performance of the proposed memory-type estimators. By integrating HEWMA, our approach enhances the efficiency and applicability of these estimators, addressing essential gaps in the existing literature and laying the groundwork for more robust and efficient estimation techniques for future studies that use mean.},
  archive      = {J_SOCO},
  author       = {Koçyiğit, Eda Gizem},
  doi          = {10.1007/s00500-025-10408-2},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1389-1406},
  shortjournal = {Soft Comput.},
  title        = {Using past sample means in exponential ratio and regression type estimators under a simple random sampling},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection of multiple types of data: A PLS
approach. <em>SOCO</em>, <em>29</em>(3), 1369–1387. (<a
href="https://doi.org/10.1007/s00500-025-10531-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of data collection techniques in recent years, multiple types of data have emerged, including scalar data, functional data (curve-like), and compositional data (pie-like). While existing studies propose predictive models for multiple-type of data, few address the issue of variable selection. The challenge lies in the fact that different data types originate from different vector spaces, making it difficult to conduct variable selection at the variable level instead of selection at their sub-component level. This study leverages the group selection ability of gPLS (group Partial Least Squares) and gsPLS (group sparse Partial Least Squares) by regarding the functional and compositional variables as natural groups and proposes two variable selection approaches, named MD-gPLS and MD-gsPLS, after building a vector space for multiple types of data. Numerical studies and real-world examples verify the effectiveness of the proposed approaches. This study broadens the statistical modeling tools of multiple types of data analysis in terms of variable selection and also contributes to the literature by introducing the vector space of multiple types of data.},
  archive      = {J_SOCO},
  author       = {Kong, Boao and Wang, Huiwen and Lu, Shan},
  doi          = {10.1007/s00500-025-10531-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1369-1387},
  shortjournal = {Soft Comput.},
  title        = {Variable selection of multiple types of data: A PLS approach},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictor–corrector approach for the numerical solution of
fuzzy fractional differential equations and linear multiterm fuzzy
fractional equations. <em>SOCO</em>, <em>29</em>(3), 1347–1368. (<a
href="https://doi.org/10.1007/s00500-025-10401-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the modeling of fuzzy fractional differential equations (FFDEs) has been a very significant issue in many new applications in applied sciences and engineering, while a natural tool for modeling such dynamical systems is to use fuzzy fractional differential equations. We establish the existence and uniqueness of solutions for fuzzy fractional differential equations under sufficient assumptions and contraction principles and study numerical solutions of FFDEs. Our study is based on Caputo’s generalized Hukuhara differentiability. By applying Schauder’s fixed point theorem and a hypothetical condition, we explore the existence of the solutions. In addition, we show the uniqueness of the system&#39;s solution by using the contraction mapping theorem. We analyze the predictor–corrector approach (PCA) for FFDEs and multiterm FFDEs. We utilize the PCA to find the approximate solutions to linear multiterm FFDEs under the Caputo fuzzy derivative. After that, we present numerical solutions to initial value problems for solving two families of fuzzy fractional problems: fuzzy fractional differential equations (FFDEs) and multiterm fuzzy fractional differential equations (MFFDEs) utilizing the PCA. The method used in this paper has several advantages; first, it is significant and yields stable results without diverging as well as its ability to solve other mathematical, physical, and engineering problems; second, it is higher accuracy, needs less effort to achieve the results and works to reduces the error between exact and approximate solutions, as depicted in the utilized figures and tables. Finally, the accuracy of our suggested approach is demonstrated by solving some specific examples and analyzing the figures and tables, along with several suggestions for future research directions.},
  archive      = {J_SOCO},
  author       = {Al-Sadi, Wadhah and Wei, Zhouchao and Moroz, Irene and Abu Arqub, Omar and Abdullah, Tariq Q. S.},
  doi          = {10.1007/s00500-025-10401-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1347-1368},
  shortjournal = {Soft Comput.},
  title        = {Predictor–corrector approach for the numerical solution of fuzzy fractional differential equations and linear multiterm fuzzy fractional equations},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach data processing: Density-based spatial
clustering of applications with noise (DBSCAN) clustering using
game-theory. <em>SOCO</em>, <em>29</em>(3), 1331–1346. (<a
href="https://doi.org/10.1007/s00500-025-10405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unpredictable growth of data in various fields, rapid clustering of big data is seriously needed in order to identify the hidden structure of data and discover the relationships between objects. Among clustering methods, density-based clustering methods have an acceptable processing speed for dealing with big data with high dimensions. However, some methods have fixed parameters that are certainly not optimized for all sections. In addition, the complexity of these clustering methods strongly depends on the number of objects. In this paper, a clustering method is presented in order to increase clustering performance and parameter sensitivity according to game-theory and using the concept of Nash equilibrium and dense games, the optimal parameter for clustering is selected and between noise and points clusters make a difference. This method includes (1) searching the grid with several spaces in which there is no cluster, (2) identifying the player through high density data points in order to determine the parameters and (3) combining the clusters to make the game and (4) merging the nearby clusters. The performance of the proposed method was evaluated in four big synthetic datasets, eight real datasets labeled and unlabeled. The obtained results indicate the superiority of the proposed method over SOM, K-means, DBSCAN, SCGPSC methods in terms of accuracy and purity in processing time.},
  archive      = {J_SOCO},
  author       = {Kazemi, Uranus and Soleimani, Seyfollah},
  doi          = {10.1007/s00500-025-10405-5},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1331-1346},
  shortjournal = {Soft Comput.},
  title        = {A new approach data processing: Density-based spatial clustering of applications with noise (DBSCAN) clustering using game-theory},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arithmetic optimization algorithm with cosine
transform-based two-dimensional composite chaotic mapping.
<em>SOCO</em>, <em>29</em>(3), 1289–1329. (<a
href="https://doi.org/10.1007/s00500-025-10412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arithmetic optimization algorithm (AOA) is a newly developed meta-heuristic algorithm that draws inspiration from the combination of arithmetic operations. Since many scholars have widely used traditional one-dimensional chaotic mapping at home and abroad in function optimization, the AOA based on cosine transform two-dimensional composite chaotic mapping is proposed. Firstly, seven two-dimensional chaotic mappings are proposed to be embedded into the MOA and MOP in AOA. Secondly, one-dimensional chaotic systems based on the cosine transform are put forward. Then the proposed chaotic system based on the cosine transform is combined with the two-dimensional chaotic mapping to form the cosine transformed two-dimensional composite chaotic mapping. Finally, six more cosine transformed two-dimensional composite chaotic mappings are embedded into the MOA and MOP of the AOA to balance the algorithm&#39;s global and local searching ability and improve the algorithm&#39;s performance. The superiority of the improved algorithm is verified by employing 12 benchmark test functions in CEC2022. Then it is compared with the Coati Optimization Algorithm (COA), Prairie Dog Optimization (PDO), Butterfly Optimization Algorithm (BOA), Reptile Search Algorithm (RSA), Bat Algorithm (BAT), and Rat Swarm Optimization (RSO) to verify its convergence. Finally, four engineering design problems (tension/compression spring problem, pressure vessel problem, cantilever beam design problem, and slotted bulkhead design problem) were optimized to validate the efficiency of the improved algorithm. The simulation experiments demonstrate that the improved AOA exhibits superior performance in addressing both function and engineering optimization problems. It showcases remarkable optimization capabilities and improves convergence accuracy.},
  archive      = {J_SOCO},
  author       = {Li, Yi-Xuan and Wang, Jie-Sheng and Zhang, Si-Wen and Zhang, Shi-Hui and Guan, Xin-Yi and Ma, Xin-Ru},
  doi          = {10.1007/s00500-025-10412-6},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1289-1329},
  shortjournal = {Soft Comput.},
  title        = {Arithmetic optimization algorithm with cosine transform-based two-dimensional composite chaotic mapping},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some new construction methods of similarity measure on
picture fuzzy sets. <em>SOCO</em>, <em>29</em>(3), 1273–1287. (<a
href="https://doi.org/10.1007/s00500-025-10536-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Picture fuzzy sets address problems characterized by ambiguity, instability and inconsistent data. Similarity measures on picture fuzzy sets play an indispensable role in determining the relationships between two such sets. Consequently, the study of similarity measures for picture fuzzy sets has garnered significant attention from scholars, yielding fruitful results. Notably, the existing research on picture fuzzy set similarity has mainly focused on overcoming the limitations of certain existing similarity measures by proposing one or a few new ones, ignoring the construction methods for similarity measures. Therefore, this paper presents two novel construction methods for similarity measures on picture fuzzy sets. The first approach combines the differences among positive membership, neutral membership, negative membership, and refusal membership within picture fuzzy sets using a strictly monotonically decreasing function. Remarkably, this method not only integrates existing similarity measures but also generates novel ones, providing a unified framework for both. The second method employs a strictly decreasing binary function to aggregate the distance measures between two picture fuzzy sets. By varying the binary function and distance measures, we obtain a range of novel similarity measures. Additionally, we apply the newly developed similarity measures to pattern recognition and compare their performance against existing measures. Based on the identification results, it is evident that these novel similarity measures yield reasonable outcomes and exhibit a high degree of reliability.},
  archive      = {J_SOCO},
  author       = {Luo, Minxia and Gao, Jianlei and Li, Wenling},
  doi          = {10.1007/s00500-025-10536-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1273-1287},
  shortjournal = {Soft Comput.},
  title        = {Some new construction methods of similarity measure on picture fuzzy sets},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
