<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml---32">ML - 32</h2>
<ul>
<li><details>
<summary>
(2025). An empirical study on impact of label noise on synthetic
tabular data generation. <em>ML</em>, <em>114</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s10994-024-06629-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data has been actively used for various machine learning-based tasks due to its benefits such as massive reproducibility and privacy enhancement compared to using the original data. The quality of the generated synthetic dataset crucially depends on the quality of the original data, and the latter is often corrupted by label noise. While there have been studies on feature noise, how label noise affects synthetic data generation is under-explored. In this paper, we evaluate the impact of the noisy label on synthetic data generation with a focus on tabular data. One challenge is how to evaluate the quality of synthetic data under label noise. To this end, we design comprehensive experiments to measure the impact of label noise on synthetic data generation in different aspects: synthetic data quality, data utility, and convergence for training synthesizers and machine learning models for downstream tasks. The empirical results cover wide aspects of synthetic data generation under label noise and they show quality and utility degrades with higher noise levels while there is no significant effect on the synthesizer convergence observed.},
  archive      = {J_ML},
  author       = {Kim, Jeonghoon and Huang, Chao and Liu, Xin},
  doi          = {10.1007/s10994-024-06629-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {An empirical study on impact of label noise on synthetic tabular data generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing cross-validation variance through seed blocking in
hyperparameter tuning. <em>ML</em>, <em>114</em>(4), 1–48. (<a
href="https://doi.org/10.1007/s10994-024-06630-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross-validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) is commonly employed to reduce the variability of CV errors. This study investigates the efficacy of blocking cross-validation partitions and algorithm initialization seeds during hyperparameter tuning. The proposed approach, termed Controlled Cross-Validation (CCV), reduces variability in error estimates, enabling fairer and more reliable comparisons of predictive model performance. We provide both theoretical and empirical evidence to demonstrate that this blocking approach lowers the variance of the estimates compared to RCV. Our experiments indicate that the algorithm’s internal random behavior often does not significantly affect CV error variability. We present extensive examples using real-world datasets to compare the effectiveness and efficiency of blocking the CV partitions when tuning the hyperparameters of different supervised predictive learning algorithms.},
  archive      = {J_ML},
  author       = {Merola, Giovanni Maria},
  doi          = {10.1007/s10994-024-06630-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-48},
  shortjournal = {Mach. Learn.},
  title        = {Reducing cross-validation variance through seed blocking in hyperparameter tuning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive learning of robot task knowledge from raw data and
online expert feedback. <em>ML</em>, <em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06636-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios. In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user’s feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.},
  archive      = {J_ML},
  author       = {Meli, Daniele and Fiorini, Paolo},
  doi          = {10.1007/s10994-024-06636-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Inductive learning of robot task knowledge from raw data and online expert feedback},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient projection-free online convex optimization using
stochastic gradients. <em>ML</em>, <em>114</em>(4), 1–61. (<a
href="https://doi.org/10.1007/s10994-024-06640-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Online Convex Optimization (OCO) problems subject to a compact convex set. An important class of projection-free online methods known as Frank–Wolfe-type (FW-type) methods have attracted considerable attention in the machine learning community, as they eschew the expensive projection operation and only require a simple linear minimization oracle in each round. Recently, the stochastic gradient technique has been integrated in FW-type online methods to circumvent the expensive full gradient computation and further reduce the per-round computational cost. However, these methods generally have high regret bounds due to high variance in gradient estimation. Although adopting a large minibatch in stochastic gradients can reduce the variance, it would in turn increase the per-round computational cost. In this paper, we develop efficient FW-type methods that only need stochastic gradients with small minibatch and achieve nearly optimal regret bounds with low per-round costs. We first explore the similarity between gradients of decision variables in consecutive rounds, and construct a lightweight variance-reduced estimator by utilizing historical gradient information. Based on this estimator, we propose a method named OFWRG for smooth problems in the stochastic setting. We prove that OFWRG achieves a nearly optimal regret bound with the lowest $$\mathcal {O}(1)$$ per-round computational cost. OFWRG is the first method with such nearly optimal result in this setting. We further extend OFWRG to OCO problems in other settings, including smooth problems in the adversarial setting and a class of non-smooth problems in the stochastic and adversarial settings. Our theoretical analyses show that these extensions of OFWRG achieve nearly optimal regret bounds and low per-round computational costs under mild conditions. Experimental results demonstrate the efficiency of our methods.},
  archive      = {J_ML},
  author       = {Xie, Jiahao and Zhang, Chao and Shen, Zebang and Qian, Hui},
  doi          = {10.1007/s10994-024-06640-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-61},
  shortjournal = {Mach. Learn.},
  title        = {Efficient projection-free online convex optimization using stochastic gradients},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated explanations for regression. <em>ML</em>,
<em>114</em>(4), 1–34. (<a
href="https://doi.org/10.1007/s10994-024-06642-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) methods are an integral part of modern decision support systems. The best-performing predictive models used in AI-based decision support systems lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature’s importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations, previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is below an arbitrary threshold. The extension for regression keeps all the benefits of Calibrated Explanations, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of feature importance, and allows both factual and counterfactual explanations. Calibrated Explanations for regression provides fast, reliable, stable, and robust explanations. Calibrated Explanations for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model, allowing dynamic selection of thresholds. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available on GitHub and for installation using both pip and conda, making the results in this paper easily replicable.},
  archive      = {J_ML},
  author       = {Löfström, Tuwe and Löfström, Helena and Johansson, Ulf and Sönströd, Cecilia and Matela, Rudy},
  doi          = {10.1007/s10994-024-06642-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Calibrated explanations for regression},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforced logical reasoning over KGs for interpretable
recommendation system. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06646-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various domains, traditional recommendation systems have demonstrated significant benefits. However, their &quot;black box&quot; mechanisms have led to a crisis of trust among users. Interpretable recommendation systems have emerged as a solution by providing explanations for recommended items, thus enhancing transparency and user confidence. Another challenge to interpretable recommendation systems is data sparsity, which causes subpar recommendation performance. Addressing the challenges of model interpretability and data sparsity, this paper introduces the Knowledge Graphs-based Logic Reasoning Recommendation (KG-LRR) method, structured around an &quot;encoder-decoder&quot; architecture. The KG-LRR method tackles these issues by leveraging a knowledge graph for items to enhance the representation of users and items during the encoding process. It introduces a propositional logic reasoning model for decoding, rendering explanations in a more comprehensible manner. This dual approach ensures a balance between the recommendation system’s efficiency and interpretability. The KG-LRR method employs a neural network to simulate human-like propositional logical reasoning. This not only mitigates data sparsity issues but also explicates users’ interest in items. It provides deeper insights into users’ preferences and delivers robust interpretability. Experimental results across three public datasets-Yelp2018, Amazon-book, and Amazon-electronics-demonstrate that the KG-LRR model outperforms existing methods in terms of Recall and nDCG in top-k ranking recommendation scenarios. This validates its superior performance compared to prevailing interpretable recommendation techniques. In summary, the KG-LRR method offers a novel approach to enhance transparency and performance through an innovative &quot;encoder-decoder&quot; architecture. Its integration of knowledge graphs and propositional logic reasoning showcases promising outcomes in addressing current challenges within interpretable recommendation systems. Our code is available at https://github.com/siri-ya/KG-LRR .},
  archive      = {J_ML},
  author       = {Wang, Shirui and Xie, Bohan and Ding, Ling and Chen, Jianting and Xiang, Yang},
  doi          = {10.1007/s10994-024-06646-4},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Reinforced logical reasoning over KGs for interpretable recommendation system},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified convergence analysis for adaptive optimization with
moving average estimator. <em>ML</em>, <em>114</em>(4), 1–51. (<a
href="https://doi.org/10.1007/s10994-024-06650-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adaptive optimization algorithms have been successful in many applications, there are still some mysteries in terms of convergence analysis that have not been unraveled. This paper provides a novel non-convex analysis of adaptive optimization to uncover some of these mysteries. Our contributions are three-fold. First, we show that an increasing or large enough momentum parameter for the first-order moment used in practice is sufficient to ensure the convergence of adaptive algorithms whose adaptive scaling factors of the step size are bounded. Second, our analysis gives insights for practical implementations, e.g., increasing the momentum parameter in a stage-wise manner in accordance with stagewise decreasing step size would help improve the convergence. Third, the modular nature of our analysis allows its extension to solving other optimization problems, e.g., compositional, min–max and bilevel problems. As an interesting yet non-trivial use case, we present algorithms for solving non-convex min–max optimization and bilevel optimization that do not require using large batches of data to estimate gradients or double loops as the literature do. Our empirical studies corroborate our theoretical results.},
  archive      = {J_ML},
  author       = {Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  doi          = {10.1007/s10994-024-06650-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-51},
  shortjournal = {Mach. Learn.},
  title        = {Unified convergence analysis for adaptive optimization with moving average estimator},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the usefulness of the fit-on-test view on evaluating
calibration of classifiers. <em>ML</em>, <em>114</em>(4), 1–75. (<a
href="https://doi.org/10.1007/s10994-024-06652-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibrated uncertainty estimates are essential for classifiers used in safety-critical applications. If a classifier is uncalibrated, then there is a unique way to calibrate its uncertainty using the idealistic true calibration map corresponding to this classifier. Although the true calibration map is typically unknown in practice, it can be estimated with many post-hoc calibration methods which fit some family of potential calibration functions on a validation dataset. This paper examines the connection between such post-hoc calibration methods and calibration evaluation. Despite the negative connotations of fitting on test data in machine learning, we claim that fitting calibration maps on test data as part of the calibration evaluation process is a method worth considering, and we refer to this view as fit-on-test. This view enables the usage of any post-hoc calibration method as an evaluation measure, unlocking missed opportunities in development of evaluation methods. We prove that even ECE, which is the most common calibration evaluation method, is actually a fit-on-test measure. This observation leads us to a new method of tuning the number of bins in ECE with cross-validation. Fitting on test data can lead to test-time overfitting, and therefore, we discuss the limitations and concerns with the fit-on-test view. Our contributions also include: (1) enhancement of reliability diagrams with diagonal filling; (2) development of new calibration map families PL and PL3; and (3) an experimental study of which families perform strongly both as post-hoc calibrators and calibration evaluators.},
  archive      = {J_ML},
  author       = {Kängsepp, Markus and Valk, Kaspar and Kull, Meelis},
  doi          = {10.1007/s10994-024-06652-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-75},
  shortjournal = {Mach. Learn.},
  title        = {On the usefulness of the fit-on-test view on evaluating calibration of classifiers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum causal entropy inverse constrained reinforcement
learning. <em>ML</em>, <em>114</em>(4), 1–44. (<a
href="https://doi.org/10.1007/s10994-024-06653-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements specific to that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide a practical implementation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and environments, and it is able to handle problems with stochastic dynamics and a continuous state-action space.},
  archive      = {J_ML},
  author       = {Baert, Mattijs and Mazzaglia, Pietro and Leroux, Sam and Simoens, Pieter},
  doi          = {10.1007/s10994-024-06653-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-44},
  shortjournal = {Mach. Learn.},
  title        = {Maximum causal entropy inverse constrained reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A prompt-driven framework for multi-domain knowledge
tracing. <em>ML</em>, <em>114</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10994-024-06660-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) models students’ knowledge states to predict future performance based on historical interactions. Due to data privacy concerns and budget constraints, the availability of high-quality student data differs across domains and it is essential to effectively utilize KT data from multiple domains. In this work, we propose a novel prompt-enhanced paradigm, i.e., promptKT, to utilize student data from multiple domains to improve KT performance simultaneously. Specifically, a unified Transformer based backbone model is first pre-trained using data from all the KT domains to capture the commonality across domains. Then, we design a novel soft domain prompt module to capture the distinctions among various domains and users. Our promptKT is evaluated on six public real-world educational datasets. The results demonstrate that our approach outperforms the majority of existing KT models in terms of AUC and accuracy. Furthermore, empirical analysis shows the decent transferability and adaptation of promptKT across multiple KT domains. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit .},
  archive      = {J_ML},
  author       = {Liu, Zitao and Huang, Shuyan and Guo, Teng and Hou, Mingliang and Liang, Qianru},
  doi          = {10.1007/s10994-024-06660-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A prompt-driven framework for multi-domain knowledge tracing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capturing the context-aware code change via dynamic control
flow graph for commit message generation. <em>ML</em>, <em>114</em>(4),
1–23. (<a href="https://doi.org/10.1007/s10994-024-06671-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commit messages that summarize code changes of each commit in natural language help developers understand code changes without digging into implementation details, thus playing an essential role in comprehending software evolution. In constructing models for automatic commit message generation, prior research has focused on extracting information from the changed code hunks (i.e., code difference), while ignoring the unchanged code hunks (i.e., code context). However, most studies often neglect the fact that the code change is context-aware, that is the semantics of the code difference are heavily dependent on its code context. To take the code context into account, a key challenge arises: the extensive code context may overshadow the minuscule code difference in capturing the changed semantics, which is a disadvantage to commit message generation. In this paper, we propose the dynamic control flow graph (DCFG), which combines both the code contexts and code differences into one dynamic global–local structure. Based on DCFG, we design a novel framework termed capturing the context-aware code change for commit message generation ( $${\text {C}^4\text {MG}}$$ ), which attempts to model the changed semantics of the code change based on the relevant code context, while avoiding being misled by the overwhelming amount of unchanged code context. Extensive experiments demonstrate that benefiting from modeling the context-aware code change, $${\text {C}^4\text {MG}}$$ outperforms not only the state-of-the-art open-source models but also the large language models (e.g., LLaMA3, GPT-4o, and Gemini) on the commit message generation.},
  archive      = {J_ML},
  author       = {Du, Yali and Li, Ying and Ma, Yi-Fan and Li, Ming},
  doi          = {10.1007/s10994-024-06671-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Capturing the context-aware code change via dynamic control flow graph for commit message generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nettop: A light-weight network of orthogonal-plane features
for image recognition. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06672-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current light-weight CNN-based networks, convolutional operators are principally utilized to extract feature maps for image representation. However, such conventional operation can lead to lack of informative patterns for the learning process. It is because the operators have just been allocated to convolute on the spatial side of an input tensor. To deal with this deficiency, we propose a competent model to efficiently exploit the full-side features of a tensor. The proposed model is based on three novel concepts as follows. i) A novel grouped-convolutional operator is defined to produce complementary features in consideration of three plane-based volumes that have been correspondingly partitioned subject to three orthogonal planes (TOP) of a given tensor. ii) An effective perceptron block is introduced to take into account the TOP-based operator for orthogonal-plane feature extraction. iii) A light-weight backbone of TOP-based blocks (named NetTOP) is proposed to take advantage of the full-side informative patterns for image representation. Experimental results for image recognition on benchmark datasets have proved the prominent performance of the proposals. The code of NetTOP is available at https://github.com/nttbdrk25/NetTOP .},
  archive      = {J_ML},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10994-024-06672-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Nettop: A light-weight network of orthogonal-plane features for image recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HorNets: Learning from discrete and continuous signals with
routing neural networks. <em>ML</em>, <em>114</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s10994-024-06673-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction of neural network architectures suitable for learning from both continuous and discrete tabular data is challenging, as contemporary high-dimensional tabular data sets are often characterized by a relatively small set of instances and the request for efficient learning. We propose HorNets (Horn Networks), a neural network architecture with state-of-the-art performance on synthetic and real-life data sets from scarce-data tabular domains. HorNets are based on a clipped polynomial-like activation function, extended by a custom discrete-continuous routing mechanism that decides which part of the neural network to optimize based on the input’s cardinality. By explicitly modeling parts of the feature combination space or combining whole space in a linear attention-like manner, HorNets dynamically decide which mode of operation is the most suitable for a given piece of data with no explicit supervision. This architecture is one of the few approaches that reliably retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art classification performance on 14 real-life biomedical high-dimensional data sets. HorNets are made freely available under a permissive license alongside a synthetic generator of categorical benchmarks.},
  archive      = {J_ML},
  author       = {Koloski, Boshko and Lavrač, Nada and Škrlj, Blaž},
  doi          = {10.1007/s10994-024-06673-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {HorNets: Learning from discrete and continuous signals with routing neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TCR: Topologically consistent reweighting for XGBoost in
regression tasks. <em>ML</em>, <em>114</em>(4), 1–52. (<a
href="https://doi.org/10.1007/s10994-024-06704-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient boosted tree ensembles (GBTEs) such as XGBoost continue to outperform other machine learning models on tabular data. However, the plethora of adjustable hyperparameters can exacerbate optimisation, especially in regression tasks with no intuitive performance measures such as accuracy and confidence. Automated machine learning frameworks alleviate the hyperparameter search for users, but if the optimisation procedure ends prematurely due to resource constraints, it is questionable whether users receive good models. To tackle this problem, we introduce a cost-efficient method to retrofit previously optimised XGBoost models by retraining them with a new weight distribution over the training instances. We base our approach on topological results, which allows us to infer model-agnostic weights for specific regions of the data distribution where the targets are more susceptible to input perturbations. By linking our theory to the training procedure of XGBoost regressors, we then establish a topologically consistent reweighting scheme, which is independent of the specific model instance. Empirically, we verify that our approach improves prediction performance, outperforms other reweighting methods and is much faster than a hyperparameter search. To enable users to find the optimal weights for their data, we provide guides based on our findings on 20 datasets. Our code is available at: https://github.com/montymaxzuehlke/tcr .},
  archive      = {J_ML},
  author       = {Zühlke, Monty-Maximilian and Kudenko, Daniel},
  doi          = {10.1007/s10994-024-06704-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {TCR: Topologically consistent reweighting for XGBoost in regression tasks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intramodal consistency in triplet-based cross-modal learning
for image retrieval. <em>ML</em>, <em>114</em>(4), 1–29. (<a
href="https://doi.org/10.1007/s10994-024-06710-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval requires building a common latent space that captures and correlates information from different data modalities, usually images and texts. Cross-modal training based on the triplet loss with hard negative mining is a state-of-the-art technique to address this problem. This paper shows that such approach is not always effective in handling intra-modal similarities. Specifically, we found that this method can lead to inconsistent similarity orderings in the latent space, where intra-modal pairs with unknown ground-truth similarity are ranked higher than cross-modal pairs representing the same concept. To address this problem, we propose two novel loss functions that leverage intra-modal similarity constraints available in a training triplet but not used by the original formulation. Additionally, this paper explores the application of this framework to unsupervised image retrieval problems, where cross-modal training can provide the supervisory signals that are otherwise missing in the absence of category labels. Up to our knowledge, we are the first to evaluate cross-modal training for intra-modal retrieval without labels. We present comprehensive experiments on MS-COCO and Flickr30k, demonstrating the advantages and limitations of the proposed methods in cross-modal and intra-modal retrieval tasks in terms of performance and novelty measures. We also conduct a case study on the ROCO dataset to assess the performance of our method on medical images and present an ablation study on one of our approaches to understanding the impact of the different components of the proposed loss function. Our code is publicly available on GitHub https://github.com/MariodotR/FullHN.git .},
  archive      = {J_ML},
  author       = {Mallea, Mario and Ñanculef, Ricardo and Araya, Mauricio},
  doi          = {10.1007/s10994-024-06710-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Intramodal consistency in triplet-based cross-modal learning for image retrieval},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain generalization via content factors isolation: A
two-level latent variable modeling approach. <em>ML</em>,
<em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06717-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of domain generalization is to develop models that exhibit a higher degree of generality, meaning they perform better when evaluated on data coming from previously unseen distributions. Models obtained via traditional methods often cannot distinguish between label-specific and domain-related features in the latent space. To confront this difficulty, we propose formulating a novel data generation process using a latent variable model and postulating a partition of the latent space into content and style parts while allowing for statistical dependency to exist between them. In this model, the distribution of content factors associated with observations belonging to the same class depends on only the label corresponding to that class. In contrast, the distribution of style factors has an additional dependency on the domain variable. We derive constraints that suffice to recover the collection of content factors block-wise and the collection of style factors component-wise while guaranteeing the isolation of content factors. This allows us to produce a stable predictor solely relying on the latent content factors. Building upon these theoretical insights, we propose a practical and efficient algorithm for determining the latent variables under the variational auto-encoder framework. Our simulations with dependent latent variables produce results consistent with our theory, and real-world experiments show that our method outperforms the competitors.},
  archive      = {J_ML},
  author       = {Gao, Erdun and Bondell, Howard and Huang, Shaoli and Gong, Mingming},
  doi          = {10.1007/s10994-024-06717-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Domain generalization via content factors isolation: A two-level latent variable modeling approach},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing exchangeability in the batch mode with e-values and
markov alternatives. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06720-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of this paper is testing the assumption of exchangeability, which is the standard assumption in mainstream machine learning. The common approaches are online testing by betting (such as conformal testing) and the older batch testing using p-values (as in classical hypothesis testing). The approach of this paper is intermediate in that we are interested in batch testing by betting; as a result, p-values are replaced by e-values. As a first step in this direction, this paper concentrates on the Markov model as alternative. The null hypothesis of exchangeability is formalized as a Kolmogorov-type compression model, and the Bayes mixture of the Markov model w.r. to the uniform prior is taken as simple alternative hypothesis. Using e-values instead of p-values leads to a computationally efficient testing procedure. Two appendixes discuss connections with the algorithmic theory of randomness; in particular, the test proposed in this paper can be interpreted as a poor man’s version of Kolmogorov’s deficiency of randomness.},
  archive      = {J_ML},
  author       = {Vovk, Vladimir},
  doi          = {10.1007/s10994-024-06720-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Testing exchangeability in the batch mode with e-values and markov alternatives},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel density estimation for multiclass quantification.
<em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-024-06726-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. Quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. The distribution-matching (DM) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. Current DM approaches model the involved populations using histograms of posterior probabilities. In this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. We propose a new representation mechanism based on multivariate densities that we model via kernel density estimation (KDE). The experiments we have carried out show our method, dubbed KDEy, yields superior quantification performance compared to previous DM approaches and other state-of-the-art quantification systems.},
  archive      = {J_ML},
  author       = {Moreo, Alejandro and González, Pablo and del Coz, Juan José},
  doi          = {10.1007/s10994-024-06726-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Kernel density estimation for multiclass quantification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical risk minimization in the interpolating regime with
application to neural network learning. <em>ML</em>, <em>114</em>(4),
1–52. (<a href="https://doi.org/10.1007/s10994-025-06738-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common strategy to train deep neural networks (DNNs) is to use very large architectures and to train them until they (almost) achieve zero training error. Empirically observed good generalization performance on test data, even in the presence of lots of label noise, corroborate such a procedure. On the other hand, in statistical learning theory it is known that over-fitting models may lead to poor generalization properties, occurring in e.g. empirical risk minimization (ERM) over too large hypotheses classes. Inspired by this contradictory behavior, so-called interpolation methods have recently received much attention, leading to consistent and optimally learning methods for, e.g., some local averaging schemes with zero training error. We extend this analysis to ERM-like methods for least squares regression and show that for certain, large hypotheses classes called inflated histograms, some interpolating empirical risk minimizers enjoy very good statistical guarantees while others fail in the worst sense. Moreover, we show that the same phenomenon occurs for DNNs with zero training error and sufficiently large architectures.},
  archive      = {J_ML},
  author       = {Mücke, Nicole and Steinwart, Ingo},
  doi          = {10.1007/s10994-025-06738-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {Empirical risk minimization in the interpolating regime with application to neural network learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression and restoration: Exploring elasticity in
continual test-time adaptation. <em>ML</em>, <em>114</em>(4), 1–32. (<a
href="https://doi.org/10.1007/s10994-025-06739-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation is a task that a pre-trained source model is updated during inference with given test data from target domains with different distributions. However, frequent updates in a long time without resetting the model will bring two main problems, i.e., error accumulation and catastrophic forgetting. Although some recent methods have alleviated the problems by designing new loss functions or update strategies, they are still very fragile to hyperparameters or suffer from storage burden. Besides, most methods treat each target domain equally, neglecting the characteristics of each target domain and the situation of the current model, which will mislead the update direction of the model. To address the above issues, we first leverage the mean cosine similarity per test batch between the features output by the source and updated models to measure the change of target domains. Then we summarize the elasticity of the mean cosine similarity to guide the model to update and restore adaptively. Motivated by this, we propose a frustratingly simple yet efficient method called Elastic-Test-time ENTropy Minimization (E-TENT) to dynamically adjust the mean cosine similarity based on the built relationship between it and the momentum coefficient. Combined with the extra three minimal improvements, E-TENT exhibits significant performance gains and strong robustness on CIFAR10-C, CIFAR100-C and ImageNet-C along with various practical scenarios.},
  archive      = {J_ML},
  author       = {Li, Jingwei and Liu, Chengbao and Bai, Xiwei and Tan, Jie and Chu, Jiaqi and Wang, Yudong},
  doi          = {10.1007/s10994-025-06739-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Compression and restoration: Exploring elasticity in continual test-time adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep errors-in-variables using a diffusion model.
<em>ML</em>, <em>114</em>(4), 1–25. (<a
href="https://doi.org/10.1007/s10994-025-06744-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Errors-in-Variables is the statistical concept used to explicitly model input variable errors caused, for example, by noise. While it has long been known in statistics that not accounting for such errors can produce a substantial bias, the vast majority of deep learning models have thus far neglected Errors-in-Variables approaches. Reasons for this include a significant increase of the numerical burden and the challenge in assigning an appropriate prior in a Bayesian treatment. To date, the attempts made to use Errors-in-Variables for neural networks do not scale to deep networks or are too simplistic to enhance the prediction performance. This work shows for the first time how Bayesian deep Errors-in-Variables models can increase the prediction performance. We present a scalable variational inference scheme for Bayesian Errors-in-Variables and demonstrate a significant increase in prediction performance for the case of image classification. Concretely, we use a diffusion model as input posterior to obtain a distribution over the denoised image data. We also observe that training the diffusion model on an unnoisy surrogate dataset can suffice to achieve an improved prediction performance on noisy data.},
  archive      = {J_ML},
  author       = {Faller, Josua and Martin, Jörg and Elster, Clemens},
  doi          = {10.1007/s10994-025-06744-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Deep errors-in-variables using a diffusion model},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover the balanced geometry in long-tailed contrastive
language-image pretraining. <em>ML</em>, <em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-025-06745-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Contrastive Language-Image Pretraining (CLIP) has become the de facto standard for vision-language pretraining tasks, the exploration on the inherent long-tailed pretraining data distribution remains limited. From a neural collapse perspective, we show in principle that the vanilla CLIP training can be vulnerable to the long-tailed distributions, which might distort the representations with reduced inter-class separation and poor discriminative ability. To combat this issue, we propose an improved method, termed as Geometry-Balanced CLIP (GeoCLIP), which automatically constructs pseudo clusters and aligns them with a predefined equiangular geometric structure, thereby enjoying the theoretical merits of better maintaining the uniformity at the semantic level. Furthermore, we enhance GeoCLIP’s generality for real-world complex distributions by incorporating harmonized clusters that integrate both empirically observed data structures and theoretically optimal geometry. Extensive experiments across various benchmarks demonstrate the consistent superiority of GeoCLIP in achieving robust and transferable representation under long-tailed distributions. The source code will be publicly available.},
  archive      = {J_ML},
  author       = {Zhou, Zhihan and Ye, Yushi and Hong, Feng and Zhao, Peisen and Yao, Jiangchao and Zhang, Ya and Tian, Qi and Wang, Yanfeng},
  doi          = {10.1007/s10994-025-06745-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Uncover the balanced geometry in long-tailed contrastive language-image pretraining},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning with pre-trained conditional generative
models. <em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-025-06748-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) Source and target task label spaces overlap, (ii) Source datasets are available, and (iii) Target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a target architecture with an artificial dataset synthesized by using conditional source generative models. P-SSL applies SSL algorithms to labeled target data and unlabeled pseudo samples, which are generated by cascading the source classifier and generative models to condition them with target samples. Our experimental results indicate that our method can outperform the baselines of scratch training and knowledge distillation.},
  archive      = {J_ML},
  author       = {Yamaguchi, Shin’ya and Kanai, Sekitoshi and Kumagai, Atsutoshi and Chijiwa, Daiki and Kashima, Hisashi},
  doi          = {10.1007/s10994-025-06748-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Transfer learning with pre-trained conditional generative models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model for intelligible interaction between agents that
predict and explain. <em>ML</em>, <em>114</em>(4), 1–40. (<a
href="https://doi.org/10.1007/s10994-025-06750-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has emerged as a powerful form of data modelling with widespread applicability beyond its roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. In this paper we view interaction between humans and ML systems within the broader context of communication between agents capable of prediction and explanation. We formalise the interaction model by taking agents to be automata with some special characteristics and define a protocol for communication between such agents. We define One- and Two-Way Intelligibility as properties that emerge at run-time by execution of the protocol. The formalisation allows us to identify conditions under which run-time sequences are bounded, and identify conditions under which the protocol can correctly implement an axiomatic specification of intelligible interaction between a human and an ML system. We also demonstrate using the formal model to: (a) identify instances of One- and Two-Way Intelligibility in literature reports on humans interacting with ML systems providing logic-based explanations, as is done in Inductive Logic Programming (ILP); and (b) map interactions between humans and machines in an elaborate natural-language based dialogue-model to One- or Two-Way Intelligible interactions in the formal model.},
  archive      = {J_ML},
  author       = {Baskar, A. and Srinivasan, Ashwin and Bain, Michael and Coiera, Enrico},
  doi          = {10.1007/s10994-025-06750-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {A model for intelligible interaction between agents that predict and explain},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A contrastive neural disentanglement approach for query
performance prediction. <em>ML</em>, <em>114</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10994-025-06752-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach, referred to as contrastive disentangled representation for query performance prediction (CoDiR-QPP), to estimate search query performance by disentangling query content semantics from query difficulty. Our proposed approach leverages neural disentanglement to isolate the information need expressed in search queries from the complexities that affect retrieval performance. Motivated by empirical observations that varying query formulations for the same information need can significantly impact retrieval outcomes, we hypothesize that separating content semantics from query difficulty can enhance query performance prediction. Utilizing contrastive learning, CoDiR-QPP distinguishes between well-performing and poorly performing query variants, facilitating the estimation of a given query’s performance. Our extensive experiments on four standard benchmark datasets demonstrate that CoDiR-QPP outperforms state-of-the-art baselines in predicting query performance, offering improved semantic similarity computation and higher correlation metrics such as Kendall $$\tau$$ , Spearman $$\rho$$ , and scaled Mean Absolute Ranking Error (sMARE).},
  archive      = {J_ML},
  author       = {Salamat, Sara and Arabzadeh, Negar and Seyedsalehi, Shirin and Bigdeli, Amin and Zihayat, Morteza and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-025-06752-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A contrastive neural disentanglement approach for query performance prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Likelihood-ratio-based confidence intervals for neural
networks. <em>ML</em>, <em>114</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10994-024-06639-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.},
  archive      = {J_ML},
  author       = {Sluijterman, Laurens and Cator, Eric and Heskes, Tom},
  doi          = {10.1007/s10994-024-06639-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Likelihood-ratio-based confidence intervals for neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pairwise learning to rank by neural networks revisited:
Reconstruction, theoretical analysis and practical performance.
<em>ML</em>, <em>114</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10994-024-06644-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reevaluate the pairwise learning to rank approach based on neural nets, called RankNet, and present a theoretical analysis of its architecture. We show mathematically that the model can, under certain conditions, learn reflexive, antisymmetric, and transitive relations, enabling simplified training and improved performance. Experimental results on the LETOR MSLR-WEB10K, MQ2007 and MQ2008 datasets show that the model outperforms numerous state-of-the-art methods (including a listwise approach), while being inherently simpler in structure and using a pairwise approach only.},
  archive      = {J_ML},
  author       = {Köppel, Marius and Segner, Alexander and Wagener, Martin and Pensel, Lukas and Karwath, Andreas and Kramer, Stefan},
  doi          = {10.1007/s10994-024-06644-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Pairwise learning to rank by neural networks revisited: Reconstruction, theoretical analysis and practical performance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on self-supervised methods for visual
representation learning. <em>ML</em>, <em>114</em>(4), 1–56. (<a
href="https://doi.org/10.1007/s10994-024-06708-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.},
  archive      = {J_ML},
  author       = {Uelwer, Tobias and Robine, Jan and Wagner, Stefan Sylvius and Höftmann, Marc and Upschulte, Eric and Konietzny, Sebastian and Behrendt, Maike and Harmeling, Stefan},
  doi          = {10.1007/s10994-024-06708-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-56},
  shortjournal = {Mach. Learn.},
  title        = {A survey on self-supervised methods for visual representation learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring individual direct causal effects under
heterogeneous peer influence. <em>ML</em>, <em>114</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s10994-024-06729-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference is central to understanding the effectiveness of policies and designing personalized interventions. Causal inference involves estimating the causal effects of treatments on outcomes of interest after modeling appropriate assumptions. Most causal inference approaches assume that a unit’s outcome is independent of the treatments or outcomes of other units. However, this assumption is unrealistic when inferring causal effects in networks where a unit’s outcome can be influenced by the treatments and outcomes of its neighboring nodes, a phenomenon known as interference. Causal inference in networks should explicitly account for interference. In interference settings, the direct causal effect measures the impact of the unit’s own treatment while controlling for the treatments of peers. Existing solutions to estimating direct causal effects under interference consider either homogeneous influence from peers or specific heterogeneous influence mechanisms (e.g., based on local neighborhood structure). In this work, we define heterogeneous peer influence (HPI) as the general interference that occurs when a unit’s outcome may be influenced differently by different peers based on their attributes and relationships, or when each network node may have a different susceptibility to peer influence. This paper presents IDE-Net, a framework for estimating individual, i.e., unit-level, direct causal effects in the presence of HPI where the mechanism of influence is not known a priori. We first propose a structural causal model for networks that can capture different possible assumptions about network structure, interference conditions, and causal dependence and that enables reasoning about causal effect identifiability and discovery of potential heterogeneous contexts. We then propose a novel graph neural network-based estimator to estimate individual direct causal effects. We show empirically that state-of-the-art methods for individual direct effect estimation produce biased results in the presence of HPI, and that our proposed estimator is robust.},
  archive      = {J_ML},
  author       = {Adhikari, Shishir and Zheleva, Elena},
  doi          = {10.1007/s10994-024-06729-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Inferring individual direct causal effects under heterogeneous peer influence},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end explainability framework for spatio-temporal
predictive modeling. <em>ML</em>, <em>114</em>(4), 1–47. (<a
href="https://doi.org/10.1007/s10994-024-06733-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising adoption of AI models in real-world applications characterized by sensor data creates an urgent need for inference explanation mechanisms to support domain experts in making informed decisions. Explainable AI (XAI) opens up a new opportunity to extend black-box deep learning models with such inference explanation capabilities. However, existing XAI approaches for tabular, image, and graph data are ineffective in contexts with spatio-temporal data. In this paper, we fill this gap by proposing a XAI method specifically tailored for spatio-temporal data in sensor networks, where observations are collected at regular time intervals and at different locations. Our model-agnostic masking meta-optimization method for deep learning models uncovers global salient factors influencing model predictions, and generates explanations taking into account multiple analytical views, such as features, timesteps, and node locations. Our qualitative and quantitative experiments with real-world forecasting datasets show that our approach effectively extracts explanations of model predictions, and is competitive with state-of-the-art approaches.},
  archive      = {J_ML},
  author       = {Altieri, Massimiliano and Ceci, Michelangelo and Corizzo, Roberto},
  doi          = {10.1007/s10994-024-06733-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-47},
  shortjournal = {Mach. Learn.},
  title        = {An end-to-end explainability framework for spatio-temporal predictive modeling},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient descent fails to learn high-frequency functions and
modular arithmetic. <em>ML</em>, <em>114</em>(4), 1–30. (<a
href="https://doi.org/10.1007/s10994-025-06747-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function. A set of functions of the form $$x\rightarrow ax \bmod p$$ , where a is taken from $${{\mathbb {Z}}}_p$$ , has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of p-periodic functions on $${{\mathbb {Z}}}$$ and is tightly connected with a class of high-frequency periodic functions on the real line. We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base p is large. This in turn prevents such a learning algorithm from being successful.},
  archive      = {J_ML},
  author       = {Takhanov, Rustem and Tezekbayev, Maxat and Pak, Artur and Bolatov, Arman and Assylbekov, Zhenisbek},
  doi          = {10.1007/s10994-025-06747-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Gradient descent fails to learn high-frequency functions and modular arithmetic},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized median of means principle for bayesian
inference. <em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-025-06754-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of robustness is experiencing a resurgence of interest in the statistical and machine learning communities. In particular, robust algorithms making use of the so-called median of means estimator were shown to satisfy strong performance guarantees for many problems, including estimation of the mean, covariance structure as well as linear regression. In this work, we propose an extension of the median of means principle to the Bayesian framework, leading to the notion of the robust posterior distribution. In particular, we (a) quantify robustness of this posterior to outliers, (b) show that it satisfies a version of the Bernstein-von Mises theorem that connects Bayesian credible sets to the traditional confidence intervals, and (c) demonstrate that our approach performs well in applications.},
  archive      = {J_ML},
  author       = {Minsker, Stanislav and Yao, Shunan},
  doi          = {10.1007/s10994-025-06754-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Generalized median of means principle for bayesian inference},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
