<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac---29">SAC - 29</h2>
<ul>
<li><details>
<summary>
(2025). Penalized empirical likelihood estimation and EM algorithms
for closed-population capture–recapture models. <em>SAC</em>,
<em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10557-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capture–recapture experiments are widely used to estimate the abundance of a finite population. Based on capture–recapture data, the empirical likelihood (EL) method has been shown to outperform the conventional conditional likelihood (CL) method. However, the current literature on EL abundance estimation ignores behavioral effects, and the EL estimates may not be stable, especially when the capture probability is low. We make three contributions in this paper. First, we extend the EL method to capture–recapture models that account for behavioral effects. Second, to overcome the instability of the EL method, we propose a penalized EL (PEL) estimation method that penalizes large abundance values. We then investigate the asymptotics of the maximum PEL estimator and the PEL ratio statistic. Third, we develop standard expectation–maximization (EM) algorithms for PEL to improve its practical performance. The EM algorithm is also applicable to EL and CL with slight modifications. Our simulation and a real-world data analysis demonstrate that the PEL method successfully overcomes the instability of the EL method and the proposed EM algorithm produces more reliable results than existing optimization algorithms.},
  archive      = {J_SAC},
  author       = {Liu, Yang and Li, Pengfei and Liu, Yukun},
  doi          = {10.1007/s11222-024-10557-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Penalized empirical likelihood estimation and EM algorithms for closed-population capture–recapture models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal distributed subsampling under heterogeneity.
<em>SAC</em>, <em>35</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10558-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed subsampling approaches have been proposed to process massive data in a distributed computing environment, where subsamples are taken from each site and then analyzed collectively to address statistical problems when the full data is not available. In this paper, we consider that each site involves a common parameter and site-specific nuisance parameters and then formulate a unified framework of optimal distributed subsampling under heterogeneity for general optimization problems with convex loss functions that could be nonsmooth. By establishing the consistency and asymptotic normality of the distributed subsample estimators for the common parameter of interest, we derive the optimal subsampling probabilities and allocation sizes under the A- and L-optimality criteria. A two-step algorithm is proposed for practical implementation and the asymptotic properties of the resultant estimator are established. For nonsmooth loss functions, an alternating direction method of multipliers method and a random perturbation procedure are proposed to obtain the subsample estimator and estimate the covariance matrices for statistical inference, respectively. The finite-sample performance of linear regression, logistic regression and quantile regression models is demonstrated through simulation studies and an application to the National Longitudinal Survey of Youth Dataset is also provided.},
  archive      = {J_SAC},
  author       = {Shao, Yujing and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-024-10558-7},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Optimal distributed subsampling under heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCA-uCPD: An ensemble method for multiple change-point
detection in moderately high-dimensional data. <em>SAC</em>,
<em>35</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10553-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-point detection (CPD) receives extensive studies due to its wide applications in various fields. However, CPD remains a challenging problem for complex data with medium or high dimensions, high correlations, outliers or heavy-tailed distribution. This article proposes an integrated change-point detection method called PCA-uCPD, which utilizes principal components analysis (PCA) to project the original data series into uncorrelated principal components (PCs). Subsequently, we apply existing univariate change-point detection methods to the mapped PCs, followed by a proposed refining technique to obtain the ultimate change-point estimates for the original data sequences. The proposed method admits a flexible architecture that is thus capable of dealing with complex data. Theoretical justifications have been provided to guarantee the feasibility of the proposed methods. Moreover, we conduct simulations to assess performance across various data-generating scenarios. The efficacy of PCA-uCPD is further demonstrated through applications in both genetic and financial datasets.},
  archive      = {J_SAC},
  author       = {Qin, Shanshan and Tan, Zhenni and Wei, Weidong and Wu, Yuehua},
  doi          = {10.1007/s11222-024-10553-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power enhancing probability subsampling using side
information. <em>SAC</em>, <em>35</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10556-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the subsampling technique for hypothesis testing in generalized linear models with large-scale datasets, focusing on testing simple null hypotheses against composite linear alternatives. We propose a subsample-based test statistic and show that it converges to non-central chi-square distributions under Pitman’s local alternatives. The optimal subsampling distribution that maximizes power requires iterative calculations on the full data, which is computationally infeasible. Furthermore, it depends on the true parameter, which cannot be consistently estimated under Pitman’s local alternatives. We maximize a lower bound of the non-central parameter to define the power enhancing probability and utilize side information under the alternative to replace the true parameter. Extensive simulations and an application to a real dataset on flight delays and cancellations show that the proposed method offers a computationally viable solution for hypothesis testing in the realm of big data.},
  archive      = {J_SAC},
  author       = {Gao, Junzhuo and Wang, Lei and Wang, Haiying},
  doi          = {10.1007/s11222-024-10556-9},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Power enhancing probability subsampling using side information},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference issue in multiscale geographically and temporally
weighted regression. <em>SAC</em>, <em>35</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-024-10559-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically and temporally weighted regression (GTWR) models assume that all of the varying coefficients operate at a same spatiotemporal scale, which reduces the flexibility of local regression models. Multiscale geographically and temporally weighted regression (MGTWR) models increase flexibility, enhance interpretability, and provide more comprehensive information by allowing regression coefficients to vary across different spatiotemporal scales. However, a limitation of the MGTWR models is that, to date, statistical inference regarding the local coefficient estimates has not been feasible. Formally, “hat matrix”, which projects the observed response variable vector onto the fitting response variable, was not available in the MGTWR model. This paper settles this limitation by reformulating the GTWR model into a Generalized Additive Model, extending this framework to the MGTWR model and then deriving standard deviations for the local coefficient estimates in the MGTWR model. In addition, we also obtain the number of effective parameters for the overall fit of the MGTWR model and for each covariate within the model. This statistic is crucial for comparing the goodness of fit between MGTWR, GTWR and classical global regression models, as well as for adjusting multiple tests. Simulation studies and a real-world example demonstrate these advances to the MGTWR framework.},
  archive      = {J_SAC},
  author       = {Hong, Zhimin and Wang, Ruoxuan and Wang, Zhiwen and Du, Wala},
  doi          = {10.1007/s11222-024-10559-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Inference issue in multiscale geographically and temporally weighted regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constrained least squares simplicial-simplicial regression.
<em>SAC</em>, <em>35</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10560-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model’s advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The strategy behind the formulation of the new model implemented is related to an existing methodology, that is of the same spirit, showcasing how other similar models can be employed as well. Finally, the performance of the proposed technique and its comparison to the existing methodology takes place using simulation studies and real data examples.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail},
  doi          = {10.1007/s11222-024-10560-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Constrained least squares simplicial-simplicial regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical investigations of boosting with pseudo-outcome
imputation for missing responses. <em>SAC</em>, <em>35</em>(2), 1–18.
(<a href="https://doi.org/10.1007/s11222-024-10561-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting techniques have gained increasing interest in both machine learning and statistical research. However, many of these methods are primarily designed for complete datasets, which limits their applicability to handle incomplete data such as missing observations. In this paper, we propose the pseudo-outcome strategy to account for missingness effects and describe a functional gradient descent algorithm. Numerical studies demonstrate the satisfactory performance of the proposed method in finite sample settings. Furthermore, we apply the proposed method to analyze the KLIPS Data.},
  archive      = {J_SAC},
  author       = {Bian, Yuan and Yi, Grace Y. and He, Wenqing},
  doi          = {10.1007/s11222-024-10561-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Empirical investigations of boosting with pseudo-outcome imputation for missing responses},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of the modality and flexibility of the inverse
stereographic normal distribution. <em>SAC</em>, <em>35</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s11222-025-10563-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular data arises in various fields including robotics, biology, geology and material sciences. Modelling such data requires flexible distribution families on the hypertorus. Common choices are the von Mises and the wrapped normal distributions. In this work we investigate the inverse stereographic normal distribution as an alternative distribution family both from a theoretical and applied perspective. We first generalize unimodality results to arbitrary dimensions by proving that the inverse stereographic normal distribution is unimodal if and only if all eigenvalues of the covariance matrix are less than or equal to 0.5. We then analyze the flexibility by fitting mixtures of shifted inverse stereographic normal distributions via gradient descent to several examples of toroidal data.},
  archive      = {J_SAC},
  author       = {Hinz, Florian B. and Mahmoud, Amr H. and Lill, Markus A.},
  doi          = {10.1007/s11222-025-10563-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {An analysis of the modality and flexibility of the inverse stereographic normal distribution},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic post-selection inference for regularized
graphical models. <em>SAC</em>, <em>35</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s11222-025-10564-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotically valid inference is obtained for graphical model edge parameters after selection using the same data set as the one used for inference. We consider Gaussian and (trans)elliptical graphical models, where the edge selection and consequent sparse estimation is operated by applying the $$\ell _1$$ , elastic net, smoothly clipped absolute deviation, or minimax concave penalty to an appropriately regular loss function. The polyhedral lemma is used to carry out conditional inference which is asymptotically valid in the (possibly wrong) selected graphical model. Simulation studies show how the method yields valid inference in a variety of finite-sample settings.},
  archive      = {J_SAC},
  author       = {Guglielmini, Sofia and Claeskens, Gerda},
  doi          = {10.1007/s11222-025-10564-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Asymptotic post-selection inference for regularized graphical models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient modeling of quasi-periodic data with seasonal
gaussian process. <em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10565-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-periodicity refers to a pattern in a function where it appears periodic at a certain frequency but exhibits evolving amplitudes over time. This is often the case in practical settings such as the modeling of case counts of infectious disease or the population dynamics of species over time. In this paper, we consider a class of Gaussian processes, called seasonal Gaussian Processes (sGP), for model-based inference of such quasi-periodic behavior. We illustrate that the exact sGP can be efficiently fitted using its state space representation for equally spaced time points. However, for large datasets with irregular spacing, the exact approach becomes computationally inefficient and unstable. To address this, we develop a continuous finite dimensional approximation for sGP using the seasonal B-spline (sB-spline) basis constructed by damping B-splines with sinusoidal functions. We prove the covariance convergence rate of the proposed approximation to the true sGP as the number of basis functions increases, and show its superior approximation quality through numerical studies. We also provide a unified and interpretable way to define priors for the sGP, based on the notion of predictive standard deviation. Finally, we implement the proposed inference method on several real data examples to illustrate its practical usage.},
  archive      = {J_SAC},
  author       = {Zhang, Ziang and Brown, Patrick and Stafford, Jamie},
  doi          = {10.1007/s11222-025-10565-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Efficient modeling of quasi-periodic data with seasonal gaussian process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy k-expectiles clustering. <em>SAC</em>, <em>35</em>(2),
1–19. (<a href="https://doi.org/10.1007/s11222-025-10566-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the Fuzzy K-expectiles clustering model is proposed. The model takes into account the asymmetry inherent in the data distribution, extending its applicability to a broader spectrum of data than the Fuzzy K-means. To achieve this, the Fuzzy K-expectiles clustering model introduces the cluster centroid expectile, and assigns data points based on expectile distances. An adaptive asymmetry parameter is specified for each variable and for each cluster The performance of the adaptive Fuzzy K-expectiles model is compared with other clustering models suggested in the literature. To show the performances of the proposed model three simulation studies and three applications to real datasets are presented.},
  archive      = {J_SAC},
  author       = {D’Urso, Pierpaolo and De Giovanni, Livia and Federico, Lorenzo and Vitale, Vincenzina},
  doi          = {10.1007/s11222-025-10566-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Fuzzy K-expectiles clustering},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geographically weighted quantile regression for count data.
<em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10568-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a methodological framework known as geographically weighted quantile regression (GWQR) has emerged for spatial data analysis. This framework offers the abilities to simultaneously explore spatial heterogeneity or nonstationarity in regression relationships and to estimate various conditional quantile functions. However, the current configuration of GWQR is limited to the analysis of continuous dependent variables. Discrete count data are observed in many disciplines. Whenever modeling such outcomes is necessary, the conventional GWQR approach is inadequate and fails to provide comprehensive insights into count data. To address this issue, this study aims to extend the GWQR framework originally designed for continuous dependent variables to accommodate count outcomes. We introduce an approach called geographically weighted count quantile regression (GWCQR), wherein the model specification is based on the smoothing of count responses through a jittering procedure. A semiparametric counterpart that allows for the inclusion of both spatially varying and invariant coefficients is also discussed. Finally, the proposed techniques are applied to a dataset of dengue fever in Taiwan as an empirical illustration.},
  archive      = {J_SAC},
  author       = {Chen, Vivian Yi-Ju and Wang, Shi-Ting},
  doi          = {10.1007/s11222-025-10568-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Geographically weighted quantile regression for count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic multidimensional smoothing using bayesian tensor
product p-splines. <em>SAC</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-025-10569-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a highly efficient fully Bayesian approach for anisotropic multidimensional smoothing. The main challenge in this context is the Markov chain Monte Carlo (MCMC) update of the smoothing parameters as their full conditional posterior comprises a pseudo-determinant that appears to be intractable at first sight. As a consequence, existing implementations are computationally feasible only for the estimation of two-dimensional tensor product smooths, which is, however, too restrictive for many applications. In this paper, we break this barrier and derive closed-form expressions for the log-pseudo-determinant and its first and second order partial derivatives. These expressions are valid for arbitrary dimension and very fast to evaluate, which allows us to set up an efficient MCMC sampler with derivative-based Metropolis–Hastings (MH) updates for the smoothing parameters. We derive simple formulas for low-dimensional slices and averages to facilitate visualization and investigate hyperprior sensitivity. We show that our new approach outperforms previous suggestions in the literature in terms of accuracy, scalability and computational cost and demonstrate its applicability through an illustrating temperature data example from spatio-temporal statistics.},
  archive      = {J_SAC},
  author       = {Bach, Paul and Klein, Nadja},
  doi          = {10.1007/s11222-025-10569-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Anisotropic multidimensional smoothing using bayesian tensor product P-splines},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logit unfolding choice models for binary data. <em>SAC</em>,
<em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10570-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete choice models with non-monotonic response functions are important in many areas of application, especially political sciences and marketing. This paper describes a novel unfolding model for binary data that allows for heavy-tailed shocks to the underlying utilities. One of our key contributions is a Markov chain Monte Carlo algorithm that requires little or no parameter tuning, fully explores the support of the posterior distribution, and can be used to fit various extensions of our core model that involve (Bayesian) hypothesis testing on the latent construct. Our empirical evaluations of the model and the associated algorithm suggest that they provide better complexity-adjusted fit to voting data from the United States House of Representatives.},
  archive      = {J_SAC},
  author       = {Lei, Rayleigh and Rodriguez, Abel},
  doi          = {10.1007/s11222-025-10570-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Logit unfolding choice models for binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle gibbs for likelihood-free inference of stochastic
volatility models. <em>SAC</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10571-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic volatility models (SVMs) are widely used in finance and econometrics for analyzing and interpreting volatility. Real financial data are often observed to have heavy tails, which violate a Gaussian assumption and may be better modeled using the stable distribution. However, the intractable density of the stable distribution hinders the use of common computational methods such as Markov chain Monte Carlo (MCMC) for parameter inference of SVMs. In this paper, we propose a new particle Gibbs sampler as a strategy to handle SVMs with intractable likelihoods in the approximate Bayesian computation (ABC) setting. The proposed sampler incorporates a conditional auxiliary particle filter, which can help mitigate the weight degeneracy often encountered when using ABC. Simulation studies demonstrate the efficacy of our sampler for inferring SVM parameters when compared to existing particle Gibbs samplers based on the conditional bootstrap filter, and for inferring both SVM and stable distribution parameters when compared to existing particle MCMC samplers. As a real data application, we apply the proposed sampler for fitting an SVM to S&amp;P 500 Index time-series data during the 2008–2009 financial crisis.},
  archive      = {J_SAC},
  author       = {Hou, Zhaoran and Wong, Samuel W. K.},
  doi          = {10.1007/s11222-025-10571-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Particle gibbs for likelihood-free inference of stochastic volatility models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time inference for smoothing quantile regression on
streaming datasets with heterogeneity detection. <em>SAC</em>,
<em>35</em>(2), 1–40. (<a
href="https://doi.org/10.1007/s11222-025-10572-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data is generated at high speed and in large quantities over time, and it calls for online learning to deal with it. In this paper, a new online updating method is established for smoothing quantile regression to make inferences in real-time. The renewable estimators are updated only by the current dataset and summary statistics of historical datasets. This method is adapted to the streaming datasets containing small samples. Theoretically, it is proved that renewable estimators have consistency and asymptotic normality and equivalence to pooled offline estimators based on all datasets. The dynamic bandwidth selection is applied to estimate the asymptotic covariance matrix in an online manner, which is theoretically highly asymptotically efficient. In particular, the renewable estimator provides asymptotic confidence intervals that are asymptotically smaller than those generated by existing methods, thereby improving the accuracy of interval estimation. Additionally, our approach addresses the common assumption of homogeneous models by accommodating non-parametric heterogeneity and detecting and removing anomalous data batches through an online screening process. Meanwhile, numerical simulations verify the theoretical results and outcomes on real datasets illustrate that our method is adapted to real streaming data situations.},
  archive      = {J_SAC},
  author       = {Wang, Yidan and Zhang, Lingyun and Gai, Yujie},
  doi          = {10.1007/s11222-025-10572-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-40},
  shortjournal = {Stat. Comput.},
  title        = {Real-time inference for smoothing quantile regression on streaming datasets with heterogeneity detection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploratory analysis of dynamic networks using latent
functions. <em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10573-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks, which capture the evolving interactions among entities, have flourished in various scientific fields. We propose a framework for exploratory analysis of these dynamic networks by representing them as latent functions. This framework comprises several visualization tools based on functional data analysis, specifically tailored for addressing typical tasks such as community detection, central node identification, and change point discovery. Besides, we develop a computationally efficient algorithm to obtain the latent functions. Through comprehensive simulation studies conducted under commonly investigated settings, we demonstrate the effectiveness of these tools. Furthermore, we apply the proposed tools to three representative and intriguing real-world networks, yielding enlightening discoveries. An R package for implementing the proposed methods, along with supplementary materials for this article, is available online.},
  archive      = {J_SAC},
  author       = {Shi, Haosheng and Dai, Wenlin},
  doi          = {10.1007/s11222-025-10573-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Exploratory analysis of dynamic networks using latent functions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference and goodness-of-fit test in functional
data via error distribution function. <em>SAC</em>, <em>35</em>(2),
1–16. (<a href="https://doi.org/10.1007/s11222-025-10574-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A kernel distribution estimator (KDE) is proposed for the error distribution in the functional data, which is computed from the residuals of the B-spline trajectories over all the measurements. The maximal stochastic process between the KDE and the error distribution is shown to converge to a Gaussian process with mean zero and specified covariance function under some mild conditions. Thus, a simultaneous confidence band (SCB) is constructed for the error distribution based on the KDE in the dense functional data. The proposed SCB is applicable in not only the independent functional data but also the functional time series. In addition, the symmetric test is proposed for the error distribution, as well as a goodness-of-fit test for mean function by using the bootstrap method. Simulation studies examine the finite sample performance of the SCB and show the bootstrap method performs well in numerical studies. The proposed theory is illustrated by the electroencephalogram (EEG) functional data.},
  archive      = {J_SAC},
  author       = {Zhong, Chen},
  doi          = {10.1007/s11222-025-10574-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference and goodness-of-fit test in functional data via error distribution function},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EM algorithm for fitting matrix-variate normal
distributions on interval-censored and missing data. <em>SAC</em>,
<em>35</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-025-10575-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix-variate distributions are powerful tools for modeling three-way datasets that often arise in longitudinal and multidimensional spatio-temporal studies. However, observations in these datasets can be missing or subject to some detection limits because of the restriction of the experimental apparatus. Here, we develop an efficient EM-type algorithm for maximum likelihood estimation of parameters, in the context of interval-censored and/or missing data, utilizing the matrix-variate normal distribution. This algorithm provides closed-form expressions that rely on truncated moments, offering a reliable approach to parameter estimation under these conditions. Results obtained from the analysis of both simulated data and real case studies concerning water quality monitoring are reported to demonstrate the effectiveness of the proposed method.},
  archive      = {J_SAC},
  author       = {Lachos, Victor H. and Tomarchio, Salvatore D. and Punzo, Antonio and Ingrassia, Salvatore},
  doi          = {10.1007/s11222-025-10575-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {An EM algorithm for fitting matrix-variate normal distributions on interval-censored and missing data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing common degree-correction parameters of multilayer
networks. <em>SAC</em>, <em>35</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-025-10576-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common properties of the networks. In this paper, we study whether multilayer networks share the same degree-correction parameters, which is a special case of the widely studied common invariant subspace problem. We first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same degree-correction parameters, and under the alternative hypothesis, there exist at least two networks that have different degree-correction parameters. We propose a weighted degree difference test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.},
  archive      = {J_SAC},
  author       = {Yuan, Mingao and Yao, Qianqian},
  doi          = {10.1007/s11222-025-10576-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Testing common degree-correction parameters of multilayer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online structural break detection in financial durations.
<em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10577-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Durations between events of interest such as intra-day transactions of assets can reflect the volatility of asset prices in financial markets. The diverse dynamics of these intervals, which we refer to as financial durations, offer valuable insights into market behavior for investors. Inspection of streaming price data for structural breaks and timely and accurate detection of transitions between different duration patterns within a trading day enables practitioners to update parameters of suitable duration models. In this article, an innovative Ensemble Penalized Estimating Function (E-PEF) approach is proposed to effectively detect change points in the logarithmic autoregressive conditional duration models for financial durations. As a quasi-score-based online detection approach, this methodology leverages Mahalanobis distances and the block bootstrap sampling method to compute critical values for finite sample time series. The online structural break detection rule is informed by comparing observed quasi-scores in the monitoring period with pre-calculated critical values from training data in an ensemble manner. Extensive simulations demonstrate that the E-PEF method has fast structural break detection performance, while effectively controlling the probability of false detection. In the real data application, we applied our method to identify structural breaks for four assets, explored their relationships with summarized changes in volatility patterns, and noted several considerations for practitioners in the financial market.},
  archive      = {J_SAC},
  author       = {Wang, Yanzhao and Zhang, Yaohua and Zou, Jian and Ravishanker, Nalini},
  doi          = {10.1007/s11222-025-10577-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Online structural break detection in financial durations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function-on-function regression models with nonlinear
dynamic effect and linear concurrent effect. <em>SAC</em>,
<em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10578-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a function-on-function regression model that predicts a functional response by both a nonlinear dynamic effect of a functional predictor and a linear concurrent effect of another functional predictor. The nonlinear dynamic effect is characterized by taking an integral of a time-dependent two-dimensional smooth surface and the linear concurrent effect is modeled through a time-varying coefficient. The model structure combines the flexibility of nonlinear modeling with the interpretability of the linear concurrent effect. To approximate the two-dimensional surface, we use tensor product basis expansions, and for the time-varying coefficient in the concurrent effect, we employ B-spline expansions. The expansion parameters for each effect are estimated iteratively to account for the mutual dependencies between these two estimated effects. Each iteration of parameter estimation involves solving a penalized least squares problem. We establish the asymptotic properties of our estimator. The numerical performance of the proposed method is illustrated by simulation studies and two real data applications.},
  archive      = {J_SAC},
  author       = {Jia, Shifan and Shi, Haolun and Guan, Tianyu},
  doi          = {10.1007/s11222-025-10578-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Function-on-function regression models with nonlinear dynamic effect and linear concurrent effect},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian functional principal component analysis.
<em>SAC</em>, <em>35</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s11222-025-10580-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a robust Bayesian functional principal component analysis (RB-FPCA) method that utilizes the skew elliptical class of distributions to model functional data, which are observed over a continuous domain. This approach effectively captures the primary sources of variation among curves, even in the presence of outliers, and provides a more robust and accurate estimation of the covariance function and principal components. The proposed method can also handle sparse functional data, where only a few observations per curve are available. We employ annealed sequential Monte Carlo for posterior inference, which offers several advantages over conventional Markov chain Monte Carlo algorithms. To evaluate the performance of our proposed model, we conduct simulation studies, comparing it with well-known frequentist and conventional Bayesian methods. The results show that our method outperforms existing approaches in the presence of outliers and performs competitively in outlier-free datasets. Finally, we demonstrate the effectiveness of our method by applying it to environmental and biological data to identify outlying functional observations. The implementation of our proposed method and applications are available at https://github.com/SFU-Stat-ML/RBFPCA .},
  archive      = {J_SAC},
  author       = {Zhang, Jiarui and Cao, Jiguo and Wang, Liangliang},
  doi          = {10.1007/s11222-025-10580-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Robust bayesian functional principal component analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on diffusion processes related to a general growth
model. <em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10562-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers two stochastic diffusion processes associated with a general growth curve that includes a wide family of growth phenomena. The resulting processes are lognormal and Gaussian, and for them inference is addressed by means of the maximum likelihood method. The complexity of the resulting system of equations requires the use of metaheuristic techniques. The limitation of the parameter space, typically required by all metaheuristic techniques, is also provided by means of a suitable strategy. Several simulation studies are performed to evaluate to goodness of the proposed methodology, and an application to real data is described.},
  archive      = {J_SAC},
  author       = {Albano, Giuseppina and Barrera, Antonio and Giorno, Virginia and Torres-Ruiz, Francisco},
  doi          = {10.1007/s11222-025-10562-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Inference on diffusion processes related to a general growth model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density regression via dirichlet process mixtures of normal
structured additive regression models. <em>SAC</em>, <em>35</em>(2),
1–20. (<a href="https://doi.org/10.1007/s11222-025-10567-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within Bayesian nonparametrics, dependent Dirichlet process mixture models provide a flexible approach for conducting inference about the conditional density function. However, several formulations of this class make either restrictive modelling assumptions or involve intricate algorithms for posterior inference. We propose a flexible and computationally convenient approach for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. We assume an additive structure for the mean of each mixture component and incorporate the effects of continuous covariates through smooth functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. Our method also seamlessly accommodates categorical covariates, linear effects of continuous covariates, varying coefficient terms, and random effects, which is why we refer our model as a Dirichlet process mixture of normal structured additive regression models. A notable feature of our method is the simplicity of posterior simulation using Gibbs sampling, as closed-form full conditional distributions for all model parameters are available. Results from a simulation study demonstrate that our approach successfully recovers the true conditional densities and other regression functionals in challenging scenarios. Applications to three real datasets further underpin the broad applicability of our method. An R package, DDPstar, implementing the proposed method is provided.},
  archive      = {J_SAC},
  author       = {Rodríguez-Álvarez, María Xosé and Inácio, Vanda and Klein, Nadja},
  doi          = {10.1007/s11222-025-10567-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Density regression via dirichlet process mixtures of normal structured additive regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel latent class models for cross-classified
categorical data: Model definition and estimation through stochastic EM.
<em>SAC</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10579-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension of the multilevel latent class model for dealing with multilevel cross-classified categorical data. Cross-classified data structures arise when observations are simultaneously nested within two or more groups, for example, children nested within both schools and neighborhoods. More specifically, we propose extending the standard hierarchical latent class model, which contains mixture components at two levels, say for children and schools, by including a separate set of mixture components for each of the higher-level crossed classifications, say for schools and neighborhoods. Because of the complex dependency structure arising from the cross-classified nature of the data, it is no longer possible to obtain maximum likelihood estimates of the model parameters, for example, using the EM algorithm. As a solution to the estimation problem, we propose an approximate estimation approach using a stochastic version of the EM algorithm. The performance of this approach, which resembles Gibbs sampling, was investigated through a set of simulation studies. Moreover, the application of the new model is illustrated using an Italian dataset on the quality of university experience at degree programme level, with degree programmes nested in both universities and fields of study.},
  archive      = {J_SAC},
  author       = {Columbu, S. and Piras, N. and Vermunt, J. K.},
  doi          = {10.1007/s11222-025-10579-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel latent class models for cross-classified categorical data: Model definition and estimation through stochastic EM},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control charts for monitoring weibull quantile under
generalized hybrid and progressive censoring schemes. <em>SAC</em>,
<em>35</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-025-10581-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose control charts to monitor quantiles of Weibull distribution for Type-I generalized hybrid censoring scheme and Type-II progressive censoring scheme. Parametric bootstrap method is employed to derive the control limits. Monte Carlo simulations are carried out to assess the in-control and out-of-control performance of the proposed charts. The phase-I analysis evaluates the performance of the charts through average run lengths for various combinations of quantile, false-alarm rate, sample size, and censoring parameters. Chart performance is thoroughly investigated in the phase-II analysis for several choices of shifts in the scale and shape parameters of Weibull distribution along with different censoring schemes. The charts for both censoring schemes have been demonstrated to be highly effective in detecting out-of-control signals, both in terms of magnitude and speed. The proposed charts are illustrated through applications in reliability and clinical practices. While both charts show similar performance in terms of speed, the chart with the optimal Type-II progressive censoring scheme outperforms the one with the Type-I generalized hybrid censoring scheme in terms of magnitude in both examples.},
  archive      = {J_SAC},
  author       = {Modok, Bidhan and Chowdhury, Shovan and Kundu, Amarjit},
  doi          = {10.1007/s11222-025-10581-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Control charts for monitoring weibull quantile under generalized hybrid and progressive censoring schemes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using prior-data conflict to tune bayesian regularized
regression models. <em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10582-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional regression models, variable selection becomes challenging from a computational and theoretical perspective. Bayesian regularized regression via shrinkage priors like the Laplace or spike-and-slab prior are effective methods for variable selection in $$p&gt;n$$ scenarios provided the shrinkage priors are configured adequately. We propose an empirical Bayes configuration using checks for prior-data conflict: tests that assess whether there is disagreement in parameter information provided by the prior and data. We apply our proposed method to the Bayesian LASSO and spike-and-slab shrinkage priors in the linear regression model and assess the variable selection performance of our prior configurations through a high-dimensional simulation study. Additionally, we apply our method to proteomic data collected from patients admitted to the Albany Medical Center in Albany NY in April of 2020 with COVID-like respiratory issues. Simulation results suggest our proposed configurations may outperform competing models when the true regression effects are small.},
  archive      = {J_SAC},
  author       = {Biziaev, Timofei and Kopciuk, Karen and Chekouo, Thierry},
  doi          = {10.1007/s11222-025-10582-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Using prior-data conflict to tune bayesian regularized regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Directional data analysis: Spherical cauchy or poisson
kernel-based distribution? <em>SAC</em>, <em>35</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-025-10583-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, two novel distributions for the analysis of directional data were introduced: the spherical Cauchy distribution and the Poisson kernel-based distribution. This paper provides a detailed exploration of both distributions within various analytical frameworks. To enhance the practical utility of these distributions, alternative parametrizations that offer advantages in numerical stability and parameter estimation are presented, such as implementation of the Newton–Raphson algorithm for parameter estimation, while facilitating a more efficient and simplified approach in the regression framework. Additionally, a two-sample location test based on the log-likelihood ratio test is introduced. This test is designed to assess whether the location parameters of two populations can be assumed equal. The maximum likelihood discriminant analysis framework is developed for classification purposes, and finally, the problem of clustering directional data is addressed, by fitting finite mixtures of Spherical Cauchy or Poisson kernel-based distributions. Empirical validation is conducted through comprehensive simulation studies and real data applications, wherein the performance of the spherical Cauchy and Poisson kernel-based distributions is systematically compared.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail and Papastamoulis, Panagiotis and Kato, Shogo},
  doi          = {10.1007/s11222-025-10583-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Directional data analysis: Spherical cauchy or poisson kernel-based distribution?},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
