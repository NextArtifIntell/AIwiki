<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---12">COAP - 12</h2>
<ul>
<li><details>
<summary>
(2025). An arc-search interior-point algorithm for nonlinear
constrained optimization. <em>COAP</em>, <em>90</em>(3), 969–995. (<a
href="https://doi.org/10.1007/s10589-025-00648-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new arc-search interior-point algorithm for the nonlinear constrained optimization problem. The proposed algorithm uses the second-order derivatives to construct a search arc that approaches the optimizer. Because the arc stays in the interior set longer than any straight line, it is expected that the scheme will generate a better new iterate than a line search method. The computation of the second-order derivatives requires to solve the second linear system of equations, but the coefficient matrix of the second linear system of equations is the same as the first linear system of equations. Therefore, the matrix decomposition obtained while solving the first linear system of equations can be reused. In addition, most elements of the right-hand side vector of the second linear system of equations are already computed when the coefficient matrix is assembled. Therefore, the computation cost for solving the second linear system of equations is insignificant and the benefit of having a better search scheme is well justified. The convergence of the proposed algorithm is established. Some preliminary test results are reported to demonstrate the merit of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Yang, Yaguang},
  doi          = {10.1007/s10589-025-00648-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {969-995},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An arc-search interior-point algorithm for nonlinear constrained optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the stochastically controlled stochastic gradient
method by the bandwidth-based stepsize. <em>COAP</em>, <em>90</em>(3),
941–968. (<a href="https://doi.org/10.1007/s10589-025-00651-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepsize plays an important role in the stochastic gradient method. The bandwidth-based stepsize allows us to adjust the stepsize within a banded region determined by some boundary functions. Based on the bandwidth-based stepsize, we propose a new method, namely SCSG-BD, for smooth non-convex finite-sum optimization problems. For the boundary functions 1/t, $$1/(t\log (t + 1))$$ and $$1/t^p$$ ( $$p\in (0,1)$$ ), SCSG-BD converges sublinearly to a stationary point at a faster rate than the stochastically controlled stochastic gradient (SCSG) method under certain conditions. Moreover, SCSG-BD is able to converge linearly to the solution if the objective function satisfies the Polyak–Łojasiewicz condition. We also introduce the 1/t-Barzilai–Borwein stepsize for practical computation. Numerical experiments demonstrate that SCSG-BD performs better than SCSG and its variants.},
  archive      = {J_COAP},
  author       = {Liu, Chenchen and Huang, Yakui and Wang, Dan},
  doi          = {10.1007/s10589-025-00651-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {941-968},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Improving the stochastically controlled stochastic gradient method by the bandwidth-based stepsize},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A second-order sequential optimality condition for nonlinear
second-order cone programming problems. <em>COAP</em>, <em>90</em>(3),
911–939. (<a href="https://doi.org/10.1007/s10589-025-00649-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, the sequential optimality conditions, which do not require constraint qualifications and allow improvement on the convergence assumptions of algorithms, had been considered in the literature. It includes the work by Andreani et al. (IMA J Numer Anal 37:1902–1929, 2017), with a sequential optimality condition for nonlinear programming, that uses the second-order information of the problem. More recently, Fukuda et al. (Set-Valued Var Anal 31:15, 2023) analyzed the conditions that use second-order information, in particular for nonlinear second-order cone programming problems (SOCP). However, such optimality conditions were not defined explicitly. In this paper, we propose an explicit definition of approximate-Karush-Kuhn-Tucker 2 (AKKT2) and complementary-AKKT2 (CAKKT2) conditions for SOCPs. We prove that the proposed AKKT2/CAKKT2 conditions are satisfied at local optimal points of the SOCP without any constraint qualification. We also present two algorithms that are based on augmented Lagrangian and sequential quadratic programming methods and show their global convergence to points satisfying the proposed conditions.},
  archive      = {J_COAP},
  author       = {Fukuda, Ellen H. and Okabe, Kosuke},
  doi          = {10.1007/s10589-025-00649-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {911-939},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A second-order sequential optimality condition for nonlinear second-order cone programming problems},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the douglas–rachford splitting method through
the lenses of moreau-type envelopes. <em>COAP</em>, <em>90</em>(3),
881–910. (<a href="https://doi.org/10.1007/s10589-024-00646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the Douglas–Rachford splitting method for weakly convex optimization problems, by the token of the Douglas–Rachford envelope, a merit function akin to the Moreau envelope. First, we use epi-convergence techniques to show that this artifact approximates the original objective function via epigraphs. Secondly, we present how global convergence and local linear convergence rates for Douglas–Rachford splitting can be obtained using such envelope, under mild regularity assumptions. The keystone of the convergence analysis is the fact that the Douglas–Rachford envelope satisfies a sufficient descent inequality alongside the generated sequence, a feature that allows us to use arguments usually employed for descent methods. We report numerical experiments that use weakly convex penalty functions, which are comparable with the known behavior of the method in the convex case.},
  archive      = {J_COAP},
  author       = {Atenas, Felipe},
  doi          = {10.1007/s10589-024-00646-9},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {881-910},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Understanding the Douglas–Rachford splitting method through the lenses of moreau-type envelopes},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convex combined symmetric alternating direction method of
multipliers for separable optimization. <em>COAP</em>, <em>90</em>(3),
839–880. (<a href="https://doi.org/10.1007/s10589-025-00647-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Alternating Direction Method of Multipliers (ADMM) is a powerful first-order method used in many practical separable optimization problems. In this paper, we propose a new variant of the symmetric ADMM, called the Convex Combined Symmetric ADMM (CcS-ADMM), by integrating a convex combination technique. CcS-ADMM retains all the favorable features of ADMM, including the ability to take full advantage of problem structures and global convergence under relaxed parameter conditions. Furthermore, using the moderate assumptions and primal-dual gap, we analyze the convergence and the O(1/N) ergodic convergence rate of the algorithm with convex setting. Additionally, we propose the convergence of the CcS-ADMM with nonconvex setting in Euclidean space under so called Kurdyka–Lojasiewicz property and some widely used assumptions, and we establish the pointwise iteration-complexity of CcS-ADMM with respect to the augmented Lagrangian function and the primal-dual residuals. Finally, we present the results from preliminary numerical experiments to demonstrate the performance of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Wang, Xiaoquan and Shao, Hu and Wu, Ting},
  doi          = {10.1007/s10589-025-00647-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {839-880},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A convex combined symmetric alternating direction method of multipliers for separable optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized methods via cubic model subspace minimization
for nonconvex optimization. <em>COAP</em>, <em>90</em>(3), 801–837. (<a
href="https://doi.org/10.1007/s10589-025-00655-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive cubic regularization methods for solving nonconvex problems need the efficient computation of the trial step, involving the minimization of a cubic model. We propose a new approach in which this model is minimized in a low dimensional subspace that, in contrast to classic approaches, is reused for a number of iterations. Whenever the trial step produced by the low-dimensional minimization process is unsatisfactory, we employ a regularized Newton step whose regularization parameter is a by-product of the model minimization over the low-dimensional subspace. We show that the worst-case complexity of classic cubic regularized methods is preserved, despite the possible regularized Newton steps. We focus on the large class of problems for which (sparse) direct linear system solvers are available and provide several experimental results showing the very large gains of our new approach when compared to standard implementations of adaptive cubic regularization methods based on direct linear solvers. Our first choice as projection space for the low-dimensional model minimization is the polynomial Krylov subspace; nonetheless, we also explore the use of rational Krylov subspaces in case where the polynomial ones lead to less competitive numerical results.},
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Palitta, Davide and Porcelli, Margherita and Simoncini, Valeria},
  doi          = {10.1007/s10589-025-00655-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {801-837},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Regularized methods via cubic model subspace minimization for nonconvex optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of the gradient descent method with
stochastic fixed-point rounding errors under the polyak–łojasiewicz
inequality. <em>COAP</em>, <em>90</em>(3), 753–799. (<a
href="https://doi.org/10.1007/s10589-025-00656-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the training of neural networks with low-precision computation and fixed-point arithmetic, rounding errors often cause stagnation or are detrimental to the convergence of the optimizers. This study provides insights into the choice of appropriate stochastic rounding strategies to mitigate the adverse impact of roundoff errors on the convergence of the gradient descent method, for problems satisfying the Polyak–Łojasiewicz inequality. Within this context, we show that a biased stochastic rounding strategy may be even beneficial in so far as it eliminates the vanishing gradient problem and forces the expected roundoff error in a descent direction. Furthermore, we obtain a bound on the convergence rate that is stricter than the one achieved by unbiased stochastic rounding. The theoretical analysis is validated by comparing the performances of various rounding strategies when optimizing several examples using low-precision fixed-point arithmetic.},
  archive      = {J_COAP},
  author       = {Xia, Lu and Massei, Stefano and Hochstenbach, Michiel E.},
  doi          = {10.1007/s10589-025-00656-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {753-799},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the convergence of the gradient descent method with stochastic fixed-point rounding errors under the polyak–Łojasiewicz inequality},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All saddle points for polynomial optimization.
<em>COAP</em>, <em>90</em>(3), 721–752. (<a
href="https://doi.org/10.1007/s10589-025-00657-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study how to compute all saddle points for the constrained and unconstrained polynomial optimization, respectively. For the constrained polynomial optimization, a scalar-type semidefinite relaxation algorithm is proposed based on the Karush-Kuhn-Tucker conditions. While for the unconstrained polynomial optimization, a matrix-type semidefinite relaxation algorithm is proposed based on the second-order optimality conditions. Both algorithms can detect the nonexistence of saddle points or find all of them if there are finitely many ones. The finite convergence of the algorithms can also be obtained under some genericity conditions.},
  archive      = {J_COAP},
  author       = {Zhou, Anwa and Yin, Shiqian and Fan, Jinyan},
  doi          = {10.1007/s10589-025-00657-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {721-752},
  shortjournal = {Comput. Optim. Appl.},
  title        = {All saddle points for polynomial optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the use of restriction of the right-hand side in spatial
branch-and-bound algorithms to ensure termination. <em>COAP</em>,
<em>90</em>(3), 691–720. (<a
href="https://doi.org/10.1007/s10589-025-00652-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial branch-and-bound algorithms for global minimization of non-convex problems require both lower and upper bounding procedures that finally converge to a globally optimal value in order to ensure termination of these methods. Whereas convergence of lower bounds is commonly guaranteed for standard approaches in the literature, this does not always hold for upper bounds. For this reason, different so-called convergent upper bounding procedures are proposed. These methods are not always used in practice, possibly due to their additional complexity or possibly due to increasing runtimes on average problems. For that reason, in this article we propose a refinement of classical branch-and-bound methods that is simple to implement and comes with marginal overhead. We prove that this small improvement already leads to convergent upper bounds, and thus show that termination of spatial branch-and-bound methods is ensured under mild assumptions.},
  archive      = {J_COAP},
  author       = {Kirst, Peter and Füllner, Christian},
  doi          = {10.1007/s10589-025-00652-5},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {691-720},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the use of restriction of the right-hand side in spatial branch-and-bound algorithms to ensure termination},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic optimal control problems with combinatorial
switching constraints, part III: Branch-and-bound algorithm.
<em>COAP</em>, <em>90</em>(3), 649–689. (<a
href="https://doi.org/10.1007/s10589-025-00654-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a branch-and-bound algorithm for globally solving parabolic optimal control problems with binary switches that have bounded variation and possibly need to satisfy further combinatorial constraints. More precisely, for a given tolerance $$\varepsilon &gt;0$$ , we show how to compute in finite time an $$\varepsilon $$ -optimal solution in function space, independently of any prior discretization. The main ingredients in our approach are an appropriate branching strategy in infinite dimension, an a posteriori error estimation in order to obtain safe dual bounds, and an adaptive refinement strategy in order to allow arbitrary switching points in the limit. The performance of our approach is demonstrated by extensive experimental results.},
  archive      = {J_COAP},
  author       = {Buchheim, Christoph and Grütering, Alexandra and Meyer, Christian},
  doi          = {10.1007/s10589-025-00654-3},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {649-689},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Parabolic optimal control problems with combinatorial switching constraints, part III: Branch-and-bound algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardinality constrained mean-variance portfolios: A penalty
decomposition algorithm. <em>COAP</em>, <em>90</em>(3), 631–648. (<a
href="https://doi.org/10.1007/s10589-025-00653-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cardinality-constrained mean-variance portfolio problem has garnered significant attention within contemporary finance due to its potential for achieving low risk while effectively managing transaction costs. Instead of solving this problem directly, many existing methods rely on regularization and approximation techniques, which hinder investors’ ability to precisely specify a portfolio’s desired cardinality level. Moreover, these approaches typically include more hyper-parameters and increase the problem’s dimensionality. To address these challenges, we propose a customized penalty decomposition algorithm. We demonstrate that this algorithm not only does it converge to a local minimizer of the cardinality-constrained mean-variance portfolio problem, but is also computationally efficient. Our approach leverages a sequence of penalty subproblems, each tackled using Block Coordinate Descent (BCD). We show that the steps within BCD yield closed-form solutions, allowing us to identify a saddle point of the penalty subproblems. Finally, by applying our penalty decomposition algorithm to real-world datasets, we highlight its efficiency and its superiority over state-of-the-art methods across several performance metrics.},
  archive      = {J_COAP},
  author       = {Mousavi, Ahmad and Michailidis, George},
  doi          = {10.1007/s10589-025-00653-4},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {631-648},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cardinality constrained mean-variance portfolios: A penalty decomposition algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-DS: A cost saving algorithm for expensive constrained
multi-fidelity blackbox optimization. <em>COAP</em>, <em>90</em>(3),
607–629. (<a href="https://doi.org/10.1007/s10589-024-00645-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces Inter-DS, a blackbox optimization algorithmic framework for computationally expensive constrained multi-fidelity problems. When applying a direct search method to such problems, the scarcity of feasible points may lead to numerous costly evaluations spent on infeasible points. Our proposed algorithm addresses this issue by leveraging multi-fidelity information, allowing for premature interruption of an evaluation when a point is estimated to be infeasible. These estimations are controlled by a biadjacency matrix, for which we propose a construction. The proposed method acts as an intermediary component bridging any non multi-fidelity direct search solver and a multi-fidelity blackbox problem, giving the user freedom of choice for the solver. A series of computational tests are conducted to validate the approach. The results show a significant improvement in solution quality when an initial feasible starting point is provided. When this condition is not met, the outcomes are contingent upon specific properties of the blackbox.},
  archive      = {J_COAP},
  author       = {Alarie, Stéphane and Audet, Charles and Diago, Miguel and Digabel, Sébastien Le and Lebeuf, Xavier},
  doi          = {10.1007/s10589-024-00645-w},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {607-629},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inter-DS: A cost saving algorithm for expensive constrained multi-fidelity blackbox optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
