<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>springer_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="springer">SPRINGER</h1>
<h2 id="aamas---22">AAMAS - 22</h2>
<ul>
<li><details>
<summary>
(2025). Epistemic selection of costly alternatives: The case of
participatory budgeting. <em>AAMAS</em>, <em>39</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10458-024-09677-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study of voting rules for participatory budgeting using the so-called epistemic approach, where one interprets votes as noisy reflections of some ground truth regarding the objectively best set of projects to fund. Using this approach, we first show that both the most studied rules in the literature and the most widely used rule in practice cannot be justified on epistemic grounds: they cannot be interpreted as maximum likelihood estimators, whatever assumptions we make about the accuracy of voters. Focusing then on welfare-maximising rules, we obtain both positive and negative results regarding epistemic guarantees.},
  archive      = {J_AAMAS},
  author       = {Rey, Simon and Endriss, Ulle},
  doi          = {10.1007/s10458-024-09677-2},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Epistemic selection of costly alternatives: The case of participatory budgeting},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ability and knowledge: From epistemic transition systems to
labelled stit models. <em>AAMAS</em>, <em>39</em>(1), 1–41. (<a
href="https://doi.org/10.1007/s10458-024-09661-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is possible to know that one can guarantee a certain result and yet not know how to guarantee it. In such cases one has the ability to guarantee something in a causal sense, but not in an epistemic sense. In this paper we focus on two formalisms used to model both conceptions of ability: one formalism based on epistemic transition systems and the other on labelled stit models. We show a strong correspondence between the two formalisms by providing mappings from the former to the latter for both the languages and the structures. Moreover, we demonstrate that our extension of labelled stit logic is more expressive than the logic of epistemic transition systems.},
  archive      = {J_AAMAS},
  author       = {Kuncová, Alexandra and Broersen, Jan and Duijf, Hein and Ramírez Abarca, Aldo Iván},
  doi          = {10.1007/s10458-024-09661-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-41},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Ability and knowledge: From epistemic transition systems to labelled stit models},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information gathering in POMDPs using active inference.
<em>AAMAS</em>, <em>39</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10458-024-09683-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gathering information about the environment state is the main goal in several planning tasks for autonomous agents, such as surveillance, inspection and tracking of objects. Such planning tasks are typically modeled using a Partially Observable Markov Decision Process (POMDP), and in the literature several approaches have emerged to consider information gathering during planning and execution. Similar developments can be seen in the field of active inference, which focuses on active information collection in order to be able to reach a goal. Both fields use POMDPs to model the environment, but the underlying principles for action selection are different. In this paper we create a bridge between both research fields by discussing how they relate to each other and how they can be used for information gathering. Our contribution is a tailored approach to model information gathering tasks directly in the active inference framework. A series of experiments demonstrates that our approach enables agents to gather information about the environment state. As a result, active inference becomes an alternative to common POMDP approaches for information gathering, which opens the door towards more cross cutting research at the intersection of both fields. This is advantageous, because recent advancements in POMDP solvers may be used to accelerate active inference, and the principled active inference framework may be used to model POMDP agents that operate in a neurobiologically plausible fashion.},
  archive      = {J_AAMAS},
  author       = {Walraven, Erwin and Sijs, Joris and Burghouts, Gertjan J.},
  doi          = {10.1007/s10458-024-09683-4},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Information gathering in POMDPs using active inference},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aggregating bipolar opinions through bipolar
assumption-based argumentation. <em>AAMAS</em>, <em>39</em>(1), 1–34.
(<a href="https://doi.org/10.1007/s10458-024-09684-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel method to aggregate bipolar argumentation frameworks expressing opinions of different parties in debates. We use Bipolar Assumption-based Argumentation (ABA) as an all-encompassing formalism for bipolar argumentation under different semantics. By leveraging on recent results on judgement aggregation in social choice theory, we prove several preservation results for relevant properties of bipolar ABA using quota and oligarchic rules. Specifically, we prove (positive and negative) results about the preservation of conflict-free, closed, admissible, preferred, complete, set-stable, well-founded and ideal extensions in bipolar ABA, as well as the preservation of acceptability, acyclicity and coherence for individual assumptions. Finally, we illustrate our methodology and results in the context of a case study on opinion aggregation for the treatment of long COVID patients.},
  archive      = {J_AAMAS},
  author       = {Dickie, Charles and Lauren, Stefan and Belardinelli, Francesco and Rago, Antonio and Toni, Francesca},
  doi          = {10.1007/s10458-024-09684-3},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Aggregating bipolar opinions through bipolar assumption-based argumentation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). La VIDA: Towards a motivated goal reasoning agent.
<em>AAMAS</em>, <em>39</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10458-024-09685-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An autonomous agent deployed to operate over extended horizons in uncertain environments will encounter situations for which it was not designed. A class of these situations involves an invalidation of agent goals and limited guidance in establishing a new set of goals to pursue. An agent will benefit from some mechanism that will allow it to pursue new goals under these circumstances such that the goals are broadly useful in its environment and take advantage of its existing skills while aligning with societal norms. We propose augmenting a goal reasoning agent, i.e., an agent that can deliberate on and self-select its goals, with a motivation system that can be used to both constrain and motivate agent behavior. A human-like motivation system coupled with a goal-self concordant selection technique allows the approach to be framed as an optimization problem in which the agent selects goals that have high utility while simultaneously in harmony with its motivations. Over the agent’s operational lifespan its motivation system adjusts incrementally to more closely reflect the reality of its goal reasoning and goal pursuit experiences. Experiments performed with an ablation testing technique comparing the average utility of goals achieved in the presence and absence of a motivation system suggest that the motivated version of the system leads to pursuing more useful goals than the baseline.},
  archive      = {J_AAMAS},
  author       = {Addison, Ursula},
  doi          = {10.1007/s10458-024-09685-2},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {La VIDA: Towards a motivated goal reasoning agent},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Budget-feasible egalitarian allocation of conflicting jobs.
<em>AAMAS</em>, <em>39</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10458-024-09686-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allocating conflicting jobs among individuals while respecting a budget constraint for each individual is an optimization problem that arises in various real-world scenarios. In this paper, we consider the situation where each individual derives some satisfaction from each job. We focus on finding a feasible allocation of conflicting jobs that maximize egalitarian cost, i.e., the satisfaction of the individual who is worst-off. To the best of our knowledge, this is the first paper to combine egalitarianism, budget-feasibility, and conflict-freeness in allocations. We provide a systematic study of the computational complexity of finding budget-feasible conflict-free egalitarian allocation and show that our problem generalizes a large number of classical optimization problems. Therefore, unsurprisingly, our problem is NP-hard even for two individuals and when there is no conflict between any jobs. We show that the problem admits algorithms when studied in the realm of approximation algorithms and parameterized algorithms with a host of natural parameters that match and in some cases improve upon the running time of known algorithms.},
  archive      = {J_AAMAS},
  author       = {Gupta, Sushmita and Jain, Pallavi and Mohanapriya, A. and Tripathi, Vikash},
  doi          = {10.1007/s10458-024-09686-1},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Budget-feasible egalitarian allocation of conflicting jobs},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reflexive anticipatory reasoning by BDI agents.
<em>AAMAS</em>, <em>39</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10458-025-09687-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how predictions about the future behaviour of an agent can be exploited to improve its decision-making in the present. Future states are foreseen by a simulation technique, which is based on models of both the environment and the agent. Although the environment model is usually taken into account for prediction in artificial intelligence (e.g., in automated planning), the agent model receives less attention. We leverage the agent model to speed up the simulation and as a source of alternative decisions. Our proposal bases the agent model on the practical knowledge the developer has given to the agent, especially in the case of BDI agents. This knowledge is thus exploited in the proposed future-concerned reasoning mechanisms. We present a prototype implementation of our approach as well as the results from its evaluation on static and dynamic environments. This allows us to better understand the relation between the improvement in agent decisions and the quality of the knowledge provided by the developer.},
  archive      = {J_AAMAS},
  author       = {Hübner, Jomi Fred and Burattini, Samuele and Ricci, Alessandro and Mayer, Simon},
  doi          = {10.1007/s10458-025-09687-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Reflexive anticipatory reasoning by BDI agents},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disagree and commit: Degrees of argumentation-based
agreements. <em>AAMAS</em>, <em>39</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s10458-025-09688-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cooperative human decision-making, agreements are often not total; a partial degree of agreement is sufficient to commit to a decision and move on, as long as one is somewhat confident that the involved parties are likely to stand by their commitment in the future, given no drastic unexpected changes. In this paper, we introduce the notion of agreement scenarios that allow artificial autonomous agents to reach such agreements, using formal models of argumentation, in particular abstract argumentation and value-based argumentation. We introduce the notions of degrees of satisfaction and (minimum, mean, and median) agreement, as well as a measure of the impact a value in a value-based argumentation framework has on these notions. We then analyze how degrees of agreement are affected when agreement scenarios are expanded with new information, to shed light on the reliability of partial agreements in dynamic scenarios. An implementation of the introduced concepts is provided as part of an argumentation-based reasoning software library.},
  archive      = {J_AAMAS},
  author       = {Kampik, Timotheus and Nieves, Juan Carlos},
  doi          = {10.1007/s10458-025-09688-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Disagree and commit: Degrees of argumentation-based agreements},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-level explainability framework for engineering and
understanding BDI agents. <em>AAMAS</em>, <em>39</em>(1), 1–42. (<a
href="https://doi.org/10.1007/s10458-025-09689-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the complexity of software systems rises, explainability - i.e. the ability of systems to provide explanations of their behaviour - becomes a crucial property. This is true for any AI-based systems, including autonomous systems that exhibit decisionmaking capabilities such as multi-agent systems. Although explainabil- ity is generally considered useful to increase the level of trust for end-users, we argue it is also an interesting property for software engineers, developers, and designers to debug and validate the system’s behaviour. In this paper, we propose a multi-level explainability framework for BDI agents to generate explanations of a running system from logs at different levels of abstraction, tailored to different users and their needs. We describe the mapping from logs to explanations, and present a prototype tool based on the JaCaMo platform which implements the framework.},
  archive      = {J_AAMAS},
  author       = {Yan, Elena and Burattini, Samuele and Hübner, Jomi Fred and Ricci, Alessandro},
  doi          = {10.1007/s10458-025-09689-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-42},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A multi-level explainability framework for engineering and understanding BDI agents},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A formal testing method for multi-agent systems using
colored petri nets. <em>AAMAS</em>, <em>39</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10458-025-09690-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomy in software, a system’s ability to make decisions and take actions independently without human intervention, is a fundamental characteristic of multi-agent systems. Testing, a crucial phase of software validation, is particularly challenging in multi-agent systems due to its complexity, as the interaction between autonomous agents can result in emergent behaviors and collective intelligence, leading to system properties not found in individual agents. A multi-agent system operates on at least three main dimensions: the individual level, the social level, and the communication interfaces. An organizational model formally defines a multi-agent system’s structure, roles, relationships, and interactions. It represents the social layer, capturing agents’ collective dynamics and dependencies, facilitating coherent and efficient collaboration to achieve individual and collective goals. During the literature review, a gap was identified when testing the social layer of multi-agent systems. This paper presents a testing approach by formally introducing steps to map an organizational model, here $$\mathcal {M}$$ oise $$^+$$ , into a colored Petri net. This mapping aims to generate a formal system model, which is used to generate and count test cases based on a coverage criterion. Finally, a use case called Inspector was presented to demonstrate the method by generating test cases, executing the test, and identifying execution errors.},
  archive      = {J_AAMAS},
  author       = {Machado, Ricardo Arend and Cardoso, Arthur da Silva Zelindro and Farias, Giovani Parente and Gonçalves, Eder Mateus Nunes and Adamatti, Diana Francisca},
  doi          = {10.1007/s10458-025-09690-z},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A formal testing method for multi-agent systems using colored petri nets},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An introduction to computational argumentation research from
a human argumentation perspective. <em>AAMAS</em>, <em>39</em>(1), 1–59.
(<a href="https://doi.org/10.1007/s10458-025-09692-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational Argumentation studies how human argumentative reasoning can be approached from a computational viewpoint. Human argumentation is a complex process that has been studied from different perspectives (e.g., philosophical or linguistic) and that involves many different aspects beyond pure reasoning, such as the role of emotions, values, social contexts, and practical constraints, which are often overlooked in computational approaches to argumentation. The heterogeneity of human argumentation is present in Computational Argumentation research, in the form of various tasks that approach the main phases of argumentation individually. With the increasing interest of researchers in Artificial Intelligence, we consider that it is of great importance to provide guidance on the Computational Argumentation research area. Thus, in this paper, we present a general overview of Computational Argumentation, from the perspective of how humans argue. For that purpose, the following contributions are produced: (i) a consistent structure for Computational Argumentation research mapped with the human argumentation process; (ii) a collective understanding of the tasks approached by Computational Argumentation and their synergies; (iii) a thorough review of important advances in each of these tasks; and (iv) an analysis and a classification of the future trends in Computational Argumentation research and relevant open challenges in the area.},
  archive      = {J_AAMAS},
  author       = {Ruiz-Dolz, Ramon and Heras, Stella and García-Fornes, Ana},
  doi          = {10.1007/s10458-025-09692-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-59},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {An introduction to computational argumentation research from a human argumentation perspective},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low variance trust region optimization with independent
actors and sequential updates in cooperative multi-agent reinforcement
learning. <em>AAMAS</em>, <em>39</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10458-025-09695-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative multi-agent reinforcement learning assumes each agent shares the same reward function and can be trained effectively using the Trust Region framework of single-agent. Instead of relying on other agents’ actions, the independent actors setting considers each agent to act based only on its local information, thus having more flexible applications. However, in the sequential update framework, it is required to re-estimate the joint advantage function after each individual agent’s policy step. Despite the practical success of importance sampling, the updated advantage function suffers from exponentially high variance problems, which likely results in unstable convergence. In this work, we first analyze the high variance advantage both empirically and theoretically. To overcome this limitation, we introduce a clipping objective to control the upper bounds of the advantage fluctuation in sequential updates. With the proposed objective, we provide a monotonic bound with sub-linear convergence to $$\varepsilon$$ -Nash Equilibria. We further derive two new practical algorithms using our clipping objective. The experiment results on three popular multi-agent reinforcement learning benchmarks show that our proposed method outperforms the tested baselines in most environments. By carefully analyzing different training settings, our proposed method is highlighted with both stable convergence properties and the desired low advantage variance estimation. For reproducibility purposes, our source code is publicly available at https://github.com/giangbang/Low-Variance-Trust-Region-MARL .},
  archive      = {J_AAMAS},
  author       = {Le, Bang Giang and Ta, Viet Cuong},
  doi          = {10.1007/s10458-025-09695-8},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Low variance trust region optimization with independent actors and sequential updates in cooperative multi-agent reinforcement learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving multi-agent games on networks. <em>AAMAS</em>,
<em>39</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10458-025-09696-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent games on networks (GoNs) have nodes that represent agents and edges that represent interactions among agents. A special class of GoNs is composed of 2-players games on each of their edges. General GoNs have games that are played by all agents in each neighborhood. Solutions to games on networks are stable states (i.e., pure Nash equilibria), and in general one is interested in efficient solutions (of high global social welfare). This study addresses the multi-agent aspect of games on networks—a system of multiple agents that compose a game and seek a solution by performing a multi-agent (distributed) algorithm. The agents playing the game are assumed to be strategic and an iterative distributed algorithm is proposed, that lets the agents interact (i.e., negotiate) in neighborhoods in a process that guarantees the convergence of any multi-agent game on network to a globally stable state. The proposed algorithm—the TECon algorithm—iterates, one neighborhood at a time, performing a repeated social choice action. A truth-enforcing mechanism is integrated into the algorithm, collecting the valuations of agents in each neighborhood and computing incentives while eliminating strategic behavior. The proposed method is proven to converge to globally stable states that are at least as efficient as the initial state, for any game on network. A specific version of the algorithm is given for the class of Public Goods Games, where the main properties of the algorithm are guaranteed even when the strategic agents playing the game consider their possible future valuations when interacting. An extensive experimental evaluation on randomly generated games on networks demonstrates that the TECon algorithm converges very rapidly. On general forms of public goods games, the proposed algorithm outperforms former solving methods, where former methods are applicable.},
  archive      = {J_AAMAS},
  author       = {Vaknin, Yair and Meisels, Amnon},
  doi          = {10.1007/s10458-025-09696-7},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Solving multi-agent games on networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-theoretic approach for hierarchical epidemic control.
<em>AAMAS</em>, <em>39</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10458-025-09697-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design and analyze a multi-level game-theoretic model of hierarchical policy interventions for epidemic control, such as those in response to the COVID-19 pandemic. Our model captures the potentially mismatched priorities among a hierarchy of policy-makers (e.g., federal, state, and local governments) with respect to two cost components that have opposite dependence on the policy strength—post-intervention infection rates and the socio-economic cost of policy implementation. Additionally, our model includes a crucial third factor in decisions: a cost of non-compliance with the policy-maker immediately above in the hierarchy, such as non-compliance of counties with state-level policies. We propose two novel algorithms for approximating solutions to such games. The first is based on best response dynamics (BRD) and exploits the tree structure of the game. The second combines quadratic integer programming (QIP), which enables us to collapse the two lowest levels of the game, with the best response dynamics. We experimentally characterize the scalability and equilibrium approximation quality of our two approaches against model parameters. Finally, we conduct experiments in simulations based on both synthetic and real-world data under various parameter configurations and analyze the resulting (approximate) equilibria to gain insight into the impact of decentralization on overall welfare (measured as the negative sum of costs) as well as emergent properties like social welfare, free-riding, and fairness in cost distribution among policy-makers.},
  archive      = {J_AAMAS},
  author       = {Jia, Feiran and Mate, Aditya and Li, Zun and Jabbari, Shahin and Chakraborty, Mithun and Tambe, Milind and Wellman, Michael P. and Vorobeychik, Yevgeniy},
  doi          = {10.1007/s10458-025-09697-6},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {A game-theoretic approach for hierarchical epidemic control},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emergent language: A survey and taxonomy. <em>AAMAS</em>,
<em>39</em>(1), 1–73. (<a
href="https://doi.org/10.1007/s10458-025-09691-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of relevant scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps.},
  archive      = {J_AAMAS},
  author       = {Peters, Jannik and Waubert de Puiseau, Constantin and Tercan, Hasan and Gopikrishnan, Arya and Lucas de Carvalho, Gustavo Adolpho and Bitter, Christian and Meisen, Tobias},
  doi          = {10.1007/s10458-025-09691-y},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-73},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Emergent language: A survey and taxonomy},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal matchings with one-sided preferences: Fixed and
cost-based quotas. <em>AAMAS</em>, <em>39</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10458-025-09693-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the well-studied many-to-one bipartite matching problem of assigning applicants $${\varvec{\mathcal {A}}}$$ to posts $${\varvec{\mathcal {P}}}$$ where applicants rank posts in the order of preference. This setting models many important real-world allocation problems like assigning students to courses, applicants to jobs, amongst many others. In such scenarios, it is natural to ask for an allocation that satisfies guarantees of the form “match at least 80% of applicants to one of their top three choices” or “it is unacceptable to leave more than 10% of applicants unassigned”. The well-studied notions of rank-maximality and fairness fail to capture such requirements due to their property of optimizing extreme ends of the signature of a matching. We, therefore, propose a novel optimality criterion, which we call the “weak dominance ” of ranks. We investigate the computational complexity of the new notion of optimality in the setting where posts have associated fixed quotas. We prove that under the fixed quota setting, the problem turns out to be NP-hard under natural restrictions. We provide randomized algorithms in the fixed quota setting when the number of ranks is constant. We also study the problem under a cost-based quota setting and show that a matching that weakly dominates the input signature and has minimum total cost can be computed efficiently. Apart from circumventing the hardness, the cost-based quota setting is motivated by real-world applications like course allocation or school choice where the capacities or quotas need not be rigid. We also show that when the objective is to minimize the maximum cost, the problem under the cost-based quota setting turns out to be NP-hard. To complement the hardness, we provide a randomized algorithm when the number of ranks is constant. We also provide an approximation algorithm which is an asymptotic faster alternative to the randomized algorithm.},
  archive      = {J_AAMAS},
  author       = {Santhini, K. A. and Sankar, Govind S. and Nasre, Meghana},
  doi          = {10.1007/s10458-025-09693-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Optimal matchings with one-sided preferences: Fixed and cost-based quotas},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the impact of direct punishment on the
emergence of cooperation in multi-agent reinforcement learning systems.
<em>AAMAS</em>, <em>39</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10458-025-09698-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the problem of cooperation is fundamentally important for the creation and maintenance of functional societies. Problems of cooperation are omnipresent within human society, with examples ranging from navigating busy road junctions to negotiating treaties. As the use of AI becomes more pervasive throughout society, the need for socially intelligent agents capable of navigating these complex cooperative dilemmas is becoming increasingly evident. Direct punishment is a ubiquitous social mechanism that has been shown to foster the emergence of cooperation in both humans and non-humans. In the natural world, direct punishment is often strongly coupled with partner selection and reputation and used in conjunction with third-party punishment. The interactions between these mechanisms could potentially enhance the emergence of cooperation within populations. However, no previous work has evaluated the learning dynamics and outcomes emerging from multi-agent reinforcement learning populations that combine these mechanisms. This paper addresses this gap. It presents a comprehensive analysis and evaluation of the behaviors and learning dynamics associated with direct punishment, third-party punishment, partner selection, and reputation. Finally, we discuss the implications of using these mechanisms on the design of cooperative AI systems.},
  archive      = {J_AAMAS},
  author       = {Dasgupta, Nayana and Musolesi, Mirco},
  doi          = {10.1007/s10458-025-09698-5},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Investigating the impact of direct punishment on the emergence of cooperation in multi-agent reinforcement learning systems},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relationship design for socially-aware behavior in static
games. <em>AAMAS</em>, <em>39</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10458-025-09699-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous agents can adopt socially-aware behaviors to reduce social costs, mimicking the way animals interact in nature and humans in society. We present a new approach to model socially-aware decision-making that includes two key elements: bounded rationality and inter-agent relationships. We capture the inter-agent relationships by introducing a novel model called a relationship game and encode agents’ bounded rationality using quantal response equilibria. For each relationship game, we define a social cost function and formulate a mechanism design problem to optimize weights for relationships that minimize social cost at the equilibrium. We address the multiplicity of equilibria by presenting the problem in two forms: Min-Max and Min-Min, aimed respectively at minimization of the highest and lowest social costs in the equilibria. We compute the quantal response equilibrium by solving a least-squares problem defined with its Karush-Kuhn-Tucker conditions, and propose two projected gradient descent algorithms to solve the mechanism design problems. Numerical results, including two-lane congestion and congestion with an ambulance, confirm that these algorithms consistently reach the equilibrium with the intended social costs.},
  archive      = {J_AAMAS},
  author       = {Chen, Shenghui and Bayiz, Yigit E. and Fridovich-Keil, David and Topcu, Ufuk},
  doi          = {10.1007/s10458-025-09699-4},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Relationship design for socially-aware behavior in static games},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double mixing networks based monotonic value function
decomposition algorithm for swarm intelligence in UAVs. <em>AAMAS</em>,
<em>39</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10458-025-09700-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-agent systems, particularly when facing challenges of partial observability, reinforcement learning demonstrates significant autonomous decision-making capabilities. Aiming at addressing resource allocation and collaboration issues in drone swarms operating in dynamic and unknown environments, we propose a novel deep reinforcement learning algorithm, DQMIX. We employ a framework of centralized training with decentralized execution and incorporate a partially observable Markov game model to describe the complex game environment of drone swarms. The core innovation of the DQMIX algorithm lies in its dual-mixing network structure and soft-switching mechanism. Two independent mixing networks handle local Q-values and synthesize them into a global Q-value. This structure enhances decision accuracy and system adaptability under different scenarios and data conditions. The soft-switching module allows the system to transition smoothly between the two networks, selecting the output of the network with smaller TD-errors to enhance decision stability and coherence. Simultaneously, we introduce Hindsight Experience Replay to learn from failed experiences. Experimental results using JSBSim demonstrate that DQMIX provides an effective solution for drone swarm game problems, especially in resource allocation and adversarial environments.},
  archive      = {J_AAMAS},
  author       = {Qu, Pingping and He, Chenglong and Wu, Xiaotong and Wang, Ershen and Xu, Song and Liu, Huan and Sun, Xinhui},
  doi          = {10.1007/s10458-025-09700-0},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Double mixing networks based monotonic value function decomposition algorithm for swarm intelligence in UAVs},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “Provably fair” algorithms may perpetuate racial and gender
bias: A study of salary dispute resolution. <em>AAMAS</em>,
<em>39</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10458-025-09703-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior work suggests automated dispute resolution tools using “provably fair” algorithms can address disparities between demographic groups. These methods use multi-criteria elicited preferences from all disputants and satisfy constraints to generate “fair” solutions. However, we analyze the potential for inequity to permeate proposals through the preference elicitation stage. This possibility arises if differences in dispositional attitudes differ between demographics, and those dispositions affect elicited preferences. Specifically, risk aversion plays a prominent role in predicting preferences. Risk aversion predicts a weaker relative preference for salary and a softer within-issue utility for each issue; this leads to worse compensation packages for risk-averse groups. These results raise important questions in AI-value alignment about whether an AI mediator should take explicit preferences at face value.},
  archive      = {J_AAMAS},
  author       = {Hale, James and Kim, Peter H. and Gratch, Jonathan},
  doi          = {10.1007/s10458-025-09703-x},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {“Provably fair” algorithms may perpetuate racial and gender bias: A study of salary dispute resolution},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On fair and efficient solutions for budget apportionment.
<em>AAMAS</em>, <em>39</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s10458-025-09694-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with an apportionment problem involving n agents and a common budget B. Each agent submits some demands which are indivisible portions of the budget, and a central authority has to decide which demands to accept. The utility of an agent corresponds to the total amount of her accepted demands. In this context, it is desirable to be fair among the agents and efficient by not wasting the budget. An ideal solution would be to spend exactly B/n for every agent but this is rarely possible because of the indivisibility of the demands. Since combining fairness with efficiency is highly desirable but often impossible, we explore relaxed notions of fairness and efficiency, in order to determine if they go together. Our approach is also constructive because polynomial algorithms that build fair and efficient solutions are also given. The fairness criteria under consideration are the maximization of the minimum agent utility (max–min), proportionality, a customized notion of envy-freeness called jealousy-freeness, and the relaxations up to one or any demand of the previous two concepts. Efficiency in this work is either the maximization of the utilitarian social welfare or Pareto optimality. First we consider fairness and efficiency separately. The existence and computation of solutions that are either fair or efficient are studied. A complete picture of the relations that connect the fairness and efficiency concepts is provided. Second, we determine when fairness and efficiency can be combined for every possible instance. We prove that Pareto optimality is compatible with two notions of fairness, namely max–min and proportionality up to any demand. In contrast, none of the fairness concepts under consideration can be paired with the maximization of utilitarian social welfare. Therefore, we finally conduct a thorough analysis of the price of fairness which bounds the loss of efficiency caused by imposing fairness or one of its relaxations.},
  archive      = {J_AAMAS},
  author       = {Cardi, Pierre and Gourvès, Laurent and Lesca, Julien},
  doi          = {10.1007/s10458-025-09694-9},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {On fair and efficient solutions for budget apportionment},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptation procedure in misinformation games.
<em>AAMAS</em>, <em>39</em>(1), 1–47. (<a
href="https://doi.org/10.1007/s10458-025-09704-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study interactions between agents in multi-agent systems, in which the agents are misinformed with regards to the game that they play, essentially having a subjective and incorrect understanding of the setting, without being aware of it. For that, we introduce a new game-theoretic concept, called misinformation games, that provides the necessary toolkit to study this situation. Subsequently, we enhance this framework by developing a time-discrete procedure (called the Adaptation Procedure) that captures iterative interactions in the above context. During the Adaptation Procedure, the agents update their information and reassess their behaviour in each step. We demonstrate our ideas through an implementation, which is used to study the efficiency and characteristics of the Adaptation Procedure.},
  archive      = {J_AAMAS},
  author       = {Varsos, Konstantinos and Papamichail, Merkouris and Flouris, Giorgos and Bitsaki, Marina},
  doi          = {10.1007/s10458-025-09704-w},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  month        = {6},
  number       = {1},
  pages        = {1-47},
  shortjournal = {Auton. Agent. Multi-Agent. Syst.},
  title        = {Adaptation procedure in misinformation games},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ail---10">AIL - 10</h2>
<ul>
<li><details>
<summary>
(2025). AI, law and beyond. A transdisciplinary ecosystem for the
future of AI &amp; law. <em>AIL</em>, <em>33</em>(1), 253–270. (<a
href="https://doi.org/10.1007/s10506-024-09404-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We live in exciting times for AI and Law: technical developments are moving at a breakneck pace, and at the same time, the call for more robust AI governance and regulation grows stronger. How should we as an AI &amp; Law community navigate these dramatic developments and claims? In this Presidential Address, I present my ideas for a way forward: researching, developing and evaluating real AI systems for the legal field with researchers from AI, Law and beyond. I will demonstrate how we at the Netherlands National Police Lab AI are developing responsible AI by combining insights from different disciplines, and how this connects to the future of our field.},
  archive      = {J_AIL},
  author       = {Bex, Floris J.},
  doi          = {10.1007/s10506-024-09404-y},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {253-270},
  shortjournal = {Artif. Intell. Law},
  title        = {AI, law and beyond. a transdisciplinary ecosystem for the future of AI &amp; law},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automating petition classification in brazil’s legal system:
A two-step deep learning approach. <em>AIL</em>, <em>33</em>(1),
227–251. (<a href="https://doi.org/10.1007/s10506-023-09385-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated classification of legal documents has been the subject of extensive research in recent years. However, this is still a challenging task for long documents, since it is difficult for a model to identify the most relevant information for classification. In this paper, we propose a two-stage supervised learning approach for the classification of petitions, a type of legal document that requests a court order. The proposed approach is based on a word-level encoder–decoder Seq2Seq deep neural network, such as a Bidirectional Long Short-Term Memory (BiLSTM) or a Bidirectional Encoder Representations from Transformers (BERT) model, and a document-level Support Vector Machine classifier. To address the challenges posed by the lengthy legal documents, the approach introduces a human-in-the-loop approach, whose task is to localize and tag relevant segments of text in the word-level training part, which dramatically reduces the dimension of the document classifier input vector. We performed experiments to validate our approach using a real-world dataset comprised of 270 intermediate petitions, which were carefully annotated by specialists from the 15th civil unit of the State of Alagoas, Brazil. Our results revealed that both BiLSTM and BERT-Convolutional Neural Networks variants achieved an accuracy of up to 95.49%, and also outperformed baseline classifiers based on the Term Frequency–Inverse Document Frequency test vectorizer. The proposed approach is currently being utilized to automate the aforementioned justice unit, thereby increasing its efficiency in handling repetitive tasks.},
  archive      = {J_AIL},
  author       = {Costa, Yuri D. R. and Oliveira, Hugo and Nogueira, Valério and Massa, Lucas and Yang, Xu and Barbosa, Adriano and Oliveira, Krerley and Vieira, Thales},
  doi          = {10.1007/s10506-023-09385-4},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {227-251},
  shortjournal = {Artif. Intell. Law},
  title        = {Automating petition classification in brazil’s legal system: A two-step deep learning approach},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward representing interpretation in factor-based models of
precedent. <em>AIL</em>, <em>33</em>(1), 199–226. (<a
href="https://doi.org/10.1007/s10506-023-09384-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the desirability and feasibility of modeling precedents with multiple interpretations within factor-based models of precedential constraint. The main idea is that allowing multiple reasonable interpretations of cases and modeling precedential constraint as a function of what all reasonable interpretations compel may be advantageous. The article explains the potential benefits of extending the models in this way with a focus on incorporating a theory of vertical precedent in U.S. federal appellate courts. It also considers the costs of extending the models in this way, such as the significant increase in the functional size of the case base and the need to provide some kind of ordering on interpretations to select a “best” interpretation. Finally, the article suggests partially incorporating multiple interpretations of dimensions as a realistic starting point for incorporating interpretations generally, and shows how doing so can help address difficulties with dimensions. The conclusion remarks on the use of interpretations to deal with inconsistent precedents.},
  archive      = {J_AIL},
  author       = {Rigoni, Adam},
  doi          = {10.1007/s10506-023-09384-5},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {199-226},
  shortjournal = {Artif. Intell. Law},
  title        = {Toward representing interpretation in factor-based models of precedent},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision support for detecting sensitive text in government
records. <em>AIL</em>, <em>33</em>(1), 171–197. (<a
href="https://doi.org/10.1007/s10506-023-09383-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freedom of information laws promote transparency by permitting individuals and organizations to obtain government documents. However, exemptions from disclosure are necessary to protect privacy and to permit government officials to deliberate freely. Deliberative language is often the most challenging and burdensome exemption to detect, leading to high processing costs and delays in responding to open-records requests. This paper describes a novel deliberative-language detection model trained on a new annotated training set. The deliberative-language detection model is a component of a decision-support system for open-records requests under the US Freedom of Information Act, the FOIA Assistant, that ingests documents responsive to an open-records requests, suggests passages likely to be subject to deliberative language, privacy, or other exemptions, and assists analysts in rapidly redacting suggested passages. The tool’s interface is based on extensive human-factors and usability studies with analysts and is currently in operational testing by multiple US federal agencies.},
  archive      = {J_AIL},
  author       = {Branting, Karl and Brown, Bradford and Giannella, Chris and Guilder, James Van and Harrold, Jeff and Howell, Sarah and Baron, Jason R.},
  doi          = {10.1007/s10506-023-09383-6},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {171-197},
  shortjournal = {Artif. Intell. Law},
  title        = {Decision support for detecting sensitive text in government records},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Correction to: Reasoning with inconsistent precedents.
<em>AIL</em>, <em>33</em>(1), 167–170. (<a
href="https://doi.org/10.1007/s10506-024-09392-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIL},
  author       = {Canavotto, Ilaria},
  doi          = {10.1007/s10506-024-09392-z},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {167-170},
  shortjournal = {Artif. Intell. Law},
  title        = {Correction to: Reasoning with inconsistent precedents},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Reasoning with inconsistent precedents. <em>AIL</em>,
<em>33</em>(1), 137–166. (<a
href="https://doi.org/10.1007/s10506-023-09382-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational models of legal precedent-based reasoning developed in AI and Law are typically based on the simplifying assumption that the background set of precedent cases is consistent. Besides being unrealistic in the legal domain, this assumption is problematic for recent promising applications of these models to the development of explainable AI methods. In this paper I explore a model of legal precedent-based reasoning that, unlike existing models, does not rely on the assumption that the background set of precedent cases is consistent. The model is a generalization of the reason model of precedential constraint. I first show that the model supports an interesting deontic logic, where consistent obligations can be derived from inconsistent case bases. I then provide an explanation of this surprising result by proposing a reformulation of the model in terms of cases that support a new potential decision and cases that conflict with it. Finally, I show that the reformulation of the model allows us to verify that inconsistent case bases do not make verification that a decision is permissible substantially more complex than consistent case bases and to introduce intuitive criteria to compare different permissible decisions.},
  archive      = {J_AIL},
  author       = {Canavotto, Ilaria},
  doi          = {10.1007/s10506-023-09382-7},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {137-166},
  shortjournal = {Artif. Intell. Law},
  title        = {Reasoning with inconsistent precedents},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network to identify requests, decisions, and
arguments in court rulings on custody. <em>AIL</em>, <em>33</em>(1),
101–135. (<a href="https://doi.org/10.1007/s10506-023-09380-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Court rulings are among the most important documents in all legal systems. This article describes a study in which natural language processing is used for the automatic characterization of Spanish judgments that deal with the physical custody (joint or individual) of minors. The model was trained to identify a set of elements: the type of custody requested by the plaintiff, the type of custody decided on by the court, and eight of the most commonly used arguments in this type of judgment. Two jurists independently annotated more than 3000 judgments, which were used to train a model based on transformers. The main difficulties encountered in this task were the complexity of the judicial language and the need to work with appellate court rulings that have a more complicated structure than decisions at first instance. For the complete court rulings, the F1 score of the inter-annotator agreement ranged from 0.60 to 0.86 and the Kappa index from 0.33 to 0.73. The F1 score of the agreement between the model and the annotators ranged from 0.66 to 0.93 and the Kappa index from 0.57 to 0.80. These results in which the model performance exceeds even the inter-annotator agreement show the high ability of transformers to identify abstract entities in legal texts.},
  archive      = {J_AIL},
  author       = {Muñoz-Soro, José Félix and del Hoyo Alonso, Rafael and Montañes, Rosa and Lacueva, Francisco},
  doi          = {10.1007/s10506-023-09380-9},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {101-135},
  shortjournal = {Artif. Intell. Law},
  title        = {A neural network to identify requests, decisions, and arguments in court rulings on custody},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural language processing for legal document review:
Categorising deontic modalities in contracts. <em>AIL</em>,
<em>33</em>(1), 79–100. (<a
href="https://doi.org/10.1007/s10506-023-09379-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The contract review process can be a costly and time-consuming task for lawyers and clients alike, requiring significant effort to identify and evaluate the legal implications of individual clauses. To address this challenge, we propose the use of natural language processing techniques, specifically text classification based on deontic tags, to streamline the process. Our research question is whether natural language processing techniques, specifically dense vector embeddings, can help semi-automate the contract review process and reduce time and costs for legal professionals reviewing deontic modalities in contracts. In this study, we create a domain-specific dataset and train both baseline and neural network models for contract sentence classification. This approach offers a more efficient and cost-effective solution for contract review, mimicking the work of a lawyer. Our approach achieves an accuracy of 0.90, showcasing its effectiveness in identifying and evaluating individual contract sentences.},
  archive      = {J_AIL},
  author       = {Graham, S. Georgette and Soltani, Hamidreza and Isiaq, Olufemi},
  doi          = {10.1007/s10506-023-09379-2},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {79-100},
  shortjournal = {Artif. Intell. Law},
  title        = {Natural language processing for legal document review: Categorising deontic modalities in contracts},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large scale benchmark for session-based recommendations on
the legal domain. <em>AIL</em>, <em>33</em>(1), 43–78. (<a
href="https://doi.org/10.1007/s10506-023-09378-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of legal documents in various formats and their dispersion across multiple courts present a significant challenge for users seeking precise matches to their information requirements. Despite notable advancements in legal information retrieval systems, research into legal recommender systems remains limited. A plausible factor contributing to this scarcity could be the absence of extensive publicly accessible datasets or benchmarks. While a few studies have emerged in this field, a comprehensive analysis of the distinct attributes of legal data that influence the design of effective legal recommenders is notably absent in the current literature. This paper addresses this gap by initially amassing a comprehensive session-based dataset from Jusbrasil, one of Brazil’s largest online legal platforms. Subsequently, we scrutinize and discourse key facets of legal session-based recommendation data, including session duration, types of recommendable legal artifacts, coverage, and popularity. Furthermore, we introduce the first session-based recommendation benchmark tailored to the legal domain, shedding light on the performance and constraints of several renowned session-based recommendation approaches. These evaluations are based on real-world data sourced from Jusbrasil.},
  archive      = {J_AIL},
  author       = {Domingues, Marcos Aurélio and de Moura, Edleno Silva and Marinho, Leandro Balby and da Silva, Altigran},
  doi          = {10.1007/s10506-023-09378-3},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {43-78},
  shortjournal = {Artif. Intell. Law},
  title        = {A large scale benchmark for session-based recommendations on the legal domain},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating legal event and context information for chinese
similar case analysis. <em>AIL</em>, <em>33</em>(1), 1–42. (<a
href="https://doi.org/10.1007/s10506-023-09377-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar case analysis (SCA) is an essential topic in legal artificial intelligence, serving as a reference for legal professionals. Most existing works treat SCA as a traditional text classification task and ignore some important legal elements that affect the verdict and case similarity, like legal events, and thus are easily misled by semantic structure. To address this issue, we propose a Legal Event-Context Model named LECM to improve the accuracy and interpretability of SCA based on Chinese legal corpus. The event-context integration mechanism, which is an essential component of the LECM, is proposed to integrate the legal event and context information based on the attention mechanism, enabling legal events to be associated with their corresponding relevant contexts. We introduce an event detection module to obtain the legal event information, which is pre-trained on a legal event detection dataset to avoid labeling events manually. We conduct extensive experiments on two SCA tasks, i.e., similar case matching (SCM) and similar case retrieval (SCR). Compared with baseline models, LECM is validated by about 13% and 11% average improvement in terms of mean average precision and accuracy respectively, for SCR and SCM tasks. These results indicate that LECM effectively utilizes event-context knowledge to enhance SCA performance and its potential application in various legal document analysis tasks.},
  archive      = {J_AIL},
  author       = {Dan, Jingpei and Xu, Lanlin and Wang, Yuming},
  doi          = {10.1007/s10506-023-09377-4},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Law},
  title        = {Integrating legal event and context information for chinese similar case analysis},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="air---32">AIR - 32</h2>
<ul>
<li><details>
<summary>
(2025). Image-based deep learning for smart digital twins: A review.
<em>AIR</em>, <em>58</em>(5), 1–36. (<a
href="https://doi.org/10.1007/s10462-024-11002-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Digital Twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation, enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, the Deep Learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe, learn, and control system behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opportunities for developing new image-based DL approaches to develop robust SDTs are provided. This includes the potential for using generative models for data augmentation, developing multi-modal DL models, and exploring the integration of DL models with other technologies, including Fifth Generation (5 G), edge computing, and the Internet of Things (IoT). In this paper, we describe the image-based SDTs, which enable broader adoption of the Digital Twins (DTs) paradigms across a broad spectrum of areas and the development of new methods to improve the abilities of SDTs in replicating, predicting, and optimizing the behavior of complex systems.},
  archive      = {J_AIR},
  author       = {Islam, Md Ruman and Subramaniam, Mahadevan and Huang, Pei-Chi},
  doi          = {10.1007/s10462-024-11002-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Image-based deep learning for smart digital twins: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural combinatorial optimization with reinforcement
learning in industrial engineering: A survey. <em>AIR</em>,
<em>58</em>(5), 1–37. (<a
href="https://doi.org/10.1007/s10462-024-11045-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent trends, machine learning is widely used to support decision-making in various domains and industrial operations. Because of the increasing complexity of modern industries, industrial engineering aims not only to increase cost-effectiveness and productivity but also to consider sustainability, resilience, and human centricity, resulting in many-objective, constrained, and stochastic operations research. Based on the above stringent requirements, combinatorial optimization (CO) problems are thus developed to support the complicated decision-making process in operations research. Due to the computational complexity of exact algorithms and the uncertain solution quality of heuristic methods, there is a growing trend to leverage the power of machine learning in solving CO problems, known as neural combinatorial optimization (NCO), where reinforcement learning (RL) is the core to achieve the sequential decision support. This survey study provides a comprehensive investigation of the theories and recent advancements in applying RL to solve hard CO problems, such as vehicle routing, bin packing, assignment, scheduling, and planning problems, and, in addition, summarizes the applications of neural combinatorial optimization with reinforcement learning (NCO-RL). The detailed review found that although the research domain of NCO-RL is still under-explored, its research potential has been proven to address environmental sustainability, adaptability, and human factors. Last but not least, the technical challenges and opportunities of the NCO-RL to embrace the industry 5.0 paradigm are discussed.},
  archive      = {J_AIR},
  author       = {Chung, K. T. and Lee, C. K. M. and Tsang, Y. P.},
  doi          = {10.1007/s10462-024-11045-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Neural combinatorial optimization with reinforcement learning in industrial engineering: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalogram based performance comparison of deep learning
architectures for dysarthric speech detection. <em>AIR</em>,
<em>58</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10462-024-11085-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dysarthria, a speech disorder commonly associated with neurological conditions, poses challenges in early detection and accurate diagnosis. This study addresses these challenges by implementing preprocessing steps, such as noise reduction and normalization, to enhance the quality of raw speech signals and extract relevant features. Scalogram images generated through wavelet transform effectively capture the time-frequency characteristics of the speech signal, offering a visual representation of the spectral content over time and providing valuable insights into speech abnormalities related to dysarthria. Fine-tuned deep learning models, including pre-trained convolutional neural network (CNN) architectures like VGG19, DenseNet-121, Xception, and a modified InceptionV3, were optimized with specific hyperparameters using training and validation sets. Transfer learning enables these models to adapt features from general image classification tasks to classify dysarthric speech signals better. The study evaluates the models using two public datasets TORGO and UA-Speech and a third dataset collected by the authors and verified by medical practitioners. The results reveal that the CNN models achieve an accuracy (acc) range of 90% to 99%, an F1-score range of 0.95 to 0.99, and a recall range of 0.96 to 0.99, outperforming traditional methods in dysarthria detection. These findings highlight the effectiveness of the proposed approach, leveraging deep learning and scalogram images to advance early diagnosis and healthcare outcomes for individuals with dysarthria.},
  archive      = {J_AIR},
  author       = {Shabber, Shaik Mulla and Sumesh, E. P. and Ramachandran, Vidhya Lavanya},
  doi          = {10.1007/s10462-024-11085-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Scalogram based performance comparison of deep learning architectures for dysarthric speech detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Psychological and physiological computing based on
multi-dimensional foot information. <em>AIR</em>, <em>58</em>(5), 1–56.
(<a href="https://doi.org/10.1007/s10462-024-11087-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the population ages, utilizing foot information to continuously monitor the physiological and psychological health status of the elderly is emerging as a pivotal tool for meeting this crucial societal demand. However, few reviews explored how multi-dimensional foot data has been integrated into physiological and psychological computing. This review is essential as it fills a critical knowledge gap in understanding the connections between physiological and psychological disorders and various components of foot information. To identify relevant literature, a thorough search was conducted across IEEE, DBLP, Elsevier, Springer, Google Scholar, and PubMed, initially yielding 2386 publications. After multiple rounds of systematic filtering, 404 publications were selected for in-depth analysis. This review examines (1) the mechanisms linking foot information to human physiological and psychological conditions, (2) the monitoring devices that collect diverse foot-based data, (3) the datasets correlating diseases with multiple foot data, (4) the prevalent feature engineering of different foot data, and (5) the cutting-edge machine and deep learning algorithms for diseases analysis. It also provides insights into future developments in foot information health monitoring for psychological and physiological computing.},
  archive      = {J_AIR},
  author       = {Li, Shengyang and Yao, Huilin and Peng, Ruotian and Ma, Yuanjun and Zhang, Bowen and Zhao, Zhiyao and Zhang, Jincheng and Chen, Siyuan and Wu, Shibin and Shu, Lin},
  doi          = {10.1007/s10462-024-11087-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Psychological and physiological computing based on multi-dimensional foot information},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep crowd anomaly detection: State-of-the-art, challenges,
and future research directions. <em>AIR</em>, <em>58</em>(5), 1–111. (<a
href="https://doi.org/10.1007/s10462-024-11092-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd anomaly detection is one of the most popular topics in computer vision in the context of smart cities. A plethora of deep learning methods have been proposed that generally outperform other machine learning solutions. Our review primarily discusses algorithms that were published in mainstream conferences and journals between 2020 and 2022. We present datasets that are typically used for benchmarking, produce a taxonomy of the developed algorithms, and discuss and compare their performances. Our main findings are that the heterogeneities of pre-trained convolutional models have a negligible impact on crowd video anomaly detection performance. We conclude our discussion with fruitful directions for future research.},
  archive      = {J_AIR},
  author       = {Sharif, Md. Haidar and Jiao, Lei and Omlin, Christian W.},
  doi          = {10.1007/s10462-024-11092-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-111},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep crowd anomaly detection: State-of-the-art, challenges, and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Musical heritage historical entity linking. <em>AIR</em>,
<em>58</em>(5), 1–41. (<a
href="https://doi.org/10.1007/s10462-024-11102-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linking named entities occurring in text to their corresponding entity in a Knowledge Base (KB) is challenging, especially when dealing with historical texts. In this work, we introduce Musical Heritage named Entities Recognition, Classification and Linking (mhercl), a novel benchmark consisting of manually annotated sentences extrapolated from historical periodicals of the music domain. mhercl contains named entities under-represented or absent in the most famous KBs. We experiment with several State-of-the-Art models on the Entity Linking (EL) task and show that mhercl is a challenging dataset for all of them. We propose a novel unsupervised EL model and a method to extend supervised entity linkers by using Knowledge Graphs (KGs) to tackle the main difficulties posed by historical documents. Our experiments reveal that relying on unsupervised techniques and improving models with logical constraints based on KGs and heuristics to predict NIL entities (entities not represented in the KB of reference) results in better EL performance on historical documents.},
  archive      = {J_AIR},
  author       = {Graciotti, Arianna and Lazzari, Nicolas and Presutti, Valentina and Tripodi, Rocco},
  doi          = {10.1007/s10462-024-11102-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Musical heritage historical entity linking},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning algorithms in ischemic stroke
detection, segmentation, and classification. <em>AIR</em>,
<em>58</em>(5), 1–48. (<a
href="https://doi.org/10.1007/s10462-025-11119-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ischemic, one of the fatal diseases characterized by insufficient blood supply to tissues poses a significant global health burden, necessitating the development of robust diagnostic and classification methodologies. Timely identification, intervention, and treatment are essential to reduce associated risk factors. Modern machine learning methods like deep learning and neural networks are being successfully employed on medical images to detect and segment the region of interest for various diseases where the performance of these computational methods is improving daily and for various tasks has surpassed natural intelligence. This success has convinced medical practitioners to trust computational methods and incorporate computer-based solutions into their clinical practices. It is, therefore, essential to examine the available solutions critically by considering their strengths and weaknesses to establish their trust and clinical applicability. In the context of the above-mentioned task, this work focuses on two aspects: first, a broad review has been done for Ischemic stroke prognostication using various brain-imaging biomarkers via diverse deep learning frameworks, and second, the reviewed works are categorized based on their computational approach employed for Ischemic stroke detection, segmentation, and classification. Finally, this work presents recent advances and future research directions to invent high-performance methods. It was concluded that recent advancements in ischemic stroke detection have achieved 85–98% accuracy using CNNs and transformer-based models with separate imaging, clinical, and molecular data, though combined analysis remains largely underexplored. Integrating vascular imaging, clinical signs, and proteomic data can enhance real-time monitoring. However, challenges persist in unifying diverse parameters, necessitating advanced methodologies such as transfer learning, multi-task learning, advanced transformers, federated learning, and standardized protocols. These findings pave the way for improved diagnostics, treatment, and outcomes in stroke management.},
  archive      = {J_AIR},
  author       = {Kousar, Tanzeela and Rahim, Mohd Shafry Mohd and Iqbal, Sajid and Yousaf, Fatima and Sanaullah, Muhammad},
  doi          = {10.1007/s10462-025-11119-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning algorithms in ischemic stroke detection, segmentation, and classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on persian question answering systems: From
traditional to modern approaches. <em>AIR</em>, <em>58</em>(5), 1–27.
(<a href="https://doi.org/10.1007/s10462-025-11122-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering systems (QAS) are designed to answer questions in natural language. The objective of these types of systems is to reduce the user’s effort to manually check the retrieved documents to find the answer to the query in natural language and to create an accurate answer to the user’s query. In recent years, with the emergence of Large Language Models (LLMs), these systems have evolved significantly across different languages. However, the development of QAS in low resource languages such as Persian, while progressing, still faces unique challenges. Development of these systems has become problematic in Persian language due to the lack of comprehensive processing tools, limited question answering datasets, and specific challenges of this language. The current study provides a brief explanation of these systems’ evolution from traditional architectures to LLM-based approaches, their classification, the challenges specific to Persian language, existing question-answering datasets and language models, and studies conducted concerning Persian QAS.},
  archive      = {J_AIR},
  author       = {Jolfaei, Safoura Aghadavoud and Mohebi, Azadeh},
  doi          = {10.1007/s10462-025-11122-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on persian question answering systems: From traditional to modern approaches},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dmixnet: A dendritic multi-layered perceptron architecture
for image recognition. <em>AIR</em>, <em>58</em>(5), 1–22. (<a
href="https://doi.org/10.1007/s10462-025-11123-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image recognition, the all-MLP architecture (MLP-Mixer) shows superior performance. However, the current MLP-Mixer is solely based on fully connected layers. The nonlinear capability of fully connected layers is relatively weak, and their simple stacked structure has limitations under complex conditions. Therefore, inspired by the diversity of neurons in the human brain, we propose an innovative DMixNet, a dendritic multi-layered perceptron architecture. Rooted in the theory of dendritic neurons from neuroscience, we propose a dendritic neural unit (DNU) that enhances DMixNet with stronger biological interpretability and more robust nonlinear processing capabilities. The flexibility of dendritic structures allows the DNU to adjust its architecture to achieve different functionalities. Based on the DNU, we propose a novel channel fusion network $$\text {DNU}_\text {E}$$ and a dendritic classifier $$\text {DNU}_\text {C}$$ . The $$\text {DNU}_\text {E}$$ substitutes the traditional two fully connected layers as the channel mixer, constructing a dendritic mixer layer to enhance the fusion capability of channel information within the entire framework. Meanwhile, the $$\text {DNU}_\text {C}$$ replaces the traditional linear classifier, effectively improving the model’s classification performance. Experimental results demonstrate that DMixNet achieves improvements of 2.13%, 4.79%, 4.71%, 23.14% on the CIFAR-10, CIFAR-100, Tiny-ImageNet and COIL-100 benchmark image recognition datasets, respectively, as well as a 14.78% enhancement on the medical image classification dataset PathMNIST, outperforming other state-of-the-art architectures. Code is available at https://github.com/KarilynXu/DMixNet .},
  archive      = {J_AIR},
  author       = {Xu, Weixiang and Song, Yaotong and Gupta, Shubham and Jia, Dongbao and Tang, Jun and Lei, Zhenyu and Gao, Shangce},
  doi          = {10.1007/s10462-025-11123-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dmixnet: A dendritic multi-layered perceptron architecture for image recognition},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic optimization algorithms for multi-area
economic dispatch of power systems: Part II—a comparative study.
<em>AIR</em>, <em>58</em>(5), 1–51. (<a
href="https://doi.org/10.1007/s10462-025-11125-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Area Economic Dispatch (MAED) plays an important role in the operation and planning of power systems. In Part I of this series, we have summarized various optimization techniques to the MAED problem comprehensively, showing clearly that metaheuristic optimization algorithms (MOAs) have become the dominant approach for solving this problem due to their ease of application and powerful search capability. Although many different types of MOAs have been proposed, there is no study on the comprehensive evaluation, comparison and recommendation of different MOAs for the MAED problem. In this part, we selected 32 algorithms including differential evolution, particle swarm optimization, teaching–learning based algorithm, JAYA algorithm, and their advanced variants to evaluate and compare their performance on the eleven reported MAED cases summarized in Part I of this series. The comparative study was comprehensively conducted based on various performance criteria including solution quality, convergence, robustness, computational efficiency, and statistical analysis. The comparisons reveal that the DE series is the most competitive overall. Nevertheless, there is no single algorithm that ranks in the top three on all cases. This study can provide a practical reference and applicability recommendation for the selection of MOAs for solving the MAED problem.},
  archive      = {J_AIR},
  author       = {Wang, Yang and Xiong, Guojiang},
  doi          = {10.1007/s10462-025-11125-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Metaheuristic optimization algorithms for multi-area economic dispatch of power systems: Part II—a comparative study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on EEG-based multimodal learning for emotion
recognition. <em>AIR</em>, <em>58</em>(5), 1–63. (<a
href="https://doi.org/10.1007/s10462-025-11126-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from electroencephalography (EEG) signals is crucial for human–computer interaction yet poses significant challenges. While various techniques exist for detecting emotions through EEG signals, contemporary studies have explored the combination of EEG signals with other modalities. However, the field is still rapidly evolving, and new advancements are constantly being made. Comprehensive research is essential by distilling all factors in one manuscript to stay up-to-date with the latest research findings. This review offers an overview of multimodal leaning in EEG-based emotion recognition and discusses current literature in this domain from 2017 to 2024. Three primary challenges addressed are the fusion algorithm, representation of different modalities, and classification scheme. The review thoroughly explores the challenges of fusion algorithms, representation of different modalities, and classification schemes through empirical studies, offering a detailed analysis of their effectiveness. The approach of fusion algorithms is compared and evaluated based on convention and deep learning fusion methods. The research results show that poor performance is attributed to a lack of rigor and inadequate methods to identify correlated patterns across modalities to create a unified representation for experiments. This indicates a need for more thorough analysis and integration of data in future studies. When more than two modalities are involved, it becomes increasingly important to consider different aspects of classification schemes, such as the number of features and model selection. However, designing a classification scheme without considering the number of parameters and emotional categories may compromise the accuracy of classification. To aid readers in understanding the findings better, the results of different classification schemes and their corresponding accuracies are summarized. The tables in this draft display the fusion algorithms researchers utilize and evaluate the effectiveness of selected modalities, providing valuable insights for decision-making. Key contributions include a systematic survey of EEG features, an exploration of EEG integration with behavioral modalities, an investigation of fusion methods, and an overview of key challenges and future research directions in implementing multimodal emotion recognition systems.},
  archive      = {J_AIR},
  author       = {Pillalamarri, Rajasekhar and Shanmugam, Udhayakumar},
  doi          = {10.1007/s10462-025-11126-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on EEG-based multimodal learning for emotion recognition},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A synergetic intuitionistic fuzzy model combining AHP,
entropy, and ELECTRE for data fabric solution selection. <em>AIR</em>,
<em>58</em>(5), 1–54. (<a
href="https://doi.org/10.1007/s10462-025-11128-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the ongoing digital transformation, enterprises face the challenge of managing ever-expanding volumes of data from multiple sources and diverse structures. Semantic data fabric emerges as a promising solution, offering an innovative approach to integrate data resources from various channels and produce meaningful insights. The selection of an appropriate data fabric solution has become a focal point amidst burgeoning data lakes and silos, garnering international attention. This research aims to precisely evaluate potential data fabric solutions using an innovative synergetic intuitionistic fuzzy evaluation model. We propose a hybrid approach, IF-AHP-Entropy-ELECTRE, which integrates the analytic hierarchy process (AHP), entropy, and elimination et choix traduisant la réalité (ELECTRE) techniques within the framework of intuitionistic fuzzy (IF) sets. This model is utilized to a data fabric solution selection (DFSS) issue for an appliance company, identifying the optimal solution based on its superior performance in foundational technology, real-time analytics, and customizable features. The effectiveness and adaptability of this approach stem from a novel hierarchical evaluative criteria system encompassing technology, capability, cost, and security. The criteria weights, derived from IF-AHP-Entropy, reflect both subjective and objective judgments of decision-makers, while the ranking generated by IF-ELECTRE employs a piecewise scoring function and a unique distance measure, factoring in optimistic perspectives and cross-information. Through sensitivity and comparative analyses, our approach demonstrates enhanced robustness, precision, and adaptability in dynamic DFSS contexts when compared to traditional multicriteria decision-making methods, such as IF-WSM, IF-TOPSIS, and IF-ELECTRE. Specifically, our model provides a decision support system that combines extensive functionality with a user-friendly design, making it highly effective for DFSS challenges. This approach not only establishes a solid foundation for data integration in data management but also enhances business competitiveness and supports sustained growth in the digital economy.},
  archive      = {J_AIR},
  author       = {Zhou, Fang and Chen, Ting-Yu},
  doi          = {10.1007/s10462-025-11128-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A synergetic intuitionistic fuzzy model combining AHP, entropy, and ELECTRE for data fabric solution selection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel reinforcement learning-based multi-operator
differential evolution with cubic spline for the path planning problem.
<em>AIR</em>, <em>58</em>(5), 1–56. (<a
href="https://doi.org/10.1007/s10462-025-11129-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning in autonomous driving systems remains a critical challenge, requiring algorithms capable of generating safe, efficient, and reliable routes. Existing state-of-the-art methods, including graph-based and sampling-based approaches, often produce sharp, suboptimal paths and struggle in complex search spaces, while trajectory-based algorithms suffer from high computational costs. Recently, meta-heuristic optimization algorithms have shown effective performance but often lack learning ability due to their inherent randomness. This paper introduces a unified benchmarking framework, named Reda’s Path Planning Benchmark 2024 (RP2B-24), alongside two novel reinforcement learning (RL)-based path-planning algorithms: Q-Spline Multi-Operator Differential Evolution (QSMODE), utilizing Q-learning (Q-tables), and Deep Q-Spline Multi-Operator Differential Evolution (DQSMODE), based on Deep Q-networks (DQN). Both algorithms are integrated under a single framework and enhanced with cubic spline interpolation to improve path smoothness and adaptability. The proposed RP2B-24 library comprises 50 distinct benchmark problems, offering a comprehensive and generalizable testing ground for diverse path-planning algorithms. Unlike traditional approaches, RL in QSMODE/DQSMODE is not merely a parameter adjustment method but is fully utilized to generate paths based on the accumulated search experience to enhance path quality. QSMODE/DQSMODE introduces a unique self-training update mechanism for the Q-table and DQN based on candidate paths within the algorithm’s population, complemented by a secondary update method that increases population diversity through random action selection. An adaptive RL switching probability dynamically alternates between these Q-table update modes. DQSMODE and QSMODE demonstrated superior performance, outperforming 22 state-of-the-art algorithms, including the IMODEII. The algorithms ranked first and second in the Friedman test and SNE-SR ranking test, achieving scores of 99.2877 (DQSMODE) and 93.0463 (QSMODE), with statistically significant results in the Wilcoxon test. The practical applicability of the algorithm was validated on a ROS-based system using a four-wheel differential drive robot, which successfully followed the planned paths in two driving scenarios, demonstrating the algorithm’s feasibility and effectiveness for real-world scenarios. The source code for the proposed benchmark and algorithm is publicly available for further research and experimentation at: https://github.com/MohamedRedaMu/RP2B24-Benchmark and https://github.com/MohamedRedaMu/QSMODEAlgorithm .},
  archive      = {J_AIR},
  author       = {Reda, Mohamed and Onsy, Ahmed and Haikal, Amira Y. and Ghanbari, Ali},
  doi          = {10.1007/s10462-025-11129-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel reinforcement learning-based multi-operator differential evolution with cubic spline for the path planning problem},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of belief entropies: From the perspective of
evidential neural network. <em>AIR</em>, <em>58</em>(5), 1–34. (<a
href="https://doi.org/10.1007/s10462-025-11130-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Dempster-Shafer’s theory, the belief entropy for total uncertainty measure of mass function has attracted the interest of many researchers in recent years. Although various belief entropies can meet some basic requirements, how to judge the performance of belief entropies is still an open issue. This paper proposes a novel evidential neural network (ENN) classifier to evaluate different belief entropies in practical application. Driven by the least commitment principle (LCP), the maximum entropy is integrated into the traditional divergence-based loss function. The proposed loss function consists of divergence and maximum entropy parts, which considers not only the distribution difference but also the degree of approaching the maximum entropy. Some classification experiments are conducted in 7 real-world datasets to validate the effectiveness of the proposed evaluation method.},
  archive      = {J_AIR},
  author       = {Mao, Kun and Wang, Yanni and Zhou, Wen and Ye, Jiangang and Fang, Bin},
  doi          = {10.1007/s10462-025-11130-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Evaluation of belief entropies: From the perspective of evidential neural network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An actor-critic based recommender system with context-aware
user modeling. <em>AIR</em>, <em>58</em>(5), 1–40. (<a
href="https://doi.org/10.1007/s10462-025-11134-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems empower users with tailored service assistance by learning about their interactions with systems and recommending items based on their preferences and interests. Typical recommender systems view the recommendation process as a static procedure disregarding the fact that users’ preferences are changed over time. Reinforcement learning (RL) approaches are the most advanced and recent techniques used by researchers to handle challenges where the user’s interest is captured by their most recent interactions with the system. However, most of the recent research on RL-based recommender systems focuses on simply the user’s recent interactions to generate the recommendations without taking into account the context of the user in which these interactions occur. The context has a great impact on users’ interests, behaviors, and ratings e.g., user mood, time, day type, companion, social circle, and location. In this paper, we propose a context-aware deep reinforcement learning-based recommender system focusing on context-specific state modeling methods. In this approach, states are designed based on the user’s most recent context. In parallel, a list-wise version of the context-aware recommender agent is also proposed, in which a list of items is recommended to users at each step of interaction based on their context. The findings of the study indicate that modeling users’ preferences in combination with contextual variables improves the performance of RL-based recommender systems. Furthermore, we evaluate the proposed method on context-based datasets in an offline environment. The performance in terms of evaluation measures optimally indicates the worth of the proposed method in comparison with existing studies. More precisely, the highest Presicion@5, MAP@10, and NDCG@10 of the context-aware recommender agent are 77%, 76%, and 74% respectively.},
  archive      = {J_AIR},
  author       = {Bukhari, Maryam and Maqsood, Muazzam and Adil, Farhan},
  doi          = {10.1007/s10462-025-11134-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An actor-critic based recommender system with context-aware user modeling},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A critical review of artificial intelligence based
techniques for automatic prediction of cephalometric landmarks.
<em>AIR</em>, <em>58</em>(5), 1–56. (<a
href="https://doi.org/10.1007/s10462-025-11135-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic cephalometric landmark detection has emerged as a pivotal area of research that combines medical imaging, computer vision, and orthodontics. The identification of cephalometric landmarks is of utmost importance in the field of orthodontics, as it contributes significantly to the process of diagnosing and planning treatments, as well as conducting research on craniofacial aspects. This practice holds the potential to improve clinical decision-making and ultimately increase the outcomes for patients. This work explores a wide range of strategies, encompassing both traditional edge-based methods and advanced deep learning approaches. The study leveraged various academic publication databases like IEEEXplore, ScienceDirect, arXiv, Springer and PubMed to thoroughly search for articles related to automatic cephalometric landmark detection. Additionally, other pertinent publications were acquired from credible sources like Google Scholar and Wiley databases. Screening the articles relied on three selection criteria: (a) publication titles, abstracts, literature reviews, (b) cephalometric radiograph datasets suitable for 2D landmarking, and (c) studies conducted over different time periods were employed to gain a comprehensive understanding of the evolution of methodologies used in landmark prediction to identify the most relevant papers for this review. The initial electronic database search identified 268 papers on landmark detection. A total of 118 publications were selected and incorporated in the present study after a meticulous screening process. Performance analysis was conducted on studies that reported Successful Detection Rates (SDRs) within different clinically accepted precision ranges, Mean Radial Error (MRE) with Standard Deviation (SD) between manually annotated and automated landmarks as outcomes. Bar graphs and custom combination plots were utilized to analyse the correlations among different methodologies employed and their evaluation metrics outcomes. The performance comparison results indicate that Deep Learning techniques showed superior accuracy in automating 2D cephalometric landmarks compared to other conventional and Machine Learning approaches. Recently, more advanced Deep Learning algorithms have been developed to improve the accuracy of automatic landmark prediction.},
  archive      = {J_AIR},
  author       = {Neeraja, R. and Anbarasi, L. Jani},
  doi          = {10.1007/s10462-025-11135-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A critical review of artificial intelligence based techniques for automatic prediction of cephalometric landmarks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum deep learning in neuroinformatics: A systematic
review. <em>AIR</em>, <em>58</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s10462-025-11136-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroinformatics involves replicating and detecting intricate brain activities through computational models, where deep learning plays a foundational role. Our systematic review explores quantum deep learning (QDL), an emerging deep learning sub-field, to assess whether quantum-based approaches outperform classical approaches in brain data learning tasks. This review is a pioneering effort to compare these deep learning domains. In addition, we survey neuroinformatics and its various subdomains to understand the current state of the field and where QDL stands relative to recent advancements. Our statistical analysis of tumor classification studies (n = 16) reveals that QDL models achieved a mean accuracy of 0.9701 (95% CI 0.9533–0.9868), slightly outperforming classical models with a mean accuracy of 0.9650 (95% CI 0.9475–0.9825). We observed similar trends across Alzheimer’s diagnosis, stroke lesion detection, cognitive state monitoring, and brain age prediction, with QDL demonstrating better performance in metrics such as F1-score, dice coefficient, and RMSE. Our findings, paired with prior documented quantum advantages, highlight QDL’s promise in healthcare applications as quantum technology evolves. Our discussion outlines existing research gaps with the intent of encouraging further investigation in this developing field.},
  archive      = {J_AIR},
  author       = {Orka, Nabil Anan and Awal, Md. Abdul and Liò, Pietro and Pogrebna, Ganna and Ross, Allen G. and Moni, Mohammad Ali},
  doi          = {10.1007/s10462-025-11136-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum deep learning in neuroinformatics: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging generative AI synthetic and social media data for
content generalizability to overcome data constraints in vision deep
learning. <em>AIR</em>, <em>58</em>(5), 1–24. (<a
href="https://doi.org/10.1007/s10462-025-11137-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing deep learning models across diverse content types is a persistent challenge in domains like facial emotion recognition (FER), where datasets often fail to reflect the wide range of emotional responses triggered by different stimuli. This study addresses the issue of content generalizability by comparing FER model performance between models trained on video data collected in a controlled laboratory environment, data extracted from a social media platform (YouTube), and synthetic data generated using Generative Adversarial Networks. The videos focus on facial reactions to advertisements, and the integration of these different data sources seeks to address underrepresented advertisement genres, emotional reactions, and individual diversity. Our FER models leverage Convolutional Neural Networks Xception architecture, which is fine-tuned using category based sampling. This ensures training and validation data represent diverse advertisement categories, while testing data includes novel content to evaluate generalizability rigorously. Precision–recall curves and ROC-AUC metrics are used to assess performance. Results indicate a 7% improvement in accuracy and a 12% increase in precision–recall AUC when combining real-world social media and synthetic data, demonstrating reduced overfitting and enhanced content generalizability. These findings highlight the effectiveness of integrating synthetic and real-world data to build FER systems that perform reliably across more diverse and representative content.},
  archive      = {J_AIR},
  author       = {Alipour, Panteha and Gallegos, Erika},
  doi          = {10.1007/s10462-025-11137-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Leveraging generative AI synthetic and social media data for content generalizability to overcome data constraints in vision deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy generator based
low-light enhancement model for referenced image datasets. <em>AIR</em>,
<em>58</em>(5), 1–31. (<a
href="https://doi.org/10.1007/s10462-025-11138-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing is a rapidly evolving research field with diverse applications across science and technology, including biometric systems, surveillance, traffic signal control and medical imaging. Digital images taken in low-light conditions are often affected by poor contrast and pixel detail, leading to uncertainty. Although various fuzzy based techniques have been proposed for low-light image enhancement, there remains a need for a model that can manage greater uncertainty while providing better structural information. To address this, an interval-valued intuitionistic fuzzy generator is proposed to develop an advanced low-light image enhancement model for referenced image datasets. The enhancement process involves a structural similarity index measure (SSIM) based optimization approach with respect to the parameters of the generator. For experimental validation, the Low-Light (LOL), LOLv2-Real and LOLv2-Synthetic benchmark datasets are utilized. The results are compared with several existing techniques using quality metrics such as SSIM, peak signal-to-noise ratio, absolute mean brightness error, mean absolute error, root mean squared error, blind/referenceless image spatial quality evaluator and naturalness image quality evaluator, demonstrating the superiority of the proposed model. Ultimately, the model’s performance is benchmarked against state-of-the-art methods, highlighting its enhanced efficiency.},
  archive      = {J_AIR},
  author       = {Selvam, Chithra and Sundaram, Dhanasekar},
  doi          = {10.1007/s10462-025-11138-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Interval-valued intuitionistic fuzzy generator based low-light enhancement model for referenced image datasets},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way decision model for multi-granular support
intuitionistic fuzzy rough sets based on overlap functions.
<em>AIR</em>, <em>58</em>(5), 1–44. (<a
href="https://doi.org/10.1007/s10462-025-11139-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision-making provides an effective framework for addressing uncertainty, aligning closely with human cognitive decision patterns. This paper proposes a novel three-way decision model based on multi-granular support intuitionistic fuzzy rough sets, integrating n-dimensional overlap and grouping functions. The model constructs optimistic and pessimistic upper and lower approximations to optimize decision rules and introduces score and precision functions for ranking. To validate the model, a consumer decision-making algorithm was developed and applied to empirical data. The results demonstrate that the proposed model effectively narrows decision boundary regions, enhances decision-making precision, and supports decision-making in complex multi-attribute scenarios. This study not only advances rough set theory but also offers practical tools for addressing real-world uncertainty in decision-making.},
  archive      = {J_AIR},
  author       = {Yu, Peng and Zhao, Xiyue},
  doi          = {10.1007/s10462-025-11139-4},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A three-way decision model for multi-granular support intuitionistic fuzzy rough sets based on overlap functions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An assessment framework for explainable AI with applications
to cybersecurity. <em>AIR</em>, <em>58</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s10462-025-11141-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several explainable AI methods are available, but there is a lack of a systematic comparison of such methods. This paper contributes in this direction, by providing a framework for comparing alternative explanations in terms of complexity and robustness. We exemplify our proposal on a real case study in the cybersecurity domain, namely, phishing website detection. In fact, in this domain explainability is a compelling issue because of its potential benefits for the detection of fraudulent attacks and for the design of efficient security defense mechanisms. For this purpose, we apply our methodology to the machine learning models obtained by analyzing a publicly available dataset containing features extracted from malicious and legitimate web pages. The experiments show that our methodology is quite effective in selecting the explainability method which is, at the same time, less complex and more robust.},
  archive      = {J_AIR},
  author       = {Calzarossa, Maria Carla and Giudici, Paolo and Zieni, Rasha},
  doi          = {10.1007/s10462-025-11141-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An assessment framework for explainable AI with applications to cybersecurity},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing dance movements using a mathematical model
based on optimized nature-inspired machine learning. <em>AIR</em>,
<em>58</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10462-025-11142-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recording dance movements nowadays becomes problematic due to complex recording procedures and unavoidable data loss caused by some resource elements, like bodily or clothing material composition. The task of filling in the missing data for the performed motion and retrieving the sequence as a whole becomes difficult due to the characteristics of physical motion, which include cinematographic perspectives that render the movements themselves non-linear. Previous works have indicated some level of success in loss motion recovery, but only for a short span. The first two-dimensional matrix computation paradigm lacks theoretical justification for the recovery of the non-linear motion information, which is a limitation. This issue has been addressed by developing a new enhanced model called the Machine Learning 2-Dimensional Matrix-Calculation (ML-2DMC), which is presumably designed to achieve the rehabilitation and recovery of human movement and dance. The proposed procedure takes advantage of the effectiveness of the machine learning algorithms and applies 2D matrix computation methods, permitting good results across a variety of experiments. A new method called fractal-chaotic map grey wolf optimizer (FCM-GWO) is introduced to optimize the parameters of ML-2DMC. This optimization itself increases the efficiency of the ML-2DMC model when it comes to the retrieval of complex movements of the processes involving dance. The paper gives experimental results validating the efficiency of the proposed approach against other methods, such as recurrent convolutional neural networks and other more sophisticated models and approaches incorporating multi-paradigm sensors and devices such as Kinect sensors along with low-rank matrix completion methods. The study shows that the ML-2DMC-FCM-GWO method effectively tackles the complexities of non-linear human motion and dance recovery, making a significant addition to the field of motion analysis and restoration.},
  archive      = {J_AIR},
  author       = {Song, Jing and Ding, Li},
  doi          = {10.1007/s10462-025-11142-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reconstructing dance movements using a mathematical model based on optimized nature-inspired machine learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based bridge maintenance management: A comprehensive
review. <em>AIR</em>, <em>58</em>(5), 1–43. (<a
href="https://doi.org/10.1007/s10462-025-11144-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent decades, the implementation of Artificial Intelligence (AI) across various industrial fields from automation to cybersecurity has been transformative. Whilst the implementations of linking AI and data sciences remain complex and thus limited, they both aim to harness data for actionable insights and future predictions. A research focal point in the application of AI in maintenance is crucial for the sustainability and efficiency of assets. Typically, in the civil infrastructure, there are significant benefits to be gained from AI-driven applications. This study reviews the implementation of the AI in bridge maintenance decision-making by conducting a review of literature on major works undertaken by researchers and analysing 102 scientific articles published from 2010 to 2023. Our literature review revealed an emerging trend in recent studies, focusing on the exploration of defect prognosis in bridge maintenance. However, upon further analysis, it becomes evident that there is a notable gap in the existing literature, in the studies related to performance-based prognostic maintenance strategies for bridges. This gap presents an opportunity for future research, one that could yield valuable insights in the field of bridge maintenance and asset management. The review also reveals the focus of the existing literature on defect identification by using the bridge imagery processing. While the AI’s potential in damage detection using bridge imagery is evident, challenges persist including the computational processing and data availability. This review of the literature includes a comprehensive overview of the current implementation of AI in bridge maintenance, highlighting limitations, challenges, and prospective directions.},
  archive      = {J_AIR},
  author       = {Shahrivar, Farham and Sidiq, Amir and Mahmoodian, Mojtaba and Jayasinghe, Sanduni and Sun, Zhiyan and Setunge, Sujeeva},
  doi          = {10.1007/s10462-025-11144-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-based bridge maintenance management: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy fusion mayfly algorithm on task offloading
and scheduling for IoT-based fog computing multi-tasks learning.
<em>AIR</em>, <em>58</em>(5), 1–46. (<a
href="https://doi.org/10.1007/s10462-025-11145-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Internet of Things (IoT) technology has accumulated a large amount of data, which needs to be stored, processed and deeply analyzed to meet the specific goals and needs of users. As an emerging computing model, Fog computing can allocate a large number of computing resources reasonably. In order to solve the problem of insufficient population diversity and low algorithm efficiency, Aiming at the task scheduling problem of Bag-of-Tasks(BoT) application in cloud and fog environment, a multi-strategy fusion Mayfly Algorithm was proposed. The method of improving the individual learning coefficient and the global learning coefficient is used to significantly improve the convergence speed, local search ability, and global search ability, and then the method of improving the social positive attraction coefficient is used to balance the development and exploration ability of the algorithm and help the algorithm to get rid of the local optimum. The main goal of the logarithm Mayfly Algorithm (lMA) is to complete the tasks of the IoT task package in the fog system efficiently with low cost in terms of reducing execution time and operating costs. The improved algorithms were compared with Mayfly Algorithm (MA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Tyrannosaurus Optimization Algorithm (TROA), Harris Hawks Optimization (HHO), Reptile Search Algorithm (RSA) and Red-Tailed Hawk Algorithm (RTH), and the results showed that lMA was significantly improved in terms of reducing processing time and operating cost. The performance of lMA is verified, and the results show that the model can not only save transmission energy consumption but also have good convergence.},
  archive      = {J_AIR},
  author       = {Sui, Xiao-Fei and Wang, Jie-Sheng and Zhang, Shi-Hui and Zhang, Si-Wen and Zhang, Yun-Hao},
  doi          = {10.1007/s10462-025-11145-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-strategy fusion mayfly algorithm on task offloading and scheduling for IoT-based fog computing multi-tasks learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attribute decision-making using q-rung orthopair fuzzy
zagreb index. <em>AIR</em>, <em>58</em>(5), 1–31. (<a
href="https://doi.org/10.1007/s10462-025-11149-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The q-rung orthopair fuzzy set (q-ROFS), an extension of intuitionistic and Pythagorean fuzzy sets, offers greater flexibility in representing vague information with two possible outcomes, yes or no. The fuzzy Zagreb index is an important graph parameter, widely used in fields such as network theory, spectral graph theory, mathematics, and molecular chemistry. In this paper, the first and second Zagreb indices for q-rung orthopair fuzzy graphs (q-ROFGs) are introduced, and bounds for these indices are established, including their behavior in regular q-ROFGs. Additionally, it is explored, how various graph operations such as union, Cartesian product, direct product, and lexicographical product affect the first Zagreb index. Furthermore, a new approach is presented that combines Multiple-Attribute Decision-Making (MADM) with graph-based models to improve decision-making, particularly in vaccine selection. The methodology constructs a bipartite graph for each attribute, where virologists assign membership and non-membership values to vaccines. The Zagreb index is used to measure the importance of each vaccine, and a weighted aggregation technique normalizes the scores. The final ranking is derived from a computed score function. The results demonstrate the effectiveness of the approach in providing a systematic and mathematically rigorous framework for multi-attribute decision-making, with rank correlation analysis confirming its robustness compared to existing methods such as q-ROF PROMETHEE, q-ROF VICOR, q-ROF TOPSIS, q-ROFWG, and q-ROFWA.},
  archive      = {J_AIR},
  author       = {Rao, Yongsheng and Kosari, Saeed and Hameed, Saira and Yousaf, Zulqarnain},
  doi          = {10.1007/s10462-025-11149-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-attribute decision-making using q-rung orthopair fuzzy zagreb index},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Csan: Cross-coupled semantic adversarial network for
cross-modal retrieval. <em>AIR</em>, <em>58</em>(5), 1–27. (<a
href="https://doi.org/10.1007/s10462-025-11152-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to correlate multimedia data by bridging the heterogeneity gap. Most cross-modal retrieval approaches learn a common subspace to project the multimedia data into the subspace for directly measuring the similarity. However, the existing cross-modal retrieval frameworks cannot fully capture the semantic consistency in the limited supervision information. In this paper, we propose a Cross-coupled Semantic Adversarial Network (CSAN) for cross-modal retrieval. The main structure of this approach is mainly composed of the generative adversarial network, i.e., each modality branch is equipped with a generator and a discriminator. Besides, a cross-coupled semantic architecture is designed to fully explore the correlation of paired heterogeneous samples. To be specific, we couple a forward branch with an inverse mapping and implement a weight-sharing strategy of the inverse mapping branch to the branch of another modality. Furthermore, a cross-coupled consistency loss is introduced to minimize the semantic gap between the representations of the inverse mapping branch and the forward branch. Extensive qualitative and quantitative experiments are conducted to evaluate the performance of the proposed approach. By comparing against the previous works, the experiment results demonstrate our approach outperforms state-of-the-art works.},
  archive      = {J_AIR},
  author       = {Li, Zhuoyi and Lu, Huibin and Fu, Hao and Meng, Fanzhen and Gu, Guanghua},
  doi          = {10.1007/s10462-025-11152-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Csan: Cross-coupled semantic adversarial network for cross-modal retrieval},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of artificial intelligence - based
algorithm towards fetal facial anomalies detection (2013–2024).
<em>AIR</em>, <em>58</em>(5), 1–49. (<a
href="https://doi.org/10.1007/s10462-025-11160-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review explores the growing need for AI-based algorithms in diagnosing fetal facial anomalies, which are often difficult to detect due to limitations in current imaging techniques like ultrasound and MRI. These challenges include low resolution, motion artifacts, and insufficient annotated data, which hinder early and accurate diagnosis. AI algorithms, particularly deep learning models like Convolutional Neural Networks (CNNs) and U-Net, offers significant potential to overcome these challenges by analyzing large datasets and improving image analysis. Early diagnosis of these anomalies is crucial for enabling timely interventions, personalized treatment plans, and better prenatal care. This study adopts a systematic review approach, to assess existing research on AI-based approaches for fetal facial anomaly detection. The review includes peer-reviewed studies from key biomedical databases like PubMed, IEEE Xplore, and ScienceDirect, focusing on the last 15 years. Studies that implemented AI techniques and manual techniques for detecting anomalies in prenatal images were considered. Among all models reviewed, CNNs and U-Net architectures were found to be the most effective. CNNs excel at classifying medical images, while U-Net is particularly powerful for image segmentation. These models have demonstrated high accuracy in identifying conditions such as cleft lip, palate, and micrognathia. The use of AI in clinical settings can greatly enhance the precision and efficiency of fetal anomaly detection, addressing current limitations in medical imaging. By integrating AI, particularly deep learning models, into clinical workflows, prenatal care can be transformed, allowing for earlier and more accurate diagnosis. This can lead to more personalized care, timely interventions, and ultimately improved health outcomes for affected individuals and their families.},
  archive      = {J_AIR},
  author       = {Sriraam, Natarajan and Chinta, Babu and Seshadri, Suresh and Suresh, Sudarshan},
  doi          = {10.1007/s10462-025-11160-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of artificial intelligence - based algorithm towards fetal facial anomalies detection (2013–2024)},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic brain MRI tumors segmentation based on deep fusion
of weak edge and context features. <em>AIR</em>, <em>58</em>(5), 1–25.
(<a href="https://doi.org/10.1007/s10462-025-11151-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors pose a significant health risk to humans. The edge boundaries in brain magnetic resonance imaging (MRI) are often blurred and poorly defined, which can easily result in inaccurate segmentation of lesion areas. To address these challenges, we proposed an Automatic Brain MRI Tumor Segmentation based on deep fusion of Weak Edge and Context features (AS-WEC). First, AS-WEC introduces the Otsu Double Threshold Weak Edges Adaptive Detection (Otsu-WD), which focuses on tumor edge information and differentiates between lesion edges and normal cerebral sulci and gyri. Second, an edge branching network based on the Gated Recurrent Unit (GRU) is constructed to fully preserve the edge context information of the lesion region. Finally, a maximum index fusion mechanism has been designed to incorporate a multilayer feature map, preventing the loss of edge details during the deep feature fusion process. The experimental results demonstrate that the Otsu-WD method outperforms the Canny and TEED algorithms in detecting brain MRI tumor edges. In brain MRI tumor segmentation, AS-WEC delivers a clearer visual segmentation effect compared to the classical UNet++ network and recent models like PVT-Former. On both datasets, AS-WEC demonstrated improvements across multiple metrics. The Dice averaged 92.96%, and the mIoU reached 93.12%, effectively validating the method’s efficacy in brain MRI tumor segmentation. Code and pre-trained models are available at https://github.com/DL-Segment/AS-WEC.git .},
  archive      = {J_AIR},
  author       = {Xiao, Leyi and Zhou, Baoxian and Fan, Chaodong},
  doi          = {10.1007/s10462-025-11151-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic brain MRI tumors segmentation based on deep fusion of weak edge and context features},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised constrained clustering: An in-depth
overview, ranked taxonomy and future research directions. <em>AIR</em>,
<em>58</em>(5), 1–127. (<a
href="https://doi.org/10.1007/s10462-024-11103-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a well-known unsupervised machine learning approach capable of automatically grouping discrete sets of instances with similar characteristics. Constrained clustering is a semi-supervised extension to this process that can be used when expert knowledge is available to indicate constraints that can be exploited. Well-known examples of such constraints are must-link (indicating that two instances belong to the same group) and cannot-link (two instances definitely do not belong together). The research area of constrained clustering has grown significantly over the years with a large variety of new algorithms and more advanced types of constraints being proposed. However, no unifying overview is available to easily understand the wide variety of available methods, constraints and benchmarks. To remedy this, this study presents in-detail the background of constrained clustering and provides a novel ranked taxonomy of the types of constraints that can be used in constrained clustering. In addition, it focuses on the instance-level pairwise constraints, and gives an overview of its applications and its historical context. Finally, it presents a statistical analysis covering 315 constrained clustering methods, categorizes them according to their features, and provides a ranking score indicating which methods have the most potential based on their popularity and validation quality. Finally, based upon this analysis, potential pitfalls and future research directions are provided.},
  archive      = {J_AIR},
  author       = {González-Almagro, Germán and Peralta, Daniel and De Poorter, Eli and Cano, José-Ramón and García, Salvador},
  doi          = {10.1007/s10462-024-11103-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-127},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Semi-supervised constrained clustering: An in-depth overview, ranked taxonomy and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum encoding whale optimization algorithm for global
optimization and adaptive infinite impulse response system
identification. <em>AIR</em>, <em>58</em>(5), 1–58. (<a
href="https://doi.org/10.1007/s10462-025-11120-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The whale optimization algorithm (WOA) is motivated by the predatory nature of bubble nets and mimics dwindling and encircling, bubble net persecuting, and randomized wandering and foraging actions to locate the expansive adequate value. However, the WOA has several deficiencies: inadequate resolution accuracy, sluggish convergence speed, susceptibility to search stagnation, and insufficient localized detection efficiency. A quantum encoding WOA (QWOA) is introduced for global optimization and adaptive infinite impulse response (IIR) system identification. The quantum encoding mechanism exploits the principle of a quantum bit to encode a search agent, which manipulates the state of an essential quantum bit and amends the location data. The quantum rotation gate modulates the quantum bit’s configuration, the quantum NOT gate accomplishes bit mutation and prohibits precocious convergence. The probability amplitude of the quantum bit represents the multistate superposition state of the search agent, which enriches the population diversity, advances individualized information, broadens the detection scope, inhibits premature convergence, facilitates estimation effectiveness, and promotes solution accuracy. The QWOA not only promptly locates the search scope nearest the most appropriate solution but also computes the spiral-shaped encircling route to promote predation diversification. Twenty-three benchmark functions, eight real-world engineering layouts, and adaptive IIR system identification are utilized to assess the QWOA’s feasibility and effectiveness. The experimental results reveal that QWOA successfully equalizes exploration and exploitation to accelerate convergence speed, ameliorate calculation accuracy, and strengthen stability and robustness.},
  archive      = {J_AIR},
  author       = {Zhang, Jinzhong and Liu, Wei and Zhang, Gang and Zhang, Tan},
  doi          = {10.1007/s10462-025-11120-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum encoding whale optimization algorithm for global optimization and adaptive infinite impulse response system identification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging machine learning and peptide design for cancer
treatment: A comprehensive review. <em>AIR</em>, <em>58</em>(5), 1–59.
(<a href="https://doi.org/10.1007/s10462-025-11148-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticancer peptides (ACPs) offer a promising alternative to traditional cancer therapies due to their specificity and reduced side effects. The development of ACPs using machine learning (ML) and deep learning (DL) follows a structured process, beginning with sequence collection from in vitro and in vivo experiments. Key features such as hydrophobicity and secondary structure are extracted, and classification models categorize peptides based on their properties. ML models predict anticancer effectiveness, followed by toxicity checks and Structure-Activity Relationship (SAR) analysis to ensure safety and efficacy, with validation tests confirming their activity. This review explores how the automated design of ACPs can be enhanced by leveraging advanced ML and DL techniques. These methods, with their ability to automate feature selection and activity prediction, have significantly improved the efficiency and accuracy of peptide discovery. This structured approach holds high potential to guide researchers in the automated design of ACPs, accelerating the discovery of effective peptides while ensuring safety. Special attention is given to new approaches such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs), which show promise in addressing key challenges like data imbalance and computational complexity. Moreover, we examine the latest published research to compare the performance of various ML models in ACP prediction. By considering these advancements and challenges, this review outlines future opportunities for improving the scalability and reliability of ACP discovery using AI-driven techniques. This structured approach underscores the transformative impact of automation in peptide design, pushing the boundaries of modern cancer therapy development.},
  archive      = {J_AIR},
  author       = {Rezaee, Khosro and Eslami, Hossein},
  doi          = {10.1007/s10462-025-11148-3},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bridging machine learning and peptide design for cancer treatment: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of artificial intelligence technology in the
study of anthropogenic earthquakes: A review. <em>AIR</em>,
<em>58</em>(5), 1–42. (<a
href="https://doi.org/10.1007/s10462-025-11157-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a crucial tool in the monitoring and research of anthropogenic earthquakes (AEs). Despite its utility, AEs monitoring faces significant challenges due to the intricate signal characteristics of seismic events, low signal-to-noise ratio (SNR) in data, and insufficient spatial coverage of monitoring networks, which complicate the effective deployment of AI technologies. This review systematically explores recent advancements in AI applications for identifying and classifying AEs, detecting weak signals, phase picking, event localization, and seismic risk analysis, while highlighting current issues and future developmental directions. Key challenges include accurately distinguishing specific anthropogenic seismic events due to their intricate signal patterns, limited model generalizability caused by constrained training datasets, and the lack of comprehensive models capable of handling event recognition, detection, and classification across diverse scenarios. Despite these obstacles, innovative approaches such as data-sharing platforms, transfer learning (TL), and hybrid AI models offer promising solutions to enhance AEs monitoring and improve predictive capabilities for induced seismic hazards. This review provides a scientific foundation to guide the ongoing development and application of AI technologies in AEs monitoring, forecasting, and disaster mitigation.},
  archive      = {J_AIR},
  author       = {Li, Jingwei and Zhai, Hongyu and Jiang, Changsheng and Wang, Ziang and Wang, Peng and Chang, Xu and Zhang, Yan and Wei, Yonggang and Si, Zhengya},
  doi          = {10.1007/s10462-025-11157-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence technology in the study of anthropogenic earthquakes: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="aism---6">AISM - 6</h2>
<ul>
<li><details>
<summary>
(2025). Model free feature screening for large scale and ultrahigh
dimensional survival data. <em>AISM</em>, <em>77</em>(1), 155–190. (<a
href="https://doi.org/10.1007/s10463-024-00912-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a novel perspective on feature screening in the analysis of high-dimensional right-censored large-p-large-N survival data. The research introduces a distributed feature screening method known as Aggregated Distance Correlation Screening (ADCS). The proposed screening framework involves expressing the distance correlation measure as a function of multiple component parameters, each of which can be estimated in a distributed manner using a natural U-statistic from data segments. By aggregating the component estimates, a final correlation estimate is obtained, facilitating feature screening. Importantly, this approach does not necessitate any specific model specification for responses or predictors and is effective with heavy-tailed data. The study establishes the consistency of the proposed aggregated correlation estimator $$\widetilde{\omega }_{j}$$ under mild conditions and demonstrates the sure screening property of the ADCS. Empirical results from both simulated and real datasets confirm the efficacy and practicality of the ADCS approach proposed in this paper.},
  archive      = {J_AISM},
  author       = {Pan, Yingli and Wang, Haoyu and Liu, Zhan},
  doi          = {10.1007/s10463-024-00912-x},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {155-190},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Model free feature screening for large scale and ultrahigh dimensional survival data},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information projection approach to smoothed propensity score
weighting for handling selection bias under missing at random.
<em>AISM</em>, <em>77</em>(1), 127–153. (<a
href="https://doi.org/10.1007/s10463-024-00913-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score weighting is widely used to correct the selection bias in the sample with missing data. The propensity score function is often developed using a model for the response probability, which completely ignores the outcome regression model. In this paper, we explore an alternative approach by developing smoothed propensity score weights that provide a more efficient estimation by removing unnecessary auxiliary variables in the propensity score model. The smoothed propensity score function is obtained by applying the information projection of the original propensity score function to the space that satisfies the moment conditions on the balancing scores obtained from the outcome regression model. By including the covariates for the outcome regression models only in the density ratio model, we can achieve an efficiency gain. Penalized regression is used to identify important covariates. Some limited simulation studies are presented to compare with the existing methods.},
  archive      = {J_AISM},
  author       = {Wang, Hengfang and Kim, Jae Kwang},
  doi          = {10.1007/s10463-024-00913-w},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {127-153},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Information projection approach to smoothed propensity score weighting for handling selection bias under missing at random},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved confidence intervals for nonlinear mixed-effects
and nonparametric regression models. <em>AISM</em>, <em>77</em>(1),
105–126. (<a href="https://doi.org/10.1007/s10463-024-00909-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference for high-dimensional parameters (HDPs) can leverage their intrinsic correlations, as spatially or temporally close parameters tend to have similar values. This is why nonlinear mixed-effects models (NMMs) are commonly used for HDPs. Conversely, in many practical applications, the random effects (REs) in NMMs are correlated HDPs that should remain constant during repeated sampling for frequentist inference. In both scenarios, the inference should be conditional on REs, instead of marginal inference by integrating out REs. We summarize recent theory of conditional inference for NMM, and then propose a bias-corrected RE predictor and confidence interval (CI). We also extend this methodology to accommodate the case where some REs are not associated with data. Simulation studies indicate our new approach leads to substantial improvement in the conditional coverage rate of RE CIs, including CIs for smooth functions in generalized additive models, compared to the existing method based on marginal inference.},
  archive      = {J_AISM},
  author       = {Zheng, Nan and Cadigan, Noel},
  doi          = {10.1007/s10463-024-00909-6},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {105-126},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Improved confidence intervals for nonlinear mixed-effects and nonparametric regression models},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hidden AR process and adaptive kalman filter. <em>AISM</em>,
<em>77</em>(1), 61–103. (<a
href="https://doi.org/10.1007/s10463-024-00908-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work discusses a model of a partially observed linear system that depends on some unknown parameters. An approximation of the unobserved component is proposed, which involves three steps. First, a method of moment estimator of unknown parameters is constructed, and second, this estimator is used to define the one-step MLE-process. Finally, the last estimator is substituted into the equations of the Kalman filter. The solution of obtained equations provides us with the desired approximation (adaptive Kalman filter). The asymptotic properties of all the mentioned estimators and both maximum likelihood and Bayesian estimators of the unknown parameters are detailed. The asymptotic efficiency of adaptive filtering is discussed.},
  archive      = {J_AISM},
  author       = {Kutoyants, Yury A.},
  doi          = {10.1007/s10463-024-00908-7},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {61-103},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Hidden AR process and adaptive kalman filter},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of value-at-risk by <span
class="math display"><em>L</em><sup><em>p</em></sup></span> quantile
regression. <em>AISM</em>, <em>77</em>(1), 25–59. (<a
href="https://doi.org/10.1007/s10463-024-00911-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring more accurate estimates of financial value at risk (VaR) has always been an important issue in applied statistics. To this end either quantile or expectile regression methods are widely employed at present, but an accumulating body of research indicates that $$L^{p}$$ quantile regression outweighs both quantile and expectile regression in many aspects. In view of this, the paper extends $$L^{p}$$ quantile regression to a general classical nonlinear conditional autoregressive model and proposes a new model called the conditional $$L^{p}$$ quantile nonlinear autoregressive regression model (CAR- $$L^{p}$$ -quantile model for short). Limit theorems for regression estimators are proved in mild conditions, and algorithms are provided for obtaining parameter estimates and the optimal value of p. Simulation study of estimation’s quality is given. Then, a CLVaR method calculating VaR based on the CAR- $$L^{p}$$ -quantile model is elaborated. Finally, a real data analysis is conducted to illustrate virtues of our proposed methods.},
  archive      = {J_AISM},
  author       = {Sun, Peng and Lin, Fuming and Xu, Haiyang and Yu, Kaizhi},
  doi          = {10.1007/s10463-024-00911-y},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {25-59},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Estimation of value-at-risk by $$L^{p}$$ quantile regression},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplified quasi-likelihood analysis for a locally
asymptotically quadratic random field. <em>AISM</em>, <em>77</em>(1),
1–24. (<a href="https://doi.org/10.1007/s10463-024-00907-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IHK program is a general framework in asymptotic decision theory, introduced by Ibragimov and Hasminskii and extended to semimartingales by Kutoyants. The quasi-likelihood analysis (QLA) asserts that a polynomial type large deviation inequality is always valid if the quasi-likelihood random field is asymptotically quadratic and if a key index reflecting the identifiability is non-degenerate. As a result, following the IHK program, the QLA gives a way to inference for various nonlinear stochastic processes. This paper provides a reformed and simplified version of the QLA and improves accessibility to the theory. As an example of the advantages of the scheme, the user can obtain asymptotic properties of the quasi-Bayesian estimator by only verifying non-degeneracy of the key index.},
  archive      = {J_AISM},
  author       = {Yoshida, Nakahiro},
  doi          = {10.1007/s10463-024-00907-8},
  journal      = {Annals of the Institute of Statistical Mathematics},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Ann. Inst. Stat. Math.},
  title        = {Simplified quasi-likelihood analysis for a locally asymptotically quadratic random field},
  volume       = {77},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="amai---14">AMAI - 14</h2>
<ul>
<li><details>
<summary>
(2025). Single MCMC chain parallelisation on decision trees.
<em>AMAI</em>, <em>93</em>(1), 219–232. (<a
href="https://doi.org/10.1007/s10472-023-09876-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees (DT) are highly famous in machine learning and usually acquire state-of-the-art performance. Despite that, well-known variants like CART, ID3, random forest, and boosted trees miss a probabilistic version that encodes prior assumptions about tree structures and shares statistical strength between node parameters. Existing work on Bayesian DT depends on Markov Chain Monte Carlo (MCMC), which can be computationally slow, especially on high dimensional data and expensive proposals. In this study, we propose a method to parallelise a single MCMC DT chain on an average laptop or personal computer that enables us to reduce its run-time through multi-core processing while the results are statistically identical to conventional sequential implementation. We also calculate the theoretical and practical reduction in run time, which can be obtained utilising our method on multi-processor architectures. Experiments showed that we could achieve 18 times faster running time provided that the serial and the parallel implementation are statistically identical.},
  archive      = {J_AMAI},
  author       = {Drousiotis, Efthyvoulos and Spirakis, Paul},
  doi          = {10.1007/s10472-023-09876-9},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {219-232},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Single MCMC chain parallelisation on decision trees},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two parameter-tuned multi-objective evolutionary-based
algorithms for zoning management in marine spatial planning.
<em>AMAI</em>, <em>93</em>(1), 187–218. (<a
href="https://doi.org/10.1007/s10472-023-09853-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic spatial planning is becoming more popular around the world as a decision-making way to build a unified vision for directing the medium- to long-term development of land/marine areas. Recently, the study of marine areas in terms of spatial planning such as Marine Spatial Planning (MSP) has received much attention. One of the challenging issues in MSP is to make a balance between determining the ideal zone for a new activity while also considering the locations of existing activities. This spatial zoning problem for multi-uses with multiple objectives could be formulated as optimization models. This paper presents and compares the results of two multi-objective evolutionary-based algorithms (MOEAs), Synchronous Hypervolume-based non-dominated sorting genetic algorithm-II (SH-NSGA-II) which is an extension of NSGA-II and a memetic algorithm (MA) in which SH-NSGA-II is enhanced with a local search. These proposed algorithms are used to solve the multi-objective spatial zoning optimization problem, which seeks to maximize the zone interest value assigned to the new activity while simultaneously maximizing its spatial compactness. We introduce several innovations in these proposed algorithms to address the problem constraints and to improve the robustness of the traditional NSGA-II and MA approaches. Unlike traditional ones, a different stop condition, multiple crossover, mutation, and repairing operators, and also a local search operator are developed. A comparative study is presented between the results obtained using both algorithms. To guarantee robust results for both algorithms, their parameters are calibrated and tuned using the Multi-Response Surface Methodology (MRSM) method. The effective and non-effective components, as well as the validity of the regression models, are determined using analysis of variance (ANOVA). Although SH-NSGA-II has revealed a good efficiency, its performance is still improved using a local search scheme within SH-NSGA-II, which is specially tailored to the problem characteristics. The two methods are designed for raster data.},
  archive      = {J_AMAI},
  author       = {Basirati, Mohadese and Billot, Romain and Meyer, Patrick},
  doi          = {10.1007/s10472-023-09853-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {187-218},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Two parameter-tuned multi-objective evolutionary-based algorithms for zoning management in marine spatial planning},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clique detection with a given reliability. <em>AMAI</em>,
<em>93</em>(1), 173–186. (<a
href="https://doi.org/10.1007/s10472-024-09928-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new notion of a clique reliability. The clique reliability is understood as the ratio of the number of statistically significant links in a clique to the number of edges of the clique. This notion relies on a recently proposed original technique for separating inferences about pairwise connections between vertices of a network into significant and admissible ones. In this paper, we propose an extension of this technique to the problem of clique detection. We propose a method of step-by-step construction of a clique with a given reliability. The results of constructing cliques with a given reliability using data on the returns of stocks included in the Dow Jones index are presented.},
  archive      = {J_AMAI},
  author       = {Semenov, Dmitry and Koldanov, Alexander and Koldanov, Petr and Pardalos, Panos},
  doi          = {10.1007/s10472-024-09928-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {173-186},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Clique detection with a given reliability},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing doubly stochastic matrices for average consensus
through swarm and evolutionary algorithms. <em>AMAI</em>,
<em>93</em>(1), 151–171. (<a
href="https://doi.org/10.1007/s10472-023-09912-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly-stochastic matrices play a vital role in modern applications of complex networks such as tracking and decentralized state estimation, coordination and control of autonomous agents. A central theme in all of the above is consensus, that is, nodes reaching agreement about the value of an underlying variable (e.g. the state of the environment). Despite the fact that complex networks have been studied thoroughly, the communication graphs are usually described by symmetric matrices due to their advantageous theoretical properties. We do not yet have methods for optimizing generic doubly-stochastic matrices. In this paper, we propose a novel formulation and framework, EvoDSM, for achieving fast linear distributed averaging by: (a) optimizing the weights of a fixed graph topology, and (b) optimizing for the topology itself. We are concerned with graphs that can be described by positive doubly-stochastic matrices. Our method relies on swarm and evolutionary optimization algorithms and our experimental results and analysis showcase that our method (1) achieves comparable performance with traditional methods for symmetric graphs, (2) is applicable to non-symmetric network structures and edge weights, and (3) is scalable and can operate effectively with moderately large graphs without engineering overhead.},
  archive      = {J_AMAI},
  author       = {Syriopoulos, Panos K. and Chatzilygeroudis, Konstantinos I. and Kalampalikis, Nektarios G. and Vrahatis, Michael N.},
  doi          = {10.1007/s10472-023-09912-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {151-171},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Optimizing doubly stochastic matrices for average consensus through swarm and evolutionary algorithms},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel method for solving universum twin bounded support
vector machine in the primal space. <em>AMAI</em>, <em>93</em>(1),
131–150. (<a href="https://doi.org/10.1007/s10472-023-09896-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised learning, the Universum, a third class that is not a part of either class in the classification task, has proven to be useful. In this study we propose (N $$ \mathfrak {U} $$ TBSVM), a Newton-based approach for solving in the primal space the optimization problems related to Twin Bounded Support Vector Machines with Universum data ( $$ \mathfrak {U} $$ TBSVM). In the N $$ \mathfrak {U} $$ TBSVM, the constrained programming problems of $$ \mathfrak {U} $$ TBSVM are converted into unconstrained optimization problems, and a generalization of Newton’s method for solving the unconstrained problems is introduced. Numerical experiments on synthetic, UCI, and NDC data sets show the ability and effectiveness of the proposed N $$ \mathfrak {U} $$ TBSVM. We apply the suggested method for gender detection from face images, and compare it with other methods.},
  archive      = {J_AMAI},
  author       = {Moosaei, Hossein and Khosravi, Saeed and Bazikar, Fatemeh and Hladík, Milan and Rosario Guarracino, Mario},
  doi          = {10.1007/s10472-023-09896-5},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {131-150},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {A novel method for solving universum twin bounded support vector machine in the primal space},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realtime gray-box algorithm configuration using
cost-sensitive classification. <em>AMAI</em>, <em>93</em>(1), 109–130.
(<a href="https://doi.org/10.1007/s10472-023-09890-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A solver’s runtime and the quality of the solutions it generates are strongly influenced by its parameter settings. Finding good parameter configurations is a formidable challenge, even for fixed problem instance distributions. However, when the instance distribution can change over time, a once effective configuration may no longer provide adequate performance. Realtime algorithm configuration (RAC) offers assistance in finding high-quality configurations for such distributions by automatically adjusting the configurations it recommends based on instances seen so far. Existing RAC methods treat the solver as a black box, meaning the solver is given a configuration as input, and it outputs either a solution or runtime as an objective function for the configurator. However, analyzing intermediate output from the solver can enable configurators to avoid wasting time on poorly performing configurations. We propose a gray-box approach that utilizes intermediate output during evaluation and implement it within the RAC method Contextual Preselection with Plackett-Luce (CPPL blue). We apply cost-sensitive machine learning with pairwise comparisons to determine whether ongoing evaluations can be terminated to free resources. We compare our approach to a black-box equivalent on several experimental settings and show that our approach reduces the total solving time in several scenarios and improves solution quality in an additional scenario.},
  archive      = {J_AMAI},
  author       = {Weiss, Dimitri and Tierney, Kevin},
  doi          = {10.1007/s10472-023-09890-x},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {109-130},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Realtime gray-box algorithm configuration using cost-sensitive classification},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel SVM-based classification approaches for evaluating
pancreatic carcinoma. <em>AMAI</em>, <em>93</em>(1), 93–108. (<a
href="https://doi.org/10.1007/s10472-023-09888-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop two SVM-based classifiers named stable nested one-class support vector machines (SN-1SVMs) and decoupled margin-moment based SVMs (DMMB-SVMs), to predict the specific type of pancreatic carcinoma using quantitative histopathological signatures of images. For each patient, the diagnosis can produce hundreds of images, which can be used to classify the pancreatic tissues into three classes: chronic pancreatitis, intraductal papillary mucinous neoplasms, and pancreatic carcinoma. The proposed two approaches tackle the classification problems from two different perspectives: the SN-1SVM treats each image as a classification point in a nested fashion to predict malignancy of the tissues, while the DMMB-SVM treats each patient as a classification point by assembling information across images. One attractive feature of the DMMB-SVM is that, in addition to utilizing the mean information, it also takes into account the covariance of features extracted from images for each patient. We conduct numerical experiments to evaluate and compare performance of the two methods. It is observed that the SN-1SVM can take advantage of the data structure more effectively, while the DMMB-SVM demonstrates better computational efficiency and classification accuracy. To further improve interpretability of the final classifier, we also consider the $$\ell _1$$ -norm in the DMMB-SVM to handle feature selection.},
  archive      = {J_AMAI},
  author       = {Washburn, Ammon and Fan, Neng and Zhang, Hao Helen},
  doi          = {10.1007/s10472-023-09888-5},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {93-108},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Novel SVM-based classification approaches for evaluating pancreatic carcinoma},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian optimization over the probability simplex.
<em>AMAI</em>, <em>93</em>(1), 77–91. (<a
href="https://doi.org/10.1007/s10472-023-09883-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Process based Bayesian Optimization is largely adopted for solving problems where the inputs are in Euclidean spaces. In this paper we associate the inputs to discrete probability distributions which are elements of the probability simplex. To search in the new design space, we need a distance between distributions. The optimal transport distance (aka Wasserstein distance) is chosen due to its mathematical structure and the computational strategies enabled by it. Both the GP and the acquisition function is generalized to an acquisition functional over the probability simplex. To optimize this functional two methods are proposed, one based on auto differentiation and the other based on proximal-point algorithm and the gradient flow. Finally, we report a preliminary set of computational results on a class of problems whose dimension ranges from 5 to 100. These results show that embedding the Bayesian optimization process in the probability simplex enables an effective algorithm whose performance over standard Bayesian optimization improves with the increase of problem dimensionality.},
  archive      = {J_AMAI},
  author       = {Candelieri, Antonio and Ponti, Andrea and Archetti, Francesco},
  doi          = {10.1007/s10472-023-09883-w},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {77-91},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Bayesian optimization over the probability simplex},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KNN classification: A review. <em>AMAI</em>, <em>93</em>(1),
43–75. (<a href="https://doi.org/10.1007/s10472-023-09882-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-nearest neighbors (k/NN) algorithm is a simple yet powerful non-parametric classifier that is robust to noisy data and easy to implement. However, with the growing literature on k/NN methods, it is increasingly challenging for new researchers and practitioners to navigate the field. This review paper aims to provide a comprehensive overview of the latest developments in the k/NN algorithm, including its strengths and weaknesses, applications, benchmarks, and available software with corresponding publications and citation analysis. The review also discusses the potential of k/NN in various data science tasks, such as anomaly detection, dimensionality reduction and missing value imputation. By offering an in-depth analysis of k/NN, this paper serves as a valuable resource for researchers and practitioners to make informed decisions and identify the best k/NN implementation for a given application.},
  archive      = {J_AMAI},
  author       = {Syriopoulos, Panos K. and Kalampalikis, Nektarios G. and Kotsiantis, Sotiris B. and Vrahatis, Michael N.},
  doi          = {10.1007/s10472-023-09882-x},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {43-75},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {KNN classification: A review},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved multi-task least squares twin support vector
machine. <em>AMAI</em>, <em>93</em>(1), 21–41. (<a
href="https://doi.org/10.1007/s10472-023-09877-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-task learning (MTL) has become a popular field in machine learning and has a key role in various domains. Sharing knowledge across tasks in MTL can improve the performance of learning algorithms and enhance their generalization capability. A new approach called the multi-task least squares twin support vector machine (MTLS-TSVM) was recently proposed as a least squares variant of the direct multi-task twin support vector machine (DMTSVM). Unlike DMTSVM, which solves two quadratic programming problems, MTLS-TSVM solves two linear systems of equations, resulting in a reduced computational time. In this paper, we propose an enhanced version of MTLS-TSVM called the improved multi-task least squares twin support vector machine (IMTLS-TSVM). IMTLS-TSVM offers a significant advantage over MTLS-TSVM by operating based on the empirical risk minimization principle, which allows for better generalization performance. The model achieves this by including regularization terms in its objective function, which helps control the model’s complexity and prevent overfitting. We demonstrate the effectiveness of IMTLS-TSVM by comparing it to several single-task and multi-task learning algorithms on various real-world data sets. Our results highlight the superior performance of IMTLS-TSVM in addressing multi-task learning problems.},
  archive      = {J_AMAI},
  author       = {Moosaei, Hossein and Bazikar, Fatemeh and Pardalos, Panos M.},
  doi          = {10.1007/s10472-023-09877-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {21-41},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {An improved multi-task least squares twin support vector machine},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Revised selected papers from the LION 16
conference. <em>AMAI</em>, <em>93</em>(1), 19–20. (<a
href="https://doi.org/10.1007/s10472-024-09958-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Kotsireas, Ilias S. and Pardalos, Panos M.},
  doi          = {10.1007/s10472-024-09958-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {19-20},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Guest editorial: Revised selected papers from the LION 16 conference},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep data density estimation through donsker-varadhan
representation. <em>AMAI</em>, <em>93</em>(1), 7–17. (<a
href="https://doi.org/10.1007/s10472-024-09943-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the data density is one of the challenging problem topics in the deep learning society. In this paper, we present a simple yet effective methodology for estimating the data density using the Donsker-Varadhan variational lower bound on the KL divergence and the modeling based on the deep neural network. We demonstrate that the optimal critic function associated with the Donsker-Varadhan representation on the KL divergence between the data and the uniform distribution can estimate the data density. Also, we present the deep neural network-based modeling and its stochastic learning procedure. The experimental results and possible applications of the proposed method demonstrate that it is competitive with the previous methods for data density estimation and has a lot of possibilities for various applications.},
  archive      = {J_AMAI},
  author       = {Park, Seonho and Pardalos, Panos M.},
  doi          = {10.1007/s10472-024-09943-9},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {7-17},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Deep data density estimation through donsker-varadhan representation},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The future starts now. <em>AMAI</em>, <em>93</em>(1), 5–6.
(<a href="https://doi.org/10.1007/s10472-025-09970-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Dix, Jürgen and Fisher, Michael},
  doi          = {10.1007/s10472-025-09970-0},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {5-6},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {The future starts now},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 35 years of math and AI. <em>AMAI</em>, <em>93</em>(1), 1–3.
(<a href="https://doi.org/10.1007/s10472-025-09969-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Golumbic, Martin Charles},
  doi          = {10.1007/s10472-025-09969-7},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {35 years of math and AI},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="apin---185">APIN - 185</h2>
<ul>
<li><details>
<summary>
(2025). MOEA/d with adaptive weight vector adjustment and parameter
selection based on q-learning. <em>APIN</em>, <em>55</em>(6), 1–43. (<a
href="https://doi.org/10.1007/s10489-024-05906-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) are widely utilized for addressing multi-objective optimization problems (MOPs), demonstrating effectiveness in handling low-dimensional and regular Pareto fronts (PFs) MOPs. However, when the number of objectives increases (&gt;3) and the PFs become increasingly intricate, maintaining both the convergence and diversity of solutions presents a significant challenge. To address this, an adaptive weight vector adjustment and parameter selection based on Q-learning (QLMOEA/D-AWA) is proposed. In the algorithm, Q-learning is employed to select both the Tchebycheff value and the number of weight vectors, aiming to balance convergence and diversity. To enhance the convergence, an improved Tchebycheff approach is proposed. To better solve problems in high-dimensional objective spaces, the niche technique is adopted to retain elite individuals. In addition, to address MOPs with irregular PFs, a two-stage weight vector deletion strategy is proposed to remove invalid weight vectors, and a certain number of weight vectors are added based on sparsity rules. An experiment study of objective numbers ranging from 2 to 10 is conducted on DTLZ, WFG, MaF and multi-objective traveling salesman problem (MOTSP). Among 115 benchmark problems, QLMOEA/D-AWA achieves 54 and 49 best results in IGD and HV, respectively.},
  archive      = {J_APIN},
  author       = {Xue, Fei and Chen, Yuezheng and Dong, Tingting and Wang, Peiwen and Fan, Wenyu},
  doi          = {10.1007/s10489-024-05906-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-43},
  shortjournal = {Appl. Intell.},
  title        = {MOEA/D with adaptive weight vector adjustment and parameter selection based on Q-learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel drift detection method using parallel detection and
anti-noise techniques. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05988-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet industry, a large amount of streaming data with significant application value will be generated on the Internet. The distribution of stream data is evolving over time compared to traditional data, posing a significant challenge in the learning process from streaming data. In order to adapt the change of data distribution, concept drift detection methods are proposed to pinpoint when the concept drift occurs. Most existing drift detection methods, however, overlook the improvement of the current classifier and the influence of noise data on drift detection. This oversight leads to a decrease in the effectiveness of drift detection. In this paper, we propose a novel adaptation drift detection method to overcome the shortcomings of previous algorithms, such as error detection and lack of anti-noise capability. Meanwhile, stream computing and parallel computing are used to enhance the efficiency of our algorithm. The results of a simulation experiment on 9 synthetic stream data and 6 real-world stream data, all exhibiting concept drift, demonstrate that our method is more effective in handling concept drift compared to other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zhang, Qian and Liu, Guanjun},
  doi          = {10.1007/s10489-024-05988-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A novel drift detection method using parallel detection and anti-noise techniques},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation for improving automatic airborne pollen
classification with expert-verified measurements. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06021-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel approach to enhance the accuracy of automatic classification systems for airborne pollen particles by integrating domain adaptation techniques. Our method incorporates expert-verified measurements into the convolutional neural network (CNN) training process to address the discrepancy between laboratory test data and real-world environmental measurements. We systematically fine-tuned CNN models, initially developed on standard reference datasets, with these expert-verified measurements. A comprehensive exploration of hyperparameters was conducted to optimize the CNN models, ensuring their robustness and adaptability across various environmental conditions and pollen types. Empirical results indicate a significant improvement, evidenced by a 22.52% increase in correlation and a 38.05% reduction in standard deviation across 29 cases of different pollen classes over multiple study years. This research highlights the potential of domain adaptation techniques in environmental monitoring, particularly in contexts where the integrity and representativeness of reference datasets are difficult to verify.},
  archive      = {J_APIN},
  author       = {Matavulj, Predrag and Jelic, Slobodan and Severdija, Domagoj and Brdar, Sanja and Radovanovic, Milos and Tesendic, Danijela and Sikoparija, Branko},
  doi          = {10.1007/s10489-024-06021-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Domain adaptation for improving automatic airborne pollen classification with expert-verified measurements},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selections based on uncertainty measurements from
dual-quantitative improvement and double-hierarchical fusion.
<em>APIN</em>, <em>55</em>(6), 1–35. (<a
href="https://doi.org/10.1007/s10489-024-06075-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selections promote classification learning, and rough set theory offers effective mathematical methods; in practice, the performance enhancement of feature selection algorithms formulates a research target and challenge, and the corresponding problem solving usually resorts to improvement constructions of uncertainty measures. By fitting fuzzy rough sets (FFRSs), the relative dependency complement mutual information (FDCIE) motivates a recent algorithm of feature selection, called FNRDCI; however, FDCIE has improvement space of quantification view and fusion hierarchy, so the corresponding feature selection and heuristic algorithm can be advanced. In this paper, the dependency is improved by information localization, while the mutual information is enriched by information fuzzification and decision-class combination, so improved fusion measures and robuster feature selections are established by double-hierarchical fusion on decision classification and class. At first, the correctional dependency is proposed by fuzzy decision localization, and it induces a classification fusion measure (i.e. FCDCIE); based on two types of fuzzy decisions, two types of mutual information (i.e. FRCEmI and FRCFmI) are established by information fuzzification and class combination. Then, two types of dependency and two types of mutual information combinedly generate $$2\times 2=4$$ classification fusion measures (i.e. IFRDCEmI, IFRDCFmI, IFRCDCEmI, IFRCDCFmI) by pursuing class-level priority fusion; these new measures acquire semantics uncertainty, system equations, and granulation monotonicity/nonmonotonicity. Furthermore, $$1+2\times 2=5$$ fusion measures yield 5 novel feature selections with heuristic algorithms. Finally, experimental comparisons demonstrate the effectiveness and efficiency of the proposed novel methods of uncertainty measures and selection algorithms.},
  archive      = {J_APIN},
  author       = {Wang, Qian and Zhang, Xianyong and Lv, Zhiying and Mo, Zhiwen},
  doi          = {10.1007/s10489-024-06075-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Appl. Intell.},
  title        = {Feature selections based on uncertainty measurements from dual-quantitative improvement and double-hierarchical fusion},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Daily power generation forecasting for a grid-connected
solar power plant using transfer learning technique. <em>APIN</em>,
<em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06090-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is efficiently used for photovoltaic power generation forecasting to handle the intermittent nature of solar energy. However, big data are required for training deep networks which are not available for newly installed plants. Therefore, in this study, a novel strategy is proposed to train a deep learning model using a transfer learning technique to cop up with the unavailability of enough training datasets. A new 400 kWp solar power plant installed in the Himalayan region is considered as a case study to evaluate the proposed model. The proposed approach utilizes solar radiation data to train a deep neural network and then fine-tune the model using the power generation data from the plant. The network architecture is optimized using grey wolf optimizer to find the best suitable model for the data. The evaluation results show that the same model can achieve higher performance in generation forecasting with percentage error improved by 2% and R-value increased by 7.7% after applying transfer learning. Moreover, SHapley Additive exPlanation and Partial Dependence Plots are used to interpret the model behavior and showed that the model is mostly dependent on the previous generation values (up to 4 days) followed by the temperature and solar radiation.},
  archive      = {J_APIN},
  author       = {Tajjour, Salwan and Chandel, Shyam Singh and Malik, Hasmat and Márquez, Fausto Pedro García and Alotaibi, Majed A.},
  doi          = {10.1007/s10489-024-06090-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Daily power generation forecasting for a grid-connected solar power plant using transfer learning technique},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Dirichlet stochastic weights averaging for
graph neural networks. <em>APIN</em>, <em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06099-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Park, Minhoi and Chang, Rakwoo and Song, Kyungwoo},
  doi          = {10.1007/s10489-024-06099-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Dirichlet stochastic weights averaging for graph neural networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the imitation game: A trust-based model for
distinguishing human and machine participants. <em>APIN</em>,
<em>55</em>(6), 1–39. (<a
href="https://doi.org/10.1007/s10489-024-06133-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 1950, the imitation game has captured the interest of researchers investigating human‒machine differences. Designed to evaluate machine cognition through a game-based framework, its complexity demands refinement. The imitation game utilizes this game-based setup, but its inherent intricacy calls for further enhancements. The fundamental question of whether machines are capable of genuine thought has been a key issue in artificial intelligence (AI) studies. Recent developments challenge the ease of differentiation, as AI enables machines to display human-like characteristics. This paper seeks to address the shortcomings of the original Turing test and criticisms of the imitation game by introducing an integrated model. Although machines currently operate based on our instructions, they can learn from errors and produce novel responses through generative AI techniques, even though they do not experience emotions. In this study, a new trust-based model was introduced to improve the imitation game. This model integrated various factors to assess the reliability of participants’ responses, including grammatical accuracy, response time, thinking duration, response speed, creativity, and the use of human-like expressions. The goal was to calculate a trust factor that determines the likelihood of a participant being a human or machine. To evaluate the model’s performance, a comprehensive dataset was developed using a chat generative pretrained transformer (ChatGPT-3.5). Two other large language models (LLMs), the large language model meta AI (Llama 3) and the Claude LLM, were also taken into account. To simulate the experiment with human participants, human-generated text was also included. The simulation results revealed that GPT-3.5 Turbo, Llama 3, and Claude LLM performed differently in terms of grammatical accuracy, human-like phrasing, creativity, and trust factors. GPT-3.5 and Llama 3 had lower error rates but struggled with human-like phrases. Claude resulted in more grammatical errors but better creativity. The human participants consistently showed greater trust and human-like phrase usage. Probability assessments categorized machines with 71–78% accuracy, whereas humans were identified with only a 29–36% chance of being a machine.},
  archive      = {J_APIN},
  author       = {Gupta, Tanisha and Tripathi, Akarsh and Dubey, Ashutosh Kumar and Chahar, Ravita},
  doi          = {10.1007/s10489-024-06133-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-39},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing the imitation game: A trust-based model for distinguishing human and machine participants},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Freeway optimal control based on emission oriented
microscopic graph convolutional neural network. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06143-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction and control in the active traffic control system is considered as one of the most critical issues in Intelligent Transportation Systems (ITS). Among the proposed AI-based approaches, Deep Learning (DL) has been largely applied while showing better performances. This research improves macroscopic traffic flow model METANET by establishing a graph convolution neural network (GCN) to explicitly and more precisely incorporate microscopic traffic flow dynamics. The microscopic emission model utilizes the feature extraction function of GCN to reduce the complexity of measuring the environmental profits for the whole traffic network. By introducing the GCN model to facilitate the aggregation of vehicle information, the proposed framework reduces the computational burden and obtains better optimization performance. The designed algorithms are tested on a microscopic simulation platform based on field data. The results demonstrate that the proposed control method produce a more robust and smooth traffic flow environment, which leads to improved traffic efficiency and overall carbon emissions of the road network.},
  archive      = {J_APIN},
  author       = {Fang, Jie and Lu, Mingwen and Fu, Lina and Wang, Juanmeizi and Xu, Mengyun},
  doi          = {10.1007/s10489-024-06143-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Freeway optimal control based on emission oriented microscopic graph convolutional neural network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving vehicle detection accuracy in complex traffic
scenes through context attention and multi-scale feature fusion module.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-024-06146-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection is a fundamental task for automated driving systems. However, achieving robust performance in complex traffic scenarios remains a formidable challenge. In this paper, we propose a novel vehicle detection model that leverages contextual attention mechanisms and multi-scale feature fusion to effectively tackle the inherent challenges presented by complex scenarios, such as occlusion, truncation, and small-scale vehicle instances. The proposed model introduces a contextual attention module tailored to address vehicle occlusion, augmenting the model’s reasoning ability and overall performance through the integration of global contextual information. Additionally, we introduce a Multi-Scale Feature Fusion Module to mitigate the impact of drastic changes in vehicle scales observed in dynamic traffic scenarios. Through the deployment of a dedicated multi-scale feature fusion module, our model adeptly adapts to significant size variations of vehicles in traffic scene images, thereby enhancing its capability to handle vehicles of varying sizes. Our contributions are validated through comprehensive qualitative and quantitative experiments conducted on both the KITTI dataset and the Cityscapes dataset. The experimental results demonstrate the exceptional robustness and accuracy of our proposed model. These findings provide conclusive evidence of the superior performance and effectiveness of our model in real-world applications.},
  archive      = {J_APIN},
  author       = {Liu, Wenbo and Zhao, Binglin and Zhu, Yuxin and Deng, Tao and Yan, Fei},
  doi          = {10.1007/s10489-024-06146-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Improving vehicle detection accuracy in complex traffic scenes through context attention and multi-scale feature fusion module},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement knowledge graph reasoning based on dual agents
and attention mechanism. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06162-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning can model knowledge graph multi-hop reasoning as Markov Decision Processes and improve the accuracy and interpretability of predicting paths between entities. Existing reasoning methods usually ignore the logic of action selection when facing one-to-many or many-to-many relationships, resulting in poor performance in knowledge graph reasoning. Furthermore, the general multi-hop reasoning only achieves effective short-path reasoning and lacks efficiency in long-distance reasoning. To address the above challenges, we propose a reinforcement learning reasoning model based on dual agents and attention mechanism, where two agents are trained at the macro and micro levels, and the macro agent guides the reasoning of the micro agent. The model employs an attention mechanism to enhance the representation of the current state of the agent, to help the policy network in making more appropriate action selections when facing one-to-many or many-to-many relationships, so as to improve the selection efficiency. Simultaneously, we propose a reward function with a penalty mechanism that penalizes the agent for prematurely reaching the correct answer without staying in place, and enhances the reward of the micro agent with the reward of the macro agent. The two agents cooperate with each other to find reasoning paths on the knowledge graph. Finally, we compare the proposed model with six well-known inference method baselines on three benchmark datasets, and the experimental results show that our proposed method achieves very competitive results.},
  archive      = {J_APIN},
  author       = {Yang, Xu-Hua and Wang, Tao and Gan, Ji-Song and Gao, Liang-Yu and Ma, Gang-Feng and Zhou, Yan-Bo},
  doi          = {10.1007/s10489-024-06162-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement knowledge graph reasoning based on dual agents and attention mechanism},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Explainable cognitive decline detection in
free dialogues with a machine learning approach based on pre-trained
large language models. <em>APIN</em>, <em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06169-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {de Arriba-Pérez, Francisco and García-Méndez, Silvia and Otero-Mosquera, Javier and González-Castaño, Francisco J.},
  doi          = {10.1007/s10489-024-06169-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Explainable cognitive decline detection in free dialogues with a machine learning approach based on pre-trained large language models},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of pre-trained CNN models and data fusion
techniques in Unity3D for connected vehicles. <em>APIN</em>,
<em>55</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s10489-024-06213-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Transportation Systems (ITS) aim to enhance road safety and Internet of Things (IoT)-related solutions are crucial in achieving this objective. By leveraging Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) technologies, drivers can access valuable information about their surroundings. This research utilized the Unity 3D game engine to simulate various traffic scenarios, exploring a stochastic environment with two data sources: camera and road sign labels. We developed a full-duplex communication system to enable the communication between Python and Unity. This allows the vehicle to capture images in Unity and classify them using Convolutional Neural Network (CNN) models coded in Python. To improve road sign detection accuracy, we applied multi-sensor Data Fusion (DF) techniques to fuse the information received from the sources. We applied DF methods such as the Kalman filter, Dempster-Shafer theory, and Fuzzy Integral Operators to combine the two sources of information. Furthermore, our proposed CNN model incorporates an Ordered Weighted Averaging (OWA) layer to fuse information from three pre-trained CNN models. Our results show that the proposed model integrating the OWA layer achieved an accuracy of 98.81%, outperforming six state-of-the-art models. We compared the Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). In our work, EKF exhibited a lower execution time (0.02 seconds), yielding less accurate results. UKF, however, provided a more accurate estimate while being more computationally complex. Furthermore, the Dempster-Shafer model showed approximately 30% better accuracy compared to the Fuzzy Integral Operator. Using this methodology on autonomous vehicles in our virtual environment led to making more accurate decisions, even in a variety of weather conditions and accident scenarios. The findings of this research contribute to the development of more efficient and safer vehicles.},
  archive      = {J_APIN},
  author       = {Norouzi, Mojtaba and Hosseini, Seyed Hossein and Khoshnevisan, Mohammad and Moshiri, Behzad},
  doi          = {10.1007/s10489-024-06213-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {Applications of pre-trained CNN models and data fusion techniques in Unity3D for connected vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational analysis of virus-host protein-protein
interactions using gene ontology and natural language processing.
<em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-024-06223-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of in-silico computational methods in identifying protein-protein interactions (PPIs) between target and host proteins is crucial for developing effective infection treatments. These methods are essential for quickly determining high-quality and accurate PPIs, predicting protein pairs with the highest likelihood of physical interaction from a large pool, and reducing the need for experimental confirmation or prioritizing pairs for experiments. This study proposes using gene ontology and natural language processing (NLP) approaches to extract and quantify features from protein sequences. In the first step, proteins were represented using gene ontology terms, and a set of features was generated. In the second step, NLP techniques treated gene ontology terms as a word dictionary, creating numerical vectors using the bag of words (BoW), count vector, term frequency-inverse document frequency (TF-IDF), and information content methods. In the third step, different machine learning methods, including Decision Tree, Random Forest, Bagging-RepTree, Bagging-RF, BayesNet, Deep Neural Network (DNN), Logistic Regression, Support Vector Machine (SVM), and VotedPerceptron, were employed to predict protein interactions in the datasets. In the fourth step, the Max-Min Parents and Children (MMPC) feature selection algorithm was applied to improve predictions using fewer features. The performance of the developed method was tested on the SARS-CoV-2 protein interaction dataset. The MMPC algorithm reduced the feature count by over 99%, enhancing protein interaction prediction. After feature selection, the DNN method achieved the highest predictive performance, with an AUC of 0.878 and an F-Measure of 0.793. Sequence-based protein encoding methods AAC, APAAC, CKSAAPP, CTriad, DC, and PAAC were applied to proteins in the SARS-CoV-2 interaction dataset and their performance was compared with GO-NLP. The performance of the relevant methods was measured separately and combined. The highest performance was obtained from the combined dataset with an AUC value of 0.888. This study demonstrates that the proposed gene ontology and NLP approach can successfully predict protein-protein interactions for antiviral drug design with significantly fewer features using the MMPC-DNN model.},
  archive      = {J_APIN},
  author       = {Cihan, Pınar and Ozger, Zeynep Banu and Cakabay, Zeynep},
  doi          = {10.1007/s10489-024-06223-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Computational analysis of virus-host protein-protein interactions using gene ontology and natural language processing},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating cruise user satisfaction through online reviews:
A method based on sentiment analysis and large-scale group
decision-making. <em>APIN</em>, <em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06241-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews of cruise tourism, as user-generated information, contain customers’ evaluations of various aspects of the cruise tourism industry. They influence the development of this industry by affecting potential users’ purchasing decisions. To promote the sustainable development of the cruise tourism industry, it is crucial to understand the factors influencing high user satisfaction and to ensure high levels of user satisfaction. With a focus on the factors influencing the cruise travel experience, this paper proposes a method for determining user requirements (URs) and evaluating user satisfaction with cruise tourism by integrating online review analysis with large-scale group decision-making (LSGDM). First, we establish a sentiment dictionary for the cruise domain based on online reviews, selecting seed sentiment words according to word frequency and expanding them using the Word2vec and semantic orientation using pointwise mutual information algorithms. Second, we use the latent Dirichlet allocation topic model to analyze online reviews and identify the 10 URs that are of actual concern to cruise customers. Then we perform dependency syntax analysis to conduct a fine-grained sentiment analysis of each review to identify the sentiment intensity values toward different cruise URs. Third, we evaluate the final satisfaction and ranking of URs using the LSGDM method, which includes a consensus model with a personalized feedback mechanism based on the minimum adjustment cost. We conclude by providing suggestions for improving the cruise tourism experience.},
  archive      = {J_APIN},
  author       = {Shi, Jing and Chen, Jing and Wu, Jian and Liu, Yujia},
  doi          = {10.1007/s10489-025-06241-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Evaluating cruise user satisfaction through online reviews: A method based on sentiment analysis and large-scale group decision-making},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive control approach incorporating incremental
learning. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06243-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Incremental Learning MPC (ILMPC), a novel Model Predictive Control (MPC) approach designed to enhance the adaptability of control systems in dynamic environments with unpredictable disturbances. Traditional MPC methods are often limited by their reliance on static models and fixed optimization schemes, making them less effective in handling disturbances and model inaccuracies. To overcome these limitations, ILMPC integrates incremental learning, enabling continuous refinement of the control model using real-time data. This innovation improves prediction accuracy and control performance, allowing the system to adapt to changing operational conditions and unknown disturbances. Key advances include the development of a sequence prediction model that continuously updates the state-space model through incremental learning, improved disturbance suppression for more stable control, and a reduction in computational complexity by incrementally model parameters. Experimental results show that ILMPC enhances deviation suppression significantly compared to conventional methods and significantly reduces control input volatility, demonstrating its superior performance in real-time disturbance suppression and adaptability.},
  archive      = {J_APIN},
  author       = {Chen, Jian and Pan, Haiwei and Zhang, Kejia and Lan, Haiyan and Xu, Xu and Luo, Wenhui},
  doi          = {10.1007/s10489-025-06243-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Predictive control approach incorporating incremental learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary-sensitive adaptive decoupled knowledge distillation
for acne grading. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06260-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acne grading is a critical step in the treatment and customization of personalized therapeutic plans. Although the knowledge distillation architecture exhibits outstanding performance on acne grading task, the impact of non-label classes is not considered separately, resulting in low distillation efficiency for non-label classes. Such insufficiency will cause the misclassification of the acne images located on the edge of the decision boundary. To address this issue, a novel method named Adaptive Decoupled Knowledge Distillation (ADKD) which considers the uniqueness of the acne images is proposed. In order to explore the influence of non-label classes and enhance the model’s distillation efficiency on them, ADKD splits the traditional KD loss into two parts: non-label class knowledge distillation (NCKD), and label class knowledge distillation (LCKD). Additionally, it dynamically adjusts the NCKD based on the distance between the sample and each non-label class. This allows the model to allocate different learning intensities to various non-label classes, reducing the overrecognition of classes near the sample and the underrecognition of distant classes. The proposed method enables the model to better learn the fuzzy features between acne images, and more accurately classify the samples located on the decision boundary. To verify the proposed method, extensive experiments were carried out on ACNE04 dataset, ACNEHX dataset, and DermaMnist dataset. The experimental results demonstrate the effectiveness of this method, and its performance surpasses that of current state-of-the-art (SOTA) method.},
  archive      = {J_APIN},
  author       = {Zhou, Xinyang and Liu, Wenjie and Zhang, Lei and Zhang, Xianliang},
  doi          = {10.1007/s10489-025-06260-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Boundary-sensitive adaptive decoupled knowledge distillation for acne grading},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial StO2-based personal identification: Dataset
construction, feasibility study, and recognition framework.
<em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06267-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics have been extensively utilized in the realm of identity recognition. However, each biometric method has its inherent limitations in specific scenarios. For example, identity recognition based on facial images is contactless but can be forged; finger vein recognition is very secure but generally requires contact collection to ensure accurate identification. In some scenarios with high security requirements, there is often a need for contactless acquisition of biometric features that cannot be forged to recognize identity. Therefore, a novel biometric, facial tissue oxygen saturation (StO2) with the advantages of robust anti-spoofing capabilities and non-contact measurement, is proposed for identity recognition. To more comprehensively verify the feasibility of facial StO2 for identity recognition, a Facial StO2 Identity Dataset (FSID148) containing 148 identities is collected and the feasibility of facial StO2 identity recognition is validated by performing verification, close-set identification, and open-set identification tasks. In order to enhance the performance of facial StO2 identity recognition, an attention-guided contrastive learning framework that enables backbones to derive discriminative identity representations from both local and global facial StO2 regions is proposed. The method proposed has achieved accuracies of 96.11%, 94.60%, and 88.51% in the aforementioned tasks, positioning facial StO2 as a promising biometric for a wide array of application scenarios.},
  archive      = {J_APIN},
  author       = {Zhang, Zheyuan and Liu, Xinyu and Jia, Yingjuan and Zhou, Ju and Wang, Hanpu and Wang, Jiaxiu and Chen, Tong},
  doi          = {10.1007/s10489-025-06267-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Facial StO2-based personal identification: Dataset construction, feasibility study, and recognition framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mind the naive forecast! A rigorous evaluation of
forecasting models for time series with low predictability.
<em>APIN</em>, <em>55</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06268-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of time series forecasting, numerous machine learning studies have assessed the performance of new methods on highly volatile data from macroeconomics and finance. Unlike in other domains, where models are also compared to simpler statistical or naive baselines, they mostly compare the performance solely relative to other complex models. This approach may lead to limited conclusions and reduce the practical significance of the results, as it overlooks the unpredictability of some highly volatile time series in the datasets used. We apply state-of-the-art methods from time-series econometrics and machine learning, including autoregressive integrated moving average (ARIMA), exponential smoothing (ETS), Bayesian vector autoregressive model (BVAR), long-short term memory neural networks (LSTM), historical consistent neural networks (HCNN), deep vector autoregressive neural networks (DeepVAR), temporal fusion transformers (TFT), and extreme gradient boosting (XGBoost). Our results demonstrate that no method consistently outperforms the naive (no-change) forecast for highly volatile time series from two popular datasets containing exchange rates and stock prices, rendering comparative analysis between complex models less meaningful. In contrast, when applied to more predictable macroeconomic price indices, many of the methods significantly outperform naive forecasts. We find that the performance of machine learning models deteriorates more than that of statistical models for high-volatility time series. This study highlights the critical importance of using appropriate benchmark models, including cost-effective, simple approaches, on datasets that permit meaningful conclusions.},
  archive      = {J_APIN},
  author       = {Beck, Nico and Dovern, Jonas and Vogl, Stefanie},
  doi          = {10.1007/s10489-025-06268-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {Mind the naive forecast! a rigorous evaluation of forecasting models for time series with low predictability},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-tailed classification by efficient contrast learning
with high quality and high relevance latent features. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06269-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust feature representations from long-tail distributed data is essential. Recently, contrastive learning has shown impressive progress in addressing long-tail learning challenges. While contrastive learning aims to optimize the lower bound of mutual information between feature distribution and label distribution, the previous approaches often substantially rely on less accurate and unrealistic assumptions about model distribution and overlook the long-tail nature of the instance space. Consequently, these methods fail to achieve a sufficiently tight lower bound. To address these concerns, we first propose a loss function derived from mini-Batch instance Features and Class Prototypes to construct a Conditional Gaussian mixture distribution (CGM-BF-CP), and prove its generalization ability from the perspective of generalization error upper bound. Then we create high quality and high relevance KNN graph to model relation between features. And propose a corresponding loss function, i.e., Graph based Contrast Learning Loss (GCLL). The feature information can be transferred between classes through this graph, so that the tail class features can be better learned. The experimental results on Cifar10/100-LT and ImageNet-LT show that our proposed model is competitive with the latest state-of-the-art methods. Our code is available at https://github.com/error030/CGM-BP-CP/tree/main .},
  archive      = {J_APIN},
  author       = {Yuan, Hong-li and Liu, Jian-wei},
  doi          = {10.1007/s10489-025-06269-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Long-tailed classification by efficient contrast learning with high quality and high relevance latent features},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision transformer-based generalized zero-shot learning with
data criticizing. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06271-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Zero-Shot Learning (GZSL) aims to enable accurate testing and recognition of unseen classes by utilizing training data from seen classes and leveraging attribute knowledge. However, GZSL faces a challenge wherein the model, trained solely on seen class data, tends to be biased towards recognizing visual features of seen classes, resulting in poorer recognition performance for unseen classes. To address this issue, we propose an approach called Vision Transformer-Based Generalized Zero-Shot Learning with Data Criticizing (ViT-DaCr). In order to obtain improved visual features, we thoroughly examine features extracted by Vision Transformer (ViT) with a new design. Additionally, we recognize that not all training data align with our model during the training process, leading the model to exhibit a bias towards recognizing visual features of seen classes and directly impacting visual feature recognition. Therefore, we propose a data critic mechanism that utilizes Adjusted Boxplot to filter out such data automatically during the training process. Extensive experiments demonstrate the advanced performance of our model on three challenging and popular datasets.},
  archive      = {J_APIN},
  author       = {Zhou, Quan and Liang, Yucuan and Zhang, Zhenqi and Cao, Wenming},
  doi          = {10.1007/s10489-025-06271-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Vision transformer-based generalized zero-shot learning with data criticizing},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Dynamic interactive weighted feature
selection using fuzzy interaction information. <em>APIN</em>,
<em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-025-06273-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Ma, Xi-Ao and Xu, Hao and Liu, Yi},
  doi          = {10.1007/s10489-025-06273-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Dynamic interactive weighted feature selection using fuzzy interaction information},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PTLO: A model-agnostic training strategy based on
progressive training and label optimization for fine-grained image
classification. <em>APIN</em>, <em>55</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s10489-025-06276-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to conventional image recognition, fine-grained classification exhibits increased vulnerability to labeling noise due to the presence of closely related categories, resulting in degraded performance on complex and non-representative samples. While existing approaches mitigate these issues through data cleaning, loss modification, and semi-supervised learning techniques, they often overlook the intrinsic attributes within training samples. Instead of designing any network architectures, this study introduces a model-agnostic progressive training strategy comprising of progressive training and label optimization, where the former is to decrease the affect from the noisy samples by facilitating a graduated learning approach in an easy-to-hard manner, while the latter is to denoise the label noises. Theoretical analysis also demonstrates that the proposed method uncovers valuable cues hidden in the training data, thereby enhancing the robustness of any learning-based models. Experimental evaluations on fine-grained classification benchmarks (e.g., CUB-200-2011, DTD, and Food-101) across various mainstream classification networks demonstrate the effectiveness of our training strategy. Code is available at https://github.com/cb-rep/LPPT .},
  archive      = {J_APIN},
  author       = {Chen, Yiming and Tao, Xiuting and Chen, Bo and Guo, Jian and Li, Shi},
  doi          = {10.1007/s10489-025-06276-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Appl. Intell.},
  title        = {PTLO: A model-agnostic training strategy based on progressive training and label optimization for fine-grained image classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-contextual stress prediction: Simple methodology for
comparing features and sample domain adaptation techniques in vital sign
analysis. <em>APIN</em>, <em>55</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s10489-025-06277-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress significantly impacts individuals, particularly in professions like nursing and driving, leading to severe health risks and accidents. Accurate stress measurement is critical for effective interventions, yet research is hindered by incomplete datasets and inconsistent methodologies, slowing the development of reliable predictive models. This paper introduces a framework for cross-contextual stress prediction, enabling the generation of general stress prediction models adaptable to specific domain challenges. The methodology leverages two general daily life datasets and three domain-specific datasets, employing steps such as dataset selection, feature extraction, significant feature identification, feature preprocessing, fine-tuning, domain adaptation, and application to specific contexts. Through this framework, key vital signs were identified as significant predictors of stress, including electrocardiography (ECG), heart rate (HR), heart rate variability (HRV) - low frequency (LF), electrodermal activity (EDA), body temperature (TEMP), and skin conductance response (SCR). The experiments conducted include: 1) Utilizing HR and HRV-LF through domain adaptation from general to automobile driving datasets; 2) Applying EDA, HR, and TEMP from general to specific nurse activity datasets; and 3) Adapting ECG, HR, and TEMP from general to automobile driving datasets. Results demonstrate the potential of the proposed framework for cross-contextual stress prediction, with HR and HRV-LF identified as pivotal features. When applied to target datasets specific to stress scenarios, the model achieved a 62% F1 score, demonstrating the effectiveness of the feature-based Correlation Alignment (CORAL) technique combined with Random Forest models in transferring learned knowledge across domains. These findings highlight the robustness of the approach in adapting general stress prediction models to specific contexts, paving the way for real-world applications such as stress monitoring in driving and nursing during high-stress periods like COVID-19.},
  archive      = {J_APIN},
  author       = {Mihirette, Samson and De la Cal, Enrique A. and Tan, Qing and Sedano, Javier},
  doi          = {10.1007/s10489-025-06277-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Cross-contextual stress prediction: Simple methodology for comparing features and sample domain adaptation techniques in vital sign analysis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised feature learning using locality-preserved
auto-encoder with complexity-invariant distance for intelligent fault
diagnosis of machinery. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06278-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature learning (UFL) has been recognized as a promising feature extractor in machinery fault diagnosis, where the auto-encoder is a very popular UFL framework. For the auto-encoder methods, however, it is still a great challenge to learn discriminative features from complex signals in an unsupervised manner. In this paper, a new UFL method named locality-preserved auto-encoder (LPAE) is proposed by explicitly designing a locality-preserved penalty term. Concretely, the penalty term constrains local geometry of samples in the original space to be well preserved in the reconstruction space, enabling more discriminative features to be learned accordingly. To better formulate this term, the complexity-invariant distance (CID) is employed to measure similarity between two mechanical signals so as to construct a reliable neighbor graph. On a rolling bearing dataset, experimental results verify that the proposed LPAE can learn sufficiently discriminative features from complex vibration signals collected from varying operating conditions, and achieves a remarkable and superior diagnosis performance over the existing advanced UFL methods. Moreover, the effectiveness of CID has been adequately validated by comparing with several other distance measurement methods. The proposed LPAE can be applied to the feature extraction stage of machinery fault diagnosis, which provides a potential solution for engineers to realize unsupervised learning of discriminative features.},
  archive      = {J_APIN},
  author       = {Lu, Zhenghua and Chu, Zhaobi and Zhu, Min and Dong, Xueping},
  doi          = {10.1007/s10489-025-06278-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised feature learning using locality-preserved auto-encoder with complexity-invariant distance for intelligent fault diagnosis of machinery},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised symmetric non-negative matrix factorization
with graph quality improvement and constraints. <em>APIN</em>,
<em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06282-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric non-negative matrix factorization (SNMF) decomposes a similarity matrix into the product of an indicator matrix and its transpose, allowing clustering results to be directly extracted from the indicator matrix without additional clustering methods. Furthermore, SNMF has been shown to be effective in clustering nonlinearly separable data. SNMF-based clustering methods significantly depend on the quality of the pairwise similarity matrix, yet their effectiveness is often hindered by the reliance on predefined matrices in most semi-supervised SNMF approaches. Thus, we propose a novel algorithm, named semi-supervised symmetric non-negative matrix factorization with graph quality improvement and constraints ( $$\text {S}^{3}\text {NMFGC}$$ ), addressing this limitation by employing an integrated clustering strategy that dynamically generates and adaptively updates the similarity matrices. This is accomplished by integrating a weighted graph construction based on multiple clustering results, a label propagation algorithm, and pairwise constraint terms into a unified optimization framework that enhances the semi-supervised SNMF model. Subsequently, we adopt an alternating iterative update method to solve the optimization problem and prove its convergence. Rigorous experiments highlight the superiority of our model, which outperforms seven state-of-the-art NMF methods across six datasets.},
  archive      = {J_APIN},
  author       = {Ren, Xiaowan and Yang, Youlong},
  doi          = {10.1007/s10489-025-06282-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised symmetric non-negative matrix factorization with graph quality improvement and constraints},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph denoising neural network for session-based
recommendation. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06283-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) predicts the next interaction of users based on their clicked items in a session. Previous studies have shown that hypergraphs are superior in capturing complex item transitions which contribute to SBR performance. However, existing hypergraph-based methods fail to model item co-occurrence and sequential patterns simultaneously, limiting the improvement of recommendation performance. Moreover, they are more sensitive to noisy items than conventional graph models due to the item association mechanism. In this paper, we propose a novel hypergraph-based method named Hypergraph Denoising Neural Network (HDNN) for SBR to tackle the abovementioned problems. The proposed method involves two newly-designed modules: a sequential pattern learning module (SPLM) and an adaptive attention selection module (AASM). In particular, SPLM models item sequential patterns to complement the hypergraph-based models which only focus on co-occurrence patterns. Meanwhile, AASM employs learnable attention score thresholds to exclude items with low attention scores, mitigating the impact of noisy items in hypergraphs. Furthermore, the sequential denoising unit (SDU) designed in SPLM is employed to eliminate noise in item sequential patterns, thus realizing the dual denoising purpose. Extensive experiments are conducted on three real-world datasets. The results of the experiments show that our HDNN framework shows better performance than the state-of-the-art models. In particular, all evaluation metrics in Tmall and RetailRocket showed improvements of over 15% and 5%, respectively.},
  archive      = {J_APIN},
  author       = {Ding, Jiawei and Tan, Zhiyi and Lu, Guanming and Wei, Jinsheng},
  doi          = {10.1007/s10489-025-06283-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Hypergraph denoising neural network for session-based recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking probability volume for multi-view stereo: A
probability analysis method. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06284-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing learning-based multi-view stereo (MVS) models primarily focus on predicting depth maps through a cascaded structure to achieve more robust reconstruction results. However, they often emphasize improving the quality of stereo matching while overlooking the importance of depth hypotheses. In this paper, we propose a novel MVS model from the perspective of probability volume analysis. First, the guiding effect of the probability volume is considered for depth refinement. Ideally, the probability distribution along the depth dimension of the probability volume follows an unimodal pattern. We design an unimodal curve to fit this pattern. Then, a reasonable depth refinement range is adaptively selected for each pixel position based on a predefined probability threshold. Additionally, considering that matching noise may cause the probability volume to appear as a blurred unimodal peak, we design the probability volume split-merge module (PVS-PVM). This module performs a peak search based on conditional constraints, splitting the probability volume into main and sub probability volumes, then computes the two sets of depth hypotheses from them. Finally, the new main and sub probability volumes are computed based on these depth hypotheses and merged to predict the depth. This approach allows for a more comprehensive consideration of the regions with higher probability, improving the robustness of depth hypotheses. Experimental results demonstrate that our method effectively utilizes probability volume information to guide depth map refinement and yields enhanced reconstruction results on the DTU and Tanks &amp; Temples datasets. Our code will be released at https://github.com/zongh5a/ProbMVSNet .},
  archive      = {J_APIN},
  author       = {Yu, Zonghua and Wang, Huaijun and Li, Junhuai and Jin, Haiyan and Cao, Ting and Cheng, Kuanhong},
  doi          = {10.1007/s10489-025-06284-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Rethinking probability volume for multi-view stereo: A probability analysis method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You are what your feeds make you: A study of user aggressive
behavior on twitter. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06286-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of aggressive language on Twitter raises concerns about potential negative influences on user behavior. Despite previous research exploring aggression and negativity on the platform, the relationship between consuming aggressive content and users’ aggressive behavior remains underexplored. This study investigates whether exposure to aggressive content on Twitter can lead users to behave more aggressively. Our methodological approach contains four stages: data collection and annotation, aggressive post detection, user aggression intensity metric, and user profiling. We proposed the English Twitter Aggression dataset (TAG-EN) with substantial inter-annotator agreement (Krippendorff’s alpha=0.78). Subsequently, we benchmark the aggression detection performance on TAG-EN dataset (macro F1=0.92) by fine-tuning a pre-trained RoBERTa-large. We quantified user aggression with a proposed “user aggression intensity” metric based on their overall aggressive activity. Our analysis of 14M posts from 63K users revealed that aggressive Twitter feeds can influence users to behave more aggressively online. Furthermore, the study found that users tend to support and encourage aggressive content on social media, which can contribute to the proliferation of aggressive behavior.},
  archive      = {J_APIN},
  author       = {Mane, Swapnil and Kundu, Suman and Sharma, Rajesh},
  doi          = {10.1007/s10489-025-06286-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {You are what your feeds make you: A study of user aggressive behavior on twitter},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view learning based on product and process metrics for
software defect prediction. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06288-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction plays a crucial role as a quality assurance technology in software development. The software metrics are associated with the software quality and are vital for prediction models. Most existing defect prediction methods build the prediction model ignoring the complementary information between these two kinds of metrics. In this work, we intend to jointly leverage these two kinds of metrics. For a software instance, we regard the product metrics and the process metrics as its two views. We model the problem of discriminative feature learning from these two kinds of metrics as the problem of multi-view learning. However, it is a challenging task to construct an effective prediction model based on both product and process metrics due to the heterogeneity in data of product and process metrics, and the defect data often has class imbalance characteristic. How to explore the discriminant both inter-view and intra-view effectively has not been well studied. These characteristics make it challenging to construct an effective prediction model. In this paper, we propose a Deep Multi-view Defect Prediction (DMDP) approach, which can predict software defect based on both product and process metrics. We design a neural network with two sub-network branches, which are enforced to share the weights in the last output layer, to map the data from different views to a common space. To guide the training of networks, we design the loss function including the discrepancy loss, discrimination loss and classification loss, which further promotes the distribution consistency across views, makes full use of label information to obtain the discriminative representations, and utilizes the complementarity information for prediction. To alleviate the class imbalance problem, we design a dynamic sampling strategy for dealing with class-imbalanced data. Comprehensive experiments are conducted on 15 projects from three widely used defect datasets. The experimental results demonstrate that multi-view learning based on product and process metrics is helpful for software defect prediction and DMDP outperforms the state-of-the-art baselines.},
  archive      = {J_APIN},
  author       = {Sun, Ying and Wu, Fei and Wu, Di and Jing, Xiao-Yuan and Sun, Yanfei},
  doi          = {10.1007/s10489-025-06288-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view learning based on product and process metrics for software defect prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user preference knowledge graph incorporating
spatio-temporal transfer features for next POI recommendation.
<em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06290-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs can improve the performance of recommendation systems and provide explanations for recommendation results, which have been widely applied in the next Point-of-Interest (POI) recommendation. However, the current knowledge graph method for the next POI recommendation focuses on the static attributes of POIs, and only describes the spatio-temporal characteristics when the user transfers between POIs. To fully tap into user preferences for different POIs, we have done the following innovative work. (1) We construct a user preference knowledge graph with spatio-temporal characteristics, named UPSTKG, which expresses preference information from both individual user and global user perspectives. (2) We use local preference triplets in preference knowledge graphs to construct user preference graphs. And use GCN to obtain user preference vectors to replace common user vectors in the sequence, thereby strengthening the potential connection between users and different POIs. (3) We combine UPSTKG and user preference graph to propose the UPSTKGRec method for the next POI recommendation. To evaluate the effectiveness of UPSTKGRec, it is compared to six highly regarded techniques on three distinct benchmark datasets. Compared with the baseline, the average performance of indicators recell@5 and NDCG@5 has increased by 13.8% and 13.1%.},
  archive      = {J_APIN},
  author       = {Sang, Chun-Yan and Yang, Yang and Zhang, Yi-Bo and Liao, Shi-Gen},
  doi          = {10.1007/s10489-025-06290-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A user preference knowledge graph incorporating spatio-temporal transfer features for next POI recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sparsity detection-based evolutionary algorithm for
large-scale sparse multi-objective optimization problems. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06291-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sparse multi-objective optimization problems (LSSMOPs) widely exist in practical applications, which have large-scale decision variables and sparse Pareto optimal solutions. Existing algorithms have some shortcomings in dealing with LSSMOPs: (1) failing to make full use of the knowledge of the sparsity of the Pareto optimal solutions, leading to insufficient sparsity detection; (2) ignoring the connection between binary and real variables, leading to insufficient variables optimization. This paper proposes an adaptive sparsity detection-based evolutionary algorithm (ASD-MOEA) to address these issues, which is a two-stage algorithm. The first stage performs an adaptive sparsity detection strategy, which dynamically adjusts the probability of binary variables flipping and the fitness of decision variables according to the iteration ratio. Then, non-zero variables are mined based on fitness. The second stage performs a variable grouping-based optimization strategy, grouping decision variables according to their sparsity in the set of non-dominated solutions to reduce the search space, then performs genetic operations in the subspace. Finally, we compare ASD-MOEA with six mainstream algorithms. The results show that the proposed algorithm significantly outperforms the existing algorithms in dealing with LSSMOPs, and achieves a balance between sparsity maintenance and variable optimization.},
  archive      = {J_APIN},
  author       = {Qiu, Feiyue and Long, Donghui and Chen, Qi and Hu, Huizhen and Qiu, Qicang},
  doi          = {10.1007/s10489-025-06291-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive sparsity detection-based evolutionary algorithm for large-scale sparse multi-objective optimization problems},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TITD: Enhancing optimized temporal position encoding with
time intervals and temporal decay in irregular time series forecasting.
<em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06293-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate Time Series (MTS) acquisition processes often exhibit irregularities, making accurate MTS forecasting challenging. Previous researches focused on interpolation approaches to address data completeness in irregular MTS, but these approaches may introduce noise, thereby altering the feature distributions of irregular MTS. Recent researches trend advocate embedding the missing temporal information through position encoding for forecasting irregular MTS. However, these position encodings were typically designed for text sequences and assumed fixed time intervals, which lead to the loss or distortion of temporal information when applied to irregular MTS. Moreover, they struggled to capture the temporal dynamic information in irregular MTS. To address these challenges, we propose a novel approach called TITD (Time Interval and Temporal Decay), which utilizes time interval and temporal decay information to enhance irregular MTS forecasting. TITD optimizes position encoding to effectively capture both local time interval features and long-term temporal decay patterns, breaking the limitations of static and fixed interval position encoding on time dynamic representation. Simultaneously, TITD integrates multi-view input information from irregular MTS to enhance the representation learning of the relationships across different views, thereby achieving superior forecasting performance without interpolation. Extensive experiments on three real-world time series datasets have demonstrated that TITD provides significant improvements over state-of-the-art methods in irregular MTS forecasting.},
  archive      = {J_APIN},
  author       = {Ji, Jinquan and Cao, Yu and Ma, Yukun and Yan, Jianzhuo},
  doi          = {10.1007/s10489-025-06293-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {TITD: Enhancing optimized temporal position encoding with time intervals and temporal decay in irregular time series forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rail-PatchCore: Unsupervised learning-based detection of
visual anomalies in the railway-turnout environment. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06294-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and openness of railway turnout environments pose great challenges to anomaly detection, and supervised methods are highly dependent on labels, making it difficult to address the diverse types of anomalies and the scarcity of samples in turnout environments. To solve these problems, this paper proposes a new method, Rail-PatchCore, which is based on unsupervised learning and effectively reduces the interference of background noise and enhances the ability to capture anomalous features by adding a Dual-Dimensional Channel Attention (DDCA) module and a projection anomaly scoring module to the PatchCore model. The experiments on our railway-turnout anomaly detection dataset(RTAD) and other datasets (RSDDs, MVTec-AD, BTAD, AEBAD-S) show that the detection performance of Rail-PatchCore is better than that of the existing methods, and the image-level and pixel-level AUCROC indices of Rail-PatchCore on the railway turnout anomaly detection dataset reach 72.2% and 95.3%, respectively. This approach provides an efficient and reliable solution for anomaly detection in railway turnout environments.},
  archive      = {J_APIN},
  author       = {Zhang, YuanHao and Yu, Zujun and Zhu, Liqiang and Guo, Baoqing and Wang, Yao},
  doi          = {10.1007/s10489-025-06294-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Rail-PatchCore: Unsupervised learning-based detection of visual anomalies in the railway-turnout environment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view human point cloud registration method with
overlapping regions semantic constraints and feature weighting.
<em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06296-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view human point cloud registration is a crucial step in 3D human reconstruction tasks. The symmetric structures and similar geometric features in human point clouds often lead to feature mismatches in point cloud registration. Therefore, we propose a pipeline for game tree registration based on semantic constraints and feature weighting (GTR-SCFW) that enhances the stability and accuracy of feature matching, thereby improving the registration precision of multi-view point clouds. First, we calculate and compare the feature similarity between multi-view point clouds and use a generalized best-first search (BFS) method to construct a multi-layered registration game tree. At each game node, overlapping regions are divided into multiple sub-regions based on semantic information, and global fast registration is used to determine the matching relationships of features within each sub-region. Then, the best matching points in each sub-region are selected based on the confidence of feature pairs, and the weights of all the best point pairs are calculated. Finally, the initial rigid transformation matrix is computed using weighted least squares (WLS), and ICP is employed to achieve fast fine registration. GTR-SCFW effectively avoids incorrect matching relationships caused by geometric feature similarity during the initial transformation estimation, providing a good initial pose for iterative closest point (ICP) fine registration. For point clouds with different initial poses, the registration’s rotational error approaches 0 $$^\circ $$ , while the translational error is as low as 1.203e-4 mm. Comparative experimental results show that this method outperforms existing feature-based registration methods regarding robustness, reliability, and computational efficiency.},
  archive      = {J_APIN},
  author       = {Li, Ming and Li, Guiqin and Li, Xihang and Li, Tiancai},
  doi          = {10.1007/s10489-025-06296-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view human point cloud registration method with overlapping regions semantic constraints and feature weighting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A complex history browsing text categorization method with
improved BERT embedding layer. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06298-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For long texts composed of multiple short fragments, the importance of each fragment to the classification task varies. Some fragments have higher discriminative power and positively contribute to the classification, while others lack discriminative power or even mislead it. Existing methods struggle to converge when handling texts with negative examples. This study analyzes user behavior and assigns interest scores to text fragments based on their classification relevance, allowing the model to focus more on important fragments. Building on bidirectional encoder representations from transformers (BERT), we propose an interest encoding layer model for historical browsing texts. By analyzing user behavior and incorporating an improved term frequency-inverse document frequency (TF-IDF) method, the model adds indicators to fragments with higher discriminative power for user behavior analysis, enabling the model to focus more on these during training. Finally, comparative experiments on the BERT model series validate the advantages of the proposed approach.},
  archive      = {J_APIN},
  author       = {Wang, Yuanhang and Zhou, Yonghua and Qi, Huiyu and Wang, Dingyi and Huang, Annan},
  doi          = {10.1007/s10489-025-06298-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {A complex history browsing text categorization method with improved BERT embedding layer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end audio classification framework with diverse
features for obstructive sleep apnea-hypopnea syndrome diagnosis.
<em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06299-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a prevalent chronic disorder that affects sleep quality and general health. The current diagnostic methods, primarily polysomnography (PSG), are laborious. Furthermore, audio-based methods for diagnosing OSAHS face limited sample sizes and neglect patients’ physiological signs and medical histories. To address these challenges, we introduce a data-driven framework called DFNet, which also considers patients’ medical histories and health indicators. DFNet incorporates an automated audio segmentation- and labeling-based preprocessing procedure to reduce expert annotation costs and subjective errors. We employed random convolutional kernels based on receptive fields for audio feature extraction purposes. These kernels captured both local and global features within the input audio. Additionally, for the first time, we introduced a medical language model that utilizes patients’ medical histories and physiological information as covariates to enhance features. We extensively validated DFNet on an OSAHS dataset obtained from a collaborative university hospital. Our framework classified patients into four categories according to their OSAHS severity: normal, mild, moderate, and severe. DFNet achieved state-of-the-art performance, with a four-class accuracy of 84.12%. DFNet offers a large-scale and cost-effective screening approach for diagnosing OSAHS, reducing the labor requirements of diagnosis. Our code is available at https://github.com/testlbin/DFNet .},
  archive      = {J_APIN},
  author       = {Li, Bin and Qiu, Xihe and Tan, Xiaoyu and Yang, Long and Tao, Jing and Fang, Zhijun and Huang, Jingjing},
  doi          = {10.1007/s10489-025-06299-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {An end-to-end audio classification framework with diverse features for obstructive sleep apnea-hypopnea syndrome diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple instance learning with hierarchical discrimination
and smoothing attention for histopathological diagnosis. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06300-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The microscopic structure of human tissue can be observed by pathological slides, which provides a strong basis for cancer diagnosis. However, the serious lack of experienced pathologists and the complexity of the diagnostic process have facilitated the development of computer-aided pathological image analysis. Pathological slides generally have high resolution, and multiple instance learning (MIL) has been widely used in histopathological whole slide image (WSI) analysis, where each WSI has a large number of unlabelled patches and only a WSI-level label is given. The bag-based MIL methods often learn the decision boundary at the bag level, and thus hard to learn the discriminative features at the instance level. Furthermore, the difficulty of identification varies between positive instances in a bag, and the existing attention-based aggregation methods always assign higher attention scores for the easy-to-identify positive instances, but assign lower attention scores for the difficult-to-identify positive instances and thus cannot learn these difficult instances sufficiently. In this paper, we propose a novel MIL method with hierarchical discrimination learning and a smoothing attention strategy for cancer subtype diagnosis. Particularly, to learn hierarchical discriminative features, the proposed MIL method simultaneously trains a bag classifier and multiple instance classifiers, where the multi-way attention scores of each instance for different categories are used to guide the selection of training samples for the instance classifimer. The smoothing strategy is designed to trade off the attention weights between the easily and hardly identifiable positive instances. We conducted experiments on histopathological diagnosis datasets and achieved state-of-the-art performance. Codes are available at https://github.com/bravePinocchio/HDSA-MIL.},
  archive      = {J_APIN},
  author       = {Zhao, Jing and Zhao, Zhikang and Song, Xueru and Sun, Shiliang},
  doi          = {10.1007/s10489-025-06300-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Multiple instance learning with hierarchical discrimination and smoothing attention for histopathological diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex layout generation for large-scale floor plans via
deep edge-aware GNNs. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06311-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In architectural layout generation, deep learning techniques have advanced the residential generation in multiple scenarios. However, current approaches fail to extract complex graph features from large-scale layouts, neglecting large-scale global context. Additionally, the lack of robust, quantitative evaluation metrics for layouts hampers the objective comparison of different generative approaches. To address these issues, we propose a multi-scale applicable layout generation method based on deep edge-aware GNNs, stressing edge-specific and non-local spatial information. Next, we introduce quantitative metrics to assess layout quality, including room accessibility index and space property proportion, whose purpose is to establish layout standards in the computer-aided design field. Lastly, we create the Public Space Floor Plan Dataset (P-PLAN), a collection of 4,535 annotated layout samples designed to serve as a robust evaluation platform for large-scale layout models. We conducted extensive qualitative and quantitative experiments on the Residential Floor Plan Dataset (R-PLAN) and P-PLAN dataset to demonstrate the effectiveness of the proposed method. Notably, with the proposed evaluation metrics, our method significantly outperforms existing models in accessibility and diversity.},
  archive      = {J_APIN},
  author       = {Lu, Zhengyang and Li, Yifan and Wang, Feng},
  doi          = {10.1007/s10489-025-06311-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Complex layout generation for large-scale floor plans via deep edge-aware GNNs},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-attention fusion and edge-guided fully supervised
contrastive learning network for rail surface defect detection.
<em>APIN</em>, <em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06314-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been significant research focus on efficiently and accurately detecting defects on rail surfaces using computer vision. Utilizing depth information from the rail surface has emerged as an effective approach for detecting visually insignificant types of defects that are unique in nature. However, previous methods have typically overlooked the long-distance dependency between the two modalities when fusing them using conventional convolutional network methods. Additionally, these methods have often relied on traditional cross-entropy loss for edge supervision without considering the intra and inter-pixel relationships associated with edge features. To address these limitations, we propose a novel approach called CECLNet (cross-attention fusion and edge-guided fully supervised contrastive learning network) for rail surface defect detection (RSDD). The proposed CECLNet incorporates a module for inter-modal cross-attention fusion, which effectively explores the complementary information by considering the long-range relationship. Furthermore, we introduce a progressive aggregation-based multiscale feature interactions decoder to promote sufficient information interaction between multiscale features, thus facilitating the generation of final predictions. Finally, we propose a pixel-level fully supervised contrastive learning approach to enhance the efficiency of utilizing edge-assisted information. Extensive experiments conducted on the industrial NEU RGB-D RSDDS-AUG dataset demonstrate the superiority of our proposed CECLNet over 17 state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Yang, Jinxin and Zhou, Wujie},
  doi          = {10.1007/s10489-025-06314-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Cross-attention fusion and edge-guided fully supervised contrastive learning network for rail surface defect detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamically modulated robot compliance via online fuzzy
neural networks for individualized ankle rehabilitation. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06317-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Admittance control, benefiting from human-robot interaction compliance, is widely used in rehabilitation robot studies. However, using inappropriate parameters for the admittance control model can cause harm like overuse syndrome. Therefore, it is necessary to dynamically adjust these parameters to assist patients under varying recovery periods, enabling active rehabilitation training across a wider range of recovery stages. Integrating multiple intelligent approaches presents a promising solution to this challenge. This paper proposes a variable admittance control strategy that employs a variable operator fuzzy neural network (VAC-VOFNN). The VOFNN facilitates the fuzzification of the inference process, leading to superior non-linear fitting capability. Additionally, the network’s parameters are updated online to match the rehabilitation stages of different subjects. Compared to the admittance control strategy with fixed parameters (ACS-FP) and the variable admittance control strategy with fuzzy neural networks (VAC-FNN), the proposed strategy reduces the root mean square (RMS) of surface electromyography (sEMG) from the medial gastrocnemius by 29.14% and 29.04%, respectively, while also decreasing the average interaction torque by 28.63% and 12.24%, respectively. These results suggest that the proposed strategy leads to reduced effort from subjects and increased training cycles before muscle fatigue during the same rehabilitation activities. This makes it beneficial for ankle rehabilitation of patients in different recovery periods.},
  archive      = {J_APIN},
  author       = {Li, Jianfeng and Zhou, Yu and Zuo, Shiping and Dong, Mingjie},
  doi          = {10.1007/s10489-025-06317-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Dynamically modulated robot compliance via online fuzzy neural networks for individualized ankle rehabilitation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust decision-making for autonomous vehicles via deep
reinforcement learning and expert guidance. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06319-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate decision-making within highly interactive driving environments is vital for the safety of self-driving vehicles. Despite the significant progress achieved by the existing models for autonomous vehicle decision-making tasks, there remains untapped potential for further exploration in this field. Previous models have focused primarily on specific scenarios or single tasks, with inefficient sample utilization and weak robustness problems, making them challenging to apply in practice. Motivated by this, a robust decision-making method named DRL-EPKG is proposed, which enables the simultaneous determination of vertical and horizontal behaviors of driverless vehicles without being limited to specific driving scenarios. Specifically, the DRL-EPKG integrates human driving knowledge into a framework of soft actor-critic (SAC), where we derive expert policy by a generative model: variational autoencoders (VAE), train agent policy by employing the SAC algorithm and further guide the behaviors of the agent by regulating the Wasserstein distance between the two policies. Moreover, a multidimensional reward function is designed to comprehensively consider safety, driving velocity, energy efficiency, and passenger comfort. Finally, several baseline models are employed for comparative evaluation in three highly dynamic driving scenarios. The findings demonstrate that the proposed model outperforms the baselines regarding the success rate, highlighting the practical applicability and robustness of DRL-EPKG in addressing complex, real-world problems in autonomous driving.},
  archive      = {J_APIN},
  author       = {Li, Feng-Jie and Zhang, Chun-Yang and Chen, C. L. Philip},
  doi          = {10.1007/s10489-025-06319-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Robust decision-making for autonomous vehicles via deep reinforcement learning and expert guidance},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaotic opposition-based plant propagation algorithm for
engineering problem. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06320-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Plant Propagation Algorithm (PPA), often exemplified by the Strawberry Algorithm, has demonstrated its effectiveness in solving lower-dimensional optimization problems as a neighborhood search algorithm. While multiple enhancements have been introduced to boost its performance, PPA remains a population-based metaheuristic algorithm. A key element of PPA involves balancing exploration and exploitation, akin to a strawberry plant seeking the best survival strategy. This paper delves into the integration of chaotic numbers and opposition theory in PPA, focusing on how these additions impact its efficiency. The primary research questions revolve around enhancing PPA’s performance and reducing its search space to expedite the algorithm, ultimately leading to faster overall results. Experiments were carried out on three challenging engineering problems: the Pressure Vessel Optimization, the Spring Design Optimization, and the Welded Beam Problem, to fully assess the effectiveness of the improved PPA. The effectiveness of the original PPA, the Chaotic Opposition-Based PPA (COPPA), and several other metaheuristic algorithms were examined in each of these problems. In terms of efficiency and solution quality, the findings consistently demonstrate that COPPA performs better than the traditional PPA and other algorithms. The results indicate that using chaotic-based oppositional processes decreases the search space and enhances performance, resulting in faster and more resource-efficient optimization. The investigation reveals that incorporating chaotic-based oppositional PPA yields improved results while conserving resources and accelerating execution.},
  archive      = {J_APIN},
  author       = {Suny, Alfe and Liza, Maimuna Akter and Fahim, Md. and Reza, Ahmed Wasif and Siddique, Nazmul},
  doi          = {10.1007/s10489-025-06320-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Chaotic opposition-based plant propagation algorithm for engineering problem},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep random walk inspired multi-view graph convolutional
networks for semi-supervised classification. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06322-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies highlight the growing appeal of multi-view learning due to its enhanced generalization. Semi-supervised classification, using few labeled samples to classify the unlabeled majority, is gaining popularity for its time and cost efficiency, particularly with high-dimensional and large-scale multi-view data. Existing graph-based methods for multi-view semi-supervised classification still have potential for improvement in further enhancing classification accuracy. Since deep random walk has demonstrated promising performance across diverse fields and shows potential for semi-supervised classification. This paper proposes a deep random walk inspired multi-view graph convolutional network model for semi-supervised classification tasks that builds signal propagation between connected vertices of the graph based on transfer probabilities. The learned representation matrices from different views are fused by an aggregator to learn appropriate weights, which are then normalized for label prediction. The proposed method partially reduces overfitting, and comprehensive experiments show it delivers impressive performance compared to other state-of-the-art algorithms, with classification accuracy improving by more than 5% on certain test datasets.},
  archive      = {J_APIN},
  author       = {Chen, Zexi and Chen, Weibin and Yao, Jie and Li, Jinbo and Wang, Shiping},
  doi          = {10.1007/s10489-025-06322-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Deep random walk inspired multi-view graph convolutional networks for semi-supervised classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overview of the application of intelligent optimization
algorithms in multi-attribute group decision making. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06324-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Optimization Algorithms (IOAs) have great potential in solving multi-attribute group decision-making (MAGDM) problems. These problems have gradually become a research hotspot in the field of intelligent decision-making due to their advantages of high decision-making accuracy, versatility, and objective evaluation. This study provides a detailed analysis of the challenges in the MAGDM process and evaluates the feasibility of applying IOAs in this context. Specifically, we study the application of IOAs in the MAGDM process and discuss the advantages and limitations across various application scenarios, including the applications of granulating linguistic information, adjusting decision information, optimizing weights, and aggregating decision information. In addition, the development prospects and challenges of IOAs integration with MAGDM are summarized.},
  archive      = {J_APIN},
  author       = {Kang, Kaiying and Xie, Jialiang and Liu, Xiaohui and Wang, Honghui},
  doi          = {10.1007/s10489-025-06324-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Overview of the application of intelligent optimization algorithms in multi-attribute group decision making},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MVSRF: Point cloud semantic segmentation and optimization
method for granular construction objects. <em>APIN</em>, <em>55</em>(6),
1–16. (<a href="https://doi.org/10.1007/s10489-025-06326-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying shapeless granular materials in complex construction scenarios is critical for achieving automation in engineering equipment such as wheel loaders. The challenges of segmenting point clouds for granular materials involve dealing with sparsity, real-time processing requirements, the lack of distinct shape representation, and the issue of different materials sharing similar shapes. This paper proposes MVSRF, a real-time multi-view based point cloud semantic segmentation method incorporating a single-frame re-segmentation component and a multi-frame semantic filter to enhance accuracy and robustness. First, the segmentation system generates a sparse pixel-depth grid map via semantic projection to encapsulate global points and their behaviors, while employing an edge detector to label boundary points around objects. Second, a zero-shot re-segmentation algorithm involving seed extension, novel one-dimensional DBSCAN, Delaunay triangulation, and semantic reassignment corrects mis-segmented points caused by mapping bias. Finally, a lightweight semantic filter is designed to suppress semantic noise during multiple observations. We have built a multi-sensor platform on a wheel loader and collected experimental data to verify the effectiveness of our method. Two optimization components illustrated exceptional performance on the annotated dataset. The MVSRF method possesses strong robustness against external calibration errors, camera pose estimation errors, and inaccurate image segmentation, providing a practical solution for real-time perception of granular materials.},
  archive      = {J_APIN},
  author       = {Zhang, Lunhui and Liu, Guangjun and Lu, Jiaqi and Wang, Changxin},
  doi          = {10.1007/s10489-025-06326-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {MVSRF: Point cloud semantic segmentation and optimization method for granular construction objects},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D-CNN and multi-loss video prediction architecture.
<em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06328-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The achievements of deep learning in the sphere of computer vision have elevated video prediction to a prominent research focus. The prevailing trend in current deep learning endeavors is to pursue advanced optimization of model architectures and enhancement of their performance metrics. The task of video prediction is inherently complex, and most of the algorithm models proposed in the past are also. In this paper, we propose a novel simple video prediction network structure based on three-Dimensional Convolutional Neural Network (3D-CNN) and multi-loss, abbreviated as ML3DVP. Our network model is completely based on 3D-CNN. Compared with Convolutional Long Short-Term Memory (ConvLSTM), Recurrent Neural Network (RNN), Generative Adversarial Network (GAN) and its variants, we start from the most basic network structure to reduce complexity, thereby improving the speed of model prediction. In addition, most models today will encounter quality problems such as insufficient clarity. To solve this problem, we introduced multiple losses for back propagation. Using multiple quality evaluation indicators, Structural Similarity (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as optimization objectives, continuously improves the prediction quality during the training process. The evaluation of model complexity, parameter count, and predictive outcomes across four datasets substantiates that our proposed model has successfully attained the objectives of structural refinement and enhanced performance.},
  archive      = {J_APIN},
  author       = {Qin, Ziru and Dai, Qun},
  doi          = {10.1007/s10489-025-06328-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {A 3D-CNN and multi-loss video prediction architecture},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concept agent network for zero-base generalized few-shot
learning. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06331-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Few-Shot Learning (GFSL) aims to recognize novel classes with limited training samples without forgetting knowledge of auxiliary data (base classes). Most current approaches re-engage the base classes after initial training to balance the predictive bias between the base and novel classes. However, re-using the auxiliary data might not always be possible due to privacy or ethical constraints. Consequently, the zero-base GFSL paradigm emerges, where models trained on the base classes are directly fine-tuned on the novel classes without revisiting the auxiliary data, avoiding the re-balancing of prediction biases. We believe that solving this paradigm relies on a critical yet often overlooked issue: feature overlap between the base and novel classes in the embedding space. To tackle this issue, we propose the Concept Agent Network, a novel framework that interprets visual features as affinity features, thereby effectively diminishing feature overlap by aggregating feature embeddings of the novel classes according to their similarity with the base classes. Additionally, we present the Concept Catena Generator, which creates multiple concepts per base class, improving understanding of the feature distribution of the base classes and clarifying the relationships between the base and novel concepts. To prevent the catastrophic forgetting of the base classes when adapting to the novel ones, we propose an Active Training Regularization strategy, promoting the preservation of base class knowledge. Extensive experimental results on two benchmarks, mini-ImageNet and tiered-ImageNet, have demonstrated the effectiveness of our framework. The potential utility of our framework spans several real-world applications, including autonomous driving, medical image analysis, and real-time surveillance, where the ability to rapidly learn from a few examples without forgetting previously acquired knowledge is critical.},
  archive      = {J_APIN},
  author       = {Wang, Xuan and Ji, Zhong and Liu, Xiyao and Pang, Yanwei and Li, Xuelong},
  doi          = {10.1007/s10489-025-06331-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Concept agent network for zero-base generalized few-shot learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAAR: Dual attention cooperative adaptive pruning rate by
data-driven for filter pruning. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06332-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model compression can address the limitations of deep learning in resource-constrained situations by reducing the computational and storage requirements of the model. Structured pruning has emerged as an important compression technique because of its operational flexibility and effectiveness. However, the existing structural pruning methods have two limitations: 1) They use a single measurement to identify the importance of the filters in all the layers, resulting in a loss of spatial information in the shallow layers. 2) The pruning rate is highly dependent on manual interference, which is highly subjective. In this paper, a filter pruning method called dual attention cooperative adaptive pruning rate (DAAR) is proposed. Specifically, a dual attention module that combines spatial attention and channel attention is proposed to measure the effectiveness of the filters. Spatial attention is used in the shallow layers, and channel attention is used in the deep layers. This allows the filter measurements to consider spatial information effectively. An adaptive pruning rate adjustment strategy is also used to eliminate manual subjectivity, achieving precision pruning of each convolutional layer. The experimental results on various datasets and networks demonstrate that the DAAR method achieves improved model performance after pruning. For example, in the CIFAR10 dataset, the precision increases from 93.5% to 93.75% after removing the floating point operations (FLOPs) of 84.1%, outperforming the state-of-the-art pruning methods.},
  archive      = {J_APIN},
  author       = {Lian, Suyun and Zhao, Yang and Pei, Jihong},
  doi          = {10.1007/s10489-025-06332-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {DAAR: Dual attention cooperative adaptive pruning rate by data-driven for filter pruning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-END: Robust watermarking technology based on deep
coupling of feature extractors. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06333-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based watermarking methods have been developed to address the shortcomings of traditional watermarking algorithms. Some methods adopt an end-to-end framework to train the watermarking model, enabling excellent watermark embedding and extraction. However, the visual quality and robustness of these approaches remain insufficient, especially ignoring the adequacy of image feature extraction and the correlation between network modules. We propose a feature extractor and decoder deep coupled watermark network, which can help generate high-robust watermarked images. Specifically, a down-sampling feature extractor is employed to supplement image features post-decoder, the extracted features are synchronously provided to the encoder for watermark embedding. Additionally, skip-connection is introduced to share each layer feature information of the decoder with the encoder, thereby improving the correlation between network modules. Comprehensive experimental results show that the proposed scheme can achieve high robustness against screen-shooting and paper printing processes while maintaining the visual quality of the watermarked image.},
  archive      = {J_APIN},
  author       = {Li, Jun and Fang, Yixiang and Zhao, Yi and Xu, Kangkang and Wang, Junxiang},
  doi          = {10.1007/s10489-025-06333-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {ED-END: Robust watermarking technology based on deep coupling of feature extractors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control of traffic network signals based on deep
deterministic policy gradients. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06208-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The centralized control of traffic signals is a challenging problem due to the high randomness and complexity of traffic flow on urban road networks and the interaction between intersections. Centralized control leads to high spatial dimensionality of joint actions for traffic road network signal control. However, the decisive action output can solve the problem of “dimensional explosion” caused by joint actions. In this paper, we propose a deep deterministic policy gradient-based algorithm for centralized control of urban traffic road network signals. We simplify the traffic signal control to a four-phase green signal ratio, and the deep deterministic policy gradient-based algorithm deterministically outputs the control signal for each intersection based on the information of the whole traffic network, thus avoiding the problem of “dimensional explosion”. In particular, a new normalization function is proposed to generate the green rate of traffic signals and constrain it to a range of maximum and minimum sustained green time by linear transformation, which makes the generated traffic signals more realistic. Our proposed algorithm is shown to be optimal and robust compared to Deep Q-Network(DQN) based and fixed time control for 7-hour SUMO simulation of a single-peak traffic network with three intersections.},
  archive      = {J_APIN},
  author       = {Hu, Huifeng and Lin, Shu and Wang, Ping and Xu, Jungang},
  doi          = {10.1007/s10489-024-06208-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Control of traffic network signals based on deep deterministic policy gradients},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Loop closure detection based on image feature matching and
motion trajectory similarity for mobile robot. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-05874-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual simultaneous localization and mapping (SLAM), loop closure detection plays an irreplaceable role in eliminating cumulative errors, optimizing robot poses, and ensuring map consistency. Most loop closure detection algorithms adopt single feature or feature fusion to detect loop closures, which makes it difficult to ensure accuracy in environments with changing lighting or high-similarity scenes. In this work, image features and motion trajectories are combined to improve the effectiveness of loop closure detection via a staged detection method. First, histogram equalization is used to reduce the algorithm’s sensitivity to lighting. Then, LBP features are used to divide keyframes into multiple sequences, and the sequence where the loop closure candidate frame is located is determined according to the image feature matching results. Then, the most matched keyframe is searched in the sequence as a candidate loop closure. Finally, the true loop closure is confirmed by comparing the motion trajectory similarity to improve the algorithm’s adaptability to high-similarity scenes. The experimental results show that in different application scenarios, the proposed method can achieve good results in terms of precision, recall, area under the curve (AUC), and recall when the precision is 100%.},
  archive      = {J_APIN},
  author       = {Hao, Weilong and Wang, Peng and Ni, Cui and Huangfu, Wenjun and Liu, Zhu and Qi, Kaiyuan},
  doi          = {10.1007/s10489-024-05874-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Loop closure detection based on image feature matching and motion trajectory similarity for mobile robot},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous multi-modal graph network for arterial travel
time prediction. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05895-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel time prediction has important influence on the overall control of urban Intelligent Transportation Systems (ITS). Urban arterial networks are typically composed of links and intersections, where each link or intersection can be regarded as a spatial node within the network. However, existing researches predominantly focus on modeling spatial nodes in the link modality to predict travel times in urban arterial networks, neglecting the potential correlations among heterogeneous modal nodes. To overcome these limitations, we propose a Heterogeneous Multi-Modal Graph Neural Network (HMGNN) specifically tailored for travel time prediction in arterial networks. Specifically, we innovatively construct spatial correlation graphs that capture the unique traffic characteristics of intersection modal nodes. Furthermore, we design a cross-modal graph generator that captures the latent spatiotemporal features between spatial nodes of distinct modalities, resulting in the generation of heterogeneous modal graphs. Finally, our proposed HMGNN model incorporates tailored network structures for graphs of varying complexities, enabling targeted mining of their inherent information to derive the final prediction results. Extensive experiments conducted using real-world traffic data from Zhangzhou, China, demonstrate that our HMGNN model achieves significant improvements in prediction accuracy.},
  archive      = {J_APIN},
  author       = {Fang, Jie and He, Hangyu and Xu, Mengyun and Wu, Xiongwei},
  doi          = {10.1007/s10489-024-05895-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Heterogeneous multi-modal graph network for arterial travel time prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel consensus reaching approach for large-scale
multi-attribute emergency group decision-making under social network
clustering based on graph attention mechanism. <em>APIN</em>,
<em>55</em>(6), 1–28. (<a
href="https://doi.org/10.1007/s10489-024-05992-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency decision-making problem is common in our daily life. To solve this kind of problem, a group of decision-makers (DMs) are usually invited to make a decision in a limited time. Since multiple attributes are usually considered, it’s called large-scale multi-attribute emergency group decision-making (LS-MA-EGDM). There are two issues in the general research of LS-MA-EGDM. First, clustering and consensus-reaching process (CRP) should consider the influence of DMs’ intrinsic features. Second, consensus adjustment within and among sub-clusters ought to be fast to prevent multi-round iteration. Accordingly, (1) we introduce graph attention mechanism to calculate the attention coefficients between DM pair’s intrinsic features. The multi-head graph attention coefficient based on social network analysis (SNA) is proposed, which is then combined with opinion similarity to construct a social network clustering method. (2) The Einstein product operator is introduced to propagate the attention coefficients and yield DMs’ weights, which is then incorporated in the subsequent adjustment allocation. (3) Identification rules are provided based on four consensus types in the CRP. The one-iteration personalized adjustment strategies corresponding to different consensus types are then proposed. (4) Evidential reasoning (ER) algorithm is finally utilized to aggregate the preferences of clusters after consensus is reaching. The proposed method is further applied to a chemical plant explosion in Texas to illustrate its effectiveness and validity in dealing with emergencies.},
  archive      = {J_APIN},
  author       = {Zhou, Mi and Zhang, Ying and Fan, Xin-Yu and Wu, Ting and Cheng, Ba-Yi and Wu, Jian},
  doi          = {10.1007/s10489-024-05992-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Appl. Intell.},
  title        = {A novel consensus reaching approach for large-scale multi-attribute emergency group decision-making under social network clustering based on graph attention mechanism},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for intelligent disease diagnosis
using audio signals: Beyond copd to a spectrum of diseases.
<em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-024-06028-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the widespread prevalence and significant patient base of COPD (Chronic Obstructive Pulmonary Disease), the development of simple and rapid diagnostic methods has emerged as a key research focus. Through pathological studies, the medical community has identified the potential of cough sounds for diagnosing COPD, sparking interest in leveraging deep learning to analyze various disease-related sounds, including those associated with COVID-19 and cardiac conditions, etc. Yet, research specifically targeting COPD remains scarce, primarily due to two challenges: traditional models trained on small medical datasets often fall short of expectations due to stringent data privacy and collection requirements in healthcare; and the scarcity of publicly accessible COPD datasets, particularly those that could obviate the need for medical equipment. Addressing these challenges, our paper introduces a novel dataset of smartphone-recorded cough sounds, termed the CC (COPD-Cough) dataset. It comprises 221 recordings from COPD patients and 632 from healthy individuals, marking the first dataset explicitly curated for COPD cough sound analysis. The dataset, endorsed by clinical professionals and collected independently of medical devices, promises to propel advancements in straightforward COPD diagnostics. Furthermore, we propose a self-supervised learning model enhanced by unique data augmentation techniques and an efficient sound feature extractor, demonstrating superior performance across three distinct disease datasets and achieving state-of-the-art results. Comprehensive ablation studies affirm our model’s efficacy, while sensitivity analyses optimize its applicability to various tasks. For further engagement, the framework’s source code and dataset are available at https://github.com/auto-chao/COPD_Diagnosis and https://zenodo.org/records/10209837 , respectively.},
  archive      = {J_APIN},
  author       = {Sun, Wenchao and Wu, Gang and Ming, Ming and Zhang, Jiameng and Shi, Chun and Qin, Linlin},
  doi          = {10.1007/s10489-024-06028-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised learning for intelligent disease diagnosis using audio signals: Beyond copd to a spectrum of diseases},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A BERT-based review helpfulness prediction model utilizing
consistency of ratings and texts. <em>APIN</em>, <em>55</em>(6), 1–14.
(<a href="https://doi.org/10.1007/s10489-024-06100-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting review helpfulness (RH) to ensure that consumers make effective purchasing decisions is a significant area of study. Many scholars have attempted to develop accurate review helpfulness prediction (RHP) methodologies. However, most previous studies have mainly focused on predictions using product review texts, and few studies have used product satisfaction as indicated by star ratings, particularly the consistency between review texts and star ratings. This study proposes a novel model called BHelP-CoRT (Bidirectional Encoder Representations from Transformers based RHP model utilizing consistency of ratings and texts) to predict RH. The proposed model consists of a review text encoder, star rating encoder, and text-rating interaction. The review text encoder was developed by applying the BERT model to extract contextual semantic features embedded in review texts. The star rating encoder was designed to embed star ratings into feature vectors. The text-rating interaction was constructed by applying an attention mechanism to extract the text-rating interaction and introduce consistency into the RHP tasks. This study conducted extensive experiments to demonstrate the effectiveness of the proposed model from multiple perspectives using real-world online reviews collected from Amazon. The experimental results show that the proposed model outperforms the state-of-the-art models, indicating that it can improve the RHP performance. Specifically, this effectiveness is reflected in the processing of reviews containing inconsistent information. This study supports the marketing efforts of the e-commerce industry by providing an RHP service to address consumer information overload.},
  archive      = {J_APIN},
  author       = {Li, Xinzhe and Li, Qinglong and Ryu, Dongyeop and Kim, Jaekyeong},
  doi          = {10.1007/s10489-024-06100-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {A BERT-based review helpfulness prediction model utilizing consistency of ratings and texts},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCSNet: A novel transformer-CNN fusion architecture for
enhanced segmentation and classification on high-resolution
semiconductor micro-scale defects. <em>APIN</em>, <em>55</em>(6), 1–18.
(<a href="https://doi.org/10.1007/s10489-024-06122-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of semiconductor integrated circuit manufacturing, accurately identifying the root causes of defects is critical for enhancing yield rates. Traditionally, this analytical process has been both time-intensive and challenged by inaccuracies, primarily due to the intricate and varied morphology of wafer defects. While convolutional neural networks (CNNs) with encoder-decoder architectures have made significant strides in the segmentation of defects, they inherently struggle to capture distant interactions and achieve high performance in classification tasks. Conversely, recent advancements in transformers have showcased their proficiency in learning global image dependencies. However, transformers often lack the specific graphical priors and the adaptability typically associated with CNNs. Addressing these limitations, we introduce SCSNet, an innovative architecture that merges the strengths of transformers and CNNs. This fusion network is designed to enhance both segmentation and classification of scanning electron microscopy (SEM) images of wafer defects. SCSNet incorporates a conventional encoder-decoder framework, supplemented by shape flow branches and multi-cross-attention (MCF) modules within a skip connection architecture. Rigorous experimentation on a dataset of 4425 high-resolution wafer defects, sourced from our operational wafer fabrication facility, demonstrates SCSNet’s superior performance. Notably, SCSNet surpasses existing advanced CNNs, transformers, and their hybrid counterparts, achieving a classification accuracy of 97.62% and a segmentation Intersection over Union (IoU) of 84.09%. Currently implemented on our local server for engineering use, SCSNet represents a major advancement in semiconductor manufacturing, offering a more precise and efficient tool for wafer defect analysis.},
  archive      = {J_APIN},
  author       = {Luo, Yuening and Mei, Zhouzhouzhou and Qiao, Yibo and Chen, Yining},
  doi          = {10.1007/s10489-024-06122-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {SCSNet: A novel transformer-CNN fusion architecture for enhanced segmentation and classification on high-resolution semiconductor micro-scale defects},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreqFaceNet: An enhanced transformer architecture with
dual-order frequency attention for deepfake detection. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-024-06168-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of AI-based image synthesis tools and techniques, Deepfakes have become a serious problem as they pose a massive threat to one’s information security and personal privacy. Several architectures have been proposed to achieve robust Deep Fake detection. However, these methods suffer a drastic drop in performance if the images are visually degraded or have low resolution. To resolve these two issues, a novel FreqFaceNet model has been proposed that employs two novel attentions namely, Wavelet Attention and Fourier Attention, for extracting important frequency-based features from low-resolution images. The extraction of frequency-based features ensures minimal interference of noise due to image compression or low resolution. The proposed model excels on two public benchmark datasets—the DFDC and CelebDF. On the DFDC dataset, FreqFaceNet achieves 98.041% accuracy, an AUC value of 99.748, and a Mathews Correlation Coefficient (MCC) value of 93.857, while on the CelebDF dataset, it obtains an accuracy of 98.325%, an AUC value of 99.81, and an MCC value of 92.819. Qualitative analysis of the proposed model indicates strong classification capabilities. An ablation study has also been conducted to verify the complementary contributions of both Wavelet and Fourier Attention mechanisms.},
  archive      = {J_APIN},
  author       = {Gupta, Varun and Srivastava, Vaibhav and Yadav, Ankit and Vishwakarma, Dinesh Kumar and Kumar, Narendra},
  doi          = {10.1007/s10489-024-06168-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {FreqFaceNet: An enhanced transformer architecture with dual-order frequency attention for deepfake detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VAEneu: A new avenue for VAE application on probabilistic
forecasting. <em>APIN</em>, <em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-024-06203-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces VAEneu, a novel autoregressive method for multistep ahead univariate probabilistic time series forecasting, designed to address the challenges of generating sharp and well-calibrated probabilistic forecasts without assuming a specific parametric form for the predictive distribution. VAEneu leverages the Conditional VAE framework and optimizes the likelihood of the predictive distribution using the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function. This approach enables the model to learn flexible, sharp, and well-calibrated predictive distributions without the need for a tractable likelihood function. In a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets, demonstrating superior performance in both forecasting accuracy and uncertainty quantification. VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting.},
  archive      = {J_APIN},
  author       = {Koochali, Alireza and Tahaei, Ensiye and Dengel, Andreas and Ahmed, Sheraz},
  doi          = {10.1007/s10489-024-06203-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {VAEneu: A new avenue for VAE application on probabilistic forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: A diverse/converged individual competition
algorithm for computationally expensive many-objective optimization.
<em>APIN</em>, <em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06225-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Lin, Jie and Zhang, Sheng Xin and Zheng, Shao Yong},
  doi          = {10.1007/s10489-024-06225-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: A diverse/converged individual competition algorithm for computationally expensive many-objective optimization},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IPAttack: Imperceptible adversarial patch to attack object
detectors. <em>APIN</em>, <em>55</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s10489-025-06246-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of deep learning, general object detectors have become increasingly popular in our daily lives. Extensive research, however, has shown that existing detectors are vulnerable to patch-based adversarial attacks, which fool such detectors by crafting adversarial patches. Although existing methods have made significant progress in terms of attack success rate, they still suffer from a highly perceptible problem, making it easy for humans to distinguish these evil examples. To address this issue, in this paper, we propose a novel spatial transform-based end-to-end patch attack method, called IPAttack, to synthesize imperceptible adversarial patches. Our approach estimates a flow field $$\varvec{f}$$ to formulate adversarial examples rather than introduce small $$L_p$$ -norm constrained external perturbations. Besides, to improve the imperceptibility and maintain a high attack performance, we propose the Object Detector Class Activation Map (OD-CAM) for objectors to extract the most interesting region, which will be applied to spatial transform to generate the final adversarial examples. Extensive experiments demonstrate that the proposed IPAttack can generate patch-wised adversarial examples with high imperceptibility while achieving the best attack performance compared to existing methods.},
  archive      = {J_APIN},
  author       = {Wen, Yongming and Si, Peiyuan and Zhou, Wei and Zhao, Zongheng and Yi, Chao and Liu, Renyang},
  doi          = {10.1007/s10489-025-06246-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {IPAttack: Imperceptible adversarial patch to attack object detectors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FinCaKG-onto: The financial expertise depiction via
causality knowledge graph and domain ontology. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06247-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality stands as an essential relation for elucidating the reasoning behind given contents. However, current causality knowledge graphs fall short in effectively illustrating the inner logic in a specific domain, i.e. finance. To generate such a functional knowledge graph, we propose the multi-faceted approach encompassing causality detection module, entity linking module, and causality alignment module to automatically construct FinCaKG-Onto with the guidance of expert financial ontology - FIBO. In this paper, we outline the resources and methodology employed for FinCaKG-Onto construction, present the schema of FinCaKG-Onto, and share the final knowledge graph FinCaKG-Onto. Through various user scenarios, we demonstrate that FinCaKG-Onto not only captures nuanced domain expertise but also explicitly unveils the causal logic for any anchor terms. To facilitate your convenience of future use, a check table is conducted as well to showcase the quality of FinCaKG-Onto. The related resources are available in the webpage&lt; https://www.ai.iee.e.titech.ac.jp/FinCaKG-Onto/ &gt;.},
  archive      = {J_APIN},
  author       = {Xu, Ziwei and Ichise, Ryutaro},
  doi          = {10.1007/s10489-025-06247-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {FinCaKG-onto: The financial expertise depiction via causality knowledge graph and domain ontology},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-guided classification and regression surrogates
co-assisted multi-objective soft subspace clustering algorithm.
<em>APIN</em>, <em>55</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s10489-025-06266-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of multi-objective soft subspace clustering algorithms (MSSCAs) can be low when applied to large-scale datasets. This inefficiency arises because the multi-objective evolutionary algorithms (MOEAs) utilized in MSSCAs often require a large number of soft subspace clustering objective function evaluations due to their population-based nature. Moreover, relying solely on negative Shannon entropy to constrain feature weights is inadequate for soft subspace clustering algorithms. To address these issues, a knowledge-guided classification and regression surrogates co-assisted multi-objective soft subspace clustering (KCRS-MOSSC) algorithm is presented. First, an inter-cluster feature weight dissimilarity function is designed to further constrain the feature weights. Furthermore, a novel surrogate-based optimization framework called the knowledge-guided classification and regression surrogates co-assisted multi-objective evolutionary framework (KCRS-MOEF) is proposed to efficiently optimize the proposed inter-cluster feature weight dissimilarity function, intra-cluster compactness function, inter-cluster separation function, and negative Shannon entropy function. In KCRS-MOEF, a classification decision tree is utilized as the classification surrogate model to help generate a set of promising offspring, while a radial basis function (RBF) model is employed as the regression surrogate model to assist in the infill criterion by predicting the objective function values of the offspring. Furthermore, to fully leverage the knowledge of the evolutionary process, an infill criterion guided by dynamic process knowledge of elite individuals is designed to enhance the convergence and diversity of the population. Finally, a clustering ensemble strategy based on knee point guidance is proposed to generate a final solution from a set of non-dominated individuals. KCRS-MOEF outperforms state-of-the-art counterparts in terms of convergence, diversity, and time efficiency, as demonstrated in four experiments conducted on the DTLZ benchmark. Furthermore, experiments on various datasets show that the clustering performance and time efficiency of KCRS-MOSSC exceed those of comparison algorithms.},
  archive      = {J_APIN},
  author       = {Zhao, Feng and Li, Lu and Liu, Hanqiang},
  doi          = {10.1007/s10489-025-06266-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {Knowledge-guided classification and regression surrogates co-assisted multi-objective soft subspace clustering algorithm},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reusability of bayesian networks case studies: A survey.
<em>APIN</em>, <em>55</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10489-025-06289-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Networks (BNs) are probabilistic graphical models used to represent variables and their conditional dependencies, making them highly valuable in a wide range of fields, such as radiology, agriculture, neuroscience, construction management, medicine, and engineering systems, among many others. Despite their widespread application, the reusability of BNs presented in papers that describe their application to real-world tasks has not been thoroughly examined. In this paper, we perform a structured survey on the reusability of BNs using the PRISMA methodology, analyzing 147 papers from various domains. Our results indicate that only 18% of the papers provide sufficient information to enable the reusability of the described BNs. This creates significant challenges for other researchers attempting to reuse these models, especially since many BNs are developed using expert knowledge elicitation. Additionally, direct requests to authors for reusable BNs yielded positive results in only 12% of cases. These findings underscore the importance of improving reusability and reproducibility practices within the BN research community, a need that is equally relevant across the broader field of Artificial Intelligence.},
  archive      = {J_APIN},
  author       = {Babakov, Nikolay and Sivaprasad, Adarsa and Reiter, Ehud and Bugarín-Diz, Alberto},
  doi          = {10.1007/s10489-025-06289-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {Reusability of bayesian networks case studies: A survey},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPGCL: Neighbor enhancement and embedding perturbation with
graph contrastive learning for recommendation. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06301-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have significantly advanced recommendation systems by modeling user-item interactions through bipartite graphs. However, real-world user-item interaction data are often sparse and noisy. Traditional bipartite graph modeling fails to capture higher-order relationships between users and items, limiting the ability of GNNs to learn high-quality node embeddings. While existing graph contrastive learning methods address data sparsity by partitioning nodes into positive and negative pairs, they also neglect these higher-order relationships, thus limiting the effectiveness of contrastive learning in recommendation systems. Furthermore, due to the inherent limitations of graph convolution, noise can propagate and amplify with increasing layers in deep graph convolutional networks. To address these challenges, Neighbor Enhancement and Embedding Perturbation for Graph Contrastive Learning (NPGCL) is proposed, which introduces two key modules - Relational Neighbor Enhancement Module and Collaborative Neighbor Enhancement Module - to capture higher-order relationships between homogeneous nodes and calculate interaction importance for noise suppression. Moreover, NPGCL employs an Embedding Perturbation Strategy and applies inter-layer contrastive learning to mitigate the noise impact caused by multi-layer graph convolutions. Experimental results demonstrate that NPGCL significantly improves performance across four publicly available datasets, with a notable enhancement in robustness, especially in noisy environments. Specifically, NPGCL achieves performance improvements of 1.77%-3.34% and 3.87%-9.07% on the Gowalla and Amazon-books datasets, respectively. In noisy datasets, NPGCL improves Recall@20 by 4.98% and 10.92%, respectively.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Wang, Haodong and Yao, Junfeng and Qian, Quan and Song, Jun},
  doi          = {10.1007/s10489-025-06301-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {NPGCL: Neighbor enhancement and embedding perturbation with graph contrastive learning for recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite gaussian processes flows for learning
discontinuous multimodal policies. <em>APIN</em>, <em>55</em>(6), 1–20.
(<a href="https://doi.org/10.1007/s10489-025-06302-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning control policies for real-world robotic tasks often involve challenges such as multimodality, local discontinuities, and the need for computational efficiency. These challenges arise from the complexity of robotic environments, where multiple solutions may coexist. To address these issues, we propose Composite Gaussian Processes Flows (CGP-Flows), a novel semi-parametric model for robotic policy. CGP-Flows integrate Overlapping Mixtures of Gaussian Processes (OMGPs) with the Continuous Normalizing Flows (CNFs), enabling them to model complex policies addressing multimodality and local discontinuities. This hybrid approach retains the computational efficiency of OMGPs while incorporating the flexibility of CNFs. Experiments conducted in both simulated and real-world robotic tasks demonstrate that CGP-flows significantly improve performance in modeling control policies. In a simulation task, we confirmed that CGP-Flows had a higher success rate compared to the baseline method, and the success rate of GCP-Flow was significantly different from the success rate of other baselines in chi-square tests.},
  archive      = {J_APIN},
  author       = {Wang, Shu-yuan and Sasaki, Hikaru and Matsubara, Takamitsu},
  doi          = {10.1007/s10489-025-06302-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Composite gaussian processes flows for learning discontinuous multimodal policies},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of a variable-parameter noise-tolerant
ZNN for solving time-variant nonlinear equations and applications.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06304-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solvers considering time-varying parameters are more suitable for addressing a variety of time-varying problems, whereas traditional fixed-parameter neural networks are somewhat insufficient for efficiently and quickly solving these problems. Many existing zeroing neural networks ensure rapid convergence using the infinite-valued AFs. For solving time-varying nonlinear equations, this paper proposes a finitely-activated variable parameter noise tolerant zeroing neural network (VPNTZNN), applied to trajectory tracking of redundant robotic arms. The designed variable parameters are error-dependent, enabling adaptive adjustment to optimal values as errors fluctuate, thereby ensuring faster convergence of the proposed VPNTZNN. And the constructed variable parameters and activation functions (AFs) do not escalate infinitely over time. Affected by the above variable parameters, the proposed finitely-activated VPNTZNN achieves rapid finite-time convergence with strong noise suppression. Simulation results validate the effectiveness of our method in solving time-variant nonlinear equations and in trajectory tracking of redundant manipulators. Moreover, this approach employs a finite-valued activation function to design a variable-parameter neural network, thereby avoiding the difficulties of practical implementation.},
  archive      = {J_APIN},
  author       = {Zhang, Yu and Wang, Liming and Zhong, Guomin},
  doi          = {10.1007/s10489-025-06304-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Design and analysis of a variable-parameter noise-tolerant ZNN for solving time-variant nonlinear equations and applications},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-GCDT: Advanced reinforcement learning with GAN-enhanced
data for continuous excavation system. <em>APIN</em>, <em>55</em>(6),
1–22. (<a href="https://doi.org/10.1007/s10489-025-06308-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automation of excavator operations entails the development and implementation of systems that allow excavators to execute tasks autonomously, thereby significantly reducing the need for human intervention. By integrating advanced sensors and artificial intelligence algorithms, these systems aim to increase operational efficiency, safety, and precision in construction and mining. However, previously developed methods have two weaknesses. First, existing automated excavator systems struggle with adapting to diverse and complex environmental conditions and with precision in control mechanisms. Second, operating an excavator involves multiple, repeated decisions that need to be modeled, planned, and executed in real time. However, there is a significant lack of comprehensive datasets that reflect real-world excavation operations to support this process. In this paper, we present an innovative system named E-GCDT. This system integrates the DoppelGANger module, which generates action time series by emulating human-mined trajectories through a generative adversarial mechanism and replays them in a simulation environment, ultimately expanding the dataset to 155 continuous mining trajectories. Furthermore, E-GCDT integrates terrain features into the decision-making process with the contrastive language-image pre-training model (CLIP), in which the decision transformer optimizes trajectory planning for efficient and accurate continuous excavation tasks. E-GCDT uniquely integrates advanced data augmentation and terrain awareness, developing an advanced Markov decision framework (DT) for continuous excavation tasks. The experimental results of a bulldozer verify that the efficiency of E-GCDT surpasses human efficiency. This system sets a new standard for continuous autonomous mining, and this study provides a new perspective on the application of reinforcement learning in industrial environments.},
  archive      = {J_APIN},
  author       = {Zhao, Qianyou and Gao, Le and Wu, Duidi and Lei, Yihao and Wang, Lingyu and Qi, Jin and Hu, Jie},
  doi          = {10.1007/s10489-025-06308-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {E-GCDT: Advanced reinforcement learning with GAN-enhanced data for continuous excavation system},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WLKA-RVS: A retinal vessel segmentation method using
weighted large kernel attention. <em>APIN</em>, <em>55</em>(6), 1–12.
(<a href="https://doi.org/10.1007/s10489-025-06309-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is an important task in medical image analysis and has a wide range of applications in the diagnosis and treatment of retinal diseases. However, existing segmentation methods still have some shortcomings in accurately segmenting thin vessels. Based on this observation, we propose a Retinal Vessel Segmentation method based on Weighted Large Kernel Attention (WLKA-RVS), which aims to improve the accuracy of retinal vessel segmentation to better assist physicians in clinical diagnosis and treatment. Our method consists of an encoder and a decoder. In the encoder, a convolution stem first reduces the dimension of the input image. Then, feature extraction is performed by four stages of Swin Transformer modules, each stage with a downsampling layer. In the decoder, there are four different stages of Weighted Large Kernel Attention Block (WLKAB) corresponding to the Swin Transformer modules in the encoder. Then WLKA-RVS applies the Patch Expanding module to achieve upsampling. Finally, a linear layer outputs the final results. We have performed extensive experiments comparing several recent advanced models on three public datasets. WLKA-RVS led by 0.32%, 1.24%, and 0.71% in the mAcc metric, respectively. At the same time, the inference speed of WLKA-RVS met the real-time requirements for medical diagnosis. A series of experiments demonstrated the efficiency, robustness, and applicability of WLKA-RVS.},
  archive      = {J_APIN},
  author       = {Li, Jiayao and Zeng, Min and Wu, Chenxi and Cheng, Qianxiang and Guo, Qiuyan and Li, Song},
  doi          = {10.1007/s10489-025-06309-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {WLKA-RVS: A retinal vessel segmentation method using weighted large kernel attention},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TADST: Reconstruction with spatio-temporal feature fusion
for deviation-based time series anomaly detection. <em>APIN</em>,
<em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06310-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is crucial in time series analysis for identifying abnormal events. To address the limitations of traditional methods in integrating spatiotemporal correlations and modeling normal patterns, we propose a Time Series Anomaly Detection Model Based on Spatio-Temporal Feature Fusion (TADST). First, the Spatio-Temporal Feature Fusion Network (STF) combines temporal convolutional networks and graph attention influence networks to capture temporal dynamic dependencies and attribute correlations respectively, facilitating joint spatiotemporal feature modeling. Then, the Time Series Reconstruction Network (TSR) employs a multi-layer encoder-decoder architecture to learn the normal sample distribution and amplify discrepancies between reconstructed and anomalous data. Finally, the Anomaly Detection Mechanism (ADM) identifies anomalies by fitting the tail distribution of reconstruction deviations. When the anomaly score exceeds a predefined threshold, the mechanism updates the parameters of the Generalized Pareto Distribution, keeping the detection criteria adaptive. Experiments demonstrate that the proposed TADST achieves state-of-the-art results on five publicly available datasets.},
  archive      = {J_APIN},
  author       = {Yang, Bin and Ma, Tinghuai and Rong, Huan and Huang, Xuejian and Wang, Yubo and Zhao, Bowen and Wang, Chaoming},
  doi          = {10.1007/s10489-025-06310-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {TADST: Reconstruction with spatio-temporal feature fusion for deviation-based time series anomaly detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph regularized independent latent low-rank representation
for image clustering. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06312-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation (LRR) has been proved to be effective in exploring low-dimensional subspace structure embedded in the observations. However, existing LRR algorithms often pay no attention to data redundancy, easily leading to performance decay. In addition, the LRR characterizes data global inter-connections, from which some latent similarity features should be further learned and exploited to improve the performance of clustering. Therefore, a novel method termed Graph Regularized Independent Latent Low-Rank Representation (GRI-LLRR) is presented to address the above issues. As we know, Hilbert–Schmidt Independence Criterion (HSIC) measures the independence between two distributions. In the proposed method, it is introduced and developed to another novel graph regularization independent term to remove the uncorrelation between vectors and to preserve the data local geometry. With other constraints, including the sparse, nonnegative and symmetric, the LRR is obtained from the observations. Then, the proposed method further learns the cosine features as latent representation of the LRR for final clustering. Massive experiments have been conducted on eight benchmark data sets. Experimental results show that the proposed GRI-LLRR outperforms some state-of-the-art (SOTA) approaches with improvements of 2.24%, 2.73%, and 2.65% on average for CCA, NMI, and Purity, respectively.},
  archive      = {J_APIN},
  author       = {Li, Bo and Pan, Lin-Feng},
  doi          = {10.1007/s10489-025-06312-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Graph regularized independent latent low-rank representation for image clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A code completion approach combining pointer network and
transformer-XL network. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06315-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code completion is a crucial aspect of contemporary integrated development environments (IDEs), as it not only streamlines the software development process but also bolsters the quality of software products. By leveraging large-scale codes to learn the probability distribution among code token units, deep learning methods have demonstrated significant improvements in the accuracy of token unit recommendations. However, the efficacy of code completion employing deep learning is often compromised by information loss. To mitigate this issue, we introduce a novel code language model that incorporates both the pointer network and the Transformer-XL architecture to surpass the constraints of current approaches in code completion. Our proposed model accepts as input the original code snippet and its corresponding abstract syntax tree (AST), utilizing the Transformer-XL model as the foundational architecture for capturing long-term dependencies. Additionally, we incorporate a pointer network as an adjunct component to forecast Out-of-Vocabulary (OoV) words. Our approach has been rigorously evaluated on the authentic PY150 and JS150 datasets. The comparative experimental results demonstrate the effectiveness of our model in improving the accuracy of the code completion task at the token unit level.},
  archive      = {J_APIN},
  author       = {Zhang, Xiangping and Liu, Jianxun and Long, Teng and Hu, Haize},
  doi          = {10.1007/s10489-025-06315-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A code completion approach combining pointer network and transformer-XL network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GenKP: Generative knowledge prompts for enhancing large
language models. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06318-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have demonstrated extensive capabilities across various natural language processing (NLP) tasks. Knowledge graphs (KGs) harbor vast amounts of facts, furnishing external knowledge for language models. The structured knowledge extracted from KGs must undergo conversion into sentences to align with the input format required by LLMs. Previous research has commonly utilized methods such as triple conversion and template-based conversion. However, sentences converted using existing methods frequently encounter issues such as semantic incoherence, ambiguity, and unnaturalness, which distort the original intent, and deviate the sentences from the facts. Meanwhile, despite the improvement that knowledge-enhanced pre-training and prompt-tuning methods have achieved in small-scale models, they are difficult to implement for LLMs in the absence of computational resources. The advanced comprehension of LLMs facilitates in-context learning (ICL), thereby enhancing their performance without the need for additional training. In this paper, we propose a knowledge prompts generation method, GenKP, which injects knowledge into LLMs by ICL. Compared to inserting triple-conversion or templated-conversion knowledge without selection, GenKP entails generating knowledge samples using LLMs in conjunction with KGs and makes a trade-off of knowledge samples through weighted verification and BM25 ranking, reducing knowledge noise. Experimental results illustrate that incorporating knowledge prompts enhances the performance of LLMs. Furthermore, LLMs augmented with GenKP exhibit superior improvements compared to the methods utilizing triple and template-based knowledge injection.},
  archive      = {J_APIN},
  author       = {Li, Xinbai and Peng, Shaowen and Yada, Shuntaro and Wakamiya, Shoko and Aramaki, Eiji},
  doi          = {10.1007/s10489-025-06318-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {GenKP: Generative knowledge prompts for enhancing large language models},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent forecasting algorithm of power industry
expansion based on time series and entropy weight method. <em>APIN</em>,
<em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06321-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately predict the electricity consumption trend of individual users and even the entire industry, this paper studies an intelligent prediction algorithm for the power industry based on time series and entropy weight method. Using ARIMA model and X12 model to establish a monthly electricity consumption prediction model, the study obtains the monthly electricity consumption prediction value for the expansion of the power industry. The entropy weight method is employed to calculate the weights of two power industry expansion month electricity consumption forecasting models, thereby achieving intelligent forecasting. The experimental results demonstrate that the maximum error of the proposed method is only 1.78%, and the average time complexity and average space complexity of the proposed algorithm are both below the set threshold.},
  archive      = {J_APIN},
  author       = {Wu, Guoyao and Lan, Zhiqiang and Wu, Xiaofang and Huang, Xiaoying and Mao, Linling},
  doi          = {10.1007/s10489-025-06321-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent forecasting algorithm of power industry expansion based on time series and entropy weight method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel data-driven model for explainable hog price
forecasting. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06323-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting hog prices is an important and challenging task for pig producers and managers as it plays a crucial role in decision-making processes. Given the significant impact of raw pork supply, public concern, animal diseases, and international markets on hog prices, this study proposes a comprehensive and explainable hybrid model for hog price forecasting by combining principal component analysis (PCA), variational mode decomposition (VMD), weighted average algorithm (WAA) algorithm, and temporal fusion transformers (TFT). To improve the quality of input variables, search engine data reflecting public concern about live pig prices are dimensionally reduced using PCA. This reduction process helps in eliminating unnecessary information and enhancing the input’s relevance. Additionally, VMD is applied to decompose raw pig futures prices, enabling the capture of their underlying trends over time. Subsequently, all the input variables, including the processed search engine data and the decomposed pig futures prices, are fed into the WAA-TFT model. WAA algorithm optimizes the parameters of the TFT model, resulting in accurate predicted values. The interpretable nature of the TFT model provides valuable decision-making insights for practitioners in the agricultural products market. The experimental results show that the proposed model achieves a mean absolute percentage error (MAPE) of only 1.76% on the Chinese hog price prediction dataset, demonstrating the excellent predictive performance of the proposed model.},
  archive      = {J_APIN},
  author       = {Wu, Binrong and Zeng, Huanze and Hu, Huanling and Wang, Lin},
  doi          = {10.1007/s10489-025-06323-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A novel data-driven model for explainable hog price forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning discriminative features for multi-hop knowledge
graph reasoning. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06327-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL)-based multi-hop knowledge graph reasoning has achieved remarkable success in real-world applications and can effectively handle knowledge graph completion tasks. This approach involves a policy-based agent navigating the graph environment to extend reasoning paths and identify the target entity. However, most existing multi-hop reasoning models are typically constrained to stepwise inference, which inherently disrupts the global information of multi-hop paths. To overcome this limitation, we introduce discriminative features between valid and invalid paths as global information. Here, we propose a multi-hop path encoder specifically designed to extract these discriminative features. Firstly, a multi-hop path encoding module is employed to derive each path’s hidden features, using cross-attention mechanisms to strengthen the interaction between triple context and path features. Secondly, a discriminative feature extraction module is used to capture the differences between valid and invalid paths. Thirdly, an attention-enhanced gated fusion mechanism is implemented to integrate these discriminative features into the multi-hop inference decoder. We further evaluate our method on five standard datasets. Our method outperforms the baseline models, demonstrating the effectiveness of discriminative features in improving prediction performance, learning speed, and path interpretability.},
  archive      = {J_APIN},
  author       = {Liu, Hao and Li, Dong and Zeng, Bing and Xu, Yang},
  doi          = {10.1007/s10489-025-06327-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Learning discriminative features for multi-hop knowledge graph reasoning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal clustering enhanced multi-graph
convolutional network for traffic flow prediction. <em>APIN</em>,
<em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06329-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamics and uncertainty are the fundamental reasons for the difficulty in accurately predicting traffic flow. In recent years, graph convolutional networks have been widely used in traffic flow prediction because of their excellent dynamic feature mapping ability. However, the existing models usually overlook the correlations among the nodes and the complex impact of external factors on traffic flow, which make it challenging to explore the complex spatial-temporal features. To overcome these shortcomings, we propose a novel Spatial-temporal Clustering enhanced Multi-Graph Convolutional Network (SCM-GCN) for traffic flow prediction. First, a Spatial-Temporal Clustering (STS) module based on the improved adjacency matrix DBSCAN clustering algorithm is constructed, this module divides traffic nodes into multiple highly correlated clusters, each of which consists of multi-graph features and time-varying features. Then, a Multi-Graph Spatial Feature Extraction (MGSFE) module that integrates the graph convolution operation and attention mechanism is designed to extract dynamic spatial features of multi-graph and time-varying features. Next, the Time-Varying Feature Extraction (TVFE) module based on the dilated convolution and gated attention mechanism is constructed. It integrates the output of the MGSFE module to extract dynamic temporal features of time-varying features and output the predicted values. Finally, the comparison and ablation experiments on four datasets show that the proposed model performs better than state-of-the-art models. The key source code and data are available at https://github.com/Bounger2/SCMGCN .},
  archive      = {J_APIN},
  author       = {Bao, Yinxin and Shen, Qinqin and Cao, Yang and Shi, Quan},
  doi          = {10.1007/s10489-025-06329-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Spatial-temporal clustering enhanced multi-graph convolutional network for traffic flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fusion of multi-source heterogeneous data using MOE
mechanism for stock prediction. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06330-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock prices are influenced by numerous factors, including social media, news, and financial reports, serving as indicators of financial market dynamics. However, harnessing diverse information from different sources and structures to predict price trends remains challenging. In this paper, we propose a dual-stage deep learning model based on the Mixture-of-Expert (MoE) mechanism. In stage one, three distinct expert networks encode information about price movements, financial news, and investor sentiments through multi-source interaction attention. In stage two, a gated network dynamically fuses outputs, capturing temporal relationships in windowed data. Experimental results on the Chinese stock market demonstrate our model outperforms existing ones in forecasting tasks.},
  archive      = {J_APIN},
  author       = {Dong, Yuxin and Wu, Zirui and Hao, Yongtao},
  doi          = {10.1007/s10489-025-06330-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic fusion of multi-source heterogeneous data using MOE mechanism for stock prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection based on multi-perspective dynamic
neighbourhood entropy measures in a dynamic neighbourhood rough set.
<em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06336-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbourhood rough set (NRS)-based feature selection has been extensively applied in data mining. However, the effectiveness of the NRS model is limited by its reliance on the grid search method to determine the optimal neighbourhood parameter, insensitivity to data distribution under different features, and consideration of uncertainty measures from only one single perspective. To address the aforementioned issues, this study first defines a spatial function that can obtain information about the distribution of samples in space according to the change in the feature subset. On this basis, three perspectives of dynamic neighbourhoods are proposed: pessimistic, neutral, and optimistic. Next, the concept of the dynamic neighbourhood rough set (DNRS) model is developed. The most significant feature of this model is its adaptive ability to dynamically update the neighbourhood radius of samples on the basis of the information of their distribution in space, without the necessity of setting neighbourhood parameters artificially. Then, algebraic and information-theoretic views are introduced to propose multi-perspective dynamic neighbourhood entropy measures, which effectively measure the uncertainty of the data. In addition, a nonmonotonic feature selection algorithm based on mutual information is designed to overcome the limitations of feature selection algorithms that rely on monotonic evaluation functions. This algorithm utilizes multi-perspective dynamic neighbourhood entropy measures from a neutral perspective. Finally, to mitigate the high time complexity in feature selection for high-dimensional datasets, the Fisher score is introduced in an initial dimensionality reduction method. The results of the experiment show that the algorithm effectively eliminates redundant features and improves accuracy.},
  archive      = {J_APIN},
  author       = {Xu, Jiucheng and Ma, Miaoxian and Zhang, Shan and Niu, Wulin},
  doi          = {10.1007/s10489-025-06336-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection based on multi-perspective dynamic neighbourhood entropy measures in a dynamic neighbourhood rough set},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GANet: Geometry-aware network for RGB-d semantic
segmentation. <em>APIN</em>, <em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06337-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of RGB-D semantic segmentation has attracted considerable interest in recent times. The challenge is to develop an effective method for combining RGB images, which capture colour variations, with depth images, which provide robust information about object geometry regardless of lighting conditions. Treating both image types equally through the same convolution operator fails to take into account their inherent differences. Thus, in this paper, we propose a novel approach that combines a geometry-aware convolution (GAConv) module and a multiscale fusion module (MFM) with the aim of enhancing the performance of RGB-D image segmentation. The GAConv module effectively captures fine-grained geometric details from depth images, while the MFM module enables efficient integration of multi-scale features, allowing the network to utilise both spatial and semantic information. Extensive experimentation was conducted on the NYUv2 and SUN RGB-D datasets, wherein our model demonstrated consistent superiority over existing state-of-the-art methods in terms of pixel accuracy and mean intersection over union (mIoU).},
  archive      = {J_APIN},
  author       = {Tian, Chunqi and Xu, Weirong and Bai, Lizhi and Yang, Jun and Xu, Yanjun},
  doi          = {10.1007/s10489-025-06337-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {GANet: Geometry-aware network for RGB-D semantic segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature optimization based on multi-order fusion and
adaptive recursive elimination for motion classification in doppler
radar. <em>APIN</em>, <em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06342-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar-based human motion recognition (HMR) technology has gained substantial importance across diverse domains such as security surveillance, post-disaster search and rescue operations, and the development of smart home environments. The intricate nature of human movements generates radar echo signals with pronounced non-stationary attributes, which encapsulate a wealth of target feature data. However, striking a balance between the precision of motion recognition and the requirement for real-time processing, especially in the context of extracting meaningful features from radar signals, remains a formidable challenge. This research paper introduces a novel approach to tackle this challenge. Firstly,we apply the multi-order fractional Fourier transform (m-FRFT) to radar echo signals, facilitating the extraction of micro-Doppler (m-D) frequency information. Secondly, we have developed an optimized feature selection model named MPG, which stands for m-D parameter screening based on genetic algorithm (GA) and adaptive weight particle swarm optimization (AWPSO). Thirdly, we apply the MPG model to the recursive feature elimination (RFE) algorithm to refine the representation of m-D frequency information, allowing for adaptive parameter adjustment and effective feature dimensionality reduction. The proposed method has been tested using human motion echo data collected from a Doppler radar prototype. The experimental outcomes demonstrate that our approach outperforms traditional feature extraction methods in terms of reducing feature dimensionality, computational efficiency, and classification accuracy.},
  archive      = {J_APIN},
  author       = {Sun, Tong and Ding, Yipeng and Chen, Yuxin and Ping, Lv},
  doi          = {10.1007/s10489-025-06342-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Feature optimization based on multi-order fusion and adaptive recursive elimination for motion classification in doppler radar},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced decision framework for two-player zero-sum markov
games with diverse opponent policies. <em>APIN</em>, <em>55</em>(6),
1–21. (<a href="https://doi.org/10.1007/s10489-025-06344-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper takes into account a general two-player zero-sum Markov game scenario in which our agent faces multi-type opponents with multiple policies. To enhance our agent’s return against opponent’s diverse policies, a novel Decision-making Framework based on Opponent Distinguishing and Policy Judgment (DF-ODPJ) is proposed. On the basis of the pre-trained Nash equilibrium strategies, DF-ODPJ can distinguish the opponent’s type by sampling from the interaction trajectory. Then a fast criterion is proposed to judge the opponent’s policy which is proven to minimize the misjudgment probability with optimal threshold calculated. According to the identification results, appropriate policies are generated to enhance the return. The proposed DF-ODPJ is more flexible since it is orthogonal to existing Nash equilibrium algorithms and single-agent reinforcement learning algorithms. The experimental results on grid world, video games, and UAV aerial combat environments illustrate the effectiveness of DF-ODPJ. The code is available at https://github.com/ChenXJ295/DF-ODPJ .},
  archive      = {J_APIN},
  author       = {Zhu, Jin and Wang, Xuan and Geir E., Dullerud},
  doi          = {10.1007/s10489-025-06344-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced decision framework for two-player zero-sum markov games with diverse opponent policies},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large scale group decision making with expert guidance via
discrete conditional variational autoencoder. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06345-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Large Scale Group Decision Making (LSGDM), the differences in decision-makers’ professional backgrounds and attitudes often lead to high-quality decisions being overshadowed by numerous low-quality decisions, thus affecting the accuracy of the final decision. This study proposes a new decision-making method to address this challenge. First, a few experts are invited to make decisions as cluster centers, followed by obtaining decisions from a large number of ordinary decision-makers. The ordinary decisions are then generated and modified using a Discrete Conditional Variational Autoencoder (DCVAE) to enhance decision quality while maintaining consistency with expert decisions. Finally, the normalized prediction selection rate (NPSR) and the Borda Count consensus method are integrated to obtain the final result. Experimental results demonstrate the effectiveness of this method in improving the quality of LSGDM, providing a new solution to the coexistence of high- and low-quality decisions.},
  archive      = {J_APIN},
  author       = {Zhang, Hengshan and He, Adong and Sun, Jiaze and Chen, Yanping},
  doi          = {10.1007/s10489-025-06345-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {A large scale group decision making with expert guidance via discrete conditional variational autoencoder},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional time-dependent dynamic graph neural
network for metro passenger flow prediction. <em>APIN</em>,
<em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06346-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate metro passenger flow prediction can provide data support for vehicle scheduling and personnel allocation by metro operation departments, ensuring the efficient utilization of related resources. In recent years, Graph Convolutional Networks (GCNs) have demonstrated excellent performance in spatial processing, making them an effective method for extracting spatiotemporal dependencies in metro passenger flow prediction. However, traditional GCN models focus solely on static relationships between stations, overlooking the dynamic changes in station relationships and typically concentrating on short-term temporal dependencies while neglecting longer-term temporal features. To fully consider the spatiotemporal relationships within the metro network, a Multi-Dimensional Temporal Dependency Graph Neural Network (MTDGNN) is proposed for metro passenger flow prediction. Specifically, 1D dilated convolutions are employed to initially extract multi-dimensional temporal dependencies, generating multiple spatiotemporal dependency extraction channels. Two correlation matrices combined with GCN are then proposed to extract spatial relationships between stations within the metro network. The extracted spatiotemporal features are further captured by a Gated Recurrent Unit (GRU) to enhance temporal feature extraction. Subsequently, a multi-head attention mechanism is utilized to integrate the extraction results from multiple channels to obtain the final prediction. Finally, the model is evaluated using metro ridership data from two cities in southwestern and central China. The results indicate that the proposed model exhibits superior predictive performance compared to other methods. The MAE values on the two datasets are 1.5% to 59.3% lower than those of other methods, and the RMSE values are 3.4% to 60.0% lower than those of other methods.},
  archive      = {J_APIN},
  author       = {Li, Ruisen and Zhao, Liqiang and Tang, Jinjin and Tang, Shuixiong and Hao, Zhenxing},
  doi          = {10.1007/s10489-025-06346-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Multi-dimensional time-dependent dynamic graph neural network for metro passenger flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An echo state network with adaptive improved pigeon-inspired
optimization for time series prediction. <em>APIN</em>, <em>55</em>(6),
1–32. (<a href="https://doi.org/10.1007/s10489-025-06347-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective alternative model to recurrent neural network (RNN), echo state network (ESN) has garnered more attention due to its efficiency in handling time series data. Despite the simple training process and rapid convergence speed of ESN, appropriate parameter settings and a concise network structure are crucial for optimal model performance. Therefore, many optimization algorithms have been proposed to obtain the optimal parameters of ESN. Among these methods, the Pigeon-Inspired Optimization (PIO) has gained attention due to its fast search speed, strong evolution capability, and excellent optimization ability. However, the main drawbacks of PIO are that it may easily get trapped in local optima and achieve lower precision results. To address these issues, this paper proposes a hybrid algorithm combining adaptive improved pigeon-inspired optimization with tabu search (TS-APIO) algorithm. By combining the improved PIO and the tabu search (TS), it not only enhances the global search capability but also strengthens its robustness. Additionally, the adaptive adjustment mechanism can improve the generalization ability. Through theoretical analysis and simulation examples, the TS-APIO algorithm can adaptively select the optimal ESN parameters and structure based on different scenarios. It can effectively enhance the ability to capture the dynamic features and reduce the prediction error.},
  archive      = {J_APIN},
  author       = {Yang, Xu and Wang, Lei and Chen, Qili},
  doi          = {10.1007/s10489-025-06347-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Appl. Intell.},
  title        = {An echo state network with adaptive improved pigeon-inspired optimization for time series prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised heterogeneous graph neural network based on
deep and broad neighborhood encoding. <em>APIN</em>, <em>55</em>(6),
1–23. (<a href="https://doi.org/10.1007/s10489-025-06348-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised heterogeneous graph neural networks have shown remarkable effectiveness in addressing the challenge of limited labeled data. However, current contrastive learning methods face limitations in leveraging neighborhood information for each node. Some approaches utilize the local information of the target node, ignoring useful signals from deeper neighborhoods. On the other hand, simply stacking convolutional layers to expand the neighborhood inevitably leads to over-smoothing. To address the problems, we propose HGNN-DB, a Self-supervised Heterogeneous Graph Neural Network Based on Deep and Broad Neighborhood Encoding to tackle the over-smoothing problem within heterogeneous graphs. Specifically, HGNN-DB aims to learn informative node representations by incorporating both deep and broad neighborhoods. We introduce a deep neighborhood encoder with a distance-weighted strategy to capture deep features of target nodes. Additionally, a single-layer graph convolutional network is employed for the broad neighborhood encoder to aggregate broad features of target nodes. Furthermore, we introduce a collaborative contrastive mechanism to learn the complementarity and potential invariance between the two views of neighborhood information. Experimental results on four real-world datasets and seven baselines demonstrate that our model significantly outperforms the current state-of-the-art techniques on multiple downstream tasks. The codes and datasets for this work are available at https://github.com/SSQiana/HGNN-DB.},
  archive      = {J_APIN},
  author       = {Song, Qianyu and Li, Chao and Fu, Jinhu and Zeng, Qingtian and Xie, Nengfu},
  doi          = {10.1007/s10489-025-06348-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Self-supervised heterogeneous graph neural network based on deep and broad neighborhood encoding},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NNBSVR: Neural network-based semantic vector representations
of ICD-10 codes. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06349-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically predicting ICD-10 codes from clinical notes using machine learning models can reduce the burden of manual coding. However, existing methods often overlook the semantic relationships between ICD-10 codes, resulting in inaccurate evaluations when clinically similar codes are considered completely different. Traditional evaluation metrics, which rely on equality-based matching, fail to capture the clinical relevance of predicted codes. This study introduces NNBSVR (Neural Network-Based Semantic Vector Representations), a novel approach for generating semantic-based vector representations of ICD-10 codes. Unlike traditional approaches that rely on exact code matching, NNBSVR incorporates contextual and hierarchical information to enhance both prediction accuracy and evaluation methods. We validate NNBSVR using intrinsic and extrinsic evaluation methods. Intrinsic evaluation assesses the vectors’ ability to reconstruct the ICD-10 hierarchy and identify clinically meaningful clusters. Extrinsic evaluation compares our relevancy-based approach, which includes customized evaluation metrics, to traditional equality-based metrics on an ICD-10 code prediction task using a 9.57 million clinical notes corpus. NNBSVR demonstrates significant improvements over equality-based metrics, achieving a 9.81% gain in micro-F1 score on the training set and a 12.73% gain on the test set. A manual review by medical experts on a sample of 10,000 predictions confirms an accuracy of 92.58%, further validating our approach. This study makes two significant contributions: first, the development of semantic-based vector representations that encapsulate ICD-10 code relationships and context; second, the customization of evaluation metrics to incorporate clinical relevance. By addressing the limitations of traditional equality-based evaluation metrics, NNBSVR enhances the automated assignment of ICD-10 codes in clinical settings, demonstrating superior performance over existing methods.},
  archive      = {J_APIN},
  author       = {Hatoum, Monah Bou and Charr, Jean Claude and Ghaddar, Alia and Guyeux, Christophe and Laiymani, David},
  doi          = {10.1007/s10489-025-06349-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {NNBSVR: Neural network-based semantic vector representations of ICD-10 codes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning sparse filters-based convolutional networks without
offline training for robust visual tracking. <em>APIN</em>,
<em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06350-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the scarcity of training samples in the visual tracking task, almost all existing Convolutional Neural Networks (CNNs) based deep tracking algorithms rely heavily on large auxiliary datasets to train the tracking model offline. However, such offline training has two inevitable disadvantages: (1) the learned generic features may be less discriminative for tracking specific objects; (2) the training process demands huge computational power provided by high-performance graphics processing units (GPUs), which is not always available in many practical applications. Therefore, learning effective generic features without offline training for robust visual tracking is a necessary and challenging task. This paper tackles this task by proposing the Sparse Filters-based Convolutional Network (SFCN), which is a fully feed-forward convolutional network with a lightweight structure including two convolutional layers. Its convolutional kernels are a set of sparse filters learned and updated online from local patches using sparse dictionary learning. Benefiting from the learned sparse filters, SFCN learns effective generic features by exploiting both the discriminative information between the foreground and background of the target region and the hierarchical layout information among the local patches inside each target candidate region. Furthermore, a dynamic model updating strategy is adopted to alleviate the drift problem. Extensive experiments on five large-scale benchmark datasets show that the proposed method performs favorably against several state-of-the-art tracking algorithms.},
  archive      = {J_APIN},
  author       = {Xu, Qi and Xu, Zhuoming and Chen, Zhe and Chen, Yun and Wang, Huabin and Tao, Liang},
  doi          = {10.1007/s10489-025-06350-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {Learning sparse filters-based convolutional networks without offline training for robust visual tracking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGCGNet: A local-global context guided network for real-time
water surface semantic segmentation. <em>APIN</em>, <em>55</em>(6),
1–20. (<a href="https://doi.org/10.1007/s10489-025-06351-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned boats will encounter many static and dynamic obstacles during navigation, and only real-time obstacle sensing can ensure safe navigation and long endurance of unmanned boats. In this paper, LGCGNet is proposed to perform real-time water surface semantic segmentation on the images captured by the on-board camera. In order to ensure that the model adapted to obstacles with extremely variable scales, a local-global module is proposed in this paper. The local-global module consisted of residual dense dilated module and context-enhanced separable self-attention. Residual dense dilated module enabled the enhancement of local detail information and context-enhanced separable self-attention enabled model receptive field expansion. In addition, the sub-pixel downsampling module is used to avoid the loss of feature information to improve segmentation accuracy. Experiments on the MaSTr1325 dataset showed that LGCGNet apprpached the segmentation accuracy of state-of-the-art semantic segmentation models with only 689,000 parameters and 9.068G floating point operations per second, with an mIoU of 84.14%. In addition, the processing speed of LGCGNet is 34.86FPS, which meets the frame rate conditions of commercially available photovoltaic equipment. The experiments demonstrated that the LGCGNet proposed in this paper strike a good balance between achieving high accuracy, reducing model size and improving real-time performance.},
  archive      = {J_APIN},
  author       = {Liu, Ting and Luo, Peiqi and Wang, Guofeng and Zhang, Yuxin and Lu, Xiangyi and Dong, Mengyu},
  doi          = {10.1007/s10489-025-06351-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {LGCGNet: A local-global context guided network for real-time water surface semantic segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scnet: Spectral convolutional networks for multivariate time
series classification. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06352-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of time series data, the study of classification techniques has become an important topic. Although existing multivariate time series classification (MTSC) methods have made progress, they often rely on one-dimensional (1D) time series, which limits their ability to capture complex temporal dynamics and multiscale features. To address these challenges, a Spectral Convolutional Network (SCNet) is introduced in this work. SCNet effectively transforms 1D time series data into the frequency domain using an enhanced Discrete Fourier Transform (enhanced_DFT), revealing periodicity and key frequency components while reshaping the data into a two-dimensional (2D) time series for better representation. Furthermore, it uses a Spectral Energy Prioritization method to optimize frequency domain energy distribution and a multiscale convolutional module to capture features at different scales, improving the model’s ability to analyze short-term and long-term trends. To validate the effectiveness and superiority, we conducted extensive experiments on 10 sub-datasets from the well-known UEA dataset. The results show that our proposed SCNet achieved the highest average accuracy of 74.3%, which is 2.2% higher than the current state-of-the-art models, demonstrating its potential for practical application and efficiency in MTSC task.},
  archive      = {J_APIN},
  author       = {Wu, Xing and Xing, Xinyu and Yao, Junfeng and Qian, Quan and Song, Jun},
  doi          = {10.1007/s10489-025-06352-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Scnet: Spectral convolutional networks for multivariate time series classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise spiking neurons for fitting any activation function
in ANN-to-SNN conversion. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06354-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) are recognized for their energy efficiency due to spike-based communication. In this regard, the shift towards SNNs is driven by their ability to significantly reduce energy consumption while maintaining the performance of ANNs. Converting Artificial Neural Networks (ANNs) to SNNs is a key research focus, but existing methods often struggle with balancing conversion accuracy and latency, and are typically restricted to ReLU activations. We introduce Precision Spiking (PS) neurons, a novel dynamic spiking neuron model that can precisely fit any activation function by jointly regulating spike timing, reset voltage, and membrane potential threshold. This capability enables exact parameter optimization via iterative methods, achieving low-latency, high-accuracy ANN-to-SNN conversion. Experiments on image classification and natural language processing benchmarks confirm state-of-the-art results, with a maximum conversion loss of 0.55% and up to 0.38% accuracy improvement over the original ANN. To the best of our knowledge, this method offers a significant advancement over existing approaches by achieving high-precision fitting of arbitrary activation functions with low latency and minimal conversion loss, thus considerably expanding the range of feasible ANN-to-SNN conversions.},
  archive      = {J_APIN},
  author       = {Wang, Tianqi and Shen, Qianzi and Li, Xuhang and Zhang, Yanting and Wang, Zijian and Yan, Cairong},
  doi          = {10.1007/s10489-025-06354-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Precise spiking neurons for fitting any activation function in ANN-to-SNN conversion},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NoRD: A framework for noise-resilient self-distillation
through relative supervision. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06355-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has become a pivotal technique in deep learning, facilitating model compression and regularization by transferring knowledge from one neural network to another, enhancing its capabilities for downstream tasks such as classification. However, real-world datasets often suffer from noisy label problems, significantly hindering neural network learning in supervised tasks. Recent advancements in KD aim to improve noise-robustness and regularization in deep neural networks through different learning paradigms. Yet, prevalent approaches often exhibit noise-prone behaviors as the student network heavily relies on the teacher’s learning. To address this challenge, we propose a robust knowledge transfer method, NoRD: a Noise-Resilient Self-Distillation framework. This approach leverages relative self-supervision combined with decision matching to minimize noise susceptibility during the knowledge transfer process. Our study evaluates this technique on CIFAR-10, CIFAR-100, and MNIST datasets with synthetic label noise. Results showcase that our method achieves 8-10% higher test accuracy compared to state-of-the-art noise-robust loss functions at noise rates exceeding 50%, surpassing well-known KD methods by 4-5% in top-1 test accuracy. The code is available at https://github.com/philsaurabh/NoRD_Applied-Intelligence .},
  archive      = {J_APIN},
  author       = {Sharma, Saurabh and Lodhi, Shikhar Singh and Srivastava, Vanshika and Chandra, Joydeep},
  doi          = {10.1007/s10489-025-06355-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {NoRD: A framework for noise-resilient self-distillation through relative supervision},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lbgcn: Lightweight bilinear graph convolutional network with
attention mechanism for recommendation. <em>APIN</em>, <em>55</em>(6),
1–17. (<a href="https://doi.org/10.1007/s10489-025-06357-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Convolutional Neural Network (GCN) is a powerful technique for learning and representing graph data, commonly utilized in model-based collaborative filtering recommendation algorithms. However, despite its effectiveness, the issues are data sparsity and interpretability. Most existing GCN-based models simply update the central node’s features by aggregating the features of its neighbors, typically via a weighted sum. Unfortunately, this approach fails to capture the cooperative information hidden in the neighbor interactions. To address this limitation, we propose a recommendation algorithm based on a convolution network of lightweight neighborhood interactive graphs, named the Lightweight Bilinear Graph Convolutional Network (LBGCN). Our approach employs a lightweight graph convolutional neural network as a multi-level feature aggregator, leveraging higher-order connectivity to aggregate neighborhood information into a multi-level feature of the node through the aggregator. Meanwhile, we introduce a local feature aggregator to capture the collaborative filtering signals in the interaction features of neighbors. Finally, we combine the results using an attention mechanism to obtain the embedded representation of final users and items. In addition, we demonstrate the rationality and effectiveness of our proposed model through experiments on three public datasets. The results show that our method could gain 2.52% NDCG improvement at most.},
  archive      = {J_APIN},
  author       = {Su, Yu and Wei, Pingzhu and Zhu, Linbo and Xu, Lixiang and Wang, Xianquan and Tong, He and Han, Ze},
  doi          = {10.1007/s10489-025-06357-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Lbgcn: Lightweight bilinear graph convolutional network with attention mechanism for recommendation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel ensemble bagging-logistic regression algorithm for
NoSQL database security. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06358-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present era, the use of the Internet has drastically increased in the sharing of digital information. In this case, the digital information is stored using cloud technology or NoSQL databases. However, there is a significant challenge in protecting and managing the cloud and NoSQL-based data and extracting required information from these sources while maintaining the actual information. The network traffic has also increased significantly, which requires more memory and sufficient systems to manage and monitor the influx of Big Data. Traditional relational databases face issues in managing and securing the cloud-based dynamic data generated from various sources. NoSQL databases have recently been used to store and manage dynamic data effectively. However, there are security and privacy issues with the NoSQL databases, which remain challenging to provide. Consequently, in the present study, we propose a novel algorithm that enhances the security of the NoSQL databases and predicts its success rate. Initially, we implemented the Fernet data masking algorithm to secure the NoSQL database. Then, the secured data is classified and predicted using an innovative proposed method called the Ensemble Bagging Classifier-Logistic Regression (EBC-LR) to validate the accuracy of the secured NoSQL database. The experimental outcomes depict that our proposed algorithm achieves 85 percent accuracy, better than traditional methods in enhancing the security of NoSQL databases. Our proposed algorithm can effectively predict secure standard databases with the highest success rate.},
  archive      = {J_APIN},
  author       = {Kanade, Anuradha and Vibhute, Amol D. and Kanade, Shantanu},
  doi          = {10.1007/s10489-025-06358-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Novel ensemble bagging-logistic regression algorithm for NoSQL database security},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixmamba-fewshot: Mamba and attention mixer-based method
with few-shot learning for bearing fault diagnosis. <em>APIN</em>,
<em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06361-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, artificial intelligence, particularly machine learning and deep learning has ushered in a new era of technological advancements leading to significant progress across various domains. In the field of computer vision, deep learning has made substantial contributions, impacting everything from daily life to production and industry. When machines, rotating devices, and engines operate, bearing failures are inevitable. Our task is to accurately detect or diagnose these failures. However, a key challenge lies in the lack of sufficient data on bearing faults to train a model capable of delivering highly accurate diagnostic results. To address this issue, in this paper, we propose a new approach named MixMamba-Fewshot, leveraging few-shot learning and using a feature extraction module that integrates an attention mechanism called the Priority Attention Mixer and Mamba - a novel theory that has recently gained considerable attention within the research community. Using Mamba for vision-based feature extraction in classification tasks, particularly in few-shot learning is an innovative approach, and it has shown promising results in improving the accuracy of bearing fault diagnosis. When we tested our model on the datasets provided by Case Western Reserve University (CWRU) and the Paderborn University (PU) Bearing Dataset, we compared it with previously published models. Our proposed approach demonstrated a significant improvement in diagnostic accuracy and clearly outperformed existing approaches. Our code will be available at: https://github.com/linhthan216/MixMamba-Fewshot .},
  archive      = {J_APIN},
  author       = {Than, Nhu-Linh and Nguyen, Van Quang and Truong, Gia-Bao and Pham, Van-Truong and Tran, Thi-Thao},
  doi          = {10.1007/s10489-025-06361-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Mixmamba-fewshot: Mamba and attention mixer-based method with few-shot learning for bearing fault diagnosis},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HiProIBM: Unsupervised continual learning through
hierarchical prototypical cross-level discrimination along with
information bottleneck subnetwork masking. <em>APIN</em>,
<em>55</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06362-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic Forgetting (CF) occurs when a machine learning model forgets the experience of previous tasks while learning new tasks due to inadequate retention mechanisms. Unsupervised continual learning (UCL) addresses this by enabling the model to adapt to new tasks using unlabeled data while retaining past knowledge. To mitigate CF in UCL, we use a parameter isolation technique to mask sub-networks dedicated to each task, thus preventing interference with previous tasks. However, relying solely on weight magnitude for constructing these sub-networks can result in the retention of irrelevant weights and the creation of redundant sub-networks. This approach also risks capacity saturation and information suppression for tasks encountered later in the sequence. To overcome this, we use masked sub-networks, inspired by the information bottleneck (IB) concept. It accumulates valuable information into essential weights to construct redundancy-free sub-networks which effectively mitigates CF and enables the new task training. The IB subnetwork masking faces challenges in balancing input compression with meaningful pattern preservation without labels. It risks overcompression and loss of crucial latent structures, which degrades model performance. We address this by learning multiple semantic hierarchies present in the data using unsupervised contrastive learning. However traditional contrastive learning techniques learn meaningful representations by contrasting similar and dissimilar data points. These approaches lack adequate representational power for modeling datasets with multiple semantic hierarchies. The inherent hierarchical semantic structures in datasets are necessary to integrate semantically related clusters into larger, coarser-grained clusters, but existing contrastive learning methods often overlook this and limit semantic understanding. We address this by constructing and updating hierarchical prototypes with cross-level group discrimination to represent semantic structures in the latent space. Our experiments on four standard datasets show performance improvements over SOTA baselines for varying task-sequences from 5 to 100, with nearly-zero forgetting.},
  archive      = {J_APIN},
  author       = {Malviya, Ankit and Kumar Maurya, Chandresh},
  doi          = {10.1007/s10489-025-06362-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {HiProIBM: Unsupervised continual learning through hierarchical prototypical cross-level discrimination along with information bottleneck subnetwork masking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting gas flow rates of wellhead chokes based on a
cascade forwards neural network with a historically limited penetrable
visibility graph. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06365-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel hybrid model that combines the cascade forward neural network (CFNN) with a historical limited penetrable visibility graph (HLPVG) for accurate prediction of gas flow rates through wellhead chokes in shale gas production. The model addresses the challenges of complex, nonlinear relationships between multiple variables affecting gas flow, including liquid–gas ratio (LGR), upstream pressure, temperature, and choke bean size. Using 11,572 field production samples from shale gas fields in the southern Sichuan Basin, the CFNN-HLPVG model demonstrates superior predictive performance compared to the conventional methods. The HLPVG algorithm transforms time series data into a graph structure, enabling the extraction of rich temporal and topological features, whereas the CFNN captures the complex interactions between variables. The model achieves a mean absolute relative error (MARE) of 0.014, significantly outperforming traditional approaches, including the Gilbert-type correlation, support vector machine, and other neural network architectures. Sobol sensitivity analysis revealed that choke bean size has the greatest impact on gas flow prediction (37.7% first-order sensitivity), followed by upstream pressure (19.3%) and temperature (11.6%), whereas LGR has a minimal influence (0.6%). The model performs particularly well under normal operating conditions but shows decreased accuracy in extreme environments with high temperature and pressure. This research provides a novel approach to gas flow prediction in wellhead chokes, offering valuable insights for optimizing shale gas production operations while highlighting areas for future improvement in handling extreme conditions and multisource data integration.},
  archive      = {J_APIN},
  author       = {Jiang, Youshi and Hu, Jingkai and Chen, Xiyu and Mo, Weiren},
  doi          = {10.1007/s10489-025-06365-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Predicting gas flow rates of wellhead chokes based on a cascade forwards neural network with a historically limited penetrable visibility graph},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-supervised anomalous machine sound detection model
based on spectrogram decomposition and parallel sub-network.
<em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06366-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalous Sound Detection (ASD) has research significance and application prospect industrial automation. Most existing models of ASD have limited ability to effectively utilize machine sound features, leading to reduced stability against sound anomalies and domain shift variations. To address the above issues, we propose a self-supervised ASD model based on spectrogram decomposition and parallel sub-network in this paper. Firstly, we decompose the spectrogram along the time and frequency dimensions to balance feature size and information integrity. This approach emphasizes the temporal and frequency variations in the feature map, facilitating a better understanding of the factors that affect machine sounds under domain shift conditions. Secondly, we design a pair of parallel training sub-networks. The parallel sub-networks employ self-attention mechanisms and shared gradients to effectively capture changes in features across both time and frequency dimensions. This approach improves model stability against anomalies and domain shifts. Finally, the anomaly scores of sub-network branches are fused as anomalous detection results. The performance of the proposed model is validated on DCASE2022 Task2 dataset. The Area under the Receiver Operating Characteristic Curve (AUC) and partial AUC (pAUC) of our model reached 72.89% and 64.83%. The results confirm the effectiveness of the proposed model, achieving better performance.},
  archive      = {J_APIN},
  author       = {Zhang, Tao and Kong, Lingguo and Zhao, Xin and Li, Donglei and Geng, Yanzhang and Ding, Biyun and Wang, Chao},
  doi          = {10.1007/s10489-025-06366-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A self-supervised anomalous machine sound detection model based on spectrogram decomposition and parallel sub-network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of deep non-smooth symmetric nonnegative matrix
factorization on hierarchical clustering. <em>APIN</em>, <em>55</em>(6),
1–16. (<a href="https://doi.org/10.1007/s10489-025-06367-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep matrix factorization (deep MF) is an increasingly popular unsupervised data-mining technique that operates as a deep decomposition rooted in traditional nonnegative matrix factorization (NMF). Compared with standard NMF, deep MF has shown excellent performance in the extraction of hierarchical information from complex datasets. For cases in which the data matrices corresponding to the dataset are symmetric—such as the adjacency matrix of an undirected graph in network analysis—this paper proposes a deep MF variant called deep non-smooth nonnegative symmetric matrix factorization (DNSSNMF). The aim of this work is to enhance the extraction of complex hierarchical structures in high-dimensional datasets and achieve the clustering of structures inherent in graphical representations by improving the goodness-of-fit of the factor matrix product. Accordingly, we successfully applied DNSSNMF to post-traumatic-stress-disorder (PTSD) datasets and synthetic datasets to extract several hierarchical communities. In particular, we extracted non-disjoint communities in the partial correlation network of psychiatric symptoms in PTSD, revealing correlations between different symptoms and leading to meaningful clinical interpretations. The results of our numerical experiments indicated promising applications of DNSSNMF in fields including network analysis and medicine.},
  archive      = {J_APIN},
  author       = {Li, Shunli and Lu, Linzhang and Liu, Qilong and Chen, Zhen},
  doi          = {10.1007/s10489-025-06367-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of deep non-smooth symmetric nonnegative matrix factorization on hierarchical clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composed image retrieval: A survey on recent research and
development. <em>APIN</em>, <em>55</em>(6), 1–35. (<a
href="https://doi.org/10.1007/s10489-025-06372-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, composed image retrieval (CIR) has gained significant attention within the research community due to its excellent research value and extensive real-world applications. CIR allows modifying query images based on user-provided text descriptions, producing search results that better match users’ intent. This paper conducts a comprehensive and up-to-date survey of CIR research and its applications. We summarise recent advancements in CIR methodologies from these perspectives by breaking down a CIR system into four key processes-feature extraction, feature alignment, feature fusion, and image retrieval. We examine feature extraction, emphasizing deep learning techniques for images and text. As deep learning evolves, feature alignment increasingly integrates with other processes, encouraging us to categorize related methods into explicit and implicit approaches. From the perspective of feature fusion, we investigate advancements in image-text feature fusion techniques, categorizing them into 6 broad categories and 17 subcategories. We also summarize different architecture types and training loss functions for image retrieval. Additionally, we review standard benchmark datasets and evaluation metrics in CIR, presenting a comparative analysis of the accuracy of crucial CIR approaches. Finally, we put forward several critical yet underexplored issues in the field.},
  archive      = {J_APIN},
  author       = {Wan, Yongquan and Zou, Guobing and Zhang, Bofeng},
  doi          = {10.1007/s10489-025-06372-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Appl. Intell.},
  title        = {Composed image retrieval: A survey on recent research and development},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anchoring actions using conditional behavior trees and
genetic programming. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06373-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic Kitting means the creation of parts assortment to be used later. These parts are selected from one or more containers in which there are different types of them randomly distributed. The Anchoring Problem should be considered if we want to provide a general solution to robotic kitting, since users want that it works with different types of parts that are not known ’a priori’. Therefore, we are working on a human supervised approach in which Behavior Trees, robot learning and human-robot interaction are used to anchor percepts and operations to symbols during commissioning or reconfiguration phases. In this paper we explain: (1) the anchoring mechanisms in our system and how behavior trees can be used to represent an anchor, and (2) how Genetic Programming is used to generate Conditional Behavior Trees that anchor symbolic actions to robot operations.},
  archive      = {J_APIN},
  author       = {Escudero-Rodrigo, Diego and Alquezar, René and Aranda, Joan},
  doi          = {10.1007/s10489-025-06373-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Anchoring actions using conditional behavior trees and genetic programming},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADIMPL: A dynamic, real-time and robustness attack detection
model for industrial cyber-physical systems based on improved meta
pseudo labels. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06374-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the introduction of networking has increased the efficiency of Industrial Cyber-Physical Systems (ICPS), it has also lowered the cost for attackers, significantly increasing security risks. Current research on ICPS attack detection focuses on deep learning methods. However, the dependence on large labeled datasets often hinders these systems from adapting quickly to the dynamic changes and real-time demands of the ICPS environment. To address these issues, we present an attack detection method based on improved meta pseudo label (ADIMPL). ADIMPL innovatively combines two-layer network traffic feature extraction with the compact SqueezeNet deep neural network, achieving high performance with a minimal number of labeled samples. Additionally, the method dynamically adapts to changing attack patterns, significantly increasing detection accuracy while enhancing the robustness and real-time processing capabilities of the detection system. Extensive experiments on real-world industrial CPS datasets (CIC-IDS2017, CIC-IDS2018, and the CIC-Attack Dataset 2023) demonstrate that ADIMPL can effectively, robustly, and in real-time detect network attacks against industrial CPS. Notably, ADIMPL achieves a detection accuracy of 99.13% with an average latency of 0.098 s and maintains a minimum attack detection accuracy of 91.99% even under our proposed GAN+OPSO malicious attacks.},
  archive      = {J_APIN},
  author       = {Zhang, Bohan and Zhang, Pan and Wang, Zhiwen and Lv, Jiaqi and Miao, Wei},
  doi          = {10.1007/s10489-025-06374-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {ADIMPL: A dynamic, real-time and robustness attack detection model for industrial cyber-physical systems based on improved meta pseudo labels},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STGFP: Information enhanced spatio-temporal graph neural
network for traffic flow prediction. <em>APIN</em>, <em>55</em>(6),
1–21. (<a href="https://doi.org/10.1007/s10489-025-06377-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial for the development of intelligent transportation systems aimed at preventing and mitigating traffic issues. We present an information-enhanced spatio-temporal graph neural network model to predict traffic flow, addressing the inefficient utilization of non-Euclidean structured traffic data. Firstly, we employ a multivariate temporal attention mechanism to capture dynamic temporal correlations across different time intervals, while a second-order graph attention network identifies spatial correlations within the network. Secondly, we construct two types of traffic topology graphs that comprehensively describe traffic flow features by integrating non-Euclidean traffic flow data, regional traffic status information, and node features. Finally, a multi-graph convolution neural network is designed to extract long-range spatial features from these traffic topology graphs. The spatio-temporal feature extraction module then combines these long-range spatial features with spatio-temporal features to fuse multiple features and improve prediction accuracy. Experimental results demonstrate that the proposed approach outperforms state-of-the-art baseline methods in predicting traffic flow performance.},
  archive      = {J_APIN},
  author       = {Li, Qi and Wang, Fan and Wang, Chen},
  doi          = {10.1007/s10489-025-06377-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {STGFP: Information enhanced spatio-temporal graph neural network for traffic flow prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-local modeling of enhancer-promoter interactions, a
correspondence on “LOCO-EPI: Leave-one-chromosome-out (LOCO) as a
benchmarking paradigm for deep learning based prediction of
enhancer-promoter interactions.” <em>APIN</em>, <em>55</em>(6), 1–5. (<a
href="https://doi.org/10.1007/s10489-025-06378-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent paper by Tahir et al. (Appl Intell 55:71, 2024) in Applied Intelligence reported a computational model of enhancer promoter interactions without realizing that many of their conclusions were previously published in 2018. In addition to correcting this record, the authors appear to be unaware of an additional body of previous work on enhancer-promoter interactions, which can explain why their computational model performs poorly. We describe how the weak predictive power of their model is consistent with new insights gained from substantial recent progress in the area of detecting and modeling enhancer promoter interactions constrained by DNA looping, extrusion by cohesin, and CTCF.},
  archive      = {J_APIN},
  author       = {Beer, Michael A.},
  doi          = {10.1007/s10489-025-06378-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-5},
  shortjournal = {Appl. Intell.},
  title        = {Non-local modeling of enhancer-promoter interactions, a correspondence on “LOCO-EPI: Leave-one-chromosome-out (LOCO) as a benchmarking paradigm for deep learning based prediction of enhancer-promoter interactions”},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). A local generation-mix cascade network for image
translation with limited data. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06379-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image translation based on deep generative models often overfits with limited data. Current methods overcome this problem through mix-based data augmentation. However, if latent features are mixed without considering semantic correspondences, augmented samples may exhibit visible artifacts and mislead model training. In this paper, we propose a Local Generation-Mix Cascade Network (LogMix), a data augmentation strategy for image translation tasks with limited data. Through cascading a local feature generation module and mixing module, LogMix enables the generation of a reference feature bank, which is mixed with the most similar local representation to form a new intermediate sample. Furthermore, we design a semantic relationship loss based on the mixed distance of latent features ensures consistency in the distribution of features between the generated and source domains. LogMix effectively mitigates the overfitting problem by learning to translate intermediate samples instead of memorizing the training data Experimental results across various tasks demonstrate that, even with limited data, LogMix data augmentation reduces image ambiguity and offers significant advantages in establishing realistic cross-domain mappings.},
  archive      = {J_APIN},
  author       = {Zhang, Yusen and Li, Min and Gou, Yao and Zhang, Xianjie and He, Yujie},
  doi          = {10.1007/s10489-025-06379-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A local generation-mix cascade network for image translation with limited data},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fuzzy knowledge graph structure for decision making
of multimodal big data. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06381-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making in the era of big data is always a challenge. Recently, various methods especially graph sampling have been presented to assist the decision more effectively. As real-world graphs are large, constantly evolving, and distributed in nature, it becomes necessary to sample their structures for many different goals. Therefore, acquiring a comprehensive and in-depth understanding of graph sampling is essential to strengthen this field. In addition, graph sampling techniques often rely on edge or vertex sampling without effective methods for rule or path sampling. In this paper, we propose a novel framework for the rule-based sampling method on fuzzy knowledge graphs. In this framework, fuzzy knowledge graphs are built on integrated databases from multiple sources. We design a purposive random sampling method based on fuzzy rules on graphs to prioritize important rules for output inference. The remaining important rules form the core structure of the fuzzy knowledge graph, known as the Fuzzy Knowledge Graph Structure (FKGS). This structure is considered as a compression mechanism to reduce computational complexity when representing and performing calculations for large-scale data problems. Experimental results based on benchmark datasets on diabetes mellitus show that the sampling method greatly reduces the calculation time while maintaining high accuracy. Moreover, the purposive random sampling method results in significantly higher accuracy than the random sampling method. Besides, the ANOVA method is also conducted to statistically validate the model. The results are significant for decision-making in the context of big data.},
  archive      = {J_APIN},
  author       = {Tan, Nguyen Hong and Long, Cu Kim and Tuan, Tran Manh and Chuan, Pham Minh and Hai, Pham Van and Khanh, Phan Hung and Son, Le Hoang},
  doi          = {10.1007/s10489-025-06381-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A novel fuzzy knowledge graph structure for decision making of multimodal big data},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instructed fine-tuning based on semantic consistency
constraint for deep multi-view stereo. <em>APIN</em>, <em>55</em>(6),
1–25. (<a href="https://doi.org/10.1007/s10489-025-06382-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing depth map-based multi-view stereo (MVS) methods typically assume that texture features remain consistent across different viewpoints. However, factors such as lighting changes, occlusions, and weakly textured regions can lead to inconsistent texture features, posing challenges for feature extraction. As a result, relying solely on texture consistency does not always yield high-quality reconstruction results in certain scenarios. In contrast, high-level semantic concepts corresponding to the same objects remain consistent across different viewpoints, which we define as semantic consistency. Since designing and training new MVS networks from scratch is both costly and labor-intensive, we propose fine-tuning existing depth map-based MVS networks during testing phase by incorporating semantic consistency constraints to improve the reconstruction quality in regions with poor results. Considering the robust open-set detection and zero-shot segmentation capabilities of Grounded-SAM, we first use Grounded-SAM to generate semantic segmentation masks for arbitrary objects in multi-view images based on text instructions. These masks are then used to fine-tune pre-trained MVS networks via aligning them from different viewpoints to the reference viewpoint and optimizing the depth maps based on the proposed semantic consistency loss function. Our method is designed as a test-time approach that is adaptable to a wide range of depth map-based MVS networks, requiring only adjustments to a small number of depth-related parameters. Comprehensive experimental evaluation across different MVS networks and large-scale scenarios demonstrates that our method effectively enhances reconstruction quality at a lower computational cost.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Yan, Hongping and Ding, Kun and Cai, Tingting and Zhou, Yueyue},
  doi          = {10.1007/s10489-025-06382-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {Instructed fine-tuning based on semantic consistency constraint for deep multi-view stereo},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DropMismatch: Removing mismatched UI elements for better
pixel to code generation. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06384-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating the generation of user interface (UI) code from design images has gained significant attention due to its potential to streamline application development. However, the effectiveness of deep learning models in this domain is often hindered by mismatches between UI images and their corresponding layout code, a common issue in image-text datasets. In this paper, we introduce a framework that locates and removes these mismatches, thereby improving the accuracy of UI code generation models. Our approach leverages a convolutional neural network to predict the alignment between UI components and layout code nodes, coupled with a tree-based heuristic algorithm to localize mismatches. Through extensive evaluation, we demonstrate that our method enhances the accuracy of UI code generation by approximately 15%, while significantly reducing the need for costly manual annotations. The proposed framework not only advances the state of automated UI code generation but also lays the foundation for creating high-quality, large-scale UI datasets, essential for future research and development in this field.},
  archive      = {J_APIN},
  author       = {Li, Ming and Lin, Tao},
  doi          = {10.1007/s10489-025-06384-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {DropMismatch: Removing mismatched UI elements for better pixel to code generation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NVS-former: A more efficient medical image segmentation
model. <em>APIN</em>, <em>55</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s10489-025-06387-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current field of medical image segmentation research, numerous Transformer-based segmentation models have emerged. However, these models often suffer from limitations in multi-scale feature extraction and struggle to capture local detail features and contextual information, thereby constraining their segmentation performance. This paper introduces a novel model for medical image segmentation, called NVS-Former, which comprises both an encoder and a decoder. The key innovation of NVS-Former lies in its redesigned core module during the encoding phase, which not only enhances feature extraction capabilities but also improves the capture of local detail information. Additionally, the decoder structure has been reengineered to further optimize the model’s class prediction abilities. NVS-Former has demonstrated superior performance in tasks involving multi-organ, pulmonary detail, and cell segmentation. In various comparative experiments, it consistently outperformed state-of-the-art methods, highlighting its efficiency and stability in medical image segmentation.},
  archive      = {J_APIN},
  author       = {Huang, Xiangdong and Huang, Junxia and Ibrahim, Noor Farizah},
  doi          = {10.1007/s10489-025-06387-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Appl. Intell.},
  title        = {NVS-former: A more efficient medical image segmentation model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for solving bias in graph-based recommender
systems with a causal perspective. <em>APIN</em>, <em>55</em>(6), 1–21.
(<a href="https://doi.org/10.1007/s10489-025-06388-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems founded on graph neural networks (GNN) have been extensively employed because of their exceptional recommendation efficiency. Nevertheless, numerous recommendation biases also crop up, We have observed that delicate details such as gender and age are frequently implicitly apprehended by recommendation systems, culminating in unfair recommendations, and the associated algorithms of GNN will magnify this bias. To tackle these difficulties, this paper puts forth a method of introducing the notion of causal fairness into the issue of fairness in GNN-based recommendation systems, to accomplish counterfactual fairness of user-sensitive information and thereby attain unbiased recommendations. Specifically, given a GNN-based recommendation system model, which is implemented in our devised fairness framework, chiefly obtaining equitable effects through two facets: (1) attaining user embedding fairness through the counterfactual fairness technique; (2) mitigating the prejudiced impact caused by the GNN algorithm using the proposed central association subgraph method. The amalgamation of these two facets ultimately delivers unbiased recommendations. The effectiveness and sophistication of our proposed method for mitigating partiality problems in GNN recommendation systems from a causal perspective (MGRC) have been proven via experiments on four real-world datasets.},
  archive      = {J_APIN},
  author       = {Yang, Kewu and Li, Guogang and Wang, Linjia and Xie, Jianrong},
  doi          = {10.1007/s10489-025-06388-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {A framework for solving bias in graph-based recommender systems with a causal perspective},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAG-unet: Multi-task dense scene understanding with
pixel-attention-guided unet. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06389-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task dense scene understanding is a fundamental research area in computer vision (CV). By predicting pixels, perceiving, and reasoning about multiple related tasks, it improves both accuracy and data efficiency. However, it faces the challenge that some tasks may require more independent feature representations, and excessive sharing can lead to interference between tasks. To address this issue, we propose a novel Pixel-Attention-Guided Unet (PAG-Unet). PAG-Unet incorporates a Pixel-Attention-Guided Fusion module (PAG Fusion) and a Multi-Task Self-Attention module (MTSA) to enhance task-specific feature extraction and reduce task interference. PAG Fusion leverages the relationship between shallow and deep features by using task-specific deep features to calibrate the distribution of shared shallow features. This suppresses background noise and enhances semantic features, thereby fully extracting task-specific features for different tasks and achieving feature enhancement. MTSA considers both global and local spatial interactions for each task during task interactions, capturing task-specific information and compensating for the loss of crucial details, thus improving prediction accuracy for each task. Our method achieves superior multi-task performance on the New York University Depth v2(NYUD-v2) and PASCAL Visual Object Classes Context(PASCAL-Context) datasets, with most metrics significantly outperforming previous state-of-the-art methods. The code is available at https://github.com/UPLI-123/Pag-Unet .},
  archive      = {J_APIN},
  author       = {Xu, Yi and Li, Changhao},
  doi          = {10.1007/s10489-025-06389-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {PAG-unet: Multi-task dense scene understanding with pixel-attention-guided unet},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting hemodynamic parameters based on arterial blood
pressure waveform using self-supervised learning and fine-tuning.
<em>APIN</em>, <em>55</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s10489-025-06391-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arterial blood pressure waveform (ABPW) serves as a less invasive technique for evaluating hemodynamic parameters, offering a lower risk compared to the more invasive pulmonary artery catheter (PAC) thermodilution method. Various studies suggest that deep learning models can potentially predict the hemodynamic parameters of ABPW. However, the scarcity of ground truth data restricts the accuracy of these models, preventing them from gaining clinical acceptance. To mitigate this data and domain challenge, this work proposed a self-supervised generative learning model for hemodynamic parameter prediction, called SSHemo (Self-Supervised Hemodynamic model). Specifically, SSHemo suggests first to leverage large amounts of unlabeled ABPW data to learn the representative embedding and then to fine-tune for the downstream task with a small amount of hemodynamic parameters’ ground truth. To verify the effectiveness of SSHemo, we utilize the public available VitalDB data set to train the model, and evaluation was conducted on two public datasets: VitalDB and MIMIC. The experimental results reveal that SSHemo’s regression mean absolute error (MAE) improved significantly from 1.63 L/min to 1.25 L/min when predicting cardiac output (CO). The trending tracking ability for CO changes meets clinical acceptance (radial limit of agreement (LOA) is $$\pm 25.56$$ °, less than $$\pm 30$$ °). In addition, SSHemo demonstrates robust stability in various conditions and cohorts, as evidenced by subgroup analysis, varying systemic vascular resistance (SVR) range analysis, and rapid CO analysis, compared to the most widely used commercial devices, the EV1000. Computational analysis further underscores the value and potential of practical application of the model in various settings.},
  archive      = {J_APIN},
  author       = {Liao, Ke and Elibol, Armagan and Gao, Ziyan and Meng, Lingzhong and Chong, Nak Young},
  doi          = {10.1007/s10489-025-06391-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Predicting hemodynamic parameters based on arterial blood pressure waveform using self-supervised learning and fine-tuning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale contrastive learning via aggregated subgraph for
link prediction. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06394-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction seeks to uncover potential or future connections within a network using structural or attribute information. Recently, Graph Neural Network (GNN)-based methods have attracted considerable attention for their effectiveness in link prediction. However, most GNN-based approaches focus solely on single-scale input graphs, which limits their ability to comprehensively capture network structure information. In this paper, multi-scale subgraphs are introduced as input graphs to obtain complementary network structures from different perspectives. Simultaneously, to obtain embedding vectors with better representational capacity, contrastive loss from self-supervised learning is incorporated for link prediction. Specifically, Multi-scale Contrastive learning framework based on Aggregated Subgraph (MCAS) is proposed for predicting missing links. Firstly, we construct enclosing subgraph by extracting neighbors of target nodes. By applying aggregation operation to these subgraphs, different granularities of multi-scale subgraphs are obtained. Secondly, encoders are used to learn information from multiple scales of subgraphs separately. Next, contrastive learning is employed to achieve information balance among the multi-scale subgraphs. Finally, the minimization of the loss allows us to improve the model’s robustness. Empirical evidence indicates that our approach excels state-of-the-art methods on nine datasets, including biological and citation networks. All source code is publicly available at: https://github.com/yabingyao/MCAS4LinkPrediction .},
  archive      = {J_APIN},
  author       = {Yao, Yabing and Guo, Pingxia and Mao, Zhiheng and Ti, Ziyu and He, Yangyang and Nian, Fuzhong and Zhang, Ruisheng and Ma, Ning},
  doi          = {10.1007/s10489-025-06394-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale contrastive learning via aggregated subgraph for link prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSFL: A blockchain-based data sharing and federated learning
framework. <em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06400-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive amount of data generated by the proliferation of Internet of Things (IoT) devices has become one of the key factors driving the advancement of artificial intelligence (AI) technology. However, the lack of storage space and limited computational power of edge devices make it difficult to directly process large data volumes or run complex machine learning algorithms on these devices. At the same time, existing Federated Learning (FL) schemes still face a number of shortcomings, including a single point of failure, vulnerability to poisoning attacks, and a lack of incentives. To address the above issues, we propose DSFL, a blockchain-based framework for fair data sharing and FL. Specifically, we combine digital envelope technology and one-way accumulator with smart contracts to design fair, secure, and trustworthy data sharing protocols that facilitate edge devices to share data proactively, realize the value of data and reduce storage pressure. In addition, we propose blockchain extension schemes suitable for coupling with FL to improve training efficiency. Importantly, the node management mechanism and incentive algorithms are designed to effectively monitor and trace the behavior of nodes, and promote the virtuous cycle of model training and the motivation of participants. Experimental results show that DSFL is able to ensure fair data sharing and efficient model training without the involvement of trusted third parties. In particular, it is able to achieve model accuracy close to that of existing popular schemes even when 40% of the nodes are lazy, providing an excellent defense against malicious nodes.},
  archive      = {J_APIN},
  author       = {Niu, Haiqian and Zhang, Xing and Chu, Zhiguang and Shi, Wei},
  doi          = {10.1007/s10489-025-06400-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {DSFL: A blockchain-based data sharing and federated learning framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROCIP: Robust continuous inertial position tracking for
complex actions emerging from the interaction of human actors and
environment. <em>APIN</em>, <em>55</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s10489-025-06409-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inertial navigation is advancing rapidly due to improvements in sensor technology and tracking algorithms, with consumer-grade inertial measurement units (IMUs) becoming increasingly compact and affordable. Despite progress in pedestrian dead reckoning (PDR), IMU-based positional tracking still faces significant noise and bias issues. While traditional model-based methods and recent machine learning approaches have been employed to reduce signal drift, error accumulation remains a barrier to long-term system performance. Inertial tracking’s self-contained nature offers broad applicability but limits integration with a global reference frame. To solve this problem, a system that could “introspect its error” and “learn from the past” is proposed. It consists of a neural statistical motion model that regresses both poses and uncertainties with DenseNet, which are then fed into Rao-Blackwellised particle filter (RBPF) for calibration with a probabilistic transition map. An inertial tracking dataset with head-mounted IMUs was collected, including walking and running with different speeds while allowing participants to rotate their heads in a self-selected manner. The dataset consisted of 19 volunteers that generated 151 sequences in 4 scenarios with a total time of 929.8 min. It was shown that our proposed method (ROCIP) outperformed the leading methods in the field, with a relative trajectory error (RTE) of 4.94m and absolute trajectory error (ATE) of 4.36m. ROCIP could also solve the problem of error accumulation in dead reckoning and maintain a small and consistent error during long-term tracking.},
  archive      = {J_APIN},
  author       = {Hou, Xinyu and Bergmann, Jeroen},
  doi          = {10.1007/s10489-025-06409-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Appl. Intell.},
  title        = {ROCIP: Robust continuous inertial position tracking for complex actions emerging from the interaction of human actors and environment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A statistical categorization-based curriculum learning
approach for multi-task classification of images. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06270-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification and the detection of features within images remain significant challenges in computer vision. Several approaches, including serial task models and multi-output models, have been explored to address these challenges. This study focuses on multitasking attention mechanisms, which enable simultaneous categorization of data and tasks. By applying a statistical framework, the proposed method enhances the efficiency and accuracy of image classification and feature detection, with a focus on handling multiple tasks concurrently. To enhance the robustness of the model, a data-driven approach based on curriculum learning was proposed. The experiments were conducted using two distinct datasets. The first dataset involves forensic examinations, specifically identifying firearms and their calibers from firing pin marks. The proposed model achieved an accuracy of 95% in brand detection and 98% in caliber detection on this dataset. In the second part of the experiments, the animals with attributes 2 (AwA2) dataset, where state-of-the-art models have previously been applied, was used. The proposed model reduced classification errors by 1 to 10% compared to traditional convolutional neural network (CNN) architectures. The experimental results from both the forensic and public datasets demonstrate that the proposed model effectively handles multitask classification tasks, validating its applicability across diverse domains.},
  archive      = {J_APIN},
  author       = {Veranyurt, Ozan and Sakar, C. Okan},
  doi          = {10.1007/s10489-025-06270-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A statistical categorization-based curriculum learning approach for multi-task classification of images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial network embedding with bootstrapped
representations for sparse networks. <em>APIN</em>, <em>55</em>(6),
1–22. (<a href="https://doi.org/10.1007/s10489-025-06343-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent sparsity of real-world networks presents challenges in learning-rich embeddings and accurately reconstructing networks. To address these challenges, a novel method termed Adversarial Network Embedding with Bootstrapped Representations (ANEBR) is proposed. Firstly, a novel network augmentation method is employed for positive sampling. ANEBR utilizes the Katz Index to extract higher-order latent information and refines it with $$\alpha $$ -entmax. The crucial information is extracted while minimizing noise generation. Secondly, ANEBR circumvents negative sampling by learning bootstrapped representations. Building on bootstrapped representations from the BYOL algorithm, ANEBR incorporates the GAN techniques to align the learned embeddings nonlinearly. Finally, ANEBR attains accurate network reconstruction by imposing a low-rank constraint on the reconstruction error through the nuclear norm. Extensive experiments with statistical and sensitivity analyses demonstrate that ANEBR outperforms state-of-the-art methods in various tasks. Specifically, ANEBR reconstructs the PPI network with a precision of 0.9992, marking a relative improvement of 6.65%. Code is available at https://github.com/wuzelong/ANEBR .},
  archive      = {J_APIN},
  author       = {Wu, Zelong and Wang, Yidan and Lin, Guoliang and Liu, Junlong},
  doi          = {10.1007/s10489-025-06343-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Adversarial network embedding with bootstrapped representations for sparse networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust hierarchical clustering algorithm for automatic
identification of clusters. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06376-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregation-based hierarchical clustering algorithms are widely used in data analysis due to their robust clustering performance. Although some existing hierarchical clustering methods can identify the number of clusters in a dataset, most are only effective for well-separated clusters and struggle to identify the number of clusters in complex datasets, particularly non-convex noisy datasets. To address these shortcomings, this paper proposes a robust hierarchical clustering algorithm for automatic identification of clusters(RHCAIC), which can identify the optimal number of clusters while providing reliable clustering results. To reduce the impact of noise in clustering, the method first calculates reverse density and designs a dynamic noise discriminator to denoise the dataset. Based on the fact that more similar points have a higher probability of being clustered into the same cluster among multiple results of hierarchical clustering, a robust solution was designed. After constructing a directed graph using the kNN algorithm, the graph merging process is performed by iteratively traversing the directed edges. During this process, the number of clusters is identified, and the clustering results of the denoised dataset are obtained. Finally, by incorporating density information into the noise clustering, the final clustering results are obtained. A series of experiments conducted on 12 synthetic datasets and 8 real datasets demonstrate that, compared to seven other benchmark algorithms, the RHCAIC algorithm not only accurately identifies the number of clusters in the dataset but also produces better clustering results.},
  archive      = {J_APIN},
  author       = {Long, Jianwu and Wang, Qiang and Liu, Luping},
  doi          = {10.1007/s10489-025-06376-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A robust hierarchical clustering algorithm for automatic identification of clusters},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating potential causes of sepsis with bayesian
network structure learning. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a life-threatening and serious global health issue. This study combines knowledge with available hospital data to investigate the potential causes of Sepsis that can be affected by policy decisions. We investigate the underlying causal structure of this problem by combining clinical expertise with score-based, constraint-based, and hybrid structure learning algorithms. A novel approach to model averaging and knowledge-based constraints was implemented to arrive at a consensus structure for causal inference. The structure learning process highlighted the importance of exploring data-driven approaches alongside clinical expertise. This includes discovering unexpected, although reasonable, relationships from a clinical perspective. Hypothetical interventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and Diabetes suggest that the presence of any of these risk factors in patients increases the likelihood of Sepsis. This finding, alongside measuring the effect of these risk factors on Sepsis, has potential policy implications. Recognising the importance of prediction in improving health outcomes related to Sepsis, the model is also assessed in its ability to predict Sepsis by evaluating accuracy, sensitivity, and specificity. These three indicators all had results around 70%, and the AUC was 80%, which means the causal structure of the model is reasonably accurate given that the models were trained on data available for commissioning purposes only.},
  archive      = {J_APIN},
  author       = {Petrungaro, Bruno and Kitson, Neville K. and Constantinou, Anthony C.},
  doi          = {10.1007/s10489-025-06405-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Investigating potential causes of sepsis with bayesian network structure learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-channel graph-level anomaly detection method based on
multi-graph representation learning. <em>APIN</em>, <em>55</em>(6),
1–16. (<a href="https://doi.org/10.1007/s10489-024-05852-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level anomaly detection plays a crucial role in anomaly identification by comparing and classifying the graph-level features of normal and anomalous graphs. Despite advancements, existing methods often suffer from low detection rates and high false-positive rates when dealing with sparse anomalous data. To address this limitation, we propose a dual-channel graph-level anomaly detection model that utilizes two graph isomorphic networks to separately learn from labeled anomalous data and unlabeled normal data. This model enhances the identification of unlabeled anomalies by learning from both types of data through separate channels. Furthermore, to enable the model to be applicable to complex graph types in graph-level anomaly detection applications, we introduce a novel multi-graph representation learning method that can transform multi-graphs into a simplified graph representation. We have rigorously evaluated the proposed model on 6 public datasets, and the experimental results demonstrate the effectiveness of the model, with significant performance improvements over 9 baseline models.},
  archive      = {J_APIN},
  author       = {Jing, Yongjun and Wang, Hao and Chen, Jiale and Chen, Xu},
  doi          = {10.1007/s10489-024-05852-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Dual-channel graph-level anomaly detection method based on multi-graph representation learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D-FaIR: 3D facial imperfection regeneration with defects by
fully convolutional mesh autoencoder. <em>APIN</em>, <em>55</em>(6),
1–15. (<a href="https://doi.org/10.1007/s10489-024-05880-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an effective approach using a fully convolutional mesh autoencoder model to reconstruct 3D facial features in the presence of imperfections. The method accurately simulates facial scars in a virtual environment, adapting to unique situations. This article presents the “Cir3D-FaIR” dataset, which is specifically tailored to address issues related to facial scars. Additionally, we propose a new technique called 3D facial imperfection regeneration (3D-FaIR), which focusses on reconstructing a complete face based on the remaining features of the patient’s face. To further enhance the applicability of this research, the article has developed an advanced outlier detection technique that isolates affected areas and provides appropriate models for wound coverage. The Cir3D-FaIR dataset, consisting of imperfect facial models and open-source package, is available at https://github.com/SIMOGroup/3DFaIR . Our findings demonstrate that the proposed approach can potentially aid in faster and safer patient recovery through convenient methods. We hope that this work inspires the development of new products and innovative solutions for facial scar regeneration.},
  archive      = {J_APIN},
  author       = {Nguyen, Phuong D. and Le, Thinh D. and Nguyen, Duong Q. and Nguyen, Thanh Q. and Chou, Li-Wei and Nguyen-Xuan, H.},
  doi          = {10.1007/s10489-024-05880-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {3D-FaIR: 3D facial imperfection regeneration with defects by fully convolutional mesh autoencoder},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online self-organizing radial basis function neural
network based on gaussian membership. <em>APIN</em>, <em>55</em>(6),
1–17. (<a href="https://doi.org/10.1007/s10489-024-05989-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial basis function neural network (RBFNN) is one of the most popular neural networks, and an appropriate selection of its structure and learning algorithms is crucial for its performance. Aiming to alleviate the sensitivity of the RBFNN to its parameters and improve the overall performance of the network, this study proposes a Gaussian Membership-based online self-organizing RBF neural network (GM-OSRBFNN). First, the Gaussian Membership is introduced to enhance network insensitivity to network parameters and used as a similarity metric to indicate the similarity between the sample to a hidden neuron and that between hidden neurons. Second, the similarity metric is used to design the neuron addition and merging rules to achieve a self-organizing network structure, and error constraints are introduced to the neuron addition rule; also, the noisy neuron deletion rule is defined to make the network structure more compact. In addition, an online fixed mini-batch gradient algorithm is used for online learning of network parameters, which can guarantee fast and stable convergence of the network. Finally, the proposed GM-OSRBFNN is tested on common nonlinear system modeling problems to verify its effectiveness. The experimental results show that compared to the existing models, the GM-OSRBFNN can achieve competitive prediction performance with a more compact network structure, faster convergence speed, and, more importantly, better insensitivity to network parameters.},
  archive      = {J_APIN},
  author       = {Jia, Lijie and Li, Wenjing and Qiao, Junfei and Zhang, Xinliang},
  doi          = {10.1007/s10489-024-05989-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {An online self-organizing radial basis function neural network based on gaussian membership},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel recurrent neural network with transformer for
anomalous trajectory detection. <em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-024-06069-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalous trajectory detection within urban road traffic networks is crucial for identifying operational vehicle fraud in intelligent transportation systems. However, most existing approaches are limited to detecting anomalous trajectories solely based on the same original point, neglecting the extraction of spatiotemporal features and contextual information embedded in trajectory data. To address these limitations, a Parallel Recurrent Neural Network with Transformer (PRNNT) model is proposed for anomalous trajectory detection. Specifically, the position embedding and a transformer encoder module are utilized to train trajectory embeddings, allowing the model to learn sequential features and contextual information of trajectories. Moreover, a parallel recurrent neural network is employed to extract hidden trajectory features, capturing the differences between normal and anomalous trajectories. Finally, a linear layer is applied to fuse the spatiotemporal features and output the probability of an anomalous trajectory, enhancing the detection of vehicle trajectory anomalies. Experimental results on Beijing and Porto datasets demonstrate that the proposed PRNNT model significantly outperforms the iBAT (Isolation-Based Anomalous Trajectory), ATDC (Anomalous Trajectory Detection and Classification), ATD-RNN (Anomalous Trajectory Detection using Recurrent Neural Network), XGBoost (Extreme Gradient Boosting), GM-VSAE (Gaussian Mixture Variational Sequence AutoEncoder), and UA-OATD (Deep Unified Attention-based Sequence Modeling for Online Anomalous Trajectory Detection) models, achieving at least a 3.8%, 22.7%, 3.8%, 22.7%, 15%, and 16.7% improvement in F1-score, respectively.},
  archive      = {J_APIN},
  author       = {Xia, Dawen and Li, Yunsong and Ao, Yuce and Wei, Xiaoduo and Chen, Yan and Hu, Yang and Li, Yantao and Li, Huaqing},
  doi          = {10.1007/s10489-024-06069-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Parallel recurrent neural network with transformer for anomalous trajectory detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Implicit regularization of a deep augmented
neural network model for human motion prediction. <em>APIN</em>,
<em>55</em>(6), 1. (<a
href="https://doi.org/10.1007/s10489-024-06148-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  author       = {Yadav, Gaurav Kumar and Abdel-Nasser, Mohamed and Rashwan, Hatem A. and Puig, Domenec and Nandi, G. C.},
  doi          = {10.1007/s10489-024-06148-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Implicit regularization of a deep augmented neural network model for human motion prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based spatial-temporal synchronous graph
convolution networks for traffic flow forecasting. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06341-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow forecasting is crucial for urban traffic control, planning, and detection. Most existing spatial-temporal modeling methods overlook the hidden dynamic correlations between road network nodes and the time series nonstationarity while synchronously capturing complex long- and short-term spatial-temporal dependencies. To this end, this paper proposes an Attention-based Spatial-Temporal Synchronous Graph Convolutional Network (AST-SGCN) to capture complex spatial-temporal correlations over long and short terms. Specifically, we design a self-attention mechanism that utilizes spatial-temporal synchronous computation to efficiently mine dynamic spatial-temporal correlations with changes in traffic and enhance computational efficiency. Then, we construct a residual adaptive adjacency matrix, which includes historical data and node vectors, to stimulate the information transfer of spatial-temporal graph nodes and mine the hidden spatial-temporal dependencies through the graph convolution layer. Next, we establish a Fourier transform layer (FTL) to handle the nonstationary data. Finally, we develop a spatial-temporal hybrid stacking module for capturing complex long-term spatial-temporal correlations, within which two layers of graph convolution and one layer of self-attention are deployed. Extensive experimental results on three real-world traffic flow datasets demonstrate that our AST-SGCN model outperforms the comparable models.},
  archive      = {J_APIN},
  author       = {Wei, Xiaoduo and Xia, Dawen and Li, Yunsong and Ao, Yuce and Chen, Yan and Hu, Yang and Li, Yantao and Li, Huaqing},
  doi          = {10.1007/s10489-025-06341-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Attention-based spatial-temporal synchronous graph convolution networks for traffic flow forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARTS-EAST: An edge-adaptive selection with topology first
differentiable architecture selection method. <em>APIN</em>,
<em>55</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s10489-025-06353-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DARTS+PT is a well-known differentiable neural architecture search (NAS) method that evaluates the contribution of operations to the performance of the super-network, ultimately deriving the final architecture. However, DARTS+PT introduces randomness into the edge discretization process by selecting edges randomly, which leads to performance instability. Moreover, the method assesses the impact of each candidate operation by iteratively removing them and measuring the resulting drop in super-network performance, leading to a high search cost. To address these issues, this paper identifies the root cause of instability and proposes a novel edge selection criterion to establish an adaptive edge discretization order, improving stability. Additionally, we introduce a topology-first discretization scheme that prioritizes topology selection over operation selection, significantly reducing the search cost. We name this approach DARTS-EAST (Edge-Adaptive Selection with Topology-First Differentiable Architecture Selection). Extensive experiments on widely used benchmarks demonstrate that DARTS-EAST not only achieves competitive performance but also offers significant improvements in both stability and search efficiency.},
  archive      = {J_APIN},
  author       = {Fang, Xuwei and Xie, Weisheng and Li, Hui and Zhou, Wenbin and Hang, Chen and Gao, Xiangxiang},
  doi          = {10.1007/s10489-025-06353-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {DARTS-EAST: An edge-adaptive selection with topology first differentiable architecture selection method},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TANGAN: Solving tangram puzzles using generative adversarial
network. <em>APIN</em>, <em>55</em>(6), 1–27. (<a
href="https://doi.org/10.1007/s10489-025-06364-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While humans show remarkable proficiency in solving visual puzzles, machines often fall short due to the complex combinatorial nature of such tasks. Consequently, there is a growing interest in developing computational methods for the automatic solution of different puzzles, especially through deep learning approaches. The Tangram, an ancient Chinese puzzle, challenges players to arrange seven polygonal pieces to construct different patterns. Despite its apparent simplicity, solving the Tangram is considered an NP-complete problem, being a challenge even for the most sophisticated algorithms. Moreover, ensuring the generality and adaptability of machine learning models across different Tangram arrangements and complexities is an ongoing research problem. In this paper, we introduce a generative model specifically designed to solve the Tangram. Our model competes favorably with previous methods regarding accuracy while delivering fast inferences. It incorporates a novel loss function that integrates pixel-based information with geometric features, promoting a deeper understanding of the spatial relationships between pieces. Unlike previous approaches, our model takes advantage of the geometric properties of the Tangram to formulate a solving strategy, exploiting its inherent properties only through exposure to training data rather than through direct instruction. Extending the proposed loss function, we present a novel evaluation metric as a better fitting measure for assessing Tangram solutions than previous metrics. We further provide a new dataset containing more samples than others reported in the literature. Our findings highlight the potential of deep learning approaches in geometric problem domains.},
  archive      = {J_APIN},
  author       = {Yamada, Fernanda Miyuki and Batagelo, Harlen Costa and Gois, João Paulo and Takahashi, Hiroki},
  doi          = {10.1007/s10489-025-06364-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Appl. Intell.},
  title        = {TANGAN: Solving tangram puzzles using generative adversarial network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Batch process quality prediction based on denoising
autoencoder-spatial temporal convolutional attention mechanism fusion
network. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06368-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In batch processes, the accurate prediction of quality variables plays a crucial role in smooth production and quality control. However, various sources of noise in the production environment cause abnormal data fluctuations that deviate from the real value. Coupled with the dynamic nonlinearity of batch processing and the complex spatiotemporal relationship of variables, which greatly increase the difficulty of prediction and pose a severe challenge to prediction performance. Therefore, a denoising autoencoder-Spatial Temporal Convolution Attention Fusion Network (DAE-STCAFN) prediction method is proposed. Firstly, combining DAE and maximum information coefficient (MIC), multi-level data features are extracted to prepare high-quality input data for the quality prediction model. DAE is used to denoise the original data, and relevant variables are selected through MIC. Then, an augmented matrix is constructed to eliminate the autocorrelation of the selected variables in the time series. Secondly, a spatial temporal convolutional attention fusion mechanism is created to extract the spatial temporal fusion features between the input and output variable sequences. Thirdly, to further enhance the learning ability of the model, a batch attention module is constructed to automatically learn the relationship among sample in small batch. Finally, experiments were carried out on the simulation platform of penicillin fermentation and hot tandem rolling process. In the prediction process of penicillin concentration, RMSE and MAE of the proposed method were 0.0099 and 0.0077, respectively. In the prediction of strip thickness, the RMSE and MAE are 0.0008 and 0.0003 respectively. The results show that the proposed method is effective both in simulation experiment and in actual industrial production in terms of prediction accuracy, stability and generalization ability.},
  archive      = {J_APIN},
  author       = {Zhang, Yan and Cao, Jie and Zhao, Xiaoqiang and Hui, Yongyong},
  doi          = {10.1007/s10489-025-06368-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Batch process quality prediction based on denoising autoencoder-spatial temporal convolutional attention mechanism fusion network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-order consensus graph learning for incomplete
multi-view clustering. <em>APIN</em>, <em>55</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10489-025-06375-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering (IMVC) aims to partition data with missing samples into distinct groups. However, most IMVC methods rarely consider the high-order neighborhood information of samples, which represents complex underlying interactions, and often neglect the weights of different views. To address these issues, we propose a High-order Consensus Graph Learning (HoCGL) model. Specifically, we integrate a reconstruction term to recover the incomplete multi-view data. High-order proximity matrices are constructed, and the self-representation similarity matrices and multiple high-order proximity matrices are learned mutually, allowing the similarity matrices to incorporate complex high-order information. Finally, the consensus graph representation is derived from the similarity matrices through a self-weighted strategy. An efficient algorithm is designed to solve the proposed model. The excellent clustering performance of the proposed model is validated by comparing it with eight state-of-the-art models across nine datasets.},
  archive      = {J_APIN},
  author       = {Guo, Wei and Che, Hangjun and Leung, Man-Fai},
  doi          = {10.1007/s10489-025-06375-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {High-order consensus graph learning for incomplete multi-view clustering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StackMFF: End-to-end multi-focus image stack fusion network.
<em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06383-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing end-to-end multi-focus image fusion (MFF) networks demonstrate excellent performance when fusing image pairs. However, when image stacks are processed, the necessity for iterative fusion leads to error accumulation, resulting in various types and degrees of image degradation, which ultimately limits the algorithms’ practical applications. To address this challenge and expand the application scenarios of multi-focus fusion algorithms, we propose a relatively simple yet effective approach: utilizing 3D convolutional neural networks to directly model and fuse entire multi-focus image stacks in an end-to-end manner. To obtain large-scale training data, we developed a refocusing pipeline based on monocular depth estimation technology that can synthesize a multi-focus image stack from any all-in-focus image. Furthermore, we extended the attention mechanisms commonly used in image pair fusion networks from two dimensions to three dimensions and proposed a comprehensive loss function group, effectively enhancing the fusion quality. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance in both fusion quality and processing speed while avoiding image degradation issues, establishing a simple yet powerful baseline for the multi-focus image stack fusion task. The codes are available at https://github.com/Xinzhe99/StackMFF .},
  archive      = {J_APIN},
  author       = {Xie, Xinzhe and Qingyan, Jiang and Chen, Dong and Guo, Buyu and Li, Peiliang and Zhou, Sangjun},
  doi          = {10.1007/s10489-025-06383-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {StackMFF: End-to-end multi-focus image stack fusion network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructive sample partition-based parameter-free sampling
for class-overlapped imbalanced data classification. <em>APIN</em>,
<em>55</em>(6), 1–29. (<a
href="https://doi.org/10.1007/s10489-025-06385-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data widely exists in real applications ranging from medical diagnosis to economic fraud detection, etc. Data level method is one of the prevalent methods to deal with imbalanced data by re-balancing the distribution between different classes. Recent researches reveal that handling the class-overlapping of imbalanced data when designing data-level approach can effectively improve the performance of imbalanced learning. However, most existing data-level methods rely on specific parameters to obtain desired performance, making them hard to generalize to other scenarios. And the intractable data difficulty factors, i.e., the most frequent class-overlapping problem, makes them confront additional challenges. Designing efficient, flexible method that considers the parameter-free designing and the class-overlapping handling simultaneously remains a challenge. This paper proposes to deal with the class-overlapped imbalanced data with parameter-free adaptive method. To be specific, we first propose a parameter-free constructive sample partition (CSP) method, and then design an adaptive parameter-free CSP-based undersampling method (CSPUS) and an adaptive parameter-free CSP-based hybrid sampling method (CSPHS) to balance the class distribution by handling the class-overlap of the original data. Numerical experiments on 18 representative high-overlap imbalanced datasets from KEEL repository and 23 state-of-the-art comparison methods demonstrate the effectiveness of CSPUS and CSPHS. The source code of our proposed methods is available at https://github.com/ytyancp/CSPS.},
  archive      = {J_APIN},
  author       = {Wang, Weiqing and Yan, Yuanting and Zhou, Peng and Zhao, Shu and Zhang, Yiwen},
  doi          = {10.1007/s10489-025-06385-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Appl. Intell.},
  title        = {Constructive sample partition-based parameter-free sampling for class-overlapped imbalanced data classification},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of traditional machine learning algorithms for
featuring educational exercises. <em>APIN</em>, <em>55</em>(6), 1–25.
(<a href="https://doi.org/10.1007/s10489-025-06386-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) algorithms are important in educational environments, and the use of machine learning algorithms to evaluate and improve the quality of education. Previous studies have individually analyzed algorithms to estimate item characteristics, such as grade, number of attempts, and time from student interactions. By contrast, this study integrated all three characteristics to discern the relationships between attempts, time, and performance in educational exercises. We analyzed 15 educational assessments using different machine learning algorithms, specifically 12 for regression and eight for classification, with different hyperparameters. This study used real student interaction data from Zenodo.org, encompassing over 150 interactions per exercise, to predict grades and to improve our understanding of student performance. The results show that, in regression, the Bayesian ridge regression and random forest regression algorithms obtained the best results, and for the classification algorithms, Random Forest and Nearest Neighbors stood out. Most exercises in both scenarios involved more than 150 student interactions. Furthermore, the absence of a pattern in the variables contributes to suboptimal outcomes in some exercises. The information provided makes it more efficient to enhance the design of educational exercises.},
  archive      = {J_APIN},
  author       = {Jiménez-Macías, Alberto and Muñoz-Merino, Pedro J. and Moreno-Marcos, Pedro Manuel and Kloos, Carlos Delgado},
  doi          = {10.1007/s10489-025-06386-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {Evaluation of traditional machine learning algorithms for featuring educational exercises},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of reservoir water levels via an improved
attention mechanism based on CNN − LSTM. <em>APIN</em>, <em>55</em>(6),
1–20. (<a href="https://doi.org/10.1007/s10489-025-06393-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water level prediction is crucial for flood control scheduling and water resource management. The application of various deep learning methods to water level prediction in reservoirs is limited. Accurate water level prediction aids in optimizing reservoir operation strategies, ensuring flood safety downstream and meeting water supply demands. To achieve accurate predictions, a new structure based on a convolutional neural network − long short-term memory (CNN − LSTM) model is proposed, which incorporates a self-attention mechanism and a local attention mechanism in an SL − CNN − LSTM coupled model. Using the Three Gorges Reservoir head area in China as a case study, hydrometeorological data from three points in the reservoir&#39;s head area and upstream water level characteristics are used as input variables. Data collected every six hours from 2008 to 2021 were used, with the model trained and tested at an 8:2 ratio. The study revealed that a two-layer CNN configuration performed best in most models. The SL − CNN − LSTM-2 model achieved the best performance across all the metrics, with an R2 of 0.9988, an MAE of 0.2767, an RMSE of 0.3404, and a MAPE of 0.1717, particularly for extreme water level predictions with minimal residuals, validating its strong ability to balance long- and short-term dependencies. Additionally, the model effectively extracts features and captures critical information in time series data, balancing learning capacity and computational efficiency. The research results are highly important for water resource management in large reservoirs, providing reliable technical support for flood control scheduling and water resource optimization.},
  archive      = {J_APIN},
  author       = {Li, Haoran and Zhang, Lili and Yao, Yunsheng and Zhang, Yaowen},
  doi          = {10.1007/s10489-025-06393-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Prediction of reservoir water levels via an improved attention mechanism based on CNN − LSTM},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A personalized consensus-reaching method for large-group
decision-making in social networks combining self-confidence and trust
relationships. <em>APIN</em>, <em>55</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06395-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large-scale group decision-making (LSGDM) in social network environments considering experts’ psychological behaviors has received increasing attention. Moreover, existing studies have shown that whether it is internal self-confidence or external trust relationships of experts, they play a crucial role in reaching consensus. Therefore, this paper integrates self-confidence and trust relationships, and proposes a personalized consensus-reaching method for LSGDM from the perspective of adjustment willingness. Firstly, we explored the promoting effect of opinion similarity on the efficiency of trust propagation and proposed a method to evaluate unknown trust relationships among experts, integrating the objectivity of trust relationships and the subjectivity of self-confidence to determine the experts’ weights. Secondly, a hierarchical fuzzy clustering algorithm based on the chi-square test is proposed for effective subgroup division, which avoids the impact of setting initial clustering parameters on the clustering results. Afterwards, the adjustment willingness of the subgroups is determined by combining the experts’ self-confidence and the trust relationships between them. In addition to this, a personalized consensus feedback adjustment mechanism that synthesizes the adjustment willingness and trust relationship is constructed to reach consensus, which can better preserve the original information. Finally, the effectiveness of the proposed method is verified through a numerical example. In addition, the advantages of the proposed method are demonstrated by comparing with other methods.},
  archive      = {J_APIN},
  author       = {Liu, Zhengmin and Ding, Ruxue and Wang, Wenxin and Liu, Peide and Gao, Shanshan},
  doi          = {10.1007/s10489-025-06395-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {A personalized consensus-reaching method for large-group decision-making in social networks combining self-confidence and trust relationships},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pursuit-evasion game with online planning using deep
reinforcement learning. <em>APIN</em>, <em>55</em>(6), 1–17. (<a
href="https://doi.org/10.1007/s10489-025-06396-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pursuit-evasion with round-up is a problem where multiple pursuers aim to capture a moving target within a specific encirclement to prevent its escape. In this paper, multi-UAV pursuit-evasion algorithms in obstacle environments are investigated and deployed on real-world micro quadcopters. In order to guide pursuers in quickly avoiding obstacles and swiftly approaching the evader, an end-to-end distributed reinforcement learning framework is proposed, and a two-stage reward function is designed. Building upon this, a MADDPG framework based on a trajectory prediction network is constructed to assist pursuers in completing round-up more quickly. Furthermore, unlike most reinforcement learning algorithms, the proposed algorithm is deployed onto a micro quadcopter controller, and a pursuit-evasion game is conducted in a real-world scenario. The results of simulations and physical experiments show that the proposed algorithm can complete round-up more quickly and can be successfully transferred to the real world.},
  archive      = {J_APIN},
  author       = {Chen, Yong and Shi, Yu and Dai, Xunhua and Meng, Qing and Yu, Tao},
  doi          = {10.1007/s10489-025-06396-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {Pursuit-evasion game with online planning using deep reinforcement learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The improved mountain gazelle optimizer for spatiotemporal
support vector regression: A novel method for railway subgrade
settlement prediction integrating multi-source information.
<em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06397-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uneven settlement of railway subgrades not only affects the comfort of train operations but, in extreme cases, may compromise operational safety. As a result, accurately predicting subgrade settlement is crucial for maintaining both safety and operational efficiency. This study introduces an Improved Mountain Gazelle Optimizer for the Spatiotemporal Support Vector Regression (IMGO-STSVR) model, which effectively predicts railway subgrade settlement. Data are collected using Permanent Scatterer Interferometric Synthetic Aperture Radar (PS-InSAR) technology in combination with a multi-source environmental monitoring system. The proposed improvement to the Mountain Gazelle Optimizer (IMGO) enhances the model’s optimization capabilities, while the Support Vector Regression model is improved by the constructed spatiotemporal kernel function (STSVR). Experimental results demonstrate that the IMGO-STSVR model achieves high accuracy and stability across various experimental sites. This method provides valuable insights for predicting subgrade settlement in the railway industry, aiding in the early identification of potential risks, optimizing maintenance strategies, and ensuring the safe and efficient operation of rail transport.},
  archive      = {J_APIN},
  author       = {Chen, Guangwu and Zhao, Shilin and Li, Peng and Wang, Shilin and Zhou, Xin and Potekhin, Vyacheslav},
  doi          = {10.1007/s10489-025-06397-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {The improved mountain gazelle optimizer for spatiotemporal support vector regression: A novel method for railway subgrade settlement prediction integrating multi-source information},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention ensemble mixture: A novel offline reinforcement
learning algorithm for autonomous vehicles. <em>APIN</em>,
<em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06403-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline Reinforcement Learning (RL), which optimizes policies from previously collected datasets, is a promising approach for tackling tasks where direct interaction with the environment is infeasible due to high risk or cost of errors, such as autonomous vehicle (AV) applications. However, offline RL faces a critical challenge: extrapolation errors arising from out-of-distribution (OOD) data. In this paper, we propose Attention Ensemble Mixture (AEM), a novel offline RL algorithm that leverages ensemble learning and an attention mechanism. Ensemble learning enhances the confidence of Q-function predictions, while the attention mechanism evaluates the uncertainty of selected actions. By assigning appropriate attention weights to each Q-head, AEM effectively down-weights OOD actions and up-weights in-distribution actions. We further introduce three key improvements to enhance the robustness and generality of AEM: attention-weighted Bellman backups, KL divergence regularization, and delayed attention updates. Extensive comparative experiments demonstrate that AEM outperforms several state-of-the-art ensemble offline RL algorithms, while ablation studies underscore the significance of the proposed enhancements. In AV tasks, AEM exhibits superior performance compared to other methods, excelling in both offline and online evaluations.},
  archive      = {J_APIN},
  author       = {Han, Xinchen and Afifi, Hossam and Moungla, Hassine and Marot, Michel},
  doi          = {10.1007/s10489-025-06403-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Attention ensemble mixture: A novel offline reinforcement learning algorithm for autonomous vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning missing instances in intact and projection spaces
for incomplete multi-view unsupervised feature selection. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06406-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view unsupervised feature selection has achieved great success in identifying a subset of prominent features from multi-view data to produce compact and meaningful representations. However, most existing methods assume that all data views are complete, which is often not the case in real-world scenarios. Multi-view data is frequently incomplete, with some instances missing in certain views. To address this issue, we propose an incomplete multi-view unsupervised feature selection model based on multiple space learning, termed Learning Missing Instances in Intact and Projection Spaces for Incomplete Multi-view Unsupervised Feature Selection (LIPS). This model integrates intact latent space learning, projection space learning, missing instance imputation, and correlation structure learning into a joint framework. Specifically, LIPS employs intact latent space learning to generate intact representations that capture the full information of multi-view data. Using these representations, LIPS calculates correlations between data through a constrained self-expression strategy, generating a sparse correlation matrix where each row contains few non-zero entries, signifying that each data point can be linearly reconstructed using only a small subset of related neighbors. Subsequently, LIPS projects data into low-dimensional spaces to retain the neighborhood correlations. Finally, it leverages complementary information to impute the missing instances from a cross-view perspective based on intact representations and utilizes neighborhood information to generate neighborhood-smooth imputations for missing instances from view-specific perspectives. Additionally, an effective algorithm is developed to resolve the optimization problem. Extensive experiments conducted on six public datasets of different types, including image datasets (MSRC-v1, Caltech101-7, and CIFAR-10), text datasets (BBCSport and WebKB), and a face dataset (Yale), measured by Acc and NMI, demonstrate that the proposed LIPS outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wu, Jian-Sheng and Yu, Hong-Wei and Li, Yanlan and Min, Weidong},
  doi          = {10.1007/s10489-025-06406-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Learning missing instances in intact and projection spaces for incomplete multi-view unsupervised feature selection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-like synthetic sperm video generation from learned
behaviors. <em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06407-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-assisted sperm analysis is an open research problem, and a main challenge is how to test its performance. Deep learning techniques have boosted computer vision tasks to human-level accuracy, when sufficiently large labeled datasets were provided. However, when it comes to sperm (either human or not) there is lack of sufficient large datasets for training and testing deep learning systems. In this paper we propose a solution that provides access to countless fully annotated and realistic synthetic video sequences of sperm. Specifically, we introduce a parametric model of a spermatozoon, which is animated along a video sequence using a denoising diffusion probabilistic model. The resulting videos are then rendered with a photo-realistic appearance via a style transfer procedure using a CycleGAN. We validate our synthetic dataset by training a deep object detection model on it, achieving state-of-the-art performance once validated on real data. Additionally, an evaluation of the generated sequences revealed that the behavior of the synthetically generated spermatozoa closely resembles that of real ones.},
  archive      = {J_APIN},
  author       = {Hernández-García, Sergio and Cuesta-Infante, Alfredo and Makris, Dimitrios and S. Montemayor, Antonio},
  doi          = {10.1007/s10489-025-06407-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Real-like synthetic sperm video generation from learned behaviors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dense image-mask attention-guided transformer network for
jaw lesions classification and segmentation in dental cone-beam computed
tomography images. <em>APIN</em>, <em>55</em>(6), 1–26. (<a
href="https://doi.org/10.1007/s10489-025-06408-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation and classification of jaw lesions from cone-beam computed tomography (CBCT) images are crucial in computer-assisted diagnosis and treatment planning for oral and maxillofacial (OMF) surgery. However, the evolutionary nature of jaw lesions and their morphological diversity pose significant challenges to both segmentation and classification tasks. Although existing deep learning-based works have achieved promising results on segmentation and classification of other types of lesions, they often consider the two tasks separately, thereby overlooking the strong guidance that lesion masks can provide in determining lesion categories. In this manuscript, we propose a dense image-mask attention-guided transformer network for end-to-end jaw lesions classification and segmentation in 3D CBCT images based on a multi-task learning (MTL) architecture. Specifically, we design multi-dimension attention (MDA) and multi-scale attention (MSA) modules to incorporate dense features from different dimensions and scales, explicitly enhancing the guidance of lesion segmentation for classification decisions. Furthermore, to effectively encode long-term contextual information, we employ a transformer as the classification decoder and design a 3D positional embedding method to preserve the 3D positional information of sequential feature inputs for the transformer. Finally, we design a task merge module that employs a per-lesion inference strategy to assign a category to each lesion instance. A large in-house dataset consisting of 358 CBCT scans with five types of jaw lesions is constructed to evaluate the proposed method. The experimental results show a binary segmentation DICE score of 90%, a mean classification accuracy of 89.23%, and a multi-class segmentation DICE score of 79.06%, surpassing many state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Li, Xiang and Liu, Wei and Tang, Wei and Guo, Jixiang},
  doi          = {10.1007/s10489-025-06408-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-26},
  shortjournal = {Appl. Intell.},
  title        = {Dense image-mask attention-guided transformer network for jaw lesions classification and segmentation in dental cone-beam computed tomography images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NTFNet: Narrowing-then-fusing network for RGB-TIR semantic
segmentation. <em>APIN</em>, <em>55</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the task of understanding scenes in visible (RGB) and thermal-infrared (TIR) images has garnered increasing interest in the field of computer vision. However, most existing methods employ simplistic fusion strategies to merge features from different modalities. These strategies often overlook the differences in shallow-level features between modalities, thereby reducing the discriminability of the fused features and resulting in suboptimal segmentation performance. To address this issue, we present a novel RGB-TIR semantic segmentation framework, named NTFNet. This framework exploits the potential consistency of semantic-level features to rectify shallow-level features and reduce discrepancies between modalities prior to integration. Specifically, auxiliary encoders are employed at each layer to capture semantically consistent information. To obtain rich multi-modal semantic features, we designed a High-Level Feature Fusion Module (HFFM) that enhances feature representation in both channel and spatial dimensions. Subsequently, the Shallow Feature Difference Rectification Module (SFDRM) is introduced to rectify the difference in shallow-level features. To address the loss of detailed information during the rectification process, the SFDRM incorporates a Detail Attention Mechanism (DAM) to preserve the original detail information, thereby further optimizing the final segmentation results. In the end, a Multi-Scale Feature Fusion module (Multi-Scale FFM) is designed to combine the rectified features. Comprehensive experiments on two public RGB-TIR datasets show that our method significantly outperforms other state-of-the-art approaches in terms of performance.},
  archive      = {J_APIN},
  author       = {Liu, Yichen and Ye, Junjie and He, Wangpeng and Qu, Zhiqiang and Xu, Ruoxuan},
  doi          = {10.1007/s10489-025-06411-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {NTFNet: Narrowing-then-fusing network for RGB-TIR semantic segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph-based graph attention network for anomaly
detection in industrial multivariate time series data. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For industrial big data, anomaly detection for multivariate time series data is of critical strategic significance. However, the complexity of industrial equipment and production processes, combined with the high dimensionality of production data, makes it challenging for traditional anomaly detection methods to effectively capture the complex interdependencies and dynamic evolutionary relationships among multiple variables. Additionally, issues such as unstable data distributions, variability, and data drift exacerbate the challenges faced by anomaly detection methods in industrial data analysis and decision-making. This study presents a dynamic graph-based graph attention network for anomaly detection in a multivariate time series data (D-GATAD) model, introducing an innovative approach to dynamic graph construction. The proposed method seamlessly integrates node content features with graph topological structure information, enabling adaptive construction of dynamic graphs based on the current sensor network structure. This design allows for precise modeling of the complex temporal dependencies between variables. Furthermore, the method incorporates an optimized prediction-based model design that organically combines embedding vectors with node data, thereby significantly enhancing the interpretability of the analytical results. Experimental evaluations demonstrate that the proposed method outperforms existing state-of-the-art models across multiple public benchmark datasets. Notably, on the highly complex WADI dataset, it achieves a 5.12% improvement in the AUC score, underscoring its robustness and effectiveness in industrial anomaly detection. This research offers an innovative and widely applicable solution for industrial data analysis and anomaly detection, with significant implications for practical deployment.},
  archive      = {J_APIN},
  author       = {Gao, Cong and Ma, Hongye and Pei, Qingqi and Chen, Yanping},
  doi          = {10.1007/s10489-025-06412-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic graph-based graph attention network for anomaly detection in industrial multivariate time series data},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved data-driven model-free adaptive control method for
an upper extremity power-assist exoskeleton. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06415-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of power-assist exoskeletons in physical labor and daily activities has increased the demand for robust control strategies to address challenges in human-exoskeleton interaction. Factors such as collisions and friction introduce uncertain disturbances, making it difficult to establish an accurate human-exoskeleton interaction model, thereby limiting the applicability of current model-based control methods. To overcome these problems, this study proposes an improved data-driven model-free adaptive control method (IMFAC) for the upper extremity power-assist exoskeleton. The stability and convergence of the closed-loop system are rigorously proven. To optimize the initial conditions of IMFAC, we propose an improved snake optimizer (ISO) algorithm incorporating opposition-based learning. The proposed ISO-IMFAC method is evaluated in two scenarios: a nonlinear Hammerstein model benchmark and a physical exoskeleton platform. Experimental results demonstrate that ISO-IMFAC outperforms other popular data-driven control methods across six metrics: integrated absolute error (4.756), mean integral of time-weighted absolute error (0.457), maximum error (1.167), minimum error (0), mean error (0.032), and error standard deviation (0.169). Additionally, the ISO-IMFAC method effectively drives the exoskeleton without relying on its dynamic model. In two load-bearing experiments conducted with five subjects wearing the exoskeleton, the proposed method reduces average muscle exertion per unit time by over 50% and extended working time by more than 180%. These findings highlight the significant potential of the proposed method to enhance user endurance and reduce physical strain, paving the way for practical applications in diverse real-world scenarios. The code is released at https://github.com/Shurun-Wang/ISO-IMFAC .},
  archive      = {J_APIN},
  author       = {Wang, Shurun and Tang, Hao and Ping, Zhaowu and Tan, Qi and Wang, Bin},
  doi          = {10.1007/s10489-025-06415-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Improved data-driven model-free adaptive control method for an upper extremity power-assist exoskeleton},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated echocardiogram image quality assessment with YOLO
and resnet in the left ventricular myocardium of A4C views.
<em>APIN</em>, <em>55</em>(6), 1–31. (<a
href="https://doi.org/10.1007/s10489-025-06419-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image quality of echocardiography is an important factor to affect cardiovascular disease diagnosis. Currently, the deep learning (DL) used in cardiac echocardiogram image quality assess model focus more on evaluating the whole dynamic video, but the outputs revealed less local anatomical details in judging the image quality in heart chambers. This study was aimed to achieve the local part image quality assess, specifically for the five locals in the left ventricle of A4C section for myocardium. The object detection model, YOLOv8 (You Only Look Once), were used to crop five local parts in the left ventricle myocardium of A4C section. Then, the ResNet-18 model was used to evaluate the image quality of each cropped part, that output from score 0 to 3, four quality levels. The YOLOv8 model demonstrated exceptional performance metrics with Precision of 98.77%, Recall of 98.84%, mAP50 of 98.95%, and mAP50-90 of 81.33%. Additionally, the model exhibited an average Inference Time of 215ms per frame. Comparatively, the ResNet-18 model achieved Accuracy scores of 79.34%, 82.41%, 77.82%, 82.33%, and 78.13%, which correspond to the assessment of the left ventricular myocardium in all five local A4C views. The aggregate performance of the ResNet-18 model was characterized by average Macro Precision of 66.77%, Recall of 59.89%, and F1 Score of 59.49%. Furthermore, the model displayed average Micro Precision of 67.42%, Recall of 70.00%, and F1 Score of 69.98%. This study determined the effectiveness of YOLOv8 to find the bounding box of local myocardium and ResNet-18 for real-time automatic quality assessment, and had the potential to improve the efficiency of diagnosis for the doctor using echocardiogram.},
  archive      = {J_APIN},
  author       = {Liu, Weiyang and Wang, Qiushuang and Zhang, Peifang and Deng, Yujiao and Zhao, Yawei and Zhang, Yongming and Xu, Hongli and Qiu, Xiaowan and Chen, Xu and Xu, Jiayu and He, Kunlun},
  doi          = {10.1007/s10489-025-06419-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Appl. Intell.},
  title        = {Automated echocardiogram image quality assessment with YOLO and resnet in the left ventricular myocardium of A4C views},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time AI-driven quality control for laboratory
automation: A novel computer vision solution for the opentrons OT-2
liquid handling robot. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-025-06334-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of robotics and automated solutions in life sciences R&amp;D has accelerated in recent years, driven by the need to process increasing sample volumes, protect laboratory staff from hazardous substances, and manage financial pressures. Various automation systems, each with distinct levels of sample processing, transportation tasks, and data management, are available to meet specific application requirements, with liquid handling robots taking pivotal positions in these systems. However, current liquid handling robots, such as the Opentrons OT-2, lack integrated vision-based quality control, which limits their accuracy and reliability. This study presents an AI-driven computer vision model designed to enhance quality control in laboratory automation. By integrating the YOLOv8 object detection model with the OT-2, our model enables precise detection of pipette tips and liquid volumes, providing real-time feedback on errors, such as missing tips and incorrect liquid levels. Our results demonstrate the model&#39;s effectiveness and accessibility, presenting an affordable solution for improving automation in academic and research laboratories. This closed-loop system transforms the OT-2 into a robust tool for automated laboratory tasks, making it an accessible and cost-effective approach for enhancing quality control in laboratory automation and addressing a critical gap in available tools for resource-limited settings.},
  archive      = {J_APIN},
  author       = {Khan, Sana Ullah and Møller, Vilhelm Krarup and Frandsen, Rasmus John Normand and Mansourvar, Marjan},
  doi          = {10.1007/s10489-025-06334-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Real-time AI-driven quality control for laboratory automation: A novel computer vision solution for the opentrons OT-2 liquid handling robot},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection in crowd scenes via cross trajectories.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06338-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel finite-time braid entropy (FTBE) theorem to extract feature vectors to detect abnormal events occurring globally and locally in crowds. Detecting abnormal events or behavior in crowd movements is a key research topic regarding community security and management. A trajectory- based method depending on the FTBE theorem and the distribution of motion vectors is presented to determine abnormal events. The FTBE theory determines the complexity of the pattern occurring during the movement of the trajectories describing the behavior. In most studies in the literature, the image is divided into equal regions and the solution is produced by separating every behavior into more than one zone. However, this may result in incorrect results. Our study separated the behavior within a certain time interval into location-independent motion clusters. Each cluster indicated a behavior, which was represented by a feature vector derived from the distribution of FTBE and motion vectors. The learning model and fully connected deep neural network were used to detect which cluster was behaving abnormally in the local area. In addition, abnormal events were determined globally by the step braid entropy score (SBES) value calculated for the current scene. The method was tested using the UMN, UCSD and UCF-Crime databases. The experimental results of the method showed an alternative approach to the detection of abnormal behavior.},
  archive      = {J_APIN},
  author       = {Akpulat, Murat and Ekinci, Murat},
  doi          = {10.1007/s10489-025-06338-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Anomaly detection in crowd scenes via cross trajectories},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional neural networks for advanced adhesive joints
application patterns. <em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06340-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adhesive bonding is a widely used joining technique across various industries. Achieving uniform adhesive coverage over the entire surface without the formation of air pockets is crucial for creating strong and durable joints. Simultaneously, it is essential to minimise waste caused by material leakage at the edges. However, generating an optimal adhesive pattern to achieve the desired adhesive distribution after compression remains a challenge, as fluids tend to spread in a circular manner, while industry-relevant target geometries are typically non-circular. This paper investigates the application of Convolutional Neural Networks (CNNs) to optimise adhesive application patterns by utilising a simplified simulation model known as the Partially Filled Gaps Model (PFGM) to generate extensive training data. The CNN is trained to predict fluid distribution outcomes based on initial adhesive application patterns and addresses the inverse problem of determining an optimal application pattern to achieve a desired target distribution after compression. Two training approaches are introduced: a basic inverse model that utilizes a straightforward input–output data exchange, and a more advanced strategy that incorporates a forward model to improve accuracy. The forward model predicts the final distribution, enabling better refinement of the initial application patterns. The results demonstrate that the CNN-based approach is highly effective in generating optimal application patterns for adhesive bonds. Its primary advantage, compared to alternative methods, lies in its ability to achieve precise results within a short computation time. However, a significant drawback is the limited flexibility in accommodating variations in parameters.},
  archive      = {J_APIN},
  author       = {Scholtes, Kiro and Flaig, Florian and Kaufmann, Marvin and Lehne, Frank Guido and Vallée, Till and Fricke, Holger and Müller, Michael},
  doi          = {10.1007/s10489-025-06340-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Convolutional neural networks for advanced adhesive joints application patterns},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffusionLight: A multi-agent reinforcement learning
approach for traffic signal control based on shortcut-diffusion model.
<em>APIN</em>, <em>55</em>(6), 1–25. (<a
href="https://doi.org/10.1007/s10489-025-06359-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous researches have shown that the Reinforcement Learning(RL) is an effective solution to solve large-scale traffic signal control(TSC) problems. However, facing multi-scenario and emergencies, cooperative control of traffic signals at multi-intersection becomes a challenging multi-agent reinforcement learning(MARL) process. In order to solve real-world problems, this paper proposes a MARL algorithm called DiffusionLight, which combines Shortcut-Diffusion Model(SDM) and Soft Actor-Critic(SAC), and a fast Diffusion model to solve the traffic signal cooperative control problem of multi-scenario and multi-intersection. DiffusionLight has the stable characteristics and powerful expression ability of the SDM as the strategy network to solve the action space, while SAC is used as the value network to better explore the solution space. Experimental results show that DiffusionLight exhibits better stability compared to the baseline algorithm in the face of multi-scenario TSC and burst data anomalies, and as well as excellent performance on multiple public datasets of grid and arterial traffic networks.},
  archive      = {J_APIN},
  author       = {Yu, JiLin and Wang, Zhiwen and Zhang, Ruonan},
  doi          = {10.1007/s10489-025-06359-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Appl. Intell.},
  title        = {DiffusionLight: A multi-agent reinforcement learning approach for traffic signal control based on shortcut-diffusion model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TsDa-ASAM: Balancing efficiency and accuracy in coke image
particle size segmentation via two-stage distillation-aware adaptive
segment anything model. <em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06427-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coke image segmentation is a crucial step in coke particle size control of the sintering process. However, due to the complexity of model architecture and the dense distribution of coke particles in the images, existing segmentation methods fail to satisfy the efficiency and accuracy requirements for coke image segmentation in industrial scenarios. To address these challenges, this paper proposes a two-stage distillation-aware adaptive segment anything model to balance efficiency and accuracy in coke image particle size segmentation, referred to as TsDa-ASAM. In the first stage, knowledge distillation methods are employed to distill the Segment Anything Model (SAM) into a lightweight model, explicitly focusing on enhancing segmentation efficiency. In the second stage, a domain knowledge injection strategy is formulated, which incorporates domain knowledge into the distillation model to effectively enhance the accuracy. Moreover, an adaptive prompt point selection algorithm is introduced to address the redundancy issue of prompt points in SAM, improving the efficiency of TsDa-ASAM. The effectiveness of TsDa-ASAM is validated through extensive experiments on the publicly available dataset SA-1B and the coke image dataset from industrial sites. After distillation and fine-tuning, the segmentation accuracy of the proposed model improved by 10%, and the segmentation efficiency of TsDa-ASAM was enhanced by 2 to 3 times with the integration of the adaptive prompt point selection algorithm. The experimental results have effectively demonstrated the potential of the proposed model in balancing accuracy and efficiency.},
  archive      = {J_APIN},
  author       = {Wang, Yalin and Peng, Yubin and Tan, Xujie and Pan, Yuqing and Liu, Chenliang},
  doi          = {10.1007/s10489-025-06427-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {TsDa-ASAM: Balancing efficiency and accuracy in coke image particle size segmentation via two-stage distillation-aware adaptive segment anything model},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Entropy guidance hierarchical rich-scale feature network
for remote sensing image semantic segmentation of high resolution.
<em>APIN</em>, <em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06433-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of high-resolution remote sensing images (HRRSIs) is crucial for a wide range of applications, such as urban planning and disaster management. However, in HRRSIs, existing multiscale feature extraction and fusion methods often fail to achieve the desired accuracy because of the challenges posed by densely distributed small objects and large-scale variations. Therefore, we propose a hierarchical rich-sale feature network with entropy guidance (HRFNet), which introduces an entropy-based weighting and feature mining strategy to enhance feature extraction and fusion. Specifically, image entropy is employed as a quantifiable index to characterize the object distribution within remote sensing images, enabling an adaptive image division strategy. The image entropy is further used as weights during network training to emphasize regions with high entropy, which often correspond to edges and densely populated small objects. Additionally, the proposed feature mining strategy effectively integrates both global and local contextual information across multilayer feature maps. Extensive experiments show that HRFNet achieves mIoU scores of 81.31%, 86.47%, and 51.5% on the Vaihingen, Potsdam, and LoveDA datasets, respectively, outperforming existing methods by 1.0–3.0% mIoU.},
  archive      = {J_APIN},
  author       = {Zhang, Haoxue and Li, Linjuan and Xie, Xinlin and He, Yun and Ren, Jinchang and Xie, Gang},
  doi          = {10.1007/s10489-025-06433-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {Entropy guidance hierarchical rich-scale feature network for remote sensing image semantic segmentation of high resolution},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty weighted policy optimization based on bayesian
approximation. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06303-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient exploration remains a major challenge in the field of reinforcement learning (RL). Bayesian methods have been widely investigated within the RL paradigm and are used to implement intelligent exploration strategies. However, most of these methods inevitably introduce some complexity within the Bayesian neural networks (BNNs) or are difficult to optimize elegantly. In this work, a novel algorithm called uncertainty weighted policy optimization (UWPO) based on Bayesian approximation, is introduced. UWPO theoretically analyzes the uncertainty of the policy space using the Dirichlet distribution and Monte Carlo (MC) dropout for both discrete and continuous spaces, eliminating the need for an explicit distribution representation in BNNs. The algorithm also proposes an implicit distributional training method for the value function, which is compatible with Bayesian inference. Moreover, an uncertainty-weighted update principle is adopted to adaptively adjust the contribution of each training instance to the objective. Finally, comparing UWPO with other prevailing deep reinforcement learning (DRL) algorithms on the Atari, MuJoCo, and Box2D platforms. The experimental results demonstrate that the algorithm improves the average reward score by nearly 15% while reducing computational costs by 20% compared to current state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Li, Tianyi and Yang, Genke and Chu, Jian},
  doi          = {10.1007/s10489-025-06303-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Uncertainty weighted policy optimization based on bayesian approximation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFP: Temporal knowledge graph completion based on
sequence-focus patterns representation learning. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06306-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extrapolation task in the temporal knowledge graph has received increasing attention from scholars due to its wide range of practical application scenarios. At present, recurrent neural networks are currently widely used in temporal knowledge graph completion techniques. These networks are employed to depict the sequential pattern of entities and relations. However, as the sequence lengthens, some critical early information may become diluted. Prediction errors ensue in the completion task as a result. Furthermore, it is observed that existing temporal knowledge graph completion methods fail to account for the topological structure of relations, which leads to relation representations with essentially little distinction across different timestamps. In order to tackle the previously mentioned concern, our research introduces a Temporal Knowledge Graph Completion Method utilizing Sequence-Focus Patterns Representation Learning (SFP). This method contains two patterns: the Focus pattern and the Sequential pattern. In the SFP model, we developed a novel graph attention network called ConvGAT. This network efficiently distinguishes and extracts complex relation information, thereby enhancing the accuracy of entity representations that are aggregated in the Focus pattern and Sequential pattern. Furthermore we proposed RelGAT, a graph attention network that simulates the topological structure of relations. This enhances the precision of relation representations and facilitates the differentiation between relation embeddings generated at various timestamps in the Focus pattern. Utilizing a time-aware attention mechanism, the Focus pattern extracts vital information at particular timestamps in order to amplify the data that the Sequential pattern dilutes. On five distinct benchmark datasets, SFP significantly outperforms the baseline, according to a comprehensive series of experiments.},
  archive      = {J_APIN},
  author       = {Wang, Jingbin and Ke, XiFan and Zhang, FuYuan and Wu, YuWei and Zhang, SiRui and Guo, Kun},
  doi          = {10.1007/s10489-025-06306-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {SFP: Temporal knowledge graph completion based on sequence-focus patterns representation learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-computing, deep reinforcement learning-based predictive
human-robot neuromechanical simulation for wearable robots.
<em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06360-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-robot interaction (HRI) is widely used in robotics to assist humans, with wearable robots enhancing mobility for both able-bodied individuals and those with impairments. Traditionally, characterizing human biomechanical responses to these robots requires extensive human testing, which is time-consuming, costly, and potentially risky. Developing computational HRI simulations for wearable robots offers a promising solution. However, modeling the high-fidelity human-exoskeleton interaction in simulations presents significant challenges that remain underexplored. These include creating a high-fidelity autonomous human motion control agent, accounting for the non-passive nature of human responses, and incorporating closed-loop control within the robotic system. In this paper, we propose an AI-computing, deep reinforcement learning-based HRI simulation to predict complex and realistic human biomechanical responses to exoskeleton assistance. The multi-neural network training process develops an end-to-end, autonomous control policy that reduces human muscle effort by utilizing current human kinematic states. This approach processes state information from both the human musculoskeletal and exoskeleton control neural network, generating control policies for robust human walking movement and reducing muscle effort. Numerical experiments demonstrated the framework’s ability to simulate human motion control, showing reductions in hip joint torque (13.04 $$\%$$ ), rectus femoris (RF) muscle activation (7.31 $$\%$$ ), and biceps femoris (BF) muscle activation (12.21 $$\%$$ ) with exoskeleton use. Validation through real-world experiments further confirmed a decrease in RF and BF muscle activations by 22.12 $$\%$$ and 11.45 $$\%$$ , respectively. These results highlight the effectiveness of our proposed AI computing-based simulation method in replicating and optimizing human biomechanics during exoskeleton-assisted movement. This AI computing-based human-exoskeleton predictive simulation may offer a general, high-fidelity platform for studying human biomechanical responses and enabling autonomous control for assistive devices without requiring intensive human testing in the rehabilitation field.},
  archive      = {J_APIN},
  author       = {Wang, Mingyi and Luo, Shuzhen},
  doi          = {10.1007/s10489-025-06360-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {AI-computing, deep reinforcement learning-based predictive human-robot neuromechanical simulation for wearable robots},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of machine learning methods for cattle
identification and recognition from retinal images. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06398-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animal identification is a critical issue in terms of security, traceability, and animal health, especially in large-scale livestock enterprises. Traditional methods (such as ear tags and branding) both negatively affect animal welfare and may lead to security vulnerabilities. This study aims to develop a biometric system based on retinal vascular patterns for the identification and recognition of cattle. This system aims to provide a safer and animal welfare-friendly alternative by using image processing techniques instead of traditional device-based methods. In the study, preprocessing, segmentation, feature extraction, and performance evaluation steps were applied for the biometric identification and recognition process using retinal images taken from both eyes. Techniques such as green channel extraction, contrast-limited adaptive histogram equalization, morphological operations, noise filtering, and threshold determination were used in the preprocessing stage. Fuzzy C-means, K-means, and Level-set methods were applied for segmentation, and feature extraction was performed using SIFT, SURF, BRISK, FAST, and HARRIS methods. At the end of the study, the highest accuracy rate was obtained as 95.6% for identification and 87.9% for recognition. In addition, the obtained dataset was shared publicly, thus creating a reusable resource that researchers from different disciplines can use. It was concluded that this study made a significant contribution to the field of biometric-based animal identification and recognition and offered a practically usable solution in terms of animal welfare and safety.},
  archive      = {J_APIN},
  author       = {Cihan, Pınar and Saygılı, Ahmet and Akyüzlü, Muhammed and Özmen, Nihat Eren and Ermutlu, Celal Şahin and Aydın, Uğur and Yılmaz, Alican and Aksoy, Özgür},
  doi          = {10.1007/s10489-025-06398-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Performance of machine learning methods for cattle identification and recognition from retinal images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of continuous s-shaped rectified linear function on
deep convolutional neural network. <em>APIN</em>, <em>55</em>(6), 1–24.
(<a href="https://doi.org/10.1007/s10489-025-06399-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vanishing gradient issue in convolutional neural networks (CNNs) is often addressed by improving activation functions, such as the S-shaped rectified linear activation unit (SReLU). However, SReLU can pose challenges in updating training parameters effectively. To mitigate this, we propose applying the Aggregation Fischer–Burmeister (AFB) function to SReLU, which smooths the secant line slope of the function from both sides. However, direct application of AFB to SReLU can intensify the vanishing gradient issue due to irregular function behavior. To address this concern, we introduce a regulated version of AFB (ReAFB) that ensures proper gradient and mean activation output conditions when applied to SReLU (ReAFBSReLU). We evaluate the performance of CNNs using ReAFBSReLU on three benchmark datasets: MNIST, CIFAR-10 (with and without data augmentation), and CIFAR-100. Specifically, we implement Network in Network (NIN) for MNIST and CIFAR-10, and LeNet for CIFAR-100 dataset. Additionally, we utilize SqueezeNet exclusively to compare the performance of CNNs using the proposed ReAFBSReLU activation function against state-of-the-art activation functions. Our results demonstrate that ReAFBSReLU outperforms other activation functions tested in this study, indicating its efficacy in enhancing training parameter updates and subsequently improving accuracy.},
  archive      = {J_APIN},
  author       = {Ghazvini, Anahita and Abdullah, Siti Norul Huda Sheikh and Ayob, Masri},
  doi          = {10.1007/s10489-025-06399-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Effect of continuous S-shaped rectified linear function on deep convolutional neural network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Use of artificial intelligence techniques in
characterization of vibration signals for application in agri-food
engineering. <em>APIN</em>, <em>55</em>(6), 1–24. (<a
href="https://doi.org/10.1007/s10489-025-06424-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bottling machinery is a critical component in agri-food industries, where maintaining operational efficiency is key to ensuring productivity and minimizing economic losses. Early detection of faulty conditions in this equipment can significantly improve maintenance procedures and overall system performance. This research focuses on health monitoring of gripping pliers in bottling plants, a crucial task that has traditionally relied on analyzing raw vibration signals or using narrowly defined, application-specific features. However, these methods often face challenges related to limited robustness, high computational costs, and sensitivity to noise. To address these limitations, we propose a novel approach based on generic features extracted through basic signal processing techniques applied to vibration signals. These features are then classified using a random forest algorithm, enabling an effective analysis of health states. The proposed method is evaluated against traditional approaches and demonstrates clear advantages, including higher accuracy in detecting and classifying faulty conditions, greater robustness against random perturbations, and a reduced computational cost. Additionally, the method requires fewer training instances to achieve reliable performance. This study highlights the potential of artificial intelligence and signal processing techniques in predictive maintenance, offering a scalable and efficient solution for fault detection in manufacturing processes, particularly within the agri-food sector.},
  archive      = {J_APIN},
  author       = {Luque, Amalia and Campos Olivares, Daniel and Mazzoleni, Mirko and Ferramosca, Antonio and Previdi, Fabio and Carrasco, Alejandro},
  doi          = {10.1007/s10489-025-06424-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Appl. Intell.},
  title        = {Use of artificial intelligence techniques in characterization of vibration signals for application in agri-food engineering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison study of several strategies in multivariate
time series clustering based on graph community detection.
<em>APIN</em>, <em>55</em>(6), 1–23. (<a
href="https://doi.org/10.1007/s10489-025-06444-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data analysis, especially forecasting, classification, imputation, and anomaly detection, has gained a lot of research attention in recent years due to its prevalence and wide application. Compared to classification, clustering is an unsupervised task and thus more applicable for analyzing massive time series without labels. One latest way is based on the idea of graph community detection: first transforming a time series set into a graph (or a network), in which a node represents a time series instance and an edge denotes that the two connected nodes (thus the represented time series) are more similar to each other; then, running a community detection algorithm on the graph to discover a community structure, that gives out a clustering result. Recently, there are several works based on the graph community detection idea to cluster multivariate time series. However, such works focus only on specific methods in each step, and a performance comparison of combinations of methods in different steps is lacking. This paper outlines the process of graph-based multivariate time clustering as four phases (referred to as framework), namely representation learning, similarity computing, relation network construction, and clustering, lists typical methods in each phase, and makes a comparison study of combinations of each phase methods (called strategies in this paper). Recent time series deep neural network models are introduced to the framework as time series representation learning methods as well. In addition, $$\varvec{\varepsilon } \varvec{k}$$ NN, an improvement of $$\varvec{k}$$ NN by filtering out unnecessary low similarity connections during network construction, is proposed. A great number of experiments are conducted on eight real-world multivariate time series with various properties to verify the performance of different strategy combinations. The results suggest that proper deep neural network is a promising way for learning time series vector representations to compute similarities, and strategies including $$\varvec{\varepsilon } \varvec{k}$$ NN for network construction, average for multi-layer network merging and Louvain for clustering are more effective from a statistical perspective.},
  archive      = {J_APIN},
  author       = {Sun, Hanlin and Jie, Wei and Chen, Yanping and Wang, Zhongmin},
  doi          = {10.1007/s10489-025-06444-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Appl. Intell.},
  title        = {A comparison study of several strategies in multivariate time series clustering based on graph community detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QuinNet: Quintuple u-shape networks for scale- and
shape-variant lesion segmentation. <em>APIN</em>, <em>55</em>(6), 1–15.
(<a href="https://doi.org/10.1007/s10489-025-06448-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches have demonstrated remarkable efficacy in medical image segmentation. However, they continue to struggle with challenges such as the loss of global context information, inadequate aggregation of multi-scale context, and insufficient attention to lesion regions characterized by diverse shapes and sizes. To address these challenges, we propose a new medical image segmentation network, which consists of one main U-shape network (MU) and four auxiliary U-shape sub-networks (AU), leading to Quintuple U-shape networks in total, thus abbreviated as QuinNet hereafter. MU devises special attention-based blocks to prioritize important regions in the feature map. It also contains a multi-scale interactive aggregation module to aggregate multi-scale contextual information. To maintain global contextual information, AU encoders extract multi-scale features from the input images, then fuse them into feature maps of the same level in MU, while the decoders of AU refine features for the segmentation task and co-supervise the learning process with MU. Overall, the dual supervision of MU and AU is very beneficial for improving the segmentation performance on lesion regions of diverse shapes and sizes. We validate our method on four benchmark datasets, showing that it achieves significantly better segmentation performance than the competitors. Source codes of QuinNet are available at https://github.com/Truman0o0/QuinNet .},
  archive      = {J_APIN},
  author       = {Fan, Gaojuan and Wang, Jie and Xia, Ruixue and Zhou, Funa and Zhang, Chongsheng},
  doi          = {10.1007/s10489-025-06448-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {QuinNet: Quintuple u-shape networks for scale- and shape-variant lesion segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning malware detection model based on
heterogeneous information network path representation. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06417-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant increase of Android malware, the APP privacy data leakage incidents occur frequently, which poses a great threat to user property and information security. Specifically, the new malware has the characteristics of high evolution rate and diverse variants, leading to the fact that the current malware detection methods still have three key problems: (1) Difficulty in acquiring Android sample structural features; (2) Weakly in representing malware behavior structure; (3) Poor robustness of the detection model. To address the above limitations, we propose a new malware detection framework MPRLDroid with reinforcement learning. First of all, the MPRLDroid model extracts the Android APP structural features and constructs the heterogeneous information network data based on the semantic call structure between APP, API and permission. Subsequently, the model utilizes reinforcement learning to adaptively generate a meta-path for each sample and combines it with a graph attention network to effectively represent the graph of nodes. Finally, the low-dimensional graph node vector data is brought into the downstream detection task for classification, where the performance change of the classification result is used as a reward function for reinforcement learning. The experimental results demonstrate that the MPRLDroid model, when integrated with reinforcement learning, outperforms the baseline models in terms of performance, and its detection model exhibits greater robustness compared to other models.},
  archive      = {J_APIN},
  author       = {Yang, Kang and Cai, Lizhi and Wu, Jianhua and Liu, Zhenyu and Zhang, Meng},
  doi          = {10.1007/s10489-025-06417-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {A reinforcement learning malware detection model based on heterogeneous information network path representation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSF-SegFormer: A feature fusion algorithm for magnetic
leakage image segmentation. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06453-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional segmentation networks have low segmentation accuracy for flux leakage images, often leading to missed or false detections of small defects, which significantly affect the evaluation of defect severity. Based on the SegFormer network, a high-accuracy decoder based on multi-scale feature fusion is proposed, which is more suitable for the segmentation of small defects in flux leakage and replaces the multi-layer perceptron (MLP) decoder of the original network. The new network model is called MSF-SegFormer. MSF-SegFormer introduces a feature fusion network MSF that integrates high-resolution and low-resolution features and introduces feature pyramid fusion, which can merge output features at different levels across different scales. A cascaded attention module is proposed, combining two local attention mechanisms in a cascade and using a residual network to enhance the local feature representation of flux leakage images, improving the accuracy and stability of the task. In the application of flux leakage defect data, compared with benchmark models such as CNN and SegFormer, this model can accurately segment target edges with fewer parameters, maintain high accuracy, reduce false detection probability, and improve the Miou value of the traditional MLP decoder from 88.21% to 90.44%.},
  archive      = {J_APIN},
  author       = {Wang, Zhujun and Ni, Rongtai and Sun, Tianhe and Jiang, Yulong and Liu, Bin},
  doi          = {10.1007/s10489-025-06453-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {MSF-SegFormer: A feature fusion algorithm for magnetic leakage image segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FA3-net: Feature aggregation and augmentation with attention
network for sound event localization and detection. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06437-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound event localization and detection (SELD) aims to identify the category and duration of sound events (SED) while also estimating their respective direction of arrival (DOA). This multi-task problem presents unique challenges, as the features required for SED and DOA tasks are not entirely aligned. Consequently, incomplete feature extraction and suboptimal feature fusion often hinder performance. To address these issues, we propose a feature aggregation and augmentation with attention network (FA3-Net). FA3-Net consists of two main components: the feature aggregation and augmentation with attention (FA3) module and the Conformer module. The FA3 module plays a critical role in fusing and enhancing high-level features, which is specifically designed to efficiently handle the distinct requirements of SED and DOA tasks. It ensures that task-specific features are extracted effectively, while also improving feature discriminability and reducing confusion. The feature aggregation residual block (FAResBlock), a component of the FA3 module, handles task-specific feature aggregation, while the feature augmentation with attention block (FAA block) enhances feature representation across multiple dimensions. The Conformer module is employed to model the temporal sequence, as it excels in capturing both local and global dependencies, making it ideal for comprehensive time sequence analysis. Finally, to overcome data limitations, audio channel swapping (ACS) is employed. Experiments on the STARSS23 dataset, DCASE2021 dataset and L3DAS22 dataset show that FA3-Net significantly outperforms other models in both feature aggregation and augmentation, while also being more efficient and lightweight. The code is available in: https://github.com/wangchuan11111111/FA3-NET},
  archive      = {J_APIN},
  author       = {Wang, Chuan and Huang, Qinghua},
  doi          = {10.1007/s10489-025-06437-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {FA3-net: Feature aggregation and augmentation with attention network for sound event localization and detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved DAB-DETR model for irregular traffic obstacles
detection in vision based driving environment perception scenario.
<em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06440-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine vision based irregular traffic obstacles recognition plays a pivotal role in the autonomous driving and Advanced Driver Assistance Systems (ADAS) by providing the necessary environment perception capabilities. Traditional models for recognizing irregular traffic obstacles suffer from challenges with small target detection, poor performance in diverse environmental conditions and computational complexity. This work addresses the critical issue of recognizing irregular traffic obstacles in roadway environments. We present an enhanced target detection model based on the Dynamic Anchor Boxes-recognition Transformer (DAB-DETR). The original model’s structure was limited in expressing relative positional information between features due to the reliance on absolute position encoding. To overcome this limitation, the improved DAB-DETR incorporates relative position encoding within the multi-headed self-attention mechanism of the Transformer encoder. Additionally, we propose a novel Average Precision (AP) loss function that unifies classification and localization losses into a single parameterized formula, addressing performance degradation observed in the original model. Experimental results demonstrate significant improvements in detection accuracy for irregular traffic objects, showcasing the effectiveness of the proposed enhancements. According to the testing results, the improved DAB-DETR model’s detection accuracy is 82.00% with Intersection over Union (IoU) equals to 0.5, which is 3.3% better than the original model and 6.20% and 7.71% better than the conventional models, YOLOv5 and Faster R-CNN, respectively.},
  archive      = {J_APIN},
  author       = {Yang, Junchao and Zhang, Hui and Zhou, Yuting and Guo, Zhiwei and Lin, Feng},
  doi          = {10.1007/s10489-025-06440-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Improved DAB-DETR model for irregular traffic obstacles detection in vision based driving environment perception scenario},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ship pipeline defect detection method based on deep learning
and transfer fusion of ultrasonic guided wave signals. <em>APIN</em>,
<em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-025-06390-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasonic guided waves (UGW) hold great promise for structural health monitoring (SHM) of pipeline structures. However, the inherent complexity of pipeline defect features within the UGW makes the intuitive and accurate identification of defects based only on UGW signals challenging. In addition, the existing neural network-based UGW signal recognition methods require a large number of defect waveform samples, which limits their applicability. This study proposes a signal recognition method based on deep learning and sample transfer fusion for the identification of UGW signals in ship pipelines, allowing to accurately detect their potential defects. A time–frequency imaging algorithm for ship pipeline UGW signals is first introduced using the continuous wavelet transform (CWT) to capture their time–frequency characteristics. Leveraging transfer learning, UGW signal samples from various operational scenarios onshore oil pipelines are then fused to pre-train the GoogLeNet convolutional neural network (CNN) model. Finally, the pre-trained GoogLeNet model is fine-tuned with ship pipeline UGW signal samples, which allows to accurately detect the underlying defects. The experimental results demonstrate that the proposed method significantly increases the classification accuracy of ship pipeline defects compared with non-transfer learning methods and time-domain imaging. More precisely, the accuracy increases from 63.3% to 97.3%. Furthermore, the obtained results show that the proposed method has high robustness.},
  archive      = {J_APIN},
  author       = {Tang, Ruoli and Li, Yongzhe and Zhang, Shangyu},
  doi          = {10.1007/s10489-025-06390-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Ship pipeline defect detection method based on deep learning and transfer fusion of ultrasonic guided wave signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning-based road surveillance system in
distributed CCTV environment: Pedestrian fall recognition using
spatio-temporal attention networks. <em>APIN</em>, <em>55</em>(6), 1–16.
(<a href="https://doi.org/10.1007/s10489-025-06451-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent CCTV systems are highly effective in monitoring pedestrian and vehicular traffic and identifying anomalies in the roadside environment. In particular, it is necessary to develop an effective recognition system to address the problem of pedestrian falls, which is a major cause of injury in road traffic environments. However, the existing systems have challenges such as communication constraints and performance instability. In this paper, we propose a novel fall recognition system based on Federated Learning (FL) to solve these challenges. The proposed system utilizes a GAT combined with LSTM and attention layers to extract spatio-temporal features, which can more accurately identify pedestrian falls. Each road CCTV works as an independent client to generate local data, and the server aggregates these models to learn a global model. This ensures robust operation in different views and environments, and solves the bottleneck of data communication and security challenges. We validated the feasibility and applicability of the FL-based fall recognition method by implementing the prototype and applying it to the UP-FALL benchmark dataset, which is widely used for fall recognition. Code has been made available at: https://github.com/Kim-Byeong-Hun/Fed-PFR .},
  archive      = {J_APIN},
  author       = {Kim, Byeonghun and Im, Jaegyun and Noh, Byeongjoon},
  doi          = {10.1007/s10489-025-06451-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Federated learning-based road surveillance system in distributed CCTV environment: Pedestrian fall recognition using spatio-temporal attention networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional requirements for reinforcement
recommendation reasoning. <em>APIN</em>, <em>55</em>(6), 1–16. (<a
href="https://doi.org/10.1007/s10489-024-05854-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized recommendation systems not only need to improve the accuracy of recommendations, but also need to focus on the variety and novelty of recommendations to improve user satisfaction. Currently, most of the existing recommendation systems focus on improving the accuracy and diversity of recommendation items, however, they usually do not consider the original user needs, and the potential relationship between diversity and novelty is not deeply explored. In addition to accuracy and diversity, we also consider novelty, and analyze the relationship between diversity and novelty (same place and different place), and propose an explainable recommendation system that integrates multiple (multidimensional) requirements such as accuracy, diversity, and novelty. The model combines semantic relations of knowledge graphs and multi-hop inference so as to analyze and consider the diversity and novelty requirements of users. Meanwhile, a recurrent neural network is used to construct a temporal multi-label classification network to predict users’ multidimensional demands and capture the dependencies between diversity and novelty demands. Finally, a composite reward function, including accuracy reward, diversity reward and novelty reward, is designed to implement a multi-demand, multi-decision recommendation method. Experiments are conducted on three real-world datasets, and the experimental results show that the model can guarantee the accuracy while improving the diversity and novelty of recommended items.},
  archive      = {J_APIN},
  author       = {Li, Yinggang and Tong, Xiangrong and Lv, Zhongming},
  doi          = {10.1007/s10489-024-05854-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Multi-dimensional requirements for reinforcement recommendation reasoning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISL-net: Dual-stream interaction network with task-optimized
modules for more accurate, complete iris segmentation and localization.
<em>APIN</em>, <em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-024-05862-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris images captured in uncooperative and unconstrained environments pose significant challenges for iris segmentation and localization owing to factors including high occlusions, specular reflections, motion blur, iris rotation, and off-angle images. To address this challenge, this paper proposes ISL-Net, a multitask segmentation network with a task-optimization module based on deep learning for joint iris segmentation and localization. We developed a dual-stream interactive module (DSIM) that combines dual-stream decoders to facilitate information exchange between tasks without interference. To optimize the iris-segmentation and iris-localization performance, we incorporated a balanced attention module (BAM) and a boundary-enhancement module (BEM) in the skip connections of the respective task stream decoders. The BEM recovers missing boundaries in iris localization, while the BAM focuses on uncertain areas in iris segmentation, enhancing the model’s ability to handle these regions. These modules complement each other, improving overall system performance without interference. The proposed model was evaluated on three challenging iris datasets, outperforming most existing models by achieving e1 index scores of 0.34, 0.79, and 0.61% and average normalized Hausdorff distances (HDs) of 0.7221, 1.1914, and 1.0396%. The results indicate that ISL-Net can generate normalized iris images with simple post-processing, making it suitable for direct application in existing iris-recognition systems.},
  archive      = {J_APIN},
  author       = {He, Lei and Yang, Xiaokai and Zheng, Jian and Liu, Zhaobang and Yang, Xiaoguo},
  doi          = {10.1007/s10489-024-05862-8},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {ISL-net: Dual-stream interaction network with task-optimized modules for more accurate, complete iris segmentation and localization},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDDP: Sensitive data detection method for user-controlled
data pricing. <em>APIN</em>, <em>55</em>(6), 1–15. (<a
href="https://doi.org/10.1007/s10489-025-06229-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, there is an urgent need for data sharing, in which data pricing is a crucial issue, because a reasonable price can not only enhance the willingness of users to share data but also promote the progress of data sharing. However, current research is mostly approached from the perspective of data sharing platforms, treating all data equally without sufficient evaluation of sensitive data within shared datasets and personalized perception of privacy from the users themselves. To address this problem, we detected sensitive data in each piece of data and then defined the pricing function based on information entropy and the user’s perception of sensitive information. To enhance the accuracy of sensitive data detection, we integrated an attention mechanism into a pre-trained model to comprehensively represent the samples. Subsequently, on the basis of automatically generating label correlation vectors to calculate the correlation matrix, a graph convolutional neural network was employed to mine the correlation between labels. Furthermore, based on the detection results, information entropy and user ratings are reasonably mapped to prices. Pricing based on user ratings is more suitable for pricing personal data rather than government or institutional data. The experimental results on the dataset of Twitter text sent by users have demonstrated that the average precision of our sensitive data detection model has improved by up to 9.26% compared to comparison models, and SDDP can provide reasonable pricing for samples containing sensitive data and fair compensation for users.},
  archive      = {J_APIN},
  author       = {Hu, Yuchuan and Hu, Bitao and Guo, Bing and Dai, Cheng and Shen, Yan},
  doi          = {10.1007/s10489-025-06229-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {SDDP: Sensitive data detection method for user-controlled data pricing},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale dual-stream visual feature extraction and graph
reasoning for visual question answering. <em>APIN</em>, <em>55</em>(6),
1–18. (<a href="https://doi.org/10.1007/s10489-025-06325-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning algorithms have significantly expanded the capabilities of systems to handle vision-to-language (V2L) tasks. Visual question answering (VQA) presents challenges that require a deep understanding of visual and language content to perform complex reasoning tasks. The existing VQA models often rely on grid-based or region-based visual features, which capture global context and object-specific details, respectively. However, balancing the complementary strengths of each feature type while minimizing fusion noise remains a significant challenge. This study propose a multi-scale dual-stream visual feature extraction method that combines grid and region features to enhance both global and local visual feature representations. Also, a visual graph relational reasoning (VGRR) approach is proposed to further improve reasoning by constructing a graph that models spatial and semantic relationships between visual objects, using Graph Attention Networks (GATs) for relational reasoning. To enhance the interaction between visual and textual modalities, we further propose a cross-modal self-attention fusion strategy, which enables the model to focus selectively on the most relevant parts of both the image and the question. The proposed model is evaluated on the VQA 2.0 and GQA benchmark datasets, demonstrating competitive performance with significant accuracy improvements compared to state-of-the-art methods. Ablation studies confirm the effectiveness of each module in enhancing visual-textual understanding and answer prediction.},
  archive      = {J_APIN},
  author       = {Yusuf, Abdulganiyu Abdu and Feng, Chong and Mao, Xianling and Li, Xinyan and Haruna, Yunusa and Duma, Ramadhani Ally},
  doi          = {10.1007/s10489-025-06325-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale dual-stream visual feature extraction and graph reasoning for visual question answering},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Locomotion mode prediction in real-life walking with and
without ankle–foot exoskeleton assistance. <em>APIN</em>,
<em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06416-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exoskeletons can assist human locomotion in real-life scenarios, but existing tools for decoding locomotion modes (LMs) focus on recognition rather than prediction, which can lead to delayed assistance. This study proposes a long short-term memory (LSTM) neural network to predict five LMs (level-walking, ramp ascent/descent, stair ascent/descent) with greater lead time compared to state-of-the-art methods. We examined the optimal sequence length (SL) for LSTM-based LM prediction, using data from inertial sensors placed on the lower limbs and the lower back, along with a waist-mounted infrared laser. Ten subjects walked in real-life scenarios, both with and without an ankle–foot exoskeleton. Results show that a 1-s SL provides the most advanced and accurate LM prediction, outperforming SLs of 0.6, 0.8, and 1.2 s. The proposed LSTM model achieved an accuracy of 98 ± 0.31%, predicting LMs 0.66 s in advance (for an average stride time of 1.98 ± 0.83 s). Level-walking presented more misclassifications, and the model primarily relied on inertial data over laser input. Overall, these findings demonstrate the LSTM’s strong predictive capability for both assisted and non-assisted walking and independent of which limb executes the transition, supporting its applicability for exoskeleton-assisted locomotion.},
  archive      = {J_APIN},
  author       = {Carvalho, Simão P. and Figueiredo, Joana and Cerqueira, João J. and Santos, Cristina P.},
  doi          = {10.1007/s10489-025-06416-2},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Locomotion mode prediction in real-life walking with and without ankle–foot exoskeleton assistance},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scale-cross non-local network with higher-level semantics
guidance for smoke segmentation. <em>APIN</em>, <em>55</em>(6), 1–17.
(<a href="https://doi.org/10.1007/s10489-025-06420-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoke semantic segmentation (SSS) is particularly challenging task due to the various patterns of the target itself, which are caused by the characteristics of smoke, like, non-rigid, translucent, fuzzy, environment-sensitive, and so forth. This paper tailor-makes the Scale-Cross Non-Local Network (SCNN) for Smoke Segmentation, aiming to accurately locate the position of smoke in complex scenes. While non-local enjoys the bonus of the excellent competence in modeling long-range contextual dependencies acquired by self-attention, the constraint on single-scale input and the suitability for low-resolution feature erode its capability in information representation. To address these issues, we bespoke a Scale-Cross Non-Local (SCNL) module to better integrate local features with global dependencies. In practical scenes, diverse non-smoke objects sharing similarity with smoke pose great obstacles to accurate location of smoke. As a solution, we design a Pyramid Irregular Convolution (PIC) module containing rich high-level semantic to further refine the feature representation of segmentation task. By supervising classification task, the high-level semantics obtained can guide the segmentation feature to correct semantic errors at the image level and alleviate the issue of between-class similarity. To assess its generalization ability, we empirically evaluate our SCNN on extensive synthetic and real data. Experimental results demonstrate that SCNN achieves state-of-the-art performance, exhibiting enhanced smoke localization, accuracy in boundary detection, and a significant reduction in the false segmentation rate for smoke-like objects.},
  archive      = {J_APIN},
  author       = {Zhang, Lin and Wu, Jing and Zhao, Yun and Yuan, Feiniu},
  doi          = {10.1007/s10489-025-06420-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Appl. Intell.},
  title        = {A scale-cross non-local network with higher-level semantics guidance for smoke segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HKGAT: Heterogeneous knowledge graph attention network for
explainable recommendation system. <em>APIN</em>, <em>55</em>(6), 1–19.
(<a href="https://doi.org/10.1007/s10489-025-06446-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the Heterogeneous Knowledge Graph Attention Network (HKGAT) for recommendation systems. As recommendation technology evolves, systems now emphasize diversity, fairness, and explainability alongside accuracy. Traditional methods encounter issues integrating knowledge graphs and lack explainability. HKGAT addresses these by leveraging heterogeneous knowledge graphs. It consists of a heterogeneous information aggregation layer, an attention-aware heterogeneous relation fusion layer, and a prediction layer. First, recommendation data forms a user-item knowledge graph. Then, the aggregation layer collects relation information, followed by the fusion layer integrating it for higher-order feature representations. The prediction layer combines link prediction and recommendation score prediction. Additionally, paths of top-ten results are analyzed and quantified for explainability to optimize ranking. Experiments on self-constructed and Amazon-book datasets show HKGAT outperforms baselines like HetGCN, with significant improvements in Precision, Recall, F1 score, and NDCG@10, and a notable 1.9% gain in NDCG@10 from explainable ranking optimization.},
  archive      = {J_APIN},
  author       = {Zhang, Yongchuan and Tian, Jiahong and Sun, Jing and Chan, Huirong and Qiu, Agen and Liu, Cailin},
  doi          = {10.1007/s10489-025-06446-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {HKGAT: Heterogeneous knowledge graph attention network for explainable recommendation system},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging CQT-VMD and pre-trained AlexNet architecture for
accurate pulmonary disease classification from lung sound signals.
<em>APIN</em>, <em>55</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s10489-025-06452-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel algorithm for classifying pulmonary diseases using lung sound signals by integrating Variational Mode Decomposition (VMD) and the Constant-Q Transform (CQT) within a pre-trained AlexNet convolutional neural network. Breathing sounds from the ICBHI and KAUHS databases are analyzed, where three key intrinsic mode functions (IMFs) are extracted using VMD and subsequently converted into CQT-based time-frequency representations. These images are then processed by the AlexNet model, achieving an impressive classification accuracy of 93.30%. This approach not only demonstrates the innovative synergy of CQT-VMD for lung sound analysis but also underscores its potential to enhance computerized decision support systems (CDSS) for pulmonary disease diagnosis. The results, showing high accuracy, a sensitivity of 91.21%, and a specificity of 94.9%, highlight the robustness and effectiveness of the proposed method, paving the way for its clinical adoption and the development of lightweight deep-learning algorithms for portable diagnostic tools. Overview of the proposed methodology for pulmonary disease classification using CQT-VMD and pre-trained AlexNet architecture applied to lung sound signals},
  archive      = {J_APIN},
  author       = {Neili, Zakaria and Sundaraj, Kenneth},
  doi          = {10.1007/s10489-025-06452-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Appl. Intell.},
  title        = {Leveraging CQT-VMD and pre-trained AlexNet architecture for accurate pulmonary disease classification from lung sound signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTMKGRL: A universal multimodal knowledge graph
representation learning framework using optimal transport and
cross-modal relation. <em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06459-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for integrating multimodal information, such as text and images, has grown significantly as it enables richer and more comprehensive knowledge representations. Most existing multimodal knowledge graph representation learning (KGRL) methods focus primarily on fusing multimodal entity information, directly applying multimodal entities and single-modal relations to downstream tasks. However, these methods face challenges related to the heterogeneity of multi-source entity data, which amplifies the differences in feature distributions between entity and relation representations. To address these challenges, we propose a universal multimodal KGRL framework, OTMKGRL, which seamlessly incorporates multimodal information into three types of single-modal KGRL methods. First, OTMKGRL employs Tucker decomposition to project entity text and image data into a shared space, thereby generating multimodal entity representations. It then uses optimal transport to integrate multimodal entity information into the original single-modal entity representations. Second, OTMKGRL introduces a cross-modal relation attention mechanism that fuses effective multimodal entity features into the original single-modal relations, yielding cross-modal relation representations. Extensive experiments across three multimodal datasets demonstrate the effectiveness and versatility of our approach. The OTMKGRL framework significantly enhances the performance of existing single-modal KGRL models in multimodal settings.},
  archive      = {J_APIN},
  author       = {Wang, Tao and Shen, Bo},
  doi          = {10.1007/s10489-025-06459-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {OTMKGRL: A universal multimodal knowledge graph representation learning framework using optimal transport and cross-modal relation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based attention deep q-network with prior-based
knowledge. <em>APIN</em>, <em>55</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s10489-024-05850-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based reinforcement learning (RL) is a potent algorithm for addressing tasks related to visual behavioural decision-making; nevertheless, it operates as a black-box, directly training models with images as input in the end-to-end fashion. Therefore, to elucidate the underlying mechanisms of the model and the agent’s focus on different features during the decision-making process, a vision-based attention (VA) mechanism is introduced into vision-based RL in this paper. A prior-based mechanism is introduced to address the issue of instability in the attention maps observed by the agent when attention mechanisms are directly integrated into network updates that results in an increase in single-step errors and larger cumulative errors. Thus, a vision-based attention deep Q-network (VADQN) method with a prior-based mechanism is proposed. Specifically, prior attention maps are obtained using a learnable Gaussian filtering and a spectral residual method. Next, the attention maps are fine-tuned using a self-attention (SA) mechanism to enhance their performance. During training, both the attention maps and the parameters of the policy network are concurrently trained to ensure explanations of the regions of interest during online training. Finally, a series of ablation experiments are conducted on Atari games to compare the proposed method with humans, convolutional neural networks, and other approaches. The results demonstrate that the proposed method not only reveals the regions of interest attended to by DRL during the decision-making process but also enhances DRL performance in certain scenarios. This approach provides valuable insights for understanding and improving the performance of DRL in visual decision-making tasks.},
  archive      = {J_APIN},
  author       = {Ma, Jialin and Li, Ce and Hong, Liang and Wei, Kailun and Zhao, Shutian and Jiang, Hangfei and Qu, Yanyun},
  doi          = {10.1007/s10489-024-05850-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Vision-based attention deep q-network with prior-based knowledge},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised text classification method based on
three-way decision with evidence theory. <em>APIN</em>, <em>55</em>(6),
1–15. (<a href="https://doi.org/10.1007/s10489-024-06129-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning methods play a crucial role in text classification tasks. However, due to limitation of scarce labeled training data, the uncertainty of pseudo labels is still an unavoidable problem in semi-supervised text classification. To address this issue, this paper introduces three-way decision theory into semi-supervised text classification model, which divides the model output pseudo-labeled samples into different regions and adopts different processing strategies. The accurate and effective pseudo-labeled samples are selected as much as possible to expand the original training set. For the pseudo-labeled outputs by the model, we use evidence theory to fuse the probability outputs of the samples to improve the stability and credibility of pseudo labels. Experimental results demonstrate that the method introduced in this paper effectively enhances the accuracy of semi-supervised text classification while exhibiting high stability.},
  archive      = {J_APIN},
  author       = {Yang, Ziping and Jiang, Chunmao and Huang, Chunmei},
  doi          = {10.1007/s10489-024-06129-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised text classification method based on three-way decision with evidence theory},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse pinball universum nonparallel support vector machine
and its safe screening rule. <em>APIN</em>, <em>55</em>(6), 1–33. (<a
href="https://doi.org/10.1007/s10489-025-06356-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparallel support vector machine (NPSVM) is an effective and popular classification technique, which introduces the $$\epsilon $$ -insensitive loss function instead of the quadratic loss function in twin support vector machine (TSVM), making the model have the same sparsity and kernel strategy as support vector machine (SVM). However, NPSVM is sensitive to noise points and does not utilize the prior knowledge embedded in the unlabeled samples. Therefore, to improve its generalization ability and robustness, a sparse pinball Universum nonparallel support vector machine (SPUNPSVM) is first proposed in this paper. On the one hand, the sparse pinball loss is employed to enhance the robustness. On the other hand, it exploits the Universum data, which do not belong to any class, to embed prior knowledge into the model. Numerical experiments have verified its effectiveness. Furthermore, to further speed up SPUNPSVM, we propose a safe screening rule (SSR-SPUNPSVM) based on its sparsity, which achieves acceleration without sacrificing accuracy. Numerical experiments and statistical tests demonstrate the superiority of our SSR-SPUNPSVM.},
  archive      = {J_APIN},
  author       = {Wang, Hongmei and Li, Ping and Zheng, Yuyan and Jiang, Kun and Xu, Yitian},
  doi          = {10.1007/s10489-025-06356-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-33},
  shortjournal = {Appl. Intell.},
  title        = {Sparse pinball universum nonparallel support vector machine and its safe screening rule},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DGCformer: 3-dimensional graph convolutional transformer
for multi-step origin–destination matrix forecasting. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06371-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting Human mobility is of great significance in the simulation and control of infectious diseases like COVID-19. To get a clear picture of potential future outbreaks, it is necessary to forecast multi-step Origin–Destination (OD) matrices for a relatively long period in the future. However, multi-step Origin–Destination Matrix Forecasting (ODMF) is a non-trivial problem. First, previous ODMF models only forecast the OD matrix for the next time-step, and they cannot perform well on long-term multi-step forecasts due to error accumulation. Second, many ODMF methods capture spatial and temporal dependencies with separate modules, which is insufficient to model spatio-temporal correlations in the time-varying OD matrix sequence. To address the challenges in multi-step ODMF, we propose 3-Dimensional Graph Convolutional Transformer (3DGCformer). As an enhancement of the original 3DGCN, we propose a novel Origin–Destination Feature Propagation (ODFP) rule between 3DGCN layers and integrate 2 3DGCNs with different spatio-temporal graphs and corresponding feature propagation rules to model the formation of OD flows in a more comprehensive way. For multi-step forecasts, 3DGCformer uses Transformer to capture long-term global temporal dependency, and adapt its decoder using labeled tokens to avoid error accumulation and improve time efficiency. To avoid information loss as the number of regions increases, we propose a patch embedding approach to convert data from 3DGCNs to the Transformer module. We perform extensive experiments on 4 real-world human mobility datasets, and the results show that our proposed model outperforms the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Huang, Yiou and Deng, Hao and Zhao, Shengjie},
  doi          = {10.1007/s10489-025-06371-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {3DGCformer: 3-dimensional graph convolutional transformer for multi-step origin–destination matrix forecasting},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HRMG-EA: Heterogeneous graph neural network recommendation
with multi-level guidance based on enhanced-attributes. <em>APIN</em>,
<em>55</em>(6), 1–22. (<a
href="https://doi.org/10.1007/s10489-025-06428-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Graph Neural Networks are an efficient and powerful tool for modeling graph structure data in recommendation systems. However, existing heterogeneous graph neural networks often fail to model the dependencies between user and item attribute preferences, limiting graph structure optimization and consequently reducing the accuracy of recommendations. To overcome these issues, we propose a Heterogeneous graph neural network Recommendation with Multi-level Guidance based on Enhanced-Attributes (HRMG-EA). First, we design an attribute enhanced gated network to model user-item interaction attribute scenarios and obtain enhanced-attributes by capturing complex attribute dependencies. It effectively avoids the expansion of the graph scale in attribute graph scenarios and further covers personalized attribute relationship distribution characteristics of users and items. Then, we propose a novel multi-level graph structure guidance strategy based on enhanced-attributes. It guides graph structure learning from three optimization levels, optimizing from two perspectives: explicit (heterogeneity and homogeneity) and implicit (contrast enhancement). The former can screen higher-quality heterogeneous neighbor nodes in a direct interaction environment, and filter out redundant or erroneous edges under different similar semantic interest paths to improve the quality of the neighborhood environment. The latter aligns representation embeddings of enhanced-attributes and graph structure in a latent space, explores their potential commonalities, and obtains more comprehensive, fine-grained semantic and beneficial structural information. Finally, on two real-world datasets, HRMG-EA significantly outperforms the state-of-the-art baseline algorithms in both recall and normalized discounted cumulative gain. A large number of ablation experiments and analytical verifications also verify its effectiveness.},
  archive      = {J_APIN},
  author       = {Wang, Longtao and Yuan, Guiyuan and Li, Chao and Zhao, Yufei and Duan, Hua and Zeng, Qingtian},
  doi          = {10.1007/s10489-025-06428-y},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Appl. Intell.},
  title        = {HRMG-EA: Heterogeneous graph neural network recommendation with multi-level guidance based on enhanced-attributes},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separable n-soft sets: A tool for multinary descriptions
with large-scale parameter sets. <em>APIN</em>, <em>55</em>(6), 1–37.
(<a href="https://doi.org/10.1007/s10489-025-06435-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft set theory builds on the idea of a parameterized family of subsets of a universal set, where for each pertinent characteristic, any specific member of the universe either satisfies it or not. The concept of an N-soft set sharpens this model with the aid of multinary parameterized descriptions; that is, N-soft sets categorize the options in terms of multiple classifications of the characteristics. The aim of this research is fourfold. First, this research focuses on daily-life decision-making problems that involve both positive and negative attributes that can be naturally distributed among classes. Each comparable group of attributes produces an N-soft set, and we can represent all these N-soft sets using separable N-soft sets. We show that this structure facilitates decision-making in the presence of large numbers of attributes. Second, to develop tools that provide a mechanism for the selection of an alternative in this new model, we first develop a complement operator for N-soft sets to uniformize the data, and then, we propose strategies for taking advantage of the qualities of the attributes. Aggregation operators are employed to aggregate the data into a resultant N-soft set, a fuzzy N-soft set, or a hesitant N-soft set. Several algorithmic procedures are proposed to define these methods. Third, we define the novel notion of a multihesitant N-soft set. This loosely defined concept is helpful for representing data with multiple and repetitive entries while avoiding information loss. Finally, we provide solutions to several real-life decision-making problems to illustrate the versatility of our approaches. We apply this theory to construct a new method for ranking countries participating in the Olympic Games. Our motivation is that the existing lexicographic procedure is unable to distinguish among gold, silver, and bronze medals won at sports with very different characteristics.},
  archive      = {J_APIN},
  author       = {Khan, Muhammad Jabir and Alcantud, Jose Carlos R. and Akram, Muhammad and Ding, Weiping},
  doi          = {10.1007/s10489-025-06435-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Appl. Intell.},
  title        = {Separable N-soft sets: A tool for multinary descriptions with large-scale parameter sets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dark-ControlNet: An enhanced dehazing universal plug-in
based on the dark channel prior. <em>APIN</em>, <em>55</em>(6), 1–16.
(<a href="https://doi.org/10.1007/s10489-025-06439-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing dehazing models have excellent performance in synthetic scenes but still face the challenge of low robustness in real scenes. In this paper, we propose Dark-ControlNet, a generalized and enhanced dehazing plug-in that uses the dark channel prior as a control condition, which can be deployed on existing dehazing models and can be simply fine-tuned to enhance their robustness in real scenes while improving their dehazing performance. We first freeze the backbone network to preserve its encoding and decoding capabilities and input the dark channel prior with high robustness as conditional information to the plug-in network to obtain prior knowledge. Then, we fuse the dark channel prior features into the backbone network in the form of mean-variance alignment via the Haze&amp;Dark(HD) module and guide the backbone network to decode clear images by fine-tuning the plug-in network. The experimental results show that the existing dehazing model enhanced by Dark-ControlNet performs well on synthetic datasets and real datasets.},
  archive      = {J_APIN},
  author       = {Yang, Yu and Yin, Xuesong and Wang, Yigang},
  doi          = {10.1007/s10489-025-06439-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Appl. Intell.},
  title        = {Dark-ControlNet: An enhanced dehazing universal plug-in based on the dark channel prior},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-enhanced and decomposed transformer for
multivariate time series anomaly detection. <em>APIN</em>,
<em>55</em>(6), 1–18. (<a
href="https://doi.org/10.1007/s10489-025-06441-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of the Internet of Things (IoT), vast amounts of multivariate time series data are generated, which reflect the operational status of systems. Accurate and efficient anomaly detection in these data is crucial for maintaining system stability. However, data from unstable environments often exhibit high volatility, data drift, and complex patterns of anomalies. Unsupervised anomaly detection models are typically designed for stable data and lack generalizability, leading to a high rate of false positives when applied to unstable data. This paper introduces the frequency-enhanced and decomposed transformer for anomaly detection (FDTAD), which is a novel anomaly detection model based on a transformer that is enhanced with frequency and time series decomposition. FDTAD addresses data drift by decomposing time series and leverages both time-domain and frequency-domain information to improve the generalization ability of the model. The model preserves major amplitudes in the frequency domain to extract primary periodic patterns, uses spectral residuals to capture detailed variations, and incorporates a frequency-domain correlation attention mechanism to extract dependencies in frequency-domain data in a sparse representation. Additionally, a spatiotemporal module is designed to extract the temporal correlations in the data and spatial correlations among the data with different attributes. FDTAD combines a data periodic pattern reconstructor and a data detailed pattern reconstructor through an adversarial mechanism to achieve maximum accuracy in reconstructing normal data. Extensive experiments on 10 public datasets demonstrate that FDTAD outperforms state-of-the-art baseline methods, with a 4.1% improvement in the F1 score and a 4.7% improvement in precision.},
  archive      = {J_APIN},
  author       = {Li, Shijiang and Wang, Zhihai and Wang, Xiaokang and Yin, Zihao and Yao, Muyun},
  doi          = {10.1007/s10489-025-06441-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Appl. Intell.},
  title        = {Frequency-enhanced and decomposed transformer for multivariate time series anomaly detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing graph representation learning via type-aware
decoupling and node influence allocation. <em>APIN</em>, <em>55</em>(6),
1–14. (<a href="https://doi.org/10.1007/s10489-025-06443-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional graph representation methods can fit the information of graph with low-dimensional vectors, but they cannot interpret their composition, resulting in insufficient security. Graph decoupling, as a method of graph representation, can analyze the latent factors composing the graph representation vectors. However, in current graph decoupling methods, the number of factors is a hyperparameter, and enforce uniform decoupling vector dimensions which leads to information loss or redundancy. To address these issues, we propose a type-aware graph decoupling based on influence called Variational Graph Decoupling Auto-Encoder (VGDAE). It uses node labels as interpretable and objectively existing natural semantics for decoupling and allocates embedding space based on node influence, addressing the issues of manually setting the number of factors in traditional graph decoupling and the mismatch between node information size and embedding space. On the Cora, Citeseer, and fb-CMU datasets, VGDAE shows the impact of different node classes as decoupling targets on classification tasks. Furthermore, we perform visualization of the representations, VGDAE exhibits performance improvements of 2% in classification tasks and 12% in clustering tasks when compared with baseline models.},
  archive      = {J_APIN},
  author       = {Zhu, Guochang and Hu, Jun and Liu, Li and Zhang, Qinghua and Wang, Guoyin},
  doi          = {10.1007/s10489-025-06443-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing graph representation learning via type-aware decoupling and node influence allocation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REFD: Recurrent encoder and fusion decoder for temporal
knowledge graph reasoning. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06445-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning over Temporal Knowledge Graphs (TKGs) presents challenges in modeling the dynamic relationships and evolving behaviors of entities and relations over time. Traditional approaches often treat entities and relations separately, which limits their ability to capture their joint temporal evolution and interactions. To overcome these limitations, REFD (Recurrent Encoder and Fusion Decoder) is proposed, a novel framework designed to improve TKG reasoning. The REFD framework consists of two primary components: a recurrent encoder and a fusion decoder. The recurrent encoder incorporates three key modules: (1) the full-domain multi-scale temporal recurrent encoder, which effectively captures temporal dependencies across varying time scales, (2) the entity-relation symbiotic temporal feature deep fusion engine, which integrates temporal features of both entities and relations, and (3) the intelligent temporal feature priority dynamic adjustment mechanism, which adaptively adjusts the importance of different features over time. The fusion decoder, particularly the entity-relation feature Fusion Decoder, combines the temporal features of entities and relations to model their joint evolution, overcoming the limitations of previous methods that model them separately. By jointly capturing the evolving dynamics of entities and relations over time, REFD significantly enhances the accuracy of temporal reasoning tasks. Experimental results show that REFD outperforms existing approaches, offering superior prediction accuracy and better handling of the complexities in TKGs.},
  archive      = {J_APIN},
  author       = {Liu, Qian and Feng, Siling and Huang, MengXing and Bhatti, Uzair Aslam},
  doi          = {10.1007/s10489-025-06445-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {REFD: Recurrent encoder and fusion decoder for temporal knowledge graph reasoning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mastering table tennis with hierarchy: A reinforcement
learning approach with progressive self-play training. <em>APIN</em>,
<em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06450-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical Reinforcement Learning (HRL) is widely applied in various complex task scenarios. In complex tasks where simple model-free reinforcement learning struggles, hierarchical design allows for more efficient utilization of interactive data, significantly reducing training costs and improving training success rates. This study delves into the use of HRL based on the model-free policy layer to learn complex strategies for a robotic arm playing table tennis. Through processes such as pre-training, self-play training, and self-play training with top-level winning strategies, the robustness of the lower-level hitting strategies has been enhanced. Furthermore, a novel decay reward mechanism has been employed in the training of the higher-level agent to improve the win rate in adversarial matches against other methods. After pre-training and adversarial training, we achieved an average of 52 rally cycles for the forehand strategy and 48 rally cycles for the backhand strategy in testing. The high-level strategy training based on the decay reward mechanism resulted in an advantageous score when competing against other strategies.},
  archive      = {J_APIN},
  author       = {Ma, Hongxu and Fan, Jianyin and Xu, Haoran and Wang, Qiang},
  doi          = {10.1007/s10489-025-06450-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {Mastering table tennis with hierarchy: A reinforcement learning approach with progressive self-play training},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new deep learning-based approach for predicting the
geothermal heat pump’s thermal power of a real bioclimatic house.
<em>APIN</em>, <em>55</em>(6), 1–20. (<a
href="https://doi.org/10.1007/s10489-025-06457-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, growing concern about climate change and the need to reduce greenhouse gas emissions have highlighted the role of energy efficiency and sustainability on the global agenda. Energy policies are decisive in establishing regulatory frameworks and incentives to address these challenges, leading to an inclusive and more resilient energy transition. In this context, geothermal energy is an essential source of renewable, low-emission energy, capable of providing heat and electricity sustainably. The present research focuses on a bioclimatic house’s geothermal energy system based on a heating pump and a horizontal heat exchanger. The main aim is to predict the generated thermal power of the heat pump using historical data from several sensors. In particular, two approaches were proposed with both uni-variate and multi-variate scenarios. Several deep learning techniques were applied: LSTM, GRU, 1D-CNN, CNN-LSTM, and CNN-GRU, obtaining satisfactory results over the whole dataset, which comprised one year of data acquisition. Specifically, promising results have been achieved using hybrid methods combining recurrent-based and convolutional neural networks.},
  archive      = {J_APIN},
  author       = {Zayas-Gato, Francisco and Díaz-Longueira, Antonio and Arcano-Bea, Paula and Michelena, Álvaro and Calvo-Rolle, Jose Luis and Jove, Esteban},
  doi          = {10.1007/s10489-025-06457-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Appl. Intell.},
  title        = {A new deep learning-based approach for predicting the geothermal heat pump’s thermal power of a real bioclimatic house},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach to software defect prediction for small-sized
datasets. <em>APIN</em>, <em>55</em>(6), 1–21. (<a
href="https://doi.org/10.1007/s10489-025-06458-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction (SDP) is an active research subject in the software engineering domain. The earlier works on SDP use the same project’s data for prediction in future releases, called within-project defect prediction (WPDP). WPDP may not perform well when the data available for training is small in size. In this work, to address the issue of small-size data, we suggest enhancing the data by borrowing data from other software projects. For better prediction accuracy of learning models, both train and test data must follow the same distribution. However, this may not be true in the case of data being transferred from the other project. Data from different projects may follow different distributions. So, to handle this issue, we have proposed a data preprocessing method, namely data transfer-based WPDP (DT-WPDP). Next, we have shown the use of the deep neural network (DNN) for WPDP and compared it with other classical machine learning (ML) models such as k nearest neighbor, decision tree, logistic regression, and Naive Bayes classifiers. Further, we have performed experimental analysis to assess the effect of the proposed DT-WPDP data preprocessing method with DNN and other ML models. Experimental results show that the proposed approach significantly improves the accuracies of different models. Among different models, the DNN model performed best for all datasets. In the case of very small-sized datasets, which is our main concern in this work, the accuracy of the DNN model is improved by 7% after using the proposed approach.},
  archive      = {J_APIN},
  author       = {Bal, Pravas Ranjan and Shukla, Suyash and Kumar, Sandeep},
  doi          = {10.1007/s10489-025-06458-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Appl. Intell.},
  title        = {An approach to software defect prediction for small-sized datasets},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ar---9">AR - 9</h2>
<ul>
<li><details>
<summary>
(2025). Mori-zwanzig approach for belief abstraction with
application to belief space planning. <em>AR</em>, <em>49</em>(1), 1–23.
(<a href="https://doi.org/10.1007/s10514-024-10185-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning-based method to extract symbolic representations of the belief state and its dynamics in order to solve planning problems in a continuous-state partially observable Markov decision processes (POMDP) problem. While existing approaches typically parameterize the continuous-state POMDP into a finite-dimensional Markovian model, they are unable to preserve fidelity of the abstracted model. To improve accuracy of the abstracted representation, we introduce a memory-dependent abstraction approach to mitigate the modeling error. The first major contribution of this paper is we propose a Neural Network based method to learn the non-Markovian transition model based on the Mori-Zwanzig (M-Z) formalism. Different from existing work in applying M-Z formalism to autonomous time-invariant systems, our approach is the first work generalizing the M-Z formalism to robotics, by addressing the non-Markovian modeling of the belief dynamics that is dependent on historical observations and actions. The second major contribution is we theoretically show that modeling the non-Markovian memory effect in the abstracted belief dynamics improves the modeling accuracy, which is the key benefit of the proposed algorithm. Simulation experiment of a belief space planning problem is provided to validate the performance of the proposed belief abstraction algorithms.},
  archive      = {J_AR},
  author       = {Hou, Mengxue and Lin, Tony X. and Zhou, Enlu and Zhang, Fumin},
  doi          = {10.1007/s10514-024-10185-1},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Auton. Robot.},
  title        = {Mori-zwanzig approach for belief abstraction with application to belief space planning},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrative biomechanics of a human–robot carrying task:
Implications for future collaborative work. <em>AR</em>, <em>49</em>(1),
1–12. (<a href="https://doi.org/10.1007/s10514-024-10184-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with sarcopenia, who face difficulties in carrying heavy loads, may benefit from collaborative robotic assistance that is modeled after human–human interaction. The objective of this study is to describe the kinematics and spatio-temporal parameters during a collaborative carrying task involving both human and robotic partners. Fourteen subjects carried a table while moving forward with a human and a robotic partner. The movements were recorded using a three-dimensional motion capture system. The subjects successfully completed the task of carrying the table with the robot. No significant differences were found in the shoulder and elbow flexion/extension angles. In human–human dyads, the center of mass naturally oscillated vertically with an amplitude of approximately 2 cm. The here presented results of the human–human interaction serve as a model for the development of future robotic systems, designed for collaborative manipulation.},
  archive      = {J_AR},
  author       = {Schuengel, Verena and Braunstein, Bjoern and Goell, Fabian and Braun, Daniel and Reißner, Nadine and Safronov, Kirill and Weiser, Christian and Heieis, Jule and Albracht, Kirsten},
  doi          = {10.1007/s10514-024-10184-2},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Auton. Robot.},
  title        = {Integrative biomechanics of a human–robot carrying task: Implications for future collaborative work},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe and stable teleoperation of quadrotor UAVs under haptic
shared autonomy. <em>AR</em>, <em>49</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10514-024-10186-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach that aims to address both safety and stability of a haptic teleoperation system within a framework of Haptic Shared Autonomy (HSA). We use Control Barrier Functions (CBFs) to generate the control input that follows the user’s input as closely as possible while guaranteeing safety. In the context of stability of the human-in-the-loop system, we limit the force feedback perceived by the user via a small $$\mathcal {L}_2$$ -gain, which is achieved by limiting the control and the force feedback via a differential constraint. Specifically, with the property of HSA, we propose two pathways to design the control and the force feedback: Sequential Control Force (SCF) and Joint Control Force (JCF). Both designs can achieve safety and stability but with different responses to the user’s commands. We conducted experimental simulations to evaluate and investigate the properties of the designed methods. We also tested the proposed method on a physical quadrotor UAV and a haptic interface.},
  archive      = {J_AR},
  author       = {Zhang, Dawei and Tron, Roberto},
  doi          = {10.1007/s10514-024-10186-0},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Auton. Robot.},
  title        = {Safe and stable teleoperation of quadrotor UAVs under haptic shared autonomy},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthesizing compact behavior trees for probabilistic
robotics domains. <em>AR</em>, <em>49</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10514-024-10187-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex robotics domains (e.g., remote exploration applications and scenarios involving interactions with humans) require encoding high-level mission specifications that consider uncertainty. Most current fielded systems in practice require humans to manually encode mission specifications in ways that require amounts of time and expertise that can become infeasible and limit mission scope. Therefore, we propose a method of automating the process of encoding mission specifications as behavior trees. In particular, we present an algorithm for synthesizing behavior trees that represent the optimal policy for a user-defined specification of a domain and problem in the Probabilistic Planning Domain Definition Language (PPDDL). Our algorithm provides access to behavior tree advantages including compactness and modularity, while alleviating the need for the time-intensive manual design of behavior trees, which requires substantial expert knowledge. Our method converts the PPDDL specification into solvable MDP matrices, simplifies the solution, i.e. policy, using Boolean algebra simplification, and converts this simplified policy to a compact behavior tree that can be executed by a robot. We present simulated experiments for a marine target search and response scenario and an infant-robot interaction for mobility domain. Our results demonstrate that the synthesized, simplified behavior trees have approximately between 15 x and 26 x fewer nodes and an average of between 8 x and 13 x fewer active conditions for selecting the active action than they would without simplification. These compactness and activity results suggest an increase in the interpretability and execution efficiency of the behavior trees synthesized by the proposed method. Additionally, our results demonstrate that this synthesis method is robust to a variety of user input mistakes, and we empirically confirm that the synthesized behavior trees perform equivalently to the optimal policy that they are constructed to logically represent.},
  archive      = {J_AR},
  author       = {Scheide, Emily and Best, Graeme and Hollinger, Geoffrey A.},
  doi          = {10.1007/s10514-024-10187-z},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Auton. Robot.},
  title        = {Synthesizing compact behavior trees for probabilistic robotics domains},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). View: Visual imitation learning with waypoints. <em>AR</em>,
<em>49</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10514-024-10188-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots can use visual imitation learning (VIL) to learn manipulation tasks from video demonstrations. However, translating visual observations into actionable robot policies is challenging due to the high-dimensional nature of video data. This challenge is further exacerbated by the morphological differences between humans and robots, especially when the video demonstrations feature humans performing tasks. To address these problems we introduce Visual Imitation lEarning with Waypoints (VIEW), an algorithm that significantly enhances the sample efficiency of human-to-robot VIL. VIEW achieves this efficiency using a multi-pronged approach: extracting a condensed prior trajectory that captures the demonstrator’s intent, employing an agent-agnostic reward function for feedback on the robot’s actions, and utilizing an exploration algorithm that efficiently samples around waypoints in the extracted trajectory. VIEW also segments the human trajectory into grasp and task phases to further accelerate learning efficiency. Through comprehensive simulations and real-world experiments, VIEW demonstrates improved performance compared to current state-of-the-art VIL methods. VIEW enables robots to learn manipulation tasks involving multiple objects from arbitrarily long video demonstrations. Additionally, it can learn standard manipulation tasks such as pushing or moving objects from a single video demonstration in under 30 min, with fewer than 20 real-world rollouts. Code and videos here: https://collab.me.vt.edu/view/},
  archive      = {J_AR},
  author       = {Jonnavittula, Ananth and Parekh, Sagar and P. Losey, Dylan},
  doi          = {10.1007/s10514-024-10188-y},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Auton. Robot.},
  title        = {View: Visual imitation learning with waypoints},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eigen-factors a bilevel optimization for plane SLAM of 3D
point clouds. <em>AR</em>, <em>49</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10514-025-10189-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern depth sensors can generate a huge number of 3D points in few seconds to be later processed by Localization and Mapping algorithms. Ideally, these algorithms should handle efficiently large sizes of Point Clouds (PC) under the assumption that using more points implies more information available. The Eigen Factors (EF) is a new algorithm that solves PC SLAM by using planes as the main geometric primitive. To do so, EF exhaustively calculates the error of all points at complexity O(1), thanks to the Summation matrix S of homogeneous points. The solution of EF is a bilevel optimization where the lower-level problem estimates the plane variables in closed-form, and the upper-level non-linear problem uses second order optimization to estimate sensor poses (trajectory). We provide a direct analytical solution for the gradient and Hessian based on the homogeneous point-plane constraint. In addition, two variants of the EF are proposed: one pure analytical derivation and a second one approximating the problem to an alternating optimization showing better convergence properties. We evaluate the optimization processes (back-end) of EF and other state-of-the-art plane SLAM algorithms in a synthetic environment, and extended to ICL dataset (RGBD) and LiDAR KITTI datasets. EF demonstrates superior robustness and accuracy of the estimated trajectory and improved map metrics. Code is publicly available at https://github.com/prime-slam/EF-plane-SLAM with python bindings and pip package.},
  archive      = {J_AR},
  author       = {Ferrer, Gonzalo and Iarosh, Dmitrii and Kornilova, Anastasiia},
  doi          = {10.1007/s10514-025-10189-5},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Auton. Robot.},
  title        = {Eigen-factors a bilevel optimization for plane SLAM of 3D point clouds},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Isolated kalman filtering: Theory and decoupled estimator
design. <em>AR</em>, <em>49</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10514-025-10191-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a state decoupling strategy for Kalman filtering problems, when the dynamics of individual estimates are decoupled and their outputs are sparsely coupled. The algorithm is termed Isolated Kalman Filtering (IsoKF) and exploits the sparsity in the output coupling by applying approximations that mitigate the need for non-involved estimates. We prove that the approximations made during the isolated coupling of estimates are based on an implicit maximum determinant completion of the incomplete a priori covariance matrix. The steady state behavior is studied on eleven different observation graphs and a buffering scheme to support delayed (i.e. out-of-order) measurements is proposed. We discussed handling of delayed measurements in both, an optimal or a suboptimal way. The credibility of the isolated estimates are evaluated on a linear and nonlinear toy example in Monte Carlo simulations. The presented paradigm is made available online to the community within a generic C++ estimation framework supporting both, modular sensor fusion and collaborative state estimation.},
  archive      = {J_AR},
  author       = {Jung, Roland and Luft, Lukas and Weiss, Stephan},
  doi          = {10.1007/s10514-025-10191-x},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Auton. Robot.},
  title        = {Isolated kalman filtering: Theory and decoupled estimator design},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Between reality and delusion: Challenges of applying large
language models to companion robots for open-domain dialogues with older
adults. <em>AR</em>, <em>49</em>(1), 1–41. (<a
href="https://doi.org/10.1007/s10514-025-10190-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Throughout our lives, we interact daily in conversations with our friends and family, covering a wide range of topics, known as open-domain dialogue. As we age, these interactions may diminish due to changes in social and personal relationships, leading to loneliness in older adults. Conversational companion robots can alleviate this issue by providing daily social support. Large language models (LLMs) offer flexibility for enabling open-domain dialogue in these robots. However, LLMs are typically trained and evaluated on textual data, while robots introduce additional complexity through multi-modal interactions, which has not been explored in prior studies. Moreover, it is crucial to involve older adults in the development of robots to ensure alignment with their needs and expectations. Correspondingly, using iterative participatory design approaches, this paper exposes the challenges of integrating LLMs into conversational robots, deriving from 34 Swedish-speaking older adults’ (one-to-one) interactions with a personalized companion robot, built on Furhat robot with GPT $$-$$ 3.5. These challenges encompass disruptions in conversations, including frequent interruptions, slow, repetitive, superficial, incoherent, and disengaging responses, language barriers, hallucinations, and outdated information, leading to frustration, confusion, and worry among older adults. Drawing on insights from these challenges, we offer recommendations to enhance the integration of LLMs into conversational robots, encompassing both general suggestions and those tailored to companion robots for older adults.},
  archive      = {J_AR},
  author       = {Irfan, Bahar and Kuoppamäki, Sanna and Hosseini, Aida and Skantze, Gabriel},
  doi          = {10.1007/s10514-025-10190-y},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-41},
  shortjournal = {Auton. Robot.},
  title        = {Between reality and delusion: Challenges of applying large language models to companion robots for open-domain dialogues with older adults},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASAP-MPC: An asynchronous update scheme for online motion
planning with nonlinear model predictive control. <em>AR</em>,
<em>49</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10514-025-10192-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Nonlinear Model Predictive Control (NMPC) update scheme targeted at motion planning for mechatronic motion systems, such as drones and mobile platforms. NMPC-based motion planning typically requires low computation times to be able to provide control inputs at the required rate for system stability, disturbance rejection, and overall performance. To achieve online NMPC updates in complex situations, works in literature typically rely on one of two approaches: attempting to reduce the solution times in NMPC by sacrificing feasibility guarantees, or allowing more time to the motion planning algorithm, which requires additional strategies to ensure robust tracking of the planned motion, e.g., state feedback. Following this second paradigm, this paper presents As-Soon-As-Possible MPC (ASAP-MPC), an asynchronous update scheme for online motion planning with optimal control that abandons the idea of having to satisfy restrictive real-time update rates and that solves the optimal control problem to full convergence. ASAP-MPC combines trajectory generation through optimal control with additional tracking control for improved robustness against disturbances and plant-model mismatch. The scheme seamlessly connects trajectories, resulting from subsequent NMPC solutions, providing a smooth and continuous overall trajectory for the motion system. This framework’s applicability to embedded applications is shown on two different experiment setups where a state-of-the-art method fails to successfully navigate through a given environment: a quadcopter flying through a cluttered environment with hardware-in-the-loop simulation and a scale model truck-trailer manoeuvring in a structured physical lab environment.},
  archive      = {J_AR},
  author       = {Dirckx, Dries and Bos, Mathias and Vandewal, Bastiaan and Vanroye, Lander and Swevers, Jan and Decré, Wilm},
  doi          = {10.1007/s10514-025-10192-w},
  journal      = {Autonomous Robots},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Auton. Robot.},
  title        = {ASAP-MPC: An asynchronous update scheme for online motion planning with nonlinear model predictive control},
  volume       = {49},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="alg---5">Alg - 5</h2>
<ul>
<li><details>
<summary>
(2025). Complexity framework for forbidden subgraphs i: The
framework. <em>Alg</em>, <em>87</em>(3), 429–464. (<a
href="https://doi.org/10.1007/s00453-024-01289-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a set of graphs $${\mathcal {H}}$$ , a graph G is $${\mathcal {H}}$$ -subgraph-free if G does not contain any graph from $${{{\mathcal {H}}}}$$ as a subgraph. We propose general and easy-to-state conditions on graph problems that explain a large set of results for $${\mathcal {H}}$$ -subgraph-free graphs. Namely, a graph problem must be efficiently solvable on graphs of bounded treewidth, computationally hard on subcubic graphs, and computational hardness must be preserved under edge subdivision of subcubic graphs. Our meta-classification says that if a graph problem $$\Pi $$ satisfies all three conditions, then for every finite set $${{{\mathcal {H}}}}$$ , it is “efficiently solvable” on $${{{\mathcal {H}}}}$$ -subgraph-free graphs if $${\mathcal {H}}$$ contains a disjoint union of one or more paths and subdivided claws, and $$\Pi $$ is “computationally hard” otherwise. We apply our meta-classification on many well-known partitioning, covering and packing problems, network design problems and width parameter problems to obtain a dichotomy between polynomial-time solvability and NP-completeness. For distance-metric problems, we obtain a dichotomy between almost-linear-time solvability and having no subquadratic-time algorithm (conditioned on some hardness hypotheses). Apart from capturing a large number of explicitly and implicitly known results in the literature, we also prove a number of new results. Moreover, we perform an extensive comparison between the subgraph framework and the existing frameworks for the minor and topological minor relations, and pose several new open problems and research directions.},
  archive      = {J_Alg},
  author       = {Johnson, Matthew and Martin, Barnaby and Oostveen, Jelle J. and Pandey, Sukanya and Paulusma, Daniël and Smith, Siani and van Leeuwen, Erik Jan},
  doi          = {10.1007/s00453-024-01289-2},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {429-464},
  shortjournal = {Algorithmica},
  title        = {Complexity framework for forbidden subgraphs i: The framework},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FREIGHT: Fast streaming hypergraph partitioning.
<em>Alg</em>, <em>87</em>(3), 405–428. (<a
href="https://doi.org/10.1007/s00453-024-01291-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning the vertices of a (hyper)graph into k roughly balanced blocks such that few (hyper)edges run between blocks is a key problem for large-scale distributed processing. A current trend for partitioning huge (hyper)graphs using low computational resources are streaming algorithms. In this work, we propose FREIGHT: a Fast stREamInG Hypergraph parTitioning algorithm which is an adaptation of the widely-known graph-based algorithm Fennel. By using an efficient data structure, we make the overall running of FREIGHT linearly dependent on the pin-count of the hypergraph and the memory consumption linearly dependent on the numbers of nets and blocks. The results of our extensive experimentation showcase the promising performance of FREIGHT as a highly efficient and effective solution for streaming hypergraph partitioning. Our algorithm demonstrates competitive running time with the Hashing algorithm, with a geometric mean runtime within a factor of four compared to the Hashing algorithm. Significantly, our findings highlight the superiority of FREIGHT over all existing (buffered) streaming algorithms and even the in-memory algorithm HYPE, with respect to both cut-net and connectivity measures. This indicates that our proposed algorithm is a promising hypergraph partitioning tool to tackle the challenge posed by large-scale and dynamic data processing.},
  archive      = {J_Alg},
  author       = {Eyubov, Kamal and Fonseca Faraj, Marcelo and Schulz, Christian},
  doi          = {10.1007/s00453-024-01291-8},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {405-428},
  shortjournal = {Algorithmica},
  title        = {FREIGHT: Fast streaming hypergraph partitioning},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared versus private randomness in distributed interactive
proofs. <em>Alg</em>, <em>87</em>(3), 377–404. (<a
href="https://doi.org/10.1007/s00453-024-01288-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed interactive proofs, the nodes of a graph G interact with a powerful but untrustable prover who tries to convince them, in a small number of rounds and through short messages, that G satisfies some property. This series of rounds is followed by a phase of distributed verification, which may be either deterministic or randomized, where nodes exchange messages with their neighbors. The nature of this last verification round defines the two types of interactive protocols. We say that the protocol is of Arthur–Merlin type if the verification round is deterministic. We say that the protocol is of Merlin–Arthur type if, in the verification round, the nodes are allowed to use a fresh set of random bits. In the original model introduced by Kol, Oshman, and Saxena [PODC 2018], the randomness was private in the sense that each node had only access to an individual source of random coins. Crescenzi, Fraigniaud, and Paz [DISC 2019] initiated the study of the impact of shared randomness (the situation where the coin tosses are visible to all nodes) in the distributed interactive model. In this work, we continue that research line by showing that the impact of the two forms of randomness is very different depending on whether we are considering Arthur–Merlin protocols or Merlin–Arthur protocols. While private randomness gives more power to the first type of protocols, shared randomness provides more power to the second. We also show that there exists at most an exponential gap between the certificate size in distributed interactive proofs with respect to distributed verification protocols without any randomness.},
  archive      = {J_Alg},
  author       = {Montealegre, Pedro and Ramírez-Romero, Diego and Rapaport, Ivan},
  doi          = {10.1007/s00453-024-01288-3},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {377-404},
  shortjournal = {Algorithmica},
  title        = {Shared versus private randomness in distributed interactive proofs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient algorithm for power dominating set.
<em>Alg</em>, <em>87</em>(3), 344–376. (<a
href="https://doi.org/10.1007/s00453-024-01283-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem Power Dominating Set (PDS) is motivated by the placement of phasor measurement units to monitor electrical networks. It asks for a minimum set of vertices in a graph that observes all remaining vertices by exhaustively applying two observation rules. Our contribution is twofold. First, we determine the parameterized complexity of PDS by proving it is W[P]-complete when parameterized with respect to the solution size. We note that it was only known to be W[2]-hard before. Our second and main contribution is a new algorithm for PDS that efficiently solves practical instances. Our algorithm consists of two complementary parts. The first is a set of reduction rules for PDS that can also be used in conjunction with previously existing algorithms. The second is an algorithm for solving the remaining kernel based on the implicit hitting set approach. Our evaluation on a set of power grid instances from the literature shows that our solver outperforms previous state-of-the-art solvers for PDS by more than one order of magnitude on average. Furthermore, our algorithm can solve previously unsolved instances of continental scale within a few minutes.},
  archive      = {J_Alg},
  author       = {Bläsius, Thomas and Göttlicher, Max},
  doi          = {10.1007/s00453-024-01283-8},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {344-376},
  shortjournal = {Algorithmica},
  title        = {An efficient algorithm for power dominating set},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry breaking in the plane. <em>Alg</em>,
<em>87</em>(3), 321–343. (<a
href="https://doi.org/10.1007/s00453-024-01286-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a fundamental question related to the feasibility of deterministic symmetry breaking in the infinite Euclidean plane for two robots that have minimal or no knowledge of the respective capabilities and “measuring instruments” of themselves and each other. Assume that two anonymous mobile robots are placed at different locations at unknown distance d from each other on the infinite Euclidean plane. Each robot knows neither the location of itself nor of the other robot. The robots cannot communicate wirelessly, but have a certain nonzero visibility radius r (with range r unknown to the robots). By rendezvous we mean that they are brought at distance at most r of each other by executing symmetric (identical) mobility algorithms. The robots are moving with unknown and constant but not necessarily identical speeds, their clocks and pedometers may be asymmetric, and their chirality inconsistent. We demonstrate that rendezvous for two robots is feasible under the studied model iff the robots have either: different speeds; or different clocks; or different orientations but equal chiralities. When the rendezvous is feasible, we provide a universal algorithm which always solves rendezvous despite the fact that the robots have no knowledge of which among their respective parameters may be different.},
  archive      = {J_Alg},
  author       = {Czyzowicz, Jurek and Gąsieniec, Leszek and Killick, Ryan and Kranakis, Evangelos},
  doi          = {10.1007/s00453-024-01286-5},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {321-343},
  shortjournal = {Algorithmica},
  title        = {Symmetry breaking in the plane},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="bcyb---2">BCYB - 2</h2>
<ul>
<li><details>
<summary>
(2025). Antifragile control systems in neuronal processing: A
sensorimotor perspective. <em>BCYB</em>, <em>119</em>(2), 1–23. (<a
href="https://doi.org/10.1007/s00422-025-01003-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability–robustness–resilience–adaptiveness continuum in neuronal processing follows a hierarchical structure that explains interactions and information processing among the different time scales. Interestingly, using “canonical” neuronal computational circuits, such as Homeostatic Activity Regulation, Winner-Take-All, and Hebbian Temporal Correlation Learning, one can extend the behavior spectrum towards antifragility. Cast already in both probability theory and dynamical systems, antifragility can explain and define the interesting interplay among neural circuits, found, for instance, in sensorimotor control in the face of uncertainty and volatility. This perspective proposes a new framework to analyze and describe closed-loop neuronal processing using principles of antifragility, targeting sensorimotor control. Our objective is two-fold. First, we introduce antifragile control as a conceptual framework to quantify closed-loop neuronal network behaviors that gain from uncertainty and volatility. Second, we introduce neuronal network design principles, opening the path to neuromorphic implementations and transfer to technical systems.},
  archive      = {J_BCYB},
  author       = {Axenie, Cristian},
  doi          = {10.1007/s00422-025-01003-7},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Biol. Cybern.},
  title        = {Antifragile control systems in neuronal processing: A sensorimotor perspective},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counteracting uncertainty: Exploring the impact of anxiety
on updating predictions about environmental states. <em>BCYB</em>,
<em>119</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00422-025-01006-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anxious emotional states disrupt decision-making and control of dexterous motor actions. Computational work has shown that anxiety-induced uncertainty alters the rate at which we learn about the environment, but the subsequent impact on the predictive beliefs that drive action control remains to be understood. In the present work we tested whether anxiety alters predictive (oculo)motor control mechanisms. Thirty participants completed an experimental task that consisted of manual interception of a projectile performed in virtual reality. Participants were subjected to conditions designed to induce states of high or low anxiety using performance incentives and social-evaluative pressure. We measured subsequent effects on physiological arousal, self-reported state anxiety, and eye movements. Under high pressure conditions we observed visual sampling of the task environment characterised by higher variability and entropy of position prior to release of the projectile, consistent with an active attempt to reduce uncertainty. Computational modelling of predictive beliefs, using gaze data as inputs to a partially observable Markov decision process model, indicated that trial-to-trial updating of predictive beliefs was reduced during anxiety, suggesting that updates to priors were constrained. Additionally, state anxiety was related to a less deterministic mapping of beliefs to actions. These results support the idea that organisms may attempt to counter anxiety-related uncertainty by moving towards more familiar and certain sensorimotor patterns.},
  archive      = {J_BCYB},
  author       = {Harris, David and Arthur, Tom and Wilson, Mark and Le Gallais, Ben and Parsons, Thomas and Dill, Ally and Vine, Sam},
  doi          = {10.1007/s00422-025-01006-4},
  journal      = {Biological Cybernetics},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Biol. Cybern.},
  title        = {Counteracting uncertainty: Exploring the impact of anxiety on updating predictions about environmental states},
  volume       = {119},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="cc---66">CC - 66</h2>
<ul>
<li><details>
<summary>
(2025). Cognitive-inspired spectral spatiotemporal analysis for
emotion recognition utilizing electroencephalography signals.
<em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10361-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of computer technologies, along with the significant role of emotions in daily life, has driven interest in intelligent emotion recognition systems. Electroencephalography (EEG) serves as a prominent objective tool in affective computing. However, effectively integrating multichannel EEG spatial and temporal information remains a critical challenge. This study introduces a novel emotion recognition model grounded in cognitive and biological principles, emphasizing the importance of spatiotemporal dynamics in emotional processing. In this research, brain frequency bands were extracted through wavelet analysis, and the signals within predefined time windows were quantified. These features were then concatenated across distinct brain channels to create a comprehensive matrix representing spatiotemporal brain information. The matrix was characterized using both the summation of matrix cells and the highest singular value to optimize computational costs during classification. The resulting attributes were input into a classification module for emotion detection. Experimental results on the Database for Emotion Analysis using Physiological Signals (DEAP) achieved a maximum accuracy of 89.55%. This work introduces a novel approach to analyzing and classifying EEG signals elicited by various emotional stimuli, demonstrating that the proposed model is competitive with the state-of-the-art classification schemes, thereby paving the way for future development of a robust spatiotemporal-based EEG emotion recognition system.},
  archive      = {J_CC},
  author       = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
  doi          = {10.1007/s12559-024-10361-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Cognitive-inspired spectral spatiotemporal analysis for emotion recognition utilizing electroencephalography signals},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A levitated controlled attention for named entity
recognition. <em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10381-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlled attention is a mechanism developed in cognitive neuroscience. It has been successfully applied to support named entity recognition, where the start and end boundaries of a possible named entity are marked by two specific tokens to indicate its position in a sentence. Then, it is fed into a deep network for classification. The entity boundary markers enable a deep neural network to be aware of entity boundaries and build the contextual dependency of a sentence relevant to entity boundaries. The problem with this strategy is that every possible named entity must be evaluated independently. This leads to very high computational complexity and cannot construct the semantic dependency between different named entities. In this paper, a levitated controlled attention mechanism is presented for named entity recognition. In this method, all possible named entities are fed together into a deep network for one-pass classification, which can establish the semantic dependency between contextual features and possible named entities. In the experiments, the levitated controlled attention is evaluated on four public datasets. The results show that it not only considerably reduces the computational complexity but also improves the performance of named entity recognition.},
  archive      = {J_CC},
  author       = {Huang, Rong and Chen, Yanping and Huang, Ruizhang},
  doi          = {10.1007/s12559-024-10381-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A levitated controlled attention for named entity recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph neural network with spatial attention for emotion
analysis. <em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10358-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition plays a crucial role in the diagnosis and treatment of various mental disorders. Research studies revealed the close relationship between brain regions and their functional roles in emotions. Propose a learning method that extends graph neural networks and takes into account the spatial relationship between EEG channels and their contributions of different regions of the brain to human emotions. Our method uses the adjacency matrix to model the spatial topological relationships in multi-channel EEG signals and learns weights to adjust their contributions to the classification. Extensive evaluation is conducted using public data sets, including comparison studies with state-of-the-art methods and performance analysis. In our comparison studies, our method demonstrates superior performance in terms of average accuracy. It is demonstrated that the proposed method improves the accuracy of emotion recognition and analyzes the brain at a fine granularity to decide the part that is most related to the triggering of the emotion.},
  archive      = {J_CC},
  author       = {Chen, Tian and Li, Lubao and Yuan, Xiaohui},
  doi          = {10.1007/s12559-024-10358-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A graph neural network with spatial attention for emotion analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining interpretable embedded multicriteria feature
cross-selection engineering and machine learning to mimic the brain for
stock trading signal prediction. <em>CC</em>, <em>17</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s12559-024-10365-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock trading signal prediction is very important for investors’ trading decisions. However, since the stock market is a complex and nonlinear system, stock trading is frequent and complex. Human beings cannot integrate all the relevant information in time and make the right decisions by their brains alone. Machine learning can mimic the brain, learn from experience, and discover the connection between different things, thus realizing correct prediction and decision-making. Therefore, this study innovatively proposes a fusion of interpretable embedded multicriteria feature cross-selection engineering to capture effective features. Meanwhile, an optimized neural network prediction model is proposed where the Bayesian (BO) algorithm assumes the task of searching for hyperparameter combinations. The methods are as follow: (1) Daily stock prices are categorized into four types of key points for stock trading signals based on the time series extreme point algorithm. (2) A more comprehensive range of impact factors is constructed. Starting from the stock’s historical trading data, based on the stock’s trend, volatility, and turnover flow, five categories of technical indicators are constructed: Overlap Study, Momentum Indicator, Momentum Indicator, Volatility Indicator, and Price Conversion. (3) To construct a feature cross-selection method with multiple feature screening criteria to find the optimal feature influencing factors from different evaluation dimensions. (4) The hyper-parameters of the Artificial Neural Network (ANN) are optimized using Bayesian optimization algorithm. The optimized ANN is then used to model the data and obtain predictions. Twenty stocks were randomly selected from Shanghai Stock Exchange and Shenzhen Stock Exchange as experimental data to verify the validity of the model. The accuracy of the model proposed in this paper is 54.83%, 55.46%, and 54.70% for stocks with upward, steady, and downward trends respectively. The accuracy is on average 7.93%, 8.09%, and 8.09% higher than the comparison model. The return on investment through the predicted results of the model is 21.87%, 7.76%, and −3.51% respectively, which is better than the other comparative models. It can be seen from the experiments that the feature cross-selection method with multi-feature screening criterion can help the model to better find the optimal feature influencing factor, which helps to improve the accuracy of prediction. The Bayesian optimization algorithm contributes to the performance improvement of the ANN. After modeling the features using the Bayesian optimized ANN, the stock trading signal prediction model proposed in this paper is significantly better than other prediction models.},
  archive      = {J_CC},
  author       = {Wang, Jujie and Dong, Ying},
  doi          = {10.1007/s12559-024-10365-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Combining interpretable embedded multicriteria feature cross-selection engineering and machine learning to mimic the brain for stock trading signal prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density estimation-based stein variational gradient descent.
<em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10370-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating a target distribution, such as a Bayesian posterior, is important in many areas, including cognitive computation. We introduce a variant of Stein variational gradient descent (SVGD) (Liu and Wang Adv Neural Inf Process Syst 29, 2016), called the density estimation-based Stein variational gradient descent (DESVGD). SVGD has proven to be promising as a sampling method for approximating target distributions. SVGD, however, suffers from discontinuity inherent in the empirical measure, making it difficult to closely monitor the convergence of the sampling-based approximation to the target. DESVGD utilizes kernel density estimation to replace the empirical measure in SVGD with its continuous counterpart. This allows direct computation of the KL divergence between the current approximation and the target distribution, thereby helping to monitor the numerical convergence of the iterative optimization process. DESVGD also offers derivatives of the KL divergence, which can be used to better design learning rates and thus to achieve faster convergence. By simply replacing the kernel used in SVGD with its weighted average, one can easily implement DESVGD based on existing SVGD algorithms. Our numerical experiments demonstrate that DESVGD approximates the target distribution well and outperforms the original SVGD in terms of approximation quality.},
  archive      = {J_CC},
  author       = {Kim, Jeongho and Lee, Byungjoon and Min, Chohong and Park, Jaewoo and Ryu, Keunkwan},
  doi          = {10.1007/s12559-024-10370-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Density estimation-based stein variational gradient descent},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HC3: A three-way clustering method based on hierarchical
clustering. <em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10379-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision is a field of research pertaining to human-inspired computation. Guided by the principle of three-way decision, three-way clustering addresses the information uncertainty problem by using the core region and the fringe region to characterize a cluster. The universe is split into three parts by these two regions, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging to. In recent years, there have been considerable three-way clustering algorithms. However, the generalization and scalability of current three-way cluster algorithms remain relatively weak, with most algorithms adhering to a fixed allocation strategy or fixed threshold parameters. In order to overcome this problem, this paper proposes a multilevel three-way clustering algorithm based on a hierarchical strategy (HC3 for short). The proposed algorithm uses kernel density estimation information of data to adaptively construct a multilevel structure of data, where the higher levels (or the internal layers) with the high-density objects are closer to core regions of clusters, and the lower levels (or the external layers) with the low-density objects are closer to fringe regions of clusters. Under the multilevel structure, we establish a three-way allocation strategy based on the stability of subclass clusters, obtaining the correct attribution of data after fully considering neighboring information. The experiments are conducted on 13 data sets with different dimensions. By comparing to other 8 clustering algorithms, the effectiveness of the proposed HC3 is verified through accuracy (ACC), adjusted Rand index (ARI), and adjusted mutual information (AMI).},
  archive      = {J_CC},
  author       = {Guan, Wenrui and Wang, Pingxin and Jiang, Wengang and Zhang, Ying},
  doi          = {10.1007/s12559-024-10379-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {HC3: A three-way clustering method based on hierarchical clustering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-head attention and long short-term network for
enhanced inpainting of occluded handwriting. <em>CC</em>,
<em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10382-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of handwritten character recognition, inpainting occluded offline characters is essential. Relying on the remarkable achievements of transformers in various tasks, we present a novel framework called “Enhanced Inpainting with Multi-head Attention and stacked long short-term memory (LSTM) Network” (E-Inpaint). This framework aims to restore occluded offline handwriting while capturing its online signal counterpart, enriched with dynamic characteristics. The proposed approach employs Convolutional Neural Network (CNN) and Multi-Layer Perceptron (MLP) in order to extract essential hidden features from the handwriting image. These features are then decoded by stacked LSTM with Multi-head Attention, achieving the inpainting process and generating the online signal corresponding to the uncorrupted version. To validate our work, we utilize the recognition system Beta-GRU on Latin, Indian, and Arabic On/Off dual datasets. The obtained results show the efficiency of using stacked-LSTM network with multi-head attention, enhancing the quality of the restored image and significantly improving the recognition rate using the innovative Beta-GRU system. Our research mainly highlights the potential of E-Inpaint in enhancing handwritten character recognition systems.},
  archive      = {J_CC},
  author       = {Rabhi, Besma and Elbaati, Abdelkarim and Hamdi, Yahia and Dhahri, Habib and Pal, Umapada and Chabchoub, Habib and Ouahada, Khmaies and Alimi, Adel M.},
  doi          = {10.1007/s12559-024-10382-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-head attention and long short-term network for enhanced inpainting of occluded handwriting},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of artificial neural network computing systems.
<em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10383-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An artificial neural network (ANN) is currently used in multiple different applications such as bio-medicine, finance, Internet, and mobile networks. Since their inception, many advances have taken place introducing new models and features. Such progress resulted in different ANN models and most importantly different types of implementation, which vary from software (SW) to hardware (HW) following specific development principles. Researchers have been working significantly the last decade in this area tackling with different aspects of ANN’s implementations. In this survey, we present the progress of ANN in terms of implementation as part of computing platforms. Thus, we present the ANN-enabled computing platforms in terms of algorithmic models, computing architectures, and SW/HW implementations. This work concludes with open challenges and lessons learned in order to summarize what is potentially useful for further research in the area of ANN computing platforms with a wide spectrum of applications. An artificial neural network (ANN) is considered the key element of future computing systems applied to different domains. While the algorithmic design of an ANN is one of the major engineering elements, the implementation of ANN is equally important with many difficulties that should be overcome by future engineers. This survey aims to provide a comprehensive tutorial about the ANN-enabled computing systems, i.e., computing architectures with embedded artificial intelligence (AI). Starting with the ANN models and their applications, the survey provides a taxonomy of the types of ANN computing systems. Both SW and HW implementations are provided for each of those types, which highlight the key architectural elements as well as the performance of the ANN-enabled computing systems. Open challenges and lessons learned follow to provide a discussion for future research in the area of AI computing systems.},
  archive      = {J_CC},
  author       = {Foukalas, Fotis},
  doi          = {10.1007/s12559-024-10383-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {A survey of artificial neural network computing systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-type image coding method acting on supervised
hierarchical deep spiking convolutional neural networks for image
classification. <em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-024-10355-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have gained significant momentum in recent times as they transmit information via discrete spikes, similar to neuromorphic low-power systems. However, existing spike coding methods are often limited to a single scale of time or rate, and typically suffer from drawbacks such as reduced accuracy or long classification latency. In this paper, we propose a pixel-based multi-type image coding (PMIC) method inspired by the functional organization of primate visual systems to address the issues at hand. The encoded information comprises both spatial and temporal details, represented by spiking firing time and intensity, respectively. Subsequently, we combine the spiking firing time and intensity as inputs of a hierarchical spiking convolutional neural network (SCNN) including several convolutional and pooling layers. During the training phase, we use error backpropagation to optimize parameters. Comparison of experimental results with some state-of-the-art approaches on MNIST dataset, Fashion-MNIST dataset as well as ETH-80 dataset of image classification demonstrates that SCNN using PMIC can achieve the best test accuracy, which is 99.13%, 90.31%, and 94.29%, respectively. The proposed PMIC utilizes multiple filters and coding strategies to extract multi-type information and is more beneficial to the performance of SNNs compared to methods that extract single-scale or single-type information.},
  archive      = {J_CC},
  author       = {Liu, Fang and Xu, Jialin and Yang, Jie and Wu, Wei},
  doi          = {10.1007/s12559-024-10355-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-type image coding method acting on supervised hierarchical deep spiking convolutional neural networks for image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRCFusionAICADx: Integrative CNN-LSTM approach for accurate
colorectal cancer diagnosis in colonoscopy images. <em>CC</em>,
<em>17</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s12559-024-10357-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer (CRC) is a critical health issue worldwide and is very treatable if diagnosed on time. This paper proposes an innovative CADx system, namely CRCFusionAICADx, that enhances the efficiency of diagnosis by fusing CNNs with LSTM networks and feature integration techniques. Using data from the CKHK-22 colonoscopy image dataset, we preprocess the images into grayscale first and then apply LBP analysis for emphasizing textural features. These are further analyzed using three different pre-trained CNN models: VGG16, DenseNet-201, and ResNet50. These were chosen because of their complementary feature extraction capabilities. The resultant features from grayscale, LBP, and raw images will be fused to create an integrated dataset. To increase variability in the dataset and reduce overfitting for the network, we decided to apply a series of data augmentation techniques, which included zooming in, rotation, and horizontal flipping. By doing so, we expanded the dataset into 57,148 images. This augmented dataset is then used to train a model, RDV-22, which includes an integration of the architectures of VGG16, DenseNet-201, and ResNet50, with CNN and CNN + LSTM layers. The LSTM network learns the temporal dependencies of frames in a sequence and hence allows for more sensitive and specific detection of CRC. CRCFusionAICADx produces very impressive results, where the RDV-22 model produces a testing accuracy of 90.81%, precision of 91.00%, recall of 90.00%, and an F1 score of 90.49% in its results. This gives the model an ROC AUC of 0.98, reflecting very strong discriminatory power. This integrative approach thus shows tremendous promise for early CRC detection by offering a strong diagnostic tool that integrates both spatial and temporal features into a new standard in clinical diagnostics.},
  archive      = {J_CC},
  author       = {Raju, Akella S. Narasimha and Jayavel, Kayalvizhi and Rajalakshmi, Thulasi and Rajababu, M.},
  doi          = {10.1007/s12559-024-10357-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Cogn. Comput.},
  title        = {CRCFusionAICADx: Integrative CNN-LSTM approach for accurate colorectal cancer diagnosis in colonoscopy images},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided diagnosis of graphomotor difficulties
utilizing direction-based fractional order derivatives. <em>CC</em>,
<em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12559-024-10360-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children who do not sufficiently develop graphomotor skills essential for handwriting often develop graphomotor disabilities (GD), impacting the self-esteem and academic performance of the individual. Current examination methods of GD consist of scales and questionaries, which lack objectivity, rely on the perceptual abilities of the examiner, and may lead to inadequately targeted remediation. Nowadays, one way to address the factor of subjectivity is to incorporate supportive machine learning (ML) based assessment. However, even with the increasing popularity of decision-support systems facilitating the diagnosis and assessment of GD, this field still lacks an understanding of deficient kinematics concerning the direction of pen movement. This study aims to explore the impact of movement direction on the manifestations of graphomotor difficulties in school-aged. We introduced a new fractional-order derivative-based approach enabling quantification of kinematic aspects of handwriting concerning the direction of movement using polar plot representation. We validated the novel features in a barrage of machine learning scenarios, testing various training methods based on extreme gradient boosting trees (XGBboost), Bayesian, and random search hyperparameter tuning methods. Results show that our novel features outperformed the baseline and provided a balanced accuracy of 87 % (sensitivity = 82 %, specificity = 92 %), performing binary classification (children with/without graphomotor difficulties). The final model peaked when using only 43 out of 250 novel features, showing that XGBoost can benefit from feature selection methods. Proposed features provide additional information to an automated classifier with the potential of human interpretability thanks to the possibility of easy visualization using polar plots.},
  archive      = {J_CC},
  author       = {Gavenciak, Michal and Mucha, Jan and Mekyska, Jiri and Galaz, Zoltan and Zvoncakova, Katarina and Faundez-Zanuy, Marcos},
  doi          = {10.1007/s12559-024-10360-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Computer-aided diagnosis of graphomotor difficulties utilizing direction-based fractional order derivatives},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fixed-time control algorithm for multiagent systems with
input delay via event-triggered strategy. <em>CC</em>, <em>17</em>(1),
1–12. (<a href="https://doi.org/10.1007/s12559-024-10366-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent systems have become increasingly relevant in numerous fields due to their potential for cooperative control and distributed networks. Developing effective adaptive algorithms for these systems is crucial, as adaptive techniques can enhance the ability of multiple agents to collaboratively complete specific tasks. In this paper, we propose an event-triggered adaptive fixed-time containment algorithm for multiagent systems with input delay. Firstly, our methodology employs Fuzzy Logic Systems to approximate uncertain terms in system dynamics and addresses the algebraic loop problem inherent in non-strict feedback functions. Additionally, an integral term is introduced to mitigate the adverse effects of time delay on system performance. Under the fixed-time stability framework, the proposed algorithm achieves containment within an exact time constant, independent of initial conditions. Furthermore, a dynamic event-triggered mechanism is incorporated to optimize the number of triggers, thereby conserving communication resources. As a result, the designed control algorithm guarantees that the closed-loop systems signals meet the criteria for semi-global practical fixed-time stability, allowing the outputs of followers to converge to the convex hull of leaders within a finite time. Finally, the feasibility and effectiveness of the algorithm are demonstrated through simulations involving underwater vehicle systems.},
  archive      = {J_CC},
  author       = {Liu, Guijiang and Wang, Xin and Guang, Weiwei and Chen, Hongyu},
  doi          = {10.1007/s12559-024-10366-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A fixed-time control algorithm for multiagent systems with input delay via event-triggered strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid semantics and syntax-based graph convolutional
network for aspect-level sentiment classification. <em>CC</em>,
<em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12559-024-10367-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-level sentiment classification seeks to ascertain the sentiment polarities of individual aspects within a sentence. Most existing research in this field focuses on individually assessing the importance of contexts on individual aspects, disregarding the negative impact of imbalanced relations between aspects due to their mutual influence. This paper presents a hybrid semantics and syntax-based graph convolutional network (SS-GCN) for aspect-level sentiment classification. This model addresses the imbalanced limitation by creating aspects-based balance relations between the strengths and weaknesses of different aspects through an auxiliary task. Furthermore, the multi-head self-attention mechanism utilizes position-enhanced encoding to identify the most relevant aspects of the current word. Extensive experiments demonstrate that SS-GCN outperforms other baselines in terms of classification performance. Compared to state-of-the-art methods, SS-GCN significantly improves 0.39–1.66% in accuracy and 0.43–1.92% in Macro-F1 on the SemEval 14-15 and MAMS datasets.},
  archive      = {J_CC},
  author       = {Huang, Chen and Li, Xianyong and Du, Yajun and Dong, Zhicheng and Huang, Dong and Kumar Jain, Deepak and Hussain, Amir},
  doi          = {10.1007/s12559-024-10367-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {A hybrid semantics and syntax-based graph convolutional network for aspect-level sentiment classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DmrNet: Dual-stream mutual information contraction and
re-discrimination network for semi-supervised temporal action detection.
<em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10374-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised temporal action detection only requires a small number of labeled samples from the dataset and utilizes the remaining unlabeled samples for model training, effectively alleviating the significant time and manpower costs associated with annotating large-scale temporal action detection datasets. However, previous semi-supervised temporal action detection methods relied on sequential action localization and classification, which leads to erroneous localization predictions that can easily affect subsequent classification predictions, resulting in error propagation problem. To overcome error propagation, we propose a dual-stream mutual information contraction and re-discrimination network (DmrNet). Specifically, the traditional two-step strategy of temporal action detection has been changed to a four-step parallel strategy by us. Firstly, this paper designs the first-step classification prediction and the second-step localization prediction as a parallel structure to prevent error propagation from localization to classification. Then, in the third step, the dual-stream mutual information contraction part maps the dual-stream features to a new vector space to ensure the cross-correlation between classification and action localization. Finally, the fourth step of classification re-discrimination part captures the consistency information of the dual-stream structure to enhance internal representation. Compared with existing methods, DmrNet achieved an average accuracy improvement of 10.7% on ActivityNet v1.3 and 5.2% on THUMOS14 using only 10% annotation data. The experimental results show that the proposed DmrNet not only achieves good detection performance in semi-supervised learning but also achieves performance comparable to state-of-the-art methods in fully supervised learning.},
  archive      = {J_CC},
  author       = {Zhang, Qiming and Hu, Zhengping and Wang, Yulu and Bi, Shuai and Zhang, Hehao and Di, Jirui},
  doi          = {10.1007/s12559-024-10374-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {DmrNet: Dual-stream mutual information contraction and re-discrimination network for semi-supervised temporal action detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event classification on subsea pipeline inspection data
using an ensemble of deep learning classifiers. <em>CC</em>,
<em>17</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12559-024-10377-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsea pipelines are the backbone of the modern oil and gas industry, transporting a total of 28% of global oil production. Due to several factors, such as corrosion or deformations, the pipelines might degrade over time, which might lead to serious economic and environmental damages if not addressed promptly. Therefore, it is crucial to detect any serious damage to subsea pipelines before they cause dangerous catastrophes. Inspections of subsea pipelines are usually made using a Remote Operating Vehicle and the inspection data is usually processed manually, which is subject to human errors, and requires experienced Remote Operating Vehicle operators. It is thus necessary to automate the inspection process to enable more efficiency as well as reduce costs. Besides, it is recognised that specific challenges of noisy and low-quality inspection data arising from the underwater environment prevent the industry from taking full advantage of the recent development in the Artificial Intelligence field to the problem of subsea pipeline inspection. In this paper, we developed an ensemble of deep learning classifiers to further improve the performance of single deep learning models in classifying anomalous events on the subsea pipeline inspection data. The output of the proposed ensemble was combined based on a weighted combining method. The weights of base classifiers were found by minimising the difference between the weighted combining result and the given associated ground truth annotation information. Three inspection datasets, gathered from different oil and gas companies in the United Kingdom, were analysed. These datasets were recorded under varying conditions and include a range of anomalies. The results showed that the proposed ensemble achieves around 78% accuracy on two datasets and more than 99% accuracy on one dataset, which is better compared to base classifiers and two popular ensembles.},
  archive      = {J_CC},
  author       = {Dang, Truong and Nguyen, Tien Thanh and Liew, Alan Wee-Chung and Elyan, Eyad},
  doi          = {10.1007/s12559-024-10377-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Event classification on subsea pipeline inspection data using an ensemble of deep learning classifiers},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DisTrack: A new tool for semi-automatic misinformation
tracking in online social networks. <em>CC</em>, <em>17</em>(1), 1–18.
(<a href="https://doi.org/10.1007/s12559-024-10378-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces DisTrack, a methodology and a tool developed for tracking and analyzing misinformation within online social networks (OSNs). DisTrack is designed to combat the spread of misinformation through a combination of natural language processing (NLP) social network analysis (SNA) and graph visualization. The primary goal is to detect misinformation, track its propagation, identify its sources, and assess the influence of various actors within the network. DisTrack’s architecture incorporates a variety of methodologies including keyword search, semantic similarity assessments, and graph generation techniques. These methods collectively facilitate the monitoring of misinformation, the categorization of content based on alignment with known false claims, and the visualization of dissemination cascades through detailed graphs. The tool is tailored to capture and analyze the dynamic nature of misinformation spread in digital environments. The effectiveness of DisTrack is demonstrated through three case studies focused on different themes: discredit/hate speech, anti-vaccine misinformation, and false narratives about the Russia-Ukraine conflict. These studies show DisTrack’s capabilities in distinguishing posts that propagate falsehoods from those that counteract them, and tracing the evolution of misinformation from its inception. The research confirms that DisTrack is a valuable tool in the field of misinformation analysis. It effectively distinguishes between different types of misinformation and traces their development over time. By providing a comprehensive approach to understanding and combating misinformation in digital spaces, DisTrack proves to be an essential asset for researchers and practitioners working to mitigate the impact of false information in online social environments.},
  archive      = {J_CC},
  author       = {Villar-Rodríguez, Guillermo and Huertas-García, Álvaro and Martín, Alejandro and Huertas-Tato, Javier and Camacho, David},
  doi          = {10.1007/s12559-024-10378-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {DisTrack: A new tool for semi-automatic misinformation tracking in online social networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust synchronization of stochastic markovian jumping CVNs
with randomly occurring nonlinearities and generally uncertain
transition rates. <em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10362-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article intents to the robust synchronization issue for Markovian jumping complex-valued networks (CVNs) subject to the stochastic noises and randomly occurring nonlinearities, where the considered transition rates are generally uncertain, which expands the existed relevant results. Meanwhile, two random variables with pre-given statistical characteristics are proposed to explain the involved randomly occurring nonlinearities phenomenon, and random variables are mutually independent. By designing the appropriate mode-dependent controller, combined with generalized complex It $$\hat{o}$$ ’s formula, Lyapunov stability theory, and the properties of the transition rate matrix, some sufficient conditions are achieved to ensure error system realizes stochastically asymptotically mean-square stable. Furthermore, sufficient mode/delay-dependent criteria of globally exponential synchronization for considered CVNs are also investigated. In the end, two illustrative examples with simulations are proposed to verify the effectiveness and feasibility of the designed control schemes.},
  archive      = {J_CC},
  author       = {Li, Qiang and Wei, Hanqing and Hua, Dingli and Wang, Jinling and Zheng, Yuanshi},
  doi          = {10.1007/s12559-024-10362-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Robust synchronization of stochastic markovian jumping CVNs with randomly occurring nonlinearities and generally uncertain transition rates},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensions and detailed analysis of synergy between
traditional classification and classification based on negative features
in deep convolutional neural networks. <em>CC</em>, <em>17</em>(1),
1–16. (<a href="https://doi.org/10.1007/s12559-024-10369-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, deep convolutional neural networks became an irreplaceable tool for pattern recognition in many different machine learning applications, especially in image classification. On the other hand, these models are often used in critical systems which are the reason for new and recent research regarding their robustness and reliability. One of the most important issues for these models is their susceptibility to different adversarial attacks. In our previous work Milošević and Racković (Neural Network World. 2019;29(4):221–34), and Milošević and Racković (Neural Comput Applic. 2021;33:7593–602), the new type of learning applicable to all the convolutional neural networks was introduced: the classification based on the negative features and the synergy of traditional and those newly introduced network models. In the case of partial inputs/image occlusion, it was shown that our new method creates models that are more robust and perform better when compared to traditional models of the same architecture. In this paper, some extensions of the earlier proposed synergy are given by introducing negatively trained features and additional synergy between four independent neural network models. A detailed analysis of the robustness of the newly proposed model is performed on EMNIST and CIFAR-10 image classification data sets in the case of the selected input occlusions and adversarial attacks. The newly proposed neural network architecture improves the robustness of the neural network and increases its resistance to various types of input damage and adversarial attacks.},
  archive      = {J_CC},
  author       = {Racković, Miloš and Vidaković, Jovana and Milošević, Nemanja},
  doi          = {10.1007/s12559-024-10369-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Extensions and detailed analysis of synergy between traditional classification and classification based on negative features in deep convolutional neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EXplainable AI for word embeddings: A survey. <em>CC</em>,
<em>17</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12559-024-10373-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, word embeddings have become integral to natural language processing (NLP), offering sophisticated machine understanding and manipulation of human language. Yet, the complexity of these models often obscures their inner workings, posing significant challenges in scenarios requiring transparency and explainability. This survey conducts a comprehensive review of eXplainable artificial intelligence (XAI) strategies focused on enhancing the interpretability of word embeddings. By classifying the existing body of work into six broad categories based on their methodological approaches—a classification that, to our knowledge, does not exist in the literature—we provide a structured overview of current techniques and their characteristics. Additionally, we uncover a noteworthy oversight: a predominant emphasis on interpreting model outputs at the expense of exploring the models’ internal mechanics. This finding underscores the necessity of shifting research efforts toward not only clarifying the results these models produce but also demystifying the models themselves. Such a shift is crucial for uncovering and addressing biases inherent in word embeddings, thus ensuring the development of fair and trustworthy AI systems. Through this analysis, we identify key research questions for future studies and advocate for a holistic approach to transparency in word embeddings, encouraging the research community to explore both the outcomes and the underlying algorithms of these models.},
  archive      = {J_CC},
  author       = {Boselli, Roberto and D’Amico, Simone and Nobani, Navid},
  doi          = {10.1007/s12559-024-10373-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {EXplainable AI for word embeddings: A survey},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence inspired task offloading and
resource orchestration in intelligent transportation systems.
<em>CC</em>, <em>17</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s12559-024-10380-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Vehicles (IoV) applications require the support of communication, caching, and computation (3C) resources to offload the computation-intensive tasks and for uplifting the traffic conditions in the development of sustainable smart cities. Intelligent Transportation Systems (ITS) lack the integrated ecosystems of addressing the low-latency task handovers, resource management issues, and centralized incentivization strategies. Digital Twin (DT) aids in capturing the real-time varying resource needs of the vehicles and the communication infrastructure that will regulate the task offloading process and facilitates in incentivizing the vehicular instances. In this manuscript, we establish a digital twin counterpart ( $$DT_{PIoV}$$ ) of the physical IoV (PIoV) to meet the QoS requirements during dynamic offloading and the time-varying resource supply–demand of computationally intensive applications. We formulate a response delay minimization function which is solved by the proposed DT-driven context-aware dynamic offloading method (CADOM). Furthermore, we use M/M/1/N/FCFS queueing method that combats the drawbacks of handling the simultaneous deadline-based tasks in a volatile environment of PIoV. In addition, we also maximize the utilities of vehicle and RSU service satisfaction by employing a reward-based mechanism for on-demand allocation of resources based on the Stackelberg game, where the DT of vehicle is deemed as a leader and service provider RSUs as a follower. The simulation results establish that the proposed system outpaces the conventional traffic management system by emphasizing the role of $$DT_{PIoV}$$ in jointly optimizing the overall response latency for different task sizes and also ensure a better utility satisfaction by catering on-demand resource allocation.},
  archive      = {J_CC},
  author       = {Rawlley, Oshin and Gupta, Shashank and Chandrakar, Jyotsana and Johnson, Manisha K. and Kalra, Chahat},
  doi          = {10.1007/s12559-024-10380-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Cogn. Comput.},
  title        = {Artificial intelligence inspired task offloading and resource orchestration in intelligent transportation systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned BERT algorithm-based automatic query expansion
for enhancing document retrieval system. <em>CC</em>, <em>17</em>(1),
1–16. (<a href="https://doi.org/10.1007/s12559-024-10354-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online retrieval systems are mostly web-based, which makes document collecting more dynamic or fluid than in traditional information retrieval systems. With the web growing in size every day, finding meaningful information on it using a search query consisting of only a few keywords which has become increasingly difficult. One important factor in making Internet searches better is query expansion, or QE. Manual query expansion method involves the user adding terms to the query, which takes a long time but produces good results. However, the automatic query expansion (AQE) method determines the best statements with minimal time consumption. Therefore, to improve document retrieval system, a fine-tuned BERT algorithm is developed for automatic query expansion. Initially, the input text was augmented using embedding augmentation (EA) approach. The augmented text was pre-processed using tokenization, normalization, splitting, stemming, stop word removal, as well as lemmatization. Then extracting the technical keywords from the pre-processed text using co-occurrence statistical information. After extracting the keywords, a fine-tuned BERT model is utilized for expanding the query to improve document retrieval system. The hyper parameters present in the BERT was tuned using frilled lizard optimization to enhance the performance of the BERT model. Proposed model provides 92% accuracy, 95% precision, and 95.6% recall. Thus, a fine-tuned BERT model minimizing query-document mismatch and thereby improving retrieval performance.},
  archive      = {J_CC},
  author       = {Vishwakarma, Deepak and Kumar, Suresh},
  doi          = {10.1007/s12559-024-10354-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Fine-tuned BERT algorithm-based automatic query expansion for enhancing document retrieval system},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic neighborhood selection for context aware temporal
evolution using graph neural networks. <em>CC</em>, <em>17</em>(1),
1–19. (<a href="https://doi.org/10.1007/s12559-024-10359-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNN) have seen significant growth recently for modeling temporal evolution in dynamic networks. Representation of complex networks in the form of graph data structures has enabled researchers to study how entities within these networks interact with each other. These interactions evolve over time. Developing a generic methodology for modeling this temporal evolution in complex networks for tracking evolving relationships has been a significant challenge. Most of the existing methods fail to extract contextual representations of historical neighborhood interactions for future link prediction. To address these challenges, this paper presents a novel method for modeling temporal evolution in complex networks using GNNs. A Context-Aware Graph Temporal Neural Network (CATGNN) method that uses dynamic neighborhood selection based on common neighbors for a given node is presented. The method uses dynamic neighborhood selection using contextual embeddings extracted from the historical interactions of the down-sampled set of neighbors of a central node based on a common neighborhood. Fixed-sized contextual memory modules are constructed for each node that store the historical interactions of its neighbors and are updated based on the recency and significance of interactions. The proposed method has been evaluated using six real-world datasets and has comparable performance against state-of-the-art methods, both in terms of accuracy and efficiency. It shows an improvement of 7.52 to 0.05% over the baselines in terms of average precision. The results demonstrate that the proposed CATGNN model can capture complex patterns of change that are difficult to identify using traditional techniques by propagating information over the graph structure. The model can be applied in various fields involving complex systems.},
  archive      = {J_CC},
  author       = {Zeb, Muhammad Ali and Uddin, M. Irfan and Alarood, Ala Abdulsalam and Shafiq, Muhammad and Habibullah, Safa and Alsulami, Abdulkream A.},
  doi          = {10.1007/s12559-024-10359-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Dynamic neighborhood selection for context aware temporal evolution using graph neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuroInteract: An innovative deep learning strategy for
effective drug repositioning in schizophrenia therapy. <em>CC</em>,
<em>17</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12559-024-10384-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia (SCZ) is a serious physiological and neurological disorder that affects an individual’s perception of factuality. It expresses different symptoms such as thinking, aberrant behavior, delusions, and hallucinations. An efficient approach for inferring potential indications for drugs is through drug repositioning. In this context, drug repositioning imparts a valuable strategy to gain safer, faster, and potentially efficient treatment options to improve schizophrenia therapy. Current treatments are insufficient and existing drug repositioning methods are unsuccessful in solving the drug-disease interactions’ difficulties, including long-term efficacy, drug synergy, and capturing genetic variations. Also, existing methods are restrained because of the incapacity to efficiently integrate heterogeneous biomedical data, which results in suboptimal predictions. This research introduces a NeuroInteract model using deep learning in order to predict candidate drugs for SCZ therapy. The proposed model enhances the accuracy of drug repositioning through the collection of various data sources such as genetic information and drug-disease associations. The novelty of the proposed model is the utilization of the heterogeneous data network that is integrated with the progressive optimization model for the purpose of improving prediction accuracy. The developed method imparts effective learning from various data characteristics through the integration of various types of neural network layers such as fully connected layers, convolutional layers, recurrent layers, and graph convolutional layers. The collected data from DrugBank 5.0 and repoDB undergoes a process of data integration, which aids in generating precise predictions for candidate drugs for repositioning. A data pre-processing technique is employed to improve the data quality. After data pre-processing, the proposed method effectively extracts the meaningful features and finds the spatial dependencies to predict the potential candidate drugs for SCZ treatment. Also, it efficiently handles sequential dependencies and genetic information. The oppositional crossover boosted meerkat optimization (OCMO) algorithm is deployed to optimize the performance of the model. The OCMO optimizes the learning process and enhances the model accuracy by dynamically adjusting its search strategy. Ultimately, comprehensive experimental analyses are conducted using several estimation parameters. The proposed method gains greater effectiveness and outperforms existing methods in drug repositioning. The developed method reaches an accuracy of 98.84% and a hit rate of 98.76%. These experimental findings ascertain the ability of NeuroInteract to find promising drugs for repurposing, furnishing a robust and more cost-effective model for SCZ treatment.},
  archive      = {J_CC},
  author       = {J., Sherine Glory and P., Durgadevi and P., Ezhumalai},
  doi          = {10.1007/s12559-024-10384-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {NeuroInteract: An innovative deep learning strategy for effective drug repositioning in schizophrenia therapy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel interpretable graph convolutional neural network for
multimodal brain tumor segmentation. <em>CC</em>, <em>17</em>(1), 1–25.
(<a href="https://doi.org/10.1007/s12559-024-10387-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have revolutionized computer vision, demonstrating remarkable performance in various tasks. However, their end-to-end learning strategy poses challenges to explainability. In this work, we explore the application of explainability techniques in brain tumor segmentation using magnetic resonance imaging (MRI) data. Our adaptive learning class activation map (AL-CAM) employs a unique multiple-pop-out training strategy and contrastive learning to enhance internal outputs, improving interpretability. Additionally, we introduce a novel approach to explainability in graph convolutional neural networks (GCNNs). The usage of traditional CNN interpretability tools such as saliency maps, CAM, and EB are often unable to handle the complexity of graph-structured data. Our work brim this gap by adapting and improving these techniques for graph convolutional neural networks (GCNN). We present two innovative tools: adaptive CAM for differentiated interpretability and contrastive EB for deeper insights into functions. Using a novel feature fusion approach, we further push the boundaries and combine the feature strengths of GNN and CNN for a holistic understanding of GCNN decision-making. Our proposed framework enables interpretability in various areas, not just medical imaging. Our work demonstrates the versatility of explainability methods and demonstrates their power in unlocking the secrets of GCNNs and ultimately solving real-world challenges, particularly in the field of medical image analysis.},
  archive      = {J_CC},
  author       = {Arshad Choudhry, Imran and Iqbal, Saeed and Alhussein, Musaed and Aurangzeb, Khursheed and Qureshi, Adnan N. and Hussain, Amir},
  doi          = {10.1007/s12559-024-10387-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive decision-making system for behavior analysis
among young adults. <em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10372-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global spread of the pandemic, secure isolation regulations, logistical limitations, and delays in reopening educational institutions such as colleges and universities have all had a severe psychological impact. Students, in particular, are regarded as a vulnerable population, experiencing higher levels of fear, stress, depression, and unhealthy eating compared to the general population. To reduce these psychological consequences, the study provides a multi-dimensional evaluation approach to bridge the gap between governments and health institutions in preventing and controlling biological hazards, such as mental illness among students. In the aftermath of the pandemic, this study presents a comprehensive evaluation approach designed to mitigate the psychological impact on students by connecting governments and health institutions in preventing and controlling biological hazards, particularly mental illness. To establish complex and vague data concerning the discussed communities, psychological details were obtained in the complex spherical fuzzy $$ \mathscr {N}$$ -soft context. An enhanced group decision-making methodology was then established in two phases. Initially, weight analytics were defined using the Reyni entropy technique. In the subsequent phase, the Combined Compromise Solution (CoCoSo) approach was applied to examine the possibilities. Students attending schools and colleges experience significant psychological impacts. To evaluate these effects, an analytical study was conducted, suggesting that improved educational amenities are necessary to mitigate these psychological consequences. Furthermore, the study validates the significance of the proposed decision system through its analysis. A unique score function is suggested for analyzing the psychological consequences among adults because it effectively addresses the ambiguity in periodic data, resulting in accurate and consistent judgments within a two-dimensional framework. Experts thoroughly analyzed the data using the complex spherical fuzzy $$ \mathscr {N}$$ -soft set-integrated CoCoSo method, and its limitations were also addressed.},
  archive      = {J_CC},
  author       = {Pragathi, Subramaniam and Narayanamoorthy, Samayan and Pamucar, Dragan and Kang, Daekook},
  doi          = {10.1007/s12559-024-10372-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {An adaptive decision-making system for behavior analysis among young adults},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced self-attention-based rapid CNN for detecting dense
objects in varying illumination. <em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10376-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of efficient detection of densely arranged unordered items under varying illumination. Specifically, a novel convolutional neural network-based method is proposed for item vector detection, recognition, and classification, termed Self-Attention and Concatenation-Based Detector (ACDet). In a benchmark pharmaceutical case study, rapid and accurate detection of pharmaceutical package contours is achieved, enabling the automatic and fast verification of both the quantity and types of pharmaceuticals during distribution. At the input stage, a combined image augmentation method is applied to improve the detection model’s ability to learn the appearance features of items from multiple angles. Based on YOLOv8 model, integrating computational module C2F with Attention (C2F-A), multidimensional self-attention reinforcement is applied to the outputs of multiple gradient streams. The designed Weighted Concatenation (WConcat) module self-learns to weight and concatenate multi-level feature maps, enhancing the model’s cognitive capability. Finally, simulation experiments are conducted to determine the optimal timing for utilizing each module. Simulation experiments compared the proposed ACDet with several state-of-the-art YOLO architecture models utilizing the benchmark Comprehensive Pharmaceutical Package Dataset (CPPD). ACDet achieved 81.0% mAP and 79.5% Smooth mAP on the CPPD dataset, outperforming other models by an average of 5.5% to 16.6%. On public datasets, the results were 52.2% and 51.0%, respectively. The impact of utilizing C2F-A at different stages on performance was also tested, concluding that the WConcat module does not necessitate spatial attention. Finally, in zero-shot testing, the verification success rate reached 99.91%. Our work shows that the proposed ACDet can overcome many challenges in complex object detection scenarios, enhancing robustness while maintaining a lightweight design. The proposed model can serve as a new benchmark.},
  archive      = {J_CC},
  author       = {Chen, Lu and Yang, Li and Jie, Tan and Haoyuan, Ma and Yu, Liu and Shenbing, Fu and Wang, Junkang and Wu, Hao and Li, Gun},
  doi          = {10.1007/s12559-024-10376-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Enhanced self-attention-based rapid CNN for detecting dense objects in varying illumination},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multitasking with adaptive tradeoff selection
strategy. <em>CC</em>, <em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-024-10386-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new emerging evolutionary framework, evolutionary multitasking aims to optimize multiple tasks simultaneously. Knowledge transfer is an important component of evolutionary multitasking. How to extract and transfer knowledge significantly affects the performance of the algorithm. A serious challenge for evolutionary multitasking is the inappropriate knowledge transfer or insufficient exploration and exploitation. To address this challenge, an evolutionary multitasking with adaptive tradeoff selection strategy (EMT-ATS) is proposed. To enhance global exploration and local exploitation during the evolution, an adaptive tradeoff selection mechanism is developed to select promising offspring during different stages to guide the population toward more promising solution regions. In addition, a Cohen’s d indicator-based is used to adjust knowledge transfer. To verify the effectiveness of the proposed EMT-ATS, a series of experiments are conducted with several popular evolutionary multitasking algorithms on multitasking benchmark problems. In addition, a multitask optimization problem involving two real-world problems is used to validate the practicability of the proposed EMT-ATS. Experimental results demonstrate the effectiveness of the proposed EMT-ATS.},
  archive      = {J_CC},
  author       = {Li, Wei and Zhou, Yinhui and Wang, Lei},
  doi          = {10.1007/s12559-024-10386-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Evolutionary multitasking with adaptive tradeoff selection strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive unsupervised graph convolution network for data
clustering with graph reconstruction. <em>CC</em>, <em>17</em>(1), 1–14.
(<a href="https://doi.org/10.1007/s12559-024-10364-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph clustering has emerged as one of the most challenging problems in the field of deep learning. With the increasing complexity of real-world networks, such as social and biological networks, more and more effective methods are needed to organize and understand these structures. Various cognition-based techniques have been explored for classifying nodes within these graphs, and graph convolution networks (GCNs) have attracted great interest. GCNs, a deep semi-supervised learning approach, provide a powerful framework for learning node representations by utilizing both local and global graph information. By iteratively aggregating information from neighboring nodes, GCNs effectively capture the intricate relationships and dependencies within complex networks. We introduce a novel deep unsupervised learning scheme built upon the foundation of GCN architecture. The key contributions are outlined below. First, the entire architecture is trained with three unsupervised learning losses. The first loss focuses on kernelized features that use node attributes to reflect the information extracted from the data. The second loss leverages spectral smoothness that uses connections between nodes to capture global cluster structure. The third loss is based on graph reconstruction that introduces additional regularization of the representation of nodes by the output of the model. Second, the spectral smoothing loss involves an adaptive approach using an additional graph matrix associated with the node representations. This adaptive integration of additional structural information increases the learning efficiency during the training phase. The adaptive fused graph used for spectral smoothing loss incorporates structural insights derived from both data features and deep node representations. To assess the performance of our approach, we conducted extensive experimental evaluations on four benchmark datasets widely used in the field of graph clustering. These datasets were carefully selected to cover diverse domains and varying degrees of complexity, ensuring a comprehensive evaluation of our method’s efficacy. Our results showcase the remarkable performance of our unsupervised GCN across multiple metrics, surpassing other state-of-the-art graph neural network techniques in terms of clustering accuracy, purity, and other relevant measures. Notably, our method consistently outperforms competing approaches across the used datasets, demonstrating its versatility and effectiveness in handling various real-world scenarios.},
  archive      = {J_CC},
  author       = {Jreidy, M. Al and Constantin, J. and Dornaika, F. and Hamad, D.},
  doi          = {10.1007/s12559-024-10364-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Adaptive unsupervised graph convolution network for data clustering with graph reconstruction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disturbance observer-based control: Weighted aggregated
aczel-alsina sum product assessment based on power operators for
managing fuzzy 2-tuple linguistic neural networks. <em>CC</em>,
<em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10371-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disturbance observer–based control (DOBC) is a valuable strategy for enhancing control system performance by compensating for disturbances. The core idea is to design an observer that accurately estimates external disturbances affecting the system, and then use this estimation to adjust the control input accordingly. This article introduces a novel approach involving fuzzy 2-tuple linguistic (F2-TL) sets, incorporating algebraic and Aczel-Alsina operational laws. Additionally, we propose several operators: the F2-TL Aczel-Alsina power averaging (F2-TLAAPA) operator, the F2-TL Aczel-Alsina power weighted averaging (F2-TLAAPWA) operator, the F2-TL Aczel-Alsina power geometric (F2-TLAAPG) operator, and the F2-TL Aczel-Alsina power weighted geometric (F2-TLAAPWG) operator. These operators are used to aggregate information into a singleton set, and we discuss their fundamental properties, including idempotency, monotonicity, and boundedness. Moreover, we explain the WASPAS (weighted aggregated sum product assessment) technique, applying the proposed methods and illustrating them with relevant examples. The article also explores the application of these techniques to multi-attribute decision-making (MADM) problems, demonstrating their value. Finally, to validate the introduced methods and highlight the superiority of the proposed approach, we compare the ranking results obtained using our methods with those from existing techniques.},
  archive      = {J_CC},
  author       = {Ali, Zeeshan and Hila, Kostaq},
  doi          = {10.1007/s12559-024-10371-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Disturbance observer-based control: Weighted aggregated aczel-alsina sum product assessment based on power operators for managing fuzzy 2-tuple linguistic neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view hierarchical graph neural network for
argumentation mining. <em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10391-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation mining (AM) aims to detect the arguments and their relations from argumentative texts. Generally, AM contains three key challenging subtasks, including argument component type classification (ACTC), argumentative relation identification (ARI), and argumentative relation type classification (ARTC). Most previous studies solve these three subtasks separately, neglecting the rich interrelation information among the three tasks. In this paper, we propose a multi-view hierarchical graph neural network (MHGNN) for AM, which resolves the three interacted subtasks in a unified multi-task learning framework. Concretely, MHGNN learns graph embeddings from multiple views (i.e., word view and semantic view) that often provide more comprehensive information. Each graph view is equipped with a two-level graph structure: (i) the first level is the argumentation graph with each argumentation component (AC) as a graph node, which learns the inter-AC knowledge from the input text; (ii) the second level is the AC graph with each word or semantic role as graph node respectively, which learn the fine-grained intra-AC knowledge within each AC from the word level or semantic level. The multi-view hierarchical GNN makes our model more effective to utilize the rich information among and within the ACs in the input text. Then, we transform ACTC, ARI, and ARTC into node classification, edge prediction, and edge type classification on the argumentation graph by devising novel graph attention mechanisms to learn comprehensive and relation-aware graph embeddings. These three subtasks are integrated into a unified model through multi-task learning and partial parameters sharing. Extensive experiments on two benchmark datasets demonstrate that the proposed MHGNN framework outperforms the strong baseline methods for all three subtasks.},
  archive      = {J_CC},
  author       = {Sun, Yang and Bao, Jianzhu and Tu, Geng and Liang, Bin and Yang, Min and Xu, Ruifeng},
  doi          = {10.1007/s12559-024-10391-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-view hierarchical graph neural network for argumentation mining},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the integration of large-scale time series distance
matrices into deep visual analytic tools. <em>CC</em>, <em>17</em>(1),
1–18. (<a href="https://doi.org/10.1007/s12559-024-10394-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series are essential for modeling a lot of activities such as software behavior, heart rate, and business processes. The analysis of the series data can prevent errors, boost profits, and improve the understanding of behaviors. Among the many techniques available, we can find deep learning techniques and data mining techniques. In data mining, distance matrices between subsequences (similarity matrices, recurrence plots) have already shown their potential in fast large-scale time series behavior analysis. In deep learning, there exist different tools for analyzing the models’ embedding space to get insights into the data behavior. DeepVATS is a tool for large time series analysis that allows the visual interaction within the embedding space (latent space) of deep learning models and the original data. The training and analysis of the model may result in a large use of computational resources, resulting in a lack of interactivity. To solve this issue, we integrate distance matrix plots within the tool. The incorporation of these plots with the associated downsampling techniques makes DeepVATS a more efficient and user-friendly tool for a first quick analysis of the data, achieving runtimes reductions of up to $$10^4$$ seconds, allowing fast preliminary analysis of datasets of up to 7 M elements. Also, this incorporation allows us to detect trends, extending its capabilities. The new functionality is tested in three use cases: the M-Toy synthetic dataset for anomaly detection, the S3 synthetic dataset for trend detection, and the real-world dataset pulsus paradoxus for anomaly checking.},
  archive      = {J_CC},
  author       = {Santamaria-Valenzuela, Inmaculada and Rodriguez-Fernandez, Victor and Camacho, David},
  doi          = {10.1007/s12559-024-10394-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {On the integration of large-scale time series distance matrices into deep visual analytic tools},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy soft topological numbers with an operation of vertex
deletion: A comparative study with TOPSIS method and its application in
car import decision-making. <em>CC</em>, <em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-024-10396-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two distinct soft computing models for representing ambiguity and uncertainty: fuzzy and soft sets. A novel mathematical technique for handling uncertainties is the soft set. This set offers a parameterized viewpoint for soft computing and uncertainty modeling. Soft sets have been shown to have potential uses in a number of disciplines, including probability theory, operations research, game theory, measurement theory, and the smoothness of functions. Topological numbers possess significant importance in graph theory. On topological numbers in fuzzy graph theory, there is also a wealth of literature. However, there has not been much research done on fuzzy soft topological numbers up until now. Fuzzy soft graphs are very versatile instruments available for decision-making. Therefore, in fuzzy soft graph theory, it would be very beneficial to introduce and apply topological numbers. Multi-criteria decision-making approaches give decision-makers the required instruments, but their underlying theories and assumptions may differ. Therefore, choosing the best way to make decisions is just as crucial as actually making the decision. The technique for order preference by similarity to ideal solution (TOPSIS) is the most widely used multi-criteria decision-making method. TOPSIS can solve the real-world problems. TOPSIS is a technique for ranking, based on the weights and impacts of the given elements. However, prior to this study, no work has been done on the application of fuzzy soft topological numbers in decision-making systems. The novelty of this research manuscript is to calculate three fuzzy soft topological numbers before and after the deletion of the vertex for a generalized graph and in a fuzzy soft framework and make a comparison in the results obtained before and after deleting the vertex. Subsequently, we demonstrated an application of international automobile importation into the United States by several nations using various graphical networks, employing a parameterized fuzzy soft graph point of view and three different modes of transportation. It is established that if a vertex is removed from a network, the profit made will decrease significantly. Additionally, the optimal network of nations for all purposes is evaluated. By using the TOPSIS approach, the ranking of networks is also generated from a set of alternatives.},
  archive      = {J_CC},
  author       = {Anwar, Shabana and Kamran Jamil, Muhammad and Azeem, Muhammad and Deveci, Muhammet and Antucheviciene, Jurgita},
  doi          = {10.1007/s12559-024-10396-9},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Fuzzy soft topological numbers with an operation of vertex deletion: A comparative study with TOPSIS method and its application in car import decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural adaptive dynamic event-triggered containment control
for uncertain multi-agent systems under markovian switching dynamics.
<em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10388-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the containment control problem for multi-agent systems with Markovian switching dynamics by proposing a novel adaptive dynamic event-triggered sliding mode control scheme based on radial basis function neural networks. First, the unknown nonlinear dynamics of the system were approximated by using radial basis function neural networks. The dynamic event-triggered control scheme designed in the framework of sliding mode control operated at specific event sampling moments, thereby reducing computational and communication burdens. The containment control was achieved through a synergistic approach integrating dynamic event-triggered control with neural network-based adaptive control in a stochastic switching system. Moreover, we proved that Zeno behavior was effectively avoided. The proposed distributed containment control technique was validated through simulations, demonstrating its effectiveness and superiority.},
  archive      = {J_CC},
  author       = {Cai, Jiayi and Wu, Wenjun and Yi, Chengbo and Chen, Yanxian},
  doi          = {10.1007/s12559-024-10388-9},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Neural adaptive dynamic event-triggered containment control for uncertain multi-agent systems under markovian switching dynamics},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional cross-modal autoencoder-based few-shot
learning for data augmentation with application to alzheimer dementia
diagnosis. <em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-024-10390-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel deep few-shot learning method for magnetic resonance images-based Alzheimer’s dementia (AD) diagnosis. The proposed method consists of two main phases namely data augmentation and data classification. With regard to data augmentation and, to deal with data scarcity issues, we designed a convolutional cross-modal autoencoder (CCMAE) model for data generation. This model, which consists of two encoders and one decoder, receives two image modalities namely longitudinal and cross-section MRI, and generates a new cross-section image. We opt for a convolutional version of the autoencoder to capture the spatial information more effectively and reduce the number of trainable parameters. Moreover, to make the model able to perform deep analysis of input image, we establish a skip connection strategy between the first encoder and the decoder similar to the UNet mechanism. With regard to classification, we design a convolutional neural network-based model in which both textual and visual features are fused to strengthen the network performance and produce more reliable decisions. A comprehensive experiment on a publicly available dataset has been conducted to demonstrate the effectiveness of the proposed method compared to some related works. The code is publicly available at: https://github.com/Bazine-Othmane/scientific-paper-code .},
  archive      = {J_CC},
  author       = {Bazine, Othmane and Rai, Omar and Aiadi, Oussama and Hedjam, Rachid and Khaldi, Belal and Zhong, Guoqiang},
  doi          = {10.1007/s12559-024-10390-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Convolutional cross-modal autoencoder-based few-shot learning for data augmentation with application to alzheimer dementia diagnosis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of metaphorical cognition in ChatGPT
and human minds. <em>CC</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12559-024-10393-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT represents a significant advancement in the field of Artificial Intelligence (AI), showcasing the development of a robust AI system capable of multitasking and generating human-like language. At present, many scholars have done evaluations on ChatGPT in terms of language, reasoning, and scientific knowledge abilities, based on benchmarks or well-crafted questions. However, to the best of our knowledge, there is currently no existing comparative analysis from a cognitive perspective that directly assesses ChatGPT alongside humans. Metaphor, serving as a manifestation of linguistic creativity, provides a valuable avenue for examining cognition. This is due to the mapping relationship it establishes between the target and source conceptual domains, reflecting distinct cognitive patterns. In this paper, we use a metaphor processing tool, MetaPro, to analyze the cognitive differences between ChatGPT and humans through the metaphorical expressions in ChatGPT- and human-generated text. We illustrate the preferences in metaphor usage, concept mapping, and cognitive pattern variances across different domains. The methodology utilized in this study makes a valuable contribution to the task-agnostic evaluation of AI systems and cognitive research. The insights garnered from this research prove instrumental in comprehending the cognitive distinctions between ChatGPT and humans, facilitating the identification of potential cognitive biases within ChatGPT.},
  archive      = {J_CC},
  author       = {Mao, Rui and Chen, Guanyi and Li, Xiao and Ge, Mengshi and Cambria, Erik},
  doi          = {10.1007/s12559-024-10393-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A comparative analysis of metaphorical cognition in ChatGPT and human minds},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel depth-connected region-based convolutional neural
network for small defect detection in additive manufacturing.
<em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-024-10397-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection on the computed tomography (CT) images plays an important role in the development of metallic additive manufacturing (AM). Although some deep learning techniques have been adopted in the CT image-based defect detection problem, it is still a challenging task to accurately detect small-size defects in the presence of undesirable noises. In this paper, a novel defect detection method, namely, the depth-connected region-based convolutional neural network (DC-RCNN), is proposed to detect small defects and reduce the influence of noises. In particular, a saliency-guided region proposal method is first developed to generate small-size region proposals with the aim to accommodate the small defects. Then, the main architecture of DC-RCNN is proposed to extract and connect the consistent features across multiple frames, thereby reducing the influence of randomly distributed noises. Moreover, the transfer learning technique is utilized to improve the generalization ability of the proposed DC-RCNN. In order to verify the effectiveness and superiority, the proposed method is applied to the real-world AM data for defect detection. The experimental validations show that the proposed DC-RCNN is able to detect the small-size defects under noises and outperforms the original RCNN method in terms of detection accuracy and running time.},
  archive      = {J_CC},
  author       = {Wang, Yiming and Wang, Zidong and Liu, Weibo and Zeng, Nianyin and Lauria, Stanislao and Prieto, Camilo and Sikström, Fredrik and Yu, Hui and Liu, Xiaohui},
  doi          = {10.1007/s12559-024-10397-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A novel depth-connected region-based convolutional neural network for small defect detection in additive manufacturing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy yager power operators
and possibility degree-based group decision-making model. <em>CC</em>,
<em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-024-10368-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extended form of intuitionistic fuzzy set, the theory of interval-valued intuitionistic fuzzy set (IVIFS) can describe fuzziness more flexibly. This study aims to develop a group decision-making model based on the distance measure, Yager power aggregation operators and the possibility measure in the context of IVIFSs. For this purpose, new distance measure is proposed to quantify the dissimilarity between two IVIFSs. In addition, comparison with existing distance measures is performed to illustrate the efficiency of introduced measure. Combining the Yager’s triangular norms with the proposed distance-based power operators, a series of interval-valued intuitionistic fuzzy (IVIF) Yager power aggregation operators are introduced with their desirable properties. Moreover, a possibility measure is developed for pairwise comparisons of IVIFSs, which overcomes the shortcomings of existing IVIF-score function, IVIF-accuracy function, and IVIF-possibility measures. The developed possibility measure is further utilized to compute the weights of criteria. To prove the practicality and effectiveness of introduced model, it is applied to a case study of manufacturing plant location selection problem with IVIF information. Finally, sensitivity and comparative analyses are carried out to test the stability and robustness of the proposed method under the setting of IVIFSs.},
  archive      = {J_CC},
  author       = {Rani, Pratibha and Mishra, Arunodaya Raj and Deveci, Muhammet and Alrasheedi, Adel Fahad and Alshamrani, Ahmad M. and Pedrycz, Witold},
  doi          = {10.1007/s12559-024-10368-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Interval-valued intuitionistic fuzzy yager power operators and possibility degree-based group decision-making model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified multi-view data clustering: Simultaneous learning of
consensus coefficient matrix and similarity graph. <em>CC</em>,
<em>17</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12559-024-10392-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating data from multiple sources or views has become increasingly common in data analysis, particularly in fields like healthcare, finance, and social sciences. However, clustering such multi-view data poses unique challenges due to the heterogeneity and complexity of the data sources. Traditional clustering methods are often unable to effectively leverage the information from different views, leading to suboptimal clustering results. To address this challenge, multi-view clustering techniques have been developed, aiming to integrate information from multiple views to improve clustering performance. These techniques typically involve learning a similarity matrix for each view and then combining these matrices to form a consensus similarity matrix, which is subsequently used for clustering. However, existing approaches often suffer from limitations such as the need for manual tuning of parameters and the inability to effectively capture the underlying structure of the data. In this paper, we propose a novel approach for multi-view clustering that addresses these limitations by jointly learning the consensus coefficient matrix and similarity graph. Unlike existing methods that follow a sequential approach of first learning the coefficient matrix and then constructing the similarity graph, our approach simultaneously learns both matrices, ensuring a more regularized consensus graph. Additionally, our method automatically adjusts the weight of each view, eliminating the need for manual parameter tuning. Our approach involves several key steps. First, we formulate an optimization problem that jointly optimizes the consensus coefficient matrix, unified spectral projection matrix, coefficient matrix, and soft cluster assignment matrix. We then propose an efficient algorithm to solve this optimization problem, which involves iteratively updating the matrices until convergence. To learn the consensus coefficient matrix and similarity graph, we leverage techniques from matrix factorization and graph-based learning. Specifically, we use a self-representation technique to learn the coefficient matrix (regularization graPh) and a graph regularization technique to learn the similarity graph. By jointly optimizing these matrices, we ensure that the resulting consensus graph is more regularized and better captures the underlying structure of the data. We evaluate our approach on several public image datasets, comparing it against state-of-the-art multi-view clustering methods. Our experimental results demonstrate that our approach consistently outperforms existing methods in terms of clustering accuracy and robustness. Additionally, we conduct sensitivity analysis to evaluate the impact of different hyperparameters on the clustering performance. We present a novel approach for multi-view data clustering that jointly learns the consensus coefficient matrix and similarity graph. By simultaneously optimizing these matrices, our approach achieves better clustering performance compared to existing methods. Our results demonstrate the effectiveness and robustness of our approach across different datasets, highlighting its potential for real-world applications in various domains.},
  archive      = {J_CC},
  author       = {Dornaika, F. and El Hajjar, S. and Charafeddine, J. and Barrena, N.},
  doi          = {10.1007/s12559-024-10392-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Unified multi-view data clustering: Simultaneous learning of consensus coefficient matrix and similarity graph},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing social issues strategies by using bipolar complex
fuzzy muirhead mean decision-making approach. <em>CC</em>,
<em>17</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12559-024-10353-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive processes that affect people perceptions, comprehension, and interactions with others, such as attribution mistakes and heuristics, are responsible for social issues. These social issues includes polarization, prejudice, and inequality. To address these issues, we must comprehend cognitive mechanisms, and this can be made by using some appropriate multi-attribute decision-making (MADM) approach, that can handle people perceptions of complex and bipolar nature. Thus, in this manuscript, we concentrate on a MADM technique that relies on certain novel aggregation operators in the framework of bipolar complex fuzzy sets. These aggregation operators include Muirhead mean (MM) operator and dual Muirhead mean (DMM) operator of several types. To authenticate the validity of these defined aggregation operators, certain properties of these operators are proved. Furthermore, we consider the interpreted operators to produce a decision-making (DM) technique to deal with bipolar complex fuzzy MADM issues. We then consider a real life example to show the application and need of the interpreted work in daily life. To confirm the viability and potential of the offered technique, we compare our established technique with some other prevailing techniques.},
  archive      = {J_CC},
  author       = {Rehman, Ubaid ur and Mahmood, Tahir and García, Gustavo Santos},
  doi          = {10.1007/s12559-024-10353-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Optimizing social issues strategies by using bipolar complex fuzzy muirhead mean decision-making approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A weakly supervised data labeling framework for machine
lexical normalization in vietnamese social media. <em>CC</em>,
<em>17</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s12559-024-10356-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of the training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing pre-trained language models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1–3%.},
  archive      = {J_CC},
  author       = {Nguyen, Dung Ha and Nguyen, Anh Thi Hoang and Van Nguyen, Kiet},
  doi          = {10.1007/s12559-024-10356-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Cogn. Comput.},
  title        = {A weakly supervised data labeling framework for machine lexical normalization in vietnamese social media},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BrainEnsemble: A brain-inspired effective ensemble pruning
algorithm for pattern classification. <em>CC</em>, <em>17</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s12559-024-10363-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain comprises distinct regions, each with specific functions. Interconnected through neural pathways, the brain regions collaborate to process complex information. Similarly, ensemble learning enhances pattern classification by leveraging the collaboration and complementarity between classifiers. The similarity between the two suggests that simulating the brain’s functional network holds the potential for groundbreaking advancements in the design of ensemble learning algorithms. Motivated by this, our paper proposes a brain-inspired ensemble pruning method called BrainEnsemble. This method provides an example of using classifier combinations to emulate the functions of brain regions. Guided by the principles of curriculum learning and the divide-and-conquer strategy, each artificial brain region can specialize in specific functions and tasks. Additionally, BrainEnsemble simulates the brain regions’ responses and connectivity mechanisms through graph connections. In this model, different artificial brain regions can dynamically reorganize and adjust their interactions to adapt to continuously changing environments or data distributions, enabling the model to maintain high performance when confronted with new data. Extensive experimental results demonstrate the superior performance of BrainEnsemble. In summary, drawing inspiration from the information processing mechanism of the human brain can provide new ideas for the design of ensemble learning algorithms, and more research can be conducted in this direction in the future.},
  archive      = {J_CC},
  author       = {Li, Danyang and Huang, Shisong and Wen, Guihua and Zhang, Zhuhong},
  doi          = {10.1007/s12559-024-10363-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {BrainEnsemble: A brain-inspired effective ensemble pruning algorithm for pattern classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting deep contrast feature for image retrieval.
<em>CC</em>, <em>17</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s12559-024-10375-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of content-based image retrieval (CBIR), fused feature-based methods have demonstrated their advanced performance on the popular benchmark datasets. However, it is inevitable increase the vector dimensionality because the fused features have diversity. Therefore, achieving both a low-dimensional representation and high retrieval performance remains challenging. To address this problem, an image retrieval method based on the deep contrast-based layer is proposed, namely the deep contrast feature histogram (DCFH), to image retrieval. There are three highlights as follows: (1) texture features based on the edge orientation are calculated to build contrast-based layer; it can enhance the discriminative power of deep features; (2) a generalized mean aggregation method is introduced to effectively aggregate the representative information in the deep feature maps of convolutional neural network (CNN); (3) a multi-orientational PCA whitening method is proposed to provide a compact yet discriminative representation. Comparative experiments demonstrated that our method can provide outstandingly competitive retrieval performance on popular benchmark datasets. This work captures visual information from both global and local perspectives, presenting an approach in line with human visual cognitive. Experiments demonstrated that our method can efficiently combine the strengths of various features to provide the robust representation, thereby improving the retrieval performance. Moreover, our method is easily to be implemented without requiring to retrain the CNN models and not the use of additional supervision.},
  archive      = {J_CC},
  author       = {Lu, Zhou and Liu, Guang-Hai and Li, Zuoyong and Yang, Lu},
  doi          = {10.1007/s12559-024-10375-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Exploiting deep contrast feature for image retrieval},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a decision support system for performance
measurement of social movements. <em>CC</em>, <em>17</em>(1), 1–27. (<a
href="https://doi.org/10.1007/s12559-024-10385-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social movements encompass the collective actions of groups gathered under the same goal, operating within a specific organizational structure to reflect their thoughts and views through a series of actions. Social movements are expected to exhibit characteristics of violence, rationality, continuity, and public benefit. The successful performance of social movements is contingent upon their ability to embody these characteristics. The primary motivation of this research is to render the performance of social movements measurable and comparable, aiming to determine their effectiveness. The underlying motivation for this approach is to move beyond subjective evaluations of social movements and instead treat them as structured decision-making processes. The core objective is to develop a decision support system for assessing the performance of social movements and facilitating insights into the performance of movements within a country or region. In this way, the performance evaluation processes of social movements are enhanced, fostering the development of more informed and deliberate social movements. To determine the performance of social movements, the type-2 neutrosophic number (T2NN)–Schweizer Sklar (SS)–symmetry point of criterion (SPC)–evaluation based on relative utility and nonlinear standardization (ERUNS)–(T2NN-SS-SPC-ERUNS) hybrid model is developed and proposed in this research. The T2NN-SS-weighted arithmetic mean aggregation operator is used to combine expert evaluations. The weights of criteria are calculated using the T2NN-SPC method. Performance rankings of social movements are determined using the T2NN-ERUNS method. An algorithm for the three-stage T2NN-SS-SPC-ERUNS hybrid model is developed to evaluate the performance of social movements in Türkiye through a case study. The robustness and consistency of the proposed hybrid method are supported by scenarios. As a result of the research, the “Early Retirement Scheme Victims” social movement is identified as having the highest performance among social movements in Türkiye.},
  archive      = {J_CC},
  author       = {Yalçın, Galip Cihan and Kara, Karahan and Işık, Gülcan and Simic, Vladimir and Pamucar, Dragan},
  doi          = {10.1007/s12559-024-10385-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Cogn. Comput.},
  title        = {Development of a decision support system for performance measurement of social movements},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I2V-CMGAN: Generative adversarial cross-modal network-based
image-to-video person re-identification. <em>CC</em>, <em>17</em>(1),
1–22. (<a href="https://doi.org/10.1007/s12559-024-10389-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information asymmetry situation amongst image and video features in image-to-video (I2V) person re-identification (Re-ID) refers to the difficulty in extracting consistent and dependable features from both image and video data in order to accurately match and identify a person in both modalities. The problem arises because images and videos have different characteristics and represent different aspects of a person. This difference in representation can result in inconsistent and unreliable features being extracted from image and video data, leading to difficulty in accurately matching and re-identifying a person between modalities. The temporal information provided by videos can also boost the accuracy of person re-identification, especially in crowded and cluttered environments. To address the information asymmetry problem, a generative adversarial cross-modal network–based I2V Person Re-ID (I2V-CMGAN) is proposed, which works by using a generator to transform the features learned from the video network into an image network with an additional loss function to improve the consistency and reliability of features extracted from both image and video data and also preserve identity information. Extensive studies show the efficacy of the proposed approach, and the aggregate results on the MARS dataset outperform the state-of-the-art methods by a substantial margin and achieved rank-1 accuracy of 88.9% (+ 2.9), rank-5 accuracy of 95.5% (+ 2.3), rank-10 accuracy of 97.1% (+ 2.9), and mean average precision of 81.2% (+ 1.1) for I2V Re-ID. On iLIDS-VID and PRID2011 datasets, the proposed method attains outstanding margins with rank-1, rank-5, rank-10, mAP of 64.7%, 89.3%, 92.7%, 74.2%, and 80.9%, 93.3%, 98.9%, and 86.8% respectively.},
  archive      = {J_CC},
  author       = {Joshi, Aditya and Diwakar, Manoj},
  doi          = {10.1007/s12559-024-10389-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {I2V-CMGAN: Generative adversarial cross-modal network-based image-to-video person re-identification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review and thematic analysis of digital games for
cognitive enhancement in children with autism spectrum disorder: Toward
a conceptual framework. <em>CC</em>, <em>17</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s12559-024-10395-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to typically developing people, children with autism spectrum disorder (ASD) have distinct cognitive and intelligence profiles. Some of these children require cognitive rehabilitation. Through the use of cutting-edge therapy and cognitive empowerment methods, some cognitive skills in children with ASD can be strengthened by digital game-based tools. This study’s main purpose is to provide a systematic review and qualitative study about designing digital games for cognitive enhancement in autistic children and to determine the main design components of such digital games. The primary focus of this study is to explore the citations in which the technical and functional elements are provided thoroughly. Furthermore, a conceptual framework is elaborated for designing a digital game for autism. A thorough review of the literature was conducted in the databases of Medline (via PubMed), Web of Science (WOS), Scopus, and IEEE Xplore for English publications published before January 23, 2023. Of 976 papers, 34 studies were found to be eligible in this systematic review. The bulk of the studies were carried out in Asia and Europe. Three (8.8%) studies used games that were built to be multilingual, while 22 (64.7%) studies used games that were only created in English. Creating motivation through narratives, providing incentive systems, raising the complexity level, targeting main skills, and adjusting the choices are the principal components of digital game design. (1) Main cognitive rehabilitation domains in ASD; (2) game designing details: platforms and game genres, motivations, evaluations, game graphics designs, aesthetic mechanisms, incentive systems, and famous game development engines; and (3) mutual interaction between child, therapist, and parents are the crucial categories that are described to devise a conceptual framework in this qualitative study. Of the total number of included studies, 25 studies reported positive effects on autism cases, and in nine, there has not been any evaluation of real cases; however, only usability tests have been conducted. Children with autism may benefit from using appropriate digital game-based interventions to improve mental indices. According to a review, it can be stated that the suitable computerized and digital game-based solutions could enhance cognitive outcomes in children with autism spectrum disorder. However, more research is required to ascertain the true efficacy of these new technologies.},
  archive      = {J_CC},
  author       = {Rezayi, Sorayya and Shahmoradi, Leila and Tehrani-Doost, Mehdi},
  doi          = {10.1007/s12559-024-10395-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Cogn. Comput.},
  title        = {Systematic review and thematic analysis of digital games for cognitive enhancement in children with autism spectrum disorder: Toward a conceptual framework},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-invasive approach for early alzheimer’s detection
through spontaneous speech analysis using deep visibility graphs.
<em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-024-10398-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying Alzheimer’s disease (AD) in its early stages is a challenging task for physicians and clinicians. This paper proposes a new algorithm for diagnosing AD, which is based on analyzing spontaneous speech signals. The proposed method uses two visibility graph methods, Natural Visibility Graph (NVG) and Horizontal Visibility Graph (HVG), to derive features from speech windows. These features are then given to a deep BiLSTM-based classifier to decide about segments of the signal. The proposed approach could obtain a sensitivity of 98.33%, specificity of 99.44%, and accuracy of 99.17%. The advantage of converting speech signals into graphs using NVG and HVG is that it allows for the extraction of complex structural features that are not easily captured by traditional methods. This method is highly beneficial due to its non-invasive nature, low cost, and lack of side effects. Patients can undergo the procedure without experiencing any discomfort, while also benefiting from its affordability and accessibility. The method’s safety and practicality make it an ideal choice for those seeking a reliable and effective solution. Moreover, the proposed algorithm has a high accuracy in detecting the early stage of AD, which makes it a promising tool to evaluate Alzheimer’s disease diagnosis in its pre-clinical stage.},
  archive      = {J_CC},
  author       = {Mohammadpoory, Zeynab and Nasrolahzadeh, Mahda and Amiri, Sekineh Asadi and Haddadnia, Javad},
  doi          = {10.1007/s12559-024-10398-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {A non-invasive approach for early alzheimer’s detection through spontaneous speech analysis using deep visibility graphs},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term power load forecasting in city based on
ISSA-BiTCN-LSTM. <em>CC</em>, <em>17</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12559-024-10401-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term power load forecasting is crucial for the stable operation of power systems. In this paper, we propose an advanced forecasting model that combines the Salp Swarm Algorithm (SSA), Bidirectional Temporal Convolutional Network (BiTCN), and Long Short-Term Memory (LSTM). The model first exploits the parallel fusion of BiTCN and LSTM (BiTCN-LSTM), taking full advantage of BiTCN’s strength in parallel processing of local features and the LSTM’s ability to capture long-term dependencies through its gating mechanisms. Subsequently, the Improved Salp Swarm Algorithm (ISSA) is enhanced through adaptive leader ratio adjustment, dual-food design, and food lure follower strategy. Finally, the hyperparameters of the BiTCN-LSTM model are optimized using ISSA to improve the model performance. In the short-term load forecasting experiments, electric load data and weather data from Los Angeles, Tetouan, and Johor were used to compare the proposed model with eight existing models. The evaluation metrics included root mean square error (RMSE), mean absolute error (MAE), normalized root mean square error (NRMSE), and mean absolute percentage error (MAPE). The experimental results showed that the model achieved lower error values than the comparison model in most cases in different seasons, working days, and rest days in different cities. In particular, the error values of RMSE, MAE, NRMSE, and MAPE were 925.11 kW, 732.63 kW, 0.019, and 1.034% for the rest days in the city of Tetouan, respectively. Compared with other algorithms, ISSA demonstrates stronger optimization capability and shorter optimization time. Additionally, model structure analysis was conducted through optimization comparison and ablation experiments, further demonstrating the proposed model’s strong predictive performance.},
  archive      = {J_CC},
  author       = {Fan, Chaodong and Li, Gongrong and Xiao, Leyi and Yi, Lingzhi and Nie, Shanghao},
  doi          = {10.1007/s12559-024-10401-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Short-term power load forecasting in city based on ISSA-BiTCN-LSTM},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of metaheuristic algorithms with supervised
machine learning for accurate power consumption prediction. <em>CC</em>,
<em>17</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s12559-025-10402-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate power consumption prediction is a crucial part of energy management. Some of the machine learning models that are the focus of this study for the prediction of power use include Support Vector Regression, Adaptive Boosting, and Decision Tree Regression. These models have been improved with the use of some novel optimizers-namely, the Trochoid Search Optimization, Red-Tailed Hawk, and Giant Armadillo Optimization methods-for hyper-parameter tuning to enhance prediction accuracy. When tested against real data, DTGA outperformed with R2 values of 0.9918, 0.9924, and 0.9934 for three zones. This work extends the study on the forecast of power consumption by integrating machine learning and optimization techniques that provide effective energy management strategies.},
  archive      = {J_CC},
  author       = {Wang, Mengxia and Zhu, Chaoyang and Zhang, Yunxiang and Deng, Jinxin and Cai, Yiwei and Wei, Wei and Guo, Mengxing},
  doi          = {10.1007/s12559-025-10402-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Cogn. Comput.},
  title        = {Application of metaheuristic algorithms with supervised machine learning for accurate power consumption prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware prediction with secure and lightweight
cognitive decision model in smart cities. <em>CC</em>, <em>17</em>(1),
1–12. (<a href="https://doi.org/10.1007/s12559-025-10403-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive networks with the integration of smart and physical devices are rapidly utilized for the development of smart cities. They are explored by many real-time applications such as smart homes, healthcare, safety systems, and other unpredictable environments to gather data and process network requests. However, due to the external conditions and inherent uncertainty of wireless systems, most of the existing approaches cannot cope with routing disturbances and timely delivery performance. Further, due to limited resources, the demand for a secure communication system raises another potential research challenge to protect sensitive data and maintain the integrity of the urban environment. This paper presents a secured decision-making model using reinforcement learning with the combination of blockchain to enhance the degree of trust and data protection. The proposed model increases the network efficiency for resource utilization and the management of communication devices with the alliance of security. It provides a reliable and more adaptive paradigm by exploring learning techniques for dealing with the intrinsic uncertainty and imprecision of cognitive systems. Also, the incorporation of blockchain technology reduces the risk of a single point of failure, malicious vulnerabilities, and data leakage, ultimately fostering trust for urban sensor applications. It validates the incoming routing links and identifies any communication fault incurred due to malicious interference. The proposed model is rigorously tested and verified using simulations and its significance has been proven for network metrics in comparison to existing solutions.},
  archive      = {J_CC},
  author       = {Al-Quayed, Fatima and Humayun, Mamoona and Alnusairi, Thanaa S. and Ullah, Inam and Bashir, Ali Kashif and Hussain, Tariq},
  doi          = {10.1007/s12559-025-10403-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Context-aware prediction with secure and lightweight cognitive decision model in smart cities},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engaging preference optimization alignment in large language
model for continual radiology report generation: A hybrid approach.
<em>CC</em>, <em>17</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s12559-025-10404-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can result in delays and inconsistencies. To address these challenges, we present a novel hybrid approach that integrates multi-modal radiology information and preference optimization alignment in LLM for continual RRG. Our method integrates a pre-trained small multi-modal model to analyze radiology images and generate an initial report, which is subsequently refined and aligned by an LLM using odds ratio preference optimization (ORPO) and with historical patient data and assessments to mimic radiologist-like responses, bypassing reinforcement learning from human feedback-based (RLHF) alignment. This two-stage fusion—supervised fine-tuning followed by preference optimization—ensures high accuracy while minimizing hallucinations and errors. We also propose a data field curation strategy extendable to various other RRG modality datasets, focusing on selecting relevant responses for preference alignment. We evaluate our approach on two public datasets, achieving state-of-the-art performance with average Bleu scores of 0.375 and 0.647, Meteor scores of 0.495 and 0.714, Rouge-L scores of 0.483 and 0.732, and average F1-RadGraph scores of 0.488 and 0.487, for chest X-rays and lung CT scan datasets, respectively. We further provide in-depth qualitative analyses and ablation studies to explain the workings of our model and grasp the clinical relevance for RRG. This work presents the first application of preference optimization in continual RRG, representing a significant advancement in automating clinically reliable report generation. By reducing cognitive burdens on radiologists through AI-powered reasoning and alignment in LLMs, the proposed model improves decision-making, perception, and diagnostic precision, streamlining workflows and enhancing patient care. Our code is available at https://github.com/AI-14/r2gpoallm .},
  archive      = {J_CC},
  author       = {Izhar, Amaan and Idris, Norisma and Japar, Nurul},
  doi          = {10.1007/s12559-025-10404-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Engaging preference optimization alignment in large language model for continual radiology report generation: A hybrid approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hyperparameter optimization approach for supervised
classification: Phase prediction of multi-principal element alloys.
<em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hyperparameter optimization approach is proposed for the phase prediction of multi-principal element alloys (MPEAs) through the introduction of two novel hyperparameters: outlier detection and feature subset selection. To gain a deeper understanding of the connection between alloy phases and their elemental properties, an artificial neural network is employed, with hyperparameter optimization performed using a genetic algorithm to select the optimum hyperparameters. The two novel hyperparameters, outlier detection and feature subset selection, are introduced within the optimization framework, along with new crossover and mutation operators for handling single and multi-valued genes simultaneously. Ablation studies are conducted, illustrating an improvement in prediction accuracy with the inclusion of these new hyperparameters. A comparison with five existing algorithms in multi-class classification is made, demonstrating an improvement in the performance of phase prediction, thereby providing a better perception of the alloy phase space for high-throughput MPEA design.},
  archive      = {J_CC},
  author       = {Fatimi, Syed Hassan and Wang, Zidong and Chang, Isaac T. H. and Liu, Weibo and Liu, Xiaohui},
  doi          = {10.1007/s12559-025-10405-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {A novel hyperparameter optimization approach for supervised classification: Phase prediction of multi-principal element alloys},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting cardiovascular disease prediction through CWCF
integration leveraging harris hawks search in deep belief networks.
<em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-025-10406-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) is a major global health concern, demanding accurate predictive models to aid preventive healthcare strategies. Heart failure, stroke, and coronary artery disease are among the disorders that fall under the umbrella term of cardiovascular disease (CVD). Leveraging the Harris Hawks Optimization (HHO) algorithm in conjunction with deep belief networks (DBNs) aims to improve CVD risk prediction accuracy. Harris Hawks Optimization (HHO) draws inspiration from the cooperative behavior of Harris’s hawks in nature, providing an efficient metaheuristic search algorithm for optimization problems. Integrating HHO into machine learning frameworks enhances the exploration and exploitation of search spaces, leading to improved model performance and convergence rates, particularly in deep learning tasks like feature selection and hyperparameter tuning. Powerful generative models called deep belief networks (DBNs) are made up of several layers of latent, stochastic variables. Restricted Boltzmann machines (RBMs) serve as building blocks in the training process of deep belief networks (DBNs), facilitating the unsupervised pre-training of hidden layers. Leveraging RBMs within DBNs enables the extraction of hierarchical representations, enhancing the network’s ability to learn intricate patterns and improve predictive performance in complex data settings. They leverage unsupervised learning techniques to extract intricate patterns and hierarchical representations from complex data, making them ideal for tasks such as feature learning and classification in machine learning research. This study introduces innovative algorithms, including the correlation-based weighted compound feature generation (CWCFG) technique, to enhance the optimization process of HHO. Comparative analysis against traditional machine learning models and rule-based firefly optimizer (RBFO) and Grey Wolf Optimizer (GWO) with the state-of-the-art deep learning techniques demonstrates the efficacy of the CWCFG-HHO-DBN model. Additionally, an in-depth feature importance analysis identifies key predictors, enriching the model’s interpretability. The research conducts a comprehensive evaluation of the proposed model, employing various performance metrics such as accuracy, precision, recall, and F-measure. With a remarkable accuracy of 97.19%, the HHO-DBN model shows promise in enhancing CVD risk prediction. The findings underscore its potential in personalized medicine, facilitating tailored interventions for high-risk individuals. Future directions include refining the algorithm and expanding its application in healthcare settings.},
  archive      = {J_CC},
  author       = {Savitha, S. and Kannan, A. Rajiv and Logeswaran, K.},
  doi          = {10.1007/s12559-025-10406-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Augmenting cardiovascular disease prediction through CWCF integration leveraging harris hawks search in deep belief networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asymmetric semantic segmentation model via lightweight
attention-guided feature enhancement and fusion. <em>CC</em>,
<em>17</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12559-025-10407-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is widely used in fields such as autonomous driving and unmanned aerial vehicle navigation. However, the huge computational burden and redundant parameters limit its application in edge devices such as mobile phones. In this study, we propose an asymmetric lightweight semantic segmentation model via lightweight attention-guided feature enhancement and fusion. Specifically, the proposed model adopts an encoder-decoder structure. In the encoder, we design an asymmetric feature extraction module to extract image information and use the locally sensitive Hash self-attention to enhance the global information. In the decoder, we first use channel attention to filter out the useless information in shallow layers and adopt the spatial attention to refine local features in deep layers. We then fuse the multi-scale features by the gating mechanism. Additionally, we also design an auxiliary loss to supervise the segmentation of small objects. The results on Cityscapes and CamVid show that the proposed model achieves a good balance between accuracy and the number of parameters. It obtains 70.68% and 72.19% mIoU on the two test datasets with 0.86M parameters, respectively. Code is available on https://github.com/year410/LAANET},
  archive      = {J_CC},
  author       = {Tang, Qingsong and Zhao, Minghui and Ren, Yalei and Shi, Xiaomeng and Jiang, Wuming},
  doi          = {10.1007/s12559-025-10407-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {An asymmetric semantic segmentation model via lightweight attention-guided feature enhancement and fusion},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic behavior of three-layer fractional-order neural
networks with multiple delays. <em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-025-10411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the complex network in the real world are not single-layer networks, and networks will be connected with each other. Networks with multi-layer is important because it means cognitive and artificial intelligence. Most current studies of networks consider the case that with n-nodes including ring network, small-word network, scale-free network, etc. This type of network is not enough to describe the complex structure of actual neural networks. However, it is more actual to study the dynamic behavior of multi-layer networks than single-layer networks. In this paper, the stability and bifurcation of a class of three-layer fractional-order neural networks with multiple delays was studied for the first time. By selecting the appropriate bifurcation parameter, the internal dynamic behavior of the given model was discussed by using the theory of Hopf bifurcation, and the critical value and criterion for Hopf bifurcation are derived. The influence of delay, fractional order, and the number of hidden neurons on the bifurcation point were discussed in detail. And the critical value of Hopf bifurcation is accurately calculated. The results show that the stability of the system can be destroyed by increasing the fractional order and the number of hidden neurons. The correctness of the theoretical results is verified by numerical simulation.},
  archive      = {J_CC},
  author       = {Li, Xinyu and Cheng, Zunshui and Xin, Youming and Shang, Yun},
  doi          = {10.1007/s12559-025-10411-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Dynamic behavior of three-layer fractional-order neural networks with multiple delays},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to calibrate prototypes for few-shot image
classification. <em>CC</em>, <em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12559-025-10412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to generalise the model to novel classes by using a limited amount of discriminative samples (a.k.a., prototypes). With few labelled samples, there is much uncertainty and randomness in the data, which makes it more difficult for the model to learn the complete underlying patterns. This paper proposes a Discriminative Property Calibration Network (DPCNet) to enable model building in linear space with robust separability. Concretely, the property features of samples are extracted to facilitate filtering out low-informative key points at the instance level, and then the key points are further refined from the perspective of property features to retain those dimensions that contain the most relevant properties. Furthermore, the discriminative key properties are re-weighted by accounting for the correlation between images, thus forcing the model to focus more on the key property information. Moreover, a new margin algorithm is proposed to optimise the data distribution of features by dynamically adjusting the distance between classes. We conduct extensive experiments on four datasets, i.e., miniImageNet, tiredImagenet, CUB-200-2011 and CIFAR-FS, achieving the accuracies of 67.96%, 72.57%, 79.6% and 74.56%, respectively, on the 5-way 1-shot setting, and the same very competitive performance on the 5-way 5-shot setting. The proposed method can well extract the most relevant and discriminative properties, the re-weighted features further emphasise the discrimination and the dynamic margin algorithm enhances the stability and generalisation ability. The proposed method achieves the state-of-the-art performance, and it will have meaningful inspiration for future works.},
  archive      = {J_CC},
  author       = {Liang, Chenchen and Jiang, Chenyi and Wang, Shidong and Zhang, Haofeng},
  doi          = {10.1007/s12559-025-10412-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Cogn. Comput.},
  title        = {Learning to calibrate prototypes for few-shot image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tweet credibility ranker: A credibility features’ fusion
model. <em>CC</em>, <em>17</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s12559-025-10413-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misinformation on social media has emerged as a modern weapon of warfare, disrupting societal peace, trust, justice, and democracy. It is quite challenging to address the issue of information credibility for microblogs. It becomes more challenging when the authenticity of the poster is hidden. The concept of information credibility has multi-perspectives. There are many necessary aspects of information credibility which must be considered for effective credibility assessment. It is observed that some important aspects of credibility are not considered in existing studies. The complete credibility assessment solution needs a comprehensive and diverse set of features for such complex identification. Therefore, these features are identified and proposed by exploring the related research studies consisting of the necessary credibility aspects. These features consist of diverse levels provided by microblogs. These levels include the post, poster, poster’s social network, and actual information propagation network. An exploratory study is also conducted to propose the best credibility features that are used in the proposed solution. The attempt is made for a hybrid features fusion model which combines feature-based or machine learning and graph-based approaches. It is a lightweight, high-performing, non-latent features model to avoid their drawbacks. It assesses the levels of credibility of the concerned post. It is designed for high-impact applications to combat low-credibility content during elections, crises, and other critical scenarios. The model is executed over a publicly available dataset extended for credibility assessment. The model provides good results with 95.6% accuracy by XGBoost using platinum features. The performance of the proposed model is compared with state-of-the-art that produced much-appreciating results.},
  archive      = {J_CC},
  author       = {Qureshi, Khubaib Ahmed and Malick, Rauf Ahmed Shams},
  doi          = {10.1007/s12559-025-10413-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Cogn. Comput.},
  title        = {Tweet credibility ranker: A credibility features’ fusion model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive neural network algorithm with quasi
opposition-based learning for numerical optimization problems.
<em>CC</em>, <em>17</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s12559-025-10415-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of artificial neural networks and the biological nervous systems serve as the foundation for the creation of the neural network algorithm (NNA). The robust global search capability of NNAs makes it an effective tool for solving a wide range of complex optimization problems. Unfortunately, its limited relevance to many optimization problems is due to its poor exploitation, weak convergence, and tendency to fall into local optima. The paper’s goal is to introduce an enhanced version of the NNA known as the adaptive quasi-opposition-based neural network algorithm (AQOBNNA) in order to overcome these issues. The quasi-opposition-based learning (QOBL) and an adaptive strategy are combined in this suggested algorithm, where the adaptive technique is added to determine whether or not to use QOBL. The QOBL technique replaces a random search individual with the best one throughout the position update phase in order to enhance exploitation and increase exploration capabilities. The performance of the suggested AQOBNNA is assessed using a set of 23 traditional benchmark functions and compared with a number of current methods. It is evident from the experimental data that AQOBNNA performs better overall and outperforms all the algorithms that were examined.},
  archive      = {J_CC},
  author       = {Kundu, Tanmay and Garg, Harish},
  doi          = {10.1007/s12559-025-10415-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Cogn. Comput.},
  title        = {An adaptive neural network algorithm with quasi opposition-based learning for numerical optimization problems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring influence of different emotions on decision-making
by analyzing the temporal, spatial, and spectral domains of EEG.
<em>CC</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12559-025-10416-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making is a complex cognitive process, in which emotion is one of the most important factors. But insights into the influence of emotion on decision-making are scarce, especially the underlying mechanism of the brain. To reveal the brain’s underlying mechanisms of the influence of emotion on decision-making, an experiment involving emotion elicitation and decision-making tasks was designed. Electroencephalography (EEG), behavioral, and subjective data were collected and conducted. We constructed time-varying weighted directed networks by phase slope index (PSI) in four frequency bands and calculated graph theory metrics. Firstly, the period that the brain processes information most efficiently is 100–300 ms after the appearance of the decision-making task. Secondly, by analyzing the temporal-spatial domains of EEG, the significant differences in global efficiency (GE) and local efficiency (LE) were found among three different emotion groups in the alpha band in the low-difficulty task during 100–300 ms. Thirdly, most activation regions of different emotions were similar and concentrated in the parietal, and occipital lobes but there were still slight differences that were more likely to be found in the prefrontal and left temporal lobes. Graph theory metrics in the decision-making process changed dynamically in the temporal domain and graph theory metrics of different emotions were different.},
  archive      = {J_CC},
  author       = {Wang, Xinyuan and Wang, Danli and Zhao, Yanyan},
  doi          = {10.1007/s12559-025-10416-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Exploring influence of different emotions on decision-making by analyzing the temporal, spatial, and spectral domains of EEG},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HLAE: Hierarchical local attention encoder for MRI brain
tumor image classification. <em>CC</em>, <em>17</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12559-025-10419-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MRI-based brain tumor classification is a challenging neuroimaging task, where the key lies in leveraging ensemble information from brain images. However, current algorithms primarily encode global appearance features of brain images and fail to account for local dependencies inherent in brain tissue adequately. In addition, most existing approaches do not thoroughly investigate the importance coefficients among different regions of brain images. To address these issues, in this work, we propose a novel cognitively-inspired hierarchical local attention encoder (HLAE) framework for capturing local dependency information in MRI images. Based on the characteristics of MRI images, we focus on local information from two perspectives: long-range visual feature dependencies and high-order structural context correlations to fully describe the content association and location relations of brain MRI images. For this purpose, a Swin Transformer is first utilized for encoding the patch-wise content dependencies of a brain MRI image by shifting windows and skipping connections. Meanwhile, a graph structure is also extracted from the MRI image, and a graph attention network is employed to capture the image’s contextual correlations. Finally, the two local information are integrated, and a softmax layer is used to obtain the final brain tumor classification result. This framework naturally contains the attention mechanism, which can effectively quantify the importance among different brain image regions, so as to locate the most discriminative regions in brain tumor classification accurately. Extensive experiments are conducted on two publicly available brain tumor MRI datasets. The results demonstrate its ability to automatically detect brain tumors with superior performance compared to state-of-the-art algorithms.},
  archive      = {J_CC},
  author       = {Dong, Changxu and Sun, Dengdi and Luo, Bin},
  doi          = {10.1007/s12559-025-10419-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {HLAE: Hierarchical local attention encoder for MRI brain tumor image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of generative AI in agriculture 4.0 for
smart and sustainable farming. <em>CC</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12559-025-10420-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence (GAI) represents a pioneering class of artificial intelligence systems renowned for producing diverse media, such as text and images. Agriculture 4.0 (AG-4.0) is a concept that integrates advanced technologies such as the Internet of Things (IoT), data analytics, artificial intelligence, and precision agriculture into the agricultural sector. The integration of GAI and AG-4.0 can generate new and valuable agricultural insights and solutions through pattern recognition and data analysis. This integration enhances farming practices by generating predictive models, simulating optimal growth conditions, diagnosing plant diseases, and optimizing genetic traits. In spite of the tremendous scope of GAI in agriculture, there has been no detailed study concerning the applications and scope of GAI in AG-4.0. Addressing this research gap, we explore various applications, real-world products, and limitations of GAI in agriculture. We explore how GAI models such as ChatGPT and Dall-E can be personalized advisors for farmers, help increase awareness about farmer relief programs, design farm layouts, and many other such applications. Additionally, we cover four real-world GAI products deployed to assist farmers. Since GAI is a growing technology, it poses challenges such as scarcity of data, data privacy, and interpretability. We elaborately discuss these limitations and suggest multiple directions for future research in GAI for agriculture.},
  archive      = {J_CC},
  author       = {Sai, Siva and Kumar, Sanjeev and Gaur, Aanchal and Goyal, Shivam and Chamola, Vinay and Hussain, Amir},
  doi          = {10.1007/s12559-025-10420-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Unleashing the power of generative AI in agriculture 4.0 for smart and sustainable farming},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative deep learning framework for accurate plant
disease detection and crop productivity enhancement. <em>CC</em>,
<em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-025-10421-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern agriculture, the detection of plant diseases is crucial for enhancing crop productivity. Predicting disease onset and providing advice to farmers are essential steps to achieve increased yields on a large scale. The research addresses the critical need for timely and accurate plant leaf disease diagnosis to prevent growth issues. Leveraging deep learning advancements, the work confronts challenges like small lesion characteristics, distorted backgrounds, data imbalances, and limited generalization in agricultural datasets. After preprocessing the leaf images with tasks like data augmentation and resizing, a sheaf attention U-net with K-means clustering (SAUKC) is employed for segmentation to identify the region of interest. The segmented features are then input into the Orientation-guided Crystal Edge Deep Network (OCEDN) for infection detection. Fine-tuning with the improved kookaburra optimization algorithm (IKOA) addresses training challenges. The proposed method accurately identifies plant leaf diseases, achieving a remarkable accuracy rate of 98%. The validity of the statistical analysis is confirmed to substantiate the outcomes regarding accuracy, specificity, and recall.},
  archive      = {J_CC},
  author       = {M., Mohan and Anandamurugan, S.},
  doi          = {10.1007/s12559-025-10421-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Innovative deep learning framework for accurate plant disease detection and crop productivity enhancement},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verifying technical indicator effectiveness in
cryptocurrency price forecasting: A deep-learning time series model
based on sparrow search algorithm. <em>CC</em>, <em>17</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s12559-025-10422-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting cryptocurrency prices is challenging due to market volatility and dynamic behavior. This study aims to enhance prediction accuracy for Bitcoin (BTC), Ethereum (ETH), and Litecoin (LTC) by proposing a novel deep learning framework. The framework integrates the Sparrow Search Algorithm (SSA) for selecting optimal technical indicators with Bidirectional Long Short-Term Memory (Bi-LSTM) networks. Technical indicators derived from historical market data, including prices and trading volume, are analyzed to improve forecasting. The results demonstrate that the proposed framework effectively enhances prediction accuracy for BTC and LTC. For ETH, the best performance is achieved using all 34 indicators with the Bi-LSTM model. These findings highlight the importance of selecting relevant indicators and demonstrate the potential of advanced deep learning models in addressing the complexities of cryptocurrency markets. This research provides valuable insights and a reliable framework for improving cryptocurrency price predictions.},
  archive      = {J_CC},
  author       = {Cheng, Ching-Hsue and Yang, Jun-He and Dai, Jia-Pei},
  doi          = {10.1007/s12559-025-10422-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Verifying technical indicator effectiveness in cryptocurrency price forecasting: A deep-learning time series model based on sparrow search algorithm},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional connectivity imbalance between positive and
negative networks in mild cognitive impairment via feature selection.
<em>CC</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s12559-024-10399-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CC},
  author       = {Wu, Haifeng and Pu, Changlin},
  doi          = {10.1007/s12559-024-10399-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Functional connectivity imbalance between positive and negative networks in mild cognitive impairment via feature selection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute reduction in a hybrid decision information system
based on fuzzy conditional information entropy using iterative model and
matrix operation. <em>CC</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12559-024-10400-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction of hybrid decision information systems (HDISs) is a significant research area within the field of machine learning. Due to the presence of nominal attributes, it is difficult to accurately measure the distance between objects in HDISs, which often results in poor attribute reduction for these systems. Rough set theory (RST) is a crucial tool for attribute reduction, but it requires computation of upper and lower approximations, which often leads to computational difficulties. In response to the aforementioned issues, this paper proposes a fast attribute reduction algorithm for HDISs based on fuzzy conditional information entropy that utilizes an iterative model and matrix operations. Firstly, a novel measurement of the distance between nominal attribute values is defined using decision attributes. Subsequently, fuzzy conditional information entropy is calculated from the perspective of “the attribute values is fed back to the attribute set” and its properties are provided. Additionally, an iterative attribute reduction model and difference matrix are established, and two new matrix operations are introduced. Finally, an iterative attribute reduction algorithm is provided. The results of experiments and statistical tests on fifteen UCI datasets, including three large datasets, demonstrate that the proposed algorithm is more effective and efficient than nine state-of-the-art algorithms. This paper not only addresses the issue of difficulty in measuring the distance between nominal attribute values but also significantly improves the computational efficiency of attribute reduction algorithms based on RST, making it possible for them to be applied to large datasets.},
  archive      = {J_CC},
  author       = {Ma, Xiaoqin and Peng, Yichun and Yu, Wenchang and Xu, Yi and Zhang, Qinli and Li, Zhaowen},
  doi          = {10.1007/s12559-024-10400-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Attribute reduction in a hybrid decision information system based on fuzzy conditional information entropy using iterative model and matrix operation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-attribute group decision-making method under
linguistic q-rung orthopair fuzzy environment based on archimedean
copula and extended power average operator. <em>CC</em>, <em>17</em>(1),
1–28. (<a href="https://doi.org/10.1007/s12559-025-10409-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute group decision-making (MAGDM) refers to a series of decision-making problems that rank all possible alternatives based on decision makers’ cognition and evaluations over alternatives from multiple attributes. Hence, the precondition of MAGDM is felicitously describing decision makers’ fuzzy and uncertain cognitive information in complicated decision-making issues. The recently proposed linguistic q-rung orthopair fuzzy set (Lq-ROFS), which uses two linguistic terms to denote membership and non-membership degrees, has been proved to be an effective and promising tool to depict decision makers’ complex cognition in real MAGDM problems. Considering the drawbacks of existing Lq-ROFS-based decision-making methods, this paper focuses on MAGDM approaches where decision makers’ cognitive information is denoted by Lq-ROFSs. The main contribution of this paper is to propose a novel MAGDM method based on Lq-ROFSs. This paper introduces a new MAGDM method under Lq-ROFSs. In order to do this, this study first puts forward some new operational rules for linguistic q-rung orthopair fuzzy numbers (Lq-ROFNs) based on Archimedean copula. These new operational rules are more flexible than existing ones and some other operations can be derived by using different generators. Second, to effectively aggregate Lq-ROFNs, the extended power average operator is applied in linguistic q-rung orthopair fuzzy environment and based on the new operational rules, some novel aggregation operators are generated. Afterward, the developed aggregation operators are used in decision-making problems and a novel MAGDM method which concentrates on linguistic q-rung orthopair fuzzy decision environment is introduced. Specific steps of the new method are illustrated in detail and it is then applied in some illustrative examples to verify its effectiveness. Our proposed method is effective for handling MAGDM problems under Lq-ROFSs. Numerical examples have shown the effectiveness in handling realistic MAGDM problems. In addition, comparison with some existing methods illustrates the advantages and superiorities of our method. This paper introduces a new MAGDM method under Lq-ROFSs. This method is based on Archimedean copula, extended power average operator, and Lq-ROFSs, and is powerful and flexible to cope with MAGDM problems in reality.},
  archive      = {J_CC},
  author       = {Tang, Fangcheng and Zhang, Yushu and Wang, Jun},
  doi          = {10.1007/s12559-025-10409-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-attribute group decision-making method under linguistic q-rung orthopair fuzzy environment based on archimedean copula and extended power average operator},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering the cognitive bias of toxic language through
metaphorical concept mappings. <em>CC</em>, <em>17</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s12559-025-10423-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prosperity of social media, toxic language spreading over social media has become an unignorable challenge for individual mental health and social harmony. Many researchers have studied toxic language identification to control or mitigate it. However, it still leaves a blank in the cognitive patterns of toxic language. Metaphors as a common feature in natural language connect literal and metaphorical meanings, which could be a useful tool to study the underlying cognitive patterns of the text. In this paper, we utilize a metaphor processing tool, MetaPro, to process a public toxic language dataset and analyze the cognitive biases between toxic and non-toxic language, multiple levels and subtypes of toxic language as well as toxic language mentioning different genders, sexual orientations, and races. Our study demonstrates that significant differences exist in cognitive patterns of the above-mentioned categories and analyzes the differences with machine learning methods.},
  archive      = {J_CC},
  author       = {Ge, Mengshi and Mao, Rui and Cambria, Erik},
  doi          = {10.1007/s12559-025-10423-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Discovering the cognitive bias of toxic language through metaphorical concept mappings},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="cis---31">CIS - 31</h2>
<ul>
<li><details>
<summary>
(2025). ConvNeXt embedded u-net for semantic segmentation in urban
scenes of multi-scale targets. <em>CIS</em>, <em>11</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s40747-024-01735-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of urban scenes is essential in urban traffic analysis and road condition information acquisition. The semantic segmentation model with good performance is the key to applying high-resolution urban locations. However, the types of these images are diverse, and the spatial relationships are complex. It is greatly affected by weather and light. Objects of different scales pose significant challenges to image segmentation of urban scenes. The existing semantic segmentation is mostly solved from the target scale and superpixel methods. Our research mainly fills the gap in image segmentation field of ConvNeXt fusion U-Net pyramid network model in specific urban scenes. These methods could be more accurate. Therefore, we propose the multi-scale fusion deformation residual pyramid network model method in this paper. This method captures features of different scales and effectively solves the problem of urban scene image segmentation of memory scenes by objects of different scales. We construct a spatial information interaction module to reduce the semantic ambiguity caused by complex spatial relations. By combining spatial and channel characteristics, a series of problems caused by weather and light can be alleviated. We verify the improved semantic segmentation model on the Cityscape dataset. The experimental results show that the method achieves 84.25% MPA and 75.61% MIoU. Our improved algorithm, ConvNeXt embedding in the U-Net algorithm architecture, is named Conv-UNet. The improved method proposed in this paper is superior to other methods in the semantic segmentation of urban scenes. The main advantage of this algorithm is to explore the specific loss function and segmentation strategy suitable for urban scene in the face of the complexity and diversity of urban scene images.},
  archive      = {J_CIS},
  author       = {Wu, Yanyan and Li, Qian},
  doi          = {10.1007/s40747-024-01735-2},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Complex Intell. Syst.},
  title        = {ConvNeXt embedded U-net for semantic segmentation in urban scenes of multi-scale targets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model of feature extraction for well logging data based on
graph regularized non-negative matrix factorization with optimal
estimation. <em>CIS</em>, <em>11</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s40747-025-01783-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir oil-bearing recognition is the process of predicting reservoir types based on well logging data, which determines the accuracy of recognition. However, the original well logging data is multidimensional and contains potential noise, which can influence the performance of sequent processing, such as clustering and classification. It is crucial to obtain key low-dimensional features and study an accurate automatic recognition algorithm under unsupervised condition. To solve this problem, we propose a feature extraction method named graph regularized non-negative matrix factorization with optimal estimation (GNMF-OE) according to the characteristics of well logging data in this paper. Firstly, the low dimensional embedding dimension of high-dimensional well logging data is modeled and estimated, which enables the method to obtain the appropriate number of features that reflect the data structure. Secondly, local features are optimized by structured initial vectors in the framework of GNMF, which encourages the basis matrix to have clear reservoir category characteristics. These two approaches are meaningful and beneficial to construct an appropriate basis matrix that discovers the intrinsic structure of well logging data. The visualized experimental results on real datasets from Jianghan oilfield in China show that the proposed method has significant clustering performance for reservoir oil-bearing recognition.},
  archive      = {J_CIS},
  author       = {Yuan, Kehong and Shang, Youlin and Guo, Haixiang and Dong, Yongsheng and Liu, Zhonghua},
  doi          = {10.1007/s40747-025-01783-2},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {A model of feature extraction for well logging data based on graph regularized non-negative matrix factorization with optimal estimation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new representation in genetic programming with hybrid
feature ranking criterion for high-dimensional feature selection.
<em>CIS</em>, <em>11</em>(4), 1–24. (<a
href="https://doi.org/10.1007/s40747-025-01784-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a common method for improving classification performance. Selecting features for high-dimensional data is challenging due to the large search space. Traditional feature ranking methods that search for top-ranked features cannot remove redundant and irrelevant features and may also ignore interrelated features. Evolutionary computation (EC) techniques are widely used in feature selection due to their global search capability. However, EC can easily fall into local optima when dealing with feature selection for high-dimensional applications. The top-ranked features are more likely to construct effective feature subsets and help EC reduce the search space. This paper proposes a feature selection method based on Genetic Programming (GP) with hybrid feature ranking criterion called GPHC, which combines multiple feature ranking methods into the GP structure using a novel GP representation to search for effective feature subsets. Experiments on eight high-dimensional datasets show that GPHC achieves significantly better classification performance compared to five feature ranking methods. Further comparisons between GPHC and other evolutionary algorithms demonstrate that GPHC has advantages in terms of classification performance, the number of features, and convergence speed.},
  archive      = {J_CIS},
  author       = {Li, Jiayi and Zhang, Fan and Ma, Jianbin},
  doi          = {10.1007/s40747-025-01784-1},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Complex Intell. Syst.},
  title        = {A new representation in genetic programming with hybrid feature ranking criterion for high-dimensional feature selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demonstration and offset augmented meta reinforcement
learning with sparse rewards. <em>CIS</em>, <em>11</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s40747-025-01785-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces DOAMRL, a novel meta-reinforcement learning (meta-RL) method that extends the Model-Agnostic Meta-Learning (MAML) framework. The method addresses a key limitation of existing meta-RL approaches, which struggle to effectively use suboptimal demonstrations to guide training in sparse reward environments. DOAMRL effectively combines reinforcement learning (RL) and imitation learning (IL) within the inner loop of the MAML framework, with dynamically adjusted weights applied to the IL component. This enables the method to leverage the exploration strengths of RL and the efficiency benefits of IL at different stages of training. Additionally, DOAMRL introduces a meta-learned parameter offset, which enhances targeted exploration in sparse reward settings, helping to guide the meta-policy toward regions with non-zero rewards. To further mitigate the impact of suboptimal demonstration data on meta-training, we propose a novel demonstration data enhancement module that iteratively improves the quality of the demonstrations. We provide a comprehensive analysis of the proposed method, justifying its design choices. A comprehensive comparison with existing methods in various stages (including training and adaptation), using both optimal and suboptimal demonstrations, along with results from ablation and sensitivity analysis, demonstrates that DOAMRL outperforms existing approaches in performance, applicability, and robustness.},
  archive      = {J_CIS},
  author       = {Li, Haorui and Liang, Jiaqi and Wang, Xiaoxuan and Jiang, Chengzhi and Li, Linjing and Zeng, Daniel},
  doi          = {10.1007/s40747-025-01785-0},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Complex Intell. Syst.},
  title        = {Demonstration and offset augmented meta reinforcement learning with sparse rewards},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDGANets: A semantically enhanced dual graph-aware network
for affine and registration of remote sensing images. <em>CIS</em>,
<em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01792-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image pairs of different time phases have complex and changeable semantic contents, and traditional convolutional registration methods are challenging in modeling subtle local changes and global large-scale deformation differences in detail. This results in poor registration performance and poor feature representation. To address these problems, a semantically enhanced dual-graph perception framework is proposed. This framework aims to gradually achieve semantic alignment and precise registration of remote sensing image pairs of different time phases via coarse to fine stages. On the one hand, a newly designed large-selection kernel convolution attention module is used to learn affine transformation parameters. Attention to global semantics perceives the large pixel displacement deviation caused by large-scale deformation, and the association relationship is established between remote sensing image pairs of different time phases. At the same time, dual-graph perception modules are embedded in multiple subspace structures, and the subtle local changes of remote sensing image pairs are modeled through the dynamic aggregation ability of graph perception nodes to achieve coarse registration of remote sensing images. On the other hand, a U-shaped module guided by global attention with deformable convolution is used to refine the local spatial structural features and global contextual semantic information of the rough registration, establish dependencies between channels, and correct the pixel displacement deviation of remote sensing image pairs of different phases through position encoding. It is worth noting that the newly designed weighted loss function supervises the learning of each module and the entire network structure from the perspective of inverse consistency, promoting the network’s optimal performance. Finally, the experimental results on the AerialData and GFRS datasets show that the proposed framework has good registration performance, with mean absolute error (MAE) of 3.64 and 3.81, respectively.},
  archive      = {J_CIS},
  author       = {Zhuli, Xie and Gang, Wan and Jia, Liu and Dongdong, Bu},
  doi          = {10.1007/s40747-025-01792-1},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {SDGANets: A semantically enhanced dual graph-aware network for affine and registration of remote sensing images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSAformer: Bidirectional sequence splitting aggregation
attention mechanism for long term series forecasting. <em>CIS</em>,
<em>11</em>(4), 1–22. (<a
href="https://doi.org/10.1007/s40747-025-01794-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting plays a crucial role across various sectors, including energy, transportation, meteorology, and epidemiology. However, existing models often struggle with capturing long-term dependencies and managing computational efficiency when handling complex and extensive time series data. To address these challenges, this paper introduces the BSAformer model, which leverages a unique combination of frequency-domain Sequence Progressive Split-Aggregation (SPSA) and Bidirectional Splitting-Agg Attention (BSAA) mechanisms. The SPSA module decomposes sequences into seasonal and trend components, enhancing the model’s ability to identify cyclical patterns, while the BSAA mechanism captures forward and backward dependencies, providing a comprehensive understanding of temporal dynamics. Extensive experiments conducted on seven benchmark datasets demonstrate the BSAformer model&#39;s superior performance, with notable improvements in accuracy and efficiency over state-of-the-art models. Specifically, the BSAformer achieves significant Mean Squared Error (MSE) reductions of 63.7% on the ECL dataset, 28.1% on the Traffic dataset, and 49.8% on the ILI dataset. These results validate the model’s robustness and its adaptability across diverse time series forecasting scenarios. The insights gained from this study contribute to the advancement of time series forecasting by providing a model that improves both accuracy and computational efficiency, especially in handling long-term dependencies and complex temporal patterns.},
  archive      = {J_CIS},
  author       = {Zhu, QingBo and Han, JiaLin and Yang, Sheng and Xie, ZhiQiang and Tian, Bo and Wan, HaiBo and Chai, Kai},
  doi          = {10.1007/s40747-025-01794-z},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Complex Intell. Syst.},
  title        = {BSAformer: Bidirectional sequence splitting aggregation attention mechanism for long term series forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of weighted based divided-search enhanced
karnik–mendel algorithms for type reduction of general type-2 fuzzy
logic systems. <em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01798-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General type-2 fuzzy logic systems (GT2 FLSs) based on the $$\alpha$$ -planes representation of general T2 fuzzy sets (FSs) have become more accessible to FL investigators in recent years. Type reduction (TR) is the most important block for GT2 FLSs. Here the weighted type-reduction algorithms based on the Newton and Cotes quadrature formulas of numerical methods of integration technique are first given, and the searching spaces are divided. Then a type of weighted divided search enhanced Karnik–Mendel (WDEKM) algorithms is shown to complete the centroid TR. In contrast to the WEKM algorithms, four simulation instances show that the WDEKM algorithms get lesser absolute errors and faster calculational speeds, which may offer the potentially application values for applying T2 FLSs.},
  archive      = {J_CIS},
  author       = {Chen, Yang},
  doi          = {10.1007/s40747-025-01798-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Design of weighted based divided-search enhanced Karnik–Mendel algorithms for type reduction of general type-2 fuzzy logic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADWTune: An adaptive dynamic workload tuning system with
deep reinforcement learning. <em>CIS</em>, <em>11</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s40747-025-01801-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to reduce the burden of DBA, the knob tuning method based on reinforcement learning has been proposed and achieved good results in some cases. However, the performance of these solutions is not ideal as the workload features are not considered enough. To address these issues, we propose a database tuning system called ADWTune. In this model, ADWTune employs the idea of multiple sampling to gather workload data at different time points during the observation period. ADWTune uses these continuous data slices to characterize the dynamic changes in the workload. The key of ADWTune is its adaptive workload handling approach, which combines the dynamic features of workloads and the internal metrics of database as the state of the environment. At the same time, ADWTune includes a data repository, which reuses historical data to improve the adaptability of model to workload shifts. We conduct extensive experiments on various workloads. The experimental results demonstrate that ADWTune is better suited for dynamic environments than other methods based on reinforcement learning.},
  archive      = {J_CIS},
  author       = {Li, Cuixia and Wang, Junhai and Shi, Jiahao and Liu, Liqiang and Zhang, Shuyan},
  doi          = {10.1007/s40747-025-01801-3},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Complex Intell. Syst.},
  title        = {ADWTune: An adaptive dynamic workload tuning system with deep reinforcement learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math display">H<sup>2</sup>CAN</span>:
Heterogeneous hypergraph attention network with counterfactual learning
for multimodal sentiment analysis. <em>CIS</em>, <em>11</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s40747-025-01806-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) has garnered significant attention for its immense potential in human-computer interaction. While cross-modality attention mechanisms are widely used in MSA to capture inter-modality interactions, existing methods are limited to pairwise interactions between two modalities. Additionally, these methods can not utilize the causal relationship to guide attention learning, making them susceptible to bias information. To address these limitations, we introduce a novel method called Heterogeneous Hypergraph Attention Network with Counterfactual Learning $$(\text {H}^2\text {CAN}).$$ The method constructs a heterogeneous hypergraph based on sentiment expression characteristics and employs Heterogeneous Hypergraph Attention Networks (HHGAT) to capture interactions beyond pairwise constraints. Furthermore, it mitigates the effects of bias through a Counterfactual Intervention Task (CIT). Our model comprises two main branches: hypergraph fusion and counterfactual fusion. The former uses HHGAT to capture inter-modality interactions, while the latter constructs a counterfactual world using Gaussian distribution and additional weighting for the biased modality. The CIT leverages causal inference to maximize the prediction discrepancy between the two branches, guiding attention learning in the hypergraph fusion branch. We utilize unimodal labels to help the model adaptively identify the biased modality, thereby enhancing the handling of bias information. Experiments on three mainstream datasets demonstrate that $$\text {H}^2\text {CAN}$$ sets a new benchmark.},
  archive      = {J_CIS},
  author       = {Huang, Changqin and Lin, Zhenheng and Huang, Qionghao and Huang, Xiaodi and Jiang, Fan and Chen, Jili},
  doi          = {10.1007/s40747-025-01806-y},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {$$\text {H}^2\text {CAN}$$: Heterogeneous hypergraph attention network with counterfactual learning for multimodal sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal multilevel attention for semi-supervised
skeleton-based gesture recognition. <em>CIS</em>, <em>11</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s40747-025-01807-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although skeleton-based gesture recognition using supervised learning has achieved promising results, the reliance on extensive annotated data poses significant costs. This paper addresses the challenge of semi-supervised skeleton-based gesture recognition, to effectively learn feature representations from labeled and unlabeled data. To resolve this problem, we propose a novel multimodal multilevel attention network designed for semi-supervised learning. This model utilizes the self-attention mechanism to polymerize multimodal and multilevel complementary semantic information of the hand skeleton, designing a multimodal multilevel contrastive loss to measure feature similarity. Specifically, our method explores the relationships between joint, bone, and motion to learn more discriminative feature representations. Considering the hierarchy of the hand skeleton, the skeleton data is divided into multilevel to capture complementary semantic information. Furthermore, the multimodal contrastive loss measures similarity among these multilevel representations. The proposed method demonstrates improved performance in semi-supervised skeleton-based gesture recognition tasks, as evidenced by experiments on the SHREC-17 and DHG 14/28 datasets.},
  archive      = {J_CIS},
  author       = {Liu, Jinting and Gan, Minggang and He, Yuxuan and Guo, Jia and Hu, Kang},
  doi          = {10.1007/s40747-025-01807-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {Multimodal multilevel attention for semi-supervised skeleton-based gesture recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metalinguist: Enhancing hate speech detection with
cross-lingual meta-learning. <em>CIS</em>, <em>11</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s40747-025-01808-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of social media has led to an increase in hate speech. Hate speech is generally described as a deliberate act of aggression aimed at a particular group, intended to harm or marginalize them based on specific attributes of their identity. While positive interactions in diverse communities can greatly enhance confidence, it is important to acknowledge that negative remarks such as hate speech can weaken community unity and present a significant impact on people’s well-being. This highlights the need for improved monitoring and guidelines on social media platforms to protect individuals from discriminatory and harmful actions. Despite extensive research on resource-rich languages, such as English and German, the detection and analysis of hate speech in less-resourced languages, such as Norwegian, remains underexplored. Addressing this gap, our study leverages a metalinguistic approach that uses advanced meta-learning techniques to enhance the detection capabilities across bilingual texts, effectively linking technical advancements directly to the pressing social issue of hate speech. In this study, we introduce techniques that adapt models that deal with hate speech detection within the same languages (intra-lingual), across different languages (cross-lingual), and techniques that adapt models to new languages with minimal extra training, independent of the model type (cross-lingual model-agnostic meta-learning-based approaches) for bilingual text analysis in Norwegian and English. Our methodology incorporates attention mechanisms (components that help the model focus on relevant parts of the text) and adaptive learning rate schedulers (tools that adjust the learning speed based on performance). Our methodology incorporates components that help the model focus on relevant parts of the text (attention mechanisms) and tools that adjust the learning speed based on performance (adaptive learning rate schedulers). We conducted various experiments using language-specific and multilingual transformers. Among these, the combination of Nor-BERT and LSTM with zero-shot and few-shot model-agnostic meta-learning achieved remarkable F1 scores of 79% and 90%, highlighting the effectiveness of our proposed framework.},
  archive      = {J_CIS},
  author       = {Hashmi, Ehtesham and Yayilgan, Sule Yildirim and Abomhara, Mohamed},
  doi          = {10.1007/s40747-025-01808-w},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Complex Intell. Syst.},
  title        = {Metalinguist: Enhancing hate speech detection with cross-lingual meta-learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern mining-based evolutionary multi-objective algorithm
for beam angle optimization in intensity-modulated radiotherapy.
<em>CIS</em>, <em>11</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s40747-025-01809-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-objective algorithms have been applied to beam angle optimization (called BAO) for generating diverse trade-off radiotherapy treatment plans. However, their performance is not so effective due to the ignorance of using the specific clinical knowledge that can be obtain intuitively by clinical physicist. To address this issue, we suggest a pattern mining based evolutionary multi-objective algorithm called PM-EMA, in which two strategies for using the knowledge are proposed to accelerate the speed of population convergence. Firstly, to discover the potential beam angle distribution and discard the worse angles, the pattern mining strategy is used to detect the maximum and minimum sets of beam angles in non-dominated solutions of the population and utilize them to generate offspring to enhance the convergence. Moreover, to improve the quality of initial solutions, a tailored population initialization strategy is proposed by using the score of beam angles defined by this study. The experimental results on six clinical cancer cases demonstrate the superior performance of the proposed algorithm over six representative algorithms.},
  archive      = {J_CIS},
  author       = {Cao, Ruifen and Chen, Wei and Zhang, Tielu and Si, Langchun and Pei, Xi and Zhang, Xingyi},
  doi          = {10.1007/s40747-025-01809-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Complex Intell. Syst.},
  title        = {Pattern mining-based evolutionary multi-objective algorithm for beam angle optimization in intensity-modulated radiotherapy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact particle flow daum-huang filters for mobile robot
localization in occupancy grid maps. <em>CIS</em>, <em>11</em>(4), 1–15.
(<a href="https://doi.org/10.1007/s40747-025-01810-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel localization algorithm for mobile robots navigating in complex planar environments, a critical capability for various real-world applications such as autonomous driving, robotic assistance, and industrial automation. Although traditional methods such as particle filters and extended Kalman filters have been widely used, there is still room for assessing the capabilities of modern filtering techniques for this task. Building on a recent localization method that employs a chamfer distance-based observation model, derived from an implicit measurement equation, we explore its potential further by incorporating exact particle flow Daum–Huang filters to achieve superior accuracy. Recent advancements have spotlighted Daum–Huang filters as formidable contenders, outshining both the extended Kalman filters and traditional particle filters in various scenarios. We introduce two new Daum–Huang-based localization algorithms and assess their tracking performance through comprehensive simulations and real-world trials. Our algorithms are benchmarked against various methods, including the widely acclaimed Adaptive Monte–Carlo Localization algorithm. Overall, our algorithm demonstrates superior performance compared to the baseline models in simulations and exhibits competitive performance in the evaluated real-world application.},
  archive      = {J_CIS},
  author       = {Csuzdi, Domonkos and Bécsi, Tamás and Gáspár, Péter and Törő, Olivér},
  doi          = {10.1007/s40747-025-01810-2},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Complex Intell. Syst.},
  title        = {Exact particle flow daum-huang filters for mobile robot localization in occupancy grid maps},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal decoupling attention transformer for 3D
skeleton-based driver action recognition. <em>CIS</em>, <em>11</em>(4),
1–12. (<a href="https://doi.org/10.1007/s40747-025-01811-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver action recognition is crucial for in-vehicle safety. We argue that the following factors limit the related research. First, spatial constraints and obstructions in the vehicle restrict the range of motion, resulting in similar action patterns and difficulty collecting the full body posture. Second, in skeleton-based action recognition, establishing the joint dependencies by the self-attention computation is always limited to a single frame, ignoring the effect of body spatial structure on dependence weights and inter-frame. Common convolution in temporal flow only focuses on frame-level temporal features, ignoring motion pattern features at a higher semantic level. Our work proposed a novel spatiotemporal decoupling attention transformer (SDA-TR). The SDA module uses a spatiotemporal decoupling strategy to decouple the weight computation according to body structure and directly establish joint dependencies between multiple frames. The TFA module aggregates sub-action-level and frame-level temporal features to improve similar recognition accuracy. On the Driver Action Recognition dataset Drive&amp;Act using driver upper body skeletons, SDA-TR achieves state-of-the-art performance. SDA-TR also achieved 92.2%/95.8% accuracy under the CS/CV benchmarks of NTU RGB+D 60, 88.6%/89.8% accuracy under the CS/CSet benchmarks of NTU RGB+D 120, on par with other state-of-the-art methods. Our method demonstrates great scalability and generalization for action recognition.},
  archive      = {J_CIS},
  author       = {Xu, Zhuoyan and Xu, Jingke},
  doi          = {10.1007/s40747-025-01811-1},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Complex Intell. Syst.},
  title        = {Spatiotemporal decoupling attention transformer for 3D skeleton-based driver action recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised random walk manifold contrastive hashing for
multimedia retrieval. <em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01814-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth in both the variety and volume of data on networks, especially within social networks containing vast multimedia data such as text, images, and video, there is an urgent need for efficient methods to retrieve helpful information quickly. Due to their high computational efficiency and low storage costs, unsupervised deep cross-modal hashing methods have become the primary method for managing large-scale multimedia data. However, existing unsupervised deep cross-modal hashing methods still need help with issues such as inaccurate measurement of semantic similarity information, complex network architectures, and incomplete constraints among multimedia data. To address these issues, we propose an Unsupervised Random Walk Manifold Contrastive Hashing (URWMCH) method, designing a simple deep learning architecture. First, we build a random walk-based manifold similarity matrix based on the random walk strategy and modal-individual similarity structure. Second, we construct intra- and inter-modal similarity preservation and coexistent similarity preservation loss based on contrastive learning to constrain the training of hash functions, ensuring that the hash codes contain complete semantic association information. Finally, we designed comprehensive experiments on the MIRFlickr-25K, NUS-WIDE, and MS COCO datasets to demonstrate the effectiveness and superiority of the proposed URWMCH method.},
  archive      = {J_CIS},
  author       = {Chen, Yunfei and Long, Yitian and Yang, Zhan and Long, Jun},
  doi          = {10.1007/s40747-025-01814-y},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Unsupervised random walk manifold contrastive hashing for multimedia retrieval},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KeyBoxGAN: Enhancing 2D object detection through annotated
and editable image synthesis. <em>CIS</em>, <em>11</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s40747-025-01817-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample augmentation, especially sample generation is conducive for addressing the challenge of training robust image and video object detection models based on the deep learning. Still, the existing methods lack sample editing capability and suffer from annotation work. This paper proposes an image sample generation method based on key box points detection and Generative adversarial network (GAN), named as KeyBoxGAN, to make image sample generation labeled and editable. KeyBoxGAN firstly predefines key box points positions, embeddings which control the objects’ positions and then the corresponding masks are generated according to Mahalanobis–Gaussuan heatmaps and Swin Transformer-SPADE generator to control objects’ generation regions, as well as the background generation. This adaptive and precisely supervised image generation method disentangles object position and appearance, enables image editable and self-labeled abilities. The experiments show KeyBoxGAN surpasses DCGAN, StyleGAN2 and DDPM in objective assessments, including Inception Distance (FID), Inception Score (IS), and Multi-Scale Structural Similarity Index (MS-SSIM), as well as in subjective evaluations by showing better visual quality. Moreover, the editable and self-labeled image generation capabilities make it a valuable tool in addressing challenges like occlusion, deformation, and varying environmental conditions in the 2D object detection.},
  archive      = {J_CIS},
  author       = {Bai, Yashuo and Song, Yong and Dong, Fei and Li, Xu and Zhou, Ya and Liao, Yizhao and Huang, Jinxiang and Yang, Xin},
  doi          = {10.1007/s40747-025-01817-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Complex Intell. Syst.},
  title        = {KeyBoxGAN: Enhancing 2D object detection through annotated and editable image synthesis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel robust multi-objective evolutionary optimization
algorithm based on surviving rate. <em>CIS</em>, <em>11</em>(4), 1–25.
(<a href="https://doi.org/10.1007/s40747-025-01822-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary optimization is widely utilized in industrial design. Despite the success of multi-objective evolutionary optimization algorithms in addressing complex optimization problems, research focusing on input disturbances remains limited. In many manufacturing processes, design parameters are vulnerable to random input disturbances, resulting in products that often perform less effectively than anticipated. To address this issue, we propose a novel robust multi-objective evolutionary optimization algorithm based on the concept of survival rate. The algorithm comprises two stages: the evolutionary optimization stage and the construction stage of the robust optimal front. In the former stage, we introduce the survival rate as a new optimization objective. Subsequently, we seek a robust optimal front that concurrently addresses convergence and robustness by employing a non-dominated sorting approach. Furthermore, we propose a precise sampling method and a random grouping mechanism to accurately recover solutions resilient to real noise while ensuring population’s diversity. In the latter stage, we introduce a performance measure that integrates both robustness and convergence to guide the construction of the robust optimal front. Experimental results demonstrate the superiority of the proposed algorithm in terms of both convergence and robustness compared to existing approaches under noisy conditions.},
  archive      = {J_CIS},
  author       = {Jiang, Wenxiang and Gao, Kai and Zhu, Shuwei and Xu, Lihong},
  doi          = {10.1007/s40747-025-01822-y},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Complex Intell. Syst.},
  title        = {A novel robust multi-objective evolutionary optimization algorithm based on surviving rate},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust-aware privacy-preserving QoS prediction with graph
neural collaborative filtering for internet of things services.
<em>CIS</em>, <em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01824-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The booming development of the Internet of Things (IoT) has led to an explosion of web services, making it more inconvenient for users to choose satisfactory services among numerous options. Therefore, ensuring quality of service (QoS) in a service-oriented IoT environment is crucial, highlighting QoS prediction as a prominent research focus. However, issues related to information credibility, user data privacy, and prediction accuracy in QoS prediction for IoT services have become significant challenges in current research. To tackle these issues, we propose TPP-GNCF, a trust-aware privacy-preserving QoS prediction framework that integrates graph neural networks with collaborative filtering methods. In TPP-GNCF, we filter out untrustworthy QoS values provided by users for certain services to select credible QoS values. Then, a message-passing graph neural network (MP-GNN) is utilized to effectively capture information transmission and relationships in the graph structure, while differential privacy is used to protect user node information. In addition, we use a similarity calculation method based on weight function in collaborative filtering to mine implicit embedded features that graph neural networks cannot directly utilize. Finally, the final missing QoS values are achieved by fusing graph neural predicted QoS and feature collaborative filtering predicted QoS. We conducted extensive experiments on the well-known WS-DREAM dataset. The results demonstrate that the TPP-GNCF framework not only surpasses existing schemes in performance but also effectively addresses issues of information credibility and user privacy.},
  archive      = {J_CIS},
  author       = {Wang, Weiwei and Ma, Wenping and Yan, Kun},
  doi          = {10.1007/s40747-025-01824-w},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {Trust-aware privacy-preserving QoS prediction with graph neural collaborative filtering for internet of things services},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small sample smart contract vulnerability detection method
based on multi-layer feature fusion. <em>CIS</em>, <em>11</em>(4), 1–26.
(<a href="https://doi.org/10.1007/s40747-025-01782-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of vulnerabilities in smart contracts is necessary for ensuring their security. As a pre-trained language model, BERT has been employed in the detection of smart contract vulnerabilities, exhibiting high accuracy in tasks. However, it has certain limitations. Existing methods solely depend on features extracted from the final layer, thereby disregarding the potential contribution of features from other layers. To address these issues, this paper proposes a novel method, which is named multi-layer feature fusion (MULF). Experiments investigate the impact of utilizing features from other layers on performance improvement. To the best of our knowledge, this is the first instance of multi-layer feature sequence fusion in the field of smart contract vulnerability detection. Furthermore, there is a special type of patched contract code that contains vulnerability features which need to be studied. Therefore, to overcome the challenges posed by limited smart contract vulnerability datasets and high false positive rates, we introduce a data augmentation technique that incorporates function feature screening with those special smart contracts into the training set. To date, this method has not been reported in the literature. The experimental results demonstrate that the MULF model significantly enhances the performance of smart contract vulnerability identification compared to other models. The MULF model achieved accuracies of 98.95% for reentrancy vulnerabilities, 96.27% for timestamp dependency vulnerabilities, and 87.40% for overflow vulnerabilities, which are significantly higher than those achieved by existing methods.},
  archive      = {J_CIS},
  author       = {Fan, Jinlin and He, Yaqiong and Wu, Huaiguang},
  doi          = {10.1007/s40747-025-01782-3},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Complex Intell. Syst.},
  title        = {Small sample smart contract vulnerability detection method based on multi-layer feature fusion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CycleGuardian: A framework for automatic respiratory sound
classification based on improved deep clustering and contrastive
learning. <em>CIS</em>, <em>11</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s40747-025-01800-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auscultation plays a pivotal role in early respiratory and pulmonary disease diagnosis. Despite the emergence of deep learning-based methods for automatic respiratory sound classification post-Covid-19, limited datasets impede performance enhancement. Distinguishing between normal and abnormal respiratory sounds poses challenges due to the coexistence of normal respiratory components and noise components in both types. Moreover, different abnormal respiratory sounds exhibit similar anomalous features, hindering their differentiation. Besides, existing state-of-the-art models suffer from excessive parameter size, impeding deployment on resource-constrained mobile platforms. To address these issues, we design a lightweight network CycleGuardian and propose a framework based on an improved deep clustering and contrastive learning. We first generate a hybrid spectrogram for feature diversity and grouping spectrograms to facilitate intermittent abnormal sound capture. Then, CycleGuardian integrates a deep clustering module with a similarity-constrained clustering component to improve the ability to capture abnormal features and a contrastive learning module with group mixing for enhanced abnormal feature discernment. Multi-objective optimization enhances overall performance during training. In experiments, we use the ICBHI2017 dataset, following the official split method and without any pre-trained weights, our method achieves Sp: 82.06 $$\%$$ , Se: 44.47 $$\%$$ , and Score: 63.26 $$\%$$ with a network model size of 38 M. Compared to the current model, our method leads by nearly 7 $$\%$$ , achieving the current best performances. Additionally, we deploy the network on Android devices, showcasing a comprehensive intelligent respiratory sound auscultation system.},
  archive      = {J_CIS},
  author       = {Chu, Yun and Wang, Qiuhao and Zhou, Enze and Fu, Ling and Liu, Qian and Zheng, Gang},
  doi          = {10.1007/s40747-025-01800-4},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Complex Intell. Syst.},
  title        = {CycleGuardian: A framework for automatic respiratory sound classification based on improved deep clustering and contrastive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized non-convex online optimization with adaptive
momentum estimation and quantized communication. <em>CIS</em>,
<em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01818-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider the decentralized non-convex online optimization problem over an undirected network. To solve the problem over a communication-efficient manner, we propose a novel quantized decentralized adaptive momentum gradient descent algorithm based on the adaptive momentum estimation methods, where quantified information is exchanged between agents. The proposed algorithm not only can effectively reduce the data transmission volume but also contribute to improved convergence. Theoretical analysis proves that the proposed algorithm can achieve sublinear dynamic regret under appropriate step-size and quantization level, which matches the convergence of the decentralized online algorithm with exact-communication. Extensive simulations are given to demonstrate the efficacy of the algorithm.},
  archive      = {J_CIS},
  author       = {Lv, Yunshan and Xiong, Hailing and Zhang, Fuqing and Dong, Shengying and Dai, Xiangguang},
  doi          = {10.1007/s40747-025-01818-8},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {Decentralized non-convex online optimization with adaptive momentum estimation and quantized communication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A local search with chain search path strategy for
real-world many-objective vehicle routing problem. <em>CIS</em>,
<em>11</em>(4), 1–32. (<a
href="https://doi.org/10.1007/s40747-025-01825-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on a new application-oriented variant of vehicle routing problem. This problem comes from the daily distribution scenarios of a real-world logistics company. It is a large-scale (with customer sizes up to 2000), many-objective (with six objective functions) NP-hard problem with six constraints. Then, a local search with chain search path strategy (LS-CSP) is proposed to effectively solve the problem. It is a decomposition-based algorithm. First, the considered problem is decomposed into multiple single-objective subproblems. Then, local search is applied to solve these subproblems one by one. The advantage of the LS-CSP lies in a chain search path strategy, which is designed for determining the order of solving the subproblems. This strategy can help the algorithm find a high-quality solution set quickly. Finally, to assess the performance of the proposed LS-CSP, three instance sets containing 132 instances are provided, and four state-of-the-art decomposition-based approaches are adopted as the competitors. Experimental results show the effectiveness of the proposed algorithm for the considered problem.},
  archive      = {J_CIS},
  author       = {Zhou, Ying and Kong, Lingjing and Wang, Hui and Cai, Yiqiao and Liu, Shaopeng},
  doi          = {10.1007/s40747-025-01825-9},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Complex Intell. Syst.},
  title        = {A local search with chain search path strategy for real-world many-objective vehicle routing problem},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled pixel-wise correction for abdominal multi-organ
segmentation. <em>CIS</em>, <em>11</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s40747-025-01796-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism has emerged as a crucial component in medical image segmentation. Attention-based deep neural networks (ADNNs) fundamentally engage in the iterative computation of gradients for both input layers and weight parameters. Our research reveals a remarkable similarity between the optimization trajectory of ADNN and non-negative matrix factorization (NMF), where the latter involves the alternate adjustment of the base and coefficient matrices. This similarity implies that the alternating optimization strategy—characterized by the adjustment of input features by the attention mechanism and the adjustment of network weights—is central to the efficacy of attention mechanisms in ADNNs. Drawing an analogy to the NMF approach, we advocate for a pixel-wise adjustment of the input layer within ADNNs. Furthermore, to reduce the computational burden, we have developed a decoupled pixel-wise attention module (DPAM) and a self-attention module (DPSM). These modules are designed to counteract the challenges posed by the high inter-class similarity among different organs when performing multi-organ segmentation. The integration of our DPAM and DPSM into traditional network architectures facilitates the creation of an NMF-inspired ADNN framework, known as the DPC-Net, which comes in two variants: DPCA-Net for attention and DPCS-Net for self-attention. Our extensive experiments on the Synapse and FLARE22 datasets demonstrate that the DPC-Net achieves satisfactory performance and visualization results with lower computational cost. Specifically, the DPC-Net achieved a Dice score of 77.98% on the Synapse dataset and 87.04% on the FLARE22 dataset, while possessing merely 14.991 million parameters. Notably, our findings indicate that DPC-Net, when equipped with convolutional attention, surpasses those networks utilizing Transformer attention mechanisms on multi-organ segmentation tasks. Our code is available at https://github.com/605671435/DPC-Net .},
  archive      = {J_CIS},
  author       = {Yu, Xiangchun and Ding, Longjun and Zhang, Dingwen and Wu, Jianqing and Liang, Miaomiao and Zheng, Jian and Pang, Wei},
  doi          = {10.1007/s40747-025-01796-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {Decoupled pixel-wise correction for abdominal multi-organ segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing geometric modeling in convolutional neural
networks: Limit deformable convolution. <em>CIS</em>, <em>11</em>(4),
1–14. (<a href="https://doi.org/10.1007/s40747-025-01799-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are constrained in their capacity to model geometric transformations due to their fixed geometric structure. To overcome this problem, researchers introduce deformable convolution, which allows the convolution kernel to be deformable on the feature map. However, deformable convolution may introduce irrelevant contextual information during the learning process and thus affect the model performance. DCNv2 introduces a modulation mechanism to control the diffusion of the sampling points to control the degree of contribution of offsets through weights, but we find that such problems still exist in practical use. Therefore, we propose a new limit deformable convolution to address this problem, which enhances the model ability by adding adaptive limiting units to constrain the offsets and adjusts the weight constraints on the offsets to enhance the image-focusing ability. In the subsequent work, we perform lightweight work on the limit deformable convolution and design three kinds of LDBottleneck to adapt to different scenarios. The limit deformable network, equipped with the optimal LDBottleneck, demonstrated an improvement in mAP75 of 1.4% compared to DCNv1 and 1.1% compared to DCNv2 on the VOC2012+2007 dataset. Furthermore, on the CoCo2017 dataset, different backbones equipped with our limit deformable module achieved satisfactory results. The source code for this work is publicly available at https://github.com/1977245719/LDCN.},
  archive      = {J_CIS},
  author       = {Wang, Wei and Meng, Yuanze and Li, Han and Chang, Guiyong and Li, Shun and Zhang, Chenghong},
  doi          = {10.1007/s40747-025-01799-8},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Enhancing geometric modeling in convolutional neural networks: Limit deformable convolution},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive transplanting of black-box adversarial
attacks from multi-class to multi-label models. <em>CIS</em>,
<em>11</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s40747-025-01805-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples generated by perturbing raw data with carefully designed, imperceptible noise have emerged as a primary security threat to artificial intelligence systems. In particular, black-box adversarial attack algorithms, which only rely on model input and output to generate adversarial examples, are easy to implement in real scenarios. However, previous research on black-box attacks has primarily focused on multi-class classification models, with relatively few studies on black-box attack algorithms for multi-label classification models. Multi-label classification models exhibit significant differences from multi-class classification models in terms of structure and output. The former can assign multiple labels to a single sample, with these labels often exhibiting correlations, while the latter classifies a sample as the class with the highest confidence. Therefore, existing multi-class attack algorithms cannot directly attack multi-label classification models. In this paper, we study the transplantation methods of multi-class black-box attack algorithms to multi-label classification models and propose the multi-label versions for eight classic black-box attack algorithms, which include three score-based attacks and five decision-based (label-only) attacks, for the first time. Experimental results indicate that the transplanted black-box attack algorithms demonstrate effective attack performance across various attack types, except for extreme attacks. Especially, most transplanted attack algorithms achieve more than 60% success rate on the ML-GCN model and more than 30% on the ML-LIW model under the hiding all attack type. However, the performance of these transplanted attack algorithms shows variation among different attack types due to the correlations between labels.},
  archive      = {J_CIS},
  author       = {Chen, Zhijian and Zhou, Qi and Liu, Yujiang and Luo, Wenjian},
  doi          = {10.1007/s40747-025-01805-z},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Complex Intell. Syst.},
  title        = {A comprehensive transplanting of black-box adversarial attacks from multi-class to multi-label models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AILDP: A research on ship number recognition technology for
complex scenarios. <em>CIS</em>, <em>11</em>(4), 1–20. (<a
href="https://doi.org/10.1007/s40747-025-01820-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of global maritime trade and the increasingly urgent need for maritime surveillance and security management, fast and accurate identification of vessels has become a crucial aspect. The task of ship number recognition mainly faces two challenges: first, the ship number is usually located in different parts of the hull, and due to the shooting distance, the size of the ship number can vary greatly on different vessels, making automated recognition complex. Second, adverse weather conditions and complex sea surface environments may affect the accuracy of visual recognition. To address the above issues, we produce a private dataset containing 2436 images of ships in a variety of scenarios and propose an algorithm (AILDP) for interactive feature learning and adaptive enhancement to tackle multiple challenges in ship number recognition. Firstly, in the detection phase, for the problem of varying size and position in the ship number recognition task, the detection effect is optimized by a module (AIFI_LPE) that combines feature interaction and learned position encoding. Secondly, to deal with the issues of blurring and occlusion of ship numbers due to ship movement or bad weather, a module (C2f_IRMB_DRB) is proposed that can capture high-quality features while weighing the computational effort when processing low-quality images. After detection, the results are divided into two categories: clear ship number and low-quality ship number. In order to save computational resources, only the low-quality images are first subjected to preliminary image enhancement processing, and then the Thin Plate Spline (TPS) is introduced in the recognition part based on the framework of PaddleOCRv4 and combined with the feature extraction and enhancement module to adjust the spatial features of the images to ensure that both types of ship number images can be accurately processed in the feature extraction and recognition process. Experimental results show that the AILDP can improve the accuracy of ship number recognition, with the precision, recall, and mAP0.5 for ship number detection increased to 95.7%, 94.5%, and 94.8%. The Character_accuracy of the recognition task can reach 95.23%.},
  archive      = {J_CIS},
  author       = {Wei, Tianjiao and Hu, Zhuhua and Zhao, Yaochi and Fan, Xiyu},
  doi          = {10.1007/s40747-025-01820-0},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Complex Intell. Syst.},
  title        = {AILDP: A research on ship number recognition technology for complex scenarios},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mamba meets tracker: Exploiting token aggregation and
diffusion for robust unmanned aerial vehicles tracking. <em>CIS</em>,
<em>11</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s40747-025-01821-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer-based tracking approach achieves excellent results in unmanned aerial vehicles (UAV) tracking tasks. However, the existing tracking framework usually deals with this problem by visual grounding and visual tracking separately. This independent framework does not consider the correlation between the two steps mentioned above, that is, natural language description can provide global semantic information. Meanwhile, a separate framework is unable to conduct end-to-end training. As a remedy, We propose a joint natural language Mamba based tracking framework (named TADMT). Specifically, we propose a token aggregator that condenses rich features into a small number of visual tokens through a coarse to fine strategy to improve subsequent tracking speed. Then, we designed a mamba module based on the serpentine scanning strategy to effectively establish the relationship between natural language and visual images. In addition, we have designed a novel shift add multilayer perceptron in the prediction head, with the aim of achieving final classification and localization with less computation. Numerous experimental results have shown that TADMT achieves good tracking performance on six UAV tracking datasets and three general tracking datasets, with an average speed of 120FPS. The experimental results on the embedded platform also demonstrate the applicability of TADMT on UAV platforms.},
  archive      = {J_CIS},
  author       = {Du, Guocai and Zhou, Peiyong and Yadikar, Nurbiya and Aysa, Alimjan and Ubul, Kurban},
  doi          = {10.1007/s40747-025-01821-z},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Complex Intell. Syst.},
  title        = {Mamba meets tracker: Exploiting token aggregation and diffusion for robust unmanned aerial vehicles tracking},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling object mask interaction for compositional action
recognition. <em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01823-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions can be abstracted as interactions between humans and objects. The recently proposed task of compositional action recognition emphasizes the independence and combinability of verbs (actions) and nouns (humans or objects) constituting human actions. Nonetheless, most traditional appearance-based action recognition methods usually extract spatial-temporal features from input videos concurrently to understand actions. This approach tends to excessively rely on overall appearance features and lacks precise modelling of interactions between objects, often leading to the neglect of the actions themselves. Consequently, the biases introduced by the appearance prevent the model from effectively generalizing to unseen combinations of actions and objects. To address this issue, we propose a method that explicitly models the object interaction path, aiming to capture interactions between humans and objects. The advantage of this approach is that these interactions are not affected by the object or environmental appearance bias, providing additional clues for appearance-based action recognition methods. Our method can easily be combined with any appearance-based visual encoder, significantly improving the compositional generalization ability of action recognition algorithms. Extensive experimental results on the Something-Else dataset and the IKEA-Assembly dataset demonstrate the effectiveness of our approach.},
  archive      = {J_CIS},
  author       = {Li, Xinya and Shen, Zhongwei and Xu, Benlian and Li, Rongchang and Lu, Mingli and Cong, Jinliang and Zhang, Longxin},
  doi          = {10.1007/s40747-025-01823-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Modelling object mask interaction for compositional action recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target pursuit for multi-AUV system: Zero-sum stochastic
game with WoLF-PHC assisted. <em>CIS</em>, <em>11</em>(4), 1–16. (<a
href="https://doi.org/10.1007/s40747-025-01788-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of the underwater environment and the difficulty of the underwater energy recharging, utilizing multiple autonomous underwater vehicles (AUVs) to pursue the invading vehicle is a challenging project. This paper focuses on devising the rational and energy-efficient pursuit motion for a multi-AUV system in an unknown three-dimensional environment. Firstly, the pursuit system model is constructed on the two-player zero-sum stochastic game (ZSSG) framework. This framework enables the fictitious play on the behaviors of the invading AUV. Fictitious play involves players updating their strategies by observing and inferring the actions of others under incomplete information. Under this framework, a relay-pursuit mechanism is adopted by the pursuit system to form the action set in an energy-efficient way. Then, to reflect the pursuit goals of capturing the invading vehicle as soon as possible and avoid it from reaching its point of attack, two corresponding pursuit factors are considered in the designed reward function. To enable the pursuit AUVs to navigate in an unknown environment, WoLF-PHC algorithm is introduced and applied to the proposed ZSSG-based model. Finally, simulations demonstrate the effectiveness, the advantages, and the robustness of the proposed approach.},
  archive      = {J_CIS},
  author       = {Hong, Le and Cui, Weicheng},
  doi          = {10.1007/s40747-025-01788-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Complex Intell. Syst.},
  title        = {Target pursuit for multi-AUV system: Zero-sum stochastic game with WoLF-PHC assisted},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective optimization approach for scheduling electric
ground-handling vehicles in an airport. <em>CIS</em>, <em>11</em>(4),
1–27. (<a href="https://doi.org/10.1007/s40747-025-01815-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce airport operating costs and minimize environmental pollution, converting ground-handling vehicles from fuel-powered to electric ones is inevitable. However, this transformation introduces complexity in scheduling due to additional factors, such as battery capacities and charging requirements. This study models the electric ground-handling vehicle scheduling problem as a bi-objective integer programming model to address these challenges. The objectives of this model are to minimize the total travel distance of vehicles serving flights and the standard deviation of the total occupancy time for each vehicle. In order to solve this model and generate optimal scheduling solutions, this study combines the non-dominated sorting genetic algorithm 2 (NSGA2) with the large neighborhood search (LNS) algorithm, proposing a novel NSGA2-LNS algorithm. A dynamic priority method is used by the NSGA2-LNS to construct the initial population, thereby speeding up the convergence. The NSGA2-LNS employs the LNS algorithm to overcome the problem that metaheuristic algorithms often lack clear directions in the process of finding solutions. In addition, this study designs the correlation-based destruction operator and the priority-based repair operator in the NSGA2-LNS algorithm, thereby significantly enhancing its ability to find optimal solutions for the electric ground-handling vehicle scheduling problem. The algorithm is verified using flight data from Chengdu Shuangliu International Airport and is compared with manual scheduling methods and traditional multi-objective optimization algorithms. Experimental results demonstrate that the NSGA2-LNS can rapidly solve the scheduling problem of allocating electric ground-handling vehicles for hundreds of flights and produce high-quality scheduling solutions.},
  archive      = {J_CIS},
  author       = {Fu, Weigang and Li, Jiawei and Liao, Zhe and Fu, Yaoming},
  doi          = {10.1007/s40747-025-01815-x},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Complex Intell. Syst.},
  title        = {A bi-objective optimization approach for scheduling electric ground-handling vehicles in an airport},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminator guided visible-to-infrared image translation.
<em>CIS</em>, <em>11</em>(4), 1–14. (<a
href="https://doi.org/10.1007/s40747-025-01827-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a discriminator-guided visible-to-infrared image translation algorithm based on a generative adversarial network and designs a multi-scale fusion generative network. The generative network enhances the perception of the image’s fine-grained features by fusing features of different scales in the channel direction. Meanwhile, the discriminator performs the infrared image reconstruction task, which provides additional infrared information to train the generator. This enhances the convergence efficiency of generator training through soft label guidance generated through knowledge distillation. The experimental results show that compared to the existing typical infrared image generation algorithms, the proposed method can generate higher-quality infrared images and achieve better performance in both subjective visual description and objective metric evaluation, and that it has better performance in the downstream tasks of the template matching and image fusion tasks.},
  archive      = {J_CIS},
  author       = {Ma, Decao and Su, Juan and Xian, Yong and Li, Shaopeng},
  doi          = {10.1007/s40747-025-01827-7},
  journal      = {Complex &amp; Intelligent Systems},
  month        = {4},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Complex Intell. Syst.},
  title        = {Discriminator guided visible-to-infrared image translation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="cms---8">CMS - 8</h2>
<ul>
<li><details>
<summary>
(2025). High-order self-excited threshold integer-valued
autoregressive model: Estimation and testing. <em>CMS</em>,
<em>13</em>(1), 233–260. (<a
href="https://doi.org/10.1007/s40304-022-00325-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the need of modeling and inference for high-order integer-valued threshold time series models, this paper introduces a pth-order two-regime self-excited threshold integer-valued autoregressive (SETINAR(2, p)) model. Basic probabilistic and statistical properties of the model are discussed. The parameter estimation problem is addressed by means of conditional least squares and conditional maximum likelihood methods. The asymptotic properties of the estimators, including the threshold parameter, are obtained. A method to test the nonlinearity of the underlying process is provided. Some simulation studies are conducted to show the performances of the proposed methods. Finally, an application to the number of people suffering from meningococcal disease in Germany is provided.},
  archive      = {J_CMS},
  author       = {Yang, Kai and Li, Ang and Li, Han and Dong, Xiaogang},
  doi          = {10.1007/s40304-022-00325-3},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {233-260},
  shortjournal = {Commun. Math. Stat.},
  title        = {High-order self-excited threshold integer-valued autoregressive model: Estimation and testing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic geometric iterative method for loop subdivision
surface fitting. <em>CMS</em>, <em>13</em>(1), 217–231. (<a
href="https://doi.org/10.1007/s40304-022-00323-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a stochastic geometric iterative method (S-GIM) to approximate the high-resolution 3D models by finite loop subdivision surfaces. Given an input mesh as the fitting target, the initial control mesh is generated using the mesh simplification algorithm. Then, our method adjusts the control mesh iteratively to make its finite loop subdivision surface approximate the input mesh. In each geometric iteration, we randomly select part of points on the subdivision surface to calculate the difference vectors and distribute the vectors to the control points. Finally, the control points are updated by adding the weighted average of these difference vectors. We prove the convergence of S-GIM and verify it by demonstrating error curves in the experiment. In addition, compared with existing geometric iterative methods, S-GIM has a shorter running time under the same number of iteration steps.},
  archive      = {J_CMS},
  author       = {Xu, Chenkai and He, Yaqi and Hu, Hui and Lin, Hongwei},
  doi          = {10.1007/s40304-022-00323-5},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {217-231},
  shortjournal = {Commun. Math. Stat.},
  title        = {Stochastic geometric iterative method for loop subdivision surface fitting},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of informatively interval-censored case–cohort
studies with the application to HIV vaccine trials. <em>CMS</em>,
<em>13</em>(1), 195–215. (<a
href="https://doi.org/10.1007/s40304-022-00322-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case–cohort studies are commonly used in various investigations, and many methods have been proposed for their analyses. However, most of the available methods are for right-censored data or assume that the censoring is independent of the underlying failure time of interest. In addition, they usually apply only to a specific model such as the Cox model that may often be restrictive or violated in practice. To relax these assumptions, we discuss regression analysis of interval-censored data, which arise more naturally in case–cohort studies than and include right-censored data as a special case, and propose a two-step inverse probability weighting estimation procedure under a general class of semiparametric transformation models. Among other features, the approach allows for informative censoring. In addition, an EM algorithm is developed for the determination of the proposed estimators and the asymptotic properties of the proposed estimators are established. Simulation results indicate that the approach works well for practical situations and it is applied to a HIV vaccine trial that motivated this investigation.},
  archive      = {J_CMS},
  author       = {Du, Mingyue and Zhou, Qingning},
  doi          = {10.1007/s40304-022-00322-6},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {195-215},
  shortjournal = {Commun. Math. Stat.},
  title        = {Analysis of informatively interval-censored Case–Cohort studies with the application to HIV vaccine trials},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical bayesian approach for finite mixture of mode
regression model using skew-normal distribution. <em>CMS</em>,
<em>13</em>(1), 173–194. (<a
href="https://doi.org/10.1007/s40304-022-00320-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data that exhibit asymmetrical behavior can be well modeled with skew-normal random errors. Moreover, data that arise from a heterogeneous population can be efficiently analyzed by a finite mixture of regression models. These observations motivate us to propose a novel finite mixture of mode regression model based on a mixture of the skew-normal distributions to explore asymmetrical data from several subpopulations. Thanks to the stochastic representation of the skew-normal distribution, we construct a Bayesian hierarchical modeling framework and then develop an efficient Markov chain Monte Carlo sampling algorithm to generate posterior samples for obtaining the Bayesian estimates of the unknown parameters and their corresponding standard errors. Simulation studies and a real-data example are presented to illustrate the performance of the proposed Bayesian methodology.},
  archive      = {J_CMS},
  author       = {Zeng, Xin and Wang, Min and Ju, Yuanyuan and Wu, Liucang},
  doi          = {10.1007/s40304-022-00320-8},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {173-194},
  shortjournal = {Commun. Math. Stat.},
  title        = {A hierarchical bayesian approach for finite mixture of mode regression model using skew-normal distribution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature screening and error variance estimation for
ultrahigh-dimensional linear model with measurement errors.
<em>CMS</em>, <em>13</em>(1), 139–171. (<a
href="https://doi.org/10.1007/s40304-022-00317-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we mainly study the feature screening and error variance estimation in ultrahigh-dimensional linear model with errors-in-variables (EV). Given that sure independence screening (SIS) method by marginal Pearson’s correlation learning may omit some important observation variables due to measurement errors, a corrected SIS called EVSIS is proposed to rank the importance of features according to their corrected marginal correlation with the response variable. Also, a corrected error variance procedure is proposed to accurately estimate the error variance, which could greatly attenuate the influence of measurement errors and spurious correlations, simultaneously. Under some regularization conditions, the proposed EVSIS possesses sure screening property and consistency in ranking and the corrected error variance estimator is also proved to be asymptotically normal. The two methodologies are illustrated by some simulations and a real data example, which suggests that the proposed methods perform well.},
  archive      = {J_CMS},
  author       = {Cui, Hengjian and Zou, Feng and Ling, Li},
  doi          = {10.1007/s40304-022-00317-3},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {139-171},
  shortjournal = {Commun. Math. Stat.},
  title        = {Feature screening and error variance estimation for ultrahigh-dimensional linear model with measurement errors},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum n-toroidal algebras and extended quantized GIM
algebras of n-fold affinization. <em>CMS</em>, <em>13</em>(1), 99–137.
(<a href="https://doi.org/10.1007/s40304-022-00316-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the notion of quantum N-toroidal algebras as natural generalization of the quantum toroidal algebras as well as extended quantized GIM algebras of N-fold affinization. We show that the quantum N-toroidal algebras are quotients of the extended quantized GIM algebras of N-fold affinization, which generalizes a well-known result of Berman and Moody for Lie algebras.},
  archive      = {J_CMS},
  author       = {Gao, Yun and Jing, Naihuan and Xia, Limeng and Zhang, Honglian},
  doi          = {10.1007/s40304-022-00316-4},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {99-137},
  shortjournal = {Commun. Math. Stat.},
  title        = {Quantum N-toroidal algebras and extended quantized GIM algebras of N-fold affinization},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the lag window estimators of the spectrum and
memory for long-memory stationary gaussian processes. <em>CMS</em>,
<em>13</em>(1), 59–98. (<a
href="https://doi.org/10.1007/s40304-022-00304-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) is a stochastic process that has been successfully applied in finance, black-box modeling of biosystems, machine learning, geostatistics, multitask learning or robotics and reinforcement learning. Effectively estimating the spectral density function (SDF) and degree of memory (DOM) of a long-memory stationary GP (LMSGP) is needed to get accurate information about the process. The practice demonstrated that the periodogram estimator (PE) and lag window estimator (LWE) that are the extremely used estimators of the SDF and DOM have some drawbacks, especially for LMSGPs. The behaviors of the PEs and LWEs are soundly investigated numerically; however, the theoretical justifications are limited and thus the challenge to improve them is still daunting. This paper gives a closer look at the theoretical justifications of the efficiency of the LWEs that provides new sufficient conditions (NSCs) for improving the LWEs of the SDF and DOM for LMSGPs. The precision, the convergence rate of the bias and variance, and the asymptotic distributions of the LWEs under the NSCs are studied. A comparison study among the LWEs under the NSCs, the LWEs without the NSCs and the PEs is given to investigate the significance of the NSCs. The main theoretical and simulation results show that: the LWEs under the NSCs are asymptotically unbiased and consistent and better than the LWEs without the NSCs and the PEs, and the asymptotic distributions of the LWEs under the NSCs are chi-square for SDF and normal for DOM.},
  archive      = {J_CMS},
  author       = {Laala, Barkahoum and Belaloui, Soheir and Fang, Kai-Tai and Elsawah, A. M.},
  doi          = {10.1007/s40304-022-00304-8},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {59-98},
  shortjournal = {Commun. Math. Stat.},
  title        = {Improving the lag window estimators of the spectrum and memory for long-memory stationary gaussian processes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensions of tri-quadratic <span
class="math display"><em>c</em><sup>1</sup></span> spline spaces over
hierarchical 3D t-meshes. <em>CMS</em>, <em>13</em>(1), 1–57. (<a
href="https://doi.org/10.1007/s40304-022-00296-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3D T-mesh is generally a cuboid grid which allows hanging vertices. Here, a hanging vertex is an interior vertex, but it is not a corner point of eight cells. Spline function spaces with high order smoothness over 3D T-meshes have great application prospect due to their local refinement and relatively low degrees of freedom, for example, 3D isogeometric analysis and implicit representation of surfaces. However, there are still no available dimension formulae of those kinds of spline spaces for application. In this paper, we explore the dimensions of trivariate quadratic spline spaces with $$ C^1 $$ continuity over hierarchical 3D T-meshes. By using space embedding method, the problem is converted into a system of linear constraints, and then a lower bound on the dimension of the spline space over a hierarchical 3D T-mesh is provided. For a special type of hierarchical 3D T-meshes, the explicit dimension formula is obtained. In addition, a topological explanation of the dimension is given, which presents a way to construct basis functions.},
  archive      = {J_CMS},
  author       = {Liu, Min and Deng, Fang and Deng, Jiansong},
  doi          = {10.1007/s40304-022-00296-5},
  journal      = {Communications in Mathematics and Statistics},
  month        = {2},
  number       = {1},
  pages        = {1-57},
  shortjournal = {Commun. Math. Stat.},
  title        = {Dimensions of tri-quadratic $$ c^1 $$ spline spaces over hierarchical 3D T-meshes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="coap---12">COAP - 12</h2>
<ul>
<li><details>
<summary>
(2025). An arc-search interior-point algorithm for nonlinear
constrained optimization. <em>COAP</em>, <em>90</em>(3), 969–995. (<a
href="https://doi.org/10.1007/s10589-025-00648-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new arc-search interior-point algorithm for the nonlinear constrained optimization problem. The proposed algorithm uses the second-order derivatives to construct a search arc that approaches the optimizer. Because the arc stays in the interior set longer than any straight line, it is expected that the scheme will generate a better new iterate than a line search method. The computation of the second-order derivatives requires to solve the second linear system of equations, but the coefficient matrix of the second linear system of equations is the same as the first linear system of equations. Therefore, the matrix decomposition obtained while solving the first linear system of equations can be reused. In addition, most elements of the right-hand side vector of the second linear system of equations are already computed when the coefficient matrix is assembled. Therefore, the computation cost for solving the second linear system of equations is insignificant and the benefit of having a better search scheme is well justified. The convergence of the proposed algorithm is established. Some preliminary test results are reported to demonstrate the merit of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Yang, Yaguang},
  doi          = {10.1007/s10589-025-00648-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {969-995},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An arc-search interior-point algorithm for nonlinear constrained optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the stochastically controlled stochastic gradient
method by the bandwidth-based stepsize. <em>COAP</em>, <em>90</em>(3),
941–968. (<a href="https://doi.org/10.1007/s10589-025-00651-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepsize plays an important role in the stochastic gradient method. The bandwidth-based stepsize allows us to adjust the stepsize within a banded region determined by some boundary functions. Based on the bandwidth-based stepsize, we propose a new method, namely SCSG-BD, for smooth non-convex finite-sum optimization problems. For the boundary functions 1/t, $$1/(t\log (t + 1))$$ and $$1/t^p$$ ( $$p\in (0,1)$$ ), SCSG-BD converges sublinearly to a stationary point at a faster rate than the stochastically controlled stochastic gradient (SCSG) method under certain conditions. Moreover, SCSG-BD is able to converge linearly to the solution if the objective function satisfies the Polyak–Łojasiewicz condition. We also introduce the 1/t-Barzilai–Borwein stepsize for practical computation. Numerical experiments demonstrate that SCSG-BD performs better than SCSG and its variants.},
  archive      = {J_COAP},
  author       = {Liu, Chenchen and Huang, Yakui and Wang, Dan},
  doi          = {10.1007/s10589-025-00651-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {941-968},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Improving the stochastically controlled stochastic gradient method by the bandwidth-based stepsize},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A second-order sequential optimality condition for nonlinear
second-order cone programming problems. <em>COAP</em>, <em>90</em>(3),
911–939. (<a href="https://doi.org/10.1007/s10589-025-00649-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, the sequential optimality conditions, which do not require constraint qualifications and allow improvement on the convergence assumptions of algorithms, had been considered in the literature. It includes the work by Andreani et al. (IMA J Numer Anal 37:1902–1929, 2017), with a sequential optimality condition for nonlinear programming, that uses the second-order information of the problem. More recently, Fukuda et al. (Set-Valued Var Anal 31:15, 2023) analyzed the conditions that use second-order information, in particular for nonlinear second-order cone programming problems (SOCP). However, such optimality conditions were not defined explicitly. In this paper, we propose an explicit definition of approximate-Karush-Kuhn-Tucker 2 (AKKT2) and complementary-AKKT2 (CAKKT2) conditions for SOCPs. We prove that the proposed AKKT2/CAKKT2 conditions are satisfied at local optimal points of the SOCP without any constraint qualification. We also present two algorithms that are based on augmented Lagrangian and sequential quadratic programming methods and show their global convergence to points satisfying the proposed conditions.},
  archive      = {J_COAP},
  author       = {Fukuda, Ellen H. and Okabe, Kosuke},
  doi          = {10.1007/s10589-025-00649-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {911-939},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A second-order sequential optimality condition for nonlinear second-order cone programming problems},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the douglas–rachford splitting method through
the lenses of moreau-type envelopes. <em>COAP</em>, <em>90</em>(3),
881–910. (<a href="https://doi.org/10.1007/s10589-024-00646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the Douglas–Rachford splitting method for weakly convex optimization problems, by the token of the Douglas–Rachford envelope, a merit function akin to the Moreau envelope. First, we use epi-convergence techniques to show that this artifact approximates the original objective function via epigraphs. Secondly, we present how global convergence and local linear convergence rates for Douglas–Rachford splitting can be obtained using such envelope, under mild regularity assumptions. The keystone of the convergence analysis is the fact that the Douglas–Rachford envelope satisfies a sufficient descent inequality alongside the generated sequence, a feature that allows us to use arguments usually employed for descent methods. We report numerical experiments that use weakly convex penalty functions, which are comparable with the known behavior of the method in the convex case.},
  archive      = {J_COAP},
  author       = {Atenas, Felipe},
  doi          = {10.1007/s10589-024-00646-9},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {881-910},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Understanding the Douglas–Rachford splitting method through the lenses of moreau-type envelopes},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convex combined symmetric alternating direction method of
multipliers for separable optimization. <em>COAP</em>, <em>90</em>(3),
839–880. (<a href="https://doi.org/10.1007/s10589-025-00647-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Alternating Direction Method of Multipliers (ADMM) is a powerful first-order method used in many practical separable optimization problems. In this paper, we propose a new variant of the symmetric ADMM, called the Convex Combined Symmetric ADMM (CcS-ADMM), by integrating a convex combination technique. CcS-ADMM retains all the favorable features of ADMM, including the ability to take full advantage of problem structures and global convergence under relaxed parameter conditions. Furthermore, using the moderate assumptions and primal-dual gap, we analyze the convergence and the O(1/N) ergodic convergence rate of the algorithm with convex setting. Additionally, we propose the convergence of the CcS-ADMM with nonconvex setting in Euclidean space under so called Kurdyka–Lojasiewicz property and some widely used assumptions, and we establish the pointwise iteration-complexity of CcS-ADMM with respect to the augmented Lagrangian function and the primal-dual residuals. Finally, we present the results from preliminary numerical experiments to demonstrate the performance of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Wang, Xiaoquan and Shao, Hu and Wu, Ting},
  doi          = {10.1007/s10589-025-00647-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {839-880},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A convex combined symmetric alternating direction method of multipliers for separable optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized methods via cubic model subspace minimization
for nonconvex optimization. <em>COAP</em>, <em>90</em>(3), 801–837. (<a
href="https://doi.org/10.1007/s10589-025-00655-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive cubic regularization methods for solving nonconvex problems need the efficient computation of the trial step, involving the minimization of a cubic model. We propose a new approach in which this model is minimized in a low dimensional subspace that, in contrast to classic approaches, is reused for a number of iterations. Whenever the trial step produced by the low-dimensional minimization process is unsatisfactory, we employ a regularized Newton step whose regularization parameter is a by-product of the model minimization over the low-dimensional subspace. We show that the worst-case complexity of classic cubic regularized methods is preserved, despite the possible regularized Newton steps. We focus on the large class of problems for which (sparse) direct linear system solvers are available and provide several experimental results showing the very large gains of our new approach when compared to standard implementations of adaptive cubic regularization methods based on direct linear solvers. Our first choice as projection space for the low-dimensional model minimization is the polynomial Krylov subspace; nonetheless, we also explore the use of rational Krylov subspaces in case where the polynomial ones lead to less competitive numerical results.},
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Palitta, Davide and Porcelli, Margherita and Simoncini, Valeria},
  doi          = {10.1007/s10589-025-00655-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {801-837},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Regularized methods via cubic model subspace minimization for nonconvex optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of the gradient descent method with
stochastic fixed-point rounding errors under the polyak–łojasiewicz
inequality. <em>COAP</em>, <em>90</em>(3), 753–799. (<a
href="https://doi.org/10.1007/s10589-025-00656-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the training of neural networks with low-precision computation and fixed-point arithmetic, rounding errors often cause stagnation or are detrimental to the convergence of the optimizers. This study provides insights into the choice of appropriate stochastic rounding strategies to mitigate the adverse impact of roundoff errors on the convergence of the gradient descent method, for problems satisfying the Polyak–Łojasiewicz inequality. Within this context, we show that a biased stochastic rounding strategy may be even beneficial in so far as it eliminates the vanishing gradient problem and forces the expected roundoff error in a descent direction. Furthermore, we obtain a bound on the convergence rate that is stricter than the one achieved by unbiased stochastic rounding. The theoretical analysis is validated by comparing the performances of various rounding strategies when optimizing several examples using low-precision fixed-point arithmetic.},
  archive      = {J_COAP},
  author       = {Xia, Lu and Massei, Stefano and Hochstenbach, Michiel E.},
  doi          = {10.1007/s10589-025-00656-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {753-799},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the convergence of the gradient descent method with stochastic fixed-point rounding errors under the polyak–Łojasiewicz inequality},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All saddle points for polynomial optimization.
<em>COAP</em>, <em>90</em>(3), 721–752. (<a
href="https://doi.org/10.1007/s10589-025-00657-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study how to compute all saddle points for the constrained and unconstrained polynomial optimization, respectively. For the constrained polynomial optimization, a scalar-type semidefinite relaxation algorithm is proposed based on the Karush-Kuhn-Tucker conditions. While for the unconstrained polynomial optimization, a matrix-type semidefinite relaxation algorithm is proposed based on the second-order optimality conditions. Both algorithms can detect the nonexistence of saddle points or find all of them if there are finitely many ones. The finite convergence of the algorithms can also be obtained under some genericity conditions.},
  archive      = {J_COAP},
  author       = {Zhou, Anwa and Yin, Shiqian and Fan, Jinyan},
  doi          = {10.1007/s10589-025-00657-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {721-752},
  shortjournal = {Comput. Optim. Appl.},
  title        = {All saddle points for polynomial optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the use of restriction of the right-hand side in spatial
branch-and-bound algorithms to ensure termination. <em>COAP</em>,
<em>90</em>(3), 691–720. (<a
href="https://doi.org/10.1007/s10589-025-00652-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial branch-and-bound algorithms for global minimization of non-convex problems require both lower and upper bounding procedures that finally converge to a globally optimal value in order to ensure termination of these methods. Whereas convergence of lower bounds is commonly guaranteed for standard approaches in the literature, this does not always hold for upper bounds. For this reason, different so-called convergent upper bounding procedures are proposed. These methods are not always used in practice, possibly due to their additional complexity or possibly due to increasing runtimes on average problems. For that reason, in this article we propose a refinement of classical branch-and-bound methods that is simple to implement and comes with marginal overhead. We prove that this small improvement already leads to convergent upper bounds, and thus show that termination of spatial branch-and-bound methods is ensured under mild assumptions.},
  archive      = {J_COAP},
  author       = {Kirst, Peter and Füllner, Christian},
  doi          = {10.1007/s10589-025-00652-5},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {691-720},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the use of restriction of the right-hand side in spatial branch-and-bound algorithms to ensure termination},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic optimal control problems with combinatorial
switching constraints, part III: Branch-and-bound algorithm.
<em>COAP</em>, <em>90</em>(3), 649–689. (<a
href="https://doi.org/10.1007/s10589-025-00654-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a branch-and-bound algorithm for globally solving parabolic optimal control problems with binary switches that have bounded variation and possibly need to satisfy further combinatorial constraints. More precisely, for a given tolerance $$\varepsilon &gt;0$$ , we show how to compute in finite time an $$\varepsilon $$ -optimal solution in function space, independently of any prior discretization. The main ingredients in our approach are an appropriate branching strategy in infinite dimension, an a posteriori error estimation in order to obtain safe dual bounds, and an adaptive refinement strategy in order to allow arbitrary switching points in the limit. The performance of our approach is demonstrated by extensive experimental results.},
  archive      = {J_COAP},
  author       = {Buchheim, Christoph and Grütering, Alexandra and Meyer, Christian},
  doi          = {10.1007/s10589-025-00654-3},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {649-689},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Parabolic optimal control problems with combinatorial switching constraints, part III: Branch-and-bound algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardinality constrained mean-variance portfolios: A penalty
decomposition algorithm. <em>COAP</em>, <em>90</em>(3), 631–648. (<a
href="https://doi.org/10.1007/s10589-025-00653-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cardinality-constrained mean-variance portfolio problem has garnered significant attention within contemporary finance due to its potential for achieving low risk while effectively managing transaction costs. Instead of solving this problem directly, many existing methods rely on regularization and approximation techniques, which hinder investors’ ability to precisely specify a portfolio’s desired cardinality level. Moreover, these approaches typically include more hyper-parameters and increase the problem’s dimensionality. To address these challenges, we propose a customized penalty decomposition algorithm. We demonstrate that this algorithm not only does it converge to a local minimizer of the cardinality-constrained mean-variance portfolio problem, but is also computationally efficient. Our approach leverages a sequence of penalty subproblems, each tackled using Block Coordinate Descent (BCD). We show that the steps within BCD yield closed-form solutions, allowing us to identify a saddle point of the penalty subproblems. Finally, by applying our penalty decomposition algorithm to real-world datasets, we highlight its efficiency and its superiority over state-of-the-art methods across several performance metrics.},
  archive      = {J_COAP},
  author       = {Mousavi, Ahmad and Michailidis, George},
  doi          = {10.1007/s10589-025-00653-4},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {631-648},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cardinality constrained mean-variance portfolios: A penalty decomposition algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-DS: A cost saving algorithm for expensive constrained
multi-fidelity blackbox optimization. <em>COAP</em>, <em>90</em>(3),
607–629. (<a href="https://doi.org/10.1007/s10589-024-00645-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces Inter-DS, a blackbox optimization algorithmic framework for computationally expensive constrained multi-fidelity problems. When applying a direct search method to such problems, the scarcity of feasible points may lead to numerous costly evaluations spent on infeasible points. Our proposed algorithm addresses this issue by leveraging multi-fidelity information, allowing for premature interruption of an evaluation when a point is estimated to be infeasible. These estimations are controlled by a biadjacency matrix, for which we propose a construction. The proposed method acts as an intermediary component bridging any non multi-fidelity direct search solver and a multi-fidelity blackbox problem, giving the user freedom of choice for the solver. A series of computational tests are conducted to validate the approach. The results show a significant improvement in solution quality when an initial feasible starting point is provided. When this condition is not met, the outcomes are contingent upon specific properties of the blackbox.},
  archive      = {J_COAP},
  author       = {Alarie, Stéphane and Audet, Charles and Diago, Miguel and Digabel, Sébastien Le and Lebeuf, Xavier},
  doi          = {10.1007/s10589-024-00645-w},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {607-629},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inter-DS: A cost saving algorithm for expensive constrained multi-fidelity blackbox optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="dmkd---6">DMKD - 6</h2>
<ul>
<li><details>
<summary>
(2025). Missing value replacement in strings and applications.
<em>DMKD</em>, <em>39</em>(2), 1–50. (<a
href="https://doi.org/10.1007/s10618-024-01074-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values arise routinely in real-world sequential (string) datasets due to: (1) imprecise data measurements; (2) flexible sequence modeling, such as binding profiles of molecular sequences; or (3) the existence of confidential information in a dataset which has been deleted deliberately for privacy protection. In order to analyze such datasets, it is often important to replace each missing value, with one or more valid letters, in an efficient and effective way. Here we formalize this task as a combinatorial optimization problem: the set of constraints includes the context of the missing value (i.e., its vicinity) as well as a finite set of user-defined forbidden patterns, modeling, for instance, implausible or confidential patterns; and the objective function seeks to minimize the number of new letters we introduce. Algorithmically, our problem translates to finding shortest paths in special graphs that contain forbidden edges representing the forbidden patterns. Our work makes the following contributions: (1) we design a linear-time algorithm to solve this problem for strings over constant-sized alphabets; (2) we show how our algorithm can be effortlessly applied to fully sanitize a private string in the presence of a set of fixed-length forbidden patterns [Bernardini et al. 2021a]; (3) we propose a methodology for sanitizing and clustering a collection of private strings that utilizes our algorithm and an effective and efficiently computable distance measure; and (4) we present extensive experimental results showing that our methodology can efficiently sanitize a collection of private strings while preserving clustering quality, outperforming the state of the art and baselines. To arrive at our theoretical results, we employ techniques from formal languages and combinatorial pattern matching.},
  archive      = {J_DMKD},
  author       = {Bernardini, Giulia and Liu, Chang and Loukides, Grigorios and Marchetti-Spaccamela, Alberto and Pissis, Solon P. and Stougie, Leen and Sweering, Michelle},
  doi          = {10.1007/s10618-024-01074-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-50},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Missing value replacement in strings and applications},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative evaluation of clustering-based outlier
detection. <em>DMKD</em>, <em>39</em>(2), 1–55. (<a
href="https://doi.org/10.1007/s10618-024-01086-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform an extensive experimental evaluation of clustering-based outlier detection methods. These methods offer benefits such as efficiency, the possibility to capitalize on more mature evaluation measures, more developed subspace analysis for high-dimensional data and better explainability, and yet they have so-far been neglected in literature. To our knowledge, our work is the first effort to analytically and empirically study their advantages and disadvantages. Our main goal is to evaluate whether or not clustering-based techniques can compete in efficiency and effectiveness against the most studied state-of-the-art algorithms in the literature. We consider the quality of the results, the resilience against different types of data and variations in parameter configuration, the scalability, and the ability to filter out inappropriate parameter values automatically based on internal measures of clustering quality. It has been recently shown that several classic, simple, unsupervised methods surpass many deep learning approaches and, hence, remain at the state-of-the-art of outlier detection. We therefore study 14 of the best classic unsupervised methods, in particular 11 clustering-based methods and 3 non-clustering-based ones, using a consistent parameterization heuristic to identify the pros and cons of each approach. We consider 46 real and synthetic datasets with up to 125k points and 1.5k dimensions aiming to achieve plausibility with the broadest possible diversity of real-world use cases. Our results indicate that the clustering-based methods are on par with (if not surpass) the non-clustering-based ones, and we argue that clustering-based methods like KMeans−− should be included as baselines in future benchmarking studies, as they often offer a competitive quality at a relatively low run time, besides several other benefits.},
  archive      = {J_DMKD},
  author       = {Sánchez Vinces, Braulio V. and Schubert, Erich and Zimek, Arthur and Cordeiro, Robson L. F.},
  doi          = {10.1007/s10618-024-01086-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-55},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comparative evaluation of clustering-based outlier detection},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-level relation extraction for informative taxonomy
learning. <em>DMKD</em>, <em>39</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10618-024-01088-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the ever-shifting domain of technological advancement, the quest for automatic taxonomy construction is intensifying. This paper confronts the nuanced challenges of distilling synonym and hyponym relationships from diverse, domain-specific scientific literature, treating it as a domain-level relation extraction problem and resulting in the creation of a hierarchical taxonomy through arborescence generation. The proposed Multi-Scale Identity Connection (MSIC) model excels in capturing inter-entity relationships across various scales, demonstrating superior empirical performance compared to existing relation extraction models. To enhance practicality, a two-stage optimization is introduced to improve efficiency without compromising performance. Additionally, the Depth-prioritized Maximum Spanning Arborescence (DMSA) algorithm has been proposed as a highly efficient strategy for generating an informative and well-structured taxonomy tree. We annotated a concise dataset to train and validate the MSIC model, subsequently applying it to a substantial domain-specific dataset for taxonomy induction. The experimental findings indicate that the DMSA efficiently constructs an information-rich taxonomy tree structure by leveraging extensive domain-specific scientific literature. These results not only affirm the efficacy of the approach but also highlight its effectiveness in supporting industrial-grade applications.},
  archive      = {J_DMKD},
  author       = {Hu, Maodi and Song, Donghuan and Qian, Li and Zhang, Zhixiong},
  doi          = {10.1007/s10618-024-01088-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Domain-level relation extraction for informative taxonomy learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximity forest 2.0: A new effective and scalable
similarity-based classifier for time series. <em>DMKD</em>,
<em>39</em>(2), 1–29. (<a
href="https://doi.org/10.1007/s10618-024-01085-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a challenging task due to the diversity of types of features that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms other state-of-the-art methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series similarity measures — (1) computationally efficient early abandoning and pruning to speedup elastic similarity computations; (2) a new elastic similarity measure, Amerced Dynamic Time Warping ( $${{\,\textrm{ADTW}\,}}$$ ); and (3) cost function tuning. It rationalizes the set of similarity measures employed, reducing the eight base measures of the original PF to four and using the first derivative transform with all similarity measures, rather than a limited subset. It also incorporates HYDRA, a dictionary-based transform. We have re-implemented PF 1.0 and implemented PF 2.0 framework in Java, making the PF framework more efficient.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Herrmann, Matthieu and Salehi, Mahsa and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-024-01085-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Proximity forest 2.0: A new effective and scalable similarity-based classifier for time series},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TED: Related party transaction guided tax evasion detection
on heterogeneous graph. <em>DMKD</em>, <em>39</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s10618-025-01091-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tax evasion causes severe losses of government revenues and disturbs the economic order of fair competition. To help alleviate this problem, the latest tax evasion detection solutions utilize expert knowledge to extract features and then train classifiers to determine whether a company is suspected of tax evasion. However, existing solutions mainly focus on the statistical features of the company, but fail to exploit the rich interactive information in tax scenarios, which affect the detection performance. In this paper, we first model the tax scenario as a heterogeneous graph and study the tax evasion detection problem under the heterogeneous graph model. To improve the performance of tax evasion detection, a novel graph neural network model is proposed to extract the comprehensive information of heterogeneous graphs. Specifically, we use heterogeneous and complex related party transaction groups to filter low-level noise information. Moreover, a hierarchical attention mechanism is designed to capture the deeper structure and semantic information hidden in the related party transaction group. We apply our method to the real risk management system of the tax bureau, and evaluate it on two human-labeled real-world tax datasets. The results demonstrate that our method significantly outperforms the state-of-the-art in the tax evasion detection task. The code and data are available at: https://github.com/yimingxu24/TED .},
  archive      = {J_DMKD},
  author       = {Xu, Yiming and Shi, Bin and Dong, Bo and Wang, Jiaxiang and Wei, Hua and Zheng, Qinghua},
  doi          = {10.1007/s10618-025-01091-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TED: Related party transaction guided tax evasion detection on heterogeneous graph},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring tie strength in temporal networks. <em>DMKD</em>,
<em>39</em>(2), 1–31. (<a
href="https://doi.org/10.1007/s10618-025-01093-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring tie strengths in social networks is an essential task in social network analysis. Common approaches classify the ties as weak and strong ties based on the strong triadic closure (STC). The STC states that if for three nodes, A, B, and C, there are strong ties between A and B, as well as A and C, there has to be a (weak or strong) tie between B and C. A variant of the STC called STC+ allows adding a few new weak edges to obtain improved solutions. So far, most works discuss the STC or STC+ in static networks. However, modern large-scale social networks are usually highly dynamic, providing user contacts and communications as streams of edge updates. Temporal networks capture these dynamics. To apply the STC to temporal networks, we first generalize the STC and introduce a weighted version such that empirical a priori knowledge given in the form of edge weights is respected by the STC. Similarly, we introduce a generalized weighted version of the STC+. The weighted STC is hard to compute, and our main contribution is an efficient 2-approximation (resp. 3-approximation) streaming algorithm for the weighted STC (resp. STC+) in temporal networks. As a technical contribution, we introduce a fully dynamic k-approximation for the minimum weighted vertex cover problem in hypergraphs with edges of size k, which is a crucial component of our streaming algorithms. An empirical evaluation shows that the weighted STC leads to solutions that better capture the a priori knowledge given by the edge weights than the non-weighted STC. Moreover, we show that our streaming algorithm efficiently approximates the weighted STC in real-world large-scale social networks.},
  archive      = {J_DMKD},
  author       = {Oettershagen, Lutz and Konstantinidis, Athanasios L. and Italiano, Giuseppe F.},
  doi          = {10.1007/s10618-025-01093-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Inferring tie strength in temporal networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ei---31">EI - 31</h2>
<ul>
<li><details>
<summary>
(2025). Data-driven weight initialization strategy for convolutional
neural networks. <em>EI</em>, <em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s12065-024-00985-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep Convolutional Neural Networks consists of multiple forward and backward passes over several epochs until the loss converges, making it time-consuming. Various factors affect the training times of networks, weight initialization being one of them. The idea of a proper weight initialization technique is to set the initial weights such that the network converges faster by extracting meaningful features from the data. This paper proposes a data-driven weight initialization to accelerate the training process. This technique is based on obtaining initial weights by appropriately analysing the training data. The proposed weight initialization strategy initializes filters from a pre-defined filter bank that is created before training, and it contains standard edge and texture feature extracting filters and data-driven filters obtained using principal component analysis, linear discriminant analysis and partial least squares. The proposed technique has been validated on AlexNet, VGG-16 and ResNet-50 for Intel Image Classification, CIFAR10 and CIFAR100 datasets. The results show that the proposed technique gives better validation results than other state-of-the-art techniques in fewer epochs and works well with state-of-the-art activation functions.},
  archive      = {J_EI},
  author       = {Narkhede, Meenal and Mahajan, Shrinivas and Bartakke, Prashant},
  doi          = {10.1007/s12065-024-00985-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Evol. Intell.},
  title        = {Data-driven weight initialization strategy for convolutional neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards load frequency management in thermal power systems
using an improved open-source development model algorithm. <em>EI</em>,
<em>18</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s12065-024-00986-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining a balance between generated and consumed power is crucial for the efficient operation of interconnected power grids. This paper focuses on the issue of load frequency management in thermal power systems. We present a metaheuristic approach to address this problem, leveraging the advantages of metaheuristic algorithms in solving complex optimization problems. Our approach is based on an improved Open-source Development model algorithm (ODMA). The proposed method, referred to as ODMA-GA, integrates ODMA with a Genetic algorithm (GA) to configure a hybrid optimization technique. This combination allows for the effective exploitation and exploration of the search space, leading to high-quality solutions. We evaluate the performance of ODMA-GA using several benchmark functions with varying scenarios. The results demonstrate that ODMA-GA outperforms its peer metaheuristic algorithms, showcasing superior optimization capabilities. This promising performance suggests that ODMA-GA offers an effective solution for the challenging task of load frequency management in thermal power systems, ensuring stable and efficient operation. Specifically, the finding shows that our method improves the frequency change area and tie-line power change by 2.3% compared to the best existing method.},
  archive      = {J_EI},
  author       = {Khezri, Edris and Rezaeipanah, Amin and Hassanzadeh, Hiwa and Majidpour, Jafar},
  doi          = {10.1007/s12065-024-00986-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Evol. Intell.},
  title        = {Towards load frequency management in thermal power systems using an improved open-source development model algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integration of metaheuristic operators through unstructured
evolutive game theory approach: A novel hybrid methodology. <em>EI</em>,
<em>18</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s12065-024-00988-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach to addressing the complexity of current optimization challenges by developing hybrid metaheuristic algorithms. These algorithms combine the strengths of different strategies to enhance solution identification and refinement. In this work, the integration of metaheuristic operators using the Unstructured Evolutive Game Theory to regulate and merge the advantageous features of the Cheetah Optimizer and Particle Swarm Optimization to solve continuous optimization problems is proposed. The Cheetah Optimizer and Particle Swarm Optimization are chosen for their superior exploitation and exploration capabilities, respectively. Our methodology unfolds in two primary phases: combination and modulation. In the combination phase, Cheetah Optimizer and Particle Swarm Optimization search strategies are merged, creating a unified population that generates new candidate solution positions based on the best solution and a modulation factor. During the modulation phase, pairwise competitions based on Unstructured Evolutive Game Theory assess candidate solutions against the objective function, adjusting their modulation factor and position accordingly. We conducted various experiments to evaluate our approach against the original Cheetah Optimizer and Particle Swarm Optimization, as well as seven other well-known metaheuristic algorithms and hybrid schemes, across 30 benchmark functions in dimensions of 30, 50, and 100. The results reveal that our hybrid scheme outperforms the comparative algorithms, demonstrating enhanced performance, effectiveness, and robustness through a detailed convergence and significance analysis.},
  archive      = {J_EI},
  author       = {Escobar-Cuevas, Hector and Cuevas, Erik and Lopez, Jesus and Perez-Cisneros, Marco},
  doi          = {10.1007/s12065-024-00988-7},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Evol. Intell.},
  title        = {Integration of metaheuristic operators through unstructured evolutive game theory approach: A novel hybrid methodology},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based spatio-temporal graph
neural network for solving job shop scheduling problem. <em>EI</em>,
<em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12065-024-00989-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job shop scheduling problem (JSSP) is a well-known NP-hard combinatorial optimization problem that focuses on assigning tasks to limited resources while adhering to certain constraints. Currently, deep reinforcement learning (DRL)-based solutions are being widely used to solve the JSSP by defining the problem structure on disjunctive graphs. Some of the proposed approaches attempt to leverage the structural information of the JSSP to capture the dynamics of the environment without considering the time dependency within the JSSP. However, learning graph representations only from the structural relationship of nodes results in a weak and incomplete representation of these graphs which does not provide an expressive representation of the dynamics in the environment. In this study, unlike existing frameworks, we defined the JSSP as a dynamic graph to explicitly consider the time-varying aspect of the JSSP environment. To this end, we propose a novel DRL framework that captures both the spatial and temporal attributes of the JSSP to construct rich and complete graph representations. Our DRL framework introduces a novel attentive graph isomorphism network (Attentive-GIN)-based spatial block to learn the structural relationship and a temporal block to capture the time dependency. Additionally, we designed a gated fusion block that selectively combines the learned representations from the two blocks. We trained the model using the proximal policy optimization algorithm of reinforcement learning. Experimental results show that our trained model exhibits significant performance enhancement compared to heuristic dispatching rules and learning-based solutions for both randomly generated datasets and public benchmarks.},
  archive      = {J_EI},
  author       = {Gebreyesus, Goytom and Fellek, Getu and Farid, Ahmed and Hou, Sicheng and Fujimura, Shigeru and Yoshie, Osamu},
  doi          = {10.1007/s12065-024-00989-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Evol. Intell.},
  title        = {Deep reinforcement learning-based spatio-temporal graph neural network for solving job shop scheduling problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep metric learning with in-batch feature vector
constraints and unsupervised label integration. <em>EI</em>,
<em>18</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12065-024-00990-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning has increasingly captured the interest of the research community in recent years, mainly due to its effectiveness in integrating distance metric learning with deep neural networks. Despite the existence of various approaches, such as pair-based angular loss functions or spherical embedding constraints, these methods often necessitate extra trainable parameters and overlook class similarities. This paper presents two approaches: ‘In-batch feature vector constraint’ and ‘Unsupervised label integration.’ These methods are notable for considering the similarities between different classes. Because of their high compatibility, these techniques can be seamlessly integrated with various loss functions. Comprehensive experimental evaluations encompassing four image classification datasets and seven network architectures have demonstrated the effectiveness of these proposed methods in enhancing network performance.},
  archive      = {J_EI},
  author       = {Kim, Wonjik},
  doi          = {10.1007/s12065-024-00990-z},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {Deep metric learning with in-batch feature vector constraints and unsupervised label integration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary algorithm framework for optimizing truck
scheduling in multi-dock truck cross-docking centers. <em>EI</em>,
<em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s12065-024-00992-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-docking optimizes logistics by reducing storage and handling times in warehouses, where cargos are unloaded from inbound trucks and loaded directly onto outbound vehicles. This paper addresses the critical challenge of minimizing makespan in a multi-dock, parallel machine setting within cross-docking centers. We propose a novel framework that integrates an evolutionary algorithm (EA) with a machine learning (ML) based hyperparameter tuning mechanism to optimize truck sequencing. This study fills the research gap by offering a quantifiable improvement over traditional heuristic methods, delivering up to a 37% improvement in the GAP metric compared to state-of-the-art techniques. It also achieves a 30% enhancement over the PCH constructive heuristic used for generating initial solutions. Additionally, our ML-based tuning strategy provides up to a 21% performance increase over static tuning methods. Notably, these improvements are attained while maintaining a competitive computational time as reported in the literature.},
  archive      = {J_EI},
  author       = {Nogueira, Thiago Henrique and Coutinho, Felipe Provezano and Peixoto, Maria Gabriela Mendonça and Carrano, Eduardo Gontijo and Ravetti, Martín Gómez},
  doi          = {10.1007/s12065-024-00992-x},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Evol. Intell.},
  title        = {Evolutionary algorithm framework for optimizing truck scheduling in multi-dock truck cross-docking centers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian mixture model and bayesian convolutional neural
network for abnormal musculoskeletal radiographs classification.
<em>EI</em>, <em>18</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12065-024-00993-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Musculoskeletal radiograph classification has been gaining great attention due to its important application in automated diagnosis of musculoskeletal disorders. Generally, musculoskeletal radiographs capture images of various bones in the whole body, which can be considered as a combination of multiple single datasets. Existing works have tried to build either a traditional neural network (NN), convolution neural network (CNN), or Bayesian convolutional neural network (BCNN) to achieve efficient classification. However, none of them explore the mixed-dataset property. In this context, we propose a novel Gaussian mixture model-based BCNN (MixBCNN) for the abnormal musculoskeletal radiographs classification problem. In the proposed method, the distribution of NN weights is represented as a Gaussian mixture distribution, expecting a new neural network model with better representation. In addition, we propose an ensemble learning approach to efficiently combine potential NN backbones. Performance evaluation is examined for the popular MURA dataset and shows the superiority of the proposed model with respect to state-of-the-art BCNN methods, notably by 0.86 in F1 score and 0.72 in Cohen’s kappa metrics while asking for negligible complexity overhead.},
  archive      = {J_EI},
  author       = {HoangVan, Xiem and Tran Van, Khoa},
  doi          = {10.1007/s12065-024-00993-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {Gaussian mixture model and bayesian convolutional neural network for abnormal musculoskeletal radiographs classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brhamo: Metaheuristic optimization algorithm for speech
emotion recognition using spectral and hybrid features. <em>EI</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-00994-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient speech emotion recognition (SER) is a crucial component in the development of natural and intuitive human-computer interaction systems. The problem addressed in this research is the challenge of achieving high accuracy in SER through optimized feature extraction, selection, and model tuning. The objective is to achieve more accurate speech emotion recognition through optimized feature extraction and selection, and the generation of an optimized random forest model. This paper presents a machine learning framework for speech emotion recognition based on Metaheuristic principles. The framework comprises two primary components: feature extraction and selection. Additionally, it generates an optimised random forest model by utilising the hybrid bat algorithm (HBA) to fine-tune the hyperparameters of the RF model, known as the Bat Random Forest Hybrid Meta Heuristic Algorithm (BRHAMO). Three speech corpora, including RAVDESS, SAVEE, and novel ANAKE Hindi speech corpus, have been utilized. The speech emotion recognition was subjected to a series of experiments and tests utilizing two unique features of speech characteristics, namely spectral features and the amalgamation of hybrid features. The experimental findings demonstrated BRHAMO based model achieved an accuracy of 81%, 79.6%, and 77.6% for the RAVDESS, SAVEE, and ANAKE datasets, respectively, in the spectral feature category. Furthermore, for the hybrid feature category, the RAVDESS, SAVEE, and ANAKE datasets achieved accuracy rates of 93.8%, 85.4%, and 89.8%, respectively. The performance of the BRHAMO, has been compared to several benchmark machine learning models, namely vanilla Random Forest, gradient boost, adaptive boost, and support vector machines. It is observed that the Meta Heuristic Algorithm (MHA) based approach can deliver better performance in terms of accuracy, precision, F1 score, and recall compared to all the individual classifiers in both categories.},
  archive      = {J_EI},
  author       = {Agrawal, Akshat and Jain, Anurag},
  doi          = {10.1007/s12065-024-00994-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Brhamo: Metaheuristic optimization algorithm for speech emotion recognition using spectral and hybrid features},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intellectual model of pest detection and classification
using enhanced optimization-assisted single shot detector and graph
attention network. <em>EI</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12065-024-00995-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying pests in the plants is considered to be an efficient, accurate, and sustainable way to improve the food quality and economic values in the agricultural industry. It equips farmers with cutting-edge instruments to safeguard their harvests, lessen their effect on the environment, and satisfy the growing need for sustenance on a worldwide scale. Deep learning can completely transform the identification of pests as it develops, protecting agriculture&#39;s production and resistance against ever-changing insect threats. It can be difficult to combine multiple methods for pest identification, including artificial intelligence, satellite imaging, and detectors. It is difficult to make sure that such innovations are available to farms and function well together. It&#39;s common for farmers as well as other agricultural personnel to require certification to properly utilize pest detection equipment and understand the findings. For these innovations to be widely adopted, the gap in understanding must be closed. Addressing these challenges requires an effective novel deep learning-based pest detection method. In this paper, a deep learning-based pest detection and classification model is developed to identify diseases caused by pests at an early stage. Initially, the images are collected from online datasets. Collected images underwent detection using the Adaptive Single Shot Detector (ASSD). To enhance the accuracy and efficiency of ASSD, the model parameters are fine-tuned by utilizing the Enhanced Running City Game Optimizer. This optimization process aims to improve the precision of pest detection. The classification of pests is done using the proposed Multi-scale and Dilated Graph Attention Network. This innovative network architecture is designed to categorize the pests effectively. The outcome of the proposed model is justified by comparing it with other classifiers and algorithms.},
  archive      = {J_EI},
  author       = {Vhatkar, Kapil Netaji},
  doi          = {10.1007/s12065-024-00995-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Evol. Intell.},
  title        = {An intellectual model of pest detection and classification using enhanced optimization-assisted single shot detector and graph attention network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing sand cat swarm optimization based on
multi-strategy mixing for solving engineering optimization problems.
<em>EI</em>, <em>18</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s12065-024-00996-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sand Cat Swarm Optimization (SCSO) algorithm is characterized by slow convergence speed and a tendency to become trapped in local optima. To address these issues, a hybrid multi-strategy variant known as HSCSO is proposed. The algorithm first employs an Elite Opposition-Based Learning strategy, which generates complementary candidate solutions, providing a more diverse set of initial solutions and improving the distribution of individual positions. Subsequently, a greedy strategy is utilized to combine the position update strategy of the golden sine algorithm, which has a fixed search pattern, with the position update strategy of the SCSO, which features random search characteristics. This combination retains the advantage of fast convergence while also providing a diverse set of random candidate solutions. Finally, a Random Inertia Weight is introduced to assist the SCSO algorithm in adaptively adjusting the search step size and optimization direction based on the different phases of the search process, thereby helping the algorithm effectively avoid local optima. The improved algorithm was compared with two classic original algorithms and six different algorithm variants on 25 benchmark functions. The experimental results indicate that HSCSO exhibits faster convergence and higher solution accuracy on most benchmark functions. Furthermore, the performance of HSCSO was evaluated on the CEC2019 and CEC2021 test suites, as well as several engineering optimization problems. The experimental results validate the competitiveness and superiority of the proposed HSCSO algorithm, demonstrating its potential for application in the field of global optimization.},
  archive      = {J_EI},
  author       = {Wang, Wen-chuan and Han, Zi-jun and Zhang, Zhao and Wang, Jun},
  doi          = {10.1007/s12065-024-00996-7},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Evol. Intell.},
  title        = {Enhancing sand cat swarm optimization based on multi-strategy mixing for solving engineering optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Draco lizard optimizer: A novel metaheuristic algorithm for
global optimization problems. <em>EI</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-00998-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces a novel meta-heuristic optimization technique, termed the Draco lizard optimizer (DLO), grounded in the distinctive behaviors exhibited by the Draco lizard, particularly its proficient gliding mechanisms and adaptive ecological strategies. The DLO algorithm ingeniously translates these survival techniques into an efficient search paradigm, aimed at resolving intricate global optimization problems. To assess the optimization efficacy of the DLO algorithm, we embarked on a comparative experimentation framework, incorporating 29 benchmark functions sourced from the prestigious CEC2017 test suite, alongside a selection of five cutting-edge metaheuristic algorithms. The analysis of the experimental outcomes highlights the advantages of the DLO algorithm, which achieved an average ranking of 1.62 on the CEC2017 benchmark set, placing it in the top position. Additionally, the average computational time of the DLO algorithm is one-third that of the second-ranked CPO algorithm, confirming that the DLO algorithm not only possesses exceptional global search performance but also demonstrates higher search efficiency compared to contemporary optimization algorithms. Source code for DLO algorithm: https://github.com/XiaoweiWang-ai/DLO},
  archive      = {J_EI},
  author       = {Wang, Xiaowei},
  doi          = {10.1007/s12065-024-00998-5},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Draco lizard optimizer: A novel metaheuristic algorithm for global optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced CEEMD hybrid model for VIX forecasting: Optimized
decision trees and ARIMA integration. <em>EI</em>, <em>18</em>(1), 1–12.
(<a href="https://doi.org/10.1007/s12065-024-00984-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the time series analysis and forecasting of the Volatility Index (VIX) data in the United States. We employed the Complete Empirical Mode Decomposition (CEEMD) method to decompose the original VIX data into seven Intrinsic Mode Functions (IMFs). Subsequently, each IMF determined its stationarity using the Augmented Dickey-Fuller (ADF) test. For stationary IMFs, an AutoRegressive Integrated Moving Average (ARIMA) model was built, while non-stationary IMFs were fitted using machine learning regression algorithms. The sum of the predicted values of all IMFs was considered as the forecast VIX value. About the parameter selection in machine learning algorithms, we employed the Optuna library to optimize the model’s parameters. We selected the parameters that resulted in the lowest Mean Squared Error (MSE) on a validation set to be the optimal model parameters. Applying this methodology, we found that our hybrid model has greater predictive capabilities compared to simple machine learning models when the market fluctuates greatly. Specifically, the AdaBoost model outperformed a basic decision tree model and a random forest model in terms of MSE and MAE (mean absolute error) accuracy.},
  archive      = {J_EI},
  author       = {Liang, Zhuqin and Ismail, Mohd Tahir},
  doi          = {10.1007/s12065-024-00984-x},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {Advanced CEEMD hybrid model for VIX forecasting: Optimized decision trees and ARIMA integration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic optimization and strategic behavior of
markovian vacation queue with retrial policy: Application to virtual
call center. <em>EI</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s12065-024-00987-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigation is concerned with social optimization and customers’ strategic behavior for a double orbit retrial queueing model with vacation, aiming to enhance the performance of virtual call centers. In many call center scenarios, if the server is busy, the arriving customer moves to premium/ordinary orbit, i.e., becomes a repeated customer; otherwise, if the server is accessible, the arriving customer joins the system to receive the required service. Once the service is completed, the server will look into the premium orbit to check whether there is any customer who needs service. If no new customer from premium/ordinary orbit or outside arrives and the system is empty, then the server takes a vacation. The customer’s decision to wait or balk from the system depends on the server’s status and the reward for receiving the service. By using a probability generating function and iterative approach, the long-run probability distribution of the queue size and other metrics, viz. equilibrium thresholds, entering probability, etc., have been obtained. Moreover, the social welfare function is analyzed based on two given information levels. The optimal solution is presented by solving the social welfare maximization problem using particle swarm optimization and harmony search techniques. The impact of different parameters on the performance metrics in Virtual Call Centers (VCC) is examined.},
  archive      = {J_EI},
  author       = {Dhibar, Sibasish and Jain, Madhu},
  doi          = {10.1007/s12065-024-00987-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Evol. Intell.},
  title        = {Metaheuristic optimization and strategic behavior of markovian vacation queue with retrial policy: Application to virtual call center},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising SMIB system stability: FOPID controller tuning
via harris hawks optimisation. <em>EI</em>, <em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12065-024-00991-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current power system stabilizer designs, including fuzzy-PID and Lead-Lag compensators, need help with adaptability and complexity in diverse and evolving power system environments. Conventional tuning methods like the Newton–Raphson approach and optimization strategies like particle swarm optimization, genetic algorithms, and cuckoo search algorithms face challenges in achieving optimal performance for Fractional Order Proportional Integral Derivative (FOPID) controllers. There is a critical need for innovative tuning methods that offer enhanced adaptability and performance in complex power system stability analysis. This research contributes to the advancement of power system stability analysis, specifically in SMIB systems, offering insights into optimizing FOPID controllers utilizing the innovative Harris Hawks Optimization (HHO) algorithm. The work expects these findings to broaden the implications for power system control and enhance the stability of Single Machine Infinite Bus (SMIB) systems, thus fostering the resilience and reliability of modern electrical infrastructure. The FOPID controller encompasses fractional order parameters, encompassing proportional, integral, and derivative gain, integral order, and derivative order, each exerting a substantial influence on control responses and stability. This research harnesses HHO, an optimization technique inspired by nature, to fine-tune FOPID parameters. The investigation involves initializing the SMIB model, formulating an objective function to minimize control errors, and employing HHO iteratively to refine the FOPID controller. The outcomes reveal enhanced stability, diminished overshoot, accelerated settling time, and transient response.},
  archive      = {J_EI},
  author       = {Kirange, Yogesh Kalidas and Nema, Pragya},
  doi          = {10.1007/s12065-024-00991-y},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Evol. Intell.},
  title        = {Optimising SMIB system stability: FOPID controller tuning via harris hawks optimisation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multitasking algorithm based on a dynamic
solution encoding strategy for the minimum s-club cover problem.
<em>EI</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s12065-024-00999-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding a cohesive subgraph, notably the s-club model, which is a subgraph with diameter at most s, is a widely applied topic in social network analysis and group of objects modeling. In particular, the minimum s-club cover problem (min s-club cover) is a recently introduced variant in the literature which asks to cover the vertices of a graph with a minimum number of s-clubs. The existence of common connections among these highly connected components encourages the application of multitasking optimization to leverage the shared meaningful knowledge in the discovery of multiple s-clubs at the same time. Therefore, this study proposes a multitasking evolutionary algorithm to solve the minimum s-club cover problem. Our proposal is designed with an effective solution representation method and evolutionary operators for the variation in the number of clubs. Each solution is represented by two components, where the gene number of a component can be different for each individual and can change during performing evolutionary operations. We also propose a solution generation method based on a random greedy algorithm that helps to ensure individual quality and population diversity in the initial population. The proposed algorithm is evaluated on two datasets in the DIMACS library. This study then analyzed the influence of different factors of the input data on the proposed algorithm results. Based on statistical analysis of the performance results, it is clear that our proposal’s solution is superior to an existing algorithm on two-thirds of the experimental data set.},
  archive      = {J_EI},
  author       = {Thanh, Pham Dinh and Long, Nguyen Binh and Vinh, Le Sy and Binh, Huynh Thi Thanh},
  doi          = {10.1007/s12065-024-00999-4},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Evol. Intell.},
  title        = {Evolutionary multitasking algorithm based on a dynamic solution encoding strategy for the minimum s-club cover problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient plant disease prediction model based on machine
learning and deep learning classifiers. <em>EI</em>, <em>18</em>(1),
1–33. (<a href="https://doi.org/10.1007/s12065-024-01000-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is the majorsource of the nation&#39;s economic growth, but the emergence of certain plant-related diseases has a negative effect on the sector&#39;s production. Diagnosing plant diseases is crucial to solving this issue, teaching farmers how to prevent diseases, and implementing effective management. The total yield may be negatively impacted by diseases if they are not discovered early, which would reduce the farmer&#39;s profits. Numerous researchers have presented various cutting-edge systems based on numerous techniques to address this issue. Numerous approaches have been used already by researchers for this purpose, but some techniques relating to vision have not yet been investigated. Six cutting-edge models for plant disease detection based on machine learning and deep learning were briefly deliberated in this research to analyze the efficiency of each approach. In this research, the Resnet-101 model is used to efficiently extract the feature, and the accuracy, sensitivity, and specificity of the metrics are measured to demonstrate the efficiency of each model. Recent research has shown average accuracies of 94.38%, 94.785, and 92.45% in the detection of Apple, potato, and strawberry diseases respectively.},
  archive      = {J_EI},
  author       = {Shinde, Nirmala and Ambhaikar, Asha},
  doi          = {10.1007/s12065-024-01000-y},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Evol. Intell.},
  title        = {An efficient plant disease prediction model based on machine learning and deep learning classifiers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient multilevel thresholding image segmentation
through improved elephant herding optimization. <em>EI</em>,
<em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12065-024-01001-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed an improved version of recently developed swarm-based metaheuristic algorithm elephant herding optimization (EHO) called Improved Elephant Herding Optimization (IEHO). In this IEHO, the opposition-based learning (OBL) rule and chaos-embedded sequences are incorporated with each iterative stage of EHO to maintain the proper balance between the exploration and exploitation phase. It regulates the movement of the search agents and avoids premature convergence. The effectiveness of the proposed model is evaluated in terms of finding the optimal threshold value in Multilevel thresholding (MTH) of image segmentation which separates the different objects of the images. The methods such as the Kapur entropy, Otsu and masi entropy are used as the objective function in this problem to determine the optimal threshold. The proposed IEHO’s performance is compared with the different variants of EHO, artificial bee colony (ABC) and artificial hummingbird algorithms (AHA). The simulation results regarding convergence speed, stability, and solution quality performance indicators, such as the structural similarity index (SSIM), feature similarity index (FSIM), and peak signal-to-noise ratio (PSNR) verify the viability of the above hybrid algorithm.},
  archive      = {J_EI},
  author       = {Chakraborty, Falguni and Roy, Provas Kumar},
  doi          = {10.1007/s12065-024-01001-x},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Evol. Intell.},
  title        = {An efficient multilevel thresholding image segmentation through improved elephant herding optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of accuracy and computational
complexity across 21 swarm intelligence algorithms. <em>EI</em>,
<em>18</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s12065-024-00997-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear, complex optimization problems are prevalent in many scientific and engineering fields. Traditional algorithms often struggle with these problems due to their high dimensionality and intricate nature, making them time-consuming. Many researchers have proposed new metaheuristic algorithms inspired by biological behaviors in nature, which comparatively show higher performance and accuracy than traditional optimization algorithms. Nature-inspired algorithms, particularly those based on swarm intelligence, offer adaptable and efficient solutions to these challenges. In recent years, swarm intelligence algorithms have made significant advancements. Classical and CEC benchmark suits are immersively useful for studying the performance of optimization algorithms. According to our literature survey, we identified that many algorithms were evaluated based on accuracy. Currently, swarm intelligence algorithms are used in many applications, and efficiency and computational complexity need to be evaluated. A broad-level study of the computational complexity and accuracy of popular swarm intelligence algorithms has not been done recently. Therefore this study we comprehensively evaluate and compare 21 bio-inspired swarm intelligence algorithms on eight non-separable unimodal, eight separable unimodal, five non-separable multimodal, seven separable multimodal functions, and two CEC 2018 many objective functions. We study the structure and mathematical model of the selected algorithms. Then we categorized selected algorithms into six different behavioral groups. We calculated the root mean square error between expected and actual values. Then we performed an RMSE cross-validation statistical test to understand how accurately an algorithm resolves an average problem. We found that Artificial Lizard Search Optimization (ALSO) is the most prominent algorithm in accuracy and efficiency. Besides that, Cat Swarm Optimization (CSO), Squirrel Search Algorithm (SSA), and Chimp Optimization Algorithm (CHOA-B) are also considered more universal algorithms. The Squirrel Search Algorithm (SSA) is ALSO’s second-best algorithm in time complexity. Wasp Swarm Algorithm (WSO), and Bat-Inspired Algorithm (BA) presented the lowest time complexity. Finally, several important issues and research directions are discussed.},
  archive      = {J_EI},
  author       = {Warnakulasooriya, Kolitha and Segev, Aviv},
  doi          = {10.1007/s12065-024-00997-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Evol. Intell.},
  title        = {Comparative analysis of accuracy and computational complexity across 21 swarm intelligence algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Student psychology based optimization algorithm integrating
differential evolution and hierarchical learning for solving data
clustering problems. <em>EI</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s12065-024-01003-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to accurately and efficiently perform data clustering in complex multidimensional data analysis and processing tasks is a challenging research problem. Traditional optimization algorithms often need help with the problems of quickly falling into local optimum and insufficient global search ability when dealing with high-dimensional, multi-peaked and complex structured data. In order to solve this challenge, a student psychology based optimization algorithm (GDLSPBO) that integrates differential evolution and hierarchical learning mechanisms was proposed, aiming to improve the accuracy, stability and global optimization ability of data clustering. GDLSPBO enhances the population diversity and prevents the algorithm from falling into local optimum by introducing the differential evolution mechanism. Simultaneously, the hierarchical learning strategy improves the algorithm’s search efficiency and local optimization ability. The experiments are validated on the CEC-BC-2017 benchmark functions. Several real datasets and the results show that GDLSPBO achieves an F-measure of 0.9595 and an Adjusted Rand coefficient of 0.8578 on the Cancer dataset, and the clustering accuracy on the Iris dataset reaches 93.33%, which is significantly better than that of other classical optimization algorithms. This indicates that GDLSPBO has a more substantial clustering effect and higher solution accuracy in solving complex data clustering problems. The experimental results verify that the global search ability and optimization accuracy of GDLSPBO on multidimensional complex data sets have been significantly improved, demonstrating its broad applicability and robustness in practical data clustering applications.},
  archive      = {J_EI},
  author       = {Bao, Yin-Yin and Wang, Jie-Sheng and Liu, Jia-Xu and Zhao, Xiao-Rui and Yang, Qing-Da and Zhang, Shi-Hui},
  doi          = {10.1007/s12065-024-01003-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Evol. Intell.},
  title        = {Student psychology based optimization algorithm integrating differential evolution and hierarchical learning for solving data clustering problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast implementation of extreme learning machine-based
directRanker for surrogate-assisted evolutionary algorithms.
<em>EI</em>, <em>18</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s12065-024-01005-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary algorithms (SAEAs) have been widely used to solve computationally expensive optimization problems. The extreme learning machine-based DirectRanker (ELDR) is a single-layer feed-forward neural network surrogate model designed for SAEAs. ELDR estimates the superiority of two solutions with a high estimation accuracy, even in high-dimensional problems. However, ELDR requires a long computation time as the problem dimensionality and the number of hidden neurons increase, thus making it difficult to apply it to high-dimensional problems. A surrogate model should be computationally efficient and enable rapid fitness estimations. Therefore, this paper proposes a fast implementation technique, i.e., fast version ELDR (fELDR) that achieves mathematically equivalent learning results with low computational complexity. Additionally, this paper proposes a pointwise score function to render the prediction results reusable. The experimental results confirmed the effectiveness of fELDR when compared with the original ELDR. The learning results of the proposed fELDR were equivalent to those of the original ELDR while reducing the training time by up to 97%, especially when using a large hidden layer on a high dimensionality problem. Moreover, due to the reusable prediction results, the computation time of the fELDR-assisted SAEA can be further decreased by 79.5% when compared with that of the original ELDR-assisted SAEA. The reduced training time and reusable prediction results of fELDR render it feasible to apply ELDR to high-dimensional optimization problems and realize a high prediction accuracy with a large number of hidden neurons.},
  archive      = {J_EI},
  author       = {Harada, Tomohiro},
  doi          = {10.1007/s12065-024-01005-7},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Evol. Intell.},
  title        = {Fast implementation of extreme learning machine-based directRanker for surrogate-assisted evolutionary algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy enhanced artificial rabbit optimization
algorithm for solving engineering optimization problems. <em>EI</em>,
<em>18</em>(1), 1–54. (<a
href="https://doi.org/10.1007/s12065-024-01002-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations of the artificial rabbit optimization (ARO) algorithm in handling complex and large-scale optimization problems, this paper proposes an enhanced multi-strategy algorithm—MARO. The proposed algorithm integrates several strategies, including elite opposition-based learning, Levy flight, random walk, and adaptive weight factors, to significantly improve the search capability and convergence accuracy of ARO. First, the algorithm applies the elite opposition-based learning strategy in each iteration to generate opposite solutions, which enriches population diversity and enhances global search capacity, effectively preventing the algorithm from becoming trapped in local optima. Second, it incorporates Levy flight and random walk strategies to increase search randomness during the exploration phase, thus improving the probability of finding the global optimum. Finally, the adaptive weight factor is introduced during the exploitation phase to dynamically adjust the search direction based on different stages, achieving a balance between global exploration and local exploitation. Experimental results demonstrate that MARO consistently outperforms ARO and other popular algorithms across multiple international benchmark test suites. On the CEC2005 test functions, MARO shows significant improvements over CMAES and LSHADE on 9 functions. For the CEC2017 suite, MARO achieves the best average ranking on nearly three-quarters of the test functions, with an average rank of 1.38, significantly outperforming CMAES (3.48) and LSHADE (3.52). On the CEC2019 suite, MARO ranks first in 70% of the test functions. Statistical significance tests, including the Friedman and Wilcoxon tests, confirm that MARO&#39;s p-values are less than 0.05 when compared with benchmark algorithms, validating the algorithm&#39;s effectiveness. Moreover, MARO also exhibits outstanding optimization performance in real-world engineering problems such as pressure vessel design and reservoir scheduling optimization. The main contribution of this paper lies in the introduction of an innovative multi-strategy optimization framework, which enhances both exploration and exploitation capabilities. This framework effectively addresses the shortcomings of existing ARO algorithms in handling complex problems, providing a more competitive solution for practical optimization tasks.},
  archive      = {J_EI},
  author       = {He, Ni-ni and Wang, Wen-chuan and Wang, Jun},
  doi          = {10.1007/s12065-024-01002-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-54},
  shortjournal = {Evol. Intell.},
  title        = {Multi-strategy enhanced artificial rabbit optimization algorithm for solving engineering optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified random-oppositional chaotic artificial rabbit
optimization algorithm for solving structural problems and optimal
sizing of hybrid renewable energy system. <em>EI</em>, <em>18</em>(1),
1–63. (<a href="https://doi.org/10.1007/s12065-024-01004-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial rabbit optimization (ARO) algorithm replicates the survival skills of rabbits in the wild. However, like other metaheuristic approaches it possesses significant drawbacks in solving challenging problems, including sluggish convergence rate, poor exploration ability and trapped in local optima region. To alleviate these shortcomings, a novel strategy, namely Modified Random Opposition (MRO) and ten chaotic maps are integrated with ARO, termed as MROCARO. This implementation MRO technique boost the population diversity and permits the population to escape from local optima while integration of chaotic map enhances the exploitation capability. To estimate the effectiveness of the MROCARO method, the well-known CEC2005, CEC2017, CEC2019 and CEC2008lsgo test functions are considered. Moreover, non-parametric tests that include the Wilcoxon rank-sum and Friedman rank test are performed to analyze the significant difference among the compared algorithms. Furthermore, the efficiency of the MROCARO algorithm has been evaluated on various structural problems and optimal sizing of renewable energy systems. The experimental findings demonstrate that MROCARO performed optimum solution with 100% renewable sources with the lowest levelized cost of electricity of 0.0934 $/kWh as compared to other methods. Also, the simulation findings reveal that MROCARO has immense potential for addressing global optimization and structural problems as contrasted to other competing algorithms.},
  archive      = {J_EI},
  author       = {Mohapatra, Sarada and Lala, Himadri and Mohapatra, Prabhujit},
  doi          = {10.1007/s12065-024-01004-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-63},
  shortjournal = {Evol. Intell.},
  title        = {Modified random-oppositional chaotic artificial rabbit optimization algorithm for solving structural problems and optimal sizing of hybrid renewable energy system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning with industrial robots: Exploring the
impact of joint angles on cartesian coordinates using explainable AI.
<em>EI</em>, <em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12065-024-01006-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study uses Explainable Artificial Intelligence techniques to reveal the complex relationship between joint angles and Cartesian coordinates in the context of industrial robotic arms. By using machine learning and Explainable Artificial Intelligence algorithms, it is aimed to distinguish the dominant effect of individual joint angles on the x, y and z coordinates of the robotic end effector. Various machine learning algorithms have been applied on the data set and performance outputs have been obtained. According to these performance results, it has been observed that the RandomForest algorithm is more suitable for our study than other models with its low mean square error and high r-square score. Along with the selected machine learning algorithm, the data set was tried to be explained by passing it through SHapley Additive exPlanations (SHAP), Descriptive Machine Learning EXplanations (DALEX) and Explain Like I’m 5 (ELI5) models, which are Explainable Artificial Intelligence models. It has been observed that the SHAP model explains the effects of joint angles on Cartesian coordinates more consistently than other models, with an average sensitivity of 0.0125 value range.Our findings shed light on the explainability aspect of AI models and provide valuable information about the fundamental mechanisms governing the complex movements of industrial robot arms.},
  archive      = {J_EI},
  author       = {Özkurt, Cem},
  doi          = {10.1007/s12065-024-01006-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Evol. Intell.},
  title        = {Machine learning with industrial robots: Exploring the impact of joint angles on cartesian coordinates using explainable AI},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-based multi-objective optimization of trusses
with greylag goose algorithm. <em>EI</em>, <em>18</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s12065-024-01011-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a multi-objective variant of the Greylag Goose Optimizer (MOGGO) to tackle complex structural optimization problems. Inspired by the cooperative behavior of geese in flight, MOGGO employs dynamic grouping to enhance problem-solving efficiency. Six truss structures undergo simultaneous topology, shape, and size optimization using MOGGO, aiming to maximize reliability while minimizing structural mass. By incorporating non-dominance sorting and archiving techniques, MOGGO extends the single-objective Greylag Goose Optimizer to effectively address trade-offs between competing objectives. Evaluation metrics and statistical tests demonstrate MOGGO&#39;s superior performance in handling large structural optimization problems, preserving more Pareto-optimal sets, and achieving greater convergence and variance in objective and decision spaces. MOGGO’s ability to manage conflicting objectives is further validated through diversity analysis, with swarm plots illustrating its superior convergence behavior across iterations. Overall, MOGGO proves to be an efficient and effective approach for addressing challenging reliability-based multi-objective structural optimization problems. Query ID=&quot;Q1&quot; Text=&quot;Please confirm if the author names are presented accurately and in the correct sequence (given name, middle name/initial, family name). Author 1 Given name: [specify authors given name] Last name [specify authors last name]. Also, kindly confirm the details in the metadata are correct.&quot;},
  archive      = {J_EI},
  author       = {Mashru, Nikunj and Tejani, Ghanshyam G. and Patel, Pinank},
  doi          = {10.1007/s12065-024-01011-9},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Evol. Intell.},
  title        = {Reliability-based multi-objective optimization of trusses with greylag goose algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing class imbalance in remote sensing using deep
learning approaches: A systematic literature review. <em>EI</em>,
<em>18</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s12065-024-01012-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is one of the major issues for the application of deep learning for remote sensing imagery. High-resolution remote sensing image sample sets are prone to cause the problem of sample imbalance among classes due to the skewed distribution of ground features, which has been a huge challenge to machine learning and data mining and aroused strong attention. Despite the emerging interest in Deep Learning (DL), empirical research on its effectiveness with imbalanced data in remote sensing remains scarce. Therefore, this study aims to systematically review existing studies using DL approaches for handling class imbalanced data in the field of remote sensing, focusing on its significance in critical applications such as land use land cover classification, pixel-wise segmentation, and object detection. This study presents the most widely used balancing algorithms published from 2016 to 2024. This survey divulges that while using DL technologies, either the data augmentation can be applied to the minority class images to generate more varied samples or using algorithm-level methods by incorporating modifications to the loss function such as cross-entropy, focal loss, dice loss by assigning higher weights to minority class samples, ensuring the model pays more attention to underrepresented classes during training. While traditional techniques such as data sampling and cost-sensitive learning continue to be relevant, emerging methods such as GAN, which leverage neural network feature learning capabilities, show promise. Our discussion identifies research gaps and provides insights to guide future efforts in utilizing DL for imbalanced data in remote sensing applications.},
  archive      = {J_EI},
  author       = {Sharma, Shweta and Gosain, Anjana},
  doi          = {10.1007/s12065-024-01012-8},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Evol. Intell.},
  title        = {Addressing class imbalance in remote sensing using deep learning approaches: A systematic literature review},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for obstacle-range estimation based on
echo-location using high-end sound with applications. <em>EI</em>,
<em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s12065-024-01014-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient mapping algorithms have been implemented in different fields of robotics applications. Some of those applications are in the mapping of environments with difficult accessibility, the planning of service or emergency assistant robots, or as supporting tools in helping people with disabilities. In those examples, the essential task of a mapping algorithm is to detect obstacles which may interfere in motion paths. One of the most widely used characteristics in pulse-echo range estimation is the time of flight. However, the ultrasonic frequencies (range from 30 to 120 KHz) are poorly suitable for the long distances faced in in-door applications. In view of the importance of developing efficient mapping algorithms, the present manuscript reports on an efficient method based on high-frequency waves in order to range static objects under a controlled environment. In the present approach, the signal acquisition is made out-of-band of the processing algorithm. This process is carried out by using an active speaker to throw the control ping sound. The resulting audio is captured through a unidirectional dynamic microphone Shure® SV100. The audio signal processing is based on selective filtering via Gabor models and wavelet decimation de-noising. The proposed method is compared against two common methodologies for range estimation in robotics based on ultrasonic and infrared sensor. The current methodology demonstrated a competitive and, in some cases, better performance than the standard methods.},
  archive      = {J_EI},
  author       = {Guerrero-Díaz-de-León, J. Antonio and Hernández-Torres, Rubén and Macías, Siegfried and Macías-Díaz, Jorge E.},
  doi          = {10.1007/s12065-024-01014-6},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Evol. Intell.},
  title        = {A method for obstacle-range estimation based on echo-location using high-end sound with applications},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent approach for analyzing the effects of normal
tumor immune unhealthy diet model through unsupervised physics informed
neural-networks integrated with meta-heuristic algorithms. <em>EI</em>,
<em>18</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s12065-024-01007-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the development of integrated intelligent computing using physics-informed neural networks to solve the mathematical normal-tumor immune-unhealthy diet model. The model considers vitamin intervention as a moderating factor within one day. A nonlinear activation function called sigmoid was used for the model across three different scenarios to define the fitness or error function. For computing the optimized biases and weights of physics-informed neural networks, hybridization of heuristic algorithms, particularly particle swarm optimization and neural networks algorithm are employed from $$-10$$ to $$10$$ . The proposed technique&#39;s accuracy, reliability, and validity are demonstrated by consistently matching the results obtained using NDsolve from mathematics built-in function as a reference solution. Additionally, statistical analyses including absolute and mean squared errors are conducted to confirm further the precision and accuracy of the physics-informed neural networks. It is observed that the absolute errors and mean square error between the reference solution and proposed technique range from $${10}^{-2}$$ to $${10}^{-9}$$ and range from $${10}^{-6}$$ to $${10}^{-11}$$ for distinct cases. The novelty of this study highlights the importance of physics-informed neural networks for a balanced diet rich in vitamins for reducing the risk of deadly diseases, particularly cancer and the potential of machine learning algorithms in modelling and analyzing complex biological systems.},
  archive      = {J_EI},
  author       = {Aslam, Muhammad Naeem and Shaukat, Nadeem and Arshad, Muhammad Sarmad and Aslam, Muhammad Waheed and Hussain, Javed},
  doi          = {10.1007/s12065-024-01007-5},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Evol. Intell.},
  title        = {An intelligent approach for analyzing the effects of normal tumor immune unhealthy diet model through unsupervised physics informed neural-networks integrated with meta-heuristic algorithms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEL: A strategy for resolving redundancy in entity pairs
within dual entity linker for relational triple extraction. <em>EI</em>,
<em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s12065-024-01008-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the mainstream methods for relational triple extraction mainly employ joint models and have achieved significant results. However, these methods still face challenges in dealing with the complexity of unstructured text. In joint models based on labeled relational triple extraction, the BiRTE model suffers from the issue of redundant entity pairs, leading to the generation of incorrect triples. To address this issue, This paper introduces a Ground Entity Extractor which is designed to aid the Dual Entity Linker (DEL), in addition, adversarial training methods are introduced during the DEL training process. Experimental results demonstrate that the DEL model extracts entity pairs from two directions, when compared to the previous BIRTE model, it generates more accurate entity pairs. On the WebNLG dataset, The accuracy increased by 2.9%, and F1 improved by 1.1%. On the NYT10 dataset, accuracy increased by 1.0%, recall increased by 0.5%, and F1 score improved by 0.7%. This brings significant enhancements to the triple entity extraction task, and in comparison with 8 baseline models across all datasets, the DEL model achieves better results.},
  archive      = {J_EI},
  author       = {Hu, Songhua and Wang, Hengxin},
  doi          = {10.1007/s12065-024-01008-4},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Evol. Intell.},
  title        = {DEL: A strategy for resolving redundancy in entity pairs within dual entity linker for relational triple extraction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BIM model generation and protection: An automated solution
under deep learning and differential privacy. <em>EI</em>,
<em>18</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s12065-024-01009-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the problems of low accuracy, low efficiency, and insufficient data security in building information modeling models, an automation scheme based on deep learning and differential privacy is introduced to improve the efficiency and security of construction projects. Firstly, this article utilized residual network and long short-term memory models to extract features from the spatial structure and temporal information of the building information modeling model. Secondly, it used the Adam optimizer for gradient descent to optimize network weights; then, for the evaluation of geometric accuracy and data integrity, this article adjusted the number of network layers and convolution kernel size. At the same time, this article integrated a differential privacy framework into the model, dynamically adjusted the privacy budget, and finally evaluated the building information modeling model for automated generation and protection. The research results indicated that the building information modeling model had an accuracy of 0.94 in aligning the edges of the structural framework, and the internal partition wall angle deviation was only 0.10. Its error in the area covered by internal partitions and floors was only 0.04, with a success rate of 0.82 for data leakage risk, a detection rate of 0.91, and a response time of 0.09 s. The model can ensure the security of critical information while achieving high accuracy and efficiency.},
  archive      = {J_EI},
  author       = {Zhang, Xuewei},
  doi          = {10.1007/s12065-024-01009-3},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Evol. Intell.},
  title        = {BIM model generation and protection: An automated solution under deep learning and differential privacy},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic PFRCosSim layer for solving filter redundancy
problem in CNNs applied on plant disease classification. <em>EI</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-01010-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved remarkable success in various artificial intelligence domains, particularly in pattern recognition, image processing, and speech recognition. However, the growing complexity of these models introduces challenges related to parameter redundancy, significantly impacting CNN performance. This paper addresses the issue of increasing parameter redundancy, focusing on the specific problem of filter redundancy during CNN training. The proposed approach involves regularization of the initialization filters to reduce redundancy at each convolutional layer. A novel layer, PFRCosSim, is introduced, computing the Cosine similarity between filters used in CNN training to ensure filter homogeneity. we reset the filters using an Orthogonal initialization based on the random choice of kernels concerning all filters in the same layers. The method is tested on a three-layer CNN model and extended to common architectures like LeNet and VGG16. Validation is performed through plant disease classification using datasets: Plant Pathology 2020 and Plant Disease Recognition. The application of this approach yields significant accuracy improvements, exceeding $$99 \%$$ .},
  archive      = {J_EI},
  author       = {Lagnaoui, Saloua and En-naimani, Zakariae and Haddouch, Khalid},
  doi          = {10.1007/s12065-024-01010-w},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Stochastic PFRCosSim layer for solving filter redundancy problem in CNNs applied on plant disease classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid PSO-GA optimization for enhancing decision tree
performance in soil classification and crop cultivation prediction.
<em>EI</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s12065-024-01015-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture holds profound significance in the lives and livelihoods of Bangladesh’s population. The escalating population has contributed to a reduction in available arable land, exacerbating concerns about the feasibility of farming. In various regions of Bangladesh, there is a prevailing perception that certain areas are unsuitable for cultivation. Consequently, a substantial amount of land still needs to be tapped and explored for agricultural purposes, contributing to underutilization and hindering potential agricultural development in these regions. Considering these issues, this study seeks to forecast the optimal crop choices for specific soil classes, enabling individuals to make more informed decisions about crop cultivation on their land. Initially, the soil class is identified based on its unique characteristics within a given area, and subsequently, crop selection is determined based on these distinct soil classes. This study explores the performance of Particle Swarm Optimization (PSO) and Genetic Algorithm (GA). It proposes a hybrid PSO-GA technique to optimize the Decision Tree model for forecasting crop cultivation based on classifying soil. The performance of a standalone decision tree model is also measured. In soil classification, the hybrid PSO-GA approach demonstrates superior performance, achieving an accuracy of 96.04%, surpassing individual PSO, GA, and standalone decision tree methods. In crop cultivation prediction, the hybrid approach outperforms individual PSO, GA, and decision tree methods with an accuracy of 92.63%. The results highlight the efficacy of the integrated PSO-GA strategy in optimizing the DT model for precise agricultural predictions. This research contributes valuable insights for enhancing decision support systems in agriculture, providing a promising avenue for improved accuracy in soil classification and crop cultivation prediction.},
  archive      = {J_EI},
  author       = {Rahman, Fardowsi and Khan, Md. Ashikur Rahman and Alam, Mahbubul},
  doi          = {10.1007/s12065-024-01015-5},
  journal      = {Evolutionary Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Evol. Intell.},
  title        = {Hybrid PSO-GA optimization for enhancing decision tree performance in soil classification and crop cultivation prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="focm---8">FoCM - 8</h2>
<ul>
<li><details>
<summary>
(2025). Strong norm error bounds for quasilinear wave equations
under weak CFL-type conditions. <em>FoCM</em>, <em>25</em>(1), 303–350.
(<a href="https://doi.org/10.1007/s10208-024-09639-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper, we consider a class of quasilinear wave equations on a smooth, bounded domain. We discretize it in space with isoparametric finite elements and apply a semi-implicit Euler and midpoint rule as well as the exponential Euler and midpoint rule to obtain four fully discrete schemes. We derive rigorous error bounds of optimal order for the semi-discretization in space and the fully discrete methods in norms which are stronger than the classical $$H^1\times L^2$$ energy norm under weak CFL-type conditions. To confirm our theoretical findings, we also present numerical experiments.},
  archive      = {J_FoCM},
  author       = {Dörich, Benjamin},
  doi          = {10.1007/s10208-024-09639-w},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {303-350},
  shortjournal = {Found. Comput. Math.},
  title        = {Strong norm error bounds for quasilinear wave equations under weak CFL-type conditions},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exotic b-series and s-series: Algebraic structures and order
conditions for invariant measure sampling. <em>FoCM</em>,
<em>25</em>(1), 271–301. (<a
href="https://doi.org/10.1007/s10208-023-09638-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {B-Series and generalizations are a powerful tool for the analysis of numerical integrators. An extension named exotic aromatic B-Series was introduced to study the order conditions for sampling the invariant measure of ergodic SDEs. Introducing a new symmetry normalization coefficient, we analyze the algebraic structures related to exotic B-Series and S-Series. Precisely, we prove the relationship between the Grossman–Larson algebras over exotic and grafted forests and the corresponding duals to the Connes–Kreimer coalgebras and use it to study the natural composition laws on exotic S-Series. Applying this algebraic framework to the derivation of order conditions for a class of stochastic Runge–Kutta methods, we present a multiplicative property that ensures some order conditions to be satisfied automatically.},
  archive      = {J_FoCM},
  author       = {Bronasco, Eugen},
  doi          = {10.1007/s10208-023-09638-3},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {271-301},
  shortjournal = {Found. Comput. Math.},
  title        = {Exotic B-series and S-series: Algebraic structures and order conditions for invariant measure sampling},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational complexity of decomposing a symmetric matrix
as a sum of positive semidefinite and diagonal matrices. <em>FoCM</em>,
<em>25</em>(1), 223–269. (<a
href="https://doi.org/10.1007/s10208-023-09637-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study several variants of decomposing a symmetric matrix into a sum of a low-rank positive-semidefinite matrix and a diagonal matrix. Such decompositions have applications in factor analysis, and they have been studied for many decades. On the one hand, we prove that when the rank of the positive-semidefinite matrix in the decomposition is bounded above by an absolute constant, the problem can be solved in polynomial time. On the other hand, we prove that, in general, these problems as well as their certain approximation versions are all NP-hard. Finally, we prove that many of these low-rank decomposition problems are complete in the first-order theory of the reals, i.e., given any system of polynomial equations, we can write down a low-rank decomposition problem in polynomial time so that the original system has a solution iff our corresponding decomposition problem has a feasible solution of certain (lowest) rank.},
  archive      = {J_FoCM},
  author       = {Tunçel, Levent and Vavasis, Stephen A. and Xu, Jingye},
  doi          = {10.1007/s10208-023-09637-4},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {223-269},
  shortjournal = {Found. Comput. Math.},
  title        = {Computational complexity of decomposing a symmetric matrix as a sum of positive semidefinite and diagonal matrices},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast optimistic gradient descent ascent (OGDA) method in
continuous and discrete time. <em>FoCM</em>, <em>25</em>(1), 163–222.
(<a href="https://doi.org/10.1007/s10208-023-09636-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of real Hilbert spaces, we study continuous in time dynamics as well as numerical algorithms for the problem of approaching the set of zeros of a single-valued monotone and continuous operator V. The starting point of our investigations is a second-order dynamical system that combines a vanishing damping term with the time derivative of V along the trajectory, which can be seen as an analogous of the Hessian-driven damping in case the operator is originating from a potential. Our method exhibits fast convergence rates of order $$o \left( \frac{1}{t\beta (t)} \right) $$ for $$\Vert V(z(t))\Vert $$ , where $$z(\cdot )$$ denotes the generated trajectory and $$\beta (\cdot )$$ is a positive nondecreasing function satisfying a growth condition, and also for the restricted gap function, which is a measure of optimality for variational inequalities. We also prove the weak convergence of the trajectory to a zero of V. Temporal discretizations of the dynamical system generate implicit and explicit numerical algorithms, which can be both seen as accelerated versions of the Optimistic Gradient Descent Ascent (OGDA) method for monotone operators, for which we prove that the generated sequence of iterates $$(z_k)_{k \ge 0}$$ shares the asymptotic features of the continuous dynamics. In particular we show for the implicit numerical algorithm convergence rates of order $$o \left( \frac{1}{k\beta _k} \right) $$ for $$\Vert V(z^k)\Vert $$ and the restricted gap function, where $$(\beta _k)_{k \ge 0}$$ is a positive nondecreasing sequence satisfying a growth condition. For the explicit numerical algorithm, we show by additionally assuming that the operator V is Lipschitz continuous convergence rates of order $$o \left( \frac{1}{k} \right) $$ for $$\Vert V(z^k)\Vert $$ and the restricted gap function. All convergence rate statements are last iterate convergence results; in addition to these, we prove for both algorithms the convergence of the iterates to a zero of V. To our knowledge, our study exhibits the best-known convergence rate results for monotone equations. Numerical experiments indicate the overwhelming superiority of our explicit numerical algorithm over other methods designed to solve monotone equations governed by monotone and Lipschitz continuous operators.},
  archive      = {J_FoCM},
  author       = {Boţ, Radu Ioan and Csetnek, Ernö Robert and Nguyen, Dang-Khoa},
  doi          = {10.1007/s10208-023-09636-5},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {163-222},
  shortjournal = {Found. Comput. Math.},
  title        = {Fast optimistic gradient descent ascent (OGDA) method in continuous and discrete time},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient random walks on riemannian manifolds.
<em>FoCM</em>, <em>25</em>(1), 145–161. (<a
href="https://doi.org/10.1007/s10208-023-09635-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to a version of Donsker’s theorem, geodesic random walks on Riemannian manifolds converge to the respective Brownian motion. From a computational perspective, however, evaluating geodesics can be quite costly. We therefore introduce approximate geodesic random walks based on the concept of retractions. We show that these approximate walks converge in distribution to the correct Brownian motion as long as the geodesic equation is approximated up to second order. As a result, we obtain an efficient algorithm for sampling Brownian motion on compact Riemannian manifolds.},
  archive      = {J_FoCM},
  author       = {Schwarz, Simon and Herrmann, Michael and Sturm, Anja and Wardetzky, Max},
  doi          = {10.1007/s10208-023-09635-6},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {145-161},
  shortjournal = {Found. Comput. Math.},
  title        = {Efficient random walks on riemannian manifolds},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extremal points and sparse optimization for generalized
kantorovich–rubinstein norms. <em>FoCM</em>, <em>25</em>(1), 103–144.
(<a href="https://doi.org/10.1007/s10208-023-09634-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A precise characterization of the extremal points of sublevel sets of nonsmooth penalties provides both detailed information about minimizers, and optimality conditions in general classes of minimization problems involving them. Moreover, it enables the application of fully corrective generalized conditional gradient methods for their efficient solution. In this manuscript, this program is adapted to the minimization of a smooth convex fidelity term which is augmented with an unbalanced transport regularization term given in the form of a generalized Kantorovich–Rubinstein norm for Radon measures. More precisely, we show that the extremal points associated to the latter are given by all Dirac delta functionals supported in the spatial domain as well as certain dipoles, i.e., pairs of Diracs with the same mass but with different signs. Subsequently, this characterization is used to derive precise first-order optimality conditions as well as an efficient solution algorithm for which linear convergence is proved under natural assumptions. This behavior is also reflected in numerical examples for a model problem.},
  archive      = {J_FoCM},
  author       = {Carioni, Marcello and Iglesias, José A. and Walter, Daniel},
  doi          = {10.1007/s10208-023-09634-7},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {103-144},
  shortjournal = {Found. Comput. Math.},
  title        = {Extremal points and sparse optimization for generalized Kantorovich–Rubinstein norms},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication lower bounds for nested bilinear algorithms
via rank expansion of kronecker products. <em>FoCM</em>, <em>25</em>(1),
55–101. (<a href="https://doi.org/10.1007/s10208-023-09633-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop lower bounds on communication in the memory hierarchy or between processors for nested bilinear algorithms, such as Strassen’s algorithm for matrix multiplication. We build on a previous framework that establishes communication lower bounds by use of the rank expansion, or the minimum rank of any fixed size subset of columns of a matrix, for each of the three matrices encoding a bilinear algorithm. This framework provides lower bounds for a class of dependency directed acyclic graphs (DAGs) corresponding to the execution of a given bilinear algorithm, in contrast to other approaches that yield bounds for specific DAGs. However, our lower bounds only apply to executions that do not compute the same DAG node multiple times. Two bilinear algorithms can be nested by taking Kronecker products between their encoding matrices. Our main result is a lower bound on the rank expansion of a matrix constructed by a Kronecker product derived from lower bounds on the rank expansion of the Kronecker product’s operands. We apply the rank expansion lower bounds to obtain novel communication lower bounds for nested Toom-Cook convolution, Strassen’s algorithm, and fast algorithms for contraction of partially symmetric tensors.},
  archive      = {J_FoCM},
  author       = {Ju, Caleb and Zhang, Yifan and Solomonik, Edgar},
  doi          = {10.1007/s10208-023-09633-8},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {55-101},
  shortjournal = {Found. Comput. Math.},
  title        = {Communication lower bounds for nested bilinear algorithms via rank expansion of kronecker products},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian beam ansatz for finite difference wave equations.
<em>FoCM</em>, <em>25</em>(1), 1–54. (<a
href="https://doi.org/10.1007/s10208-023-09632-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with the construction of Gaussian Beam (GB) solutions for the numerical approximation of wave equations, semi-discretized in space by finite difference schemes. GB are high-frequency solutions whose propagation can be described, both at the continuous and at the semi-discrete levels, by microlocal tools along the bi-characteristics of the corresponding Hamiltonian. Their dynamics differ in the continuous and the semi-discrete setting, because of the high-frequency gap between the Hamiltonians. In particular, numerical high-frequency solutions can exhibit spurious pathological behaviors, such as lack of propagation in space, contrary to the classical space-time propagation properties of continuous waves. This gap between the behavior of continuous and numerical waves introduces also significant analytical difficulties, since classical GB constructions cannot be immediately extrapolated to the finite difference setting, and need to be properly tailored to accurately detect the propagation properties in discrete media. Our main objective in this paper is to present a general and rigorous construction of the GB ansatz for finite difference wave equations, and corroborate this construction through accurate numerical simulations.},
  archive      = {J_FoCM},
  author       = {Biccari, Umberto and Zuazua, Enrique},
  doi          = {10.1007/s10208-023-09632-9},
  journal      = {Foundations of Computational Mathematics},
  month        = {2},
  number       = {1},
  pages        = {1-54},
  shortjournal = {Found. Comput. Math.},
  title        = {Gaussian beam ansatz for finite difference wave equations},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="gpem---15">GPEM - 15</h2>
<ul>
<li><details>
<summary>
(2025). Review: “Computational evolution of neural and morphological
development,” yaochu jin, ISBN 978-981-99-1853-9, springer, 2023.
<em>GPEM</em>, <em>26</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s10710-024-09499-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Vroomans, Renske},
  doi          = {10.1007/s10710-024-09499-x},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Review: “Computational evolution of neural and morphological development”, yaochu jin, ISBN 978-981-99-1853-9, springer, 2023},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on batch training in genetic programming.
<em>GPEM</em>, <em>26</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s10710-024-09501-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Machine Learning (ML), the use of subsets of training data, referred to as batches, rather than the entire dataset, has been extensively researched to reduce computational costs, improve model efficiency, and enhance algorithm generalization. Despite extensive research, a clear definition and consensus on what constitutes batch training have yet to be reached, leading to a fragmented body of literature that could otherwise be seen as different facets of a unified methodology. To address this gap, we propose a theoretical redefinition of batch training, creating a clearer and broader overview that integrates diverse perspectives. We then apply this refined concept specifically to Genetic Programming (GP). Although batch training techniques have been explored in GP, the term itself is seldom used, resulting in ambiguity regarding its application in this area. This review seeks to clarify the existing literature on batch training by presenting a new and practical classification system, which we further explore within the specific context of GP. We also investigate the use of dynamic batch sizes in ML, emphasizing the relatively limited research on dynamic or adaptive batch sizes in GP compared to other ML algorithms. By bringing greater coherence to previously disjointed research efforts, we aim to foster further scientific exploration and development. Our work highlights key considerations for researchers designing batch training applications in GP and offers an in-depth discussion of future research directions, challenges, and opportunities for advancement.},
  archive      = {J_GPEM},
  author       = {Rosenfeld, Liah and Vanneschi, Leonardo},
  doi          = {10.1007/s10710-024-09501-6},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {A survey on batch training in genetic programming},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing evolutionary algorithms for enhanced
characterization of ENSO events. <em>GPEM</em>, <em>26</em>(1), 1–24.
(<a href="https://doi.org/10.1007/s10710-024-09497-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The El Niño-Southern Oscillation (ENSO) significantly influences the complexity and variability of the global climate system, driving its variability. ENSO events’ irregularity and unpredictability arise from intricate ocean–atmosphere interactions and nonlinear feedback mechanisms, complicating their prediction of timing, intensity, and geographic impacts. This study applies Genetic Programming and Genetic Algorithms within the EASEA (EAsy Specification of Evolutionary Algorithms) Evolutionary Algorithms (EA) framework to develop a repository of symbolic equations for El Niño and La Niña events, spanning their various intensities. By analyzing data from the Oceanic Niño Index, this approach yields equation-based characterizations of ENSO events. This methodology not only enhances ENSO characterization strategies but also contributes to expanding the use of EAs in climate event analysis. The resulting equations have the potential to offer insights beyond academia, benefiting education, climate policy, and environmental management. This highlights the importance of ongoing refinement, validation, and exploration in these fields through EAs.},
  archive      = {J_GPEM},
  author       = {Abdulkarimova, Ulviya and Abarca-del-Rio, Rodrigo and Collet, Pierre},
  doi          = {10.1007/s10710-024-09497-z},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Harnessing evolutionary algorithms for enhanced characterization of ENSO events},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware real-time individualised blood glucose predictor
generator based on grammars and cartesian genetic programming.
<em>GPEM</em>, <em>26</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10710-024-09500-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel grammar-guided technique based on genetic programming for on-chip, real-time, configurable hardware design of model generators on an FPGA. The technique integrates grammar-based design, Cartesian Genetic Programming, and a (1+ $$\lambda$$ ) Evolutionary Strategy and is demonstrated through the implementation of a wearable hardware predictor for blood glucose prediction. People with diabetes need to manage their blood glucose levels to prevent life-threatening situations and long-term complications. Effective glucose management requires accurate blood glucose predictions, yet most existing methods rely on heuristic estimators. This system enables the training and testing of personalized models using real patient data. We validated the approach by generating and evaluating models for 30- and 60-min forecasting predictions on ten patients, creating a total of 200 models. The system achieved state-of-the-art results, with 98% and 90% of predictions falling within clinically acceptable regions according to Clarke error grid analysis, for 30- and 60-min horizons, respectively. Unlike software implementations, our technique does not suffer from hardware limitations and provides an efficient, adaptable solution through wearable hardware with minimal errors and low power consumption. This is the first demonstration of combining Cartesian Genetic Programming with a hardware implementation for grammar-based blood glucose prediction, potentially enabling real-time embedded systems for portable devices.},
  archive      = {J_GPEM},
  author       = {Cano, Jorge and Hidalgo, J. Ignacio and Garnica, Óscar},
  doi          = {10.1007/s10710-024-09500-7},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Hardware real-time individualised blood glucose predictor generator based on grammars and cartesian genetic programming},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of representations in grammar-guided genetic
programming in the context of glucose prediction in people with
diabetes. <em>GPEM</em>, <em>26</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10710-024-09502-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The representation of individuals in Genetic Programming (GP) has a large impact on the evolutionary process. In previous work, we investigated the evolutionary process of three Grammar-Guided GP (GGGP) methods, Context-Free Grammars GP (CFG-GP), Grammatical Evolution (GE) and Structured Grammatical Evolution (SGE), in the context of the complex, real-world problem of predicting the glucose level of people with diabetes two hours ahead of time. We concluded that representation choice is more impactful with a higher maximum depth, and that CFG-GP better explores the search space for deeper trees, achieving better results. Furthermore, we find that CFG-GP relies more on feature construction, whereas GE and SGE rely more on feature selection. Additionally, we altered the GGGP methods in two ways: using $$\epsilon$$ -lexicase selection, which solved the overfitting problem of CFG-GP and helps it to adapt to patients with high glucose variability; and with a penalization of complex trees, to create more interpretable trees. Combining $$\epsilon$$ -lexicase selection with CFG-GP performed best. In this work, we extend on the previous work and evaluated the impact of initialization methods in the quality of solutions. We found that they have no significant impact, even when the change of representation has.},
  archive      = {J_GPEM},
  author       = {Ingelse, Leon and Hidalgo, J. Ignacio and Colmenar, J. Manuel and Lourenço, Nuno and Fonseca, Alcides},
  doi          = {10.1007/s10710-024-09502-5},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {A comparison of representations in grammar-guided genetic programming in the context of glucose prediction in people with diabetes},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking GSGP: Still competitive 10 years later?
<em>GPEM</em>, <em>26</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10710-024-09504-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric Semantic Genetic Programming (GSGP) reimagined how to search for symbolic models using an evolutionary process. It addressed one of the main weaknesses of standard Genetic Programming (GP) by performing the search directly within the semantic space of a problem, which can define a convex and unimodal fitness landscape. Since the method does not require the syntactic evaluation of offspring, GSGP allowed more efficient and effective learning systems compared to standard GP. However, recent benchmarking results have suggested that GSGP is no longer a competitive approach, particularly in the symbolic regression domain, despite its previous success in several real-world tasks. Therefore, the research question of this work is an empirical one, stated as: Is GSGP still a competitive symbolic regression method 10 years after it was proposed? A comprehensive benchmark of black-box problems and comparisons with state-of-the-art methods were used to answer this question. In particular, a recently developed parallel version of GSGP is used, extending the implementation to also include the previously proposed optimal mutation step computation, as well as using the analytical quotient operator instead of a protected division. Results show that with simple, but important, extensions to the original GSGP algorithm, the answer to the research question is yes.},
  archive      = {J_GPEM},
  author       = {Muñoz Contreras, Jose Manuel and Trujillo, Leonardo and Hernandez, Daniel E. and Cardenas Florido, Luis A.},
  doi          = {10.1007/s10710-024-09504-3},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Benchmarking GSGP: Still competitive 10 years later?},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Review of PySR: High-performance symbolic regression in
python and julia. <em>GPEM</em>, <em>26</em>(1), 1–4. (<a
href="https://doi.org/10.1007/s10710-024-09503-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Tonda, Alberto},
  doi          = {10.1007/s10710-024-09503-4},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-4},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Review of PySR: High-performance symbolic regression in python and julia},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using FPGA devices to accelerate the evaluation phase of
tree-based genetic programming: An extended analysis. <em>GPEM</em>,
<em>26</em>(1), 1–48. (<a
href="https://doi.org/10.1007/s10710-024-09505-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes the potential of accelerating the evaluation phase of tree-based genetic programming through contemporary field-programmable gate array (FPGA) technology. This exploration stems from the fact that FPGAs can sometimes leverage increased levels of both data and function parallelism, as well as superior power/energy efficiency, when compared to general-purpose CPU/GPU systems. In this investigation, we introduce a fixed-depth, tree-based architecture that can fully parallelize tree evaluation for type-consistent primitives that are unrolled and pipelined. We show that our accelerator on a 14nm FPGA achieves an average speedup of 43 $$\times$$ when compared to a recent open-source GPU solution, TensorGP, implemented on 8nm process-node technology, and an average speedup of 4,902 $$\times$$ when compared to a popular baseline GP software tool, DEAP, running parallelized across all cores of a 2-socket, 28-core (56-thread), 14nm CPU server. Despite our single-FPGA accelerator being 2.4 $$\times$$ slower on average when compared to the recent state-of-the-art Operon tool executing on the same 2-processor, 28-core CPU system, we show that this single-FPGA system is 1.4 $$\times$$ better than Operon in terms of performance-per-watt. Importantly, we also describe six future extensions that could provide at least a 64–192 $$\times$$ speedup over our current design. Therefore, our initial results provide considerable motivation for the continued exploration of FPGA-based GP systems. Overall, any success in significantly improving runtime and energy efficiency could potentially enable novel research efforts through faster and/or less costly GP runs, similar to how GPUs unlocked the power of deep learning during the past fifteen years.},
  archive      = {J_GPEM},
  author       = {Crary, Christopher and Piard, Wesley and Stitt, Greg and Hicks, Benjamin and Bean, Caleb and Burlacu, Bogdan and Banzhaf, Wolfgang},
  doi          = {10.1007/s10710-024-09505-2},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-48},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Using FPGA devices to accelerate the evaluation phase of tree-based genetic programming: An extended analysis},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The science of soft robots, koichi suzumori, kenjiro fukuda,
ryuma niiyama, and kohei nakajima: ISBN 978-9811951732, springer 2023.
<em>GPEM</em>, <em>26</em>(1), 1–3. (<a
href="https://doi.org/10.1007/s10710-025-09508-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Medvet, Eric and Salvato, Erica},
  doi          = {10.1007/s10710-025-09508-7},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {The science of soft robots, koichi suzumori, kenjiro fukuda, ryuma niiyama, and kohei nakajima: ISBN 978-9811951732, springer 2023},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “Machine learning assisted evolutionary multi- and
many-objective optimization” by dhish kumar saxena, sukrit mittal,
kalyanmoy deb, and erik d. Goodman, ISBN 978-981-99-2095-2, springer,
2024. <em>GPEM</em>, <em>26</em>(1), 1–3. (<a
href="https://doi.org/10.1007/s10710-025-09509-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Selçuklu, Saltuk Buğra},
  doi          = {10.1007/s10710-025-09509-6},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {“Machine learning assisted evolutionary multi- and many-objective optimization” by dhish kumar saxena, sukrit mittal, kalyanmoy deb, and erik d. goodman, ISBN 978-981-99-2095-2, springer, 2024},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memetic semantic boosting for symbolic regression.
<em>GPEM</em>, <em>26</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10710-024-09506-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach called semantic boosting regression (SBR), leveraging the principles of boosting algorithms in symbolic regression using a Memetic Semantic GP for Symbolic Regression (MSGP) algorithm as weak learners. Memetic computation facilitates the integration of domain knowledge into a population-based approach, and semantic-based algorithms enhance local improvements to achieve targeted outputs. The fusion of memetic and semantic approaches allows us to augment the exploration and exploitation capabilities inherent in Genetic Programming (GP) and identify concise symbolic expressions that maintain interpretability without compromising the expressive power of symbolic regression. Our approach echoes the boosting algorithm’s characteristic, where weak learners (e.g., MSGP) are sequentially improved upon, focusing on correcting previous errors and continuously enhancing overall performance. This iterative strategy, intrinsic to boosting methods, is adeptly adapted to our SBR model. Experimental results demonstrate that our memetic-semantic approach has equal or better performance when compared to state-of-the-art evolutionary-based techniques when addressing real-world symbolic regression challenges. This advancement helps tackle the bloating issue in GP and significantly improves generalization capabilities. However, akin to classic boosting algorithms, one limitation of our approach is the increased computational cost due to the sequential training of boosting learners.},
  archive      = {J_GPEM},
  author       = {Leite, Alessandro and Schoenauer, Marc},
  doi          = {10.1007/s10710-024-09506-1},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Memetic semantic boosting for symbolic regression},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial introduction for the special issue on highlights
of genetic programming 2023 events. <em>GPEM</em>, <em>26</em>(1), 1–3.
(<a href="https://doi.org/10.1007/s10710-025-09507-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {Pappa, Gisele L. and Giacobini, Mario and Hu, Ting and Jakobović, Domagoj},
  doi          = {10.1007/s10710-025-09507-8},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Editorial introduction for the special issue on highlights of genetic programming 2023 events},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraining genetic symbolic regression via semantic
backpropagation. <em>GPEM</em>, <em>26</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s10710-025-09510-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary symbolic regression approaches are powerful tools that can approximate an explicit mapping between input features and observation for various problems. However, ensuring that explored expressions maintain consistency with domain-specific constraints remains a crucial challenge. While neural networks are able to employ additional information like conservation laws to achieve more appropriate and robust approximations, the potential remains unrealized within genetic algorithms. This disparity is rooted in the inherent discrete randomness of recombining and mutating to generate new mapping expressions, making it challenging to maintain and preserve inferred constraints or restrictions in the course of the exploration. To address this limitation, we propose an approach centered on semantic backpropagation incorporated into the Gene Expression Programming (GEP), which integrates domain-specific properties in a vector representation as corrective feedback during the evolutionary process. By creating backward rules akin to algorithmic differentiation and leveraging pre-computed subsolutions, the mechanism allows the enforcement of any constraint within an expression tree by determining the misalignment and propagating desired changes back. To illustrate the effectiveness of constraining GEP through semantic backpropagation, we take the constraint of physical dimension as an example. This framework is applied to discover physical equations from the Feynman lectures. Results have shown not only an increased likelihood of recovering the original equation but also notable robustness in the presence of noisy data.},
  archive      = {J_GPEM},
  author       = {Reissmann, Maximilian and Fang, Yuan and Ooi, Andrew S. H. and Sandberg, Richard D.},
  doi          = {10.1007/s10710-025-09510-z},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {Constraining genetic symbolic regression via semantic backpropagation},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSCID: Requirements selection considering interactions and
dependencies. <em>GPEM</em>, <em>26</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10710-025-09511-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Requirements selection is one of the essential aspects of requirement engineering. So far, a lot of work has been done in this field. But, it is difficult to choose the right set of software requirements, taking into account their interactions and dependencies and only a few researches have paid attention to interactions and dependencies between requirements. However, in this paper, an attempt has been made to provide a method by considering interactions and dependencies between requirements. To better manage these features, we have also improved the search-based methods used in this area. According to the proposed method called RSCID, before choosing the optimized subset of requirements, dependencies between requirements are extracted. In  the next step, an algorithm is proposed based on the NSGA-II method. In this algorithm, a hybrid fitness function is introduced in addition to two other functions that are used. To tradeoff between cost and value functions, user interactions are also deployed. Another algorithm is used in this paper to choose an appropriate requirements subset, the combination of the NSGA-II method and a genetic algorithm to obtain three fitness functions. The results of the proposed methods have been compared to other methods based on the evaluation criteria in this field. The experiments show the efficiency of the proposed methods to select efficient and useful requirements.},
  archive      = {J_GPEM},
  author       = {Keyvanpour, Mohammad Reza and Karimi Zandian, Zahra and Sodagari, Elham},
  doi          = {10.1007/s10710-025-09511-y},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {RSCID: Requirements selection considering interactions and dependencies},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of “symbolic regression” by gabriel kronberger,
bogdan burlacu, michael kommenda, stephan m. Winkler, and michael
affenzeller, ISBN 978-1-138-05481-3, 2024, CRC press. <em>GPEM</em>,
<em>26</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s10710-025-09513-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_GPEM},
  author       = {La Cava, William G},
  doi          = {10.1007/s10710-025-09513-w},
  journal      = {Genetic Programming and Evolvable Machines},
  month        = {6},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Genet. Program. Evol. Mach.},
  title        = {A review of “Symbolic regression” by gabriel kronberger, bogdan burlacu, michael kommenda, stephan m. winkler, and michael affenzeller, ISBN 978-1-138-05481-3, 2024, CRC press.},
  volume       = {26},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijcis---71">IJCIS - 71</h2>
<ul>
<li><details>
<summary>
(2025a). Image registration using the arithmetic optimization
algorithm for robotic visual servoing. <em>IJCIS</em>, <em>18</em>(1),
1–12. (<a href="https://doi.org/10.1007/s44196-024-00612-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual servoing using image registration is a method employed in robotics to control the movement of a system using visual information. In this context, we propose a new intensity-based image registration algorithm (IBIR) that uses information derived from images acquired at different times or from different views to determine the parameters of the geometric transformations needed to align these images. The Arithmetic Optimization Algorithm (AOA) is used to optimize these parameters, minimizing the difference between the images to be aligned. The proposed algorithm, Intensity-Based Image Registration via Arithmetic Optimisation Algorithm (IBIRAOA), is robust to image data fluctuations and perturbations and can avoid local optima. Simulation results prove the importance and efficiency of the proposed algorithm in terms of computation time and similarity of aligned images compared to other methods based on various metaheuristics. In addition, our results confirm a significant improvement in the trajectory of the wheeled mobile robot, thus reinforcing the overall effectiveness of our method in practical navigation and robotic control applications.},
  archive      = {J_IJCIS},
  author       = {Kmich, Mohamed and Harrade, Inssaf and Karmouni, Hicham and Sayyouri, Mhamed and Askar, S. S. and Abouhawwash, Mohamed},
  doi          = {10.1007/s44196-024-00612-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Image registration using the arithmetic optimization algorithm for robotic visual servoing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MORKO: A multi-objective runge–kutta optimizer for
multi-domain optimization problems. <em>IJCIS</em>, <em>18</em>(1),
1–34. (<a href="https://doi.org/10.1007/s44196-024-00714-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current landscape, there is a rapid increase in the creation of new algorithms designed for specialized problem scenarios. The performance of these algorithms in unfamiliar or practical settings often remains untested. This paper presents a new development, the multi-objective Runge–Kutta optimizer (MORKO), which is built upon the principles of elitist non-dominated sorting and crowding distance. The goal is to achieve superior efficiency, diversity, and robustness in solutions. MORKO effectiveness is further enhanced by incorporating various strategies that maintain a balance between diversity and execution efficiency. This approach not only directs the search toward optimal regions but also ensures that the process does not become stagnant. The efficiency of MORKO is compared against renowned algorithms like the multi-objective marine predicator algorithm (MOMPA), multi-objective gradient-based optimizer (MOGBO), multi-objective evolutionary algorithm based on decomposition (MOEA/D), and non-dominated sorting genetic algorithm (NSGA-II) on several test benchmarks such as ZDT, DTLZ, constraint (CONSTR, TNK, SRN, BNH, OSY and KITA) and real-world engineering design (brushless DC wheel motor, safety isolating transformer, helical spring, two-bar truss, welded beam, disk brake, tool spindle and cantilever beam) problems. We used unique, non-overlapping performance metrics for this comparison and suggested a fresh correlation analysis technique for exploration. The MORKO algorithm outcomes were rigorously tested and confirmed using the non-parametric statistical evaluations. The MORKO algorithm proves to excel in deriving comprehensive and varied solutions for many tests and practical challenges, owing to its multifaceted features. Looking ahead, MORKO has potential applications in complex engineering and management tasks.},
  archive      = {J_IJCIS},
  author       = {Kalita, Kanak and Jangir, Pradeep and Pandya, Sundaram B. and Alzahrani, Ahmed Ibrahim and Alblehai, Fahad and Abualigah, Laith and Ezugwu, Absalom E.},
  doi          = {10.1007/s44196-024-00714-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MORKO: A multi-objective Runge–Kutta optimizer for multi-domain optimization problems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective particle swarm optimization with integrated
fireworks algorithm and size double archiving. <em>IJCIS</em>,
<em>18</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s44196-024-00722-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective particle swarm optimization (MOPSO) is an optimization technique that mimics the foraging behavior of birds to solve difficult optimization problems. MOPSO is well known for its strong global search capability, which efficiently locates solutions that are close to the global optimum across a wide search domain. However, similar to many other optimization algorithms, the fast convergence property of MOPSO can occasionally lead to the population entering the local optimum too soon, obstructing researchers from investigating more efficient solutions. To address this challenge, the study proposes a novel framework that integrates the fireworks algorithm (FA) into MOPSO and establishes a size-double archiving mechanism to maintain population diversity. By preventing population homogenization, this mechanism promotes the retention of better solutions. Additionally, by fusing evolutionary data analysis with particle information, the study offers new individual optimal choices and adaptive parameter tuning to improve the algorithm’s robustness and adaptability and better manage the complexity of multi-objective optimization problems (MOPs). The suggested algorithm is compared with several existing MOPSOs and multi-objective evolutionary algorithms (MOEAs) in simulation experiments. Standard test problems like ZDT, UF, and DTLZ are used in the experiments. The new algorithm performs exceptionally well in terms of improving convergence and population diversity, as well as demonstrating significant competitiveness for solving MOPs.},
  archive      = {J_IJCIS},
  author       = {Zhang, Yansong and Liu, Yanmin and Zhang, Xiaoyan and Song, Qian and Ouyang, Aijia and Yang, Jie},
  doi          = {10.1007/s44196-024-00722-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multi-objective particle swarm optimization with integrated fireworks algorithm and size double archiving},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stock market prediction based multi-attribute decision
making model using picture fuzzy <span
class="math display"><em>Ẑ</em></span> -information. <em>IJCIS</em>,
<em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s44196-024-00664-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As demonstrated in the section above, the stock market place is a dynamic factor, which makes it possible for traders and investors to make good decisions based on the information acquired through accurate prediction. This research aims at improving the prediction of stock market by applying a new method to Multi-attribute Group Decision making (MAGDM). MAGDM goes through a cycle of evaluating and ranking several criteria hence enhancing the decision-making aspects further. To overcome the shortcomings of prior models, some EU and FU is incorporated by combining Zadeh’s $${\hat{Z}}$$ -numbers with Picture Fuzzy Sets (PFSs). This integration is to enhance the ability of the model to address completely unclear decisions utilizing the peculiarities of $${\hat{Z}}$$ -numbers. To compare decisions between decision-makers, we proposed picture fuzzy $${\hat{Z}}$$ -numbers (PF $${\hat{Z}}$$ N) and for their aggregation, introduced picture fuzzy weighted averaging, picture fuzzy ordered weighted averaging, picture fuzzy hybrid averaging, picture fuzzy weighted geometric, picture fuzzy ordered weighted geometric and picture fuzzy hybrid geometric operators based algebraic $${\mathfrak {T}}$$ -norm ( $${\mathfrak {T}}-N$$ ) and $${\mathfrak {T}}$$ -conorm ( $${\mathfrak {T}}-CNs$$ ) To verify the efficiency of our suggested technique, we compare these operators with the Combined Compromised Solution (CoCoSo) model focusing on the stock market analysis. Our results, therefore, show how these operators are important in improving decision making accuracy and precision in conditions of risk. This research laid down the basis for enhancing decision-making and dealing with uncertainty in different fields especially in the application of stock market prediction. The proposed methodology can be attributed to providing a systematic and a more efficient way of dealing with uncertainty which in one way or the other has an outcome of enhancing the credibility of the decision making process in the financial sector.},
  archive      = {J_IJCIS},
  author       = {Ashraf, Shahzaib and Khalid, Amna and Batool, Bushra and Tlija, Mehdi and Jana, Chiranjibe and Pamucar, Dragan},
  doi          = {10.1007/s44196-024-00664-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Stock market prediction based multi-attribute decision making model using picture fuzzy $${\hat{Z}}$$ -information},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid dynamic harris hawks optimized gated
recurrent unit approach for breast cancer prediction. <em>IJCIS</em>,
<em>18</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s44196-024-00712-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The breast cancer (BC) prediction is improved through the machine learning (ML) techniques. In this study, we develop an innovative forecasting framework called the Dynamic Harris Hawks Optimized Gated Recurrent Unit (DHH-GRU) for the prediction of BC. It combines the Gated Recurrent Unit (GRU) and Harris Hawks Optimization (HHO) methods. We gathered data and a training set that included the Wisconsin diagnostic BC (WDBC) dataset, which contains 569 patients with malignant and beginning cases. The collected data were pre-processed using min–max normalization, and important features were extracted by Fast Fourier transform (FFT) and the process of reducing the dimensionality with principal component analysis (PCA). Decimal scaling is employed to equalize the various feature effects. The proposed DHH-GRU technique incorporated the GRU for capturing sequential connections on temporal medical information, and the optimization process, DHH optimization, is utilized. The proposed method&#39;s effectiveness is compared and estimated with various existing techniques in terms of log-loss (0.06%), accuracy (98.05%), precision (98.09%), F1-score (98.28%), and recall (98.15%). The proposed DHH-GRU method has a more predictive ability with the sequential dependency in capturing GRU and DHH optimization’s combined behaviour of hunting. This method significantly improved the accuracy of BC prediction.},
  archive      = {J_IJCIS},
  author       = {Natarajan, Rajesh and Krishna, Sujatha and Gururaj, H. L. and Flammini, Francesco and Alfurhood, Badria Sulaiman and Kumar, C. M. Naveen},
  doi          = {10.1007/s44196-024-00712-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid dynamic harris hawks optimized gated recurrent unit approach for breast cancer prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive ensemble learning model-based binary white shark
optimizer for software defect classification. <em>IJCIS</em>,
<em>18</em>(1), 1–51. (<a
href="https://doi.org/10.1007/s44196-024-00716-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software dominates modern enterprises, affecting numerous functions. Software firms constantly experiment with new methodologies to define and assess software quality to stay competitive and ensure excellence. Software engineering uses fundamentals and cutting-edge technology to develop great software. In recent decades, Data-mining techniques and machine learning for classifying problematic software projects have emerged to improve software quality. ML approaches, especially ensemble learning models, are becoming fundamental to software engineers’ daily jobs. This work created a binary white shark optimizer (WSO) to optimize standard ensemble learning models. The objective is to identify the most suitable ensemble number for weak learners to maximize accuracy on benchmark datasets. The EM model uses 14 weak learners. Twenty-one experimental runs are performed on 15 software-defective module datasets. The optimized ensemble model outperforms the standard Ensemble learning model in AUC-ROC, Accuracy, Precision, Recall, F1-Score, and Specificity. The enhanced model has an average accuracy of 86%, compared to 76% for the standard ensemble model across all datasets. The optimized model outperformed the conventional ensemble for the same datasets, with an average AUC of 72% compared to 61% for the standard ensemble. The optimized model was more stable than the standard model, with an STD of 5.53E−03 vs 7.24E−02 for the ensemble model. The WSO optimization process strengthens and generalizes optimizeels. The study suggests that evolutionary metaheuristic approaches can enhance EM models’ accuracy, trustworthiness, and adaptability.},
  archive      = {J_IJCIS},
  author       = {Saraireh, Jameel and Agoyi, Mary and Kassaymeh, Sofian},
  doi          = {10.1007/s44196-024-00716-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-51},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Adaptive ensemble learning model-based binary white shark optimizer for software defect classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Properties and applications of neutrosophic burr XII
distribution. <em>IJCIS</em>, <em>18</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s44196-024-00721-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Primarily, when the hazards function has intricate structures, the BrXII distribution is an established framework for lifetime data analysis. However, classical probability models are limited in the sense that they cannot measure or record the data in exactness fueling the notion of indeterminacy in data collection process. This study addresses this issue by proposing the idea of neutrosophic BrXII (NeS-BrXII) distribution. The primary objective is to study the statistical properties of the proposed model by providing explicit expressions of reliability properties, the expression of moments and generating function, expression of order statistics, mean residual life, mean inactivity time, stochastic ordering, income inequality measures, and entropy in neutrosophic realm. In addition, the neutrosophic model parameters are estimated using the principle of maximum-likelihood estimation. Further, the precision of these model estimates is verified via a simulation study of the proposed model. Applying the model on two real-world material sciences data sets reinforces its efficacy with the NeS-BrXII distribution proving to be more suitable for managing anomalies in neutrosophic surface analysis among other models.},
  archive      = {J_IJCIS},
  author       = {Al-Essa, Laila A. and Jamal, Farrukh and Shafiq, Shakaiba and Khan, Sadaf and Abbas, Qamer and Khan Sherwani, Rehan Ahmad and Aslam, Muhammad},
  doi          = {10.1007/s44196-024-00721-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Properties and applications of neutrosophic burr XII distribution},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). A two-way crossed effects fuzzy panel linear regression
model. <em>IJCIS</em>, <em>18</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s44196-024-00723-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last two decades, the panel data model has become a focus of applied research. While there are numerous proposals for soft regression models in the literature, only a few linear regression models have been proposed based on fuzzy panel data. However, these models have serious limitations. This study is an attempt to propose a kind of two-way fuzzy panel regression model with crossed effects, fuzzy responses and crisp predictors to overcome the shortcomings of these models in real applications. The corresponding parameter estimation is provided based on a three-step procedure. For this purpose, the conventional least absolute error technique is employed. Two real data sets are analyzed to investigate the fitting and predictive capabilities of the proposed fuzzy panel regression model. These real data applications demonstrate that our proposed model has good fitting accuracy and predictive performance.},
  archive      = {J_IJCIS},
  author       = {Hesamian, Gholamreza and Johannssen, Arne},
  doi          = {10.1007/s44196-024-00723-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A two-way crossed effects fuzzy panel linear regression model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: A model for estimating resiliency of AI-based
classifiers defending against cyber attacks. <em>IJCIS</em>,
<em>18</em>(1), 1. (<a
href="https://doi.org/10.1007/s44196-024-00725-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Barik, Kousik and Misra, Sanjay and Fernandez-Sanz, Luis},
  doi          = {10.1007/s44196-024-00725-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: A model for estimating resiliency of AI-based classifiers defending against cyber attacks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel conflict deduction algorithm based on contradiction
separation inference rule. <em>IJCIS</em>, <em>18</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s44196-024-00726-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated reasoning, a significant field within artificial intelligence, has attracted increased attention in recent years due to the rising demand for trustworthy AI. Binary resolution, among other inference rules, is crucial in automated reasoning of first-order logic, including the new conflict resolution method. Conflict resolution processes only two clauses in each deduction step and eliminates a complementary pairs of literals from input clauses. This paper proposes a contradiction separation conflict deduction (CSCD) method based on the contradiction separation rule to address these limitations. This novel resolution methodology, together with its automated reasoning theory and method, handles several clauses in each deduction step to seek for conflicts and generates learnt clauses through synergized deduction. Thus, the approach improves deduction by detecting conflicts more effectively, especially with lengthier input clauses. CSCD and conflict resolution are analyzed in detail, then how to create a practical CSCD algorithm and its implementation is summarized. We tested the CSCD algorithm to solve the CASC-26 problems and also applied it to the current leading ATP system (Eprover). Experimental results show that the CSCD deduction approach improves reasoning capability of conflict deduction method. Additionally, the Eprover with the proposed CSCD algorithm improves its performance and has solved various problems with a rating of 1 from the benchmark database TPTP.},
  archive      = {J_IJCIS},
  author       = {Guo, Hailin and Cao, Feng and Yi, Jianbing and Wu, Guanfeng and Li, Weicai},
  doi          = {10.1007/s44196-024-00726-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel conflict deduction algorithm based on contradiction separation inference rule},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing FACTS device placement using the fata morgana
algorithm: A cost and power loss minimization approach in uncertain load
scenario-based systems. <em>IJCIS</em>, <em>18</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s44196-024-00727-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, reliable power delivery and increasing demand are important issues in modern power systems. Flexible Alternating Current Transmission Systems (FACTS) devices are used to control transmission line parameters to increase power transfer and stability. Nevertheless, the problem of determining the optimal placement and sizing of these devices is still challenging, as the placement and sizing of the devices affects generation costs, power losses, voltage stability, and system reliability. This study proposes the Fata Morgana Algorithm (FATA), an optimization algorithm inspired by the natural process of mirage formation to optimize placement and sizing of FACTS devices in an IEEE 30 bus system with wind turbine integration. The FATA algorithm is evaluated against recently developed and improved optimization techniques, such as rime-ice formation phenomenon based Improved RIME (IRIME) Algorithm, Newton–Raphson-Based Optimization (NRBO), Resistance Capacitance Algorithm (RCA), Krill Optimization Algorithm (KOA), and Grey Wolf Optimizer (GWO), across multiple optimization objectives: reduction in generation cost, reduction in power loss and combined generation cost plus power loss, termed as Gross cost function. Results obtained show that FATA consistently outperforms the other algorithms in terms of convergence and solution quality, offering a robust approach to solving single objective optimization problems. FATA theoretically provides a good balance between exploration and exploitation, and produces better global solutions. It practically increases power system efficiency by lowering operational costs and losses and improving stability. Results indicate that the FATA algorithm produced minimum generation cost of 807.0405 $/h, which is 0.088–0.426% less than the competing algorithms. It also reduced power losses to 5.5917 MW, which is 1.095–6.781% less than other methods. For gross cost minimization, the FATA algorithm achieved a minimum gross cost of 1366.3727 $/h, which is 0.4799% better than the next best algorithm and 3.2261% better than the worst. The results also show that FATA is robust in solving complex optimization problems in power systems, and it provides significant improvements in run time and convergence efficiency. The main advantage for readers is that FATA provides a reliable and efficient way to optimize power systems. Future work could also investigate the application of FATA in real time, as well as in larger power networks with more renewable energy sources.},
  archive      = {J_IJCIS},
  author       = {Aljaidi, Mohammad and Jangir, Pradeep and Agrawal, Sunilkumar P. and Pandya, Sundaram B. and Parmar, Anil and Alkoradees, Ali Fayez and Arpita and Smerat, Aseel},
  doi          = {10.1007/s44196-024-00727-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimizing FACTS device placement using the fata morgana algorithm: A cost and power loss minimization approach in uncertain load scenario-based systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time facial expression recognition based on image
processing in virtual reality. <em>IJCIS</em>, <em>18</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s44196-024-00729-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More virtual reality (VR) scenarios have become more prevalent in recent years. More and more people are getting into VR, meaning that objective physiological measures to assess a user&#39;s emotional state automatically are becoming more critical. Individuals’ emotional states impact their behaviour, opinions, emotions, and decisions. They may be used to analyze VR experiences and make systems react to and engage with the user’s emotions. VR environments require users to wear head-mounted displays (HMDs), blocking off their upper faces. That makes traditional Facial Expression Recognition (FER) approaches very limited in their usefulness. Thus, a Deep Learning (DL) solution combined with image processing is utilized to classify universal emotions: sadness, happiness, disgust, anger, fear and surprise. Hence, this paper suggests the Deep Automatic Facial Expression Recognition Model (DAFERM) for interactive virtual reality (VR) applications such as intelligent education, social networks, and virtual training. Two main parts comprise the system: one that uses deep neural networks (DNNs) for facial emotion identification and another that automatically tracks and segments faces. The system begins by following a marker on the front of the head-mounted display (HMD). With the help of the spatial data that has been retrieved, the positions and rotations of the face are estimated to segment the mouth. Finally, the system interacts with DNN using the pixels processed by the lips. It obtains the facial expression results in real time using an adaptive method for histogram-based mouth segmentation.},
  archive      = {J_IJCIS},
  author       = {Gong, Qingzhen and Liu, Xuefang and Ma, Yongqiang},
  doi          = {10.1007/s44196-024-00729-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Real-time facial expression recognition based on image processing in virtual reality},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized novel text embedding approach for fake news
detection on twitter x: Integrating social context, temporal dynamics,
and enhanced interpretability. <em>IJCIS</em>, <em>18</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s44196-024-00730-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of widespread misinformation, detecting fake news has become a crucial challenge, particularly on social media platforms. This paper introduces an optimized approach for Fake News Detection, combining BERT and GloVe embeddings with Principal Component Analysis (PCA) and attention mechanisms, enriched by social and temporal features for more effective text representation. Leveraging the CIC Truth Seeker Dataset 2023, we applied SHAP for feature selection and interpretability, ensuring transparency in the model’s predictions. Our methodology achieved a remarkable accuracy of 99.9% using a Random Forest classifier, showcasing the efficacy of this optimized hybrid approach. The integration of interpretability techniques such as LIME and SHAP provides deeper insights into the model’s decisions, making it a reliable tool for combating misinformation. This novel approach offers a robust and transparent solution to the growing threat of fake news, contributing significantly to the integrity of online information and public discourse on platforms like Twitter X.},
  archive      = {J_IJCIS},
  author       = {AlJamal, Mahmoud and Alquran, Rabee and Alsarhan, Ayoub and Aljaidi, Mohammad and Al-Jamal, Wafa’ Q. and Alkoradees, Ali Fayez},
  doi          = {10.1007/s44196-024-00730-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimized novel text embedding approach for fake news detection on twitter x: Integrating social context, temporal dynamics, and enhanced interpretability},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-driven optimized chaotic encryption scheme for
medical image transmission in IoT-edge environment. <em>IJCIS</em>,
<em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s44196-024-00731-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) is adopted in a wide spectrum of applications in which a vast amount of data are produced and distributed to centralized cloud platforms to deliver various services. It involves smart devices that collect thousands of terabytes of heterogeneous data and deployed this to make instant decision that aids for the better performance and most comfort life. Traditional IoT architecture is heavily centralized, where it stores the most sensitive information that creates the multiple threats and security breaches as the attackers target towards these centralized cloud systems. To improve the security chain in IoT environment, edge computing (EC) was introduced to distribute the applications of IoT at the edge of the communication networks. However, these edge-based IoT are also vulnerable to many threats due to their decentralized and in secured management. Block chain (BC) technology offers a most trusted solution to resolve the security issues in the IoT-Edge computing environment. This research study presents the block chain driven medical image encryption technique using modified honey badger optimization with the ensemble chaotic systems. The proposed block chain framework uses the divergent methods that integrates differential scroll, Hénon chaotic maps and modified honey badger optimization to generate the optimum keys and high secured image data. These high secured data are stored in the block chain, ensuring the image security to be stored in edge nodes. The complete framework was experimented using Ethereum using Ganache API and Python3.19 are utilized as the major programs for designing the varied interfaces of the recommended model. The comprehensive experimentation is undertaken to assess the security strength of the recommended encryption scheme. The evaluation metrics like as NACI, UACI, Entropy and standard verification methods such as NIST standard tests are deployed and analyzed. To prove it security strength, proposed secured BC framework is compared with the wide-variety of secured frameworks. The experimental findings reveal that the suggested framework establishes a more robust and secure environment for image exchange, surpassing the performance of other blockchain-based systems in terms of integrity, robustness and security. Finally, the paper spreads the bright light of advantages in deploying the proposed framework to formulate the most secured environment in the IoT-Edge environment for medical image transmission.},
  archive      = {J_IJCIS},
  author       = {Archana, Goli and Goyal, Rajeev and Kumar, K. M. V. Madan},
  doi          = {10.1007/s44196-024-00731-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Blockchain-driven optimized chaotic encryption scheme for medical image transmission in IoT-edge environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing stock portfolio selection with trapezoidal bipolar
fuzzy VIKOR technique with boruta-GA hybrid optimization model: A
multicriteria decision-making approach. <em>IJCIS</em>, <em>18</em>(1),
1–30. (<a href="https://doi.org/10.1007/s44196-025-00733-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The investors’ main objective is to minimize the risks and to get the maximum returns in the random stock market requires them to choose the correct mix of the stocks. The traditional portfolio selection methods often struggle with market volatility, leading to less-than-the targeted profits. This research attempts to apply trapezoidal bipolar fuzzy Vise Kriterijumska Optimizacija I Kompromisno Resenje (VIKOR) method to help in decision-making. It also combines the fuzzy set theory with the VIKOR method, the trapezoidal bipolar fuzzy with VIKOR (TrBFV) approach offers a comprehensive and flexible system for evaluating investment options. The proposed approach has been validated through real-world illustrations. The analysis has been made with the VIKOR method and its integration with trapezoidal bipolar fuzzy sets. Financial decision-making can be very hard for investors who have access to big data due to the overwhelming amount of information they are required to interpret. The Boruta-GA approach combines the advantages of the Genetic Algorithm (GA) and Boruta Optimization Algorithm (BOA) methods, implementing the comprehensive feature selection capability of Boruta to detect all relevant features and harnessing the strength of GA to bring about improvement within a wide range of datasets. The result shows that this novel approach is effective and can help to investors to take decisions in the unpredictable financial market. This study is an attempt to provides investors with an appropriate approach to navigate the challenges of stock market investment. Abbreviations serve as a nomenclature table, detailing all acronyms.},
  archive      = {J_IJCIS},
  author       = {Sharma, Sunil Kumar},
  doi          = {10.1007/s44196-025-00733-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhancing stock portfolio selection with trapezoidal bipolar fuzzy VIKOR technique with boruta-GA hybrid optimization model: A multicriteria decision-making approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Publisher correction: Image registration using the
arithmetic optimization algorithm for robotic visual servoing.
<em>IJCIS</em>, <em>18</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s44196-025-00735-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Kmich, Mohamed and Harrade, Inssaf and Karmouni, Hicham and Sayyouri, Mhamed and Askar, S. S. and Abouhawwash, Mohamed},
  doi          = {10.1007/s44196-025-00735-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Publisher correction: Image registration using the arithmetic optimization algorithm for robotic visual servoing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal sizing and techno-economic analysis of combined
solar wind power system, fuel cell and tidal turbines using
meta-heuristic algorithms: A case study of lavan island. <em>IJCIS</em>,
<em>18</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s44196-025-00737-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combined renewable energy sources (RESs) are emerging as a competitive alternative to conventional energy production facilities due to their sustainability and zero-emission characteristics. However, determining the optimal system size is complicated by two major challenges: the cost of energy (COE) and the intermittent nature of RESs. This study introduces a novel mathematical approach to optimize the sizing of photovoltaic (PV), wind, hydrogen, battery, and fuel cell systems with electrolyzers, specifically tailored for the remote area of Lavan Island. The proposed method aims to deliver electricity without reliance on the traditional electricity distribution grid, while offering a scalable solution applicable to other geographical regions. The primary objective is to achieve cost-effective electricity generation while ensuring a reliable energy supply through the evaluation of system reliability indices. A fuzzy logic system is employed to minimize the costs of a hybrid system incorporating hydroelectric, wind, solar, and battery technologies, while simultaneously calculating two key reliability metrics: the Loss of Power Supply Probability (LPSP) and the Dump Energy Probability (DEP). To optimize the objective function, this study applies three advanced algorithms: the Shuffled Frog Leaping Algorithm (SFLA), the Grasshopper Optimization Algorithm (GOA), and the Honey Badger Algorithm (HBA). These algorithms are used to determine the global optimum, with comparative analyses conducted to highlight the performance of the proposed approach. The results are evaluated based on statistical metrics, including consistency, execution time, convergence speed, and the minimization of the objective function. The findings demonstrate the superiority and the reliability of the proposed method over alternative approaches, paving the way for cost-efficient and sustainable energy solutions in isolated regions.},
  archive      = {J_IJCIS},
  author       = {Talebi, Hessameddin and Nikoukar, Javad and Gandomkar, Majid},
  doi          = {10.1007/s44196-025-00737-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimal sizing and techno-economic analysis of combined solar wind power system, fuel cell and tidal turbines using meta-heuristic algorithms: A case study of lavan island},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic graph convolutional network relation extraction
model combining dependency syntax and contrastive learning.
<em>IJCIS</em>, <em>18</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s44196-025-00738-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current relation extraction tasks, when the input sentence structure is complex, the performance of in-context learning methods based on large language model is still lower than that of traditional pre-train fine-tune models. For complex sentence structures, dependency syntax information can provide effective prior text structure information for relation extraction. However, most studies are affected by the noise in the syntactic information automatically extracted by natural language processing toolkits. Additionally, traditional pre-training encoders have issues such as an overly centralized representation of word embedding for high-frequency words, which adversely affects the model to learn contextual semantic information. To address proposed problem, the paper proposes a Hyperbolic Graph Convolutional Network Relation Extraction Model Combine Dependency Syntax and Contrastive Learning. Based on the hyperbolic graph neural network, dependent syntactic information and information optimization strategies are introduced to solve the problem of word embedding concentration. Simultaneously, to mitigate the impact of noise in dependency syntax information on the relation extraction task, a contrastive learning approach is employed. After the model learns context semantics separately in the original dependency syntax information and dependency syntax information with added random noise, it maximizes the mutual information between entity words to assist the model in distinguishing noise in dependency syntax. The experiments indicate that the proposed model in this paper can effectively enhance the performance of relation extraction on public datasets, especially achieving significantly higher precision on datasets with complex sentence structures compared to in-context learning.},
  archive      = {J_IJCIS},
  author       = {Li, Jinzhe},
  doi          = {10.1007/s44196-025-00738-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hyperbolic graph convolutional network relation extraction model combining dependency syntax and contrastive learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced LSTM approach for detecting IoT-based DDoS
attacks using honeypot data. <em>IJCIS</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00741-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the widening perils in network security is the Distributed Denial of Service (DDoS) attacks on the Internet of Things (IoT) ecosystem. This paper presents an enhanced Intrusion Detection System (IDS) through the proposal of an enhanced version of the long short-term memory (LSTM) model to detect DDoS attacks using honeypot-generated data. The proposed model aggregates the Conv1D, Bidirectional Long Short-Term Memory (Bi-LSTM), Bidirectional Gated Recurrent Unit (Bi-GRU), and dropout layers to extract temporal and spatial features from IoT traffic effectively. We tested the efficacy of the proposed system on a real-world IoT-DH dataset, which showed a remarkable accuracy of 99.41%, with an AUC score of 0.9999. A comparative analysis with other baseline models, such as LSTM, Bidirectional LSTM (Bi-LSTM), Gated Recurrent Unit (GRU), Recurrent Neural Network (RNN), Feedforward Neural Network (FNN), and Temporal Convolutional Network (TCN), proved that enhanced LSTM outperformed the other models. This indicates the robustness of the proposed model in correctly detecting DDoS attacks with high generalization capability for unseen traffic data. The contribution of this paper will be an addition to the deep learning techniques applied for the solution of intrusion detection systems (IDS), which will also allow the building and implementation of more efficient security mechanisms in IoT environments.},
  archive      = {J_IJCIS},
  author       = {Arnob, Arjun Kumar Bose and Mridha, M. F. and Safran, Mejdl and Amiruzzaman, Md and Islam, Md. Rajibul},
  doi          = {10.1007/s44196-025-00741-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An enhanced LSTM approach for detecting IoT-based DDoS attacks using honeypot data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). ExAIRFC-GSDC: An advanced machine learning-based
interpretable framework for accurate gas leakage detection and
classification. <em>IJCIS</em>, <em>18</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s44196-025-00742-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gas leakage detection is imperative in various sectors, including chemical industries, coal mines, and household applications. The escalating number of accidents in coal mines, chemical industries, and homes underscores the urgency of swift and accurate gas detection methods. This research focuses on developing advanced systems that promptly identify gas types to prevent harm to human lives and the environment. This paper addresses the challenges of gas leakage detection and classification in diverse environments, such as industrial, residential, and mining scenarios. The proposed ExAIRFC-GSDC model integrates machine learning algorithms, particularly a Random Forest Classifier, with explainable artificial intelligence (XAI) techniques to enhance interpretability. This study employs a dataset comprising gas sensor measurements that encompassing gasses, such as Liquid Petroleum Gas (LPG), Compressed Natural Gas (CNG), Methane, Propane, and others. Various machine learning classifiers, including K-Nearest Neighbors, Decision Tree, Support Vector Machines, XGBoost, and others, are compared with ExAIRFC-GSDC for gas detection. The model demonstrates superior performance, achieving an accuracy rate of 98.67%. Incorporating SHAP and LIME explanations enhances the model&#39;s interpretability, providing insights into the contributions of individual sensors. Statistical analysis confirms the significant differences in sensor readings across different gas types. ExAIRFC-GSDC is a robust and explainable solution for accurate gas detection and classification in complex environments.},
  archive      = {J_IJCIS},
  author       = {Lalithadevi, B. and Krishnaveni, S.},
  doi          = {10.1007/s44196-025-00742-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {ExAIRFC-GSDC: An advanced machine learning-based interpretable framework for accurate gas leakage detection and classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERT-BiGRU-senti-GCN: An advanced NLP framework for
analyzing customer sentiments in e-commerce. <em>IJCIS</em>,
<em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s44196-025-00747-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis plays an important role in understanding employee feedback and improving workplace culture. By leveraging NLP techniques to analyze this feedback accurately, organizations can pinpoint specific areas that need improvement, address employee concerns, and foster a positive work environment. These NLP-driven deep learning models offer valuable tools for E-Commerce HR and sales departments, enabling monitoring employee and users’ sentiment trends over time and assisting in implementing targeted interventions. Focusing on the e-commerce industry, this work utilizes NLP-driven deep learning methodologies to analyze employee and user feedback, aiming to identify sentiments. The proposed NLP-driven, deep learning-based framework is designed to classify user feedback into positive, negative, or neutral sentiments. The key steps in this framework include data collection, NLP-enhanced feature extraction using BERT-BiGRU, and final classification using a Graph Neural Network-based finite-state automata. The effectiveness of this NLP-centric approach was tested on diverse datasets of customer feedback from the e-commerce industry. The results demonstrate the framework’s efficacy, achieving an impressive 93.35% accuracy rate, surpassing existing benchmark methods. The research significantly benefits e-commerce by refining product portfolios and enhancing workplace culture.},
  archive      = {J_IJCIS},
  author       = {Rana, Muhammad Rizwan Rashid and Nawaz, Asif and Rehman, Saif Ur and Abid, Muhammad Ali and Garayevi, Mubariz and Kajanová, Jana},
  doi          = {10.1007/s44196-025-00747-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {BERT-BiGRU-senti-GCN: An advanced NLP framework for analyzing customer sentiments in E-commerce},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of hybrid intrusion detection system leveraging
ensemble stacked feature selectors and learning classifiers to mitigate
the DoS attacks. <em>IJCIS</em>, <em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s44196-025-00750-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denial of service (DoS) attacks occur more frequently with the progressive development of the Internet of things (IoT) and other Internet-based communication technologies. Since these technologies are deeply rooted in the individual’s comfort life, protecting the user’s privacy and security against the growing DoS attack has become a major challenge among researchers. In recent times, intrusion detection systems (IDS) have developed a vital part in ensuring security against these growing attacks. IDS is still unable to attain the optimum categorization performance due to a few bottlenecks. The speed and performance of the existing IDS are challenged by the intricacy of high-dimensional data and the efficacy of the conventional base classifiers. To tackle this aforementioned problem, this research article presents the hybrid IDS based on the combination of stacked feature selection methods such as Random Boruta Selector (RFS), Relief, Pearson coefficient (PCE) and Stacked learning classifiers (SLF). To reduce the dimension of the data features and to select the optimal feature sets, novel integration of RFS, Relief, PCE are deployed. As the final step, stacked classifiers are used for the classification of DoS attacks. All the trials in this framework were accompanied utilizing CICDDoS-2019 datasets and contrasted with the other similar models. The validation boundaries such as accuracy, precision, recall, specificity, and F1-score are used to evaluate the proposed framework. With an F1-score of 96%, accuracy of 96.5%, precision of 96.0%, and recall of 95.8%, the suggested model obtained a CICDDoS-2019 score of 96%. Compared with the other traditional classifiers, the suggested framework has produced the best classification performance in detecting the DoS attacks.},
  archive      = {J_IJCIS},
  author       = {Mamatha, P. and Balaji, S. and Anuraghav, S. Sai},
  doi          = {10.1007/s44196-025-00750-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Development of hybrid intrusion detection system leveraging ensemble stacked feature selectors and learning classifiers to mitigate the DoS attacks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessments of student’s adaptability using convoluted
geyser bidirectional long short-term memory in online education.
<em>IJCIS</em>, <em>18</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s44196-024-00724-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid transition from traditional in-person education to online classrooms has highlighted the need to effectively assess student engagement in virtual learning environments supported by learning management systems. Despite this shift, there remains a significant gap in predictive models that generalize across diverse blended courses, disciplines, and student demographics. Addressing this gap is essential for improving the accuracy and efficiency of predicting student adaptability in online education. To address these issues, this research introduces a novel student adaptability learning model named Convoluted Geyser Bidirectional Long Short-Term Memory (CGBiLSTM) model, designed to predict student adaptability in online entrepreneurship education. This technique has its ability to capture complicated patterns and dependencies in sequential data, which is critical for accurately assessing student adaptability. Furthermore, incorporating the Geyser Optimization Algorithm (GOA) into CGBiLSTM improves the performance by optimizing the learning process and training capabilities, resulting in more accurate and dependable predictions. The CGBiLSTM technique achieves an accuracy of 98.94%, a precision of 99.03%, a recall of 98.71%, and an F1-score of 98.15% proving its efficacy in assessing student adaptability. The CGBiLSTM model, enhanced by the GOA, provides a highly accurate and reliable solution for predicting student adaptability in online education, making it a vital tool for educators in the evolving virtual learning landscape.},
  archive      = {J_IJCIS},
  author       = {Baskar, B. S. Vijaya and Kesavan, Ramesh},
  doi          = {10.1007/s44196-024-00724-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Assessments of student’s adaptability using convoluted geyser bidirectional long short-term memory in online education},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coverage-based variable precision (i, PSO)-fuzzy rough sets
with applications to emergency decision-making. <em>IJCIS</em>,
<em>18</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s44196-024-00728-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the characteristics of imprecise, incomplete and fuzzy data in emergency environment, a novel emergency decision-making method based on coverage-based variable precision (I, PSO)-fuzzy rough set model is proposed. First, an improved (I, PSO)-fuzzy rough set model is proposed, which combines the covering-based fuzzy rough set (CFRS) and the variable precision fuzzy rough set (VPFRS). Second, inspired by the idea of attribute reduction, a novel method for determining attribute weights is introduced to optimize weight assignment in emergency decision-making. Last but not least, to illustrate the feasibility and effectiveness of the proposed method, an example of post-flood rescue force allocation in urban areas is demonstrated. Finally, the stability and superiority of the method are verified through sensitivity analysis and comparative evaluation.},
  archive      = {J_IJCIS},
  author       = {Yin, Ran and Chen, Minge and Wu, Jian and Liu, Yu},
  doi          = {10.1007/s44196-024-00728-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Coverage-based variable precision (I, PSO)-fuzzy rough sets with applications to emergency decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time athlete fatigue monitoring using fuzzy decision
support systems. <em>IJCIS</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00732-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports scientists worry about fatigue, because it affects performance, increases injury risk, and harms health. Traditional fatigue measurements may miss this complicated and ever-changing state, placing athletes at risk of injury while training. This study tests the premise that cutting-edge, real-time monitoring devices improve athlete health and performance. This research aims to reduce tiredness by developing a Fuzzy Decision Support System for Real-Time Athlete Weariness Monitoring. Fuzzy logic can handle unclear performance data, making it a more flexible and advanced alternative to standard methods. Sports athlete fatigue is complicated and dynamic, requiring improved, more precise, and real-time monitoring approaches. The FDSS-RAFM model uses fuzzy logic to account for human performance and physiology. The FDSS-RAFM model assesses athlete fatigue in a comprehensive and context-aware manner. This study’s findings can help coaches, players, and sports scientists improve training programs, reduce injury risk, and improve performance in ever-changing athletic contexts. Fuzzy decision-support systems and other cutting-edge technology can improve athletes’ health and performance, adding to sports science literature. Experimental results show that the proposed FDSS-RAFM model outperforms competing models in sensitivity (97%), specificity (89%), accuracy (96%), and dynamic adaptation error analysis (2.41%).},
  archive      = {J_IJCIS},
  author       = {Li, Aiqin},
  doi          = {10.1007/s44196-025-00732-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Real-time athlete fatigue monitoring using fuzzy decision support systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced heart disease prediction through spatial and
temporal feature learning with SCN-deep BiLSTM. <em>IJCIS</em>,
<em>18</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s44196-025-00734-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease prediction using machine learning methods faces various challenges, such as low data quality, missing irrelevant values, and underfit and overfit problems, which increase the time complexity and degrade the model&#39;s prediction performance. Moreover, the hybrid models for heart disease prediction showed poor accuracy due to the irrelevancy in the dataset. Therefore, a search optimizer with a deep convolutional neural network coupled with a Deep Bidirectional long short-term memory classifier (SCN-Deep BiLSTM) is proposed to handle the abovementioned issue. The importance of SCN-Deep BiLSTM relies upon establishing the spatial information and temporal features from the ECG signals that support learning while minimizing the computational complexity associated with learning from raw signals.The SCN-Deep BiLSTM model achieves the accuracy, F-score, precision, recall, and critical success index of 0.97, 0.97, 0.98, 0.99, and 0.97, respectively for 80% of model training, whereas the SCN-Deep BiLSTM model attained 0.97, 0.98, 0.96, 0.94, and 0.96 for accuracy, F-score, precision, recall and critical success index, respectively when K-Fold is 10. The performance outcome emphasizes the model’s efficacy and accurate prediction and classification of heart disease.},
  archive      = {J_IJCIS},
  author       = {Pandey, Vivek and Lilhore, Umesh Kumar and Walia, Ranjan},
  doi          = {10.1007/s44196-025-00734-6},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Advanced heart disease prediction through spatial and temporal feature learning with SCN-deep BiLSTM},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of physical education teaching quality based on
hierarchical fuzzy set theory. <em>IJCIS</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s44196-025-00736-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of physical education often faces challenges due to inadequate evaluation methods that fail to provide accurate, real-time teaching evaluation. These challenges influence student performance and overall teaching quality. This article introduces a quality assessment method using fuzzy set theory (QAM-FST) to evaluate physical education teaching. The proposed method extracts and classifies all the available teaching data to compute the students&#39; performance over sessions through fuzzy rough set differentiations over partial and complete derivatives. The complete derivatives identify the factors contributing the maximum to the teaching quality, while the partial derivatives acquire the minimal influence factors. These derivatives are clubbed together through the hierarchical process to identify the precise quality impacting factors and least impacting factors to replace or recommend alternate suggestions. The QAM-FST framework offers a comprehensive, data-driven assessment ensuring the enhancement of PE teaching quality. The QAM-FST outperforms three current models in terms of suggestion accuracy (96.8%), assessment time reduction (22.66%), and total performance evaluation efficiency (16.78%). This data-driven platform guarantees improved physical education instruction quality through actionable insights obtained from real-time feedback.},
  archive      = {J_IJCIS},
  author       = {Tang, Chunlong},
  doi          = {10.1007/s44196-025-00736-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Analysis of physical education teaching quality based on hierarchical fuzzy set theory},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An algorithm for extracting features of basketball players’
foul actions based on an attention mechanism. <em>IJCIS</em>,
<em>18</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s44196-025-00743-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basketball foul action usually involves multiple features, and it is difficult to extract effective features from complex and diverse features. Therefore, a feature extraction algorithm for basketball players&#39; foul action based on attention mechanism is proposed. On the basis of ResNet50 network model, the attention mechanism is integrated to build a basketball player foul action feature extraction model. The machine vision system is used to obtain basketball player action video as the input of the model. The space attention module and time attention module are, respectively, introduced into the video slow frame rate branch and video fast frame rate branch of ResNet50 network. By making the network give greater weight to the key areas of a single frame of video, and improving the network&#39;s attention to important video frames, the best spatio-temporal characteristics of foul actions are obtained. After fusion, the feature fusion results are input into the classifier with the cross-entropy loss function as the loss function, and the probability value of each possible foul action and the tag target probability are output, complete basketball player foul action feature extraction. The experimental results show that the algorithm can effectively identify the identification of basketball players, the cumulative matching feature value of foul action recognition can reach more than 95%, and the average accuracy is more than 70%; the F1 value and stability are high, which can reduce the error caused by data fluctuation and noise; the error rate of real-time detection is less than 4.5%, the omission rate is less than 4.7%, the detection time is lower than 14 ms, and the application effect is good.},
  archive      = {J_IJCIS},
  author       = {Wang, Peng},
  doi          = {10.1007/s44196-025-00743-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An algorithm for extracting features of basketball players&#39; foul actions based on an attention mechanism},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-commerce live-streaming platform and decision support
system based on fuzzy association rule mining. <em>IJCIS</em>,
<em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00744-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live streaming of e-commerce platforms attracts consumers for their products/purchases and hence the familiarity is retained high amid different competitors. Fuzzy decision systems are incorporated to filter the streaming content of the platforms to improve consumer augmentations at different promotions. Therefore to support such augmentation and consumer building process, this article proposes a filtered sale streaming model to improve the circulation of new launches and to project the existing products through sustainable promotions. In this process, the comprehensive transition rule for product promotions and sale improvements is defined using fuzzy mining. The fuzzy process introduces the different performance members based on consumer access rate and sale count. The rule modifications are defined using the above factors’ decrease over filtered promotions to boost the augmentation. Using the highest possible member weights over a product, sale, and consumers, the linear improvements between the three factors are estimated over the closure observed at each sale interval. Thus, the streaming modifications and the product exposures are modeled using different mining rules adaptable for e-commerce platforms.},
  archive      = {J_IJCIS},
  author       = {Liao, Hua},
  doi          = {10.1007/s44196-025-00744-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {E-commerce live-streaming platform and decision support system based on fuzzy association rule mining},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal AC power flow with energy storage and renewable
energy: An efficient RL algorithm capable of handling equality
constraints. <em>IJCIS</em>, <em>18</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s44196-025-00745-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using energy storage to solve the multiperiod OPF problem for renewable energy fluctuation is an effective way to increase operation safety and reduce the cost of power systems. However, in solving this OPF problem, model-based methods cannot accurately model uncertain scenarios, while traditional RL methods cannot satisfy the constraints well, and both methods have limitations. Therefore, we propose an RL method, ERL-HC, that does not require scene modelling and can handle general forms of physical constraints. First, a constraint policy network (CPN) is proposed that corrects the output of a neural network on the basis of the inequality generalized reduced gradient (GRG) method; the outputs of this network satisfy all constraints, and it can be trained in an end-to-end manner. Second, the critic network is improved based on the IM method to increase the sample learning efficiency by improving the agent&#39;s understanding of state interdependencies. Finally, the adaptive-tuning Lagrange multiplier method is applied in the AC framework to reduce the number of iterations of the inequality GRG in the CPN and efficiently train ERL-HC. ERL-HC was tested on two systems of different sizes. The results show that ERL-HC has a better learning ability than general safe RL algorithms, overcomes the limitations of mainstream safe RL methods in handling equality constraints, and addresses the poor generalization issues of RL methods that can handle equality constraints.},
  archive      = {J_IJCIS},
  author       = {Liu, Mingde and Zhu, Jianquan and Liu, Mingbo},
  doi          = {10.1007/s44196-025-00745-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimal AC power flow with energy storage and renewable energy: An efficient RL algorithm capable of handling equality constraints},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-weighted TOPSIS-based bird strike risk assessment
for an airport. <em>IJCIS</em>, <em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s44196-025-00746-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To identify the factors with higher bird strike risk at a certain airport, and to provide a theoretical basis for the airport to develop targeted and dynamic bird strike prevention strategies, this study has established a bird strike risk assessment index system for the airport based on the “3M1E” theory and relevant content of the “Management Measures for the Prevention and Control of Bird Strikes and Animal Intrusions at Transport Airports”. Then, under the premise of reasonably selecting the parameters of generalized intuitionistic fuzzy entropy, the entropy method is used to simultaneously determine the expert weights and indicator weights. Finally, a weighted TOPSIS model was utilized to assess and rank the high-risk factors contributing to bird strike incidents. Then, taking the bird strike event occurrence at a certain airport as an example for analysis, the second-level indicators are ranked. The results show that the top three indicators are Bird Prevention Funding ( $$D_3$$ ), Habitat Distribution ( $$C_1$$ ), and Dispersal Activities ( $$D_5$$ ). The assessment results provide the necessary basic data for the bird strike prevention work of the airport.},
  archive      = {J_IJCIS},
  author       = {Yu, Changyang and Zhao, Fan and Yin, Yu and Wu, Yi},
  doi          = {10.1007/s44196-025-00746-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Entropy-weighted TOPSIS-based bird strike risk assessment for an airport},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation evaluation of personalized and
differentiated teaching strategies for preschool children based on fuzzy
decision support systems. <em>IJCIS</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00748-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {School operations have changed due to information technology. Personalized teaching for preschool children requires innovative and adaptable strategies for providing the best-afford experience and learning environment. Conventional teaching approaches may not always accommodate students’ learning styles. This can lower performance and involvement. The expanding variety of pre-schoolers necessitates creative evaluation and optimization methods that promote inclusion and success for all children. The article introduces a preschool teaching evaluation model to improve the students’ learning experience. In particular, the evaluation model is designed for various assessment strategies irrespective of personalization. The proposed model inputs the students’ learning feasibility and the teaching mode to evaluate the personalization adaptability. The fuzzy decision support system improves the evaluation based on the above factors. It evaluates student-specific indicators, including attentiveness, body language, and engagement, to determine instructional flexibility. The model uses real-time classroom observations and student assessments to find realistic approaches with low-to-high fuzzy derivatives. The proposed system is designed to compute the adaptability from both factors under low-to-high fuzzy derivatives. By defining the maximum feasibility range, the teaching strategy is optimized to meet the adaptability. Thus, the low-level strategies are discarded using adaptability measures to reduce personalization failures. The proposed model is verified using adaptability, feasibility, and mode improvements. Research shows that the fuzzy decision support system makes courses more adaptive and feasible in many contexts, particularly those that involve games, audiovisual approaches, and crafts. Preschoolers, parents, and teachers indicated increased enjoyment, fewer customization failures, and greater involvement. Fuzzy-based evaluation increased feasibility ratings and approach versatility. The research provides a valuable foundation for solving classroom customization difficulties in preschool settings, emphasizing data-driven, adaptive teaching techniques. This method enables scalable applications in early childhood education through fuzzy decision-making and real-time evaluations.},
  archive      = {J_IJCIS},
  author       = {Hou, Yali},
  doi          = {10.1007/s44196-025-00748-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Design and implementation evaluation of personalized and differentiated teaching strategies for preschool children based on fuzzy decision support systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-source fusion positioning system based on MDAW-PF
algorithm and PDR. <em>IJCIS</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00749-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to multiple occlusions and strong interference in an indoor environment, the traditional single signal source location method is difficult to meet the requirements of high precision and high robustness. Therefore, this paper proposes a multi-source fusion location system based on an adaptive vector particle filter, which combines fingerprint location, pedestrian dead reckoning and map information. The received signal intensity is optimized by offline fingerprint calibration and Kalman filter. The adaptive vector particle filter adopts multi-direction sampling and weight adjustment, which effectively improves the diversity of particles and reduces errors. Compared with the single-source method and other multi-source systems, the positioning accuracy and trajectory fitting degree of the proposed system were significantly improved. The positioning error probability was 97%, the average error was 0.67 m, and the positioning accuracy reached 90.1%. In summary, the proposed multi-source fusion system provides an effective solution for indoor high-precision and reliable positioning.},
  archive      = {J_IJCIS},
  author       = {Wu, Wennan and Xu, Yigang and Li, Zhimin and Lai, Jizhou},
  doi          = {10.1007/s44196-025-00749-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A multi-source fusion positioning system based on MDAW-PF algorithm and PDR},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter adaptive manta ray foraging optimization for
global continuous optimization problems and parameter estimation of
solar photovoltaic models. <em>IJCIS</em>, <em>18</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s44196-025-00753-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manta ray foraging optimization (MRFO) algorithm suffers from a fixed parameter $$ S $$ , limiting its adaptability in balancing search capability and convergence speed during different optimization stages. To address this limitation, a success-history-based parameter adaptation strategy is proposed to dynamically adjust $$ S $$ . Furthermore, to enhance population diversity and avoid premature convergence, a randomly selected individual from the top $$ G $$ high-quality solutions replaces the current best individual in the somersault foraging behavior. Based on these improvements, a parameter adaptive manta ray foraging optimization (PAMRFO) algorithm is developed. The experimental results demonstrate the effectiveness of PAMRFO. On the IEEE CEC2017 benchmark function set, PAMRFO achieved an average win rate of 82.39% across 29 functions compared to seven state-of-the-art algorithms. On 22 IEEE CEC2011 real-world optimization problems, PAMRFO achieved an average win rate of 55.91% compared to ten advanced algorithms. Sensitivity analysis identified optimal parameter settings, and further stability analysis revealed that PAMRFO exhibits higher success rates and computational efficiency among the four MRFO variants. Population diversity and exploration-exploitation analysis demonstrated the effectiveness of the proposed update mechanism in maintaining diversity and balancing exploration and exploitation. In solving parameter estimation problems for six multimodal solar photovoltaic models, PAMRFO outperformed other competing methods with a 100% success rate, highlighting its superior performance in the photovoltaic field. These findings validate the robustness, efficiency, and wide applicability of PAMRFO, providing advanced solutions for optimization problems in the new energy domain.},
  archive      = {J_IJCIS},
  author       = {Tang, Zhentao and Wang, Kaiyu and Yao, Yongxuan and Zhu, Mingxin and Zhuang, Lan and Chen, Huiqin and Li, Jing and Yan, Li and Gao, Shangce},
  doi          = {10.1007/s44196-025-00753-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Parameter adaptive manta ray foraging optimization for global continuous optimization problems and parameter estimation of solar photovoltaic models},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure authentication and key exchange protocol for
vehicles to infrastructure network. <em>IJCIS</em>, <em>18</em>(1),
1–16. (<a href="https://doi.org/10.1007/s44196-025-00754-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles technology has been widely applied in various communication scenarios, including vehicles to vehicles, vehicles to roadside facilities, vehicles to pedestrians, and vehicles to cloud. Owing to transmitting data through public channels, various security issues like identity leakage, man in the middle attack, key leakage and etc., are also introduced simultaneously and still challenging to be solved. Researches and practices have shown that authentication and key exchange protocols are effective methods to solve such security issues. However, most existing security protocols for Internet of Vehicles are established on the premise that the registration process is with a secure channel, which is usually not satisfied and deviates from practical applications. Accordingly, an authentication and key exchange protocol with an insecure channel has been proposed, in which the operations of symmetric encryption and XOR encryption are adopted for all interactive processes to improve protocol security. The theoretical analysis and formal verification demonstrate that the proposed protocol satisfies security properties including authentication and confidentiality, and reduces the costs of computation and communication compared with the method with public key encryption.},
  archive      = {J_IJCIS},
  author       = {Xu, Peng and Wang, Xiuzhen and Chen, Meirong},
  doi          = {10.1007/s44196-025-00754-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A secure authentication and key exchange protocol for vehicles to infrastructure network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Publisher correction: A two-way crossed effects fuzzy panel
linear regression model. <em>IJCIS</em>, <em>18</em>(1), 1. (<a
href="https://doi.org/10.1007/s44196-025-00756-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Hesamian, Gholamreza and Johannssen, Arne},
  doi          = {10.1007/s44196-025-00756-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Publisher correction: A two-way crossed effects fuzzy panel linear regression model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Correction: ExAIRFC-GSDC: An advanced machine
learning-based interpretable framework for accurate gas leakage
detection and classification. <em>IJCIS</em>, <em>18</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s44196-025-00758-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCIS},
  author       = {Lalithadevi, B. and Krishnaveni, S.},
  doi          = {10.1007/s44196-025-00758-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Correction: ExAIRFC-GSDC: an advanced machine learning-based interpretable framework for accurate gas leakage detection and classification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing solid oxide fuel cell efficiency through advanced
model identification using differential evolutionary mutation fennec fox
algorithm. <em>IJCIS</em>, <em>18</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s44196-025-00759-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuel cells (FCs) are increasingly attracting attention for their efficient conversion of chemical energy into electricity without the need for combustion. Their high efficiency and versatility make them a promising technology across various applications. Researchers are actively exploring ways to optimize FC systems to meet specific energy needs. Among the different types of fuel cells, solid oxide fuel cells (SOFCs) stand out as a promising clean energy technology that generates electricity through electrochemical reactions. However, accurately modeling SOFCs, which is essential for reducing design costs, presents a challenge due to their complex and nonlinear characteristics. An ideal model should be adaptable to varying operating pressures and temperatures. This research introduces a novel approach for optimal SOFC model identification using a differential evolutionary mutation Fennec fox algorithm (DEMFFA). A real-world case study demonstrates the superior effectiveness of DEMFFA compared to existing methods. Additionally, a sensitivity analysis evaluates the influence of temperature and pressure on the model, with results indicating that the proposed method achieves higher efficiency than other approaches. The sum of the square error of the proposed algorithm is 1.18E-11 followed by the parent algorithm, Fennec fox algorithm (FFA) (1.24E-09), and some of the compared algorithms. The computational time of the proposed algorithm is 1.001 s, followed by the parent algorithm FFA (1.199 s) and some of the compared algorithms. DEMFFA offers significant potential, enhancing renewable energy, minimizing SOFC&#39;s environmental impact, and improving real-world applications like distributed power generation and hydrogen integration.},
  archive      = {J_IJCIS},
  author       = {Singla, Manish Kumar and Gupta, Jyoti and Kumar, Ramesh and Jangir, Pradeep and Louzazni, Mohamed and Giri, Nimay Chandra and Al-Gburi, Ahmed Jamal Abdullah and EI-Kenawy, E. I.-Sayed M. and Alharbi, Amal H.},
  doi          = {10.1007/s44196-025-00759-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhancing solid oxide fuel cell efficiency through advanced model identification using differential evolutionary mutation fennec fox algorithm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid LECNN architecture: A computer-assisted early
diagnosis system for lung cancer using CT images. <em>IJCIS</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s44196-025-00761-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is one of the most common causes of cancer-related death. Therefore, early diagnosis of this cancer is crucial for planning patient treatment. This paper proposes a hybrid Lung Ensemble Convolutional Neural Network (LECNN) architecture for the computer-aided early diagnosis of lung cancer via CT images. The proposed hybrid approach integrates a transfer learning (TL) mechanism with ensemble learning (EL) on the basis of majority voting. Initially, CNN architectures (GoogLeNet, EfficientNet, DarkNet19, and ResNet18) are trained via TL, and the resulting CNN models are used as inputs in EL. The outputs from all the CNN architectures are evaluated via majority voting to identify the top-performing triple CNN combination, which is then utilized in the hybrid approach. The performance of the proposed method was assessed via the widely used IQ-OTH/NCCD dataset. Additionally, the impact of the elastic transformation method, a data augmentation technique, on performance improvement was investigated in the proposed method. The triple combination of the GoogLeNet, EfficientNet, and DarkNet19 CNN architectures, as part of the EL method in the hybrid approach, achieved superior performance on both the raw and augmented datasets according to the obtained performance results. The performance evaluations revealed that the proposed approach achieved more than a 5% improvement with the augmented dataset compared with the raw IQ-OTH/NCCD dataset, resulting in the highest performance. The proposed hybrid approach achieved 99% accuracy, 98.82% sensitivity, 99.48% specificity, 99.06% precision, and 98.94% F1 score on the augmented IQ-OTH/NCCD dataset. When compared with findings from previous studies using the same dataset, the proposed hybrid approach outperformed state-of-the-art methods. In conclusion, it demonstrates significant potential as a robust tool for computer-aided early lung cancer diagnosis systems and may also contribute to the development of future hybrid approaches in this field.},
  archive      = {J_IJCIS},
  author       = {Güraksın, Gür Emre and Kayadibi, Ismail},
  doi          = {10.1007/s44196-025-00761-3},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A hybrid LECNN architecture: A computer-assisted early diagnosis system for lung cancer using CT images},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy multi-attribute
decision-making based on entropy and bidirectional projection.
<em>IJCIS</em>, <em>18</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s44196-025-00763-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel arctangent-based interval-valued intuitionistic fuzzy entropy function designed to address multi-attribute group decision-making problems, particularly in situations where attribute weights are either completely unknown or only partially known. The proposed entropy function computes the objective weights of the attributes and, by integrating these with subjective weights, derives a comprehensive weight. This methodology effectively addresses uncertainties and the incomplete nature of weight information in decision-making processes. Furthermore, the paper extends the bidirectional projection method to the interval intuitionistic fuzzy context, developing a multi-attribute decision-making model that integrates the arctangent-based interval-valued intuitionistic fuzzy entropy and the bidirectional projection method. Finally, a series of comparative experiments are conducted to validate the effectiveness and robustness of the proposed entropy function and bidirectional projection method in multi-attribute decision-making.},
  archive      = {J_IJCIS},
  author       = {Zheng, Jian and Dong, Minggao},
  doi          = {10.1007/s44196-025-00763-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Interval-valued intuitionistic fuzzy multi-attribute decision-making based on entropy and bidirectional projection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of communication transmission frequency linear
algebraic model under aerial computing architecture. <em>IJCIS</em>,
<em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s44196-025-00764-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of the demand for wireless communication systems, when the existing algebraic model processes real-time data, the frequency adjustment is inflexible, it is difficult to quickly optimize the transmission parameters to cope with network load changes, and the long-term operation fails to effectively control the CPU frequency, resulting in increased energy consumption. To better promote the development of wireless communication systems, this article aimed to use aerial computing architecture to optimize the linear algebraic model of communication transmission frequency, to better meet the needs of today&#39;s wireless communication systems. The article first designed a communication frequency stabilization device structure to ensure high stability of the output spectrum. Then it introduced a dynamic frequency adjustment module to achieve real-time adjustment of communication transmission frequency. It then improved the data transmission rate through the design of the aerial computing perception module. This article used a frequency optimization algorithm based on linear algebra to adjust its linear relationship, optimize transmission energy consumption, and improve transmission efficiency. Finally, to verify the application effect of aerial computing architecture in optimizing the linear algebraic model of communication transmission frequency, this paper compared it with traditional dynamic adjustment models and parallel computational models. The research results showed that for packet 13, the round-trip time required to transmit the model in this article was 1.21 ms; the response time was 0.009 ms, and the total energy consumption was 89.6-W hours. The traditional dynamic adjustment model required a round-trip time of 4.92 ms, a response time of 0.093 ms, and a total energy consumption of 119.1-W hours for packet 13 transmissions. The parallel computational model required a round-trip time of 6.33 ms, a response time of 0.063 ms, and a total energy consumption of 131.4-W hours for packet 13 transmissions. The results showed that the optimized communication transmission frequency linear algebraic model using aerial computing architecture had shorter communication delay and response time, lower energy consumption, and better frequency control performance. This article highlighted the important impact of aerial computing architecture on the stability, real-time performance, and transmission rate of linear algebraic models of communication transmission frequencies, providing more ideas for the design and planning of wireless communication systems.},
  archive      = {J_IJCIS},
  author       = {Gao, Yufeng},
  doi          = {10.1007/s44196-025-00764-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Optimization of communication transmission frequency linear algebraic model under aerial computing architecture},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DenseNet-ABiLSTM: Revolutionizing multiclass arrhythmia
detection and classification using hybrid deep learning approach
leveraging PPG signals. <em>IJCIS</em>, <em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s44196-025-00765-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrhythmias (AM) are heart conditions that can lead to fatal cardiac arrest. Automated identification of arrhythmias is crucial for detecting cardiac diseases. Previous studies have used photoplethysmography (PPG) signals to identify arrhythmias, but there is limited research on their application for multiclass arrhythmia classification. This study introduces a Hybrid Deep Learning (HDL) model called DenseNet-ABiLSTM, which uses densely connected convolutional networks and Attention-based Bidirectional Long Short-Term Memory (ABiLSTM) to categorize various types of arrhythmias. The model uses 1D convolutional kernels to acquire multiscale conceptual features, followed by BiLSTM to understand temporal relationships among features. The Attention Mechanism layer is presented to improve detection performance. The model categorizes arrhythmia rhythms into six types: Sinus Rhythm (SR), Early Ventricular Contraction (EVC), Early Atrial Contraction (EAC), Ventricular Tachycardia (VT), Supraventricular Tachycardia (ST), and AF. Various metrics were assessed and compared with Electrocardiogram (ECG) results to determine AM rhythms. The mean performance measures showed strong overall performance, with a mean F1 score and accuracy of 87.74% and 89.14%, respectively.},
  archive      = {J_IJCIS},
  author       = {Saranya, K. and Karthikeyan, U. and Kumar, A. Saran and Salau, Ayodeji Olalekan and Tin Tin, Ting},
  doi          = {10.1007/s44196-025-00765-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {DenseNet-ABiLSTM: Revolutionizing multiclass arrhythmia detection and classification using hybrid deep learning approach leveraging PPG signals},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection system for network security using novel
adaptive recurrent neural network-based fox optimizer concept.
<em>IJCIS</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s44196-025-00767-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of daily networks and communications rely heavily on network security. Researchers in cybersecurity emphasize the necessity of developing effective intrusion detection systems (IDS) to safeguard networks. The importance of efficient IDS escalates as attackers devise new types of attacks and network volumes expand. Furthermore, IDS aims to ensure the integrity, confidentiality, and availability of data transmitted across networked systems by preventing unauthorized access. Following numerous studies utilizing machine learning (ML) to develop effective IDS, the focus has shifted towards deep learning (DL) techniques as artificial neural networks (ANNs) and DL systems have become prevalent. ANNs are capable of generating features autonomously, eliminating the need for manual intervention. This paper introduces an innovative adaptive recurrent neural network-based fox optimizer (ARNN-FOX) method. The primary objective of the ARNN-FOX system is to efficiently detect and classify network intrusions, thereby enhancing network security. Data normalization is conducted to scale the incoming data into a usable format. The gray level co-occurrence matrix (GLCM) method is proposed for selecting the optimal subset of features for the ARNN-FOX method. In the proposed approach, the fox algorithm (FOX) is utilized for the adjustment of hyperparameters in the ARNN model. The efficacy of the ARNN-FOX approach is assessed using benchmark datasets. Based on comparative results, the ARNN-FOX method demonstrates superior performance in parameters such as accuracy, specificity, sensitivity, F1 Score, recall value, and precision values over existing models. The proposed ARNN-FOX-based IDS model for the network security in terms of accuracy is 15.12%, 8.79%, 6.45%, and 4.21% better than RNN, CNN-LSTM, DASO-RNN, and ChCSO-LSTM, respectively. Similarly, with respect to specificity, the suggested ARNN-FOX-based IDS model for network security outperforms RNN, CNN-LSTM, DASO-RNN, and ChCSO-LSTM by 32.43%, 8.89%, 3.16%, and 2.08%, respectively.},
  archive      = {J_IJCIS},
  author       = {Manivannan, R. and Senthilkumar, S.},
  doi          = {10.1007/s44196-025-00767-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Intrusion detection system for network security using novel adaptive recurrent neural network-based fox optimizer concept},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-perspective learning based on transformer for stock
price trend. <em>IJCIS</em>, <em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s44196-025-00768-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock constitutes a crucial element of the financial market, and accurately forecasting stock trends remains a significant and unresolved issue. Nonetheless, the stock’s considerable complexity renders accurate prediction of stock trends more challenging. This paper proposes a novel multi-perspective approach that converts the time series prediction challenge into an image classification problem, referred to as the Multi-perspective Denoise Transformer (MPDTransformer). We initially multi-factor features into two-dimensional images employing a multi-perspective approach to more comprehensively explain the actual market conditions and enhance the model’s practicality and adaptability; secondly, we utilize a Convolutional Autoencoder (CAE) to extract features, which effectively eliminates noise and enhances data purity; finally, to comprehensively capture the temporal relationships within the data and gain a deeper understanding of the overall time series, we employ a Transformer for prediction. Experimental results demonstrate that our method outperforms other prevalent stock trend prediction techniques.},
  archive      = {J_IJCIS},
  author       = {Li, Xiliang and Chen, Shuoru and Qiao, Xiaoyan and Zhang, Mingli and Zhang, Caiming and Zhao, Feng},
  doi          = {10.1007/s44196-025-00768-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Multi-perspective learning based on transformer for stock price trend},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid-CID: Securing IoT with mongoose optimization.
<em>IJCIS</em>, <em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s44196-025-00751-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) technology has evolved beyond personal devices to power global deployments across a wide range of networks which has a significant impact on global commerce. However, security challenges arise due to the wide range of protocols and computational capabilities in IoT devices. To combat these issues, a novel hybrid optimization-enabled neural network for classification of intrusion data against IoT system (Hybrid-CID) is proposed particularly to identify intrusions in resource-constrained IoT devices. Initially, the incoming data are standardized by removing the irrelevant information through preprocessing to ensure the performance of detection models. After preprocessing, the Hybrid-CID framework develops a hybrid optimization algorithm to identify the intrusions from the traffic data which ensures data privacy by maintaining the reliability and integrity of IoT deployments. Finally, the combined deep learning (DL) network classifies the identified intrusions to contribute proactive threat mitigation by ensuring the confidentiality of IoT system data. The Hybrid-CID system is validated through the benchmark CSE-CIC-IDS 2018 and CICIDS 2017 datasets using accuracy, specificity, precision, F1-score, recall, execution time, communication cost, detection rate, detection time, and computational cost. The Hybrid-CID framework achieves an overall accuracy of 97.82%, whereas the WDLSTM, TLBO-IDS, and DIS-IoT techniques achieve 87.42%, 89.58%, and 94.72%, respectively, for efficiently detecting intrusions in IoT networks.},
  archive      = {J_IJCIS},
  author       = {Sheeba, S. Merlin and Shaji, R. S.},
  doi          = {10.1007/s44196-025-00751-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hybrid-CID: Securing IoT with mongoose optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lattice-based decision models for green urban development:
Insights from <span
class="math display"><em>L</em><sub><em>q</em></sub>*</span> q-rung
orthopair multi-fuzzy soft set. <em>IJCIS</em>, <em>18</em>(1), 1–27.
(<a href="https://doi.org/10.1007/s44196-025-00755-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location selection is a critical process in decision-making for projects that involve multiple criteria, such as urban planning, industrial site development, or green building projects. Multiple criteria decision making (MCDM) is a systematic approach that evaluates and ranks potential alternatives based on a set of often conflicting criteria. This study focuses on selecting the optimal urban location for a green building project by employing the $$L_{q}*$$ q-rung orthopair multi-fuzzy soft-MCDM( $$L_{q}*$$ q-ROMFS) techniques. The $$L_{q}*$$ q-ROMFS set combines elements from two distinct theories with lattice ordering parameters: q-rung orthopair fuzzy set and multi-fuzzy soft set. It provides a mathematical framework with multiple parameters that effectively represents problems involving multi-dimensional data within a dataset. We expand this concept by establishing the algebraic structures of $$L_{q}*$$ q-ROMFS sets, including properties like modularity and distributivity, while also analyzing their homomorphism under lattice mappings. Finally, leveraging the $$L_{q}*$$ q-ROMFS matrix, we propose both a choice matrix and a weighted choice matrix to effectively address the selection of the optimal urban location for a green building project.},
  archive      = {J_IJCIS},
  author       = {Jayakumar, Vimala and Pethaperumal, Mahalakshmi and Kausar, Nasreen and Pamucar, Dragan and Simic, Vladimir and Salman, Mohammed Abdullah},
  doi          = {10.1007/s44196-025-00755-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Lattice-based decision models for green urban development: Insights from $$L_{q}*$$ q-rung orthopair multi-fuzzy soft set},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced model for gestational diabetes mellitus prediction
using a fusion technique of multiple algorithms with explainability.
<em>IJCIS</em>, <em>18</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s44196-025-00760-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High glucose levels during pregnancy cause Gestational Diabetes Mellitus (GDM). The risks include cesarean deliveries, long-term type 2 diabetes, fetal macrosomia, and infant respiratory distress syndrome. These risks highlight the need for accurate GDM prediction. This research proposes a novel fusion model for early GDM prediction. It uses conventional Machine Learning (ML) and advanced Deep Learning (DL) algorithms. Subsequently, it combines the strengths of both ML and DL algorithms using various ensemble techniques. It incorporates a meta-classifier that further reinforces its robust prediction performance. The dataset is split into training and testing sets in a 70/30 ratio. The initial steps involve exploratory analysis and data preprocessing techniques such as iterative imputation and feature engineering. Subsequently, oversampling is applied to the training set to address class imbalance which ensures the model learns effectively. The testing set remains imbalanced to maintain the credibility of the model’s performance evaluation. The fusion model achieves an accuracy of 98.21%, precision of 97.72%, specificity of 98.64%, recall of 97.47%, F1 score of 97.59%, and an Accuracy Under the Curve (AUC) of 99.91%. The model exhibits efficiency with an average processing time of 0.06 s to predict GDM. These results outperform the previous studies using the same GDM prediction dataset and demonstrate the model&#39;s superior performance. Additionally, Explainable Artificial Intelligence (XAI) techniques are utilized to interpret the model’s decisions. They highlight the most influential features in GDM prediction and ensures transparency. The proposed fusion model can facilitate proactive GDM prediction to elevate GDM management and maternal–fetal health outcomes.},
  archive      = {J_IJCIS},
  author       = {Hassan, Ahmad and Ahmad, Saima Gulzar and Iqbal, Tassawar and Munir, Ehsan Ullah and Ayyub, Kashif and Ramzan, Naeem},
  doi          = {10.1007/s44196-025-00760-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Enhanced model for gestational diabetes mellitus prediction using a fusion technique of multiple algorithms with explainability},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of distributionally robust optimization markov
decision-making under uncertainty in scheduling of multi-category
emergency medical materials. <em>IJCIS</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s44196-025-00762-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the preliminary stages of public health emergencies, many regional public health systems do not have enough medical resources to address the needs that arise from the emergencies. Additionally, due to the rapid development of emergency situations, it is difficult to accurately understand all medical material demands. Thus, we develop a multi-category emergency medical materials robust scheduling method for multi-period, which continuously schedules the availability of multi-category emergency medical materials during times of uncertain demand. First, we developed a single-period Distributionally Robust Optimization (DRO) model to provide a powerful strategy for scheduling multi-category emergency medical materials with little information on material demand. In the DRO model, we prioritize medical material requirements into different categories, assuming only that the means and variances of information on the demand are available to seek an optimal implementation strategy in a single period. We then combine the DRO scheduling model with the Markov Decision Process (MDP) and extend it to the multi-period Distributionally Robust Optimization Markov Decision Process scheduling model (DRO-MDP). Our DRO-MDP model provides encouraging guidelines to solve the multi-period scheduling problem of multi-category emergency medical materials in uncertain situations. A simulated experiment is used to demonstrate the effectiveness of the proposed model. The simulation uses COVID-19 data from New Delhi, India in the spring of 2021. It is important to note that the model we propose can be easily generalized as a framework for any multi-category resource allocation problem with uncertain needs.},
  archive      = {J_IJCIS},
  author       = {Liang, Zhizhen and Wang, Xiaojia and Xu, Sheng and Chen, Wei},
  doi          = {10.1007/s44196-025-00762-2},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Application of distributionally robust optimization markov decision-making under uncertainty in scheduling of multi-category emergency medical materials},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SED-NET: Real-time suspicious event detection via deep
learning-based di-stream neural network. <em>IJCIS</em>, <em>18</em>(1),
1–19. (<a href="https://doi.org/10.1007/s44196-025-00766-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suspicious event detection (SED) identifies anomalous activities in surveillance data using computer vision and machine learning techniques. However, existing approaches have high false positive rates difficulty distinguishing suspicious from normal behaviors, and limited adaptability to dynamic environments. This research introduces a novel deep learning-based SED-NET model for detecting suspicious events in public places. Initially, input images are collected from two data for detecting Suspicious events. Surveillance camera videos are converted into frames and the suspicious images are pre-processed using a Gaussian adaptive bilateral filter (GABF) to reduce noise while preserving edges. A Sobel edge detector is used to detect the fine edges in the pre-processed frames for enhancing the structural details. Di-Stream Neural Network (DSNN) is introduced with the dual-branch EfficientNet-based feature extractor that retrieves both motion and pose features. The weighted Average Fusion method is used to combine pose and motion features to classify suspicious activities using the Simplified Spiking Neural Network (SSNN). The effectiveness of the proposed SED-NET method was evaluated using specificity, accuracy, sensitivity, and F1 score. The proposed SED-NET model attains an accuracy of 98.97% for UCSD Pedestrian and 98.84% for CUHK Avenue datasets. Moreover, the proposed SED-NET improved the overall accuracy by 7.44%, 4.18%, and 2.73% better than DenseNet121, SegAD, and 3DCNN, respectively.},
  archive      = {J_IJCIS},
  author       = {Siva Senthil, D. and Sivarani, T. S.},
  doi          = {10.1007/s44196-025-00766-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {SED-NET: Real-time suspicious event detection via deep learning-based di-stream neural network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hesitant fuzzy <span class="math display"><em>β</em></span>
-covering <span class="math display">(ℐ,</span> <span
class="math display">𝒪)</span> rough set models and applications to
multi-attribute decision-making. <em>IJCIS</em>, <em>18</em>(1), 1–29.
(<a href="https://doi.org/10.1007/s44196-025-00769-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hesitant fuzzy $$\beta $$ -covering rough set offers stronger representational capabilities than earlier hesitant fuzzy rough sets. Its flexibility makes it more suitable for hesitant fuzzy multi-attribute decision-making (MADM). As a result, it has become a popular research focus in decision analysis and has drawn significant attention from scholars. However, the existing hesitant fuzzy $$\beta $$ -covering rough set based on t-norms cannot handle the overlap and correlation between hesitant information well. Addressing this problem, we propose the hesitant fuzzy overlap function and hesitant fuzzy $$\beta $$ -covering $$({\mathcal {I}},$$ $${\mathcal {O}})$$ rough set (HF $$\beta $$ CIORS) models based on the hesitant fuzzy overlap function. First, we establish the definition of the hesitant overlap function and representable hesitant fuzzy overlap function on a partial order relation. Based on proposed definitions, we provide examples of representable and unrepresentable hesitant fuzzy overlap functions and offer a detailed proof to explain the unrepresentable function. Second, we construct four types of HF $$\beta $$ CIORS models and prove some of its important properties. Thirdly, we integrate the HF $$\beta $$ CIORS models with the TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) method and apply them to solve MADM problems. The validity of the proposed method is demonstrated through a practical application, and its stability and effectiveness are confirmed via sensitivity and comparative analyses. Based on these validations, our method proves effective in addressing MADM problems, offering reliable decision-making support.},
  archive      = {J_IJCIS},
  author       = {Wang, Jingyi and Shao, Songtao and Mao, Xiaoyan and Zhang, Xiaohong},
  doi          = {10.1007/s44196-025-00769-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hesitant fuzzy $$\beta $$ -covering $$({\mathcal {I}},$$ $${\mathcal {O}})$$ rough set models and applications to multi-attribute decision-making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced hybrid machine learning model for accurate
detection of cardiovascular disease. <em>IJCIS</em>, <em>18</em>(1),
1–20. (<a href="https://doi.org/10.1007/s44196-025-00771-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) is one of the foremost reasons behind the death of people worldwide. Prevention and early diagnosis are the only ways to control its progression and onset. Thus, there is an urgent need for a detection model comprising intelligent technologies, including Machine Learning (ML) and deep learning, to predict the future state of an individual suffering from cardiovascular disease by effectively analyzing patient data. This study aims to propose a hybrid model that provides a deep insight into the data under consideration to enhance model accuracy for effectively detecting cardiovascular disease. This current research proposes a hybrid model comprising four stages. In the first stage of the proposed hybrid model, the data imbalance problem is solved using a hybrid sampling technique named Synthetic Minority Oversampling Technique-Edited Nearest Neighbors Rule. In the second stage, the Chi-square is applied as a feature selection method to select the highly relevant features from the records of 1190 with 11 clinical features, curated by combining the 5 most popular datasets, including Long Beach VA, Hungarian, Switzerland, and Statlog (Heart). In the third stage, the preprocessed dataset is passed to a stacking ensemble model comprising three base learners: Random Forest Tree (RFT), K-Nearest Neighbor (K-NN), and AdaBoost classifier and one meta-learner: Logistic Regression (LR), optimized with Grid Search Cross-Validation (GSCV) optimization approach, whose performance is evaluated against individual classifier. In the fourth stage, the performance is evaluated in terms of accuracy, sensitivity, specificity, F1 score, and ROC_AUC score.. The comparative results prove that the proposed hybrid model scored the highest accuracy of 97.8%, 96.15% sensitivity, and 96.75% specificity and 98.6% ROC_AUC score when compared with the existing techniques and models after applying the SMOTE–ENN (for data balancing) and Chi-square (for feature selection) methods for the efficient detection of cardiovascular disease. The implementation results demonstrate that the suggested hybrid model may accurately identify cardiovascular disease among patients. It facilitates the application of robust clinical treatment strategies.},
  archive      = {J_IJCIS},
  author       = {Navita and Mittal, Pooja and Sharma, Yogesh Kumar and Lilhore, Umesh Kumar and Simaiya, Sarita and Saleem, Kashif and Ghith, Ehab Seif},
  doi          = {10.1007/s44196-025-00771-1},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Advanced hybrid machine learning model for accurate detection of cardiovascular disease},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted hybrid random forest model for significant feature
prediction in alzheimer’s disease stages. <em>IJCIS</em>,
<em>18</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s44196-025-00780-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent studies, several machine learning and deep learning prediction models have been proposed for the early detection and classification of various stages of Alzheimer’s Disease (AD). Many years before the actual onset of AD, there occur several structural changes in the brain. These structural brain features can be utilized in learning the disease progression from an early stage of the disease. The various stages of pathology cause mild cognitive impairment (MCI) from normal cognition and AD from normal cognition. This work intends to develop a weighted and hybrid random forest learning model that utilizes a relevant subset of predictors to diagnose the progression of the disease. The conversion from normal cognition to MCI is identified at an early stage of the onset of structural brain changes. The importance of proposed research works lies in more early identification of significant feature that increase disease progression and appropriate interventions greatly improve subjects’ recovery. The Alzheimer’s Disease Neuro Imaging Initiative (ADNI) cross-sectional MRI data were analyzed in this study that utilized brain curvature, grey matter density, white matter density, the volume of cortical and sub-cortical structures, shape of hippocampus, hippocampal subfield volume, Mini-mental state exam (MMSE), Clinical Dementia Rating (CDR), Estimated Total Intracranial Volume, Normalize Whole Brain Volume, and Atlas Scaling Factor for constructing randomized trees and thus predicting the features that cause the progression of disease stages from MCI to Alzheimer’s disease that causes dementia. Based on previous studies, there is a significant shortfall in understanding Alzheimer’s disease progression from pre-MCI stages and the classification of progressive and stable MCI groups. As a consequence of this challenge discussed, whether all the mild cognitively impaired people change to AD cohorts or remain in normal cognition and identification of the structural and functional features remains underexplored. Thus, the proposed Weighted Hybrid Random Forest algorithm (WHBM) utilized the 63 features that comprise the whole brain volume. The most significant and weighted features are derived which segregate 39% of subjects with cognitively progressive MCI and 51% of subjects with normal age-related cognitive decline. This implementation model proved to give robust AD conversion probability and identify significant features with 93% accuracy and 88% sensitivity that are sufficient for future clinical inferences. The optimized model thus resulted in the prediction of disease conversion probability from Mild Cognitive Impairment to AD because of significant structural features that are key-requisite for affected geriatric cohorts.},
  archive      = {J_IJCIS},
  author       = {Rohini, M. and Surendran, D.},
  doi          = {10.1007/s44196-025-00780-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Weighted hybrid random forest model for significant feature prediction in alzheimer’s disease stages},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid android malware detection and classification using
deep neural networks. <em>IJCIS</em>, <em>18</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s44196-025-00783-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a deep learning-based framework for Android malware detection that addresses critical limitations in existing methods, particularly in handling obfuscation and scalability under rapid mobile app development cycles. Unlike prior approaches, the proposed system integrates a multi-dimensional analysis of Android permissions, intents, and API calls, enabling robust feature extraction even under reverse engineering constraints. Experimental results demonstrate state-of-the-art performance, achieving 98.2% accuracy (a 7.5% improvement over DeepAMD) on a cross-dataset evaluation spanning 15 malware families and 45,000 apps. The framework’s novel architecture enhances explainability by mapping detection outcomes to specific behavioral patterns while rigorous benchmarking across five public datasets (including Drebin, AndroZoo, and VirusShare) mitigates dataset bias and validates generalization. By outperforming existing techniques in accuracy, adaptability, and interpretability, this work advances the practicality of deep learning for real-world Android malware defense in evolving threat landscapes.},
  archive      = {J_IJCIS},
  author       = {Rashid, Muhammad Umar and Qureshi, Shahnawaz and Abid, Abdullah and Alqahtany, Saad Said and Alqazzaz, Ali and ul Hassan, Mahmood and Al Reshan, Mana Saleh and Shaikh, Asadullah},
  doi          = {10.1007/s44196-025-00783-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Hybrid android malware detection and classification using deep neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Multicriteria decision-making under conditions
of uncertainty: A fuzzy set perspective. John wiley &amp; sons. ISBN:
978–1-119–53,492-1. <em>IJCIS</em>, <em>18</em>(1), 1–5. (<a
href="https://doi.org/10.1007/s44196-025-00784-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This overview is focused on the book reflecting research results on the fundamentals of the theory of multicriteria (multiobjective and multiattribute) decision-making under conditions of uncertainty. The facet of uncertainty is formalized based on a possibilistic (not probabilistic) approach. These results are based on the fuzzy set theory and its fusion with other branches of mathematics of uncertainty. The overview identifies the crucial arguments behind the ultimate need for this theory, reflects the book’s primary objectives, identifies the key possibilities delivered by the presented book&#39;s results, and elaborates on real-world problems solved by applying the findings reported in the book. The thorough critical analysis summarizes the advantages and limitations of the main results covered by the book.},
  archive      = {J_IJCIS},
  author       = {Ekel, Petr Iakovlevitch and Libório, Matheus Pereira and Pedrycz, Witold},
  doi          = {10.1007/s44196-025-00784-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-5},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Book review: multicriteria decision-making under conditions of uncertainty: a fuzzy set perspective. john wiley &amp; sons. ISBN: 978–1-119–53,492-1.},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MicrobeNet: An automated approach for microbe organisms
prediction using feature fusion and weighted CNN model. <em>IJCIS</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s44196-025-00777-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbial organisms are everywhere, millions residing within the human body and also cover 60% of the living earth. These microbes can pose significant health risks, causing diseases such as malaria and toxoplasmosis. Toxoplasmosis is notably prevalent, with seroprevalence rates ranging from 3.6 to 84% in an African region, underscoring the necessity for automated microorganism detection techniques. This research work aims to predict the presence of microorganisms in the human body. We propose a novel approach that combines and integrates principal-component analysis, Chi-square, and analysis-of-variance features using a weighted convolutional-neural-network model called MicrobeNet. The results highlight the efficacy of the proposed method, achieving a remarkable 99.97% in accuracy, recall, precision, and F1-score. The experiments use multiple deep and machine learning models to detect ten distinct microbial forms. The results of the proposed model are compared with those of previously published research. Additionally, k-fold cross validation confirms the robustness of these findings. This research significantly advances the field of microbiology by providing a highly accurate method for microorganism identification, facilitating early disease detection and prevention.},
  archive      = {J_IJCIS},
  author       = {Alnowaiser, Khaled},
  doi          = {10.1007/s44196-025-00777-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MicrobeNet: An automated approach for microbe organisms prediction using feature fusion and weighted CNN model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging multi-source data for local government financing
vehicles debt risk assessment via random forests. <em>IJCIS</em>,
<em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00778-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local government financing vehicles (LGFVs), in China, are pivotal indirect financing channels for urban development projects. However, the significant debt accumulated by these vehicles presents considerable long-term risks, including challenges to fiscal sustainability, threats to financial system stability, and disruptions to regional economic development. Timely monitoring of LGFV debt risks is essential for enabling effective interventions. This study develops a robust risk evaluation system for LGFVs by leveraging multi-source data and employing the Random Forest (RF) machine learning algorithm. We collected and analyzed a sample of 1584 Chinese LGFVs from a major state-owned bank. Through an examination of the mechanisms underlying LGFV debt risk and a review of relevant literature, we identified seven primary categories and 20 key risk indicators to construct our risk indicator system. After comparing several machine learning algorithms, we selected the RF algorithm to build the LGFV debt risk prediction model due to its superior performance. Our findings emphasize the External Guarantee Ratio, GDP growth rate, and proportion of the tertiary industry as critical risk indicators. The model evaluation demonstrates high accuracy, underscoring its significant potential for practical application. This study contributes to the management of local government debt risks and introduces a novel methodology with potential applicability in other areas of risk management.},
  archive      = {J_IJCIS},
  author       = {Li, Kejia and Chen, Zhen-Song},
  doi          = {10.1007/s44196-025-00778-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Leveraging multi-source data for local government financing vehicles debt risk assessment via random forests},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy decision support system for medical service quality
management in hospitals. <em>IJCIS</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s44196-025-00773-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical service quality in hospitals and clinical centres is expected to satisfy the patient’s satisfaction and the high precision diagnosis. Periodic assessment of the medical services increases the quality and the staff efficiency. Such quality assessments are analyzed using sophisticated computing techniques such as fuzzy, genetic algorithms, etc. Therefore, this article introduces a Quality Improvement Method using Fuzzy Decision Support (QIM-FDS) for periodic service enhancements in hospitals. This method acquires the diagnosis, care-taking, and environmental factors validated using the two levels of FDS. The first FDS handles the quality improvement and the second deals with recommendations. From the previous patient suggestions/complaints, the quality improvements are performed such that the varying inputs result in high service recommendations. The first FDS and the overall recommendations (from patients) towards the above considerations are accounted for in the second fuzzification process. This consideration assimilates the service demands and is the highest patient response for retaining the same level. Therefore, the fuzzification performs the different condition verification in the second decision-making. The joint FDS processes focus on delivering high-level improvements towards the considered factors.},
  archive      = {J_IJCIS},
  author       = {Cui, Hongrui and Tan, Qingli},
  doi          = {10.1007/s44196-025-00773-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A fuzzy decision support system for medical service quality management in hospitals},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction and optimization of student grades based on
genetic algorithm and graph convolutional neural networks.
<em>IJCIS</em>, <em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s44196-025-00775-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to limited support in registered courses, students frequently struggle to complete their courses in higher education institutions. To combat this, educational systems are incorporating intelligent prediction tools to help students improve their academic performance by predicting their grades. Students&#39; demographic information, past performance in the subject, and course characteristics are some of the factors used by the grade prediction system to foretell how they will do in future. Complexity and non-linearity in the analysis of inter-variable connections pose problems for traditional prediction models. Our solution to these problems is a GGCNN, or Genetic Algorithm with Graph Convolutional Neural Networks. In order to improve the accuracy of predictions, GGCNN examines educational data and finds intricate linkages. Using a graph structure, the graph convolution model emphasizes the interdependencies among academic metrics, course features, and student performance. Relationships and correlations can be better predicted with the use of this dependency metric. Use of the genetic algorithm improves the grade prediction system by optimizing the network and making better use of features. Administrators and teachers alike can find ways to boost their kids&#39; grades through the optimization process. To test how well the system performs on different measures, we utilize the Student Performance Kaggle dataset. This continues until the convergence requirements are satisfied. With Python as its implementation, the system was able to get an accuracy of 0.98% after 100 epochs and 0.97% after 1000 epochs.},
  archive      = {J_IJCIS},
  author       = {Li, Ting},
  doi          = {10.1007/s44196-025-00775-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Prediction and optimization of student grades based on genetic algorithm and graph convolutional neural networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning model leveraging time-series system call
data to detect malware attacks in virtual machines. <em>IJCIS</em>,
<em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00781-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Tenant Virtual Machine (TVM) user in the cloud may misuse its computing power to launch malware attack against other tenant VMs, Host OS, Hypervisor, or any other computing devices/resources inside the cloud environment of a Cloud Service Provider. The security solutions deployed within the TVM may not be reliable, as malware can disable them or remain undetected due to its hidden nature. Therefore, security solutions deployed outside the virtual machine are necessary. This research proposes deploying an Intrusion Detection System (IDS) at the Hypervisor layer, utilizing time series system call data and employing a Convolutional Neural Network (CNN) model to accurately detect the presence of malicious (malware) computer programs within virtual machines. The raw VMM system call traces are transformed into novel Time Series System Call patterns and utilized by a deep learning algorithm for training and building the classifier model. A deep learning model, CNN, is used to build the classifier model for detecting intrusions with high accuracy. It is capable of detecting both known and unknown malware. The CNN model is compared with machine learning algorithms for the results and discussions, and it outperforms ML algorithms in terms of intrusion detection accuracy when utilizing novel time series system call data..},
  archive      = {J_IJCIS},
  author       = {Melvin, A. Alfred Raja and Kathrine, Jaspher W. and Jeyabose, Andrew and Cenitta, D.},
  doi          = {10.1007/s44196-025-00781-z},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A deep learning model leveraging time-series system call data to detect malware attacks in virtual machines},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for cancer diagnosis in medical images: A
compendious study. <em>IJCIS</em>, <em>18</em>(1), 1–46. (<a
href="https://doi.org/10.1007/s44196-025-00772-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s world, cancer stands out as one of the most perilous diseases, caused by the uncontrolled proliferation of cells within the human body. Early detection is paramount to ensuring that patients receive the necessary medical intervention in a timely manner. Recently, deep learning techniques, particularly convolutional neural networks, have proven to be incredibly effective in developing computer-aided diagnosis systems due to their remarkable accuracy in analyzing medical images. However, the process of training these neural networks from scratch is often complex and requires significant computational resources. Transfer learning has emerged as a powerful solution to overcome this challenge. This study examines the fundamental concepts of machine learning and deep learning-based computer-aided diagnostic systems. It underscores the significant role of transfer learning in enhancing diagnostic accuracy. It also illustrates the various transfer learning models employed to diagnose various cancer forms, including skin cancer, brain tumors, breast cancer, lung cancer, leukemia, prostate cancer, bladder cancer, and cervical cancer. This paper summarizes 151 studies conducted in recent years. In the end, the article offers a thorough discussion of the research findings, overall conclusions, and directions for future work.},
  archive      = {J_IJCIS},
  author       = {Kaur, Navreet and Hans, Rahul},
  doi          = {10.1007/s44196-025-00772-0},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-46},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Transfer learning for cancer diagnosis in medical images: A compendious study},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid method using grey wolf algorithm and genetic
algorithm for IoT botnet DDoS attacks detection. <em>IJCIS</em>,
<em>18</em>(1), 1–61. (<a
href="https://doi.org/10.1007/s44196-025-00774-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is a vast network of interconnected physical objects that has improved the conditions for a computer-based physical world and improved efficiency. With the increase in communication in an IoT system, Internet security has decreased, and the most dangerous and sophisticated attacks in the IoT have emerged, i.e., DDoS and Botnet attacks. DDoS attacks are a serious threat to the availability of Internet services, especially since botnets can now be launched by almost anyone. In this situation, the use of an intrusion detection system (IDS) is essential to detect intruders and maintain the security of IoT networks. In this paper, a new IDS is proposed to detect IoT-Botnet DDoS attacks. This IDS is a new three-phase system, the first phase is related to preprocessing on the dataset and the second phase includes a new hybrid method for feature selection using filter and wrapper methods based on the Grey Wolf (GW) algorithm and genetics called GW-GA. In this method, the initial population is randomly selected and then at each stage, feature selection is done by both algorithms simultaneously and the final answer is compared and the best solutions are given as a new population to both algorithms and the third phase includes the use of machine learning and metaheuristic algorithms as classifiers. In the proposed method and to verify the performance, it is evaluated using the large BOT-IoT dataset. The results show that the proposed method significantly reduces the feature and also increases the classification accuracy compared to other methods, and the RF and Bagging algorithms have achieved a maximum recognition accuracy of 0.999. The dimensions of BOT-IoT have been reduced from 46 features to 12.},
  archive      = {J_IJCIS},
  author       = {Maazalahi, Mahdieh and Hosseini, Soodeh},
  doi          = {10.1007/s44196-025-00774-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid method using grey wolf algorithm and genetic algorithm for IoT botnet DDoS attacks detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral patterns in micro-lending: Enhancing credit risk
assessment with collaborative filtering and federated learning.
<em>IJCIS</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s44196-025-00776-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk assessment uses finance-based behavioural patterns for micro-lending purposes and organizations. The repayment behaviour and credit stability patterns are analyzed across varying repayment tenures and financed amounts. Due to limited borrower data and fluctuating financial patterns, micro-lending platforms have substantial hurdles when it comes to effectively evaluating credit risk. This article introduces a Collaborative Filtering Method using Lending Pattern Analysis (CFM-LPA). The proposed method is enhanced through collaborative federated learning, enabling the analysis of these patterns. This approach evaluates the return rate, credit limit, and consumer response behaviours. Federated learning processes one or more of these factors to assess diverse lending patterns. Based on these evaluations, the behavioural factor is updated for each return period, influencing the credit risk for subsequent return periods and supporting the financial stability of micro-lending operations. The model is trained individually on the identified factors, allowing the behavioural factor to be filtered. New credit risks are identified using this filtered factor from the previous return period. These insights help define new behavioural patterns for the specified credit limit. The proposed method enhances risk detection accuracy by 14.03% and improves return rate analysis by 13.28% across financed amounts. The above abstract is also graphically presented.},
  archive      = {J_IJCIS},
  author       = {Aldrees, Asma and Shahab, Sana and Dutta, Ashit Kumar and Ahmad, Waseem and Anjum, Mohd},
  doi          = {10.1007/s44196-025-00776-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Behavioral patterns in micro-lending: Enhancing credit risk assessment with collaborative filtering and federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSegNet: A multi-view coupled cross-modal attention model
for enhanced MRI brain tumor segmentation. <em>IJCIS</em>,
<em>18</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s44196-025-00787-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor incidence and mortality rates are increasing due to unique location and treatment challenges. Early detection, robust diagnosis, and prompt treatment are crucial for better clinical evaluations. However, traditional neural network-based diagnostic methods often overlook issues such as variation in multimodality information, loss of spatial information, and under-utilization of boundary information. This study presents the Multi-View Coupled Cross-Modal Attention Network (MSegNet), a novel Transformer-based segmentation framework that integrates cross-modal attention mechanisms and a multi-view architecture. MSegNet is designed to exploit multimodal MRI data’s spatial and depth dimensions, effectively capturing nuanced intermodal relationships and modeling long-range dependencies. The proposed framework also employs three data augmentation methods, which help prevent overfitting and improve the performance of segmentation network training, enhancing the model’s robustness and generalizability. The proposed model is validated using BraTS2019, BraTS2020 and Figshate brain datasets and is compared against three state-of-the-art 3D segmentation networks. Extensive experiments, including ablation studies and hyperparameter sensitivity analyses, highlight MSegNet’s robust performance. The dice scores for the whole tumor (WT), tumor core (TC) and enhancing tumor (ET) regions improved by 13. 96%, 12. 39%, and 11. 83%, respectively, while the Hausdorff distances were reduced by 3.64 mm, 2.98 mm, and 14.72 mm. These results demonstrate the model’s efficacy in enhancing segmentation precision, making it a valuable tool for clinical diagnosis and treatment planning.},
  archive      = {J_IJCIS},
  author       = {Wang, Yu and Xu, Juan and Guan, Yucheng and Ahmad, Faizan and Mahmood, Tariq and Rehman, Amjad},
  doi          = {10.1007/s44196-025-00787-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {MSegNet: A multi-view coupled cross-modal attention model for enhanced MRI brain tumor segmentation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual feature-based intrusion detection system for IoT
network security. <em>IJCIS</em>, <em>18</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s44196-025-00790-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has enabled widespread connectivity of smart devices but remains susceptible to cyber intrusions. In this research, a novel dual feature optimization using deep learning network for intrusion Detection (FOUND) technique has been proposed for enhancing security in IoT environments. The proposed method utilizes the bald eagle search (BES) algorithm and butterfly optimization algorithm (BOA) to capture both flow and packet level features to enhance the accuracy of the intrusion detection process. Moreover, a multi-head attention-based bidirectional gated recurrent unit (MHA-BiGRU) is utilized to classify Attack and Non-Attack classes with high precision. The efficacy of the suggested approach is measured utilizing metrics including recall (RC), accuracy, precision (PR), and f1score (F1S). Experimental outcomes utilizing BoT-IoT and UNSW-NB15 datasets demonstrate greater accuracy over existing models. In BoT-IoT, the accuracy of the FOUND approach is 1.5%, 1.1%, and 2.5% increase compared to existing GRU, RNN, and GCN methods, respectively.},
  archive      = {J_IJCIS},
  author       = {Biju, A. and Franklin, S. Wilfred},
  doi          = {10.1007/s44196-025-00790-y},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Dual feature-based intrusion detection system for IoT network security},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved transformer-based model for urban pedestrian
detection. <em>IJCIS</em>, <em>18</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s44196-025-00791-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is a crucial task in computer vision, applicable in object tracking, video surveillance, and autonomous driving. Recent years have witnessed substantial advancements in pedestrian detection due to the fast evolution of deep learning in object detection. Nonetheless, obstacles such as inadequate detection accuracy persist, mostly because of varied pedestrian postures and intricate environments. This study proposes the RT-DETR-improved model to overcome these issues based on the real-time detection transformer (RT-DETR). First, we incorporate the high-low frequency (HiLo) attention into the encoder, therefore enhancing the model’s detection performance. Furthermore, we present a nonlinear feature fusion module that fuses information from various feature scales and contexts more successfully. We also introduce a novel loss function, InnerMPDIoU, to enhance detection efficacy in congested environments. To evaluate our model’s performance, extensive experiments are conducted on the CityPersons dataset. Compared to the baseline model, the RT-DETR-improved model attains a 4.2% enhancement in mAP50, a 2.0% improvement in mAP, a 2.2% rise in accuracy, and a 3.1% gain in recall. The results demonstrate that the proposed method exhibits superior detection accuracy and robustness.},
  archive      = {J_IJCIS},
  author       = {Wu, Tianyong and Li, Xiang and Dong, Qiuxuan},
  doi          = {10.1007/s44196-025-00791-x},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {An improved transformer-based model for urban pedestrian detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraitBertGCN: Personality trait prediction using BertGCN
with data fusion technique. <em>IJCIS</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s44196-025-00792-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality prediction via different techniques is an established and trending topic in psychology. The advancement of machine learning algorithms in multiple fields also attracted the attention of Automatic Personality Prediction (APP). This research proposes a novel TraitBertGCN method with a data fusion technique for predicting personality traits. Initially, this work integrates a pre-trained language model, Bidirectional Encoder Representations from Transformers (BERT), with a three-layer Graph Convolutional Network (GCN) to leverage large-scale language understanding and graph-based learning for personality prediction. This study fuses the two datasets (essays and myPersonality) to overcome the bias and generalize the model across different domains. We fine-tuned our TraitBertGCN model on the fused dataset and then evaluated it on both datasets individually to assess its adaptability and accuracy in varied contexts. We compared the proposed model’s results with previous studies; our model achieved better performance in personality trait prediction across multiple datasets, with an average accuracy of 77.42% on the essays dataset and 87.59% on the myPersonality dataset.},
  archive      = {J_IJCIS},
  author       = {Waqas, Muhammad and Zhang, Fengli and Laghari, Asif Ali and Almadhor, Ahmad and Petrinec, Filip and Iqbal, Asif and Khalil, Mian Muhammad Yasir},
  doi          = {10.1007/s44196-025-00792-w},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {TraitBertGCN: Personality trait prediction using BertGCN with data fusion technique},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid differential evolutionary algorithm for
solving multi-objective distributed permutation flow-shop scheduling
problem. <em>IJCIS</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s44196-025-00793-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Distributed Permutation Flow-Shop Scheduling Problem (DPFSP) is a classic issue in distributed scheduling that involves job allocation and processing order within a factory, and it is known to be NP-hard. Numerous researchers have proposed various intelligent optimization algorithms to address the DPFSP; however, there are fewer studies related to the multi-objective DPFSP problem, and the algorithms for solving this problem also suffer from poor solution quality and tend to fall into local optimization and so on. To tackle the multi-objective DPFSP, this paper proposes a novel hybrid differential evolutionary algorithm aimed at minimizing both the maximum completion time and total delay time. In this algorithm, Bernoulli chaotic mapping is applied during the population initialization process to enhance the diversity of the initial population. Additionally, an adaptive mutation factor and crossover rate are designed to balance the global and local search capabilities of the algorithm. Furthermore, a novel selection strategy is constructed based on the NEH algorithm, specular reflection learning, and Pareto dominance relation to improve the quality of the solution set when solving instances of varying sizes. This strategy enhances the algorithm&#39;s optimization ability and helps it escape local optima. The effectiveness and superiority of the proposed algorithm are verified through 24 instances of different sizes. The results demonstrate that the proposed algorithm outperforms other improved algorithms in terms of convergence, and the uniformity and diversity of the solution set, making it an effective solution for the multi-objective distributed permutation flow-shop scheduling problem.},
  archive      = {J_IJCIS},
  author       = {Du, Xinzhe and Zhou, Yanping},
  doi          = {10.1007/s44196-025-00793-9},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A novel hybrid differential evolutionary algorithm for solving multi-objective distributed permutation flow-shop scheduling problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on cascade propagation of collaborative innovation
risks in industrial clusters considering entities heterogeneity.
<em>IJCIS</em>, <em>18</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s44196-025-00795-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this study is to effectively mitigate the cascading propagation of collaborative innovation risks within industrial clusters and bolster the stability of their innovation networks. Drawing upon the cascade failure theory in complex networks, we employ the BA scale-free network model to construct an industrial cluster innovation network. We develop a cascade propagation model for collaborative innovation risk, addressing three dimensions: risk load, risk capacity, and load redistribution following node failures. To enhance the model, we propose a risk load distribution strategy that considers the heterogeneity among innovation entities, focusing on the similarity degree, importance degree, and cooperation degree of neighboring innovative entities. Through simulation experiments, we demonstrate that the integrated allocation strategy significantly improves the resistance to destruction of industrial cluster innovation networks. However, under intentional attacks, the resilience of these networks remains relatively weak. Further investigation reveals that the risk load capacity enhanced by the integrated allocation strategy can somewhat fortify the resistance of industrial cluster innovation networks to such attacks. The findings offer valuable insights for risk management and stability enhancement in industrial cluster innovation networks.},
  archive      = {J_IJCIS},
  author       = {Shi, Xiaowei and Wang, Jifa and Wang, Yang},
  doi          = {10.1007/s44196-025-00795-7},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Research on cascade propagation of collaborative innovation risks in industrial clusters considering entities heterogeneity},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better pseudo-labeling for semi-supervised domain
generalization in medical magnetic resonance image segmentation.
<em>IJCIS</em>, <em>18</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s44196-025-00786-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance image (MRI) is the primary diagnostic test used clinically for the diagnosis and assessment of a wide range of diseases. In recent years, many studies have employed artificial intelligence techniques for MRI segmentation. Deep learning methods have demonstrated potential to enhance segmentation performance. However, they still face two challenges: annotation scarcity and domain shift. The annotation of MRI is both challenging and costly, and well-annotated datasets are scarce and valuable. Moreover, due to variations in MRI machines, ensuring the independence and identical distribution between model training data and real-world data is difficult, which may lead to noisy model predictions and weak generalization ability. We aim to address the challenges through a multi-pronged approach. First, we propose a method that integrates confidence and uncertainty for generating reliable pseudo-labels. Second, we introduce a consistency learning method that employs self-perturbation at both the image and feature levels to encourage the learning of more generalized feature representations. Finally, we optimize pseudo-labels end-to-end with the teacher–student framework. To evaluate the effectiveness of our method, we conduct experiments on six different MRI segmentation datasets. The results showed that our method was superior to the existing methods in DSC, ASD and HD95 metrics. In addition, we evaluated the quality and quantity of the generated pseudo-labels, and the results showed that our method generated better pseudo-labels than other methods. Overall, our proposed method shows promising potential in assisting clinicians in practical applications.},
  archive      = {J_IJCIS},
  author       = {Hu, Liangqing and Meng, Zuqiang and Tan, Chaohong and Zhou, Yumin},
  doi          = {10.1007/s44196-025-00786-8},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Better pseudo-labeling for semi-supervised domain generalization in medical magnetic resonance image segmentation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A greedy constructive heuristic for solving the team
orienteering problem with variable time windows. <em>IJCIS</em>,
<em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s44196-025-00797-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orienteering problems are a subclass of routing problems, in which a selection of the set of nodes should be made for visiting, due to route length restrictions. These nodes can also impose time-window constraints, which can be variable if they are defined by a spread process, which behavior can be modified. The problem including all these features is the Team Orienteering Problem with Variable Time Windows (TOPVTW). In this paper, deterministic and randomized greedy constructive heuristic schemes are developed for solving the problem, along with the definition of some metrics that guide the constructive processes. One of the heuristics is combined with an existing exact mixed integer programming model to improve the outputs. All the solving strategies proposed are tested with instances representing the spread of a wildfire in a landscape, demonstrating improvements in performance when compared with existing exact solving methodologies.},
  archive      = {J_IJCIS},
  author       = {Granda, Bibiana and Vitoriano, Begoña},
  doi          = {10.1007/s44196-025-00797-5},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {A greedy constructive heuristic for solving the team orienteering problem with variable time windows},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance assignment coverage feature for operation control
of SAT solver. <em>IJCIS</em>, <em>18</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s44196-025-00798-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Conflict-Driven Clause Learning (CDCL) framework integrates multiple heuristic components to solve Boolean satisfiability (SAT) problems through synergistic cooperation. Understanding the characteristics of these components in the underlying architecture provides crucial insights for designing corresponding methods to enhance the performance of CDCL solvers. Although numerous studies from diverse perspectives have been conducted, there remains a need to develop efficient methods and algorithms to meet the requirements for enhancing the performance efficiency of SAT solving. In this paper, we introduced two fundamental innovations: deep restart, a strategic reset mechanism that clears variable activity states while preserving learned clauses and making phase randomization, and assignment coverage time (CoverT), a novel metric quantifying the minimum—conflict count required to assign all variables at least once during search exploration. The CoverT metric provided unique insight into the characteristics of the instance structure, allowing dynamic adaptation of branching heuristics in our proposed Deep Restart-Enhanced Conflict-Driven Clause Learning algorithm framework (DR-CDCL). Experimental validation in 2021–2023 SAT Competition benchmarks demonstrated statistically significant improvements: Notably, the performance trade-off analysis revealed that while deep restart enhances solution diversity for satisfiable instances, it introduced a 2.1% overhead on unsatisfiable proofs due to clause learning pattern disruption, a phenomenon requiring further investigation. This work advances solver architecture design by establishing formal connections between exploratory search patterns and instance structural complexity. The implemented solution prototype and benchmark data are publicly available to facilitate reproducibility.},
  archive      = {J_IJCIS},
  author       = {Li, Zhihui and Chen, Shuwei and Wu, Guanfeng and Xu, Yang},
  doi          = {10.1007/s44196-025-00798-4},
  journal      = {International Journal of Computational Intelligence Systems},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Int. J. Comput. Intell. Syst.},
  title        = {Instance assignment coverage feature for operation control of SAT solver},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijcv---25">IJCV - 25</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Variational rectification inference for learning
with noisy labels. <em>IJCV</em>, <em>133</em>(3), 1434. (<a
href="https://doi.org/10.1007/s11263-024-02242-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Sun, Haoliang and Wei, Qi and Feng, Lei and Hu, Yupeng and Liu, Fan and Fan, Hehe and Yin, Yilong},
  doi          = {10.1007/s11263-024-02242-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1434},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Variational rectification inference for learning with noisy labels},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on computer vision approaches
for animal tracking and modeling 2023. <em>IJCV</em>, <em>133</em>(3),
1433. (<a href="https://doi.org/10.1007/s11263-024-02241-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-024-02241-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1433},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on computer vision approaches for animal tracking and modeling 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on german conference on pattern
recognition (DAGM GCPR). <em>IJCV</em>, <em>133</em>(3), 1432. (<a
href="https://doi.org/10.1007/s11263-024-02212-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Goldluecke, Bastian},
  doi          = {10.1007/s11263-024-02212-6},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1432},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on german conference on pattern recognition (DAGM GCPR)},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSKNet: A foundation lightweight backbone for remote
sensing. <em>IJCV</em>, <em>133</em>(3), 1410–1431. (<a
href="https://doi.org/10.1007/s11263-024-02247-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection, semantic segmentation and change detection, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet .},
  archive      = {J_IJCV},
  author       = {Li, Yuxuan and Li, Xiang and Dai, Yimain and Hou, Qibin and Liu, Li and Liu, Yongxiang and Cheng, Ming-Ming and Yang, Jian},
  doi          = {10.1007/s11263-024-02247-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1410-1431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LSKNet: A foundation lightweight backbone for remote sensing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified frequency-assisted transformer framework for
detecting and grounding multi-modal manipulation. <em>IJCV</em>,
<em>133</em>(3), 1392–1409. (<a
href="https://doi.org/10.1007/s11263-024-02245-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and grounding multi-modal media manipulation ( $$\hbox {DGM}^4$$ ) has become increasingly crucial due to the widespread dissemination of face forgery and text misinformation. In this paper, we present the Unified Frequency-Assisted transFormer framework, named UFAFormer, to address the $$\hbox {DGM}^4$$ problem. Unlike previous state-of-the-art methods that solely focus on the image (RGB) domain to describe visual forgery features, we additionally introduce the frequency domain as a complementary viewpoint. By leveraging the discrete wavelet transform, we decompose images into several frequency sub-bands, capturing rich face forgery artifacts. Then, our proposed frequency encoder, incorporating intra-band and inter-band self-attentions, explicitly aggregates forgery features within and across diverse sub-bands. Moreover, to address the semantic conflicts between image and frequency domains, the forgery-aware mutual module is developed to further enable the effective interaction of disparate image and frequency features, resulting in aligned and comprehensive visual forgery representations. Finally, based on visual and textual forgery features, we propose a unified decoder that comprises two symmetric cross-modal interaction modules responsible for gathering modality-specific forgery information, along with a fusing interaction module for aggregation of both modalities. The proposed unified decoder formulates our UFAFormer as a unified framework, ultimately simplifying the overall architecture and facilitating the optimization process. Experimental results on the $$\hbox {DGM}^4$$ dataset, containing several perturbations, demonstrate the superior performance of our framework compared to previous methods, setting a new benchmark in the field.},
  archive      = {J_IJCV},
  author       = {Liu, Huan and Tan, Zichang and Chen, Qiang and Wei, Yunchao and Zhao, Yao and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02245-x},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1392-1409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unified frequency-assisted transformer framework for detecting and grounding multi-modal manipulation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-VLGM: Bi-level class-severity-aware vision-language graph
matching for text guided medical image segmentation. <em>IJCV</em>,
<em>133</em>(3), 1375–1391. (<a
href="https://doi.org/10.1007/s11263-024-02246-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be bridged by vision-language matching (VLM). However, current vision-language models distort the intra-model relation and only include class information in reports that is insufficient for segmentation task. In this paper, we introduce a novel Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation, composed of a word-level VLGM module and a sentence-level VLGM module, to exploit the class-severity-aware relation among visual-textual features. In word-level VLGM, to mitigate the distorted intra-modal relation during VLM, we reformulate VLM as graph matching problem and introduce a vision-language graph matching (VLGM) to exploit the high-order relation among visual-textual features. Then, we perform VLGM between the local features for each class region and class-aware prompts to bridge their gap. In sentence-level VLGM, to provide disease severity information for segmentation task, we introduce a severity-aware prompting to quantify the severity level of disease lesion, and perform VLGM between the global features and the severity-aware prompts. By exploiting the relation between the local (global) and class (severity) features, the segmentation model can include the class-aware and severity-aware information to promote segmentation performance. Extensive experiments proved the effectiveness of our method and its superiority to existing methods. The source code will be released.},
  archive      = {J_IJCV},
  author       = {Chen, Wenting and Liu, Jie and Liu, Tianming and Yuan, Yixuan},
  doi          = {10.1007/s11263-024-02246-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1375-1391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bi-VLGM: Bi-level class-severity-aware vision-language graph matching for text guided medical image segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MapTRv2: An end-to-end framework for online vectorized HD
map construction. <em>IJCV</em>, <em>133</em>(3), 1352–1374. (<a
href="https://doi.org/10.1007/s11263-024-02235-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present Map TRansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at https://github.com/hustvl/MapTR for facilitating further studies and applications.},
  archive      = {J_IJCV},
  author       = {Liao, Bencheng and Chen, Shaoyu and Zhang, Yunchi and Jiang, Bo and Zhang, Qian and Liu, Wenyu and Huang, Chang and Wang, Xinggang},
  doi          = {10.1007/s11263-024-02235-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1352-1374},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MapTRv2: An end-to-end framework for online vectorized HD map construction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissecting out-of-distribution detection and open-set
recognition: A critical analysis of methods and benchmarks.
<em>IJCV</em>, <em>133</em>(3), 1326–1351. (<a
href="https://doi.org/10.1007/s11263-024-02222-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting test-time distribution shift has emerged as a key capability for safely deployed machine learning models, with the question being tackled under various guises in recent years. In this paper, we aim to provide a consolidated view of the two largest sub-fields within the community: out-of-distribution (OOD) detection and open-set recognition (OSR). In particular, we aim to provide rigorous empirical analysis of different methods across settings and provide actionable takeaways for practitioners and researchers. Concretely, we make the following contributions: (i) We perform rigorous cross-evaluation between state-of-the-art methods in the OOD detection and OSR settings and identify a strong correlation between the performances of methods for them; (ii) We propose a new, large-scale benchmark setting which we suggest better disentangles the problem tackled by OOD detection and OSR, re-evaluating state-of-the-art OOD detection and OSR methods in this setting; (iii) We surprisingly find that the best performing method on standard benchmarks (Outlier Exposure) struggles when tested at scale, while scoring rules which are sensitive to the deep feature magnitude consistently show promise; and (iv) We conduct empirical analysis to explain these phenomena and highlight directions for future research. Code: https://github.com/Visual-AI/Dissect-OOD-OSR},
  archive      = {J_IJCV},
  author       = {Wang, Hongjun and Vaze, Sagar and Han, Kai},
  doi          = {10.1007/s11263-024-02222-4},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1326-1351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dissecting out-of-distribution detection and open-set recognition: A critical analysis of methods and benchmarks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2D semantic-guided semantic scene completion. <em>IJCV</em>,
<em>133</em>(3), 1306–1325. (<a
href="https://doi.org/10.1007/s11263-024-02244-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic scene completion (SSC) aims to simultaneously perform scene completion (SC) and predict semantic categories of a 3D scene from a single depth and/or RGB image. Most existing SSC methods struggle to handle complex regions with multiple objects close to each other, especially for objects with reflective or dark surfaces. This primarily stems from two challenges: (1) the loss of geometric information due to the unreliability of depth values from sensors, and (2) the potential for semantic confusion when simultaneously predicting 3D shapes and semantic labels. To address these problems, we propose a Semantic-guided Semantic Scene Completion framework, dubbed SG-SSC, which involves Semantic-guided Fusion (SGF) and Volume-guided Semantic Predictor (VGSP). Guided by 2D semantic segmentation maps, SGF adaptively fuses RGB and depth features to compensate for the missing geometric information caused by the missing values in depth images, thus performing more robustly to unreliable depth information. VGSP exploits the mutual benefit between SC and SSC tasks, making SSC more focused on predicting the categories of voxels with high occupancy probabilities and also allowing SC to utilize semantic priors to better predict voxel occupancy. Experimental results show that SG-SSC outperforms existing state-of-the-art methods on the NYU, NYUCAD, and SemanticKITTI datasets. Models and code are available at https://github.com/aipixel/SG-SSC .},
  archive      = {J_IJCV},
  author       = {Liu, Xianzhu and Xie, Haozhe and Zhang, Shengping and Yao, Hongxun and Ji, Rongrong and Nie, Liqiang and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02244-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1306-1325},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {2D semantic-guided semantic scene completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From gaze jitter to domain adaptation: Generalizing gaze
estimation by manipulating high-frequency components. <em>IJCV</em>,
<em>133</em>(3), 1290–1305. (<a
href="https://doi.org/10.1007/s11263-024-02233-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze, as a pivotal indicator of human emotion, plays a crucial role in various computer vision tasks. However, the accuracy of gaze estimation often significantly deteriorates when applied to unseen environments, thereby limiting its practical value. Therefore, enhancing the generalizability of gaze estimators to new domains emerges as a critical challenge. A common limitation in existing domain adaptation research is the inability to identify and leverage truly influential factors during the adaptation process. This shortcoming often results in issues such as limited accuracy and unstable adaptation. To address this issue, this article discovers a truly influential factor in the cross-domain problem, i.e., high-frequency components (HFC). This discovery stems from an analysis of gaze jitter-a frequently overlooked but impactful issue where predictions can deviate drastically even for visually similar input images. Inspired by this discovery, we propose an “embed-then-suppress&quot; HFC manipulation strategy to adapt gaze estimation to new domains. Our method first embeds additive HFC to the input images, then performs domain adaptation by suppressing the impact of HFC. Specifically, the suppression is carried out in a contrasive manner. Each original image is paired with its HFC-embedded version, thereby enabling our method to suppress the HFC impact through contrasting the representations within the pairs. The proposed method is evaluated across four cross-domain gaze estimation tasks. The experimental results show that it not only enhances gaze estimation accuracy but also significantly reduces gaze jitter in the target domain. Compared with previous studies, our method offers higher accuracy, reduced gaze jitter, and improved adaptation stability, marking the potential for practical deployment.},
  archive      = {J_IJCV},
  author       = {Liu, Ruicong and Wang, Haofei and Lu, Feng},
  doi          = {10.1007/s11263-024-02233-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1290-1305},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From gaze jitter to domain adaptation: Generalizing gaze estimation by manipulating high-frequency components},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEO: Generative latent image animator for human video
synthesis. <em>IJCV</em>, <em>133</em>(3), 1277–1289. (<a
href="https://doi.org/10.1007/s11263-024-02231-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing. Project page: https://wyhsirius.github.io/LEO-project/ .},
  archive      = {J_IJCV},
  author       = {Wang, Yaohui and Ma, Xin and Chen, Xinyuan and Chen, Cunjian and Dantcheva, Antitza and Dai, Bo and Qiao, Yu},
  doi          = {10.1007/s11263-024-02231-3},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1277-1289},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LEO: Generative latent image animator for human video synthesis},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual prompt leaning for vision language models.
<em>IJCV</em>, <em>133</em>(3), 1258–1276. (<a
href="https://doi.org/10.1007/s11263-024-02243-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and visual sub-branches, existing prompt approaches are mainly divided into text and visual prompts. Recent text prompt methods have achieved great performance by designing input-condition prompts that encompass both text and image domain knowledge. However, roughly incorporating the same image feature into each learnable text token may be unjustifiable, as it could result in learnable text prompts being concentrated on one or a subset of characteristics. In light of this, we propose a fine-grained text prompt (FTP) that decomposes the single global image features into several finer-grained semantics and incorporates them into corresponding text prompt tokens. On the other hand, current methods neglect valuable text semantic information when building the visual prompt. Furthermore, text information contains redundant and negative category semantics. To address this, we propose a text-reorganized visual prompt (TVP) that reorganizes the text descriptions of the current image to construct the visual prompt, guiding the image branch to attend to class-related representations. By leveraging both FTP and TVP, we enable mutual prompting between the text and visual modalities, unleashing their potential to tap into the representation capabilities of VLMs. Extensive experiments on 11 classification benchmarks show that our method surpasses existing methods by a large margin. In particular, our approach improves recent state-of-the-art CoCoOp by 4.79% on new classes and 3.88% on harmonic mean over eleven classification benchmarks.},
  archive      = {J_IJCV},
  author       = {Long, Sifan and Zhao, Zhen and Yuan, Junkun and Tan, Zichang and Liu, Jiangjiang and Feng, Jingyuan and Wang, Shengsheng and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02243-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1258-1276},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mutual prompt leaning for vision language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust deep object tracking against adversarial attacks.
<em>IJCV</em>, <em>133</em>(3), 1238–1257. (<a
href="https://doi.org/10.1007/s11263-024-02226-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the vulnerability of deep neural networks (DNNs) has attracted significant attention in recent years. While recent studies on adversarial attack and defense mainly reside in a single image, few efforts have been made to perform temporal attacks against video sequences. As the temporal consistency between frames is not considered, existing adversarial attack approaches designed for static images do not perform well for deep object tracking. In this work, we generate adversarial examples on top of video sequences to improve the tracking robustness against adversarial attacks under white-box and black-box settings. To this end, we consider motion signals when generating lightweight perturbations over the estimated tracking results frame-by-frame. For the white-box attack, we generate temporal perturbations via known trackers to degrade significantly the tracking performance. We transfer the generated perturbations into unknown targeted trackers for the black-box attack to achieve transferring attacks. Furthermore, we train universal adversarial perturbations and directly add them into all frames of videos, improving the attack effectiveness with minor computational costs. On the other hand, we sequentially learn to estimate and remove the perturbations from input sequences to restore the tracking performance. We apply the proposed adversarial attack and defense approaches to state-of-the-art tracking algorithms. Extensive evaluations on large-scale benchmark datasets, including OTB, VOT, UAV123, and LaSOT, demonstrate that our attack method degrades the tracking performance significantly with favorable transferability to other backbones and trackers. Notably, the proposed defense method restores the original tracking performance to some extent and achieves additional performance gains when not under adversarial attacks.},
  archive      = {J_IJCV},
  author       = {Jia, Shuai and Ma, Chao and Song, Yibing and Yang, Xiaokang and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-024-02226-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1238-1257},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust deep object tracking against adversarial attacks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slimmable networks for contrastive self-supervised learning.
<em>IJCV</em>, <em>133</em>(3), 1222–1237. (<a
href="https://doi.org/10.1007/s11263-024-02211-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning makes significant progress in pre-training large models, but struggles with small models. Mainstream solutions to this problem rely mainly on knowledge distillation, which involves a two-stage procedure: first training a large teacher model and then distilling it to improve the generalization ability of smaller ones. In this work, we introduce another one-stage solution to obtain pre-trained small models without the need for extra teachers, namely, slimmable networks for contrastive self-supervised learning (SlimCLR). A slimmable network consists of a full network and several weight-sharing sub-networks, which can be pre-trained once to obtain various networks, including small ones with low computation costs. However, interference between weight-sharing networks leads to severe performance degradation in self-supervised cases, as evidenced by gradient magnitude imbalance and gradient direction divergence. The former indicates that a small proportion of parameters produce dominant gradients during backpropagation, while the main parameters may not be fully optimized. The latter shows that the gradient direction is disordered, and the optimization process is unstable. To address these issues, we introduce three techniques to make the main parameters produce dominant gradients and sub-networks have consistent outputs. These techniques include slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes. Furthermore, theoretical results are presented to demonstrate that a single slimmable linear layer is sub-optimal during linear evaluation. Thus a switchable linear probe layer is applied during linear evaluation. We instantiate SlimCLR with typical contrastive learning frameworks and achieve better performance than previous arts with fewer parameters and FLOPs. The code is available at https://github.com/mzhaoshuai/SlimCLR .},
  archive      = {J_IJCV},
  author       = {Zhao, Shuai and Zhu, Linchao and Wang, Xiaohan and Yang, Yi},
  doi          = {10.1007/s11263-024-02211-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1222-1237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Slimmable networks for contrastive self-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking the limits of reliable prediction via generated
data. <em>IJCV</em>, <em>133</em>(3), 1195–1221. (<a
href="https://doi.org/10.1007/s11263-024-02221-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open-world recognition of safety-critical applications, providing reliable prediction for deep neural networks has become a critical requirement. Many methods have been proposed for reliable prediction related tasks such as confidence calibration, misclassification detection, and out-of-distribution detection. Recently, pre-training has been shown to be one of the most effective methods for improving reliable prediction, particularly for modern networks like ViT, which require a large amount of training data. However, collecting data manually is time-consuming. In this paper, taking advantage of the breakthrough of generative models, we investigate whether and how expanding the training set using generated data can improve reliable prediction. Our experiments reveal that training with a large quantity of generated data can eliminate overfitting in reliable prediction, leading to significantly improved performance. Surprisingly, classical networks like ResNet-18, when trained on a notably extensive volume of generated data, can sometimes exhibit performance competitive to pre-training ViT with a substantial real dataset.},
  archive      = {J_IJCV},
  author       = {Cheng, Zhen and Zhu, Fei and Zhang, Xu-Yao and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-024-02221-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1195-1221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Breaking the limits of reliable prediction via generated data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FastComposer: Tuning-free multi-subject image generation
with localized attention. <em>IJCV</em>, <em>133</em>(3), 1175–1194. (<a
href="https://doi.org/10.1007/s11263-024-02227-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300 $$\times $$ –2500 $$\times $$ speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here ( https://github.com/mit-han-lab/fastcomposer ).},
  archive      = {J_IJCV},
  author       = {Xiao, Guangxuan and Yin, Tianwei and Freeman, William T. and Durand, Frédo and Han, Song},
  doi          = {10.1007/s11263-024-02227-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1175-1194},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FastComposer: Tuning-free multi-subject image generation with localized attention},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lidar panoptic segmentation in an open world. <em>IJCV</em>,
<em>133</em>(3), 1153–1174. (<a
href="https://doi.org/10.1007/s11263-024-02166-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing Lidar Panoptic Segmentation (LPS) is crucial for safe deployment of autnomous vehicles. LPS aims to recognize and segment lidar points w.r.t. a pre-defined vocabulary of semantic classes, including thing classes of countable objects (e.g., pedestrians and vehicles) and stuff classes of amorphous regions (e.g., vegetation and road). Importantly, LPS requires segmenting individual thing instances (e.g., every single vehicle). Current LPS methods make an unrealistic assumption that the semantic class vocabulary is fixed in the real open world, but in fact, class ontologies usually evolve over time as robots encounter instances of novel classes that are considered to be unknowns w.r.t. thepre-defined class vocabulary. To address this unrealistic assumption, we study LPS in the Open World (LiPSOW): we train models on a dataset with a pre-defined semantic class vocabulary and study their generalization to a larger dataset where novel instances of thing and stuff classes can appear. This experimental setting leads to interesting conclusions. While prior art train class-specific instance segmentation methods and obtain state-of-the-art results on known classes, methods based on class-agnostic bottom-up grouping perform favorably on classes outside of the initial class vocabulary (i.e., unknown classes). Unfortunately, these methods do not perform on-par with fully data-driven methods on known classes. Our work suggests a middle ground: we perform class-agnostic point clustering and over-segment the input cloud in a hierarchical fashion, followed by binary point segment classification, akin to Region Proposal Network (Ren et al. NeurIPS, 2015). We obtain the final point cloud segmentation by computing a cut in the weighted hierarchical tree of point segments, independently of semantic classification. Remarkably, this unified approach leads to strong performance on both known and unknown classes.},
  archive      = {J_IJCV},
  author       = {Chakravarthy, Anirudh S. and Ganesina, Meghana Reddy and Hu, Peiyun and Leal-Taixé, Laura and Kong, Shu and Ramanan, Deva and Osep, Aljosa},
  doi          = {10.1007/s11263-024-02166-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1153-1174},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lidar panoptic segmentation in an open world},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical active learning for low-altitude drone-view
object detection. <em>IJCV</em>, <em>133</em>(3), 1140–1152. (<a
href="https://doi.org/10.1007/s11263-024-02228-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various object detection techniques are employed on drone platforms. However, the task of annotating drone-view samples is both time-consuming and laborious. This is primarily due to the presence of numerous small-sized instances to be labeled in the drone-view image. To tackle this issue, we propose HALD, a hierarchical active learning approach for low-altitude drone-view object detection. HALD extracts unlabeled image information sequentially from different levels, including point, box, image, and class, aiming to obtain a reliable indicator of image information. The point-level module is utilized to ascertain the valid count and location of instances, while the box-level module screens out reliable predictions. The image-level module selects candidate samples by calculating the consistency of valid boxes within an image, and the class-level module selects the final selected samples based on the distribution of candidate and labeled samples across different classes. Extensive experiments conducted on the VisDrone and CityPersons datasets demonstrate that HALD outperforms several other baseline methods. Additionally, we provide an in-depth analysis of each proposed module. The results show that the performance of evaluating the informativeness of samples can be effectively improved by the four hierarchical levels.},
  archive      = {J_IJCV},
  author       = {Hu, Haohao and Han, Tianyu and Wang, Yuerong and Zhong, Wanjun and Yue, Jingwei and Zan, Peng},
  doi          = {10.1007/s11263-024-02228-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1140-1152},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical active learning for low-altitude drone-view object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In search of lost online test-time adaptation: A survey.
<em>IJCV</em>, <em>133</em>(3), 1106–1139. (<a
href="https://doi.org/10.1007/s11263-024-02213-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive survey of online test-time adaptation (OTTA), focusing on effectively adapting machine learning models to distributionally different target data upon batch arrival. Despite the recent proliferation of OTTA methods, conclusions from previous studies are inconsistent due to ambiguous settings, outdated backbones, and inconsistent hyperparameter tuning, which obscure core challenges and hinder reproducibility. To enhance clarity and enable rigorous comparison, we classify OTTA techniques into three primary categories and benchmark them using a modern backbone, the Vision Transformer. Our benchmarks cover conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C, as well as real-world shifts represented by CIFAR-10.1, OfficeHome, and CIFAR-10-Warehouse. The CIFAR-10-Warehouse dataset includes a variety of variations from different search engines and synthesized data generated through diffusion models. To measure efficiency in online scenarios, we introduce novel evaluation metrics, including GFLOPs, wall clock time, and GPU memory usage, providing a clearer picture of the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, revealing that (1) transformers demonstrate heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods relies on large batch sizes, and (3) stability in optimization and resistance to perturbations are crucial during adaptation, particularly when the batch size is 1. Based on these insights, we highlight promising directions for future research. Our benchmarking toolkit and source code are available at https://github.com/Jo-wang/OTTA_ViT_survey .},
  archive      = {J_IJCV},
  author       = {Wang, Zixin and Luo, Yadan and Zheng, Liang and Chen, Zhuoxiao and Wang, Sen and Huang, Zi},
  doi          = {10.1007/s11263-024-02213-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1106-1139},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {In search of lost online test-time adaptation: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WeakCLIP: Adapting CLIP for weakly-supervised semantic
segmentation. <em>IJCV</em>, <em>133</em>(3), 1085–1105. (<a
href="https://doi.org/10.1007/s11263-024-02224-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) as pseudo masks, but heavily relies on inductive biases like hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a novel text-to-pixel matching paradigm for WSSS. However, directly applying CLIP to WSSS is challenging due to three critical problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the $$\frac{1}{16}$$ down-sampling resolution of ViT. Thus, we propose WeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP to WSSS. Specifically, we first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically. WeakCLIP provides an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released at https://github.com/hustvl/WeakCLIP .},
  archive      = {J_IJCV},
  author       = {Zhu, Lianghui and Wang, Xinggang and Feng, Jiapei and Cheng, Tianheng and Li, Yingyue and Jiang, Bo and Zhang, Dingwen and Han, Junwei},
  doi          = {10.1007/s11263-024-02224-2},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1085-1105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WeakCLIP: Adapting CLIP for weakly-supervised semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual face forgery detection via historical distribution
preserving. <em>IJCV</em>, <em>133</em>(3), 1067–1084. (<a
href="https://doi.org/10.1007/s11263-024-02160-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery techniques have advanced rapidly and pose serious security threats. Existing face forgery detection methods try to learn generalizable features, but they still fall short of practical application. Additionally, finetuning these methods on historical training data is resource-intensive in terms of time and storage. In this paper, we focus on a novel and challenging problem: Continual Face Forgery Detection (CFFD), which aims to efficiently learn from new forgery attacks without forgetting previous ones. Specifically, we propose a Historical Distribution Preserving (HDP) framework that reserves and preserves the distributions of historical faces. To achieve this, we use universal adversarial perturbation (UAP) to simulate historical forgery distribution, and knowledge distillation to maintain the distribution variation of real faces across different models. We also construct a new benchmark for CFFD with three evaluation protocols. Our extensive experiments on the benchmarks show that our method outperforms the state-of-the-art competitors. Our code is available at https://github.com/skJack/HDP .},
  archive      = {J_IJCV},
  author       = {Sun, Ke and Chen, Shen and Yao, Taiping and Sun, Xiaoshuai and Ding, Shouhong and Ji, Rongrong},
  doi          = {10.1007/s11263-024-02160-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1067-1084},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continual face forgery detection via historical distribution preserving},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fuzzy positive learning for annotation-scarce
semantic segmentation. <em>IJCV</em>, <em>133</em>(3), 1048–1066. (<a
href="https://doi.org/10.1007/s11263-024-02217-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation-scarce semantic segmentation aims to obtain meaningful pixel-level discrimination with scarce or even no manual annotations, of which the crux is how to utilize unlabeled data by pseudo-label learning. Typical works focus on ameliorating the error-prone pseudo-labeling, e.g., only utilizing high-confidence pseudo labels and filtering low-confidence ones out. But we think differently and resort to exhausting informative semantics from multiple probably correct candidate labels. This brings our method the ability to learn more accurately even though pseudo labels are unreliable. In this paper, we propose Adaptive Fuzzy Positive Learning (A-FPL) for correctly learning unlabeled data in a plug-and-play fashion, targeting adaptively encouraging fuzzy positive predictions and suppressing highly probable negatives. Specifically, A-FPL comprises two main components: (1) Fuzzy positive assignment (FPA) that adaptively assigns fuzzy positive labels to each pixel, while ensuring their quality through a T-value adaption algorithm (2) Fuzzy positive regularization (FPR) that restricts the predictions of fuzzy positive categories to be larger than those of negative categories. Being conceptually simple yet practically effective, A-FPL remarkably alleviates interference from wrong pseudo labels, progressively refining semantic discrimination. Theoretical analysis and extensive experiments on various training settings with consistent performance gain justify the superiority of our approach. Codes are at A-FPL .},
  archive      = {J_IJCV},
  author       = {Qiao, Pengchong and Wang, Yu and Liu, Chang and Shang, Lei and Sun, Baigui and Wang, Zhennan and Zheng, Xiawu and Ji, Rongrong and Chen, Jie},
  doi          = {10.1007/s11263-024-02217-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1048-1066},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive fuzzy positive learning for annotation-scarce semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic evaluation of uncertainty calibration in
pretrained object detectors. <em>IJCV</em>, <em>133</em>(3), 1033–1047.
(<a href="https://doi.org/10.1007/s11263-024-02219-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of deep learning based computer vision, the development of deep object detection has led to unique paradigms (e.g., two-stage or set-based) and architectures (e.g., Faster-RCNN or DETR) which enable outstanding performance on challenging benchmark datasets. Despite this, the trained object detectors typically do not reliably assess uncertainty regarding their own knowledge, and the quality of their probabilistic predictions is usually poor. As these are often used to make subsequent decisions, such inaccurate probabilistic predictions must be avoided. In this work, we investigate the uncertainty calibration properties of different pretrained object detection architectures in a multi-class setting. We propose a framework to ensure a fair, unbiased, and repeatable evaluation and conduct detailed analyses assessing the calibration under distributional changes (e.g., distributional shift and application to out-of-distribution data). Furthermore, by investigating the influence of different detector paradigms, post-processing steps, and suitable choices of metrics, we deliver novel insights into why poor detector calibration emerges. Based on these insights, we are able to improve the calibration of a detector by simply finetuning its last layer.},
  archive      = {J_IJCV},
  author       = {Huseljic, Denis and Herde, Marek and Hahn, Paul and Müjde, Mehmet and Sick, Bernhard},
  doi          = {10.1007/s11263-024-02219-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1033-1047},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Systematic evaluation of uncertainty calibration in pretrained object detectors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting class-incremental learning with pre-trained
models: Generalizability and adaptivity are all you need. <em>IJCV</em>,
<em>133</em>(3), 1012–1032. (<a
href="https://doi.org/10.1007/s11263-024-02218-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. (1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. (2) Due to the distribution gap between pre-trained and downstream datasets, PTM can be further cultivated with adaptivity via model adaptation. We propose AdaPt and mERge (Aper), which aggregates the embeddings of PTM and adapted models for classifier construction. Aper is a general framework that can be orthogonally combined with any parameter-efficient tuning method, which holds the advantages of PTM’s generalizability and adapted model’s adaptivity. (3) Additionally, considering previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data overlapping, we propose four new benchmarks for assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the effectiveness of Aper with a unified and concise framework. Code is available at https://github.com/zhoudw-zdw/RevisitingCIL .},
  archive      = {J_IJCV},
  author       = {Zhou, Da-Wei and Cai, Zi-Wen and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02218-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1012-1032},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight high-speed photography built on coded exposure
and implicit neural representation of videos. <em>IJCV</em>,
<em>133</em>(3), 991–1011. (<a
href="https://doi.org/10.1007/s11263-024-02198-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for compact cameras capable of recording high-speed scenes with high resolution is steadily increasing. However, achieving such capabilities often entails high bandwidth requirements, resulting in bulky, heavy systems unsuitable for low-capacity platforms. To address this challenge, leveraging a coded exposure setup to encode a frame sequence into a blurry snapshot and subsequently retrieve the latent sharp video presents a lightweight solution. Nevertheless, restoring motion from blur remains a formidable challenge due to the inherent ill-posedness of motion blur decomposition, the intrinsic ambiguity in motion direction, and the diverse motions present in natural videos. In this study, we propose a novel approach to address these challenges by combining the classical coded exposure imaging technique with the emerging implicit neural representation for videos. We strategically embed motion direction cues into the blurry image during the imaging process. Additionally, we develop a novel implicit neural representation based blur decomposition network to sequentially extract the latent video frames from the blurry image, leveraging the embedded motion direction cues. To validate the effectiveness and efficiency of our proposed framework, we conduct extensive experiments using benchmark datasets and real-captured blurry images. The results demonstrate that our approach significantly outperforms existing methods in terms of both quality and flexibility. The code for our work is available at https://github.com/zhihongz/BDINR .},
  archive      = {J_IJCV},
  author       = {Zhang, Zhihong and Yang, Runzhao and Suo, Jinli and Cheng, Yuxiao and Dai, Qionghai},
  doi          = {10.1007/s11263-024-02198-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {991-1011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight high-speed photography built on coded exposure and implicit neural representation of videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijdar---10">IJDAR - 10</h2>
<ul>
<li><details>
<summary>
(2025). Deep learning approaches for information extraction from
visually rich documents: Datasets, challenges and methods.
<em>IJDAR</em>, <em>28</em>(1), 121–142. (<a
href="https://doi.org/10.1007/s10032-024-00493-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on Information Extraction from Visually Rich Documents, exploring how deep learning methods are applied in this field. For the purpose of comparing the performance of available resources, including datasets and methods, we first investigate an overview of the existing datasets. Then, we categorize and review published methods, highlighting their strengths and weaknesses in addressing key challenges like text recognition, layout analysis, and information fusion. This survey serves as a valuable resource for researchers and practitioners seeking to advance the field of information extraction (IE) from visually rich documents (VRD) and contribute to its real-world applications.},
  archive      = {J_IJDAR},
  author       = {Gbada, Hamza and Kalti, Karim and Mahjoub, Mohamed Ali},
  doi          = {10.1007/s10032-024-00493-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {121-142},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deep learning approaches for information extraction from visually rich documents: Datasets, challenges and methods},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based modified-EAST scene text detector:
Insights from a novel multiscript dataset. <em>IJDAR</em>,
<em>28</em>(1), 97–119. (<a
href="https://doi.org/10.1007/s10032-024-00491-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of computer vision has seen significant transformation with the emergence and advancement of deep learning models. Deep learning waves have a significant impact on scene text detection, a vital and active area in computer vision. Numerous scientific, industrial, and academic procedures make use of text analysis. Natural scene text detection is more difficult than document image text detection owing to variations in font, size, style, brightness, etc. The National Institute of Technology Jalandhar-Text Detection dataset (NITJ-TD) is a new dataset that we have put forward in this study for various text analysis tasks including text detection, text segmentation, script identification, text recognition, etc. a deep learning model that seeks to identify the text’s location within the image,which are gathered in an unrestricted setting. The system consists of an NMS to choose the best match and prevent repeated predictions, and a modified EAST to pinpoint the exact ROI in the image. To improve the model’s performance, an enhancement module is added to the fundamental Efficient and Accurate Scene Text detector (EAST). The suggested approach is contrasted in terms of text word detection in the image. Several pre-trained models are used to assign the text word to various intersections over Union (IoU) values. We made use of our NITJ-TD dataset, which is made up of 1500 photos that were gathered from various North Indian sites. Punjabi, English, and Hindi scripts can be seen on the images. We also examined the outcomes of the ICDAR-2013 benchmark dataset. On both the suggested dataset and the benchmarked dataset, our approach performed better.},
  archive      = {J_IJDAR},
  author       = {Mahajan, Shilpa and Rani, Rajneesh and Kamboj, Aman},
  doi          = {10.1007/s10032-024-00491-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {97-119},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deep learning-based modified-EAST scene text detector: Insights from a novel multiscript dataset},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D scanning based image processing technique for measuring
the sequence of intersecting lines. <em>IJDAR</em>, <em>28</em>(1),
85–96. (<a href="https://doi.org/10.1007/s10032-024-00495-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of the sequence of two intersecting lines is still an ongoing and important issue in questioned document examination. In literature, scanning electron microscopy, optical profilometry, laser profilometry, and video reflectance spectroscopy were used for analyzing the intersecting lines. In this study, a cheap and easy-to-operate 3D scanner was used to study this problem. Intersections of homogenous and heterogeneous strokes were drawn using different brands of 1.0 mm blue, black, and red ballpoint pens on conventional printing paper, i.e., 80 g/m2, and uncoated A4 white paper by two different handwriting examiners. The analysis of the 3D surface of the crossing lines shows that the bottom of the profile of the first line has a considerable trough, while the second line has comparably smaller fluctuations. The 3D scan analysis is not affected by the brands and colors of the pen and is independent of the substrate and the number of sheets lying underneath the paper. The effect of the pen pressure shows that if the pressure of the second line is less than the first one, the sequence determination becomes harder and sometimes impossible. As these cases can be eliminated before the 3D scan, the developed method is sensitive, cheap, and easy to operate.},
  archive      = {J_IJDAR},
  author       = {Asicioglu, Faruk and Gelir, Ali and Yilmaz, Aysegul Sen and De Kinder, Jan and Kadi, Omer F. and Ozdemir, Onur B. and Pekacar, Ilgim and Sasun, Ugur and Ciftci, Saltuk B. and Dayioglu, Nurten},
  doi          = {10.1007/s10032-024-00495-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {85-96},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A 3D scanning based image processing technique for measuring the sequence of intersecting lines},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fully automated processing and analysis of
construction diagrams: AI-powered symbol detection. <em>IJDAR</em>,
<em>28</em>(1), 71–84. (<a
href="https://doi.org/10.1007/s10032-024-00492-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction drawings are frequently stored in undigitised formats and consequently, their analysis requires substantial manual effort. This is true for many crucial tasks, including material takeoff where the purpose is to obtain a list of the equipment and respective amounts required for a project. Engineering drawing digitisation has recently attracted increased attention, however construction drawings have received considerably less interest compared to other types. To address these issues, this paper presents a novel framework for the automatic processing of construction drawings. Extensive experiments were performed using two state-of-the-art deep learning models for object detection in challenging high-resolution drawings sourced from industry. The results show a significant reduction in the time required for drawing analysis. Promising performance was achieved for symbol detection across various classes, with a mean average precision of 79% for the YOLO-based method and 83% for the Faster R-CNN-based method. This framework enables the digital transformation of construction drawings, improving tasks such as material takeoff and many others.},
  archive      = {J_IJDAR},
  author       = {Jamieson, Laura and Moreno-Garcia, Carlos Francisco and Elyan, Eyad},
  doi          = {10.1007/s10032-024-00492-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {71-84},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Towards fully automated processing and analysis of construction diagrams: AI-powered symbol detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based text line segmentation method for challenging
handwritten documents. <em>IJDAR</em>, <em>28</em>(1), 59–69. (<a
href="https://doi.org/10.1007/s10032-024-00488-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text line segmentation (TLS) is an essential step of the end-to-end document analysis systems. The main purpose of this step is to extract the individual text lines of any handwritten documents with high accuracy. Handwritten and historical documents mostly contain touching and overlapping characters, heavy diacritics, footnotes and side notes added over the years. In this work, we present a new TLS method based on generative adversarial networks (GAN). TLS problem is tackled as an image-to-image translation problem and the GAN model was trained to learn the spatial information between the individual text lines and their corresponding masks including the text lines. To evaluate the segmentation performance of the proposed GAN model, two challenging datasets, VML-AHTE and VML-MOC, were used. According to the qualitative and quantitative results, the proposed GAN model achieved the best segmentation accuracy on the VML-MOC dataset and showed competitive performance on the VML-AHTE dataset.},
  archive      = {J_IJDAR},
  author       = {Özşeker, İbrahim and Demir, Ali Alper and Özkaya, Ufuk},
  doi          = {10.1007/s10032-024-00488-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {59-69},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {GAN-based text line segmentation method for challenging handwritten documents},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image quality determination of palm leaf heritage documents
using integrated discrete cosine transform features with vision
transformer. <em>IJDAR</em>, <em>28</em>(1), 41–57. (<a
href="https://doi.org/10.1007/s10032-024-00490-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of Palm leaf images into various quality categories is an important step towards the digitization of these heritage documents. Manual inspection and categorization is not only laborious, time-consuming and costly but also subject to inspector’s biases and errors. This study aims to automate the classification of palm leaf document images into three different visual quality categories. A comparative analysis between various structural and statistical features and classifiers against deep neural networks is performed. VGG16, VGG19 and ResNet152v2 architectures along with a custom CNN model are used, while Discrete Cosine Transform (DCT), Grey Level Co-occurrence Matrix (GLCM), Tamura, and Histogram of Gradient (HOG) are chosen from the traditional methods. Based on these extracted features, various classifiers, namely, k-Nearest Neighbors (k-NN), multi-layer perceptron (MLP), Support Vector Machines (SVM), Decision Tree (DT) and Logistic Regression (LR) are trained and evaluated. Accuracy, precision, recall, and F1 scores are used as performance metrics for the evaluation of various algorithms. Results demonstrate that CNN embeddings and DCT features have emerged as superior features. Based on these findings, we integrated DCT with a Vision Transformer (ViT) for the document classification task. The result illustrates that this incorporation of DCT with ViT outperforms all other methods with 96% train F1 score and a test F1 score of 90%.},
  archive      = {J_IJDAR},
  author       = {Sivan, Remya and Pati, Peeta Basa and Kesiman, Made Windu Antara},
  doi          = {10.1007/s10032-024-00490-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {41-57},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Image quality determination of palm leaf heritage documents using integrated discrete cosine transform features with vision transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene text recognition: An indic perspective.
<em>IJDAR</em>, <em>28</em>(1), 31–40. (<a
href="https://doi.org/10.1007/s10032-024-00489-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring Scene Text Recognition (STR) in Indian languages is an important research domain due to its wide applications. This paper proposes a spatial attention-based model (LaSA-Net) that combines visual features and language knowledge for word recognition from scene image word segments. We augment the classical cross-entropy loss with a novel language-attunement loss that enables the model to learn valid and prevalent character sequences in the word. This enhances the model’s ability to perform zero-shot word recognition. Further, to compensate for the lack of rotational invariance in CNN based feature extraction backbone, we propose a training data augmentation strategy involving the creation of glyphs: images of individual characters of different orientations. This improves LaSA-Net’s ability to recognize words in images with curved/vertically aligned text, alleviating the need for computationally expensive preprocessing modules. Our experiments with Tamil, Malayalam, and Telugu scripts on the IIIT-ILST datasets have achieved new benchmark results and outperformed other state-of-the-art STR models.},
  archive      = {J_IJDAR},
  author       = {Vijayan, Vasanthan P. and Chanda, Sukalpa and Doermann, David and Krishnan, Narayanan C.},
  doi          = {10.1007/s10032-024-00489-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {31-40},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Scene text recognition: An indic perspective},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic floor plan analysis using a boundary
attention-based deep network. <em>IJDAR</em>, <em>28</em>(1), 19–30. (<a
href="https://doi.org/10.1007/s10032-024-00487-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floor plan is an important communication tool between architects, construction engineers, and clients for a building project. Estimation of building features from a floor plan image is often a time-consuming task. Automatic analysis of floor plan images can significantly improve work efficiency and accuracy. A few research works have been reported in the literature on automated floor image analysis. However, the scope and performance of the existing techniques are limited. In this paper, a CNN-based technique, referred to as FloorNet, is proposed for the multiclass semantic segmentation of a floor plan. The proposed FloorNet has five modules: Encoder, Room type decoder, Room boundary decoder, Multiscale room boundary attention model and Floor classification. The proposed technique is evaluated using simple brochure type and complex architectural type floor plan images. Experimental results show that the proposed technique provides an improvement of 5–11% mIoU for semantic segmentation (for 9–11 classes) compared to the state-of-the-art techniques.},
  archive      = {J_IJDAR},
  author       = {Xu, Zhongguo and Yang, Cheng and Alheejawi, Salah and Jha, Naresh and Mehadi, Syed and Mandal, Mrinal},
  doi          = {10.1007/s10032-024-00487-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {19-30},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Automatic floor plan analysis using a boundary attention-based deep network},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handwritten stenography recognition and the LION dataset.
<em>IJDAR</em>, <em>28</em>(1), 3–18. (<a
href="https://doi.org/10.1007/s10032-024-00479-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish the first baseline for handwritten stenography recognition, using the novel LION dataset, and investigate the impact of including selected aspects of stenographic theory into the recognition process. We make the LION dataset publicly available with the aim of encouraging future research in handwritten stenography recognition. A state-of-the-art text recognition model is trained to establish a baseline. Stenographic domain knowledge is integrated by transforming the target sequences into representations which approximate diplomatic transcriptions, wherein each symbol in the script is represented by its own character in the transliteration, as opposed to corresponding combinations of characters from the Swedish alphabet. Four such encoding schemes are evaluated and results are further improved by integrating a pre-training scheme, based on synthetic data. The baseline model achieves an average test character error rate (CER) of 29.81% and a word error rate (WER) of 55.14%. Test error rates are reduced significantly (p&lt; 0.01) by combining stenography-specific target sequence encodings with pre-training and fine-tuning, yielding CERs in the range of 24.5–26% and WERs of 44.8–48.2%. An analysis of selected recognition errors illustrates the challenges that the stenographic writing system poses to text recognition. This work establishes the first baseline for handwritten stenography recognition. Our proposed combination of integrating stenography-specific knowledge, in conjunction with pre-training and fine-tuning on synthetic data, yields considerable improvements. Together with our precursor study on the subject, this is the first work to apply modern handwritten text recognition to stenography. The dataset and our code are publicly available via Zenodo.},
  archive      = {J_IJDAR},
  author       = {Heil, Raphaela and Nauwerck, Malin},
  doi          = {10.1007/s10032-024-00479-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {3-18},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Handwritten stenography recognition and the LION dataset},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). International journal on document analysis and recognition
editorial leadership change. <em>IJDAR</em>, <em>28</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s10032-025-00520-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Lopresti, Daniel and Kise, Koichi and Marinai, Simone},
  doi          = {10.1007/s10032-025-00520-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {International journal on document analysis and recognition editorial leadership change},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijfs---23">IJFS - 23</h2>
<ul>
<li><details>
<summary>
(2025). Retraction note: A fuzzy-social network multi-criteria group
decision-making framework for selection of renewable energy project: A
case of china. <em>IJFS</em>, <em>27</em>(1), 305. (<a
href="https://doi.org/10.1007/s40815-024-01958-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Su, Weihua and Zhang, Le and Zeng, Shouzhen and Jin, Huanhuan},
  doi          = {10.1007/s40815-024-01958-y},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {305},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: a fuzzy-social network multi-criteria group decision-making framework for selection of renewable energy project: a case of china},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Interaction power partitioned maclaurin
symmetric mean operators under q-rung orthopair uncertain linguistic
information. <em>IJFS</em>, <em>27</em>(1), 304. (<a
href="https://doi.org/10.1007/s40815-024-01950-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Yang, Zaoli and Garg, Harish},
  doi          = {10.1007/s40815-024-01950-6},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {304},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Interaction power partitioned maclaurin symmetric mean operators under q-rung orthopair uncertain linguistic information},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Secure cloud storage for medical IoT data
using adaptive neuro-fuzzy inference system. <em>IJFS</em>,
<em>27</em>(1), 303. (<a
href="https://doi.org/10.1007/s40815-024-01949-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Mohiyuddin, Aqsa and Javed, Abdul Rehman and Chakraborty, Chinmay and Rizwan, Muhammad and Shabbir, Maryam and Nebhen, Jamel},
  doi          = {10.1007/s40815-024-01949-z},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {303},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Secure cloud storage for medical IoT data using adaptive neuro-fuzzy inference system},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Multi-criteria group decision-making
approach for express packaging recycling under interval-valued fuzzy
information: Combining objective and subjective compatibilities.
<em>IJFS</em>, <em>27</em>(1), 302. (<a
href="https://doi.org/10.1007/s40815-024-01948-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Zheng, Chengli and Zhou, Yuanyuan},
  doi          = {10.1007/s40815-024-01948-0},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {302},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: multi-criteria group decision-making approach for express packaging recycling under interval-valued fuzzy information: combining objective and subjective compatibilities},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Health safety and environment risk
assessment using an extended BWM-COPRAS approach based on g-number
theory. <em>IJFS</em>, <em>27</em>(1), 301. (<a
href="https://doi.org/10.1007/s40815-024-01947-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Ghoushchi, Saeid Jafarzadeh and Nik, Masoud Soleimani and Pourasad, Yaghoub},
  doi          = {10.1007/s40815-024-01947-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {301},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Health safety and environment risk assessment using an extended BWM-COPRAS approach based on G-number theory},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Green supply chain network optimization
under random and fuzzy environment. <em>IJFS</em>, <em>27</em>(1), 300.
(<a href="https://doi.org/10.1007/s40815-024-01946-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Yu, Zhang and Khan, Syed Abdul Rehman},
  doi          = {10.1007/s40815-024-01946-2},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {300},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: Green supply chain network optimization under random and fuzzy environment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: A new extended VIKOR approach using q-rung
orthopair fuzzy sets for sustainable enterprise risk management
assessment in manufacturing small and medium-sized enterprises.
<em>IJFS</em>, <em>27</em>(1), 299. (<a
href="https://doi.org/10.1007/s40815-024-01945-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJFS},
  author       = {Cheng, Sun and Jianfu, Sun and Alrasheedi, Melfi and Saeidi, Parvaneh and Mishra, Arunodaya Raj and Rani, Pratibha},
  doi          = {10.1007/s40815-024-01945-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {299},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Retraction note: A new extended VIKOR approach using q-rung orthopair fuzzy sets for sustainable enterprise risk management assessment in manufacturing small and medium-sized enterprises},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy portfolio with a novel power membership function based
on GARCH and black–litterman model. <em>IJFS</em>, <em>27</em>(1),
267–298. (<a href="https://doi.org/10.1007/s40815-024-01777-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a fuzzy mean-semi-absolute deviation portfolio with novel power membership functions. The portfolio return is measured by the Black–Litterman model, which combines the market’s objective information and investors’ subjective preference by the GARCH model. In addition, we use semi-absolute deviation to measure portfolio risk instead of variance, which not only reduces the computational complexity but also considers the corresponding risk lower than the expected return. Considering the investors’ psychological satisfaction with return and risk, we propose two novel power membership functions of return and risk, respectively. We fully demonstrate the shapes of membership functions with different preference parameters and medium satisfaction levels and deeply discuss monotonicity, convexity and concavity using the first- and second-order derivatives by strict mathematical proof. Furthermore, a fuzzy mean-semi-absolute deviation portfolio is proposed based on the satisfaction maximization principle and the absolute value optimization theory. Finally, we give a numerical example and present the results of neutral, pessimistic, and optimistic portfolios. In terms of Sharpe ratio and satisfaction degree, the portfolio model with our proposed novel membership functions is superior to those with S-type membership functions (Watada (2001) Dynamical aspects in fuzzy decision making. Physica, Heidelberg). Moreover, it is more effective than that with standard deviation or absolute deviation to measure risk.},
  archive      = {J_IJFS},
  author       = {Deng, Xue and Chen, Shiting},
  doi          = {10.1007/s40815-024-01777-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {267-298},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Fuzzy portfolio with a novel power membership function based on GARCH and Black–Litterman model},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELECTRE TRI-c with hesitant fuzzy sets and interval type 2
trapezoidal fuzzy numbers using stochastic parameters: Application to a
brazilian electrical power company problem. <em>IJFS</em>,
<em>27</em>(1), 250–266. (<a
href="https://doi.org/10.1007/s40815-024-01775-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ELECTRE TRI-C is a method for sorting problems with imprecise evaluations and stable criteria weights, typically for a single decision-maker. While some extensions have addressed uncertain criteria weights and outranking functions using hesitant fuzzy sets (HFS) and interval type 2 trapezoidal fuzzy numbers (IT2TrfN), there is a gap in handling situations where multiple decision-makers provide uncertain information. This paper presents an extension of the ELECTRE TRI-C method incorporating a stochastic framework to model HFS and IT2TrfN, thereby accommodating subjective judgments from multiple decision-makers. The extended method was validated by sorting 49 projects based on their criticality in a Brazilian electrical power company, involving three decision-makers. The application shows strong correlations in project rankings among decision-makers, but with some exceptions. However, significant variations in acceptability ratings for sorting among decision-makers lead to notable error dispersion, highlighting differences between ranking and sorting outcomes. The key contributions of our approach are as follows: (1) Integration of subjective judgments from multiple decision-makers using IT2TrFN and Monte Carlo Simulation for constructing outranking functions; (2) Aggregation of preferences from multiple decision-makers using HFS; (3) Stochastic processing of both quantitative and qualitative criteria; (4) Integration of linear equations to represent weight constraints; and (5) Introduction of a novel visualization method for comprehensive analysis of stochastic results, enhancing robustness analysis. The proposal’s advantages over alternative methods are also highlighted.},
  archive      = {J_IJFS},
  author       = {Pereira, Javier and de Oliveira, Elaine C. B. and Morais, Danielle C. and Costa, Ana Paula C. S. and Alencar, Luciana H.},
  doi          = {10.1007/s40815-024-01775-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {250-266},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {ELECTRE TRI-C with hesitant fuzzy sets and interval type 2 trapezoidal fuzzy numbers using stochastic parameters: Application to a brazilian electrical power company problem},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting turkey’s primary energy demand based on fuzzy
auto-regressive distributed lag models with symmetric and non-symmetric
triangular coefficients. <em>IJFS</em>, <em>27</em>(1), 237–249. (<a
href="https://doi.org/10.1007/s40815-024-01773-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to guide policymakers in allocating resources and planning for the future by consistently estimating energy data trends. Because of the complexity and uncertainty of energy demand behavior and many influencing factors, we decide to take advantage of a fuzzy regression model to determine the actual relationships in the energy demand system and provide an accurate forecast of energy demand. For this purpose, because of energy demand drivers, fuzzy possibilistic approaches with symmetric and non-symmetric triangular coefficients are integrated with the autoregressive distributed lag (ARDL) model, each in a time-series format with feedback mechanisms inside. After regularizing the L1 (Lasso regression) and L2 (ridge regression) metrics to minimize the overfitting problem, the optimal fuzzy-ARDL model is obtained. Turkey’s primary energy consumption is projected based on the best model by benchmarking the static and dynamic possibilistic fuzzy regression models according to their training and test values.},
  archive      = {J_IJFS},
  author       = {Eren, Miraç and Baets, Bernard De},
  doi          = {10.1007/s40815-024-01773-5},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {237-249},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Forecasting turkey’s primary energy demand based on fuzzy auto-regressive distributed lag models with symmetric and non-symmetric triangular coefficients},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Associated probabilities’ aggregations in fuzzy
collaborative filtering recommender system prediction. <em>IJFS</em>,
<em>27</em>(1), 204–236. (<a
href="https://doi.org/10.1007/s40815-024-01772-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the expert evaluations transformed in the multi-attribute decision-making (MADM) model are presented in the discrimination q-rung picture linguistic fuzzy numbers (q-RPLFNs). In the construction of a second-order additive fuzzy measure (TOAFM) the attributes’ interaction indexes and Shapley values are taken into account. The Shapley entropy maximum principle for identification of associated probabilities class (APC) of a TOAFM is constructed. Based on the APC of the TOAFM, a new aggregation operators’ class is constructed which represents some hybrid extensions of ordered weighted averaging (OWA), geometric (OWG), the Choquet integral averaging (CA) and geometric (CG) operators under discrimination q-rung orthopair fuzzy (q-ROF) and q-rung picture linguistic fuzzy (q-RPLF) information. These operators, constructed for the q-RPLF and q-ROF environments, take into account the overall pair interactions among attributes. Main properties on the correctness of extensions are proved: for the lower and upper capacities of order 2, all constructed operators consequently coincide with q-ROF and q-RPLF Choquet averaging and geometric operators, respectively. Constructed operators in the evaluation of prediction of fuzzy Collaborative Filtering Recommender Systems (CFRS) are used. New symmetric discrimination measures as some extensions of discrimination measures for the fuzzy CFRS are proposed. Users’ profile data by the constructed operators in the new similarity measure under q-rung picture linguistic environment are aggregated. The developed new approach is schematically described in such a way that it can be “embedded” in any existing CFRS model. An example is given to illustrate the results, for which the software designed to aggregate profile data for similarity comparison provides the use of new and well-known classical aggregation operators.},
  archive      = {J_IJFS},
  author       = {Sirbiladze, Gia and Garg, Harish and Khutsishvili, Irina and Midodashvili, Bidzina and Gugunava, Oleg},
  doi          = {10.1007/s40815-024-01772-6},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {204-236},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Associated probabilities’ aggregations in fuzzy collaborative filtering recommender system prediction},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Z-cloud rough fuzzy-based PIPRECIA and CoCoSo integration to
assess agriculture decision support tools. <em>IJFS</em>,
<em>27</em>(1), 190–203. (<a
href="https://doi.org/10.1007/s40815-024-01771-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The livestock sector has exacerbated the problems of ensuring global food safety and greenhouse gas emissions. The rapid increase in livestock production has called to shed light on decision-support tools that develop sustainable production strategies. In this context, this study aims to expand the application of multiple-criteria decision analysis (MCDM) methods to assign weights to criteria and classify decision support tools for livestock with a high degree of certainty. In order to begin serious steps to address the global sustainability problem, this study extended the PIPRECIA method with a high-certainty fuzzy environment called Z-cloud rough numbers (ZCRNs) to record the weight of 19 criteria for decision support tools in livestock farming. An innovative and advanced method called CoCoSo has been utilized to rank decision-support tools for livestock farming. The methodology included two stages. The first phase involved developing the decision matrix. The second phase encompassed developing MCDM methods by clarifying the steps of the PIvot Pairwise RElative Criteria Importance Assessment (PIPRECIA) method for assigning weight to criteria, in addition to highlighting the steps of the CoCoSo method for classifying decision support tools in the livestock industry. The results of the PIPRECIA method extended to the fuzzy environment of ZCRNs confirmed that visualization and herd characteristics received the highest weight compared to the rest of the criteria of decision support tools. The CoCoSo results provided insight into ranking alternatives for livestock decision support tools. AgRECalc has the highest ranking, and FCFC has the lowest ranking. This study conducted an evaluation test to increase the chances of generalizing the results of ranking decision-support tools of the livestock industry.},
  archive      = {J_IJFS},
  author       = {Alnoor, Alhamzah and Muhsen, Yousif Raad and Husin, Nor Azura and Chew, XinYing and Zolkepli, Maslina Binti and Manshor, Noridayu},
  doi          = {10.1007/s40815-024-01771-7},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {190-203},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Z-cloud rough fuzzy-based PIPRECIA and CoCoSo integration to assess agriculture decision support tools},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bipolar-valued complex hesitant fuzzy dombi aggregating
operators based on multi-criteria decision-making problems.
<em>IJFS</em>, <em>27</em>(1), 162–189. (<a
href="https://doi.org/10.1007/s40815-024-01770-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex Hesitant Fuzzy sets are a powerful tool for depicting vagueness and uncertainty. This paper addresses to Bipolar-Valued Complex Hesitant Fuzzy sets (BVCHFSs) to decode inconsistent, complexity data because of including bipolarity being opposite polar, complexity dividing membership value into two parts, hesitation degree including several membership values. Then, we interpret some new rules such as addition, scalar multiplication, scalar power, multiplication, and present score function. Moreover, some aggregation operators based on BVCHFSs are presented, such as Bipolar-valued Complex Hesitant Fuzzy-Weighted Dombi Averaging operator (BVCHFWDA), Ordered and Hybrid concepts, and Bipolar valued Complex Hesitant Fuzzy-Weighted Dombi Geometric operator (BVCHFWDG), Ordered and Hybrid structures, and some properties, such as idempotency, monotonicity, and boundedness. Later on, the obtained operators are applied over an investment example to show originality and efficiency of suggested instructions. We test to merits and restrictions of the new instructions by comparing them with some existing measures based on bipolar complex fuzzy sets. The comparative analysis indicates that our discussed operators and distance measures over bipolar complex fuzzy sets are agreement especially for BVCHFWDA.},
  archive      = {J_IJFS},
  author       = {Özlü, Şerif},
  doi          = {10.1007/s40815-024-01770-8},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {162-189},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Bipolar-valued complex hesitant fuzzy dombi aggregating operators based on multi-criteria decision-making problems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllability and observability of non-homogeneous
granular descriptor fractional dynamical systems applied in electrical
circuit. <em>IJFS</em>, <em>27</em>(1), 144–161. (<a
href="https://doi.org/10.1007/s40815-024-01769-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the study of controllability and observability of fuzzy fractional descriptor dynamical system in terms of granular differentiability. The granular fuzzy solution for granular descriptor fractional dynamical system (GrDFDS) is obtained by using the Mittag–Leffler function and granular Laplace transform and the solution is represented in terms of the state transfer matrix. The controllability and observability of GrDFDS is analysed with the help of theorems using the controllability Gramian matrix and observability Gramian matrix. In order to illustrate the efficacy of our findings, we hereby give the controllability analysis of an engineering problem pertaining to the compatibility of a descriptor fractional electric circuit. The graph visually represents the outcome, indicating that the granular descriptor dynamical system of the electric circuit can be effectively controlled, leading to the successful transition of its state from the granular initial to the final state.},
  archive      = {J_IJFS},
  author       = {Srilekha, R. and Parthiban, V.},
  doi          = {10.1007/s40815-024-01769-1},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {144-161},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Controllability and observability of non-homogeneous granular descriptor fractional dynamical systems applied in electrical circuit},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplicative sampled-data control for interval type-2
fuzzy interconnected PDE systems under memory event-triggered scheme.
<em>IJFS</em>, <em>27</em>(1), 125–143. (<a
href="https://doi.org/10.1007/s40815-024-01768-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the multiplicative sampled-data control for the interconnected non-linear partial differential equation (PDE) systems with parameter uncertainties. First, an interval type-2 (IT2) Takagi–Sugeno fuzzy model is employed to reconstruct the studied system. In contrast to type-1 fuzzy sets, IT2 fuzzy sets can handle parameter uncertainties that type-1 fuzzy sets cannot handle, and they can characterize parameter uncertainties by utilizing upper and lower membership functions. Next, based on the IT2 fuzzy model, a sampled-data IT2 fuzzy controller containing multiplicative control gain uncertainties is designed to reduce the control cost, where a Bernoulli distribution is adopted to depict the stochastically occurring multiplicative gain uncertainties. Moreover, to conserve communication resources, a memory event-triggered strategy (METS) is employed to decrease the amount of useless data transmitted in the network channel. In contrast to the event-triggered strategy (ETS), the METS triggers these data with a small relative error between the current data and the latest published data, thereby achieving better control. Finally, an example is given to demonstrate the validity of the proposed methodology.},
  archive      = {J_IJFS},
  author       = {Zheng, Danjing and Song, Xiaona and Zhang, Liang and Song, Shuai and Peng, Zenglong},
  doi          = {10.1007/s40815-024-01768-2},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {125-143},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Multiplicative sampled-data control for interval type-2 fuzzy interconnected PDE systems under memory event-triggered scheme},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel robust control and optimal design for fuzzy unmanned
surface vehicles (USVs). <em>IJFS</em>, <em>27</em>(1), 110–124. (<a
href="https://doi.org/10.1007/s40815-024-01767-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel high-order robust control is proposed for unmanned surface vehicles (USVs) steering dynamic systems. We creatively adopt the fuzzy set-based characterization to describe the uncertainty. In this way, the fuzzy dynamical USV system is constructed. Then, the proposed control is proven to render the USV system uniformly bounded and uniformly ultimately bounded regardless of the uncertainty. Furthermore, a performance index is designed by considering both system performances and control costs. This index utilized the fuzzy characteristics of the uncertainty. We solve for the optimal control design parameter by minimizing this index. Finally, simulations are performed for different cases. By comparing the detailed simulation results, the effectiveness of the high-order control is demonstrated and the functions of the control parameter are analysed.},
  archive      = {J_IJFS},
  author       = {Li, Chenming and Zhao, Xu and Yu, Rongrong and Chen, Ye-Hwa and Lin, Fei},
  doi          = {10.1007/s40815-024-01767-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {110-124},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A novel robust control and optimal design for fuzzy unmanned surface vehicles (USVs)},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distance-based approach to fuzzy cognitive maps using
pythagorean fuzzy sets. <em>IJFS</em>, <em>27</em>(1), 93–109. (<a
href="https://doi.org/10.1007/s40815-024-01766-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Cognitive Maps (FCMs) have been attracting researchers from a wide application area due to being easy to apply and interpret. Since its proposal, the method has been improved to satisfy the diverse needs of practitioners such as solving different types of problems and representing particular types of uncertainty. The classical FCMs depend highly on the decision-maker judgments and the uncertainty inherent in the judgments deserves significant attention. Although there are several fuzzy extensions integrated into FCMs, the uncertainty caused by the lack of knowledge, the hesitancy of decision makers, and also the limited capacity of humans to deal with pre-defined rules should be considered. To address this issue, a new distance-based approach integrating Pythagorean Fuzzy Sets and FCMs is proposed. To the best of our knowledge, this is the first time this extension is integrated into FCMs. Besides allowing to represent the uncertainty until the end of the calculations, the new approach offers decision makers an easier and more flexible way to assess the strength of existing causal relationships. To provide a comparison between the proposed approach and the classical FCMs, two real-life applications are selected as case studies.},
  archive      = {J_IJFS},
  author       = {Bozdag, Erhan and Kadaifci, Cigdem},
  doi          = {10.1007/s40815-024-01766-4},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {93-109},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A distance-based approach to fuzzy cognitive maps using pythagorean fuzzy sets},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissipative constraint-based saturation control for fuzzy
markov jump systems within a finite-time interval. <em>IJFS</em>,
<em>27</em>(1), 77–92. (<a
href="https://doi.org/10.1007/s40815-024-01761-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problems of finite-time boundedness and dissipative analysis for a class of discrete-time nonlinear Markov jump systems (MJSs) with disturbances. In particular, the Takagi-Sugeno fuzzy model is applied to the nonlinear plant, and the impact of time-varying actuator saturation is considered in the controller design. The main purpose of this paper is to develop a mode-dependent fuzzy saturation control for fuzzy MJSs over a finite-time interval. With the help of the Lyapunov stability theory and Abel lemma-based finite-sum inequality, it is established that convergence of all states are confirmed through the addressed control design. Correspondingly, the resulting closed-loop system is stochastically finite-time bounded and $$({\mathcal {Q}},{\mathcal {S}},{\mathcal {R}})$$ - $$\gamma$$ -dissipative under linear matrix inequality (LMI) framework. At last, two numerical examples are given to demonstrate the effectiveness and usefulness of the obtained LMI conditions.},
  archive      = {J_IJFS},
  author       = {Kavikumar, Ramasamy and Kaviarasan, Boomipalagan and Kwon, Oh-Min and Sakthivel, Rathinasamy},
  doi          = {10.1007/s40815-024-01761-9},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {77-92},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Dissipative constraint-based saturation control for fuzzy markov jump systems within a finite-time interval},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel interval type-2 fuzzy CPT-TODIM method for
multi-criteria group decision making and its application to credit risk
assessment in supply chain finance. <em>IJFS</em>, <em>27</em>(1),
54–76. (<a href="https://doi.org/10.1007/s40815-024-01759-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assessment of credit risk in supply chain finance (SCF) stands as a pivotal procedure in facilitating enterprises to identify appropriate financing solutions, reduce financing costs, enhance capital utilization efficiency, and mitigate the risk of debt default. Multi-criteria group decision-making (MCGDM), a systematic evaluation tool, is widely used for the assessment of both qualitative and quantitative criteria. However, the conventional framework of MCGDM exhibits limitations in addressing scenarios characterized by high uncertainty in risk information, disparity in weights among decision-makers (DMs) and criteria, alongside complex and non-linear risk perception. To address these limitations, this paper introduces an analytical model that integrates Interval Type-2 Fuzzy Sets (IT2FSs), Cumulative Prospect Theory (CPT), and the TODIM (an acronym from Portuguese for Interactive and Multicriteria Decision Making) method to evaluate credit risk in SCF. Firstly, the IT2FSs are utilized to represent high uncertainty in risk assessment information of DMs. Secondly, the Dice Similarity is applied to determine the weights of DMs. Then, we seek to improve the Criterion Importance Through Intercriteria Correlation (CRITIC) method by addressing its limitations and further integrating it with the Bayesian Best–Worst Method (BBWM), offering a robust computational framework of integrated weights for criteria. Finally, the CPT-TODIM method based on IT2FSs is applied in a real case from Ping An Bank. Through rigorous sensitivity and comparative analyses conducted within the real-world context of SCF credit risk assessments, the proposed model’s theoretical robustness and practical applicability are emphatically validated.},
  archive      = {J_IJFS},
  author       = {Li, Wen and Wang, Luqi and Rehman, Obaid Ur},
  doi          = {10.1007/s40815-024-01759-3},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {54-76},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {A novel interval type-2 fuzzy CPT-TODIM method for multi-criteria group decision making and its application to credit risk assessment in supply chain finance},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotonic fuzzy systems with goniometric membership
functions. <em>IJFS</em>, <em>27</em>(1), 43–53. (<a
href="https://doi.org/10.1007/s40815-024-01758-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy logic-based systems are nowadays commonly used in nonlinear function approximation when incoming data are available. Their main advantage is that the resulting rules can be interpreted understandably. Nevertheless, when the data are noisy an overfitting may occur which leads to poor accuracy and generalization ability. Prior information about the nonlinear function may improve fuzzy system performance. In this paper the case when the function is monotonic with respect to some or all variables is considered. Sufficient conditions for the monotonicity of first-order Takagi–Sugeno fuzzy systems with raised cosine membership functions are derived. Performance of the proposed fuzzy system is tested on two benchmark datasets},
  archive      = {J_IJFS},
  author       = {Hušek, Petr},
  doi          = {10.1007/s40815-024-01758-4},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {43-53},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Monotonic fuzzy systems with goniometric membership functions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double uncertainty driving and integrated decision-making
under the mixed probabilistic hesitant environment. <em>IJFS</em>,
<em>27</em>(1), 27–42. (<a
href="https://doi.org/10.1007/s40815-024-01755-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic hesitant fuzzy sets are an emerging and popular tool in the decision-making field, skilled at describing subjective information. However, it does not take into account the cognitive limitations of decision makers and the complexity of the actual environment can lead to situations such as multiple values or missing probabilistic information. Considering these issues, we first develop the uncertain probabilistic hesitant fuzzy set (UPHFS) to propose the mixed hesitant fuzzy set (MHFS). Moreover, we further give the computational rules and prove the generalization and robustness of the MHFS. Then, we design two new ratio models, namely the mixed probabilistic hesitant crossover ratio (MPHCR) model and mixed preference probabilistic hesitant crossover ratio (MPPHCR) model, and construct an integrated decision-making model based on them. The integrated decision-making model is a collection of integrated decision models for deriving unknown and mixed probabilities and computing optimal decision outcomes, which is different from the previous studies than the similar studies. Therefore, the model results could be more robust and reliable. Further, based on the proposed fuzzy environments and new decision-making models, we give the whole calculation process and integrated decision-making steps. Lastly, an illustrated example of project investment is provided to apply the above methods, processes, and steps and shows their effectiveness.},
  archive      = {J_IJFS},
  author       = {Zhou, Wei and Luo, Danxue and Xu, Zeshui},
  doi          = {10.1007/s40815-024-01755-7},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {27-42},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Double uncertainty driving and integrated decision-making under the mixed probabilistic hesitant environment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel interval type-2 ANFIS modeling based on one-step type
reducer algorithm. <em>IJFS</em>, <em>27</em>(1), 13–26. (<a
href="https://doi.org/10.1007/s40815-024-01754-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel structure of Interval Type-2 Adaptive Network-Fuzzy Inference System (IT2-ANFIS) for modeling dynamic systems is proposed. Optimization algorithms are introduced to adjust the antecedent and consequent parameters of a fuzzy model. In order to avoid the classical iterative process commonly used in type reduction algorithms, a new one-step type reduction algorithm (OSTRA) is proposed, which in conjunction with IT2-ANFIS is tested on nonlinear dynamical system and one real system datasets. Furthermore, to validate the complete structure, experiments with numerical models are performed, in order to show the advantages of the proposed novel IT2-ANFIS structure, by obtaining better results than previously published T2-ANFIS structures. To illustrate a real application of the proposed modeling technique, a model obtained from a tractor steering wheel system was embedded in an electronic I/O board, to obtain a comparison of the on-line fuzzy model against the physical system, with satisfactory results in the approximation error.},
  archive      = {J_IJFS},
  author       = {Alberto-Rodríguez, Adrián and López-Morales, Virgilio and Ramos-Fernández, Julio Cesar},
  doi          = {10.1007/s40815-024-01754-8},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {13-26},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Novel interval type-2 ANFIS modeling based on one-step type reducer algorithm},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy rules data-driven equivalent model with multi-gradient
learning for discrete-time nearly optimal control. <em>IJFS</em>,
<em>27</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s40815-024-01727-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A behavior of switchable control directions is investigated for the non-holonomic robotic system considered as a class of unknown nonlinear discrete-time systems. The data-driven equivalent model is established by a multi-input fuzzy rule emulated network and the multi-gradient learning law is developed to tune all adjustable parameters. Thereafter, the nearly optimal controller is derived using the dynamics of the equivalent model and the closed-loop performance is analyzed through rigorous mathematical analysis. The experimental system is constructed to validate the effectiveness of the proposed scheme and the advantage of the multi-gradient approach.},
  archive      = {J_IJFS},
  author       = {Treesatayapun, C.},
  doi          = {10.1007/s40815-024-01727-x},
  journal      = {International Journal of Fuzzy Systems},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Fuzzy Syst.},
  title        = {Fuzzy rules data-driven equivalent model with multi-gradient learning for discrete-time nearly optimal control},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijmir---10">IJMIR - 10</h2>
<ul>
<li><details>
<summary>
(2025). STCA: An action recognition network with spatio-temporal
convolution and attention. <em>IJMIR</em>, <em>14</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-024-00350-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution and self-attention mechanisms are two commonly used methods in the field of video understanding. Convolution preserves spatiotemporal relationships in video data while reducing the number of parameters and computations. The self-attention mechanism captures global and long-distance dependencies in sequence data. To address the challenges of low accuracy and excessive parameters in networks for action recognition, we propose a new network that combines convolution and self-attention mechanisms (STCA). STCA consists of two modules: efficient spatiotemporal convolution (ESTConv) and spatiotemporal self-attention (STA). ESTConv extracts local spatiotemporal features of actions, enabling fast reasoning. STA consists of two sub-modules: the spatial self-attention (SA) and the temporal self-attention (TA). SA analyzes the spatial characteristics of actions, while TA analyzes their temporal characteristics. We conducted experiments on the Kinetics400, UCF101, HMDB51, and Something-Something V2 datasets to evaluate our network. Results show that STCA achieves accuracy comparable to the leading action recognition models while reducing parameters by over 20%, making it more lightweight than current best-performing models.},
  archive      = {J_IJMIR},
  author       = {Tian, Qiuhong and Miao, Weilun and Zhang, Lizao and Yang, Ziyu and Yu, Yang and Zhao, Yanying and Yao, Lan},
  doi          = {10.1007/s13735-024-00350-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {STCA: An action recognition network with spatio-temporal convolution and attention},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAMIR: Fine-tuning CLIP and multi-head cross-attention
mechanism for multimodal image retrieval with sketch and text features.
<em>IJMIR</em>, <em>14</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s13735-024-00352-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches and texts are two input modes of queries that are widely used in image retrieval tasks of different granularities. Text-based image retrieval (TBIR) is mainly used for coarse-grained retrieval, while sketch-based image retrieval (SBIR) aims to retrieve images based on hand-drawn sketches, which pose unique challenges due to the abstract nature of sketches. Existing methods mainly focus on retrieval based on a single modality but fail to explore the connections between multiple modalities comprehensively. In addition, the emerging contrastive language image pre-training (CLIP) model and powerful contrastive learning methods are underexplored in this field. We propose a novel multimodal image retrieval framework (CAMIR) to address these challenges. It obtains sketch and text features through a fine-tuned CLIP model, fuses the extracted features using multi-head cross-attention, and combines contrastive learning for retrieval tasks. In the indexing stage, we introduce Faiss, an open-source similarity search library developed by Meta AI Research, to enhance retrieval efficiency. Comprehensive experiments on the benchmark dataset Sketchy demonstrate the effectiveness of our proposed framework, achieving superior performance compared to existing methods while highlighting the potential of integrating sketch and text features for retrieval tasks.},
  archive      = {J_IJMIR},
  author       = {Yang, Fan and Ismail, Nor Azman and Pang, Yee Yong and Alsayed, Alhuseen Omar},
  doi          = {10.1007/s13735-024-00352-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {CAMIR: Fine-tuning CLIP and multi-head cross-attention mechanism for multimodal image retrieval with sketch and text features},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving skeleton-based action recognition with interactive
object information. <em>IJMIR</em>, <em>14</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-024-00351-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7%, and on cross-view split, it is 99.2%. The project page: https://github.com/moonlight52137/ST-VGCN .},
  archive      = {J_IJMIR},
  author       = {Wen, Hao and Lu, Ziqian and Shen, Fengli and Lu, Zhe-Ming and Cui, Jialin},
  doi          = {10.1007/s13735-024-00351-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Improving skeleton-based action recognition with interactive object information},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-matrix guided reconstruction hashing for unsupervised
cross-modal retrieval. <em>IJMIR</em>, <em>14</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-025-00353-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-modal hashing, due to its independence from heavy label information, is more convenient for application to other fields. In recent years, this area has gained widespread attention and achieved great success. However, existing unsupervised cross-modal hashing methods still face some issues, such as simple fusion after feature extraction, the use of a single similarity measure to express data relationships, and guiding hash code learning through a single affinity matrix. To address these problems, we propose a new method called Dual-Matrix Guided Reconstruction Hashing for Unsupervised Cross-Modal Retrieval. We construct an effective matrix from the extracted raw semantic information to guide the generation of reconstructed hash codes for images and texts. Simultaneously, we construct another matrix for the extracted image and text features, guiding the generation of reconstructed hash codes using graph convolution, thus directing hash code learning through dual matrices. In evaluations on three standard datasets, our method achieved an average improvement of approximately 1.3% in MAP@5000 and 1.5% in MAP@50, particularly showing significant performance gains with shorter hash codes.},
  archive      = {J_IJMIR},
  author       = {Lin, Ziyong and Jiang, Xiaolong and Zhang, Jie and Li, Mingyong},
  doi          = {10.1007/s13735-025-00353-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Dual-matrix guided reconstruction hashing for unsupervised cross-modal retrieval},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task classification network for few-shot learning.
<em>IJMIR</em>, <em>14</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s13735-025-00354-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic information provides both internal coherence within categories and distinctiveness between categories that surpass mere visual concepts. Semantic information has been employed in Few-Shot Learning (FSL) to achieve additional performance improvements. Previous methods usually combine support image and semantic information to classify query image. However, in FSL, it is challenging to train a model on a limited base dataset such that the model can effectively fuse or interact with both modalities and obtain better feature representation on the novel dataset. To address this problem, we propose a Multi-task Classification Network (MCN) to decompose the current classification problem into a image-image classification problem and a semantic-image classification problem. Considering the issue that the results of image-image classification and semantic-image classification may not always be trustworthy, we introduce an Uncertainty-Aware Decision Module (UADM) which biases the final classification result towards the result with lower uncertainty in the two types of classification. Extensive experimental results on three datasets have consistently shown that our proposed method achieves impressive results. Particularly, compared to the baseline, we achieved a 2–3% improvement on the CUB, SUN, and Flower datasets in both the 5-way 1-shot and 5-way 5-shot settings.},
  archive      = {J_IJMIR},
  author       = {Ji, Zhong and Liu, Yuanheng and Wang, Xuan and Liu, Jingren and Cao, Jiale and Yu, YunLong},
  doi          = {10.1007/s13735-025-00354-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-task classification network for few-shot learning},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized RT-DETR for accurate and efficient video object
detection via decoupled feature aggregation. <em>IJMIR</em>,
<em>14</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s13735-025-00355-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection (VOD) is a challenging task, and image object detectors are difficult to detect degradation phenomena in certain video frames. However, existing research on VOD mostly trades high computational costs for accuracy, making it difficult to achieve a balance between accuracy and speed. This work proposes an optimized Real-Time Detection Transformer (RT-DETR) model for VOD that introduces a decoupled Feature Aggregation Module (FAM) to separately refine the localization and classification detection heads. This method only requires a minimal increase in the number of parameters to achieve significant improvements in accuracy. Specifically, we insert FAM before the localization detection head and classification detection head, and first freeze all parameters of the feature extractor and classification detection head to train only the parameters of the localization detection head to obtain more accurate localization results. Then, we freeze all parameters of the feature extractor and localization detection head to train only the parameters of the classification detection head to improve the final detection accuracy. We have conducted a large number of ablation experiments to verify the effectiveness of the method. Without using any post-processing methods, we achieved 90.0% mAP on the ImageNet-VID dataset, with only 77.9 M parameters and an average inference speed of 14.1ms.},
  archive      = {J_IJMIR},
  author       = {Chen, Hao and Huang, Wu and Zhang, Tao},
  doi          = {10.1007/s13735-025-00355-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Optimized RT-DETR for accurate and efficient video object detection via decoupled feature aggregation},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAMoE-MSA: Polarity-aware mixture of experts network for
multimodal sentiment analysis. <em>IJMIR</em>, <em>14</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s13735-025-00362-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) is a challenging task that aims to understand human emotions from text, visual, and audio modalities. Existing studies struggle to capture the monotonic relationship between emotional expressions. This monotonic relationship means that the emotional intensity changes consistently with the expression amplitude when considering emotional polarities, which is a crucial aspect of MSA tasks. To tackle this, we propose a polarity-aware mixture of experts network (PAMoE-MSA). PAMoE-MSA is capable of learning polarity-specific and polarity-common features to capture the monotonic relationship of emotional expressions from multimodal sentiment data. Our model consists of three experts: a positive expert, a negative expert, and a general expert. They are trained through a unique Guide Task, where the positive and negative experts are trained by non-neutral samples, while the general expert is trained by all samples. A gating mechanism is utilized to adaptively perceive the monotonic relationship within emotional expressions. Moreover, the self-supervised labels are introduced to preserve modality-specific information. The experts module is fed with the fusion features, which contain richer emotional information. To enhance model stability during the training phase, we employ multi-side contrastive learning before making predictions. Our evaluation of PAMoE-MSA on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets shows notable improvements over state-of-the-art methods, with increases of approximately 1.3% in Acc-7 for CMU-MOSI, 1.2% in Acc-2 for CMU-MOSEI, and 0.8% in F1-score for CH-SIMS.},
  archive      = {J_IJMIR},
  author       = {Huang, Changqin and Lin, Zhenheng and Han, Zhongmei and Huang, Qionghao and Jiang, Fan and Huang, Xiaodi},
  doi          = {10.1007/s13735-025-00362-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {PAMoE-MSA: Polarity-aware mixture of experts network for multimodal sentiment analysis},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFAFD: A few-shot learning method for cascading models with
parameter free attention and finite discrete space. <em>IJMIR</em>,
<em>14</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s13735-025-00357-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on existing learning models, multimodal approaches have demonstrated promising performance in the realm of few-shot learning, owing to contrastive language-image pretraining. However, shortcomings persist in multimodal fusion methods, particularly in aligning textual and visual features across different granularity levels, and in the independent feature extraction by encoders lacking interaction. Therefore, this paper proposes MFAFD: a cascaded model for few-shot learning featuring parameter free attention mechanisms and a finite discrete space. Initially, the model employs a parameter free attention module in the pretraining phase to facilitate cross-modal interactions, enhancing alignment between spatial features of images and generated text prior to extracting global features from images via CLIP. This bidirectional update of textual and visual information addresses the issue of feature alignment. During training, the model leverages a representation based on Finite Discrete Space (FDS), constructing a finite discrete space foundation for textual and image features, effectively bridging modal differences. Ultimately, using text as a baseline, the model predicts image classification based on similarity weights between images and text. Through quantitative and qualitative analyses, this study demonstrates that parameter free attention mechanisms and finite discrete space modules significantly enhance the performance of cascaded multimodal aggregation models. The model exhibits robust performance in few-shot classification across multiple datasets. The code is available at https://github.com/turelove999/MFDFA-dj .},
  archive      = {J_IJMIR},
  author       = {Xue, Lixia and Dong, Jiang and Wang, Ronggui and Yang, Juan},
  doi          = {10.1007/s13735-025-00357-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MFAFD: A few-shot learning method for cascading models with parameter free attention and finite discrete space},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image forgery classification and localization through vision
transformers. <em>IJMIR</em>, <em>14</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s13735-025-00358-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the easy availability of software over the Internet, any naive user can tamper the images for entertainment purposes or to defame a personality by circulating over social media networks. The practice of image tampering is a serious issue and can attract legal action if proven guilty. Forensic researchers employ various methods to detect and localize image forgeries. In this research, we use a Vision transformer (ViT) as a method for binary classification of images distinguishing forged and unforged images. Further, we use a pre-trained Segment Anything Model(SAM) which is fine-tuned with custom data to adaptively recognize patterns indicating forged regions within the images. SAM can localize these forged areas and is leveraged to create templates by extracting the identified regions. The proposed method is rigorously tested across various datasets, including CASIA v1.0, CASIA v2.0, MICC-F2000, MICC-F600, and Columbia. Through comprehensive experimentation, our approach showcases considerable promise yielding accuracy in image forgery classification and localization. Our model’s robustness and adaptability make it an attractive tool for forensic analysis in diverse scenarios, contributing to the advancement of multimedia forensics security research.},
  archive      = {J_IJMIR},
  author       = {Pawar, Digambar and Gowda, Raghavendra and Chandra, Krishna},
  doi          = {10.1007/s13735-025-00358-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Image forgery classification and localization through vision transformers},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPC-VoxelNet: Multi-modal fusion 3D object detection
networks based on virtual point clouds. <em>IJMIR</em>, <em>14</em>(1),
1–11. (<a href="https://doi.org/10.1007/s13735-025-00360-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the impact of sparsity and disorder of point clouds on object detection accuracy, this paper proposes a multi-modal fusion network VPC-VoxelNet based on virtual point clouds. Firstly, virtual point clouds are constructed using image detection object information to increase the density of point clouds, thus improving the performance of object features; Secondly, increasing the dimensionality of point cloud features, distinguishing virtual point clouds and avoiding the accumulation of multi model errors; Finally, an optimized loss function such as the scale factor of the virtual point cloud is used to improve the training efficiency of the multi-modal network. The object detection network, VPC-VoxelNet, was tested on the KITTI dataset, and the detection accuracy was better than that of the classical 3D point cloud detection network and certain multi-modal information fusion networks, with a vehicle detection accuracy of 86.9%.},
  archive      = {J_IJMIR},
  author       = {Zhang, Qiang and Shi, Qin and Cheng, Teng and Zhang, Junning and Chen, Jiong},
  doi          = {10.1007/s13735-025-00360-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {VPC-VoxelNet: Multi-modal fusion 3D object detection networks based on virtual point clouds},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijmlc---39">IJMLC - 39</h2>
<ul>
<li><details>
<summary>
(2025). Beyond traditional visual object tracking: A survey.
<em>IJMLC</em>, <em>16</em>(2), 1435–1460. (<a
href="https://doi.org/10.1007/s13042-024-02345-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single object tracking is a vital task of many applications in critical fields. However, it is still considered one of the most challenging vision tasks. In recent years, computer vision, especially object tracking, witnessed the introduction or adoption of many novel techniques, setting new fronts for performance. In this survey, we visit some of the cutting-edge techniques in vision, such as Sequence Models, Generative Models, Self-supervised Learning, Unsupervised Learning, Reinforcement Learning, Meta-Learning, Continual Learning, and Domain Adaptation, focusing on their application in single object tracking. We propose a novel categorization of single object tracking methods based on novel techniques and trends. Also, we conduct a comparative analysis of the performance reported by the methods presented on popular tracking benchmarks. Moreover, we analyze the pros and cons of the presented approaches and present a guide for non-traditional techniques in single object tracking. Finally, we suggest potential avenues for future research in single-object tracking.},
  archive      = {J_IJMLC},
  author       = {Abdelaziz, Omar and Shehata, Mohamed and Mohamed, Mohamed},
  doi          = {10.1007/s13042-024-02345-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1435-1460},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Beyond traditional visual object tracking: A survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grouping attributes: An accelerator for attribute reduction
based on similarity. <em>IJMLC</em>, <em>16</em>(2), 1417–1433. (<a
href="https://doi.org/10.1007/s13042-024-02344-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of attribute reduction, the method of attribute selection holds significant importance. Different selection methods can significantly impact the efficiency and outcomes of attribute reduction. Presently, the common approach involves constructing a measure to evaluate the significance of attribute and subsequently selecting the optimal attribute to include in the reduction set based on this measure. However, most existing measures predominantly focus on the relationship between attributes and target concepts, overlooking the inter-relationships among attributes themselves. This paper defines the relationships between attributes based on the variation in significance and utilizes this concept to design a general algorithm for accelerating attribute reduction. Firstly, we introduce the concept of the similarity of attributes, where attributes exhibiting greater overlap in their classification abilities for the target set are deemed more similar. Subsequently, utilizing this concept, a cover is constructed over the original set of attributes. Then, an algorithm for accelerating attribute reduction is devised by integrating the constructed cover into a greedy attribute reduction algorithm. Finally, we conduct experiments on 12 UCI datasets. Compared to four benchmark algorithms, the proposed algorithm effectively reduces runtime while maintaining classification accuracy. The effectiveness of the algorithm is further validated using the Wilcoxon signed-rank test.},
  archive      = {J_IJMLC},
  author       = {Jia, Yunlong and Zhu, Ping},
  doi          = {10.1007/s13042-024-02344-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1417-1433},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Grouping attributes: An accelerator for attribute reduction based on similarity},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning cluster-wise label distribution for label
enhancement. <em>IJMLC</em>, <em>16</em>(2), 1403–1415. (<a
href="https://doi.org/10.1007/s13042-024-02343-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label enhancement (LE) refers to the process of recovering label distributions from logical labels for less ambiguity. Current LE techniques concentrate on learning each instance individually, which ignores the instance correlation. In this paper, we propose to learn a cluster-wise label distribution (CWLD) shared by all instances of the cluster to explore the instance correlation. The softmax-normalized sum of the CWLD and the logical label vector yields the label distribution. CWLD is learned in an iterative manner. Following instance clustering, the label distributions of all instances in each cluster are averaged. The asymmetric label correlation is then mined using heat conduction. This process is repeated until the label distribution has reached a point of convergence. Experiments were undertaken on thirteen real-world datasets compared with six state-of-the-art algorithms. Results demonstrate the effectiveness and superiority of our proposed method.},
  archive      = {J_IJMLC},
  author       = {Fan, Jun and Zhang, Heng-Ru and Min, Fan},
  doi          = {10.1007/s13042-024-02343-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1403-1415},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Learning cluster-wise label distribution for label enhancement},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relevance-aware visual entity filter network for multimodal
aspect-based sentiment analysis. <em>IJMLC</em>, <em>16</em>(2),
1389–1402. (<a
href="https://doi.org/10.1007/s13042-024-02342-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal aspect-based sentiment analysis, which aims to identify the sentiment polarities over each aspect mentioned in an image-text pair, has sparked considerable research interest in the field of multimodal analysis. Despite existing approaches have shown remarkable results in incorporating external knowledge to enhance visual entity information, they still suffer from two problems: (1) the image-aspect global relevance. (2) the entity-aspect local alignment. To tackle these issues, we propose a Relevance-Aware Visual Entity Filter Network (REF) for MABSA. Specifically, we utilize the nouns of ANPs extracted from the given image as bridges to facilitate cross-modal feature alignment. Moreover, we introduce an additional “UNRELATED” marker word and utilize Contrastive Content Re-sourcing (CCR) and Contrastive Content Swapping (CCS) constraints to obtain accurate attention weight to identify image-aspect relevance for dynamically controlling the contribution of visual information. We further adopt the accurate reversed attention weight distributions to selectively filter out aspect-unrelated visual entities for better entity-aspect alignment. Comprehensive experimental results demonstrate the consistent superiority of our REF model over state-of-the-art approaches on the Twitter-2015 and Twitter-2017 datasets.},
  archive      = {J_IJMLC},
  author       = {Chen, Yifan and Xiong, Haoliang and Li, Kuntao and Mai, Weixing and Xue, Yun and Cai, Qianhua and Li, Fenghuan},
  doi          = {10.1007/s13042-024-02342-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1389-1402},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Relevance-aware visual entity filter network for multimodal aspect-based sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering hidden patterns: Low-rank label correlations for
multi-label weak-label learning. <em>IJMLC</em>, <em>16</em>(2),
1371–1387. (<a
href="https://doi.org/10.1007/s13042-024-02341-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning has emerged as a prominent research area in machine learning, as each instance can be associated with multiple class labels. However, many multi-label learning algorithms assume that the label space is complete, whereas in real-world applications, we often only have access to partial label information. To address this issue, we propose a novel Multi-label Weak-label learning algorithm via Low-rank Label correlations (MW2L). First, we propagate the structural and semantic information from the feature space to the label space to effectively capture label-related information and recover lost labels. Second, we incorporate global and local low-rank label correlation information to ensure that the label-related matrix is informative. Last, we use label correlations to supplement the original weak-label matrix and form a unified learning framework. We evaluate the performance of our approach on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of accuracy and robustness to weak-label noise. The proposed approach can effectively handle incomplete and noisy weak labels in multi-label learning and outperforms existing methods.},
  archive      = {J_IJMLC},
  author       = {Li, Tianli and Nasrudin, Mohammad Faidzul and Zhao, Dawei and Chen, Fei and Peng, Xing and Sarim, Hafiz Mohd},
  doi          = {10.1007/s13042-024-02341-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1371-1387},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Uncovering hidden patterns: Low-rank label correlations for multi-label weak-label learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new neural network method for solving bratu type equations
with rational polynomials. <em>IJMLC</em>, <em>16</em>(2), 1355–1369.
(<a href="https://doi.org/10.1007/s13042-024-02340-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bratu-type equation is a fundamental differential equation with numerous applications in engineering fields, such as radiative heat transfer, thermal reaction, and nanotechnology. This paper introduces a novel approach known as the rational polynomial neural network. In this approach, rational orthogonal polynomials are utilized within the neural network’s hidden layer. To solve the equation, the initial boundary value conditions of both the differential equation and the rational polynomial neural network are integrated into the construction of the numerical solution. This construction transforms the Bratu-type equation into a set of nonlinear equations, which are subsequently solved using an appropriate optimization technique. Finally, three sets of numerical examples are presented to validate the efficacy and versatility of the proposed rational orthogonal neural network method, with comparisons made across different hyperparameters. Furthermore, the experimental results are juxtaposed against traditional methods such as the Adomian decomposition method, genetic algorithm, Laplace transform method, spectral method, and multilayer perceptron, our method exhibits consistently optimal performance.},
  archive      = {J_IJMLC},
  author       = {He, Jilong and Cao, Cong},
  doi          = {10.1007/s13042-024-02340-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1355-1369},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A new neural network method for solving bratu type equations with rational polynomials},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-patch combination loss for refined edge detection.
<em>IJMLC</em>, <em>16</em>(2), 1341–1354. (<a
href="https://doi.org/10.1007/s13042-024-02338-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental image characteristic, edge features encapsulate a wealth of information, serving as a crucial foundation in image segmentation networks for accurately delineating and partitioning object edges. Convolutional neural networks (CNNs) have gained prominence recently, finding extensive utility in edge detection. Previous methods primarily emphasized edge prediction accuracy, ignoring edge refinement. In this work, we introduce a novel encoder-decoder architecture that effectively harnesses hierarchical features. By extending the decoder horizontally, we progressively enhance resolution to preserve intricate details from the original image, thereby producing sharp edges. Additionally, we propose a novel loss function named the Pixel-Patch Combination Loss (P2CL), which employs distinct detection strategies in edge and non-edge regions to bolster network accuracy and yield crisp edges. Furthermore, considering the practicality of the algorithm, our method strikes a fine balance between accuracy and model size. It delivers precise and sharp edges while ensuring efficient model operation, thereby laying a robust foundation for advancements deployed on mobile devices or embedded systems. Our method was evaluated on three publicly available datasets, including BSDS500, Multicue, and BIPED. The experimental results show the superiority of our approach, achieving a competitive ODS F-score of 0.832 on the BSDS500 benchmark and significantly enhancing edge detection accuracy.},
  archive      = {J_IJMLC},
  author       = {Li, Wenlin and Zhang, Wei and Liu, Yanyan and Liu, Changsong and Jing, Rudong},
  doi          = {10.1007/s13042-024-02338-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1341-1354},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Pixel-patch combination loss for refined edge detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal 6-DoF object pose tracking: Integrating spatial
cues with monocular RGB imagery. <em>IJMLC</em>, <em>16</em>(2),
1327–1340. (<a
href="https://doi.org/10.1007/s13042-024-02336-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate six degrees of freedom (6-DoF) pose estimation is crucial for robust visual perception in fields such as smart manufacturing. Traditional RGB-based methods, though widely used, often face difficulties in adapting to dynamic scenes, understanding contextual information, and capturing temporal variations effectively. To address these challenges, we introduce a novel multi-modal 6-DoF pose estimation framework. This framework uses RGB images as the primary input and integrates spatial cues, including keypoint heatmaps and affinity fields, through a spatially aligned approach inspired by the Trans-UNet architecture. Our multi-modal method enhances both contextual understanding and temporal consistency. Experimental results on the Objectron dataset demonstrate that our approach surpasses existing algorithms across most categories. Furthermore, real-world tests confirm the accuracy and practical applicability of our method for robotic tasks, such as precision grasping, highlighting its effectiveness for real-world applications.},
  archive      = {J_IJMLC},
  author       = {Mei, Yunpeng and Wang, Shuze and Li, Zhuo and Sun, Jian and Wang, Gang},
  doi          = {10.1007/s13042-024-02336-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1327-1340},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-modal 6-DoF object pose tracking: Integrating spatial cues with monocular RGB imagery},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LWTD: A novel light-weight transformer-like CNN architecture
for driving scene dehazing. <em>IJMLC</em>, <em>16</em>(2), 1303–1326.
(<a href="https://doi.org/10.1007/s13042-024-02335-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of artificial intelligence and automation technology, interest in autonomous driving research is also growing. However, under heavy rain, fog, and other adverse weather conditions, the visual quality of the images is reduced due to suspended atmospheric particles that affect the vehicle’s visual perception system, which is not conducive to the autonomous driving system’s accurate perception of the road environment. To address these challenges, this article presents a computationally efficient end-to-end light-weight Transformer-like neural network called LWTD (Light-Weight Transformer-like DehazeNet) to reconstruct haze-free images for driving tasks, which based on the reformulated ASM theory without prior knowledge. First, a strategy for simplifying the atmospheric light and transmission map into a feature map is adopted, a CMT (Convolutional Mapping Transformer) module for the extraction of global features is developed, and the hazy image is decomposed into a base layer (global features) and a detail layer (local features) for Low-Level, Medium-Level, and High-Level stages. Meanwhile, a channel attention module is introduced to weigh and assign the weights of each feature, and to fuse them with the reformulated ASM (Atmospheric Scattering Model) model to restore the haze-free image. Second, a joint loss function of the graphical features is formulated to further direct the network to converge in the direction of abundant features. In addition, a dataset of real-world fog driving is constructed. Extensive experiments with synthetic and natural hazy images confirmed the superiority of the proposed method through quantitative and qualitative evaluations on various datasets. Furthermore, additional experiments validated the applicability of the proposed method for traffic participant detection and semantic segmentation tasks. The source code has been made publicly available on https://github.com/ZebGH/LWTD-Net.},
  archive      = {J_IJMLC},
  author       = {Zhang, Zhenbo and Feng, Zhiguo and Long, Aiqi and Wang, Zhiyu},
  doi          = {10.1007/s13042-024-02335-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1303-1326},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {LWTD: A novel light-weight transformer-like CNN architecture for driving scene dehazing},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation extraction method based on pre-trained model and
bidirectional semantic union. <em>IJMLC</em>, <em>16</em>(2), 1291–1302.
(<a href="https://doi.org/10.1007/s13042-024-02334-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is an important task in natural language processing, which aims to extract the semantic relationships between entities from unstructured text. In traditional relation extraction methods, semantic, contextual and deep representation extraction are not sufficient. In this paper, we propose a relation extraction model (RoBBS, RoBERTa + Bi-GRU + Self Attention) based on pre-training and bi-directional semantic union. Firstly, the RoBERTa pre-training model is used to extracting the contextual features of distant sentences. Then Bi-GRU is leveraged to realize bidirectional semantic union, comprehensively extracting bidirectional semantic information. Combining this network with the attention mechanism allows for assigning greater weights to the semantic information that has a more significant role in determining the relationship classes of sentences. This, in turn, makes feature selection more efficient. Finally, the relationship is classified using the Softmax function. The experimental results show that the model achieves F-score of 89.02% on the SemEval2010 task8 dataset outperforming state-of-the-art models.},
  archive      = {J_IJMLC},
  author       = {He, Xinyu and Yan, Ge and Han, Xue and Kan, Manfei and Ren, Yonggong},
  doi          = {10.1007/s13042-024-02334-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1291-1302},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Relation extraction method based on pre-trained model and bidirectional semantic union},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way decision method based on COPRAS in the weak
probabilistic linguistic term set information systems. <em>IJMLC</em>,
<em>16</em>(2), 1265–1290. (<a
href="https://doi.org/10.1007/s13042-024-02333-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development and progress of technology, information becomes increasingly diverse, which poses higher demands on decision-making methods. Probabilistic linguistic term set (PLTS) is a tool that can more intuitively express the evaluations of decision makers (DMs). As a specialized form of PLTS with ignored probabilities, weak probabilistic linguistic term set (WPLTS) can describe incomplete or inaccurate evaluation information. Three-way decision (3WD) is an efficient decision-making method that reduces decision cost by adopting delayed decisions on the boundary domain. In this paper, we propose a novel 3WD method by combining 3WD with the complex proportional assessment (COPRAS) method under the WPLTS environment, named the WPLTS-3WD method. Firstly, we introduce the notion of the WPLTS information system. For a WPLTS information system, we propose a method of complementing the ignored probabilities and a new score function. Secondly, the objects are ranked by the COPRAS method. According to the ranking result, we define the dominance relation and dominance sets. Based on the dominance sets, the conditional probabilities can be estimated. By combining the conditional probabilities with relative loss functions, the expected losses will be obtained and the objects can be classified. Moreover, we propose two conversion functions that can convert real-valued and linguistic term evaluation information into PLTS evaluation information. Finally, we use the proposed WPLTS-3WD method to analyze the air quality of four cities. The rationality and advantages of our method are verified through experimental comparisons with other methods and parameter analysis.},
  archive      = {J_IJMLC},
  author       = {Yang, Hai-Long and Liu, Xu and Guo, Zhi-Lian},
  doi          = {10.1007/s13042-024-02333-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1265-1290},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A three-way decision method based on COPRAS in the weak probabilistic linguistic term set information systems},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5G-SIID: An intelligent hybrid DDoS intrusion detector for
5G IoT networks. <em>IJMLC</em>, <em>16</em>(2), 1243–1263. (<a
href="https://doi.org/10.1007/s13042-024-02332-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The constrained resources of Internet of Things (IoT) devices make them susceptible to Distributed Denial-of-Service (DDoS) attacks that disrupt service availability by overwhelming systems. Thus, effective intrusion detection is critical to ensuring uninterrupted IoT activities. This research presents a scalable system that combines machine and deep learning models with optimized data processing to secure IoT devices against DDoS attacks. A real-world 5G-IoT network simulation dataset was used to evaluate performance. Robust feature selection identified the 10 most informative features from the high-dimensional data. These features were used to train eight classifiers, namely: k-Nearest Neighbors (KNN), Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), Long-Short-Term Memory (LSTM) and hybrid CNN-LSTM models for DDoS attack detection. Experiments demonstrated 99.99% and 99.98% accuracy for multiclass and binary classification using the proposed hybrid CNN-LSTM model. Crucially, time- and space-complexity analysis validates real-world feasibility. Unlike prior works, this system optimally balances accuracy, efficiency, and adaptability through a precisely engineered model architecture, outperforming existing models. In general, this accurate, efficient, and adaptable system addresses critical IoT security challenges, improving cyber resilience in smart cities and autonomous vehicles.},
  archive      = {J_IJMLC},
  author       = {Sadhwani, Sapna and Mathur, Aakar and Muthalagu, Raja and Pawar, Pranav M.},
  doi          = {10.1007/s13042-024-02332-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1243-1263},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {5G-SIID: An intelligent hybrid DDoS intrusion detector for 5G IoT networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive consensus model for managing non-cooperative
behaviors in portfolio optimization for large companies. <em>IJMLC</em>,
<em>16</em>(2), 1219–1242. (<a
href="https://doi.org/10.1007/s13042-024-02331-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean–variance (MV) model provides numerous optimal portfolios for managing a firm&#39;s asset portfolio. Portfolio decisions in large corporations involve many interest groups, such as shareholders, bondholders, and employees, and require the assistance of large experts. However, experts from different departments with different cognitive levels and interests can differ or even conflict in their assessments of portfolios. To guarantee their interests, some experts may exhibit non-cooperative behavior, thus reducing the efficiency of reaching a consensus. To tackle this issue, the research aims to develop a large-scale group interactive portfolio optimization method that incorporates non-cooperative behaviors and leverages social network analysis (SN-LSGDM-NC-PO). First, various consensus feedback strategies based on minimum adjustment are formulated to provide advice during the negotiation process according to the global and local levels. Then, considering the acceptance of advice and the effect of expert adjustment on consensus, a new measure of non-cooperative behavior is designed. Non-cooperative behavior by experts can affect trust relations in a social network. Therefore, trust reward and penalty mechanisms, preference penalty mechanisms, and an exit mechanism are developed to manage different types of non-cooperative behavior. Experimental and comparison results demonstrate that the proposed SN-LSGDM-NC-PO algorithm can effectively manage the non-cooperative behaviors and reduce interaction consensus costs.},
  archive      = {J_IJMLC},
  author       = {Li, Danping and Hu, Shicheng},
  doi          = {10.1007/s13042-024-02331-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1219-1242},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An adaptive consensus model for managing non-cooperative behaviors in portfolio optimization for large companies},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight self-ensemble feedback recurrent network for
fast MRI reconstruction. <em>IJMLC</em>, <em>16</em>(2), 1201–1218. (<a
href="https://doi.org/10.1007/s13042-024-02330-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the speed of MRI acquisition is a key issue in modern medical practice. However, existing deep learning-based methods are often accompanied by a large number of parameters and ignore the use of deep features. In this work, we propose a novel Self-Ensemble Feedback Recurrent Network (SEFRN) for fast MRI reconstruction inspired by recursive learning and ensemble learning strategies. Specifically, a lightweight but powerful Data Consistency Residual Group (DCRG) is proposed for feature extraction and data stabilization. Meanwhile, an efficient Wide Activation Module (WAM) is introduced between different DCRGs to encourage more activated features to pass through the model. In addition, a Feedback Enhancement Recurrent Architecture (FERA) is designed to reuse the model parameters and deep features. Moreover, combined with the specially designed Automatic Selection and Integration Module (ASIM), different stages of the recurrent model can elegantly implement self-ensemble learning and synergize the sub-networks to improve the overall performance. Extensive experiments demonstrate that our model achieves competitive results and strikes a good balance between the size, complexity, and performance of the model.},
  archive      = {J_IJMLC},
  author       = {Li, Juncheng and Yang, Hanhui and Lui, Lok Ming and Zhang, Guixu and Shi, Jun and Zeng, Tieyong},
  doi          = {10.1007/s13042-024-02330-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1201-1218},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A lightweight self-ensemble feedback recurrent network for fast MRI reconstruction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended random forest for multivariate air quality
forecasting. <em>IJMLC</em>, <em>16</em>(2), 1175–1199. (<a
href="https://doi.org/10.1007/s13042-024-02329-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, an extended random forest algorithm for multivariate time series several steps forecasting is proposed. Peoposed method consists input layer and hidden layers involves random forest. In addition, a new algorithm is proposed in the third step to an ensemble of the tree’s outputs with the concept of correlation with the final results to reduce redundancy. In the output layer, a new algorithm is proposed to learn the weight of each random forest tree to calculate the result. Beijing PM25 and Italian air quality, were used to evaluate the proposed method. The results of the proposed method in this research were compared with the other state-of-the-art methods like deep learning and deep forest. We evaluated our proposed model based on evaluation metrics RMSE, MAE and MAPE and achieved good results. According to the results, the proposed method on the Beijing PM2.5 dataset’s RMSE and MAE value respectively are 40.97 and 24.81 for the average forecast for the next 1–6 h, 2.51 and 0.48 less than the best of the others. The average forecast for the next 1–3 h are 2.71 and 1.42 less than the best value of the others and are equal to 31.64 and 18.09. On the Italian air quality dataset, the RMSE, MAE and MAPE value for the next 1-h forecast are 0.6109, 0.4224 and 33.80 and better than the others.},
  archive      = {J_IJMLC},
  author       = {mirzadeh, Hossein and omranpour, Hesam},
  doi          = {10.1007/s13042-024-02329-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1175-1199},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Extended random forest for multivariate air quality forecasting},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced side information fusion framework for sequential
recommendation. <em>IJMLC</em>, <em>16</em>(2), 1157–1173. (<a
href="https://doi.org/10.1007/s13042-024-02328-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of side information in sequential recommendation (SR) is a recommendation system technique that combines a user’s historical behavior sequence with additional side information to provide more accurate personalized recommendations. Recent methods are based on self-attention mechanisms, incorporating side information as part of the attention matrix to update item representations. We believe that the integration method via self-attention mechanisms does not fully utilize side information. Therefore, we designed a new Enhanced Side Information Fusion framework (ESIF) for sequential recommendations. Specifically, we have altered the fusion strategy by using an attention matrix to simultaneously update the representations of items and side information, thereby increasing the use of side information. The attention matrix serves to balance various features, ensuring effective utilization of side information throughout the fusion process. We designed a Gated Linear Representation Fusion Module, comprising linear transformations and gated units. The linear transformation processes the input data, while the gated unit dynamically adjusts the degree of information flow based on the input. This module then combines the updated item representation with the side information representation for more efficient use of side information. Additionally, user interaction behavior data inevitably contains noise. The presence of noise can disrupt the model’s performance, affecting the accuracy and reliability of the results. Therefore, we introduced a denoising module in ESIF to enhance recommendation accuracy by reducing noise. Our experimental results demonstrate that ESIF achieves superior performance across five real-world datasets, surpassing the current state-of-the-art side information fusion SR models.},
  archive      = {J_IJMLC},
  author       = {Su, Zheng-Ang and Zhang, Juan and Fang, Zhijun and Gao, Yongbin},
  doi          = {10.1007/s13042-024-02328-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1157-1173},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Enhanced side information fusion framework for sequential recommendation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural network based time estimator for SAT solver.
<em>IJMLC</em>, <em>16</em>(2), 1145–1156. (<a
href="https://doi.org/10.1007/s13042-024-02327-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SAT-based formal verification is a systematic process to prove the correctness of computer hardware design based on formal specifications, providing an alternative to time-consuming simulations and ensuring design reliability and accuracy. Predicting the runtime of SAT solvers is important to effectively allocate verification resources and determine if the verification can be completed within time limits. Predicting SAT solver runtime is challenging due to variations in solving time across different solvers and dependence on problem complexity and solver mechanisms. Existing approaches rely on feature engineering and machine learning, but they have drawbacks in terms of expert knowledge requirements and time-consuming feature extraction. To address this, using graph neural networks (GNNs) for runtime prediction is considered, as they excel in capturing graph topology and relationships. However, directly applying existing GNNs to predict SAT solver runtime does not yield satisfactory results, as SAT solvers’ proving procedure is crucial. In this paper, we propose a novel model, TESS, that integrates the working mechanism of SAT solvers with graph neural networks (GNNs) for predicting solving time. The model incorporates a graph representation inspired by the CDCL paradigm, proposes adaptive aggregation for multilayer information and separate modules for conflict learning. Experimental results on multiple datasets validate the effectiveness, scalability, and robustness of our model, outperforming baselines in SAT solver runtime prediction.},
  archive      = {J_IJMLC},
  author       = {Liu, Jiawei and Xiao, Wenyi and Cheng, Hongtao and Shi, Chuan},
  doi          = {10.1007/s13042-024-02327-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1145-1156},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Graph neural network based time estimator for SAT solver},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design your own universe: A physics-informed agnostic method
for enhancing graph neural networks. <em>IJMLC</em>, <em>16</em>(2),
1129–1144. (<a
href="https://doi.org/10.1007/s13042-024-02326-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can fit both homophilic and heterophilic graphs. Empirical validations on benchmarks for homophilic, heterophilic graphs, and long-term graph datasets show that GNNs enhanced by our method significantly outperform their original counterparts.},
  archive      = {J_IJMLC},
  author       = {Shi, Dai and Han, Andi and Lin, Lequan and Guo, Yi and Wang, Zhiyong and Gao, Junbin},
  doi          = {10.1007/s13042-024-02326-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1129-1144},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Design your own universe: A physics-informed agnostic method for enhancing graph neural networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven mixed integer programming approach for joint
chance-constrained optimal power flow under uncertainty. <em>IJMLC</em>,
<em>16</em>(2), 1111–1127. (<a
href="https://doi.org/10.1007/s13042-024-02325-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel mixed integer programming (MIP) reformulation for the joint chance-constrained optimal power flow problem under uncertain load and renewable energy generation. Unlike traditional models, our approach incorporates a comprehensive evaluation of system-wide risk without decomposing joint chance constraints into individual constraints, thus preventing overly conservative solutions and ensuring robust system security. A significant innovation in our method is the use of historical data to form a sample average approximation that directly informs the MIP model, bypassing the need for distributional assumptions to enhance solution robustness. Additionally, we implement a model improvement strategy to reduce the computational burden, making our method more scalable for large-scale power systems. Our approach is validated against benchmark systems, i.e., IEEE 14-, 57- and 118-bus systems, demonstrating superior performance in terms of cost-efficiency and robustness, with lower computational demand compared to existing methods.},
  archive      = {J_IJMLC},
  author       = {Qin, James Ciyu and Jiang, Rujun and Mo, Huadong and Dong, Daoyi},
  doi          = {10.1007/s13042-024-02325-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1111-1127},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A data-driven mixed integer programming approach for joint chance-constrained optimal power flow under uncertainty},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dap-SiMT: Divergence-based adaptive policy for simultaneous
machine translation. <em>IJMLC</em>, <em>16</em>(2), 1091–1110. (<a
href="https://doi.org/10.1007/s13042-024-02323-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Simultaneous Machine Translation (SiMT), a robust read/write (R/W) policy is essential alongside a high-quality translation model. Traditional methods typically employ either a fixed wait-k policy in sync with a wait-k translation model or an adaptive policy that is co-developed with a dedicated translation model. This study introduces a more versatile approach by decoupling the adaptive policy from the translation model. Our rationale is based on the finding that an independent multi-path wait-k model, when combined with adaptive policies utilized in advanced SiMT systems, can perform competitively. Specifically, we present DaP, a divergence-based adaptive policy, which dynamically adjusts read/write decisions for any translation model, taking into account potential divergence in translation distributions resulting from future information. Extensive experiments across multiple benchmarks reveal that our method significantly enhances the balance between translation accuracy and latency, surpassing strong baselines.},
  archive      = {J_IJMLC},
  author       = {Zhao, Libo and Zeng, Ziqian},
  doi          = {10.1007/s13042-024-02323-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1091-1110},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Dap-SiMT: Divergence-based adaptive policy for simultaneous machine translation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical dual-view model for fake news detection
guided by discriminative lexicons. <em>IJMLC</em>, <em>16</em>(2),
1071–1090. (<a
href="https://doi.org/10.1007/s13042-024-02322-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news detection aims to automatically identify the credibility of source posts, mitigating potential societal harm and conserving human resources. Textual fake news detection methods can be categorized into pattern- and fact-based. Pattern-based models focus on identifying shared writing patterns in source posts, while fact-based models leverage auxiliary external knowledge. Researchers have recently attempted to merge these two views into a comprehensive detection system, achieving superior performance to single-view methods. However, existing dual-view methods often prioritize integrating single-view methods over exploring nuanced characteristics of both perspectives. To address this, we propose a novel hierarchical dual-view model for fake news detection guided by discriminative lexicons. First, we construct two lexicons based on distinct word usage tendencies in fake and real news and further augment them with synonyms sourced from large language models. We then devise a hierarchical attention network to derive semantic representations for the source post, incorporating a lexicon attention loss to guide the prioritization of words from the two lexicons. Subsequently, a lexicon-guided interaction network is employed to model the relations between the source post and its relevant articles, assigning authenticity-aware weights to each article. Finally, the representations of source post and relevant articles are concatenated for joint detection. According to experimental results, our model outperforms many competitive baselines in terms of the macro F1 score ranging from 1.1% to 10.5% on Weibo and from 3.2% to 10.8% on Twitter.},
  archive      = {J_IJMLC},
  author       = {Yang, Sijia and Li, Xianyong and Du, Yajun and Huang, Dong and Chen, Xiaoliang and Fan, Yongquan and Wang, Shumin},
  doi          = {10.1007/s13042-024-02322-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1071-1090},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A hierarchical dual-view model for fake news detection guided by discriminative lexicons},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-stage zero-shot object detection network based on
CLIP and pseudo-labeling. <em>IJMLC</em>, <em>16</em>(2), 1055–1070. (<a
href="https://doi.org/10.1007/s13042-024-02321-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of unknown objects is a challenging task in computer vision because, although there are diverse real-world detection object categories, existing object-detection training sets cover a limited number of object categories . Most existing approaches use two-stage networks to improve a model’s ability to characterize objects of unknown classes, which leads to slow inference. To address this issue, we proposed a single-stage unknown object detection method based on the contrastive language-image pre-training (CLIP) model and pseudo-labelling, called CLIP-YOLO. First, a visual language embedding alignment method is introduced and a channel-grouped enhanced coordinate attention module is embedded into a YOLO-series detection head and feature-enhancing component, to improve the model’s ability to characterize and detect unknown category objects. Second, the pseudo-labelling generation is optimized based on the CLIP model to expand the diversity of the training set and enhance the ability to cover unknown object categories. We validated this method on four challenging datasets: MSCOCO, ILSVRC, Visual Genome, and PASCAL VOC. The results show that our method can achieve higher accuracy and faster speed, so as to obtain better performance of unknown object detection. The source code is available at https://github.com/BJUTsipl/CLIP-YOLO .},
  archive      = {J_IJMLC},
  author       = {Li, Jiafeng and Sun, Shengyao and Zhang, Kang and Zhang, Jing and Zhuo, Li},
  doi          = {10.1007/s13042-024-02321-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1055-1070},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Single-stage zero-shot object detection network based on CLIP and pseudo-labeling},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic configuration network modeling method based on
information superposition and mixture correntropy. <em>IJMLC</em>,
<em>16</em>(2), 1041–1054. (<a
href="https://doi.org/10.1007/s13042-024-02320-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the generalizability and robustness of stochastic configuration networks (SCNs), this paper proposes a robust modeling method based on information superposition and mixture correntropy. First, the mapping information of the (sigmoid) activation function and its derivative function is superimposed, and the hidden layer parameters are randomly assigned through a supervisory mechanism to improve the diversity of the hidden layer mapping. Second, mixture correntropy is used to construct a robust loss function, and different Gaussian kernels are used to measure the contribution of training samples to suppress the negative impact of data noise on the accuracy of the model. Finally, the performance of the proposed modeling method is tested on functional approximation, four benchmark datasets, and historical data from the municipal solid waste incineration process. The experimental results show that the modeling method proposed in this paper has advantages in terms of generalizability and robustness.},
  archive      = {J_IJMLC},
  author       = {Yan, Aijun and Hu, Kaicheng and Wang, Dianhui},
  doi          = {10.1007/s13042-024-02320-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1041-1054},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Stochastic configuration network modeling method based on information superposition and mixture correntropy},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for medicine: A survey.
<em>IJMLC</em>, <em>16</em>(2), 1015–1040. (<a
href="https://doi.org/10.1007/s13042-024-02318-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address challenges in the digital economy’s landscape of digital intelligence, large language models (LLMs) have been developed. Improvements in computational power and available resources have significantly advanced LLMs, allowing their integration into diverse domains for human life. Medical LLMs are essential application tools with potential across various medical scenarios. In this paper, we review LLM developments, focusing on the requirements and applications of medical LLMs. We provide a concise overview of existing models, aiming to explore advanced research directions and benefit researchers for future medical applications. We emphasize the advantages of medical LLMs in applications, as well as the challenges encountered during their development. Finally, we suggest directions for technical integration to mitigate challenges and potential research directions for the future of medical LLMs, aiming to meet the demands of the medical field better.},
  archive      = {J_IJMLC},
  author       = {Zheng, Yanxin and Gan, Wensheng and Chen, Zefeng and Qi, Zhenlian and Liang, Qian and Yu, Philip S.},
  doi          = {10.1007/s13042-024-02318-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {1015-1040},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Large language models for medicine: A survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term time series forecasting based on siamese network:
A perspective on few-shot learning. <em>IJMLC</em>, <em>16</em>(2),
999–1014. (<a href="https://doi.org/10.1007/s13042-024-02317-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-term time series forecasting (LTSF) plays a crucial role in various domains, utilizing a large amount of historical data to forecast trends over an extended future time range. However, in real-life scenarios, the performance of LTSF is often hindered by missing data. Few-shot learning aims to address the issue of data scarcity, but there is relatively little research on using few-shot learning to tackle sample scarcity in long-term time series forecasting tasks, and most few-shot learning methods rely on transfer learning. To address this problem, this paper proposes a Siamese network-based time series Transformer (SiaTST) for the task of LTSF in a few-shot setting. To increase the diversity of input scales and better capture local features in time series, we adopt a dual-level hierarchical input strategy. Additionally, we introduce a learnable prediction token (LPT) to capture global features of the time series. Furthermore, a feature fusion layer is utilized to capture dependencies among multiple variables and integrate information from different levels. Experimental results on 7 popular LSTF datasets demonstrate that our proposed model achieves state-of-the-art performance.},
  archive      = {J_IJMLC},
  author       = {Fan, Jin and Xiang, Jiaqian and Liu, Jie and Wang, Zheyu and Wu, Huifeng},
  doi          = {10.1007/s13042-024-02317-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {999-1014},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Long-term time series forecasting based on siamese network: A perspective on few-shot learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Color attention tracking with score matching.
<em>IJMLC</em>, <em>16</em>(2), 983–997. (<a
href="https://doi.org/10.1007/s13042-024-02316-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an ordinary practice that deep networks are utilized to extract deep features from RGB images. Typically, the popular trackers adopt pre-trained ResNet as a backbone to extract target features, achieving excellent performance. Moreover, Staple has shown that color statistics have complementary cues, while the combination of color statistics and deep features in a unified deep framework has rarely been reported. Therefore, we employ color statistics to construct color attention maps, which are encoded into the deep network to guide the generation of target-aware feature maps. Additionally, DCF-based trackers have an online update module to dynamically update the tracking model, it is particularly necessary to collect reliable target samples. Hence, we refer to the template matching thought to design a score matching method, which is intended to score the tracked targets, this method has the advantage of considering the target extent. In this paper, we conduct sufficient ablation analyses on the color attention module and score matching method to verify their effectiveness. Furthermore, our approaches are combined into the DCF frameworks to construct two brand-new trackers, and both quantitative and qualitative results demonstrate that our trackers can perform favorably against recent and far more sophisticated trackers on multiple public benchmarks.},
  archive      = {J_IJMLC},
  author       = {He, Xuedong and Huang, Jiehui},
  doi          = {10.1007/s13042-024-02316-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {983-997},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Color attention tracking with score matching},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing automated street crime detection: A drone-based
system integrating CNN models and enhanced feature selection techniques.
<em>IJMLC</em>, <em>16</em>(2), 959–981. (<a
href="https://doi.org/10.1007/s13042-024-02315-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a pioneering solution to the growing challenge of escalating global crime rates through the introduction of an automated drone-based street crime detection system. Leveraging advanced Convolutional Neural Network (CNN) models, the system integrates several key components for analyzing images captured by drones. Initially, the Embedding Bilateral Filter (EBF) technique divides images into base and detail layers to enhance detection accuracy. The fusion model, IR with attention-based Conv-ViT, combines Inception-V3, ResNet-50, and Convolution Vision Transformer (Conv-ViT) to capture both shape and texture details efficiently. Further enhancement is achieved through the Improved Shark Smell Optimization Algorithm (ISSOA), which optimizes feature selection and minimizes redundancy in image extraction. Additionally, a Multi-scale Contextual Semantic Guidance Network (MCS-GNet) ensures robust image classification by integrating features from multiple layers to prevent data loss. Evaluation on the UCF-Crime and UCSD Ped2 datasets demonstrates superior accuracy, with remarkable results of 0.783 and 0.974, respectively. This innovative approach offers a promising solution to the arduous and continuous task of monitoring security camera feeds for suspicious activities, thereby addressing the pressing need for automated crime detection systems on a global scale.},
  archive      = {J_IJMLC},
  author       = {Vuyyuru, Lakshma Reddy and Purimetla, NagaMalleswara Rao and Reddy, Kancharakunt Yakub and Vellela, Sai Srinivas and Basha, Sk Khader and Vatambeti, Ramesh},
  doi          = {10.1007/s13042-024-02315-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {959-981},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Advancing automated street crime detection: A drone-based system integrating CNN models and enhanced feature selection techniques},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale frequency domain learning for texture
classification. <em>IJMLC</em>, <em>16</em>(2), 947–958. (<a
href="https://doi.org/10.1007/s13042-024-02314-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, methods for modeling images in frequency domain have attracted widespread attention. Frequency methods transform image into spectrum as model input by defining a set of basis functions, where each point in frequency domain represents image’s projection at the corresponding frequency, which helps to understand structure, texture, and other basic features of the image. Studies have shown that frequency learning methods based on fixed small-scale Discrete Cosine Transform (DCT) basis functions outperform spatial domain methods. However, frequency learning also faces the challenge of feature saturation bottleneck, particularly fixed DCT basis functions makes the frequency learning model more prone to reaching saturation. Therefore, we propose a multi-scale frequency domain learning approach that utilizes multi-scale DCT basis functions to extract diversified frequency features. Our method employs sampling and cropping techniques to extend the frequency features from a single-scale DCT basis functions mapping to a multi-scale mapping. This ensures that the image projection exists at as many frequencies as possible so that advanced features can be extracted over multiple frequency ranges to overcome the saturation bottleneck of network models. Experimental results demonstrate the superior performance of our approach in texture classification tasks.},
  archive      = {J_IJMLC},
  author       = {Zang, Liguang and Li, Yuancheng},
  doi          = {10.1007/s13042-024-02314-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {947-958},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-scale frequency domain learning for texture classification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient federated learning with cross-resource client
collaboration. <em>IJMLC</em>, <em>16</em>(2), 931–945. (<a
href="https://doi.org/10.1007/s13042-024-02313-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning paradigm. Traditional federated learning performs well on the premise that all clients have the same learning ability or similar learning tasks. However, resource and data heterogeneity are inevitable among clients in real-world scenarios, leading to the situation that existing federated learning mechanisms cannot achieve high accuracy in short response time. In this study, an effective federated learning framework with cross-resource client collaboration, termed CEFL, is proposed to coordinate clients with different capacities to help each other, efficiently and adequately reflecting collective intelligence. Clients are categorized into different clusters based on their computational resources in the hierarchical framework. Resource-rich clusters use their knowledge to assist resource-limited clusters converge rapidly. Once resource-limited clusters have the ability to mentor others, resource-rich clusters learn from the resource-limited clusters in their favor to improve their own effectiveness. A cloud server provides tailored assistance to each cluster with a personalized model through an adaptive multi-similarity metric, in order for each cluster to learn the most useful knowledge. The experiments fully demonstrate that the proposed method not only has superior accuracy with significantly reduced latency but also improves the convergence rate compared to other state-of-the-art federated learning methods in addressing the problem of resource and data heterogeneity.},
  archive      = {J_IJMLC},
  author       = {Shen, Qi and Yang, Liu},
  doi          = {10.1007/s13042-024-02313-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {931-945},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Efficient federated learning with cross-resource client collaboration},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-enhanced recommendation via dynamic co-attention
and high-order connectivity. <em>IJMLC</em>, <em>16</em>(2), 919–930.
(<a href="https://doi.org/10.1007/s13042-024-02312-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) based recommender systems have shown promise in improving accuracy and interpretability. They reveal the intrinsic relationships of knowledge through the associations and paths between entities for personalized recommendations. However, existing approaches do not adequately consider the high-order connections between neighboring nodes in the relational graph, resulting in a lack of sufficient capture of structured information. In this paper, we propose a knowledge-enhanced recommendation model via dynamic co-attention and high-order connectivity (DCHC) to address this issue. First, we construct a hybrid graph by aligning users and items in the user-item bipartite graph with entities in the KG. As a result, we are able to simultaneously consider the interaction between users and items as well as the entity information in the KG, thereby gaining a more comprehensive understanding of user behavior and interests. Second, we explicitly model the high-order connections between entities through the hybrid structured graphs in an end-to-end manner. Therefore, we not only explored the complex interactive relationships between entities but also ensured the accurate capture of structural information in the graph. Third, we employ a dynamic co-attention mechanism to enhance the representation of users and items, effectively exploiting the potential correlation between them. We therefore effectively exploited the potential correlation between users and items and successfully integrating these relationships into their representations. Extensive experiments conducted on three benchmarks demonstrate that DCHC outperforms state-of-the-art KG-based recommendation methods.},
  archive      = {J_IJMLC},
  author       = {Wang, Dan-Dong and Min, Fan},
  doi          = {10.1007/s13042-024-02312-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {919-930},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Knowledge-enhanced recommendation via dynamic co-attention and high-order connectivity},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial recurrent neural network coordinated secured
transmission towards safeguarding confidentiality in smart industrial
internet of things. <em>IJMLC</em>, <em>16</em>(2), 891–917. (<a
href="https://doi.org/10.1007/s13042-024-02310-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces a new method to tackle the issue of exchanging cryptographic keys in the Industrial Internet of Things (IIoT). This study focuses on the inefficiency and lengthy evaluation procedures of conventional cryptographic key exchange algorithms, which are not appropriate for the rapid and constantly changing IIoT device environment. In the solution domain, the proposed approach uses synchronization of neural networks with vector valued and Recurrent Neural Networks (RNNs), merging drive-response mechanisms to enhance speed and efficiency in crucial operations. The research examines the influence of postponements on the generating arbitrary inputs and coordination challenges in RNNs that incorporate drive-response mechanisms for synchronized input vector creation. This article explains an elementary evaluation of coordination in Artificial Neural Networks (ANNs) by utilizing an RNN framework to structure ANNs for sharing session keys. The study provides multiple contributions: (1) employing the polynomial coordination technique to generate coordinated inputs for the ANN synchronization process using RNNs, (2) using Lyapunov formulas and inequality assessment methods to identify required control parameters and time-varying conditions for achieving synchronization in the drive-response systems proposed with polynomial and non-polynomial functions, (3) demonstrating the connection between polynomial and non-polynomial synchronization with numerical illustrations, and (4) designing symmetric layouts of ANNs to create a session keys in the IIoT network. The suggested technique outperforms existing methods in the literature by offering a quicker, more dependable solution for cryptographic key exchange, paving the way for improved and secure industrial applications. This new method not only fixes current inefficiencies but also paves the way for future improvements in secure communication in the IIoT environment.},
  archive      = {J_IJMLC},
  author       = {Sarkar, Arindam and Singh, Moirangthem Marjit and Sharma, Hanjabam Saratchandra},
  doi          = {10.1007/s13042-024-02310-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {891-917},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Artificial recurrent neural network coordinated secured transmission towards safeguarding confidentiality in smart industrial internet of things},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dynamic residual graph convolutional network with
global feature enhancement for traffic flow prediction. <em>IJMLC</em>,
<em>16</em>(2), 873–889. (<a
href="https://doi.org/10.1007/s13042-024-02307-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to achieving an accurate and reliable traffic flow prediction lies in modeling the complex and dynamic correlations among sensors. However, existing studies ignore the fact that such correlations are influenced by multiple dynamic factors and the original sequence features of the traffic data, which limits the deep modeling of such correlations and leads to a biased understanding of such correlations. The extraction strategies for global features are less developed, which has degraded the reliability of the predictions. In this study, a novel multi-dynamic residual graph convolutional network with global feature enhancement is proposed to solve these problems and achieve an accurate and reliable traffic flow prediction. First, multiple graph generators are proposed, which fully preserve the original sequence features of the traffic data and enable layered depth-wise modeling of the dynamic correlations among sensors through a residual mechanism. Second, an output module is proposed to explore extraction strategies for global features, by employing a residual mechanism and parameter sharing strategy to maintain the consistency of the global features. Finally, a new layered network architecture is proposed, which not only leverages the advantages of both static and dynamic graphs, but also captures the spatiotemporal dependencies among sensors. The superiority of the proposed model has been verified through extensive experiments on two real-world datasets.},
  archive      = {J_IJMLC},
  author       = {Li, Xiangdong and Yin, Xiang and Huang, Xiaoling and Liu, Weishu and Zhang, Shuai and Zhang, Dongping},
  doi          = {10.1007/s13042-024-02307-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {873-889},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-dynamic residual graph convolutional network with global feature enhancement for traffic flow prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RHGNN: Imposing relational inductive bias for heterogeneous
graph neural network. <em>IJMLC</em>, <em>16</em>(2), 855–871. (<a
href="https://doi.org/10.1007/s13042-024-02305-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph data are ubiquitous and extracting information from them is increasingly crucial. Existing approaches for modeling heterogeneous graphs often rely on the splitting strategy guided by meta-paths or relations. However, the splitting strategy falls short in capturing the rich semantic information and topological structure of heterogeneous graphs comprehensively, as well as the hidden semantic associations between various edge types. Moreover, establishing meta-paths for heterogeneous graphs requires substantial expert experience and domain knowledge, making the process time-consuming, laborious, and often inaccurate. To address these challenges, we propose a novel heterogeneous graph neural network called RHGNN, which overcomes the reliance on meta-paths by modeling the heterogeneous graph as a whole instead of splitting it, and jointly obtains personalized feature representations for nodes and edges. Specifically, we first introduce the pre-training strategy Type2vec to learn feature representations of edge types and extract the semantic and structural information of the entire heterogeneous graph. Then we view the heterogeneous graph as a homogeneous one, and design a new information propagation and aggregation mechanism for both nodes and edges to learn their representations. Based on the assumption that edges with the same type have similar representations, we introduce an explicit regularization method based on the relational inductive bias to impose smoothness constraints on the model. Experimental results demonstrate that RHGNN significantly outperforms state-of-the-art models on both node-level and edge-level tasks.},
  archive      = {J_IJMLC},
  author       = {Zhu, Shichao and Zhang, Shuai and Liu, Yang and Zhou, Chuan and Pan, Shirui and Li, Zhao and Chen, Hongyang},
  doi          = {10.1007/s13042-024-02305-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {855-871},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {RHGNN: Imposing relational inductive bias for heterogeneous graph neural network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection and analysis of android malwares using hybrid dual
path bi-LSTM kepler dynamic graph convolutional network. <em>IJMLC</em>,
<em>16</em>(2), 835–853. (<a
href="https://doi.org/10.1007/s13042-024-02303-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In past decade, the android malware threats have been rapidly increasing with the widespread usage of internet applications. In respect of security purpose, there are several machine learning techniques attempted to detect the malwares effectively, but failed to achieve the accurate detection due to increasing number of features, more time consumption decreases in detection efficiency. To overcome these limitations, in this research work an innovative Hybrid dual path Bidirectional long short-term memory Kepler dynamic graph Convolutional Network (HBKCN) is proposed to analyze and detect android malwares effectively. First, the augmented abstract syntax tree is applied for pre-processing and extracts the string function from each malware. Second, the adaptive aphid ant optimization is utilized to choose the most appropriate features and remove irrelevant features. Finally, the proposed HBKCN classifies benign and malware apps based on their specifications. Four benchmark datasets, namely Drebin, VirusShare, Malgenome -215, and MaMaDroid datasets, are employed to estimate the effectiveness of the technique. The result demonstrates that the HBKCN technique achieved excellent performance with respect to a few important metrics compared to existing methods. Moreover, detection accuracies of 99.2%, 99.1%,99.8% and 99.8% are achieved for the considered datasets, respectively. Also, the computation time is greatly reduced, illustrating the efficiency of the proposed model in identifying android malwares.},
  archive      = {J_IJMLC},
  author       = {Lingayya, Sadananda and Kulkarni, Praveen and Salins, Rohan Don and Uppoor, Shruthi and Gurudas, V. R.},
  doi          = {10.1007/s13042-024-02303-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {835-853},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Detection and analysis of android malwares using hybrid dual path bi-LSTM kepler dynamic graph convolutional network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reciprocal interlayer-temporal discriminative target model
for robust visual tracking. <em>IJMLC</em>, <em>16</em>(2), 819–834. (<a
href="https://doi.org/10.1007/s13042-024-02296-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Siamese algorithms take little account of the information interaction between the target and search region, leading to tracking results that are often disturbed by the cumulative effect of target-like distractors between layers. In this paper, we propose a reciprocal interlayer-temporal discriminative target model for robust visual tracking. Firstly, an interlayer target-aware enhancement model is constructed, which establishes pixel-by-pixel correlation between the template and search region to achieve interlayer feature information interaction. This alleviates the cumulative error caused by the blindness of the target to search region during feature extraction, enhancing target perception. Secondly, to weaken the impact of interference, a temporal interference evaluation strategy is designed. It utilizes the interframe candidate propagation module to build associations among multi-candidates in the current frame and the previous frame. Then, the similar distractors are eliminated by using object inference constraint, so as to obtain a more accurate target location. Finally, we integrate the interlayer target-aware enhancement model and temporal interference evaluation strategy into the Siamese framework to achieve reciprocity for robust target tracking. Experimental results show that our tracking approach performs well, especially on seven benchmark datasets, including OTB-100, TC-128, DTB, UAV-123, VOT-2016, VOT-2018 and GOT-10k.},
  archive      = {J_IJMLC},
  author       = {Zhang, Huanlong and Ma, Zonghao and Zhao, Yanchun and Wang, Yong and Jiang, Bin},
  doi          = {10.1007/s13042-024-02296-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {819-834},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Reciprocal interlayer-temporal discriminative target model for robust visual tracking},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-graph aggregated graph neural network for
heterogeneous graph representation learning. <em>IJMLC</em>,
<em>16</em>(2), 803–818. (<a
href="https://doi.org/10.1007/s13042-024-02294-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural networks have attracted considerable attention for their proficiency in handling intricate heterogeneous structures. However, most existing methods model semantic relationships in heterogeneous graphs by manually defining meta-paths, inadvertently overlooking the inherent incompleteness of such graphs. To address this issue, we propose a multi-graph aggregated graph neural network (MGAGNN) for heterogeneous graph representation learning, which simultaneously leverages attribute similarity and high-order semantic information between nodes. Specifically, MGAGNN first employs the feature graph generator to generate a feature graph for completing the original graph structure. A semantic graph is then generated using a semantic graph generator, capturing higher-order semantic information through automatic meta-path learning. Finally, we aggregate the two candidate graphs to reconstruct a new heterogeneous graph and learn node embedding by graph convolutional networks. Extensive experiments on real-world datasets demonstrate the superior performance of the proposed method over state-of-the-art approaches.},
  archive      = {J_IJMLC},
  author       = {Zhu, Shuailei and Wang, Xiaofeng and Lai, Shuaiming and Chen, Yuntao and Zhai, Wenchao and Quan, Daying and Qi, Yuanyuan and Lv, Laishui},
  doi          = {10.1007/s13042-024-02294-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {803-818},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-graph aggregated graph neural network for heterogeneous graph representation learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial attack method based on enhanced spatial
momentum. <em>IJMLC</em>, <em>16</em>(2), 789–802. (<a
href="https://doi.org/10.1007/s13042-024-02290-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been widely applied in many fields, but it is found that they are vulnerable to adversarial examples, which can mislead the DNN-based models with imperceptible perturbations. Many adversarial attack methods can achieve great success rates when attacking white-box models, but they usually exhibit poor transferability when attacking black-box models. Momentum iterative gradient-based methods can effectively improve the transferability of adversarial examples. Still, the momentum update mechanism of existing methods may lead to a problem of unstable gradient update direction and result in poor local optima. In this paper, we propose an enhanced spatial momentum iterative gradient-based adversarial attack method. Specifically, we introduce the spatial domain momentum accumulation mechanism. Instead of only accumulating the gradients of data points on the optimization path in the gradient update process, we additionally accumulate the average gradients of multiple sampling points within the neighborhood of data points. This mechanism fully utilizes the contextual gradient information of different regions within the image to smooth the accumulated gradients and find a more stable gradient update direction, thus escaping from poor local optima. Empirical results on the standard ImageNet dataset demonstrate that our method can significantly improve the attack success rate of momentum iterative gradient-based methods and shows excellent attack performance not only against normally trained models but also against adversarial training and defense models, outperforming the state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Hu, Jun and Wei, Guanghao and Xia, Shuyin and Wang, Guoyin},
  doi          = {10.1007/s13042-024-02290-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {789-802},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adversarial attack method based on enhanced spatial momentum},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPERM: Sequential pairwise embedding recommendation with
MI-FGSM. <em>IJMLC</em>, <em>16</em>(2), 771–787. (<a
href="https://doi.org/10.1007/s13042-024-02288-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual recommendation systems have shown remarkable performance by leveraging consumer feedback and the visual attributes of products. However, recent concerns have arisen regarding the decline in recommendation quality when these systems are subjected to attacks that compromise the model parameters. While the fast gradient sign method (FGSM) and iterative FGSM (I-FGSM) are well-studied attack strategies, the momentum iterative FGSM (MI-FGSM), known for its superiority in the computer vision (CV) domain, has been overlooked. This oversight raises the possibility that visual recommender systems may be vulnerable to MI-FGSM, leading to significant vulnerabilities. Adversarial training, a regularization technique designed to withstand MI-FGSM attacks, could be a promising solution to bolster model resilience. In this research, we introduce MI-FGSM for visual recommendation. We propose the Sequential Pairwise Embedding Recommender with MI-FGSM (SPERM), a model that incorporates visual, temporal, and sequential information for visual recommendations through adversarial training. Specifically, we employ higher-order Markov chains to capture consumers’ sequential behaviors and utilize visual pairwise ranking to discern their visual preferences. To optimize the SPERM model, we employ a learning method based on AdaGrad. Moreover, we fortify the SPERM approach with adversarial training, where the primary objective is to train the model to withstand adversarial inputs introduced by MI-FGSM. Finally, we evaluate the effectiveness of our approach by conducting experiments on three Amazon datasets, comparing it with existing visual and adversarial recommendation algorithms. Our results demonstrate the efficacy of the proposed SPERM model in addressing adversarial attacks while enhancing visual recommendation performance.},
  archive      = {J_IJMLC},
  author       = {Paul, Agyemang and Wan, Yuxuan and Chen, Boyu and Wu, Zhefu},
  doi          = {10.1007/s13042-024-02288-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {771-787},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {SPERM: Sequential pairwise embedding recommendation with MI-FGSM},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi-framelets: Robust graph neural networks via adaptive
framelet convolution. <em>IJMLC</em>, <em>16</em>(2), 755–770. (<a
href="https://doi.org/10.1007/s13042-024-02286-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to provide a novel design of a multiscale framelet convolution for spectral graph neural networks (GNNs). While current spectral methods excel in various graph learning tasks, they often lack the flexibility to adapt to noisy, incomplete, or perturbed graph signals, making them fragile in such conditions. Our newly proposed framelet convolution addresses these limitations by decomposing graph data into low-pass and high-pass spectra through a finely-tuned multiscale approach. Our approach directly designs filtering functions within the spectral domain, allowing for precise control over the spectral components. The proposed design excels in filtering out unwanted spectral information and significantly reduces the adverse effects of noisy graph signals. Our approach not only enhances the robustness of GNNs but also preserves crucial graph features and structures. Through extensive experiments on diverse, real-world graph datasets, we demonstrate that our framelet convolution achieves superior performance in node classification tasks. It exhibits remarkable resilience to noisy data and adversarial attacks, highlighting its potential as a robust solution for real-world graph applications. This advancement opens new avenues for more adaptive and reliable spectral GNN architectures.},
  archive      = {J_IJMLC},
  author       = {Yang, Mengxi and Shi, Dai and Zheng, Xuebin and Yin, Jie and Gao, Junbin},
  doi          = {10.1007/s13042-024-02286-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  month        = {2},
  number       = {2},
  pages        = {755-770},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Quasi-framelets: Robust graph neural networks via adaptive framelet convolution},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="invent---6">INVENT - 6</h2>
<ul>
<li><details>
<summary>
(2025). The amplituhedron BCFW triangulation. <em>INVENT</em>,
<em>239</em>(3), 1009–1138. (<a
href="https://doi.org/10.1007/s00222-025-01316-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amplituhedron $\mathcal{A}_{n,k,4}$ is a geometric object, introduced by Arkani-Hamed and Trnka (2013) in the study of scattering amplitudes in quantum field theories. They conjecture that $\mathcal{A}_{n,k,4}$ admits a decomposition into images of BCFW positroid cells, arising from the Britto–Cachazo–Feng–Witten recurrence (2005). We prove that this conjecture is true.},
  archive      = {J_INVENT},
  author       = {Even-Zohar, Chaim and Lakrec, Tsviqa and Tessler, Ran J.},
  doi          = {10.1007/s00222-025-01316-1},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {1009-1138},
  shortjournal = {Invent. Math.},
  title        = {The amplituhedron BCFW triangulation},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curvature and sharp growth rates of log-quasimodes on
compact manifolds. <em>INVENT</em>, <em>239</em>(3), 947–1008. (<a
href="https://doi.org/10.1007/s00222-025-01315-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We obtain new optimal estimates for the $L^{2}(M)\to L^{q}(M)$ , $q\in (2,q_{c}]$ , $q_{c}=2(n+1)/(n-1)$ , operator norms of spectral projection operators associated with spectral windows $[\lambda ,\lambda +\delta (\lambda )]$ , with $\delta (\lambda )=O((\log \lambda )^{-1})$ on compact Riemannian manifolds $(M,g)$ of dimension $n\ge 2$ all of whose sectional curvatures are nonpositive or negative. We show that these two different types of estimates are saturated on flat manifolds or manifolds all of whose sectional curvatures are negative. This allows us to classify compact space forms in terms of the size of $L^{q}$ -norms of quasimodes for each Lebesgue exponent $q\in (2,q_{c}]$ , even though it is impossible to distinguish between ones of negative or zero curvature sectional curvature for any $q&gt;q_{c}$ .},
  archive      = {J_INVENT},
  author       = {Huang, Xiaoqi and Sogge, Christopher D.},
  doi          = {10.1007/s00222-025-01315-2},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {947-1008},
  shortjournal = {Invent. Math.},
  title        = {Curvature and sharp growth rates of log-quasimodes on compact manifolds},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hausdorff dimension of the apollonian gasket.
<em>INVENT</em>, <em>239</em>(3), 909–946. (<a
href="https://doi.org/10.1007/s00222-024-01311-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Apollonian gasket is a well-studied circle packing. Important properties of the packing, including the distribution of the circle radii, are governed by its Hausdorff dimension. No closed form is currently known for the Hausdorff dimension, and its computation is a special case of a more general and hard problem: effective, rigorous estimates of dimension of a parabolic limit set. In this paper we develop an efficient method for solving this problem which allows us to compute the dimension of the gasket to 128 decimal places and rigorously justify the error bounds. We expect our approach to generalise easily to other parabolic fractals.},
  archive      = {J_INVENT},
  author       = {Vytnova, Polina L. and Wormell, Caroline L.},
  doi          = {10.1007/s00222-024-01311-y},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {909-946},
  shortjournal = {Invent. Math.},
  title        = {Hausdorff dimension of the apollonian gasket},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analytic invariant of <span
class="math inline"><em>G</em><sub>2</sub></span> manifolds.
<em>INVENT</em>, <em>239</em>(3), 865–907. (<a
href="https://doi.org/10.1007/s00222-024-01310-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that the moduli space of holonomy $G_{2}$ -metrics on a closed 7-manifold can be disconnected by presenting a number of explicit examples. We detect different connected components of the $G_{2}$ -moduli space by defining an analytic refinement $\bar{\nu }(M, g) \in \mathbb{Z}$ of the defect invariant $\nu (M,\varphi )\in \mathbb{Z}/48$ of $G_{2}$ -structures $\varphi $ on a closed 7-manifold $M$ introduced by the first and third authors. The $\bar{\nu }$ -invariant is defined using $\eta $ -invariants and Mathai-Quillen currents on $M$ and we compute it for twisted connected sums à la Kovalev, Corti-Haskins-Nordström-Pacini and extra-twisted connected sums as constructed by the second and third authors. In particular, we find examples of $G_{2}$ -holonomy metrics in different components of the moduli space where the associated $G_{2}$ -structures are homotopic and other examples where they are not.},
  archive      = {J_INVENT},
  author       = {Crowley, Diarmuid and Goette, Sebastian and Nordström, Johannes},
  doi          = {10.1007/s00222-024-01310-z},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {865-907},
  shortjournal = {Invent. Math.},
  title        = {An analytic invariant of $G_{2}$ manifolds},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterations of symplectomorphisms and <span
class="math inline"><em>p</em></span> -adic analytic actions on the
fukaya category. <em>INVENT</em>, <em>239</em>(3), 801–864. (<a
href="https://doi.org/10.1007/s00222-024-01308-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the work of Bell on the dynamical Mordell-Lang conjecture, and by family Floer cohomology, we construct $p$ -adic analytic families of bimodules on the Fukaya category of a monotone or negatively monotone symplectic manifold, interpolating the bimodules corresponding to iterates of a symplectomorphism $\phi $ isotopic to the identity. This family can be thought of as a $p$ -adic analytic action on the Fukaya category. Using this, we deduce that the ranks of the Floer cohomology groups $HF(\phi ^{k}(L),L&#39;;\Lambda )$ are constant in $k\in {\mathbb{Z}}$ , with finitely many possible exceptions. We also prove an analogous result without the monotonicity assumption for generic $\phi $ isotopic to the identity by showing how to construct a $p$ -adic analytic action in this case. We give applications to categorical entropy and a conjecture of Seidel.},
  archive      = {J_INVENT},
  author       = {Kartal, Yusuf Barış},
  doi          = {10.1007/s00222-024-01308-7},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {801-864},
  shortjournal = {Invent. Math.},
  title        = {Iterations of symplectomorphisms and $p$ -adic analytic actions on the fukaya category},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizing lusztig’s total positivity. <em>INVENT</em>,
<em>239</em>(3), 707–799. (<a
href="https://doi.org/10.1007/s00222-024-01303-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the notion of $\Theta $ -positivity in real semisimple Lie groups. This notion at the same time generalizes Lusztig’s total positivity in split real Lie groups and invariant orders in Lie groups of Hermitian type. We show that there are four families of simple Lie groups which admit a positive structure relative to a subset $\Theta$ of simple roots, and investigate fundamental properties of $\Theta $ -positivity. We define and describe the positive and nonnegative unipotent semigroups and show that they give rise to a notion of positive $n$ -tuples in flag varieties.},
  archive      = {J_INVENT},
  author       = {Guichard, Olivier and Wienhard, Anna},
  doi          = {10.1007/s00222-024-01303-y},
  journal      = {Inventiones Mathematicae},
  month        = {3},
  number       = {3},
  pages        = {707-799},
  shortjournal = {Invent. Math.},
  title        = {Generalizing lusztig’s total positivity},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jamc---50">JAMC - 50</h2>
<ul>
<li><details>
<summary>
(2025). Unveiling the dynamics of plasma dilution in medical science
through analytical and numerical approaches via fractional
integro-differential equations. <em>JAMC</em>, <em>71</em>(1),
1219–1245. (<a
href="https://doi.org/10.1007/s12190-024-02279-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a rigorous mathematical analysis utilizing a fractional integro-differential equation model to depict the intricate dynamics of plasma dilution and provide a comprehensive understanding of its implications in various biomedical and clinical contexts. The integro-differential equation governing plasma dilution is formulated by employing Caputo fractional operators and it is addressed and examined using analytical and numerical techniques, Laplace transform which gives an accurate results for the proposed equation and the composite trapezoidal rule with finite difference method as a numerical approach to validate the proposed model, offering a practical tool for predicting plasma dilution under divers. We also discussed the existence and the uniqueness of solutions. Whereas this model has effectively been utilized with real data obtained from a previous human investigation of thyroid surgery, the graphical illustrations are being presented to clarify the results obtained during two time periods, the infusion and postinfusion periods, the two periods are considered a sequential events the infusion phase transpired within the initial 30 min of the surgical procedure following this, the postinfusion phase extended from 30 min to a limited time.},
  archive      = {J_JAMC},
  author       = {Ali, Khalid K. and Mohamed, Mohamed S. and Maneea, M.},
  doi          = {10.1007/s12190-024-02279-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1219-1245},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Unveiling the dynamics of plasma dilution in medical science through analytical and numerical approaches via fractional integro-differential equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian approach for identifying fractional order and
time-dependent source in a fractional pseudo-parabolic equation.
<em>JAMC</em>, <em>71</em>(1), 1189–1218. (<a
href="https://doi.org/10.1007/s12190-024-02237-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the inverse problem of identifying fractional order and time-dependent source term for fractional pseudo-parabolic equation is investigated. We prove that the simultaneous recovery of the fractional order and the source term is unique, and then give the estimate of measurement data. Such problem is formulated into Bayesian inverse problem, and the continuity of forward mapping is proved. Moreover, the iterative regularizing ensemble Kalman method is applied to solve Bayesian inverse problem and two numerical examples are conducted to show its efficiency.},
  archive      = {J_JAMC},
  author       = {Lyu, Kaiyu and Cheng, Hao},
  doi          = {10.1007/s12190-024-02237-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1189-1218},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Bayesian approach for identifying fractional order and time-dependent source in a fractional pseudo-parabolic equation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced numerical resolution of the duffing and van der pol
equations via the spectral homotopy analysis method employing chebyshev
polynomials of the first kind. <em>JAMC</em>, <em>71</em>(1), 1159–1187.
(<a href="https://doi.org/10.1007/s12190-024-02271-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our investigation, we apply the spectral homotopy analysis method (SHAM) to tackle the numerical resolution of the Duffing and Van Der Pol equations. We compute the numerical solution using Chebyshev polynomials of the first kind as our foundational framework. We optimize the computation of derivative matrices by employing Gauss–Lobatto collocation points. To underscore the practicality and efficiency of our proposed SHAM, we validate our numerical solution against the exact solution and present the error behavior. Furthermore, we demonstrate the superior efficacy of our approach, supported by suitable graphical representations and tables.},
  archive      = {J_JAMC},
  author       = {Bouakkaz, Mouaad and Arar, Nouria and Meflah, Mabrouk},
  doi          = {10.1007/s12190-024-02271-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1159-1187},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Enhanced numerical resolution of the duffing and van der pol equations via the spectral homotopy analysis method employing chebyshev polynomials of the first kind},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deferred riesz statistical convergence via power series
method. <em>JAMC</em>, <em>71</em>(1), 1141–1158. (<a
href="https://doi.org/10.1007/s12190-024-02283-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, deferred Riesz statistical convergence as well as $$\hslash $$ -deferred Riesz statistical convergence in terms of power series method for real or complex sequences are introduced and studied. Their interconnection with Riesz statistical convergence is explored and illustrative examples in support of our results are presented. Applications of these convergences in the form of a Korovkin-type approximation theorem are established and illustrations demonstrating the superiority of our proven theorem over the classical Korovkin theorem are offered. Finally, the rate of convergence is computed.},
  archive      = {J_JAMC},
  author       = {Cai, Qing-Bo and Gorka, Samrati and Raj, Kuldip},
  doi          = {10.1007/s12190-024-02283-1},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1141-1158},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Deferred riesz statistical convergence via power series method},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synchronization of fuzzy reaction–diffusion neural networks
via semi-intermittent hybrid control. <em>JAMC</em>, <em>71</em>(1),
1109–1139. (<a
href="https://doi.org/10.1007/s12190-024-02234-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of synchronizing fuzzy reaction–diffusion neural networks (FRDNNs) with time-varying transmission delays using aperiodic semi-intermittent hybrid controls and explores its application within the realm of image encryption. The main challenge in analyzing the dynamics of FRDNNs included diffusion terms with uncertainty, and the inclusion of fuzzy logic operations further increases the system’s complexity. We propose a new concept called the average control width (ACW) for aperiodic semi-intermittent control (ASIC) systems; it is used in conjunction with the idea of average dwell time (ADT) for switched systems. A sufficient flexible condition for master–slave synchronization of neural networks using average-width semi-intermittent hybrid control assures ADT and ACW conditions. By utilizing these concepts, the proposed synchronization method can overcome the challenges posed by the diffusion terms and fuzzy logic operations in FRDNNs with time-varying transmission delays. Finally, the paper presents a theoretical framework for synchronizing FRDNNs with time-varying transmission delays using semi-intermittent hybrid control via LMI and suitable Lyapunov functional, validated through simulations. The proposed synchronization method is also applied to develop a novel chaos-based elliptic curve cryptography algorithm for medical image encryption.},
  archive      = {J_JAMC},
  author       = {Kathiresan, S. and Kashkynbayev, Ardak and Mohanrasu, S. S. and Rajan, Rakkiyappan},
  doi          = {10.1007/s12190-024-02234-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1109-1139},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Synchronization of fuzzy reaction–diffusion neural networks via semi-intermittent hybrid control},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parameter uniform orthogonal spline collocation method for
time delay singularly perturbed semilinear reaction–diffusion problems.
<em>JAMC</em>, <em>71</em>(1), 1077–1107. (<a
href="https://doi.org/10.1007/s12190-024-02253-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A class of singularly perturbed semilinear time-dependent problems with a delay term in time is considered. A numerical approach is formulated and analysed to unravel the complexity for such problems due to the presence of a small perturbation parameter and a semilinear term simultaneously. We use Shishkin mesh to capture the boundary layer. We estimate errors in the energy and balanced norms occurred in orthogonal spline collocation discretization for the spatial direction. For the full discretization an extrapolated Crank–Nicolson formula is used in time direction. Some numerical results are presented to confirm the effectiveness of the method. The computed convergence corroborated the predicted theory in different norms.},
  archive      = {J_JAMC},
  author       = {Howlader, Jewel and Mishra, Pankaj and Sharma, Kapil K.},
  doi          = {10.1007/s12190-024-02253-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1077-1107},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A parameter uniform orthogonal spline collocation method for time delay singularly perturbed semilinear reaction–diffusion problems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel supply chain decision making model under m-polar
quadripartitioned neutrosophic environment. <em>JAMC</em>,
<em>71</em>(1), 1051–1076. (<a
href="https://doi.org/10.1007/s12190-024-02256-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state of the universe is shifting towards multi-polarity, a well-established phenomenon that plays a crucial role across various scientific and technological fields, particularly in information and data domains. In this study, we propose the concept of m-polar quadripartitioned neutrosophic sets and explore their graphical representations, presenting key findings. We introduce novel operations on m-polar quadripartitioned neutrosophic graphs, such as the strong product and direct product. Additionally, we discuss concepts like complement, homomorphism, isomorphism, weak and co-weak isomorphism within the context of m-polar neutrosophic graphs. Furthermore, we elucidate several associated properties and theorems concerning m-polar quadripartitioned neutrosophic graphs. Finally, we demonstrate the practical application of our findings in a supply chain management model.},
  archive      = {J_JAMC},
  author       = {Satham Hussain, S. and Nagarajan, Durga and Rashmanlou, Hossein and Mofidnakhaei, Farshid},
  doi          = {10.1007/s12190-024-02256-4},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1051-1076},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Novel supply chain decision making model under m-polar quadripartitioned neutrosophic environment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On trees of a fixed maximum degree with extremal general
atom-bond sum-connectivity index. <em>JAMC</em>, <em>71</em>(1),
1035–1049. (<a
href="https://doi.org/10.1007/s12190-024-02275-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider general atom-bond sum-connectivity indices $$\mathcal {ABS}_{\ell }$$ for $$1/2 \le \ell \le 1$$ and study their values over all trees on a given number of vertices with a fixed maximum degree $$\Delta $$ . We obtain both the minimum and the maximum values and characterize the corresponding trees. The obtained results recover previously known results for the atom-bond sum-connectivity index and imply analogous results for the well-known harmonic index. The results remain valid when the class of considered graphs is restricted to the class of molecular trees.},
  archive      = {J_JAMC},
  author       = {Ali, Akbar and Došlić, Tomislav and Raza, Zahid},
  doi          = {10.1007/s12190-024-02275-1},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1035-1049},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On trees of a fixed maximum degree with extremal general atom-bond sum-connectivity index},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical simulations of rosenau–burgers equations via
crank–nicolson spectral pell matrix algorithm. <em>JAMC</em>,
<em>71</em>(1), 1009–1033. (<a
href="https://doi.org/10.1007/s12190-024-02273-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current work addresses the use of the numerical Crank–Nicolson technique along with spectral collocation to seek the approximate solution of a class of high-order nonlinear partial differential equations known as the Rosenau–Burgers equation. First, the Crank–Nicolson approach is applied to reduce the Rosenau–Burgers equation to a set of linear equations. The solvability and uniqueness of the semi-discretized form is established. Then, the spectral based collocation method by using the Pell functions is employed to approximate the derived system of linearized equations in terms of unknown coefficients. The theoretical part and error analysis are comprehensively studied in the weighted $$L^2$$ -norm. Finally, four illustrative test cases are conducted to showcase the efficiency of the suggested combined technique. The results are well compared with existing results and exact solutions in the literature to demonstrate the advantages of the applied novel approach.},
  archive      = {J_JAMC},
  author       = {Izadi, Mohammad and Srivastava, Hari Mohan and Mamehrashi, Kamal},
  doi          = {10.1007/s12190-024-02273-3},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1009-1033},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Numerical simulations of Rosenau–Burgers equations via Crank–Nicolson spectral pell matrix algorithm},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to determine the sombor-type indices via
m-polynomial. <em>JAMC</em>, <em>71</em>(1), 983–1007. (<a
href="https://doi.org/10.1007/s12190-024-02272-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological indices can be interpreted as the mathematical characterizations of a molecular compound and are significantly employed to forecast its physical, chemical and biological information. Computation of topological indices of a graph through its associated graph polynomial is a modern and optimal approach. One such method is to determine the degree-based topological indices of a graph using its M-polynomial. Among the class of degree-based topological indices, the Sombor indices are one of the most investigated indices in recent times. In this article, the M-polynomial-based derivation formulas are derived to compute the different Sombor-type indices, namely the Sombor index, modified Sombor index, first and second Banhatti–Sombor indices, and their reduced form of the Sombor indices. Furthermore, our proposed derivation formulas are applied to compute the Sombor-type indices of the jagged-rectangle benzenoid system $$B_{m,n}$$ . Additionally, the comparison among the Sombor-type indices of $$B_{m,n}$$ is presented through numerical and graphical representations.},
  archive      = {J_JAMC},
  author       = {Kumar, Virendra and Das, Shibsankar},
  doi          = {10.1007/s12190-024-02272-4},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {983-1007},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A novel approach to determine the sombor-type indices via M-polynomial},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vulnerability assessment of a new class of cayley graph.
<em>JAMC</em>, <em>71</em>(1), 969–982. (<a
href="https://doi.org/10.1007/s12190-024-02270-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of links and processors in an interconnection network increases, faulty links and processors are constantly emerging. When a network fails, how to evaluate the state of the network and mitigate the vulnerability of the network itself is the focus of attention in recent years. Therefore, the parameters for assessing network vulnerability have received considerable attention. In general, we use connectivity and diagnosability to reflect the vulnerability of the network. At present, the connectivity and diagnosability of most networks have been determined. In this paper, we mainly analyze Cayley graphs $$EC_m$$ , which are generated by disjoint paths with length 2. We explain that connectivity and super connectivity of $$EC_m$$ are uniformly 2m, the 1-extra and 3-component connectivity of $$EC_m$$ are uniformly $$4m-2$$ . In addition, we also analyze the local diagnosability of $$EC_m$$ under the PMC and $$\hbox {MM}^*$$ models is 2m.},
  archive      = {J_JAMC},
  author       = {Zhang, Hong and Bian, Hong},
  doi          = {10.1007/s12190-024-02270-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {969-982},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Vulnerability assessment of a new class of cayley graph},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the zero-divisor graph over commutative ring:
Topological examine of algebraic structure. <em>JAMC</em>,
<em>71</em>(1), 945–967. (<a
href="https://doi.org/10.1007/s12190-024-02260-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A commutative ring over a graph was first studied by Beck in 1988. The properties of zero-divisor graphs of commutative and non-commutative rings were then studied extensively. The commutative ring over graph is also widely used in robotics, communication theory, and elliptic curve cryptography. In this paper, we derive a few degree-based topological expressions namely, the first Zagreb index and the forgotten index of zero-divisor graphs of $${\mathbb {Z}}_p^n$$ , $${\mathbb {Z}}_q\times {\mathbb {Z}}_p^n$$ and $${\mathbb {Z}}_{q^2}\times {\mathbb {Z}}_p^n$$ where p and q are primes. The numerical expression of the zero-divisor graphs is analysed and compared the indices graphically and numerically.},
  archive      = {J_JAMC},
  author       = {Akhila, S. and Al-Shamiri, Mohammed M. Ali and Alsinai, Ammar and Xavier, D. Antony},
  doi          = {10.1007/s12190-024-02260-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {945-967},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Exploring the zero-divisor graph over commutative ring: Topological examine of algebraic structure},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The multiplicity of radial p-k-convex solutions for the
p-k-hessian equation. <em>JAMC</em>, <em>71</em>(1), 927–943. (<a
href="https://doi.org/10.1007/s12190-024-02262-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on radial p-k-convex solutions for the following p-k-Hessian equation $$\begin{aligned} \left\{ \begin{array}{ll} S_{k}(\xi (D_{i}(|Dv|^{p-2}D_{j}v)))=M(|z|){(|v|+1)}^{m}(ln (|v|+1))^{\mu }, z\in E,\\ v=+\infty , ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~z\in \partial E, \end{array} \right. \end{aligned}$$ where $$p\ge 2$$ , $$k\in \{1,2,...,n\}$$ , $$E\subset \mathbb {R}^{n}(n\ge 2)$$ denotes a ball. For the case of $$0&lt;m&lt;(p-1)k$$ , $$\mu =0$$ , the multiplicity of radial p-k-convex solutions of the above p-k-Hessian equation is established by the sub-supersolutions method. For the case of $$m=(p-1)k$$ , $$\mu &gt;(p-1)k$$ , we construct a new supporting function to overcome the difficulty caused by logarithmic nonlinearity, which ensures that the above p-k-Hessian equation has infinitely many radial p-k-convex solutions.}},
  archive      = {J_JAMC},
  author       = {Wang, Guotao and Guo, Mengjie},
  doi          = {10.1007/s12190-024-02262-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {927-943},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {The multiplicity of radial p-k-convex solutions for the p-k-hessian equation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling and analysis of a fractional-order epidemic model
incorporating genetic algorithm-based optimization. <em>JAMC</em>,
<em>71</em>(1), 901–925. (<a
href="https://doi.org/10.1007/s12190-024-02224-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infectious diseases have always been a threat to the smooth running of our daily activities. To regulate the disease’s devastating outcome, we have performed a qualitative study of infectious disease using an SIR model. While formulating the model, we have taken into account the saturated incidence function with three controls, namely, treatment control, vaccination control, and media awareness. To make the model more robust, we have updated the model using Caputo fractional-order differential equation. We have determined the existence and uniqueness of the solution along with all possible equilibrium points. We have also obtained the basic reproduction number and the criteria of asymptotic local and global stability, taking the basic reproduction number as the threshold parameter. Finally, to control the disease, we have performed the optimization using a metaheuristic search and optimization technique, genetic algorithm (GA).},
  archive      = {J_JAMC},
  author       = {Adak, Sayani and Barman, Snehasis and Jana, Soovoojeet and Majee, Suvankar and Kar, T. K.},
  doi          = {10.1007/s12190-024-02224-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {901-925},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Modelling and analysis of a fractional-order epidemic model incorporating genetic algorithm-based optimization},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overlapping containment rough neighborhoods and their
generalized approximation spaces with applications. <em>JAMC</em>,
<em>71</em>(1), 869–900. (<a
href="https://doi.org/10.1007/s12190-024-02261-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rough set theory, we distinguish confirmed and possible data, extracted through subsets utilizing lower and upper approximations, respectively. Earlier studies have presented several rough approximation models, drawing inspiration from neighborhood systems, aimed at enhancing accuracy degree and satisfying the axioms of standard approximation space, introduced by Pawlak. In this article, we first introduce novel rough neighborhoods so-called overlapping containment rough neighborhoods, denoted by $$\widetilde{\textrm{C}}_k$$ -neighborhoods, using inclusion relations between $$\mathcal {D}_r$$ -neighborhoods and $$\mathcal {D}_l$$ -neighborhoods, as well as $$\mathcal {D}_{\langle r\rangle }$$ -neighborhoods and $$\mathcal {D}_{\langle l\rangle }$$ -neighborhoods, all defined under an arbitrary relation. We explore their main characterizations and reveal the relationships between them under specific types of binary relations, such as symmetric, transitive, and partial order relations. As a unique contribution, we successfully derive an indicator inspired by $$\widetilde{\textrm{C}}_k$$ -neighborhoods for $$k\in \{r,l,i\}$$ to determine whether a relation is symmetric. Additionally, we describe the behavior of $$\widetilde{\textrm{C}}_k$$ -neighborhoods as they navigate between two generalized approximation spaces, where the relations are reflexive and transitive, and one is a subset of the other. Then, we exploit $$\widetilde{\textrm{C}}_k$$ -neighborhoods to present fresh rough set models. We examine their main properties and demonstrate that they keep most characterizations of Pawlak’s paradigm while reducing the uncertainty in the data compared to some previous studies, also, we show that they satisfy the monstrosity property under quasi-order relations. To elucidate the superiority and accuracy of the present approach, we apply it to analyze the information systems related to the authorship of articles and books by selected authors and conduct a comparative analysis with several preceding approaches. Finally, a summary of the obtained results and relationships and suggestion for some forthcoming work are offered.},
  archive      = {J_JAMC},
  author       = {Al-shami, Tareq M. and Mhemdi, Abdelwaheb},
  doi          = {10.1007/s12190-024-02261-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {869-900},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Overlapping containment rough neighborhoods and their generalized approximation spaces with applications},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some basic mathematical properties of the misbalance hadeg
index of graphs. <em>JAMC</em>, <em>71</em>(1), 851–867. (<a
href="https://doi.org/10.1007/s12190-024-02250-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2010, Vukičević and Gašperov proposed a group of 148 discrete Adriatic indices which have represented good predictive characteristics on the testing sets conducted by the International Academy of Mathematical Chemistry (IAMC). One of such indices was the misbalance Hadeg ( $$\mathcal{M}\mathcal{H}$$ ) index which showed good ability in predicting the enthalpy of vaporization and standard enthalpy of vaporization for octane isomers. For a simple graph G with the edge set $$E_G$$ , the $$\mathcal{M}\mathcal{H}$$ index is expressed by $$\mathcal{M}\mathcal{H}(G)=\sum _{\alpha \beta \in E_G} \Big |\Big (\frac{1}{2}\Big )^{d^{G}_{\alpha }}-\Big (\frac{1}{2}\Big )^{d^{G}_{\beta }}\Big |$$ , where $$d^{G}_{\beta }$$ stands for the degree of the vertex $$\beta $$ in G. Despite passing almost 14 years since the introduction of the $$\mathcal{M}\mathcal{H}$$ index, it has been relatively less investigated and left into oblivion. A main purpose of this research is to attract the attention of the mathematical chemistry community towards this molecular structure descriptor by studying some of its basic mathematical properties over trees, molecular trees, unicyclic graphs, bicyclic graphs and, in general, over c-cyclic graphs. More specifically, we give the minimum values of the $$\mathcal{M}\mathcal{H}$$ index over trees, unicyclic graphs and bicyclic graphs of a given number of vertices and with a fixed maximum degree and characterize the corresponding minimal graphs. The special case when the class of considered trees is restricted to the class of molecular trees and the general case when the class of considered unicyclic and bicyclic graphs is extended to the class of c-cyclic graphs are also examined.},
  archive      = {J_JAMC},
  author       = {Azari, Mahdieh and Dehgardi, Nasrin},
  doi          = {10.1007/s12190-024-02250-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {851-867},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Some basic mathematical properties of the misbalance hadeg index of graphs},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical solution for third order singularly perturbed
turning point problems with integral boundary condition. <em>JAMC</em>,
<em>71</em>(1), 829–849. (<a
href="https://doi.org/10.1007/s12190-024-02266-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a third-order singularly perturbed differential equation with integral boundary condition (IBC) is considered. The problem is reduced into system of differential equation, one compromises initial value problem and another one is second order singularly perturbed differential equation with integral boundary condition. Due to the presence of turning point at $$r=0,$$ the problem exhibit boundary layer at $$r=-1$$ and $$r=1.$$ To tackle this type of problem, a thorough study is required to obtain a priori estimations on the solution and its derivatives of the considered problem. We present a numerical technique adopting an upwind finite difference scheme on a dense piece-wise uniform mesh at the boundary layers. The proposed method is almost first-order convergent. Some numerical examples are provided to validate the theoretical findings.},
  archive      = {J_JAMC},
  author       = {Raja, V. and Geetha, N. and Mahendran, R. and Senthilkumar, L. S.},
  doi          = {10.1007/s12190-024-02266-2},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {829-849},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Numerical solution for third order singularly perturbed turning point problems with integral boundary condition},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust dissipativity analysis for stochastic markov jump
competitive neural networks with mixed delays. <em>JAMC</em>,
<em>71</em>(1), 801–828. (<a
href="https://doi.org/10.1007/s12190-024-02257-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research addresses the critical issue of dissipativity in Markovian jump stochastic competitive neural networks, particularly focusing on the complex challenges posed by mixed time delays and parameter uncertainties. The primary objective of this study is to derive adequate conditions for dissipativity by constructing suitable Lyapunov–Krasovskii functionals (LKFs) that incorporates triple integral terms, thereby providing a rigorous mathematical framework for analysis. To achieve this, a generalized delay-dependent reciprocal convex inequality is employed, which enables us to effectively calculate the derivative of the LKFs and derive a linear matrix polynomial. Our findings extend the conventional dissipativity criteria to include passivity, which is articulated in terms of linear matrix inequalities (LMIs). This enhancement significantly simplifies the computational process and allows for practical implementation using standard numerical software. Further, Numerical examples demonstrate that the proposed strategy outperforms existing findings.},
  archive      = {J_JAMC},
  author       = {Subhashri, A. R. and Radhika, T.},
  doi          = {10.1007/s12190-024-02257-3},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {801-828},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Robust dissipativity analysis for stochastic markov jump competitive neural networks with mixed delays},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cluster approximate synchronization for probabilistic
asynchronous finite field networks. <em>JAMC</em>, <em>71</em>(1),
783–800. (<a href="https://doi.org/10.1007/s12190-024-02255-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have established probabilistic asynchronous finite field networks (PAFFNs) and investigated the cluster approximate synchronization of it. Firstly, using the semi-tensor product, the dynamics of PAFFNs are transformed into algebraic form. Secondly, based on algebraic form, cluster approximate synchronization with probabilistic asynchronous update scheme is converted into the set stability of the probabilistic systems, and sufficient and necessary conditions of PAFFNs cluster approximate synchronization are obtained. Finally, an example is given to illustrate the correctness of theorems.},
  archive      = {J_JAMC},
  author       = {Zhang, Hao and Xu, Lunshi and Luo, Chao and Zhang, Chuan and Su, Xianghui},
  doi          = {10.1007/s12190-024-02255-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {783-800},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Cluster approximate synchronization for probabilistic asynchronous finite field networks},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new method for converting impulsive riemann–liouville
fractional order system into the integral equation. <em>JAMC</em>,
<em>71</em>(1), 765–782. (<a
href="https://doi.org/10.1007/s12190-024-02258-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For impulsive fractional order system (IFOS), its equivalent integral equation (EIE) is an important process tool for discussing the IFOS’s properties (such as existence of solution, numerical solution, stability and controllability etc). However, there appeared three incorrect EIEs for the same IFOS in existing papers due to misunderstandings about the segmented calculation for the fractional calculus of piecewise function. Hence, we re-study the EIEs of two impulsive Riemann–Liouville fractional order systems (IRFOSs) in this paper. For one IRFOS, we find the linear additivity of the impulsive effects by two limit properties, and construct the EIE for the IRFOS of one impulse by fractional order property of piecewise function, which derive the EIE’s structure of the IRFOS. Next, we apply the EIE’s structure of the IRFOS and a limit property to obtain the IRFOS’s EIE with an arbitrary constant and confirm the nonuniqueness of the IRFOS’s solution. And then, we combine the EIE of the IRFOS and the relation of two IRFOSs to deduce the EIE of the other IRFOS. Finally, we offer two numerical examples to show the computation of the EIEs and the nonuniqueness of the solutions for two IRFOSs.},
  archive      = {J_JAMC},
  author       = {Zhang, Xianmin},
  doi          = {10.1007/s12190-024-02258-2},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {765-782},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A new method for converting impulsive Riemann–Liouville fractional order system into the integral equation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical solution of nonlinear equations of traffic flow
density using spectral methods by filter. <em>JAMC</em>, <em>71</em>(1),
743–763. (<a href="https://doi.org/10.1007/s12190-024-02252-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative approach that marries the spectral method with a time-dependent partial differential equation filter to tackle the phenomenon of shock waves in traffic flow modeling. Through the strategic application of Discrete low-pass filters, this method effectively mitigates shock-induced deviations, leading to significantly more accurate results compared to conventional spectral techniques. We conduct a thorough examination of the stability conditions inherent to this approach, providing valuable insights into its robustness. To substantiate its effectiveness, we present a series of numerical examples illustrating the method’s prowess in delivering precise solutions. Comparative analysis against established methods such as Lax and Cu reveals a marked superiority in accuracy. This work not only contributes a novel numerical technique to the field of traffic flow modeling but also addresses a persistent challenge, offering a promising avenue for further research and practical applications.},
  archive      = {J_JAMC},
  author       = {Najafi, Seyed Esmaeil Sadat and Allahviranloo, Tofigh and Abbasbandy, Saeid and Malkhalifeh, Mohsen Rostamy},
  doi          = {10.1007/s12190-024-02252-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {743-763},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Numerical solution of nonlinear equations of traffic flow density using spectral methods by filter},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wiener index of inverse fuzzy mixed graphs with application
in education system. <em>JAMC</em>, <em>71</em>(1), 725–742. (<a
href="https://doi.org/10.1007/s12190-024-02263-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The notion of Wiener index has a wide range of application in chemical science and in network science. In this research article, the idea of connectivity index and Wiener index have been introduced in an inverse fuzzy mixed graph (IFMG) with the help of the idea of geodesic. Throughout the article, we have explored the Wiener index of various inverse fuzzy mixed graph structures. Most importantly the relation between connectivity index and inverse fuzzy mixed Wiener index (IFMWI) have been established. It has been shown that connectivity index is less than or equal to the IFMWI for an inverse fuzzy mixed tree containing at-least three vertices. The idea of IFMWI has been also discussed for the inverse fuzzy mixed cycles and investigated some of the important properties. At the end of our discussion, an application is presented to assess teaching and learning outcomes based on specific parameters.},
  archive      = {J_JAMC},
  author       = {Mondal, Rahul and Ghorai, Ganesh},
  doi          = {10.1007/s12190-024-02263-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {725-742},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Wiener index of inverse fuzzy mixed graphs with application in education system},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective nonmonotone trust region method based on a simple
cubic model for unconstrained optimization problems. <em>JAMC</em>,
<em>71</em>(1), 707–723. (<a
href="https://doi.org/10.1007/s12190-024-02241-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a new nonmonotone trust region method with a simple cubic model to solve the unconstrained optimization problems (UCM). We improved the adaptive cubic regularization method by using a real positive definite scalar matrix instead of the exact Hessian and combining it with the nonmonotone technique. In addition, under some proper assumptions, the global convergence of the introduced method is established. Numerical tests on a set of standard minimization problems are reported and show that the proposed algorithm is efficient and robust compared to the other given methods.},
  archive      = {J_JAMC},
  author       = {Niri, T. Dehghan and Amini, K.},
  doi          = {10.1007/s12190-024-02241-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {707-723},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Effective nonmonotone trust region method based on a simple cubic model for unconstrained optimization problems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability and hopf bifurcation analysis of a networked SIR
epidemic model with two delays. <em>JAMC</em>, <em>71</em>(1), 669–706.
(<a href="https://doi.org/10.1007/s12190-024-02240-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population migration within spatial structures significantly influences disease spread. Given the typically uneven environment, individuals randomly interact with various others, facilitating disease transmission over time. Complex networks are integrated into infectious disease models and effectively capture these contact dynamics. In this paper, we propose a two-delay networked SIR epidemic model featuring a Crowley-Martin type incidence rate and Holling III type treatment rate. The stability of three equilibria of the model is proved by analyzing the distribution of characteristic roots. When two delays change at the same time, the stable region of the equilibrium on delays plane and the existence of Hopf bifurcation are obtained by the method of stability switching curves. Furthermore, we calculate the normal form of Hopf bifurcation to obtain the direction of Hopf bifurcation and the stability of bifurcation periodic solutions. Finally, numerical simulation on a small-world Watts-Strogatz network is performed to illustrate the theoretical results.},
  archive      = {J_JAMC},
  author       = {Zhou, Shumin and Dai, Yunxian and Wang, Hongyan},
  doi          = {10.1007/s12190-024-02240-y},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {669-706},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Stability and hopf bifurcation analysis of a networked SIR epidemic model with two delays},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel optimal fourth-order iteration scheme for solving
nonlinear problems in applied sciences. <em>JAMC</em>, <em>71</em>(1),
643–667. (<a href="https://doi.org/10.1007/s12190-024-02259-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous real life models in applied sciences and engineering commonly deal with nonlinear equations that have to be solved using reliable numerical methods. The computational science research is continuously growing, which is a marked by either introduction of new iteration algorithms or improvement of the existing ones. Nevertheless, these numerical methods may have high computational cost, but they do have a faster rate of convergence. Hence, the aim of this paper is create new two-step fourth order iteration algorithms for finding simple roots of nonlinear equations. To achieve this goal we have taken Steffensen’s method in the first sub-step and a general quadratic interpolation polynomial in the second sub-step of a two-step iterative scheme. The main theorem demonstrates the fourth-order convergence of the proposed scheme. The proposed scheme is derivative free and achieves optimal fourth order convergence by using only three function evaluations per iteration satisfying the conjecture of Kung and Traub. To validate the theoretical results and to demonstrate the effectiveness of the presented methods, we explore some nonlinear models in medical sciences such as the law of blood flow, blood rheology, fluid permeability in biogels, and thermal regulation of the human body. The numerical results of the proposed scheme are presented in terms of number of iterations, errors in consecutive iterations, computational convergence order (CCO) and CPU time (sec). Several researchers have investigated basins of attraction on simple polynomials of the form $$z^n-1$$ in the complex plan. However, we have studied the fractal behavior of different methods on real world problems stated above in terms of basins of attraction for comparison of their convergence regions. The proposed scheme generate the basins of attraction in less time with wider regions of convergence in comparison with the existing methods of similar kind. Dynamical analysis of the proposed derivative free scheme illustrate its superiority in comparison with some existing optimal fourth order methods involving derivatives.},
  archive      = {J_JAMC},
  author       = {Kumar, Sunil and Ishwariya, R. and Junjua, Moin-ud-Din and Akram, Saima},
  doi          = {10.1007/s12190-024-02259-1},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {643-667},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A novel optimal fourth-order iteration scheme for solving nonlinear problems in applied sciences},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Observer-based impulsive control for finite-time
synchronization of delayed neural networks on time scales.
<em>JAMC</em>, <em>71</em>(1), 627–642. (<a
href="https://doi.org/10.1007/s12190-024-02268-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the finite-time synchronization (FTS) of delayed neural networks on time scales via impulsive control. First, an impulsive controller is designed when system states are accessible. Based on the time scale theory and mathematical induction method, a sufficient condition for FTS is presented. Then, an observer is provided to estimate system states when partial states can not be available. An observer-based impulsive controller is devised to ensure that both the observer error system and the synchronization error system converges to zero in finite time. Furthermore, the explicit expression for settling time of the FTS is given. Finally, the validity of our methods is verified by a numerical simulation.},
  archive      = {J_JAMC},
  author       = {Zhang, Chuan and Liu, Ruihong and Zhang, Xianfu and Guo, Yingxin},
  doi          = {10.1007/s12190-024-02268-0},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {627-642},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Observer-based impulsive control for finite-time synchronization of delayed neural networks on time scales},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical modeling of zika virus with vertical
transmission in the presence of wolbachia-infected mosquitoes.
<em>JAMC</em>, <em>71</em>(1), 605–625. (<a
href="https://doi.org/10.1007/s12190-024-02236-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Zika virus, a notorious flavivirus, infiltrates human populations via infected Aedes mosquitoes or sexual transmission, often resulting in devastating birth defects like microcephaly. Our study delves into Zika’s transmission dynamics, revealing the role of Wolbachia-infected mosquitoes and the impact of asymptomatic carriers. Central factors such as mosquito mortality rates and transmission dynamics steer Zika’s trajectory. Meticulous simulations show Wolbachia-infected mosquitoes effectively reducing new cases. Navigating through scenarios of pregnancy delays, we seamlessly integrate them into our mathematical tapestry, enhancing our understanding of Zika’s dynamics. While postponing conception holds promise in mitigating microcephaly, its impact on overall transmission remains marginal, a sobering reality amidst progress. Embracing a unified approach, combining Wolbachia-infected mosquitoes and personal protective measures, we forge a formidable defense against Zika’s onslaught. As the specters of microcephaly and Zika recede, our quest for solutions offers a glimmer of hope in the battle against this formidable adversary.},
  archive      = {J_JAMC},
  author       = {Jamal, Muhammad and Batool, Sadia and Ahmed, Iftikhar and Azhar, Ehtsham and Nawaz, Tayyab},
  doi          = {10.1007/s12190-024-02236-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {605-625},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Mathematical modeling of zika virus with vertical transmission in the presence of wolbachia-infected mosquitoes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deterministic SAIR model with vaccination and treatment:
Dynamical behaviors and control strategies. <em>JAMC</em>,
<em>71</em>(1), 573–604. (<a
href="https://doi.org/10.1007/s12190-024-02238-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to provide valuable information for the timely design and determination of public health measures during epidemic outbreaks, a deterministic SAIR model with vaccination programs and saturated treatment is established in this paper, as they have been demonstrated to be effective ways to change the evolution and prevent the spread of infectious diseases. The dynamical behaviors of the proposed model is analyzed theoretically and numerically. It is found that multiple endemic equilibria occur under certain conditions, suggesting that decreasing the basic reproduction number $$\mathcal {R}_0$$ is insufficient for disease eradication and improving the efficiency of vaccination and treatment can have a significant impact on disease progression. Furthermore, sensitivity analysis is conducted to quantify how changes in concerned parameters impact $$\mathcal {R}_0$$ , and the proposed model is applied to the pandemic wave of COVID-19. Finally, optimal control problem with time-varying vaccination and treatment is studied and cost-effectiveness analysis is examined under different scenarios.},
  archive      = {J_JAMC},
  author       = {Ouyang, Yun and Zhang, Suxia and Xu, Jinhu},
  doi          = {10.1007/s12190-024-02238-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {573-604},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A deterministic SAIR model with vaccination and treatment: Dynamical behaviors and control strategies},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A projected hybridization of the hestenes–stiefel and
dai–yuan conjugate gradient methods with application to nonnegative
matrix factorization. <em>JAMC</em>, <em>71</em>(1), 551–571. (<a
href="https://doi.org/10.1007/s12190-024-02245-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid conjugate gradient methods are considered as an efficient family of conjugate gradient methods to solve unconstrained optimization problems. In this work, based on the memoryless BFGS update, a convex hybridization of the Hestenes–Stiefel and Dai–Yuan conjugate parameters is presented. To put in place safeguards to protect the sufficient descent property, the given search direction is projected to the orthogonal subspace to the gradient of the objective function. The convergence analysis of the proposed method is addressed under standard assumptions for general functions. The practical merits of the proposed method are computationally demonstrated on a set of CUTEr test functions as well as the well-known nonnegative matrix factorization problem.},
  archive      = {J_JAMC},
  author       = {Khoshsimaye-Bargard, Maryam and Ashrafi, Ali},
  doi          = {10.1007/s12190-024-02245-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {551-571},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A projected hybridization of the Hestenes–Stiefel and Dai–Yuan conjugate gradient methods with application to nonnegative matrix factorization},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On computational analysis via fibonacci wavelet method for
investigating some physical problems. <em>JAMC</em>, <em>71</em>(1),
531–550. (<a href="https://doi.org/10.1007/s12190-024-02251-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we considered wavelet analysis and the application of the Fibonacci wavelet collocation method (FWCM) for solving partial differential equations (PDEs). The proposed technique starts with formulating Fibonacci wavelets using Fibonacci polynomials. Subsequently, the spectral collocation technique is applied to convert the given problem into a system of algebraic equations, which are then solved using the Newton method. Error estimation and convergence analysis of the proposed scheme are also investigated. The effectiveness and precision of the FWCM are demonstrated through a comparative analysis with exact solutions and other existing methods in the literature. The obtained results demonstrate that the proposed technique is an efficient tool for solving PDEs and is also applicable for numerically examining similar types of physical problems.},
  archive      = {J_JAMC},
  author       = {Ahmed, Shahid and Jahan, Shah and Shah, Kamal and Abdeljawad, Thabet},
  doi          = {10.1007/s12190-024-02251-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {531-550},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On computational analysis via fibonacci wavelet method for investigating some physical problems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace transform method for a coupled system of (p,
q)-caputo fractional differential equations. <em>JAMC</em>,
<em>71</em>(1), 511–530. (<a
href="https://doi.org/10.1007/s12190-024-02254-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research uses the Laplace transform method to analyze a coupled system of (p, q)-Caputo fractional differential equations, employing the (p, q)-generalized Caputo derivative. This system includes a function with a Volterra integral operator and integral kernel. By applying the Laplace transform method, we transform the differential equations into algebraic equations in the Laplace domain, which facilitates a systematic analysis of the system’s dynamics and stability properties. Furthermore, we examine the existence and uniqueness of solutions with initial conditions using Shaefer’s fixed point theorem and the Banach fixed point theorem in Banach space. Additionally, we explore the stability of the relevant solutions of the Ulam–Hyers (UH) type. An illustrative example will be presented to demonstrate the theoretical findings.},
  archive      = {J_JAMC},
  author       = {Baihi, Asmaa and Kajouni, Ahmed and Hilal, Khalid and Lmou, Hamid},
  doi          = {10.1007/s12190-024-02254-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {511-530},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Laplace transform method for a coupled system of (p, q)-caputo fractional differential equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic behaviors for fractional epidemiological model
featuring vaccination and quarantine compartments. <em>JAMC</em>,
<em>71</em>(1), 489–509. (<a
href="https://doi.org/10.1007/s12190-024-02249-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have been affected by various epidemic diseases, mostly are airborne and exhibit high transmission rates. Given these nature properties, quarantine measures are essential to control the spread of the diseases effectively. Motivated by this fact, and due to the successful use of mathematical modeling, we investigate a SIR model with quarantine and vaccination compartments. This model uses a system of fractional differential equations (SIQVR-based) with specific parameters to track the dynamics of model variables. We examine the well-posedness and boundedness results via standard tools. An effective threshold parameter $$\mathcal {R}_0$$ is determined using a generation matrix and equilibrium points of the model are obtained. To effectively manage the transmission of infection within the outlined model, we employ the strategy of optimal control. This approach involves implementing control measures and interventions guided by mathematical optimization techniques to minimize the spread of disease. These control strategies may encompass vaccination campaigns, quarantine protocols, social distancing measures and other preventive actions. Further, to evaluate the effectiveness of proposed model and the applied optimal control strategy, we conduct a series of numerical simulations. Computational results involve running the model under different scenarios, considering a range of parameters and meticulously analyzing the resulting outcomes.},
  archive      = {J_JAMC},
  author       = {Hariharan, S. and Shangerganesh, L. and Debbouche, A. and Antonov, V.},
  doi          = {10.1007/s12190-024-02249-3},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {489-509},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Dynamic behaviors for fractional epidemiological model featuring vaccination and quarantine compartments},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-step projected forward–backward algorithms for
constrained minimization problem. <em>JAMC</em>, <em>71</em>(1),
465–487. (<a href="https://doi.org/10.1007/s12190-024-02248-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design new projective forward–backward algorithms for constrained minimization problems. We then discuss its weak convergence via a new linesearch that the hypothesis on the Lipschitz constant of the gradient of functions is avoided. We provide its applications to solve image deblurring and image inpainting. Finally, we discuss the optimal selection of parameters that are proposed in algorithms in terms of PSNR and SSIM. It reveals that our new algorithm outperforms some recent methods introduced in the literature.},
  archive      = {J_JAMC},
  author       = {Kankam, Kunrada and Noor, Muhammad Aslam and Cholamjiak, Prasit},
  doi          = {10.1007/s12190-024-02248-4},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {465-487},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Three-step projected forward–backward algorithms for constrained minimization problem},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the rule of trajectory structure for a third-order
nonlinear difference equation using semi-cycle analysis method.
<em>JAMC</em>, <em>71</em>(1), 453–463. (<a
href="https://doi.org/10.1007/s12190-024-02247-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper is concerned with the rule of trajectory structure and global asymptotical stability of the positive equilibrium for the following third-order nonlinear difference equation $$\begin{aligned} w_{m+1}=\frac{w_{m-1}^{\alpha }w_{m-2}+1+c}{w_{m-1}^{\alpha }+w_{m-2}+c},\ \ \ m\in N, \end{aligned}$$ where $$\alpha \in [0,1], c\in [0,\infty )$$ , the initial values $$w_{j}\in (0,\infty ), j=0,-1,-2.$$ By virtue of semi-cycle analysis method, the rules of trajectory structure are tested in prime period 7. The continuous length of positive and negative semi-cycles of any nontrivial solution appears periodically and the rule is $$3^{-},2^{+},1^{-},1^{+}$$ . Also, the positive equilibrium is globally asymptotically stable. Finally, two examples are given to show effectiveness of our theoretic analysis.},
  archive      = {J_JAMC},
  author       = {Shen, Liqin and Zhang, Qianhong},
  doi          = {10.1007/s12190-024-02247-5},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {453-463},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On the rule of trajectory structure for a third-order nonlinear difference equation using semi-cycle analysis method},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a higher order fuzzy difference equation with a quadratic
term. <em>JAMC</em>, <em>71</em>(1), 429–452. (<a
href="https://doi.org/10.1007/s12190-024-02243-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A qualitative analysis of a second-order fuzzy difference equation featuring a quadratic term was recently explored in this journal. The study presented was limited to a second-order equation. Here, we generalize the study to a higher-order fuzzy difference equation with a quadratic component. Furthermore, we establish adequate conditions on the qualitative dynamics involving boundedness, persistence, and the convergence of positive fuzzy solutions to the equation. In addition, we provide two simulation instances to validate our theoretical examination.},
  archive      = {J_JAMC},
  author       = {Redjam, Ibtissem and Halim, Yacine and Fečkan, Michal},
  doi          = {10.1007/s12190-024-02243-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {429-452},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On a higher order fuzzy difference equation with a quadratic term},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient numerical solution of linear fredholm
integro-differential equations via backward finite difference and
nyström methods. <em>JAMC</em>, <em>71</em>(1), 415–428. (<a
href="https://doi.org/10.1007/s12190-024-02246-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel numerical approach for solving linear Fredholm integro-differential equations. We integrate the backward finite difference method with the Nyström method to reduce the system size by half compared to the method proposed by Tair et al. (J. Appl. Math. Comput. Mech. 20(3):53-64, 2021). This reduction enhances computational efficiency and is particularly advantageous for large integration intervals. Our method ensures convergence by constructing a new norm for $$\mathbb {R}^{N+1}$$ . Numerical tests demonstrate the superiority of our method in terms of execution time and accuracy. The results contribute to the ongoing efforts in solving integro-differential equations more effectively, offering a robust tool for applications in various scientific and engineering domains.},
  archive      = {J_JAMC},
  author       = {Dida, Ridha and Guebbai, Hamza and Segni, Sami},
  doi          = {10.1007/s12190-024-02246-6},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {415-428},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Efficient numerical solution of linear fredholm integro-differential equations via backward finite difference and nyström methods},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stable higher-order numerical method for solving a system
of third-order singular emden-fowler type equations. <em>JAMC</em>,
<em>71</em>(1), 387–414. (<a
href="https://doi.org/10.1007/s12190-024-02233-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new higher-order numerical method based on a difference scheme with uniform steps to solve a strongly nonlinear system of third-order singular Emden-Fowler-type equations. These problems are challenging to solve because of their singularity or strong nonlinearity. To handle the singularity of the problem, we approximate the derivatives at the endpoints and develop a new difference scheme. This scheme provides a system of nonlinear equations solved by an iterative method. Also, we mathematically establish the method’s stability, consistency, and convergence analysis using a matrix analysis approach. We also verify the presented technique’s efficiency, accuracy and applicability by solving different examples from the literature. We also show that the theoretical order of the technique is consistent with the numerical convergence rates. Additionally, our method easily achieves higher-order accuracy with minimal grid points, unlike most methods that typically require modifying the equation into an equivalent integral equation or using L’Hospital’s rule to remove singularities, resulting in lower-order accuracy approaches.},
  archive      = {J_JAMC},
  author       = {Sahoo, Nirupam and Singh, Randhir},
  doi          = {10.1007/s12190-024-02233-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {387-414},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A stable higher-order numerical method for solving a system of third-order singular emden-fowler type equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel global algorithm for solving linear multiplicative
problem by integrating linear combination rule and branch-and-bound
framework. <em>JAMC</em>, <em>71</em>(1), 365–386. (<a
href="https://doi.org/10.1007/s12190-024-02244-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel global algorithm to solve linear multiplicative problem (LMP) by integrating branch-and-bound framework with convex relaxation problem, linear combination rule and region reduction technique. Firstly, LMP is reformulated via D.C. form, and the reformulated LMP is converted into one of its equivalent problems. Secondly, a convex relaxation problem is constructed based on the characteristics of the equivalent problem and utilizing convex relaxation method. Thirdly, the linear combination rule and the region reduction technique are utilized to enhance efficiency of the algorithm and to reduce its computational time, respectively. Finally, numerical experiments validate that the new algorithm can solve LMP efficiently.},
  archive      = {J_JAMC},
  author       = {Zhang, Yanzhen and Shen, Peiping},
  doi          = {10.1007/s12190-024-02244-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {365-386},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {A novel global algorithm for solving linear multiplicative problem by integrating linear combination rule and branch-and-bound framework},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal error estimates of second-order semi-discrete
stabilized scheme for the incompressible MHD equations. <em>JAMC</em>,
<em>71</em>(1), 323–363. (<a
href="https://doi.org/10.1007/s12190-024-02242-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to construct optimal error estimates of second-order stabilized scheme for the incompressible magnetohydrodynamic(MHD) system. For this purpose, we first construct first- and second-order semi-discrete schemes in which the time derivative term is treated by the first-order backward Euler method and the second-order backward difference formulation, respectively. Moreover, the nonlinear terms are treated by semi-implicit method, and the coupling of velocity and pressure is decoupled by a Gauge–Uzawa method. Thus, the schemes achieve decoupling of velocity, magnetic field and pressure. Most importantly, the proposed schemes do not need to deal with the artificial boundary conditions on pressure and do not need to give an initial value of pressure. Then, the unconditional stability of the two schemes is demonstrated. Furthermore, through rigorous error analysis, we provide optimal convergence orders for all unknowns. Finally, some numerical experiments demonstrate the accuracy and effectiveness of the proposed schemes.},
  archive      = {J_JAMC},
  author       = {Wang, Zhaowei and Wang, Danxia and Zhang, Jun and Jia, Hongen},
  doi          = {10.1007/s12190-024-02242-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {323-363},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Optimal error estimates of second-order semi-discrete stabilized scheme for the incompressible MHD equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge resolvability of generalized honeycomb rhombic torus.
<em>JAMC</em>, <em>71</em>(1), 303–322. (<a
href="https://doi.org/10.1007/s12190-024-02231-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum resolving sets (edge or vertex) have become integral to computer science, molecular topology, and combinatorial chemistry. Resolving sets for a specific network provide crucial information required for uniquely identifying each item in the network. The metric(respectively edge metric) dimension of a graph is the smallest number of the nodes needed to determine all other nodes (resp. edges) based on shortest path distances uniquely. Metric and edge metric dimensions as graph invariants have numerous applications, including robot navigation, pharmaceutical chemistry, canonically labeling graphs, and embedding symbolic data in low-dimensional Euclidean spaces. A honeycomb torus network can be obtained by joining pairs of nodes of degree two of the honeycomb mesh. Honeycomb torus has recently gained recognition as an attractive alternative to existing torus interconnection networks in parallel and distributed applications. In this article, we will discuss the Honeycomb Rhombic torus graph on the basis of edge metric dimension.},
  archive      = {J_JAMC},
  author       = {Kiran, Ayesha Andalib and Shaker, Hani and Saputro, Suhadi Wido},
  doi          = {10.1007/s12190-024-02231-z},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {303-322},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Edge resolvability of generalized honeycomb rhombic torus},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global stabilization and boundary control of coupled
fisher–stream equation and application to SIS–stream model.
<em>JAMC</em>, <em>71</em>(1), 279–302. (<a
href="https://doi.org/10.1007/s12190-024-02226-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the global stabilization of solutions to the initial-boundary value problem for the coupled model of the Fisher equation and Stream temperature equation (i.e., Fisher–Stream model) is studied. It is shown that under the non-homogeneous Dirichlet condition, the large time behavior of model analytical solutions is controlled by boundary conditions. Promoting to application, we establish similar conclusions in the coupled equation of the SIS model and Stream temperature equation.},
  archive      = {J_JAMC},
  author       = {Wang, Fang and Liu, Yuting and Chen, Yuxue},
  doi          = {10.1007/s12190-024-02226-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {279-302},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Global stabilization and boundary control of coupled Fisher–Stream equation and application to SIS–Stream model},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized rough approximation spaces inspired by
cardinality neighborhoods and ideals with application to dengue disease.
<em>JAMC</em>, <em>71</em>(1), 247–277. (<a
href="https://doi.org/10.1007/s12190-024-02235-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to define four new kinds of rough set models based on cardinality neighborhoods and two ideals. The significance of these methods lies in their foundation on ideals, which serve as topological tools. Furthermore, the use of two ideals offers two perspectives instead of just one, thereby reducing the boundary region and increasing the accuracy, which is the primary objective of rough set theory. The concepts of lower and upper approximations based on ideals are presented for the four types. Additionally, we establish essential properties and results for these approximations and construct counterexamples to demonstrate how some of Pawlak’s properties have dissipated in the proposed models. The relationships between the current and previous approximations are discussed, and algorithms to classify whether a subset is exact or rough are introduced. Furthermore, we demonstrate how one combination of ideals is applied to address rough paradigms from a topological perspective. Practically, we apply the proposed paradigms to dengue disease management and elucidate two key points: first, our models are distinguished compared to previous ones by retaining most properties of the original approximation operators proposed by Pawlak; and second, we identify which of the proposed models is better at increasing the accuracy of subsets. In conclusion, we debate the advantages of the suggested models and the motivations behind each type, while also highlighting some of their shortcomings.},
  archive      = {J_JAMC},
  author       = {Al-shami, Tareq M. and Hosny, M. and Arar, Murad and Hosny, Rodyna A.},
  doi          = {10.1007/s12190-024-02235-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {247-277},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Generalized rough approximation spaces inspired by cardinality neighborhoods and ideals with application to dengue disease},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical analysis for interacting multi functional
extreme learning machines. <em>JAMC</em>, <em>71</em>(1), 203–246. (<a
href="https://doi.org/10.1007/s12190-024-02225-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we analyze interacting multi functional extreme learning machines by applying graph theory, groupoid theory, representation theory, operator theory, operator algebra theory and free probability.},
  archive      = {J_JAMC},
  author       = {Cho, Ilwoo and Jorgensen, Palle E. T.},
  doi          = {10.1007/s12190-024-02225-x},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {203-246},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Mathematical analysis for interacting multi functional extreme learning machines},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical modeling and stability of SARS-CoV-2
transmission dynamics among domestic tourists in thailand.
<em>JAMC</em>, <em>71</em>(1), 173–202. (<a
href="https://doi.org/10.1007/s12190-024-02228-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The defined epidemiological model system explaining the spread of infectious diseases characterized with SARS-CoV-2 is analysed. The resulting SEIQR model is analysed in a closed system. It considers the basic reproductive value, the equilibrium point, local subclinical stability of the disease-free equilibrium point and local subclinical stability of the endemic equilibrium point. This is examined and the asymptotic dynamics of the appropriate model system are investigated. Further, a sensitivity analysis supplemented by simulations is prepared in advance to impose how changes in parameters involve the dynamic behaviours of the model.},
  archive      = {J_JAMC},
  author       = {Sungchasit, Rattiya and Pongsumpun, Puntani},
  doi          = {10.1007/s12190-024-02228-8},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {173-202},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Mathematical modeling and stability of SARS-CoV-2 transmission dynamics among domestic tourists in thailand},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-resolution numerical method for the time-fractional
fourth-order diffusion problems via improved quintic b-spline function.
<em>JAMC</em>, <em>71</em>(1), 133–171. (<a
href="https://doi.org/10.1007/s12190-024-02229-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we design and analyse a high-order numerical algorithm based on the improvised quintic B-spline collocation method for solving the fourth-order fractional diffusion equation. The time-fractional derivative is approximated by Caputo’s time derivative. The space derivative is approximated by the collocation method based on improvised quintic B-spline functions. It is shown that the proposed algorithm is unconditionally stable. Through rigorous convergence analysis, the method is shown $$(2-\beta )$$ order convergent in time and almost sixth-order convergent in space direction. It is also shown that the theoretical rate of convergence is the same as that acquired experimentally. To confirm the theoretical results and to test the efficiency and robustness, the method is tested on three problems. The main contribution of the developed algorithm is that the order of convergence and numerical results obtained are better than the existing methods, like the sextic B-spline collocation method (Roul and Goura in Appl Math Comput 366:124727, 2020), the quintic B-spline method (Siddiqi and Arshed in Int J Comput Math 92(7):1496–1518, 2015), and the quintic spline method (Tariq and Akram in Numer Methods Part Differ Equ 33(2):445–466, 2017). It has been proved that the order of convergence of the proposed method is six, which is two orders of magnitude higher than the other spline collocation methods.},
  archive      = {J_JAMC},
  author       = {Alam, Mohammad Prawesh and Khan, Arshad and Roul, Pradip},
  doi          = {10.1007/s12190-024-02229-7},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {133-171},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {High-resolution numerical method for the time-fractional fourth-order diffusion problems via improved quintic B-spline function},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extension of the subgradient extragradient algorithm for
solving variational inequalities without monotonicity. <em>JAMC</em>,
<em>71</em>(1), 103–131. (<a
href="https://doi.org/10.1007/s12190-024-02219-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two improved subgradient extragradient algorithms are proposed for solving nonmonotone variational inequalities under the nonempty assumption of the solution set of the dual variational inequalities. First, when the mapping is Lipschitz continuous, we propose an improved subgradient extragradient algorithm with self-adaptive step-size (ISEGS for short). In ISEGS, the next iteration point is obtained by projecting sequentially the current iteration point onto two different half-spaces, and only one projection onto the feasible set is required in the process of constructing the half-spaces per iteration. The self-adaptive technique allows us to determine the step-size without using the Lipschitz constant. Second, we extend our algorithm into the case where the mapping is merely continuous. The Armijo line search approach is used to handle the non-Lipschitz continuity of the mapping. The global convergence of both algorithms is established without monotonicity assumption of the mapping. The computational complexity of the two proposed algorithms is analyzed. Some numerical examples are given to show the efficiency of the new algorithms.},
  archive      = {J_JAMC},
  author       = {Chen, Jiaxin and Huang, Zunjie and Zhang, Yongle},
  doi          = {10.1007/s12190-024-02219-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {103-131},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Extension of the subgradient extragradient algorithm for solving variational inequalities without monotonicity},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P,q-quasirung orthopair fuzzy multi-criteria group
decision-making algorithm based on generalized dombi aggregation
operators. <em>JAMC</em>, <em>71</em>(1), 69–102. (<a
href="https://doi.org/10.1007/s12190-024-02227-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The p,q-quasirung orthopair fuzzy (p,q-ROF) sets offer a superior approach to describing fuzzy and uncertain information compared to q-rung orthopair fuzzy sets. This paper first introduces generalized Dombi operational laws for p,q-ROF numbers. Utilizing these laws, we develop the p,q-ROF generalized Dombi weighted average (p,q-ROFGDWA) operator, the p,q-ROF generalized Dombi weighted geometric (p,q-ROFGDWG) operator, and their ordered weighted forms. We thoroughly examine the desirable properties and special cases of these new aggregation operators. Subsequently, we devise a multiple-criteria group decision-making method based on the p,q-ROFGDWA and p,q-ROFGDWG operators. Also an example regarding the selection of infectious medical waste treatment technology in Lahore, Pakistan is provided to exemplify the practicality and effectiveness of the developed model. The obtained results are then compared with other relevant methods, highlighting the efficacy and authenticity of the propound approach. Additionally, sensitivity analysis is performed to verify the suggested method’s stability. The findings indicate that the framed approach delivers robust and credible results for determining the ideal healthcare waste treatment technology.},
  archive      = {J_JAMC},
  author       = {Ali, Jawad and Mehmood, Zahid},
  doi          = {10.1007/s12190-024-02227-9},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {69-102},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {P,q-quasirung orthopair fuzzy multi-criteria group decision-making algorithm based on generalized dombi aggregation operators},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Different wave structures in water wave mechanics with two
conformable models. <em>JAMC</em>, <em>71</em>(1), 49–68. (<a
href="https://doi.org/10.1007/s12190-024-02222-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the exact wave solutions of the time-fractional modified Liouville equation (mLE) and time-fractional modified regularized long wave equation (mRLWE) which arise in water wave mechanics, via a new version of trial equation method (NVTEM). The present nonlinear models are reduced to nonlinear ordinary differential equations (NLODEs) by the traveling wave transform and the proposed solution by the NVTEM is used to evaluate the solutions of mLE and mRLWE. This analytical method is not applied before to these equations and novel wave solutions are acquired in the form of rational, exponential, hyperbolic, and Jacobi elliptic types. The solitary solutions of the equations under consideration make them essential models in shallow water dynamics, in liquids and gas bubbles, in magneto-hydrodynamics, and in plasma. This fact has become a motivation for this research.},
  archive      = {J_JAMC},
  author       = {Kırcı, Özlem and Pandır, Yusuf and Bulut, Hasan},
  doi          = {10.1007/s12190-024-02222-0},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {49-68},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Different wave structures in water wave mechanics with two conformable models},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the modulus-based methods without auxiliary variable for
vertical linear complementarity problems. <em>JAMC</em>, <em>71</em>(1),
31–48. (<a href="https://doi.org/10.1007/s12190-024-02218-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we conduct a further analysis of the modulus-based matrix splitting iteration method proposed in J-W He, S Vong. (Appl Math Lett 134: 108344, 2022) for vertical linear complementarity problems. Our study extends and enhances the existing theoretical theorems, which are also validated through numerical examples.},
  archive      = {J_JAMC},
  author       = {Zheng, Hua and Vong, Seakweng},
  doi          = {10.1007/s12190-024-02218-w},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {31-48},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {On the modulus-based methods without auxiliary variable for vertical linear complementarity problems},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Higher-order predictor–corrector methods for fractional
benjamin–bona–mahony–burgers’ equations. <em>JAMC</em>, <em>71</em>(1),
1–30. (<a href="https://doi.org/10.1007/s12190-024-02223-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we construct a higher order predictor–corrector technique for time fractional Benjamin–Bona–Mahony–Burgers’ equations. Instead of directly using an explicit scheme as the predictor in traditional predictor–corrector methods, we employ a new predictor scheme based on the author’s previous work ([24] https://doi.org/10.1007/s10910-024-01589-6 ), in which the given nonlinear equation is linearized by several linearization techniques and solved by Adams–Moulton scheme for the temporal direction and fourth order finite difference scheme for the spatial direction. Once the predictor solution is obtained, the higher order Adams–Moulton method is used as the corrector. Moreover, to make much higher order technique, a multiple correction technique is introduced by repeatedly correcting the results induced from the predictor. Numerical results demonstrate the efficiency of the proposed schemes.},
  archive      = {J_JAMC},
  author       = {Bu, Sunyoung and Jeon, Yonghyeon},
  doi          = {10.1007/s12190-024-02223-z},
  journal      = {Journal of Applied Mathematics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {J. Appl. Math. Comput.},
  title        = {Higher-order predictor–corrector methods for fractional Benjamin–Bona–Mahony–Burgers’ equations},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jar---8">JAR - 8</h2>
<ul>
<li><details>
<summary>
(2025). Use and abuse of instance parameters in the lean
mathematical library. <em>JAR</em>, <em>69</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10817-024-09712-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lean mathematical library Mathlib features extensive use of the typeclass pattern for organising mathematical structures, based on Lean’s mechanism of instance parameters. Related mechanisms for typeclasses are available in other provers including Agda, Coq and Isabelle with varying degrees of adoption. This paper analyses representative examples of design patterns involving instance parameters in the finalized Lean 3 version of Mathlib, focussing on complications arising at scale and how the Mathlib community deals with them.},
  archive      = {J_JAR},
  author       = {Baanen, Anne},
  doi          = {10.1007/s10817-024-09712-7},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {J. Auto. Reasoning},
  title        = {Use and abuse of instance parameters in the lean mathematical library},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpolation and SAT-based model checking revisited:
Adoption to software verification. <em>JAR</em>, <em>69</em>(1), 1–29.
(<a href="https://doi.org/10.1007/s10817-024-09702-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article Interpolation and SAT-Based Model Checking (McMillan in: Proc. CAV 2003, LNCS, Springer [56]) describes a formal-verification algorithm, which was originally devised to verify safety properties of finite-state transition systems. It derives interpolants from unsatisfiable BMC queries and collects them to construct an overapproximation of the set of reachable states. Although 20 years old, the algorithm is still state-of-the-art in hardware model checking. Unlike other formal-verification algorithms, such as or PDR, which have been extended to handle infinite-state systems and investigated for program analysis, McMillan’s interpolation-based model-checking algorithm from 2003 has not been used to verify programs so far. Our contribution is to close this significant, two decades old gap in knowledge by adopting the algorithm to software verification. We implemented it in the verification framework CPAchecker and evaluated the implementation against other state-of-the-art software-verification techniques on the largest publicly available benchmark suite of C safety-verification tasks. The evaluation demonstrates that McMillan’s interpolation-based model-checking algorithm from 2003 is competitive among other algorithms in terms of both the number of solved verification tasks and the run-time efficiency. Our results are important for the area of software verification, because researchers and developers now have one more approach to choose from.},
  archive      = {J_JAR},
  author       = {Beyer, Dirk and Lee, Nian-Ze and Wendler, Philipp},
  doi          = {10.1007/s10817-024-09702-9},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Auto. Reasoning},
  title        = {Interpolation and SAT-based model checking revisited: Adoption to software verification},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Satisfiability of non-linear transcendental arithmetic as a
certificate search problem. <em>JAR</em>, <em>69</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s10817-024-09716-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For typical first-order logical theories, satisfying assignments have a straightforward finite representation that can directly serve as a certificate that a given assignment satisfies the given formula. For non-linear real arithmetic augmented with trigonometric and exponential functions ( $$\mathcal {N\hspace{-0.55542pt}T\hspace{-2.22214pt}A}$$ ), however, there is no known direct representation of satisfying assignments that allows for a simple independent check of whether the represented numbers exist and satisfy the given formula. Hence, in this paper, we introduce a different form of satisfiability certificate for $$\mathcal {N\hspace{-0.55542pt}T\hspace{-2.22214pt}A}$$ , and formulate the satisfiability problem as the problem of searching for such a certificate. This does not only ease the independent verification of satisfiability, but also allows the design of new algorithms that show satisfiability by systematically searching for such certificates. Computational experiments document that the resulting algorithms are able to prove satisfiability of a substantially higher number of benchmark problems than existing methods. We also characterize the formulas whose satisfiability can be demonstrated by such a certificate, by providing lower and upper bounds in terms of relevant well-known classes. Finally we show the existence of a procedure for checking the satisfiability of $$\mathcal {N\hspace{-0.55542pt}T\hspace{-2.22214pt}A}$$ -formulas that terminates for formulas that satisfy certain robustness assumptions.},
  archive      = {J_JAR},
  author       = {Lipparini, Enrico and Ratschan, Stefan},
  doi          = {10.1007/s10817-024-09716-3},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-35},
  shortjournal = {J. Auto. Reasoning},
  title        = {Satisfiability of non-linear transcendental arithmetic as a certificate search problem},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-assisted proofs for lyapunov stability via sums of
squares certificates and constructive analysis. <em>JAR</em>,
<em>69</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s10817-024-09717-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a computer-assisted approach to ensure that a given discrete-time polynomial system is (asymptotically) stable. Our framework relies on constructive analysis together with formally certified sums of squares Lyapunov functions. The crucial steps are formalized within the proof assistant $$\texttt {Minlog}$$ . We illustrate our approach with an example issued from the control system literature.},
  archive      = {J_JAR},
  author       = {Devadze, Grigory and Magron, Victor and Streif, Stefan},
  doi          = {10.1007/s10817-024-09717-2},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. Auto. Reasoning},
  title        = {Computer-assisted proofs for lyapunov stability via sums of squares certificates and constructive analysis},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formalization of the prime number theorem with a remainder
term. <em>JAR</em>, <em>69</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10817-025-09718-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the formalization of the prime number theorem with a remainder term in the Isabelle/HOL proof assistant. First, we formalized several lemmas in complex analysis that were not available in the library, such as the Borel–Carathéodory theorem and the factorization of an analytic function on a compact region. Then, we use these results to formalize a zero-free region of the Riemann zeta function with an explicitly computed constant and deduce the asymptotic growth order of $$\zeta &#39;(s) / \zeta (s)$$ near $$\textrm{Re}(s) = 1$$ . Finally, using a specific form of Perron’s formula, we prove the prime number theorem with the classical remainder term, expressed in terms of $$\psi (x)$$ . We also formalized the result that the prime number theorem stated using $$\psi (x)$$ can imply the version stated using $$\pi (x)$$ . Thus, we can achieve the main result of this paper. Our work extensively utilizes the rich libraries of complex analysis and asymptotic analysis in Isabelle/HOL, including concepts such as the winding number, the residue theorem, and proof automation tools such as the tactic. This is why we chose Isabelle to formalize analytic number theory instead of using other interactive provers.},
  archive      = {J_JAR},
  author       = {Song, Shuhao and Yao, Bowen},
  doi          = {10.1007/s10817-025-09718-9},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {J. Auto. Reasoning},
  title        = {Formalization of the prime number theorem with a remainder term},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Certified first-order AC-unification and
applications. <em>JAR</em>, <em>69</em>(1), 1. (<a
href="https://doi.org/10.1007/s10817-024-09715-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JAR},
  author       = {Ayala-Rincón, Mauricio and Fernández, Maribel and Ferreira Silva, Gabriel and Kutsia, Temur and Nantes-Sobrinho, Daniele},
  doi          = {10.1007/s10817-024-09715-4},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Auto. Reasoning},
  title        = {Correction to: Certified first-order AC-unification and applications},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast, verified computation for HOL ITPs. <em>JAR</em>,
<em>69</em>(1), 1–40. (<a
href="https://doi.org/10.1007/s10817-025-09719-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We add an efficient function for computation to the kernels of higher-order logic interactive theorem provers. First, we develop and prove sound our approach for Candle. Candle is a port of HOL Light which has been proved sound with respect to the inference rules of its higher-order logic; we extend its implementation and soundness proof. Second, we replicate our now-verified implementation for HOL4 with only minor changes, and build additional automation for ease of use. The automation exists outside of the HOL4 kernel, and requires no additional trust. We exercise our new computation function and associated automation on the evaluation of the CakeML compiler backend within HOL4’s logic, demonstrating an order of magnitude speedup. This is an extended version of our previous conference paper [2], which described implementation and soundness proofs for Candle. Our HOL4 implementation and automation are new, as are the CakeML benchmarks.},
  archive      = {J_JAR},
  author       = {Abrahamsson, Oskar and Myreen, Magnus O. and Norrish, Michael and Kanabar, Hrutvik and Pohjola, Johannes Åman},
  doi          = {10.1007/s10817-025-09719-8},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-40},
  shortjournal = {J. Auto. Reasoning},
  title        = {Fast, verified computation for HOL ITPs},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lessons for interactive theorem proving researchers from a
survey of coq users. <em>JAR</em>, <em>69</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10817-025-09720-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coq Community Survey 2022 was an online public survey of users of the Coq proof assistant conducted during February 2022. Broadly, the survey asked about use of Coq features, user interfaces, libraries, plugins, and tools, views on renaming Coq and Coq improvements, and also demographic data such as education and experience with Coq and other proof assistants and programming languages. The survey received 466 submitted responses, making it the largest survey of users of an interactive theorem prover (ITP) so far. We present the design of the survey, a summary of key results, and analysis of answers relevant to ITP technology development and usage. In particular, we analyze user characteristics associated with adoption of tools and libraries and make comparisons to adjacent software communities. Notably, we find that experience has significant impact on Coq user behavior, including on usage of tools, libraries, and integrated development environments (IDEs).},
  archive      = {J_JAR},
  author       = {de Almeida Borges, Ana and Casanueva Artís, Annalí and Falleri, Jean-Rémy and Gallego Arias, Emilio Jesús and Martin-Dorel, Érik and Palmskog, Karl and Serebrenik, Alexander and Zimmermann, Théo},
  doi          = {10.1007/s10817-025-09720-1},
  journal      = {Journal of Automated Reasoning},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Auto. Reasoning},
  title        = {Lessons for interactive theorem proving researchers from a survey of coq users},
  volume       = {69},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jgo---10">JGO - 10</h2>
<ul>
<li><details>
<summary>
(2025). Convergence of solutions to set optimization problems with
variable ordering structures. <em>JGO</em>, <em>91</em>(3), 677–699. (<a
href="https://doi.org/10.1007/s10898-024-01452-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider set optimization problems with variable ordering structures. Within the framework of the set less order relation with variable ordering structures, we investigate the existence, the upper convergence, and the lower convergence of solutions to such problems in the image spaces. For both the existence and the upper convergence of solutions, we employ new techniques to obtain various results without assuming the compactness of the constraint sets. Additionally, we utilize the domination property concerning the variable ordering cones to address the lower convergence of solutions. The obtained results are presented in several versions from different aspects for convenient comparison with existing results. Many examples are provided to illustrate the novelty of our results or to compare them with existing ones in the literature.},
  archive      = {J_JGO},
  author       = {Anh, L. Q. and Hien, D. V.},
  doi          = {10.1007/s10898-024-01452-7},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {677-699},
  shortjournal = {J. Glob. Optim.},
  title        = {Convergence of solutions to set optimization problems with variable ordering structures},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive nested monte carlo approach for multi-objective
efficient global optimization. <em>JGO</em>, <em>91</em>(3), 647–676.
(<a href="https://doi.org/10.1007/s10898-024-01442-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel algorithm, namely the adaptive nested Monte Carlo based multi-objective Efficient Global Optimization (ANMC-MOEGO), which aims to enhance efficiency and accuracy while minimizing programming complexity in contrast to traditional multi-objective Efficient Global Optimization (MOEGO). In this algorithm, the programming complexity is streamlined by employing Monte Carlo simulation for both hypervolume improvement (HVI) and expected hypervolume improvement (EHVI) calculations. Furthermore, the efficiency and accuracy of HVI and EHVI calculations are improved through the utilization of a novel technique called adaptive Monte Carlo hypercube boundaries (AMCHB), which is based on the bisection method. The algorithm is validated via a set of test functions from the open literature. The numerical results demonstrate that the ANMC-MOEGO algorithm produces solutions closer to the theoretical results, with improved distributions on the corresponding Pareto fronts compared to the algorithm without AMCHB technique. Moreover, when obtaining a better Pareto front, the proposed algorithm is found to be more time-efficient, achieving speedups of up to 22.57 times.},
  archive      = {J_JGO},
  author       = {Xu, Shengguan and Tan, Jianfeng and Zhang, Jiale and Chen, Hongquan and Gao, Yisheng},
  doi          = {10.1007/s10898-024-01442-9},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {647-676},
  shortjournal = {J. Glob. Optim.},
  title        = {Adaptive nested monte carlo approach for multi-objective efficient global optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inexact proximal methods for weakly convex functions.
<em>JGO</em>, <em>91</em>(3), 611–646. (<a
href="https://doi.org/10.1007/s10898-024-01460-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes and develops inexact proximal methods for finding stationary points of the sum of a smooth function and a nonsmooth weakly convex one, where an error is present in the calculation of the proximal mapping of the nonsmooth term. A general framework for finding zeros of a continuous mapping is derived from our previous paper on this subject to establish convergence properties of the inexact proximal point method when the smooth term is vanished and of the inexact proximal gradient method when the smooth term satisfies a descent condition. The inexact proximal point method achieves global convergence with constructive convergence rates when the Moreau envelope of the objective function satisfies the Kurdyka–Łojasiewicz (KL) property. Meanwhile, when the smooth term is twice continuously differentiable with a Lipschitz continuous gradient and a differentiable approximation of the objective function satisfies the KL property, the inexact proximal gradient method achieves the global convergence of iterates with constructive convergence rates.},
  archive      = {J_JGO},
  author       = {Khanh, Pham Duy and Mordukhovich, Boris S. and Phat, Vo Thanh and Tran, Dat Ba},
  doi          = {10.1007/s10898-024-01460-7},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {611-646},
  shortjournal = {J. Glob. Optim.},
  title        = {Inexact proximal methods for weakly convex functions},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New iterative algorithms for solving split variational
inclusions. <em>JGO</em>, <em>91</em>(3), 587–609. (<a
href="https://doi.org/10.1007/s10898-024-01444-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study a class of split variational inclusion (SVI) and regularized split variational inclusion (RSVI) problems in real Hilbert spaces. We discuss various analytical properties of the net generated by the RSVI and establish the existence and uniqueness of the solution to the RSVI. Using analytical properties of this net and under certain assumptions on the parameters and mappings associated with the SVI, we establish the strong convergence of the sequence generated by our proposed iterative algorithm. We also deduce another iterative algorithm by taking the regularization parameters to be zero in our proposed algorithm. We establish the weak convergence of the sequence generated by our new algorithm under certain assumptions. Moreover, we discuss two special cases of the SVI, namely the split convex minimization and the split variational inequality problems, and give several numerical examples.},
  archive      = {J_JGO},
  author       = {Dey, Soumitra and Izuchukwu, Chinedu and Taiwo, Adeolu and Reich, Simeon},
  doi          = {10.1007/s10898-024-01444-7},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {587-609},
  shortjournal = {J. Glob. Optim.},
  title        = {New iterative algorithms for solving split variational inclusions},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new modified halpern-type splitting algorithm for solving
monotone inclusion problems in reflexive banach spaces. <em>JGO</em>,
<em>91</em>(3), 559–585. (<a
href="https://doi.org/10.1007/s10898-025-01467-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly introduces a new modified Halpern-type splitting algorithm for solving the monotone inclusion problem in real reflexive Banach spaces. Furthermore, the strong convergence of the sequence generated by our algorithm is proved under some mild assumptions imposed on the operators and parameters. Finally, several numerical experiments are performed, which illustrate the effectiveness of our algorithm.},
  archive      = {J_JGO},
  author       = {Chen, Lulu and Cai, Gang and Cholamjiak, Prasit and Inkrong, Papatsara},
  doi          = {10.1007/s10898-025-01467-8},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {559-585},
  shortjournal = {J. Glob. Optim.},
  title        = {A new modified halpern-type splitting algorithm for solving monotone inclusion problems in reflexive banach spaces},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On generalized KKT points for the motzkin–straus program.
<em>JGO</em>, <em>91</em>(3), 535–557. (<a
href="https://doi.org/10.1007/s10898-024-01457-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 1965, T.S. Motzkin and E. G. Straus established an elegant connection between the clique number of a graph and the global maxima of a quadratic program defined on the standard simplex. Over the years, this seminal finding has inspired a number of studies aimed at characterizing the properties of the (local and global) solutions of the Motzkin–Straus program. The result has also been generalized in various ways and has served as the basis for establishing new bounds on the clique number and developing powerful clique-finding heuristics. Despite the extensive work done on the subject, apart from a few exceptions, the existing literature pays little or no attention to the Karush–Kuhn–Tucker (KKT) points of the program. In the conviction that these points might reveal interesting structural properties of the graph underlying the program, this paper tries to fill in the gap. In particular, we study the generalized KKT points of a parameterized version of the Motzkin–Straus program, which are defined via a relaxation of the usual first-order optimality conditions, and we present a number of results that shed light on the symmetries and regularities of certain substructures associated with the underlying graph. These combinatorial structures are further analyzed using barycentric coordinates, thereby providing a link to a related quadratic program that encodes local structural properties of the graph. This turns out to be particularly useful in the study of the generalized KKT points associated with a certain class of graphs that generalize the notion of a star graph. Finally, we discuss the associations between the generalized KKT points of the Motzkin–Straus program and the so-called replicator dynamics, thereby offering an alternative, dynamical-system perspective on the results presented in the paper.},
  archive      = {J_JGO},
  author       = {Beretta, Guglielmo and Torcinovich, Alessandro and Pelillo, Marcello},
  doi          = {10.1007/s10898-024-01457-2},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {535-557},
  shortjournal = {J. Glob. Optim.},
  title        = {On generalized KKT points for the Motzkin–Straus program},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative mix thresholding algorithm with continuation
technique for mix sparse optimization and application. <em>JGO</em>,
<em>91</em>(3), 511–534. (<a
href="https://doi.org/10.1007/s10898-024-01441-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mix sparse structure is inherited in a wide class of practical applications, namely, the sparse structure appears as the inter-group and intra-group manners simultaneously. In this paper, we propose an iterative mix thresholding algorithm with continuation technique (IMTC) to solve the $$\ell _0$$ regularized mix sparse optimization. The significant advantage of the IMTC is that it has a closed-form expression and low storage requirement, and it is able to promote the mix sparse structure of the solution. We prove the convergence property and the linear convergence rate of the ITMC to a local minimum; moreover, we show that the ITMC approaches an approximate true mix sparse solution within a tolerance relevant to the noise level under an assumption of restricted isometry property. We also apply the mix sparse optimization to model the differential optical absorption spectroscopy analysis with the wavelength misalignment, and numerical results indicate that the IMTC can exactly and quantitatively predict the existing materials and the factual wavelength misalignment simultaneously within 0.1 s, which meets the demand of improvement of the automatic analysis software.},
  archive      = {J_JGO},
  author       = {Hu, Yaohua and Lu, Jian and Yang, Xiaoqi and Zhang, Kai},
  doi          = {10.1007/s10898-024-01441-w},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {511-534},
  shortjournal = {J. Glob. Optim.},
  title        = {Iterative mix thresholding algorithm with continuation technique for mix sparse optimization and application},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity of linearized quadratic penalty for optimization
with nonlinear equality constraints. <em>JGO</em>, <em>91</em>(3),
483–510. (<a href="https://doi.org/10.1007/s10898-024-01456-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider a nonconvex optimization problem with nonlinear equality constraints. We assume that both, the objective function and the functional constraints, are locally smooth. For solving this problem, we propose a linearized quadratic penalty method, i.e., we linearize the objective function and the functional constraints in the penalty formulation at the current iterate and add a quadratic regularization, thus yielding a subproblem that is easy to solve, and whose solution is the next iterate. Under a new adaptive regularization parameter choice, we provide convergence guarantees for the iterates of this method to an $$\epsilon $$ first-order optimal solution in $${\mathcal {O}}({\epsilon ^{-2.5}})$$ iterations. Finally, we show that when the problem data satisfy Kurdyka–Lojasiewicz property, e.g., are semialgebraic, the whole sequence generated by the proposed algorithm converges and we derive improved local convergence rates depending on the KL parameter. We validate the theory and the performance of the proposed algorithm by numerically comparing it with some existing methods from the literature.},
  archive      = {J_JGO},
  author       = {Bourkhissi, Lahcen El and Necoara, Ion},
  doi          = {10.1007/s10898-024-01456-3},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {483-510},
  shortjournal = {J. Glob. Optim.},
  title        = {Complexity of linearized quadratic penalty for optimization with nonlinear equality constraints},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A branch-and-bound algorithm for parametric mixed-binary
nonlinear programs. <em>JGO</em>, <em>91</em>(3), 457–481. (<a
href="https://doi.org/10.1007/s10898-024-01447-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As rapid response to changes becomes more imperative, optimization under uncertainty has continued to grow in both the continuous and mixed-integer fields. We design a branch-and-bound (BB) algorithm for mixed-binary nonlinear optimization problems with parameters in general locations. At every node of the BB tree we apply a state-of-the-art algorithm we have recently developed to approximately optimize parametric programs containing objectives and constraints biconvex in the variables and parameters. Numerical results are included.},
  archive      = {J_JGO},
  author       = {Pangia, Andrew C. and Wiecek, Margaret M.},
  doi          = {10.1007/s10898-024-01447-4},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {457-481},
  shortjournal = {J. Glob. Optim.},
  title        = {A branch-and-bound algorithm for parametric mixed-binary nonlinear programs},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridizing two linear relaxation techniques in
interval-based solvers. <em>JGO</em>, <em>91</em>(3), 437–456. (<a
href="https://doi.org/10.1007/s10898-024-01449-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deterministic global optimization, techniques for linear relaxation of a non-convex program are used in the lower bound calculation phase. To achieve this phase, most deterministic global optimization codes use reformulation-linearization techniques. However, there exist also two interval-based polyhedral relaxation techniques which produce reliable bounds without adding new auxiliary variables, and which can take into account mathematical operations and most transcendental functions: (i) the affine relaxation technique, used in the IBBA code, based on affine forms and affine arithmetic, and (ii) the extremal Taylor technique, used in the Ibex-Opt code, which is based on a specific interval-based Taylor form. In this paper, we describe how these two interval-based linear relaxation techniques can be hybridized. These two approaches appear to be complementary, and such a hybrid method performs well on a representative sample of constrained global optimization instances.},
  archive      = {J_JGO},
  author       = {Araya, Ignacio and Messine, Frédéric and Ninin, Jordan and Trombettoni, Gilles},
  doi          = {10.1007/s10898-024-01449-2},
  journal      = {Journal of Global Optimization},
  month        = {3},
  number       = {3},
  pages        = {437-456},
  shortjournal = {J. Glob. Optim.},
  title        = {Hybridizing two linear relaxation techniques in interval-based solvers},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jiis---15">JIIS - 15</h2>
<ul>
<li><details>
<summary>
(2025). A self-supervised seed-driven approach to topic modelling
and clustering. <em>JIIS</em>, <em>63</em>(1), 333–353. (<a
href="https://doi.org/10.1007/s10844-024-00891-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models are useful tools for extracting the most salient themes within a collection of documents, grouping them to construct clusters representative of each specific topic. These clusters summarize and represent the semantic contents of the documents for better document interpretation. In this work, we present a light approach able to learn topic representations in a Self-Supervised fashion. More specifically, we propose a lightweight and scalable architecture using a seed-word driven approach to simultaneously co-learn a representation from a document and its corresponding word embeddings. The results obtained on a variety of datasets of different sizes and natures show that our model is capable of extracting meaningful topics. Furthermore, our experiments on five benchmark datasets illustrate that our model outperforms both traditional and neural topic modelling baseline models in terms of different coherence and clustering accuracy measures.},
  archive      = {J_JIIS},
  author       = {Ravenda, Federico and Bahrainian, Seyed Ali and Raballo, Andrea and Mira, Antonietta and Crestani, Fabio},
  doi          = {10.1007/s10844-024-00891-8},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {333-353},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {A self-supervised seed-driven approach to topic modelling and clustering},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement negative sampling recommendation based on
collaborative knowledge graph. <em>JIIS</em>, <em>63</em>(1), 313–332.
(<a href="https://doi.org/10.1007/s10844-024-00892-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling high-quality negative samples and training together with positive samples can help improve the performance and generalization ability of the recommendation model. However, traditional negative sampling methods, such as random sampling or heuristic rules, often fail to adequately capture negative samples that reflect the user’s true taste. To provide interpretability and diversity of negative samples, we propose a collaborative knowledge graph-based reinforcement negative sampling recommendation model called KGRec-RNS that formalizes the problem of finding negative signals into a Markov decision process (MDP). Firstly, user interaction data and external knowledge are integrated into a collaborative Bipartite-Knowledge graph (BKG) as MDP context information environment. Then, an Actor based sampler including graph learning module, neighbor attention module and neighbor pruning module is designed. The graph learning module utilizes graph convolutional neural network (GCN) to extract high-order information of each node, the neighbor attention module is adopted to distinguish the influence of different nodes, and the user conditional action pruning strategy is integrated. Thus, negative samples with interpretability can be screened out. Finally, a Critic based recommender was designed to evaluate the current state and the action reward to guide the policy update of the Actor network, thereby matching high-quality negative samples with positive samples. The experimental results on three real datasets demonstrate that our KGRec-RNS has significant advantages in providing more accurate and diverse recommendations.},
  archive      = {J_JIIS},
  author       = {Zhao, Mengjie and Xun, Yaling and Zhang, Jifu and Li, Yanfeng},
  doi          = {10.1007/s10844-024-00892-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {313-332},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Reinforcement negative sampling recommendation based on collaborative knowledge graph},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph attention networks with adaptive neighbor graph
aggregation for cold-start recommendation. <em>JIIS</em>,
<em>63</em>(1), 293–312. (<a
href="https://doi.org/10.1007/s10844-024-00888-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cold-start problem is a long-standing problem in recommender systems, i.e., lack of historical interaction information hinders effective recommendations for new users and items. Existing methods typically incorporate attribute information of users and items to address the strict cold-start problem. Most existing recommendation methods overlook the sparsity of user attributes in cold start recommendation systems. In this paper, we develop a novel framework, Graph Attention Networks with Adaptive Neighbor Graph Aggregation for cold-start Recommendation (A-GAR), which utilizes the user/item relationship information in cold-start recommendation systems to alleviate the sparsity of attributes. we can achieve more accurate recommendations in cold-start scenarios by fully exploring the complex relations between users/items using graph structures. Specifically, to learn the complex relationships between user/item attributes, we utilize SENet (Squeeze and Excitation Network) and MLP (Multilayer Perceptron) networks to adaptively fuse the embeddings of user/item and their second-order interaction vectors, achieving high-order feature aggregation. To address the issue of lacking preference information in cold-start recommendations, we extend the variational autoencoder to reconstruct missing user preferences (item characteristics) from higher-order attribute features of users/items. In order to learn the potential semantic relationships of nodes in the neighbor graph structure, an attribute graph attention network is used to aggregate the neighbor information of users and the interaction information between neighbors. In this way, the high-order relationships between nodes and the potential semantics of adjacent graphs can be fully explored. Extensive experiments on three real-word datasets with various cold-start scenarios demonstrate that A-GAR yields significant improvements for strict cold-start recommendations.},
  archive      = {J_JIIS},
  author       = {Hu, Qian and Tan, Lei and Gong, Daofu and Li, Yan and Bu, Wenjuan},
  doi          = {10.1007/s10844-024-00888-3},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {293-312},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Graph attention networks with adaptive neighbor graph aggregation for cold-start recommendation},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nirdizati: An advanced predictive process monitoring
toolkit. <em>JIIS</em>, <em>63</em>(1), 259–291. (<a
href="https://doi.org/10.1007/s10844-024-00890-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive Process Monitoring (PPM) is a field of Process Mining that aims at predicting how an ongoing execution of a business process will develop in the future using past process executions recorded in event logs. The recent stream of publications in this field shows the need for tools able to support researchers and users in comparing and selecting the techniques that are the most suitable for them. In this paper, we present Nirdizati , a dedicated tool for supporting users in building, comparing and explaining the PPM models that can then be used to perform predictions on the future of an ongoing case. Nirdizati has been constructed by carefully considering the necessary capabilities of a PPM tool and by implementing them in a client-server architecture able to support modularity and scalability. The features of Nirdizati support researchers and practitioners within the entire pipeline for constructing reliable PPM models. The assessment using reactive design patterns and load tests provides an evaluation of the interaction among the architectural elements, and of the scalability with multiple users accessing the prototype in a concurrent manner, respectively. By providing a rich set of different state-of-the-art approaches, Nirdizati offers to Process Mining researchers and practitioners a useful and flexible instrument for comparing and selecting PPM techniques.},
  archive      = {J_JIIS},
  author       = {Rizzi, Williams and Di Francescomarino, Chiara and Ghidini, Chiara and Maggi, Fabrizio Maria},
  doi          = {10.1007/s10844-024-00890-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {259-291},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Nirdizati: An advanced predictive process monitoring toolkit},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedGR: Cross-platform federated group recommendation system
with hypergraph neural networks. <em>JIIS</em>, <em>63</em>(1), 227–257.
(<a href="https://doi.org/10.1007/s10844-024-00887-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group recommendation systems are widely applied in social media, e-commerce, and diverse platforms. These systems face challenges associated with data privacy constraints and protection regulations, impeding the sharing of user data for model improvement. To address the issue of data silos, federated learning emerges as a viable solution. However, difficulties arise due to the non-independent and non-identically distributed nature of data across different platforms, affecting performance. Furthermore, conventional federated learning often overlooks individual differences among stakeholders. In response to these challenges, we propose a pioneering cross-platform federated group recommendation system named FedGR. FedGR integrates hypergraph convolution, attention aggregation, and fully connected fusion components with federated learning to ensure exceptional model performance while preserving the confidentiality of private data. Additionally, we introduce a novel federated model aggregation strategy that prioritizes models with high training effectiveness, thereby improving overall model performance. To address individual differences, we design a temporal personalization update strategy for updating item representations, allowing local models to focus more on their individual characteristics. To evaluate FedGR, we apply our approach to three real-world datasets, demonstrating the robust capabilities of our cross-platform group recommendation system.},
  archive      = {J_JIIS},
  author       = {Zeng, Junlong and Huang, Zhenhua and Wu, Zhengyang and Chen, Zonggan and Chen, Yunwen},
  doi          = {10.1007/s10844-024-00887-4},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {227-257},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {FedGR: Cross-platform federated group recommendation system with hypergraph neural networks},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-BAG: A multi-model fusion text classification method
combining BERT and GCN using self-domain adversarial training.
<em>JIIS</em>, <em>63</em>(1), 205–225. (<a
href="https://doi.org/10.1007/s10844-024-00889-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training-based methods are considered some of the most advanced techniques in natural language processing tasks, particularly in text classification. However, these methods often overlook global semantic information. In contrast, traditional graph learning methods focus solely on structured information from text to graph, neglecting the hidden local information within the syntactic structure of the text. When combined, these approaches may introduce new noise and training biases. To tackle these challenges, we introduce DA-BAG, a novel approach that co-trains BERT and graph convolution models. Utilizing a self-domain adversarial training method on a single dataset, DA-BAG extracts multi-domain distribution features across multiple models, enabling self-adversarial domain adaptation training without the need for additional data, thereby enhancing model generalization and robustness. Furthermore, by incorporating an attention mechanism in multiple models, DA-BAG effectively combines the structural semantics of the graph with the token-level semantics of the pre-trained model, leveraging hidden information within the text’s syntactic structure. Additionally, a sequential multi-layer graph convolutional neural(GCN) connection structure based on a residual pre-activation variant is employed to stabilize the feature distribution of graph data and adjust the graph data structure accordingly. Extensive evaluations on 5 datasets(20NG, R8, R52, Ohsumed, MR) demonstrate that DA-BAG achieves state-of-the-art performance across a diverse range of datasets.},
  archive      = {J_JIIS},
  author       = {Shao, Dangguo and Su, Shun and Ma, Lei and Yi, Sanli and Lai, Hua},
  doi          = {10.1007/s10844-024-00889-2},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {205-225},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {DA-BAG: A multi-model fusion text classification method combining BERT and GCN using self-domain adversarial training},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing user experience: A content-based recommendation
approach for addressing cold start in music recommendation.
<em>JIIS</em>, <em>63</em>(1), 183–204. (<a
href="https://doi.org/10.1007/s10844-024-00872-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems play a major role in modern music streaming platforms, assisting consumers in finding new music that suits their tastes. However, a significant challenge persists when it comes to recommending new songs that lack historical data. This study introduces a Content based Attentive Sequential Recommendation Model (CASRM) that deals with item cold start issue and recommends relevant and fresh music using Gated Graph Neural Networks (GNNs). Music metadata such as artists, albums, genres, and tags are included in the content information, along with context data incorporating user behaviour such as sessions, listening logs, and music playing sequences. By representing the music data as a graph, we can effectively capture the intricate relationships between songs and users. To capture users’ music preferences, we analyse their interactions with songs within the sessions. We incorporate content-based item embeddings for newly added items, enabling personalized recommendations for new songs based on their characteristics and similarities to the songs listened by users in the past. Specifically, we examined the proposed model on three distinct datasets, and the experimental outcomes show its efficacy in predicting music ratings for new songs. Compared to other baseline methods, the CASRM model achieves superior performance in providing accurate and diverse music recommendations in cold-start scenarios.},
  archive      = {J_JIIS},
  author       = {Jangid, Manisha and Kumar, Rakesh},
  doi          = {10.1007/s10844-024-00872-x},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {183-204},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Enhancing user experience: A content-based recommendation approach for addressing cold start in music recommendation},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting bipolar disorder on social media by post grouping
and interpretable deep learning. <em>JIIS</em>, <em>63</em>(1), 161–182.
(<a href="https://doi.org/10.1007/s10844-024-00884-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipolar disorder is a disorder in which a person expresses manic and depressed emotions repeatedly. Diagnosing bipolar disorder accurately can be difficult because other mood disorders or even regular mood changes may have similar symptoms. Therefore, psychiatrists need to spend a long time observing and interviewing clients to make the diagnosis. Recent studies have trained machine learning models for detecting bipolar disorder on social media. However, most of these studies focused on increasing the accuracy of the model without explaining the classification results. Although the posts of a bipolar disorder user can be observed manually, doing so is not practical since a user can have many posts which may not depict any signs of bipolar disorder. Without any explanations, the trustworthiness of the model decreases. We propose a deep learning model that not only detects and classifies bipolar disorder users but also explains how the model generates the classification results. The posts are first grouped using Latent Dirichlet Allocation, a method commonly used to classify the topic of a text. These groups are then input into the model, and attention mechanisms are utilized to determine which groups have more attention weights and are considered more heavily. Finally, an explanation of the classification results is obtained by visualizing the attention weights. Several case studies are presented to demonstrate the explanations generated through our proposed model. Our model is also compared to other models, achieving the best performance with an F1-Score of 0.92.},
  archive      = {J_JIIS},
  author       = {Thamrin, Syauki Aulia and Chen, Eva E. and Chen, Arbee L. P.},
  doi          = {10.1007/s10844-024-00884-7},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {161-182},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Detecting bipolar disorder on social media by post grouping and interpretable deep learning},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathways to success: A machine learning approach to
predicting investor dynamics in equity and lending crowdfunding
campaigns. <em>JIIS</em>, <em>63</em>(1), 135–159. (<a
href="https://doi.org/10.1007/s10844-024-00883-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdfunding has evolved into a formidable mechanism for collective financing, challenging traditional funding sources such as bank loans, venture capital, and private equity with its global reach and versatile applications across various sectors. This paper explores the complex dynamics of crowdfunding platforms, particularly focusing on investor behaviour and investment patterns within equity and lending campaigns in Italy. By leveraging advanced machine learning techniques, including XGBoost and LSTM networks, we develop predictive models that dynamically analyze real-time and historical data to accurately forecast the success or failure of crowdfunding campaigns. To address the existing gaps in crowdfunding analysis tools, we introduce two novel datasets—one for equity crowdfunding and another for lending. Moreover, our approach extends beyond traditional binary success metrics, proposing novel measures. The insights gained from this study could support crowdfunding strategies, significantly improving project selection and promotional tactics on platforms. By enhancing decision-making processes and providing forward-looking guidance to investors, our computational model aims to empower both campaign creators and platform administrators, ultimately improving the overall efficacy and sustainability of crowdfunding as a financing tool.},
  archive      = {J_JIIS},
  author       = {Porro, Rosa and Ercole, Thomas and Pipitò, Giuseppe and Vessio, Gennaro and Loglisci, Corrado},
  doi          = {10.1007/s10844-024-00883-8},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {135-159},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Pathways to success: A machine learning approach to predicting investor dynamics in equity and lending crowdfunding campaigns},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning approaches to lexical simplification: A
survey. <em>JIIS</em>, <em>63</em>(1), 111–134. (<a
href="https://doi.org/10.1007/s10844-024-00882-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lexical Simplification (LS) is the task of substituting complex words within a sentence for simpler alternatives while maintaining the sentence’s original meaning. LS is the lexical component of Text Simplification (TS) systems with the aim of improving accessibility to various target populations such as individuals with low literacy or reading disabilities. Prior surveys have been published several years before the introduction of transformers, transformer-based large language models (LLMs), and prompt learning that have drastically changed the field of NLP. The high performance of these models has sparked renewed interest in LS. To reflect these recent advances, we present a comprehensive survey of papers published since 2017 on LS and its sub-tasks focusing on deep learning. Finally, we describe available benchmark datasets for the future development of LS systems.},
  archive      = {J_JIIS},
  author       = {North, Kai and Ranasinghe, Tharindu and Shardlow, Matthew and Zampieri, Marcos},
  doi          = {10.1007/s10844-024-00882-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {111-134},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Deep learning approaches to lexical simplification: A survey},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning approaches to predict the execution time of
the meteorological simulation software COSMO. <em>JIIS</em>,
<em>63</em>(1), 85–109. (<a
href="https://doi.org/10.1007/s10844-024-00880-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the execution time of weather forecast models is a complex task, since these models are usually performed on High Performance Computing systems that require large computing capabilities. Indeed, a reliable prediction can imply several benefits, by allowing for an improved planning of the model execution, a better allocation of available resources, and the identification of possible anomalies. However, to make such predictions is usually hard, since there is a scarcity of datasets that benchmark the existing meteorological simulation models. In this work, we focus on the runtime predictions of the execution of the COSMO (COnsortium for SMall-scale MOdeling) weather forecasting model used at the Hydro-Meteo-Climate Structure of the Regional Agency for the Environment and Energy Prevention Emilia-Romagna. We show how a plethora of Machine Learning approaches can obtain accurate runtime predictions of this complex model, by designing a new well-defined benchmark for this application task. Indeed, our contribution is twofold: 1) the creation of a large public dataset reporting the runtime of COSMO run under a variety of different configurations; 2) a comparative study of ML models, which greatly outperform the current state-of-practice used by the domain experts. This data collection represents an essential initial benchmark for this application field, and a useful resource for analyzing the model performance: better accuracy in runtime predictions could help facility owners to improve job scheduling and resource allocation of the entire system; while for a final user, a posteriori analysis could help to identify anomalous runs.},
  archive      = {J_JIIS},
  author       = {De Filippo, Allegra and Di Giacomo, Emanuele and Borghesi, Andrea},
  doi          = {10.1007/s10844-024-00880-x},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {85-109},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Machine learning approaches to predict the execution time of the meteorological simulation software COSMO},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Span-based semantic syntactic dual enhancement for aspect
sentiment triplet extraction. <em>JIIS</em>, <em>63</em>(1), 63–83. (<a
href="https://doi.org/10.1007/s10844-024-00881-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-Based Sentiment Triple Extraction (ASTE), a critical sub-task of Aspect-Based Sentiment Analysis (ABSA), has received extensive attention in recent years. ASTE aims to extract structured sentiment triples from texts, with most existing studies focusing on designing new strategic frameworks. Nonetheless, these methods often overlook the complex characteristics of linguistic expression and the deeper semantic nuances, leading to deficiencies in extracting the semantic representations of triples and effectively utilizing syntactic relationships in texts. To address these challenges, this paper introduces a span-based semantic and syntactic Dual-Enhanced model that deeply integrates rich syntactic information, such as part-of-speech tagging, constituent syntax, and dependency syntax structures. Specifically, we designed a semantic encoder and a syntactic encoder to capture the semantic-syntactic information closely related to the sentence’s underlying intent. Through a Feature Interaction Module, we effectively integrate information across different dimensions and promote a more comprehensive understanding of the relationships between aspects and opinions. We also adopted a span-based tagging scheme that generates more precise aspect sentiment triple extractions by exploring cross-level information and constraints. Experimental results on benchmark datasets derived from the SemEval challenge prove that our model significantly outperforms existing baselines.},
  archive      = {J_JIIS},
  author       = {Ren, Shuxia and Guo, Zewei and Li, Xiaohan and Zhong, Ruikun},
  doi          = {10.1007/s10844-024-00881-w},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {63-83},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Span-based semantic syntactic dual enhancement for aspect sentiment triplet extraction},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal knowledge completion enhanced self-supervised
entity alignment. <em>JIIS</em>, <em>63</em>(1), 43–62. (<a
href="https://doi.org/10.1007/s10844-024-00878-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal graph entity alignment aims at finding the equivalent entity pairs across different temporal knowledge graphs (TKGs). Primarily methods mainly utilize a time-aware and relationship-aware approach to embed and align. However, the existence of long-tail entities in TKGs still restricts the accuracy of alignment, as the limited neighborhood information may restrict the available neighborhood information for obtaining high-quality embeddings, and hence would impact the efficiency of entity alignment in representation space. Moreover, most previous researches are supervised, with heavy dependence on seed labels for alignment, restricting their applicability in scenarios with limited resources. To tackle these challenges, we propose a Temporal Knowledge Completion enhanced Self-supervised Entity Alignment (TSEA). We argue that, with high-quality embeddings, the entities would be aligned in a self-supervised manner. To this end, TSEA is constituted of two modules: A graph completion module to predict the missing links for the long-tailed entities. With the improved graph, TSEA further incorporates a self-supervised entity alignment module to achieve unsupervised alignment. Experimental results on widely adopted benchmarks demonstrate improved performance compared to several recent baseline methods. Additional ablation experiments further corroborate the efficacy of the proposed modules.},
  archive      = {J_JIIS},
  author       = {Fu, Teng and Zhou, Gang},
  doi          = {10.1007/s10844-024-00878-5},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {43-62},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Temporal knowledge completion enhanced self-supervised entity alignment},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint entity and relation extraction with fusion of
multi-feature semantics. <em>JIIS</em>, <em>63</em>(1), 21–42. (<a
href="https://doi.org/10.1007/s10844-024-00871-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity relation extraction is a key technology for extracting structured information from unstructured text and serves as the foundation for building large-scale knowledge graphs. Current joint entity relation extraction methods primarily focus on improving the recognition of overlapping triplets to enhance the overall performance of the model. However, the model still faces numerous challenges in managing intra-triplet and inter-triplet interactions, expanding the breadth of semantic encoding, and reducing information redundancy during the extraction process. These issues make it challenging for the model to achieve satisfactory performance in both normal and overlapping triple extraction. To address these challenges, this study proposes a comprehensive prediction network that includes multi-feature semantic fusion. We have developed a semantic fusion module that integrates entity mask embedding sequences, which enhance connections between entities, and context embedding sequences that provide richer semantic information, to enhance inter-triplet interactions and expand semantic encoding. Subsequently, using a parallel decoder to simultaneously generate a set of triplets, improving the interaction between them. Additionally, we utilize an entity mask sequence to finely prune these triplets, optimizing the final set of triplets. Experimental results on the publicly available datasets NYT and WebNLG demonstrate that, with BERT as the encoder, our model outperforms the baseline model in terms of accuracy and F1 score.},
  archive      = {J_JIIS},
  author       = {Wang, Ting and Yang, Wenjie and Wu, Tao and Yang, Chuan and Liang, Jiaying and Wang, Hongyang and Li, Jia and Xiang, Dong and Zhou, Zheng},
  doi          = {10.1007/s10844-024-00871-y},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {21-42},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Joint entity and relation extraction with fusion of multi-feature semantics},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning and mutual information maximization with
crossmodal transformer for multimodal sentiment analysis. <em>JIIS</em>,
<em>63</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10844-024-00858-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of multimodal sentiment analysis hinges on the seamless integration of information from diverse modalities, where the quality of modality fusion directly influences sentiment analysis accuracy. Prior methods often rely on intricate fusion strategies, elevating computational costs and potentially yielding inaccurate multimodal representations due to distribution gaps and information redundancy across heterogeneous modalities. This paper centers on the backpropagation of loss and introduces a Transformer-based model called Multi-Task Learning and Mutual Information Maximization with Crossmodal Transformer (MMMT). Addressing the issue of inaccurate multimodal representation for MSA, MMMT effectively combines mutual information maximization with crossmodal Transformer to convey more modality-invariant information to multimodal representation, fully exploring modal commonalities. Notably, it utilizes multi-modal labels for uni-modal training, presenting a fresh perspective on multi-task learning in MSA. Comparative experiments on the CMU-MOSI and CMU-MOSEI datasets demonstrate that MMMT improves model accuracy while reducing computational burden, making it suitable for resource-constrained and real-time performance-requiring application scenarios. Additionally, ablation experiments validate the efficacy of multi-task learning and probe the specific impact of combining mutual information maximization with Transformer in MSA.},
  archive      = {J_JIIS},
  author       = {Shi, Yang and Cai, Jinglang and Liao, Lei},
  doi          = {10.1007/s10844-024-00858-9},
  journal      = {Journal of Intelligent Information Systems},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Intell. Inf. Syst.},
  title        = {Multi-task learning and mutual information maximization with crossmodal transformer for multimodal sentiment analysis},
  volume       = {63},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jim---36">JIM - 36</h2>
<ul>
<li><details>
<summary>
(2025). Evaluation of data augmentation and loss functions in
semantic image segmentation for drilling tool wear detection.
<em>JIM</em>, <em>36</em>(2), 1491–1503. (<a
href="https://doi.org/10.1007/s10845-023-02313-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tool wear monitoring is crucial for quality control and cost reduction in manufacturing processes, of which drilling applications are one example. Identification of the wear area in images of cutting inserts is important to building a reliable ground truth for the development of indirect monitoring approaches. Therefore, we present a semantic image segmentation pipeline for wear detection on microscopy images of cutting inserts. A broadly used convolutional neural net, namely a U-Net, is trained with different preprocessing and optimisation task configurations: On the one hand the problem is considered as binary problem, and on the other hand as multiclass problem by differentiating the wear into two different types. By comparing these two problem formulations we investigate whether the separation of the two wear structures improves the performance of the recognition of the wear types. For both problem formulations three loss functions, i. e., Cross Entropy, Focal Cross Entropy, and a loss based on the Intersection over Union (IoU), are investigated.The use of different augmentation intensities during training suggests adequate but not too excessive augmentation, and that with optimal augmentation the choice of loss function gets less important. Furthermore, models are trained on image tiles of different sizes, which has an impact on producing artefacts on the whole image predictions performed by the overlap-tile strategy. In summary, the best performing models are binary models, trained on data with moderate augmentation and an IoU-based loss function.},
  archive      = {J_JIM},
  author       = {Schlager, Elke and Windisch, Andreas and Hanna, Lukas and Klünsner, Thomas and Hagendorfer, Elias Jan and Feil, Tamara},
  doi          = {10.1007/s10845-023-02313-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1491-1503},
  shortjournal = {J. Intell. Manuf.},
  title        = {Evaluation of data augmentation and loss functions in semantic image segmentation for drilling tool wear detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NG-net: No-grasp annotation grasp detection network for
stacked scenes. <em>JIM</em>, <em>36</em>(2), 1477–1490. (<a
href="https://doi.org/10.1007/s10845-024-02321-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving a high grasping success rate in a stacked environment is the core of the robot’s grasping task. Most methods achieve a high grasping success rate by training the network on a dataset containing a large number of grasping annotations which requires a lot of manpower and material resources. Therefore, achieving a high grasping success rate for stacked scenes without grasping annotations is a challenging task. To address this, we propose a No-Grasp annotation grasp detection network for stacked scenes (NG-Net). Our network consists of two modules: an object selection module and a grasp generation module. Specifically, the object selection module performs instance segmentation on the raw point cloud to select the object with the highest score as the object to be grasped, and the grasp generation module uses mathematical methods to analyze the geometric features of the point cloud surface to achieve grasping pose generation without grasping annotations. Experiments show that on the modified IPA-Binpicking dataset G, NG-Net has an average grasp success rate of 97% in the stacked scene grasp experiment, 14–22% higher than PointNetGPD.},
  archive      = {J_JIM},
  author       = {Shi, Min and Hou, Jingzhao and Li, Zhaoxin and Zhu, Dengming},
  doi          = {10.1007/s10845-024-02321-6},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1477-1490},
  shortjournal = {J. Intell. Manuf.},
  title        = {NG-net: No-grasp annotation grasp detection network for stacked scenes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin-driven real-time suppression of delamination
damage in CFRP drilling. <em>JIM</em>, <em>36</em>(2), 1459–1476. (<a
href="https://doi.org/10.1007/s10845-023-02315-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delamination damage should be avoided because it severely affects the quality of CFRP products. This paper proposes a digital twin (DT) driven method for real-time suppression of delamination damage to ensure the highest quality hole exit. The relationship between the increase in thrust caused by tool wear and CFRP delamination was analyzed through extensive drilling experiments. The evolving twin models were developed to integrate the virtual space of the drilling process. Once the cutting parameters and thrust signals were input into the twin, the Gaussian process regression and mathematical models predicted the current tool wear and thrust curve, respectively. The feedback results from the DT dynamically interact with the real drilling operation after the optimization function solves the current critical feed rate (CFR). A DT scheme was designed, and the performance of the deployed DT was tested through an online service panel. The results show that the DT has excellent real-time prediction capability within 100 hole-making cycles, with maximum errors of 4.1% and 4.2% for tool wear and thrust prediction at the exit, respectively. Compared to conventional drilling (CD), DT technology provides closed-loop feedback on the time-varying CFR for each hole, resulting in no delamination mode I and up to 48.4% suppression of delamination mode III. This research has achieved intelligent virtual-real linkage in the CFRP drilling process, providing important theoretical support for effectively suppressing delamination damage in the automated production process.},
  archive      = {J_JIM},
  author       = {Chen, Jielin and Li, Shuang and Teng, Hanwei and Leng, Xiaolong and Li, Changping and Kurniawan, Rendi and Ko, Tae Jo},
  doi          = {10.1007/s10845-023-02315-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1459-1476},
  shortjournal = {J. Intell. Manuf.},
  title        = {Digital twin-driven real-time suppression of delamination damage in CFRP drilling},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible dual-mode sensor with accurate contact pressure
sensing and contactless distance detection functions for robotic
perception. <em>JIM</em>, <em>36</em>(2), 1445–1457. (<a
href="https://doi.org/10.1007/s10845-023-02314-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel flexible dual-mode sensor with both contact pressure and distance sensing abilities for robotic grasping and manipulation applications. The proposed flexible dual-mode sensor measures contactless distances by flat interdigitated electrodes, based on electrical field detection principle. Meanwhile the sensor detects contact pressures by truncated pyramid-shaped porous composites based on graphene nanoplate and silicone rubber. Both the functions of the sensor are encapsulated by cascading assembly, the different sensing units are nested and arranged to avoid coupling effects between different sensing signals. The structural design, working principle, and fabrication process to make the flexible dual-mode sensor were presented. Characterization tests showed that the developed flexible dual-mode sensor has a high sensitivity of 0.33 V/N and stability for contact pressure sensing, this sensor can also detect the distances between objects and sensor with high accuracy. The dual-mode sensor was then mounted onto a robotic arm to perform object’s grasping and collision experiments, results demonstrated that the sensor can accurately measure the distributed contact force and distance between objects for tactile perception. Thus, our proposed flexible dual-mode sensor would have great prospects in robotic safety detection and manipulation applications.},
  archive      = {J_JIM},
  author       = {Chen, Zhijian and Wang, Yancheng and Zhang, Zhongtan and Mei, Deqing and Liu, Weijie},
  doi          = {10.1007/s10845-023-02314-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1445-1457},
  shortjournal = {J. Intell. Manuf.},
  title        = {Flexible dual-mode sensor with accurate contact pressure sensing and contactless distance detection functions for robotic perception},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label oxide classification in float-zone silicon
crystal growth using transfer learning and asymmetric loss.
<em>JIM</em>, <em>36</em>(2), 1429–1444. (<a
href="https://doi.org/10.1007/s10845-023-02302-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Float-Zone (FZ) crystal growth process allows for producing higher purity silicon crystal with much lower concentrations of impurities, in particular low oxygen content. Nevertheless, the FZ process occasionally faces the problem of small contamination from oxidation. This can come in the form of a thin oxide layer that may form on un-melted polysilicon surface. The appearance of the oxide layer indicates degraded machine performance and the need for machine maintenance. Therefore, oxide investigation is important for improving both the FZ process and FZ machines, and the first step is oxide recognition. In this study, we characterized oxide into mainly three varieties, according to their surface texture characteristics, which are: (i) spot (ii) shadow and (iii) ghost curtain. We leveraged FZ images captured from the vision system integrated on the FZ machine to establish an oxide dataset. Targeted for data imbalance problem in our dataset, a method based on transfer learning and asymmetric loss for multi-label oxide classification is presented in this work. The results showed that the pre-trained model and the asymmetric loss used for training outperformed the baseline models and improved the classification performance. Furthermore, this study deeply investigated the effectiveness of the components of asymmetric loss. Finally, Gradient-weighted Class Activation Mapping (Grad-CAM) was employed to explain decision process of the models in order to adopt them in the industry.},
  archive      = {J_JIM},
  author       = {Chen, Tingting and Tosello, Guido and Calaon, Matteo},
  doi          = {10.1007/s10845-023-02302-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1429-1444},
  shortjournal = {J. Intell. Manuf.},
  title        = {Multi-label oxide classification in float-zone silicon crystal growth using transfer learning and asymmetric loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential contrast guidance for aeroengine fault
diagnosis with limited data. <em>JIM</em>, <em>36</em>(2), 1409–1427.
(<a href="https://doi.org/10.1007/s10845-023-02305-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven methods have high requirements for data samples and the ideal state is to have sufficient samples and labels for model training. However, due to the limited sample of aeroengine fault data, existing methods often cannot achieve good classification results. To solve this problem, a contrastive learning strategy guided by fault type differences for aeroengine fault diagnosis with limited samples is proposed. Different from the traditional contrastive learning paradigm using data augmentation, the proposed method uses the fault data to construct sample pairs, uses similarity comparison to learn fault features from limited data, and uses the learned fault features for fault diagnosis. A deep learning model for joint training of feature extractor and classifier is built to improve the fault diagnosis accuracy. Finally, the aeroengine dataset and bearing dataset are used to verify the effectiveness of the proposed method in the case of limited data. The experimental results show that compared with the most advanced methods, the proposed method can achieve higher fault diagnosis accuracy.},
  archive      = {J_JIM},
  author       = {He, Wenhui and Lin, Lin and Fu, Song and Tong, Changsheng and Zu, Lizheng},
  doi          = {10.1007/s10845-023-02305-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1409-1427},
  shortjournal = {J. Intell. Manuf.},
  title        = {Differential contrast guidance for aeroengine fault diagnosis with limited data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective sustainable production planning for a hybrid
multi-stage manufacturing-remanufacturing system with grade-based
classification of recovered and remanufactured products. <em>JIM</em>,
<em>36</em>(2), 1385–1407. (<a
href="https://doi.org/10.1007/s10845-023-02308-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of multi-objective production planning in a hybrid manufacturing and remanufacturing system (HMRS), introducing several significant contributions. First, we propose a new formulation of the problem that extends the existing literature by introducing a multi-objective model. This model aims to minimize both total costs and $$CO_2$$ emissions within a hybrid system composed of various machines in charge of producing new and remanufactured products of different qualities. To efficiently solve this complex problem, we present an innovative approach that integrates several techniques, including NSGA-II, the entropy weight method and the TOPSIS technique. Our research focuses on the economic and environmental aspects of the remanufacturing process, seeking to determine the optimal manufacturing and remanufacturing plan. This plan aims to meet demand for new products and maximize satisfaction for remanufactured products of different qualities, while minimizing the total economic costs and $$CO_2$$ emissions incurred during the various manufacturing and remanufacturing stages, including set-up, production, inventory and disposal. To address the multi-objective nature of this problem, we develop a mathematical model and introduce an approach based on the non-dominated genetic sorting algorithm (NSGA-II). To help decision-making, we use the technique of performance ranking by similarity to the ideal solution (TOPSIS) in combination with the entropy weight method (EWM) to objectively obtain the optimal compromise solution from the Pareto front provided by NSGA-II. Finally, we conduct computational experiments to assess the environmental impact of carbon emissions associated with new, remanufactured and discarded products over a finite production horizon. We illustrate the adaptability of the proposed approach by applying it to two distinct remanufacturing strategies: one where remanufacturing is used to reduce waste, and one where demand for remanufactured products is critical, with a penalty cost associated with any shortfall in demand.},
  archive      = {J_JIM},
  author       = {Lahmar, Houria and Dahane, Mohammed and Mouss, Kinza Nadia and Haoues, Mohammed},
  doi          = {10.1007/s10845-023-02308-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1385-1407},
  shortjournal = {J. Intell. Manuf.},
  title        = {Multi-objective sustainable production planning for a hybrid multi-stage manufacturing-remanufacturing system with grade-based classification of recovered and remanufactured products},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal deep learning for explainable vision-based quality
inspection under visual interference. <em>JIM</em>, <em>36</em>(2),
1363–1384. (<a
href="https://doi.org/10.1007/s10845-023-02297-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based quality inspection is a key step to ensure the quality control of complex industrial products. However, accurate defect recognition for complex products with information-rich, structure-irregular and significantly different patterns is still a tough problem, since it causes the strong visual interference. This paper proposes a causal deep learning method (CDLM) to tackle the explainable vision-based quality inspection under visual interference. First, a structural causal model for defect recognition of complex industrial products is constructed and a causal intervention strategy to overcome the background interference is generated. Second, a defect-guided recognition neural network (DGRNN) is constructed, which can realize accurate defect recognition under the training of CDLM via feature-wise causal intervention using two sub-networks with feature difference mechanism. Finally, the causality between defect features and defective product labels can guide the DGRNN to complete the accurate and explainable learning of defect in a causal direction of optimization. Quantitative experiments show that the proposed method achieves recognition accuracy of 94.09% and 93.95% on two fabric datasets respectively, which outperforms the cutting-edge inspection models. Besides, Grad-CAM visualization experiments show that the proposed method successfully captures the data causality and realizes the explainable defect recognition.},
  archive      = {J_JIM},
  author       = {Liang, Tianbiao and Liu, Tianyuan and Wang, Junliang and Zhang, Jie and Zheng, Pai},
  doi          = {10.1007/s10845-023-02297-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1363-1384},
  shortjournal = {J. Intell. Manuf.},
  title        = {Causal deep learning for explainable vision-based quality inspection under visual interference},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated porosity segmentation in laser powder bed fusion
part using computed tomography: A validity study. <em>JIM</em>,
<em>36</em>(2), 1341–1361. (<a
href="https://doi.org/10.1007/s10845-023-02296-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection in laser powder bed fusion (LPBF) parts is a critical step for in their quality control. Ensuring the integrity of these parts is essential for a broader adoption of this manufacturing process in highly standardized industries such as aerospace. With many challenges to overcome, there is currently no standardized image analysis and segmentation process for the defect analysis of LPBF parts. This process is often manual and operator-dependent, which limits the repeatability and the reproducibility of the analytical methods applied, raising questions about the validity of the analysis. The pore segmentation step is critical for porosity analysis since the pore size and morphology metrics are calculated directly from the results of the segmentation process. In this work, Ti6Al4V specimens with purposely induced and controlled porosity were printed, scanned 5 times on two CT scan systems by two different operators, and then reconstructed as 3D volumes. The porosity in these specimens was analyzed using manual and Otsu thresholding and a convolutional neural network (CNN) deep learning segmentation algorithm. Then, a variance component estimation realized over 75 porosity analyses indicated that, independently of the operator and the CT scan system used, the CNN provided the best repeatability and reproducibility in the LPBF specimens of this study. Finally, a multimodal correlative study using higher resolution laser confocal microscopy observations was used for a multi-scale pore-to-pore comparison and as a reliability assessment of the segmentation algorithms. The validity of the CNN-based pore segmentation was thus assessed through improved repeatability, reproducibility, and reliability.},
  archive      = {J_JIM},
  author       = {Desrosiers, Catherine and Letenneur, Morgan and Bernier, Fabrice and Piché, Nicolas and Provencher, Benjamin and Cheriet, Farida and Guibault, François and Brailovski, Vladimir},
  doi          = {10.1007/s10845-023-02296-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1341-1361},
  shortjournal = {J. Intell. Manuf.},
  title        = {Automated porosity segmentation in laser powder bed fusion part using computed tomography: A validity study},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacked encoded cascade error feedback deep extreme learning
machine network for manufacturing order completion time. <em>JIM</em>,
<em>36</em>(2), 1313–1339. (<a
href="https://doi.org/10.1007/s10845-023-02303-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel stacked encoded cascade error feedback deep extreme learning machine (SEC-E-DELM) network is proposed to predict order completion time (OCT) considering the historical production planning and control data. Usually, the actual OCT significantly deviates from the planned because of recessive disturbances. The disturbances do not shut down production but slow down the production that accumulates over time, causing significant deviation of actual time from planned. The generation of weight parameters in neural networks using a randomization approach has a significant effect on generalization performance. To predict the OCT, firstly, the stacked autoencoder is used to generate input connection weights for the network by learning a deep representation of the real data. Secondly, the learned distribution of the encoder is connected to the network output through output connection weights incrementally learned by the Moore–Penrose inverse. Thirdly, the new hidden unit is added one by one to the network, which receives input connections from the input units and the last layer of the encoder to avoid overfitting and improve model generalization. The input connection weights for the newly added hidden unit are analytically calculated by the error feedback function to enhance the convergence rate by further learning deep features. Lastly, the hidden unit keeps on adding one by one by receiving connections from input units and some of the existing hidden units to make a deep cascade architecture. An extensive comparative study demonstrates that calculating connection weights by the proposed method helps to significantly improve the generalization performance and robustness of the network.},
  archive      = {J_JIM},
  author       = {Khan, Waqar Ahmed and Masoud, Mahmoud and Eltoukhy, Abdelrahman E. E. and Ullah, Mehran},
  doi          = {10.1007/s10845-023-02303-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1313-1339},
  shortjournal = {J. Intell. Manuf.},
  title        = {Stacked encoded cascade error feedback deep extreme learning machine network for manufacturing order completion time},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Correction: Optimal data-driven control of manufacturing
processes using reinforcement learning: An application to wire arc
additive manufacturing. <em>JIM</em>, <em>36</em>(2), 1311. (<a
href="https://doi.org/10.1007/s10845-024-02450-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JIM},
  author       = {Mattera, Giulio and Caggiano, Alessandra and Nele, Luigi},
  doi          = {10.1007/s10845-024-02450-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1311},
  shortjournal = {J. Intell. Manuf.},
  title        = {Correction: optimal data-driven control of manufacturing processes using reinforcement learning: an application to wire arc additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Optimal data-driven control of manufacturing processes
using reinforcement learning: An application to wire arc additive
manufacturing. <em>JIM</em>, <em>36</em>(2), 1291–1310. (<a
href="https://doi.org/10.1007/s10845-023-02307-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, artificial intelligence (AI) has become a crucial Key Enabling Technology with extensive application in diverse industrial sectors. Recently, considerable focus has been directed towards utilizing AI for the development of optimal control in industrial processes. In particular, reinforcement learning (RL) techniques have made significant advancements, enabling their application to data-driven problem-solving for the control of complex systems. Since industrial manufacturing processes can be treated as MIMO non-linear systems, RL can be used to develop complex data-driven intelligent decision-making or control systems. In this work, the workflow for developing a RL application for industrial manufacturing processes, including reward function setup, development of reduced order models and control policy construction, is addressed, and a new process-based reward function is proposed. To showcase the proposed approach, a case study is developed with reference to a wire arc additive manufacturing (WAAM) process. Based on experimental tests, a Reduced Order Model of the system is obtained and a Deep Deterministic Policy Gradient Controller is trained with aim to produce a simple geometry. Particular attention is given to the sim-to-real process by developing a WAAM simulator which allows to simulate the process in a realistic environment and to generate the code to be deployed on the motion platform controller.},
  archive      = {J_JIM},
  author       = {Mattera, Giulio and Caggiano, Alessandra and Nele, Luigi},
  doi          = {10.1007/s10845-023-02307-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1291-1310},
  shortjournal = {J. Intell. Manuf.},
  title        = {Optimal data-driven control of manufacturing processes using reinforcement learning: An application to wire arc additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A digital solution for CPS-based machining path optimization
for CNC systems. <em>JIM</em>, <em>36</em>(2), 1261–1290. (<a
href="https://doi.org/10.1007/s10845-023-02289-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the deep integration of advanced information technology and advanced manufacturing technology has gradually become one of the main ways to achieve smart manufacturing. The computer numerical control (CNC) system is the basic equipment for machining and manufacturing, and the quality and efficiency of the system’s machining are the basis for supporting and ensuring smart manufacturing. However, the G-code used in CNC machining is usually generated with computer-aided manufacturing (CAM) according to a static model, and its tool path is relatively rough, with uneven adjacent paths and bad points in the path causing machining defects. To solve these problems, a modeling approach combining the basic elements of the intelligent CNC system with the human-cyber-physical system (HCPS) model is proposed, and a digital solution for tool path optimization is further proposed, integrating the redesign process of CAM tool path into cyber application. In addition, the process of tool path optimization is processed in steps, and a pipelined processing flow is established to accelerate the optimization process. Finally, the effectiveness of the proposed method is demonstrated using an example of process file optimization for a pentagram convex rib model.},
  archive      = {J_JIM},
  author       = {Zhang, Lipeng and Yu, Haoyu and Wang, Chuting and Hu, Yi and He, Wuwei and Yu, Dong},
  doi          = {10.1007/s10845-023-02289-9},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1261-1290},
  shortjournal = {J. Intell. Manuf.},
  title        = {A digital solution for CPS-based machining path optimization for CNC systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent g-code-based power prediction of ultra-precision
CNC machine tools through 1DCNN-LSTM-attention model. <em>JIM</em>,
<em>36</em>(2), 1237–1260. (<a
href="https://doi.org/10.1007/s10845-023-02293-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the most promising and advanced technology, ultra-precision machining (UPM) has dramatically increased its production volume for wide-range applications in various high-tech fields such as chips, optics, microcircuits, biotechnology, etc. The concomitantly negative environmental impact resulting from huge-volume UPM has attracted unprecedented attention from both academia and industry. Accurate energy prediction of ultra-precision machine tools (UPMTs) can provide significant insight into energy planning, machining strategy, and energy conservation. Data-driven models for predicting energy have become increasingly popular due to their high accuracy and low modeling difficulty. However, existing data-driven models only focus on ordinary precision machine tools, and their applications on UPMTs are hardly studied. To fill the gap, this paper proposed a data-driven model constructed with 1DCNN-LSTM-Attention layers for predicting the instantaneous power profile of a five-axes UPMT. In the data-preparation phase, an advanced G-code interpreter was developed to generate the working status dataset from the G-code command and accurately match them with the power data collected. Random hyperparameters searching method was adopted to tune the 1DCNN-LSTM-Attention structure for better accuracy in the model creation phase. Finally, the sensitivity of these hyperparameters on the model performance was analyzed. Results demonstrate that the learning rate, 1DCNN, LSTM and dense layer numbers are identified as critical parameters affecting the model performance. The optimized 1DCNN-LSTM-Attention model outperforms other models, achieving an R2 value of 0.93. This work first validate the feasibility of utilizing advanced machine learning techniques for predicting energy consumption in UPM field, which can further promoting energy-efficient and sustainable UPM practices by digitalizing the energy consumption process.},
  archive      = {J_JIM},
  author       = {Xu, Zhicheng and Selvaraj, Vignesh and Min, Sangkee},
  doi          = {10.1007/s10845-023-02293-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1237-1260},
  shortjournal = {J. Intell. Manuf.},
  title        = {Intelligent G-code-based power prediction of ultra-precision CNC machine tools through 1DCNN-LSTM-attention model},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using GANs to predict milling stability from limited data.
<em>JIM</em>, <em>36</em>(2), 1201–1235. (<a
href="https://doi.org/10.1007/s10845-023-02291-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Milling is a key manufacturing process that requires the selection of operating parameters that provide efficient performance. However, the presence of chatter, a self-excited vibration causing poor surface finish and potential damage to the machine and cutting tool, makes it challenging to select the appropriate parameters. To predict chatter, stability maps are commonly used, but their generation requires expensive data, making it difficult to employ these maps in industry. Therefore, there is a pressing need for an approach that can accurately predict stability maps using limited experimental data. This study introduces the new Encoder GAN (EGAN) approach based on Generative Adversarial Networks (GANs) that predicts stability maps using limited experimental data. The approach consists of the encoder, generator, and discriminator subnetworks and uses the trained encoder and generator to predict the target stability map. This versatile method can be applied to various tool setups and can accurately predict stability maps with limited experimental data (five to 10 cutting tests) even when there is little information available for unknown parameters. The study evaluates the proposed approach using both numerical data and experiments and demonstrates its superior performance compared to state-of-the-art benchmarks.},
  archive      = {J_JIM},
  author       = {Rezaei, Shahrbanoo and Cornelius, Aaron and Karandikar, Jaydeep and Schmitz, Tony and Khojandi, Anahita},
  doi          = {10.1007/s10845-023-02291-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1201-1235},
  shortjournal = {J. Intell. Manuf.},
  title        = {Using GANs to predict milling stability from limited data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards scalability for resource reconfiguration in robotic
assembly line balancing problems using a modified genetic algorithm.
<em>JIM</em>, <em>36</em>(2), 1175–1199. (<a
href="https://doi.org/10.1007/s10845-023-02292-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assembly lines are still one of the most used manufacturing systems in modern-day production. Most research affects the building of new lines and, less frequently, the reconfiguration of existing lines. However, the first is insufficient to meet the reconfigurable production paradigm required by volatile market demands. Consequent reconfiguration of resources by production requests affects companies’ competitiveness. This paper introduces a problem-specific genetic algorithm for optimizing the reconfiguration of a Robotic Assembly Line Balancing Problem with Task Types, including additional company constraints. First, we present the greenfield and brownfield optimization objectives, then a mathematical problem formulation and the composition of the genetic algorithm. We evaluate our model against an Integer Programming baseline on a reconfiguration dataset with multiple equipment alternatives. The results demonstrate the capabilities of the genetic algorithm for the greenfield case and showcase the possibilities in the brownfield case. With a scalability improvement through computation time decrease of up to $$\sim $$ 2.75 $$\times $$ , reduced number of equipment and workstations, but worse objective values, the genetic algorithm holds the potential for reconfiguring assembly lines. However, the genetic algorithm has to be further optimized for the reconfiguration to leverage its full potential.},
  archive      = {J_JIM},
  author       = {Albus, Marcel and Hornek, Timothée and Kraus, Werner and Huber, Marco F.},
  doi          = {10.1007/s10845-023-02292-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1175-1199},
  shortjournal = {J. Intell. Manuf.},
  title        = {Towards scalability for resource reconfiguration in robotic assembly line balancing problems using a modified genetic algorithm},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human operator decision support for highly transient
industrial processes: A reinforcement learning approach. <em>JIM</em>,
<em>36</em>(2), 1159–1174. (<a
href="https://doi.org/10.1007/s10845-023-02295-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most industrial processes are not fully-automated. Although fast and low-level control can be handled by controllers, initializing and adjusting the reference, or setpoint, values, are commonly tasks assigned to human operators. A major challenge is the control policy variation among operators. In turn this can result in inconsistencies in the final product. In order to guide operators to pursue better and more consistent performance, researchers explore the optimal control policy through different approaches. Although in different applications, researchers use different approaches, an accurate process model is still crucial to the approaches. However, for a highly transient process (e.g., the startup of a manufacturing process), modeling can be challenging and inaccurate, and approaches highly relying on a process model may not work well. In this paper, we apply the idea of offline reinforcement learning (RL), which requires the RL agent to learn control policies from a previously collected dataset. More specifically, a modified advantage weighted regression is used to guide the agent to take the more advantageous actions. In addition, we train and verify the agent by using casting data of multiple human operators from an industrial twin-roll steel strip casting process.},
  archive      = {J_JIM},
  author       = {Ruan, Jianqi and Nooning, Bob and Parkes, Ivan and Blejde, Wal and Chiu, George and Jain, Neera},
  doi          = {10.1007/s10845-023-02295-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1159-1174},
  shortjournal = {J. Intell. Manuf.},
  title        = {Human operator decision support for highly transient industrial processes: A reinforcement learning approach},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embodied intelligence in manufacturing: Leveraging large
language models for autonomous industrial robotics. <em>JIM</em>,
<em>36</em>(2), 1141–1157. (<a
href="https://doi.org/10.1007/s10845-023-02294-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the potential of Large Language Model (LLM) agents for industrial robotics, with an emphasis on autonomous design, decision-making, and task execution within manufacturing contexts. We propose a comprehensive framework that includes three core components: (1) matches manufacturing tasks with process parameters, emphasizing the challenges in LLM agents’ understanding of human-imposed constraints; (2) autonomously designs tool paths, highlighting the LLM agents’ proficiency in planar tasks and challenges in 3D spatial tasks; and (3) integrates embodied intelligence within industrial robotics simulations, showcasing the adaptability of LLM agents like GPT-4. Our experimental results underscore the distinctive performance of the GPT-4 agent, especially in Component 3, where it is outstanding in task planning and achieved a success rate of 81.88% across 10 samples in task completion. In conclusion, our study accentuates the transformative potential of LLM agents in industrial robotics and suggests specific avenues, such as visual semantic control and real-time feedback loops, for their enhancement.},
  archive      = {J_JIM},
  author       = {Fan, Haolin and Liu, Xuan and Fuh, Jerry Ying Hsi and Lu, Wen Feng and Li, Bingbing},
  doi          = {10.1007/s10845-023-02294-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1141-1157},
  shortjournal = {J. Intell. Manuf.},
  title        = {Embodied intelligence in manufacturing: Leveraging large language models for autonomous industrial robotics},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A rotary extrusion system with a rectangular-orifice nozzle:
Toward adaptive resolution in material extrusion additive manufacturing.
<em>JIM</em>, <em>36</em>(2), 1123–1139. (<a
href="https://doi.org/10.1007/s10845-023-02288-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material extrusion additive manufacturing (MEAM) has revolutionized the production of complex designs while minimizing the amount of effort required due to its simple production pipeline. However, MEAM naturally comes with a well-known trade-off; higher build resolution often tends to enhance the product quality at the cost of a slower build rate. Nozzles, the standard tool for thermoplastic extrusion in MEAM, have evolved into a crucial component of the process for controlling the product’s build resolution. The purpose of this study is to investigate the details of a novel extrusion system that makes use of a rotating nozzle with an unconventional aperture, in contrast to its typical (i.e., circular-orifice) counterparts. The unique nozzle configuration that lacks axial symmetry allows for precise control over the effective dimension of the extrusion via rotational guiding. By positioning the oblong orifice at intermediate orientations, the presented approach seeks to provide continuously variable intralayer and interlayer resolutions for MEAM processes. This paper explores the distinctive characteristics of this new nozzle design as well as the potential uses of the novel extrusion system. The outcomes of the conducted tests demonstrate the proof-of-concept for creating variable bead width within the layers, in addition to adaptable layer heights throughout the 3D objects. Possible limitations of the new approach and future perspectives are discussed in detail.},
  archive      = {J_JIM},
  author       = {Gharehpapagh, Bahar and Dilberoglu, Ugur M. and Yaman, Ulas and Dolen, Melik},
  doi          = {10.1007/s10845-023-02288-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1123-1139},
  shortjournal = {J. Intell. Manuf.},
  title        = {A rotary extrusion system with a rectangular-orifice nozzle: Toward adaptive resolution in material extrusion additive manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural lumped parameter differential equations with
application in friction-stir processing. <em>JIM</em>, <em>36</em>(2),
1111–1121. (<a
href="https://doi.org/10.1007/s10845-023-02271-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lumped parameter methods aim to simplify the evolution of spatially-extended or continuous physical systems to that of a “lumped” element representative of the physical scales of the modeled system. For systems where the definition of a lumped element or its associated physics may be unknown, modeling tasks may be restricted to full-fidelity physics simulations. In this work, we consider data-driven modeling tasks with limited point-wise measurements of otherwise continuous systems. We build upon the notion of the Universal Differential Equation (UDE) to construct data-driven models for reducing dynamics to that of a lumped parameter and inferring its properties. The flexibility of UDEs allow for composing various known physical priors suitable for application-specific modeling tasks, including lumped parameter methods. The motivating example for this work is the plunge and dwell stages for friction-stir welding; specifically, (i) mapping power input into the tool to a point-measurement of temperature and (ii) using this learned mapping for process control.},
  archive      = {J_JIM},
  author       = {Koch, James and Choi, WoongJo and King, Ethan and Garcia, David and Das, Hrishikesh and Wang, Tianhao and Ross, Ken and Kappagantula, Keerti},
  doi          = {10.1007/s10845-023-02271-5},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1111-1121},
  shortjournal = {J. Intell. Manuf.},
  title        = {Neural lumped parameter differential equations with application in friction-stir processing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine-learning based process monitoring for automated
composites manufacturing. <em>JIM</em>, <em>36</em>(2), 1095–1110. (<a
href="https://doi.org/10.1007/s10845-023-02282-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated fibre placement (AFP) is an advanced robotic manufacturing technique which can overcome the challenges of traditional composite manufacturing. The interlaminar strength of AFP-manufactured composites depends on the in-situ thermal history during manufacturing. The thermal history is controlled by the choice of processing conditions and improper interfacial temperatures may result in insufficient bonding. Being able to better predict such maintenance issues in real-time is an important focus of smart manufacturing and Industry 4.0 to improve manufacturing operations. The data analysis of real-time temperature measurements during AFP composites manufacturing requires the temperature profiles from Finite Element Analysis (FEA) based simulations of the AFP process to better predict the quality of layup. However, the FEA simulations of the AFP process are computationally expensive. This study focuses on developing a digital tool enabling real-time process monitoring and predictive maintenance of the AFP process. The digital tool constitutes a machine learning-based surrogate model based on results from Finite Element Analysis (FEA) simulations of the AFP process to predict the in-situ thermal profile during AFP manufacturing. Multivariate Linear Regression, Multivariate Polynomial Regression, Support Vector Machine, Random Forest and Artificial Neural Network (ANN)-based models are compared to conclude that ANN based surrogate model performs best by predicting the important parameters of thermal profiles with a mean absolute percentage error of 1.56% on additional test data and reducing the time by four orders of magnitude as compared to FEA simulations. The predicted thermal profile can be compared with the real-time in-situ temperatures during manufacturing to predict the quality of the layup. A GUI application is developed to provide predicted thermal profiles data for analysis in conjunction with real-time temperatures during manufacturing enabling monitoring and predictive maintenance of the AFP process and paving way for the development of a digital twin of the AFP composites manufacturing process.},
  archive      = {J_JIM},
  author       = {Mujtaba, Ahmed and Islam, Faisal and Kaeding, Patrick and Lindemann, Thomas and Gangadhara Prusty, B.},
  doi          = {10.1007/s10845-023-02282-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1095-1110},
  shortjournal = {J. Intell. Manuf.},
  title        = {Machine-learning based process monitoring for automated composites manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thermal-structure finite element simulation system
architecture in a cloud-edge-end collaborative environment.
<em>JIM</em>, <em>36</em>(2), 1063–1094. (<a
href="https://doi.org/10.1007/s10845-023-02269-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the urgent need for finite element simulation, a thermal-structure finite element simulation system architecture is designed to shorten the simulation cycle and improve the mechanical structure design efficiency under a cloud-edge-end collaborative environment. Then, a calculation kernel of the boundary conditions is proposed, and a thermal-structure closed-loop iterative model is established and embedded into the finite element simulation system. The interactions among the simulation results and boundary conditions are considered, and the boundary conditions are corrected by the simulation results. Finally, the thermal-structure finite element simulation system architecture is verified. The optimal configuration of the central processing unit number and memory size for different finite element models is identified, and the simulation efficiency and throughput are improved significantly. In addition, the proposed thermal-structure finite element simulation system architecture with a cloud-edge-end is applied to conduct the thermal-structure behavior simulation for the feed drive system, spindle system, precision horizontal machining center, and gantry machining center. The machine tool designer without specialized knowledge about thermal-structure simulation is able to achieve a high simulation accuracy at the design stage, and the executing performance of the proposed thermal-structure finite element simulation system with cloud-edge-end architecture is far higher than that of the thermal-structure finite element simulation systems with cloud-end and cloud architectures. With the implementation of the simulation system with the cloud-edge-end architecture, the execution time is reduced from 2557 and 2082s to 1642 s as compared with the simulation systems with the cloud-end and cloud architectures, respectively. The simulation kernel is effective in simulating thermal-structure behaviors. The average deviations between the measured and simulated temperatures for the rear and front bearings are 4.36% and 3.15%, respectively. The average deviation between the measured and simulated deformations is 8.17%.},
  archive      = {J_JIM},
  author       = {Liu, Jialan and Ma, Chi and Wang, Shilong},
  doi          = {10.1007/s10845-023-02269-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1063-1094},
  shortjournal = {J. Intell. Manuf.},
  title        = {Thermal-structure finite element simulation system architecture in a cloud-edge-end collaborative environment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial networks and hessian locally linear
embedding for geometric variations management in manufacturing.
<em>JIM</em>, <em>36</em>(2), 1033–1062. (<a
href="https://doi.org/10.1007/s10845-023-02284-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric variations and uncertainty are generally observed on every manufactured workpiece and have a critical influence on the functional performance of mechanical parts. In computer-aided tolerancing, the Skin Model Shapes framework is recognized as a novel paradigm to embed the expected and observed geometric variations of mechanical products based on discrete geometry representation schemes. Currently, the generation of Skin Model Shapes is still limited due to the lack of knowledge-based parameter settings in the design process, and the consideration of enriched simulation and measurement data. In this paper, a novel method based on two distinct techniques, namely Generative Adversarial Networks (GAN) and Hessian Locally Linear Embedding (HLLE), is proposed to generate Skin Model Shapes without any explicitly defined parameters. A Wasserstein GAN structure is trained for generating patterns of geometric deviations based on simulation data. Geometric deviations on planar and cylindrical surfaces are considered in a training process since both types of surfaces are widely used in mechanical engineering. HLLE is used in the paper to extend the implementation of the proposed deviation mapping process from planar/cylindrical surfaces to other types of surfaces scattered in 3D space. The proposed Skin Model Shapes generation process enables the efficient generation of part representatives with geometric deviations without the need for extensive deviation modeling. Meanwhile, the proposed method overcomes the common limitation of simulating different types (e.g. rotational and freeform) of non-ideal surfaces on Skin Model Shapes. The implemented case studies show that our method can be used to generate hundreds of distinct Skin Model Shapes within seconds while the distributions of simulated geometric deviations on the surfaces are consistent with the measurement results. Meanwhile, the generated Skin Model Shapes can be used for further applications such as assembly simulation and tolerance analysis to obtain more realistic simulation results.},
  archive      = {J_JIM},
  author       = {Qie, Yifan and Schleich, Benjamin and Anwer, Nabil},
  doi          = {10.1007/s10845-023-02284-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1033-1062},
  shortjournal = {J. Intell. Manuf.},
  title        = {Generative adversarial networks and hessian locally linear embedding for geometric variations management in manufacturing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning for steel surface inspection using
magnetic flux leakage signal. <em>JIM</em>, <em>36</em>(2), 1021–1031.
(<a href="https://doi.org/10.1007/s10845-023-02286-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a semi-supervised learning model for detecting multi-defect classification and localization on the steel surface for industries with limited labeled datasets. This study uses 1-D data from magnetic flux leakage (MFL) testing, a powerful and cost-effective nondestructive inspection method for steel bars. Most steel surface defect systems are based on supervised learning classification with 2-D image datasets. However, acquiring labeled datasets for developing supervised learning models is practically limited in the actual steel manufacturing process. Furthermore, due to the frequent occurrence of multiple defect classes on the same steel bar, the problem of multi-defect classification and localization needs to be addressed. Therefore, this paper proposes a steel bar surface inspection system for multi-defect classification and localization based on a semi-supervised learning model and MFL signals. The proposed system solves the multi-defect classification and localization problem by reducing the feature dimension with an autoencoder. Then, it classifies the defects based on the semi-supervised support vector machines that require only a small portion of the labeled dataset. Also, the classification process is repeated on the overlapped small steel section to address the multi-defect classification and localization issue. When it was evaluated on an industry MFL inspection dataset, the accuracy ranged from 81% to 90% when the labeled data ratio varied from 2% to 90%.},
  archive      = {J_JIM},
  author       = {Park, Jae-Eun and Kim, Young-Keun},
  doi          = {10.1007/s10845-023-02286-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1021-1031},
  shortjournal = {J. Intell. Manuf.},
  title        = {Semi-supervised learning for steel surface inspection using magnetic flux leakage signal},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving imbalanced industrial datasets to enhance the
accuracy of mechanical property prediction and process optimization for
strip steel. <em>JIM</em>, <em>36</em>(2), 1003–1020. (<a
href="https://doi.org/10.1007/s10845-023-02275-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of imbalanced regression is widely prevalent in various intelligent manufacturing systems, significantly constraining the industrial application of machine learning models. Existing research has overlooked the impact of redundant data and has lost valuable information within unlabeled data, therefore, the effectiveness of the models is limited. To this end, we propose a novel model framework (sNN-ST, similarity-based nearest neighbor and Self-Training fusion) to address imbalanced regression in industrial big data. This approach comprises two main steps: first, we identify and remove redundant samples by analyzing the redundancy relationships among samples. Then, we perform pseudo-labeling on unlabeled data, selectively incorporating reliable and non-redundant samples into the labeled dataset. We validate the proposed method on two imbalanced regression datasets. Removing redundant data and effectively utilizing unlabeled data optimize the dataset&#39;s distribution and enhance its information entropy. Consequently, the processed dataset significantly improves the overall model performance. We used this model to conduct a Multi-Parameter Global Relative Sensitivity Analysis within a production system. This analysis optimized existing process parameters and improved product quality consistency. This research presents a promising approach to addressing imbalanced regression problems.},
  archive      = {J_JIM},
  author       = {Li, Feifei and He, Anrui and Song, Yong and Shen, Chengzhe and Wang, Fenjia and Yuan, Tieheng and Zhang, Shiwei and Xu, Xiaoqing and Qiang, Yi and Liu, Chao and Liu, Pengfei and Zhao, Qiangguo},
  doi          = {10.1007/s10845-023-02275-1},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {1003-1020},
  shortjournal = {J. Intell. Manuf.},
  title        = {Improving imbalanced industrial datasets to enhance the accuracy of mechanical property prediction and process optimization for strip steel},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A component-based design approach for energy flexibility in
cyber-physical manufacturing systems. <em>JIM</em>, <em>36</em>(2),
975–1001. (<a href="https://doi.org/10.1007/s10845-023-02280-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy flexibility of manufacturing systems helps to meet sustainable manufacturing requirements and is getting significant importance with rising energy prices and noticeable climate changes. Considering the need to proactively enable energy flexibility in modern manufacturing systems, this work presents a component-based design approach that aims to embed energy flexibility in the design of cyber-physical production systems. To this end, energy management using Industry 4.0 technologies (e.g., Internet of Things and Cyber-physical Systems) is compared to the literature on energy flexibility in order to evaluate to what extent the energy flexibility practice takes advantage of Industry 4.0 technologies. Another dimension is the coverage of the life cycle of the manufacturing system which guarantees its sustainable design and the ability to achieve energy flexibility by configuring the energy consumption behaviour. A data-based design approach of the energy-flexible components is proposed in the spirit of the Reference Architectural Model Industrie 4.0 (RAMI 4.0), and then it is exemplified using an electric drive (as a component) in order to show the practical applicability of the approach. The energy consumption behaviour of the component is modelled using machine learning techniques. The digital twin of the studied component is developed using Visual Components virtual engineering environment, then its energy consumption behaviour is included in the model allowing the system integrator/decision-maker to configure the component based on the energy availability/price. Finally, external services in terms of an optimisation module and a deep learning module are connected to the digital twin.},
  archive      = {J_JIM},
  author       = {Assad, Fadi and Rushforth, Emma J. and Harrison, Robert},
  doi          = {10.1007/s10845-023-02280-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {975-1001},
  shortjournal = {J. Intell. Manuf.},
  title        = {A component-based design approach for energy flexibility in cyber-physical manufacturing systems},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EXplainable artificial intelligence for automatic defect
detection in additively manufactured parts using CT scan analysis.
<em>JIM</em>, <em>36</em>(2), 957–974. (<a
href="https://doi.org/10.1007/s10845-023-02272-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive Manufacturing (AM) and in particular has gained significant attention due to its capability to produce complex geometries using various materials, resulting in cost and mass reduction per part. However, metal AM parts often contain internal defects inherent to the manufacturing process. Non-Destructive Testing (NDT), particularly Computed Tomography (CT), is commonly employed for defect analysis. Today adopted standard inspection techniques are costly and time-consuming, therefore an automatic approach is needed. This paper presents a novel eXplainable Artificial Intelligence (XAI) methodology for defect detection and characterization. To classify pixel data from CT images as pores or inclusions, the proposed method utilizes Support Vector Machine (SVM), a supervised machine learning algorithm, trained with an Area Under the Curve (AUC) of 0.94. Density-Based Spatial Clustering with the Application of Noise (DBSCAN) is subsequently applied to cluster the identified pixels into separate defects, and finally, a convex hull is employed to characterize the identified clusters based on their size and shape. The effectiveness of the methodology is evaluated on Ti6Al4V specimens, comparing the results obtained from manual inspection and the ML-based approach with the guidance of a domain expert. This work establishes a foundation for automated defect detection, highlighting the crucial role of XAI in ensuring trust in NDT, thereby offering new possibilities for the evaluation of AM components.},
  archive      = {J_JIM},
  author       = {Bordekar, Harsh and Cersullo, Nicola and Brysch, Marco and Philipp, Jens and Hühne, Christian},
  doi          = {10.1007/s10845-023-02272-4},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {957-974},
  shortjournal = {J. Intell. Manuf.},
  title        = {EXplainable artificial intelligence for automatic defect detection in additively manufactured parts using CT scan analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online tool condition monitoring in micromilling using LSTM.
<em>JIM</em>, <em>36</em>(2), 935–955. (<a
href="https://doi.org/10.1007/s10845-023-02273-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality and cost-effective production in micro-milling involves the use of tools of diameter 50–800 $$\mu $$ m, at high rotational speeds, along complex tool paths. These tools are susceptible to high wear and unexpected breakage, and hence a high-precision tool condition monitoring system is required to predict the tool wear states. In this work, we propose a novel approach for high-precision tool condition monitoring in micro-milling using cutting force signals. The method correlates dominant frequency variations with the tool condition along its complete life cycle, considering both straight and circular tool paths to mimic real-life machining scenarios. Therefore, using multiple micro-milling experiments, dominant frequency was characterized using Wavelet transform and Short Time Fourier Transform, and a tool condition prognostic model was developed using LSTM networks. The model accurately predicts force signals with an RMSE less than 0.09, enabling indirect prediction of the tool condition.},
  archive      = {J_JIM},
  author       = {Manwar, Ashish and Varghese, Alwin and Bagri, Sumant and Joshi, Suhas S.},
  doi          = {10.1007/s10845-023-02273-3},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {935-955},
  shortjournal = {J. Intell. Manuf.},
  title        = {Online tool condition monitoring in micromilling using LSTM},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time monitoring and quality assurance for laser-based
directed energy deposition: Integrating co-axial imaging and
self-supervised deep learning framework. <em>JIM</em>, <em>36</em>(2),
909–933. (<a href="https://doi.org/10.1007/s10845-023-02279-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) has emerged as a promising solution for real-time monitoring of the quality of additively manufactured (AM) metallic parts. This study focuses on the Laser-based Directed Energy Deposition (L-DED) process and utilizes embedded vision systems to capture critical melt pool characteristics for continuous monitoring. Two self-learning frameworks based on Convolutional Neural Networks and Transformer architecture are applied to process zone images from different DED process regimes, enabling in-situ monitoring without ground truth information. The evaluation is based on a dataset of process zone images obtained during the deposition of titanium powder (Cp-Ti, grade 1), forming a cube geometry using four laser regimes. By training and evaluating the Deep Learning (DL) algorithms using a co-axially mounted Charged Couple Device (CCD) camera within the process zone, the down-sampled representations of process zone images are effectively used with conventional classifiers for L-DED process monitoring. The high classification accuracies achieved validate the feasibility and efficacy of self-learning strategies in real-time quality assessment of AM. This study highlights the potential of AI-based monitoring systems and self-learning algorithms in quantifying the quality of AM metallic parts during fabrication. The integration of embedded vision systems and self-learning algorithms presents a novel contribution, particularly in the context of the L-DED process. The findings open avenues for further research and development in AM process monitoring, emphasizing the importance of self-supervised in situ monitoring techniques in ensuring part quality during fabrication.},
  archive      = {J_JIM},
  author       = {Pandiyan, Vigneashwara and Cui, Di and Richter, Roland Axel and Parrilli, Annapaola and Leparoux, Marc},
  doi          = {10.1007/s10845-023-02279-x},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {909-933},
  shortjournal = {J. Intell. Manuf.},
  title        = {Real-time monitoring and quality assurance for laser-based directed energy deposition: Integrating co-axial imaging and self-supervised deep learning framework},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-processing of powder bed fused stainless steel:
Micro-machining and micro-electrical discharge machining. <em>JIM</em>,
<em>36</em>(2), 897–908. (<a
href="https://doi.org/10.1007/s10845-023-02277-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface quality is often a specific requirement when dealing with Additive Manufacturing and dimensional accuracy, especially during 3D printing of metals. Therefore, it is crucial to evaluate the material removal behavior of Powder Bed Fusion specimens during Micro-Mechanical Machining and Micro-Electrical Discharge Machining procedures to detect their machinability responses. In this paper, micro-machining of 17-4PH stainless steel samples produced by Laser Powder Bed Fusion is reported. Specifically, we performed Micro-Mechanical Machining and Micro-Electrical Discharge Machining operations, analysed the process performances, and compared the machining conditions. Additionally, we investigated the surface roughness and burrs extension as a function of geometrical configuration and process parameters. The outcomes of this work regard the processing conditions and parameters to optimize the machining operations on 3D-printed metal samples. This work allows the identification of specific features resulting from each process in relation to the material removal rate, giving the possibility to compare and evaluate the machining conditions for 3D parts post-processing.},
  archive      = {J_JIM},
  author       = {Abeni, Andrea and Quarto, Mariangela and Ginestra, Paola Serena},
  doi          = {10.1007/s10845-023-02277-z},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {897-908},
  shortjournal = {J. Intell. Manuf.},
  title        = {Post-processing of powder bed fused stainless steel: Micro-machining and micro-electrical discharge machining},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural-network-based automatic trajectory adaptation for
quality characteristics control in powder compaction. <em>JIM</em>,
<em>36</em>(2), 875–895. (<a
href="https://doi.org/10.1007/s10845-023-02274-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future manufacturing systems will have to become more intelligent to be able to guarantee a constantly high quality of products while simultaneously reducing labor-intensive quality-assurance tasks to address the shortage in workforce. In this work, we study the application of neural networks to the field of powder metallurgy and more specifically the production of green parts as part of a typical sintering process. More specifically, we explore the usage of neural-network-based predictions in closed-loop control. We train neural networks based on a series of produced workpieces, and use these networks in closed-loop production to predict quality characteristics like weight and dimensions of the workpiece in real-time. Based on these predictions an adaptive trajectory planner adjusts then trajectory key points and with this the final piston trajectories to bring and keep quality characteristics of workpieces within tolerance. We finally compare the control performance of this neural network-based approach with a pure sensor-based approach. Results indicate that both approaches are able to bring and keep quality characteristics within their tolerance limits, but that the neural network-based approach outperforms the sensor-based approach in the transient phase, whereas in steady state the neural network needed to be updated from time to time to reach the same high performance as the sensor-based approach. Since updating needs to be performed only from time to time, required expensive sensors can be shared among multiple machines and thus, costs can be reduced. At the same time the superior prediction performance of the neural-network-based approach in transient phases can be exploited to accelerate setting up times for new workpieces. Future work will target the automation of the recording of the training dataset, the exploration of further machine learning methods as well as the integration of additional sensor data to further improve predictions.},
  archive      = {J_JIM},
  author       = {MoradiMaryamnegari, Hoomaan and Hasseni, Seif-El-Islam and Ganthaler, Elias and Villgrattner, Thomas and Peer, Angelika},
  doi          = {10.1007/s10845-023-02274-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {875-895},
  shortjournal = {J. Intell. Manuf.},
  title        = {Neural-network-based automatic trajectory adaptation for quality characteristics control in powder compaction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study on anti-interference detection of machining surface
defects under the influence of complex environment. <em>JIM</em>,
<em>36</em>(2), 853–874. (<a
href="https://doi.org/10.1007/s10845-023-02276-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When detecting surface defects in a complex industrial cutting environment, the defects are easily polluted and covered by interfering factors (chips or coolant residues). The defect of the surface images with interference factors is a novel problem in the existing studies, and it is also a difficulty in the detection field. Hence, this paper proposes a high-precision anti-interference detection method for surface defects under the influence of complex environment. The detection method provides a new research idea, which is divided into three main processes: interference regions location, interference regions repair, defect detection. The regions affected by interference factors are adaptively located through the proposed Efficient Channel Attention Network (ECANet)-DeeplabV3 + network model. The mean Pixel Accuracy (mPA) and mean Intersection over Union (mIoU) of ECANet-DeeplabV3 + network model for interference factor identification are 98.37% and 95.46%, respectively. The Criminisi algorithm is improved from priority, finding the best matching block, and searching regions. Directional repair based on the improved Criminisi algorithm is performed on the identified interfering regions removing the interfering factors in the image, which is the research core. Then, defect detection is performed on the repaired image using the improved superpixel technology. At the same time, the defect detection results provide a variety of surface defect information for the cutting staff, including defect types, the number of pixels in different defect regions, and the area ratio of different defect regions. This information improves predictive maintenance and surface quality control.},
  archive      = {J_JIM},
  author       = {Chen, Wei and Zou, Bin and Lei, Ting and Zheng, Qinbing and Huang, Chuanzhen and Li, Lei and Liu, Jikai},
  doi          = {10.1007/s10845-023-02276-0},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {853-874},
  shortjournal = {J. Intell. Manuf.},
  title        = {Study on anti-interference detection of machining surface defects under the influence of complex environment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-planar slicing for filled free-form geometries in
robot-based FDM. <em>JIM</em>, <em>36</em>(2), 833–851. (<a
href="https://doi.org/10.1007/s10845-023-02250-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-axis techniques in Additive Manufacturing (AM) unlock promising features such as supportless fabrication, reduced material consumption, improved surface quality and mechanical properties. Among those techniques, non-planar (NP) slicing promises to be the most suitable approach to fabricate 3D-components with significant curvature such as free-form geometries. Those are characterized by a layer thickness variation (LTV) along the curvature, which should be minimized. Industrial 6-axis robots are mandatory to achieve such performances. NP slicing generalization is challenging. On one side, there is a need to define a suitable contouring method compatible with the different geometrical features present in objects. On the other side, the generalized slicing method must be able to reconstruct the inner side not provided by superficial information provided by triangular mesh. In this work, a new algorithm to generate filled NP layer has been proposed using a contouring method that reduces the LTV. The bidirectional rectilinear infill strategy has been adapted for NP layers providing a more feasible toolpath to Fused Deposition Modeling (FDM) and such Direct processes where curved paths are detrimental. The proposed strategy has been validated by fabricating a tubular geometry with a robotized FDM system. Tubular geometry provides a sub-optimal solution of LTV known analytically for the contour. The infill algorithm has been tested with a complex surface applying the NP torus on a waved shape. Previous studies consider only contour providing an LTV ranging in +0% $$\div $$ -46%. This study considers only the inner side. The analytical LTV resulted in a range of +0% $$\div $$ -60%. The cross sections of the components were analyzed and compared with the analytical results. Although the proposed infill strategy does not maintain completely the contour layer thickness in the infill side, it shows to be able to cover more complex NP layers without saddle points.},
  archive      = {J_JIM},
  author       = {Insero, Federico and Furlan, Valentina and Giberti, Hermes},
  doi          = {10.1007/s10845-023-02250-w},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {833-851},
  shortjournal = {J. Intell. Manuf.},
  title        = {Non-planar slicing for filled free-form geometries in robot-based FDM},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin for product versus project lifecycles’
development in manufacturing and construction industries. <em>JIM</em>,
<em>36</em>(2), 801–831. (<a
href="https://doi.org/10.1007/s10845-023-02301-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin, as an important enabling tool for digital transformation, has received increasing attention from researchers and practitioners since its definition was formalised. Especially in the global context and exacerbated by Covid-19, the applications of the digital twin have offered opportunities for many industries. While the digital twin has already been widely used in many sectors such as manufacturing and the construction industry—one of the key engines of economic development, is still lagging behind many other sectors. This study uses the systematic literature review to assess the applications of digital twin in manufacturing and construction respectively, the benefits it brings, and the impediments to its application. Based on this, a comparison is made of digital twin applications in the manufacturing and construction industries to draw lessons. This study concluded that although the use of digital twin in manufacturing is better than construction overall, it is still not reaching its full potential. Despite many benefits brought by the digital twin to construction during the project lifecycle, the construction sector faces even greater challenges than manufacturing in digital twin adoption. By comparison, this study drew five lessons to drive better adoption of the digital twin. The construction industry needs to accelerate the deployment of relevant hardware, promote the standard unification of digital twin, explore the whole lifecycle application of the digital twin, enhance data protection, and embrace changes. This study was limited in the scope of data collection. Future research could focus on gathering information from specific case studies, to produce more comprehensive perspectives.},
  archive      = {J_JIM},
  author       = {Abanda, F. H. and Jian, N. and Adukpo, S. and Tuhaise, V. V. and Manjia, M. B.},
  doi          = {10.1007/s10845-023-02301-2},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {801-831},
  shortjournal = {J. Intell. Manuf.},
  title        = {Digital twin for product versus project lifecycles’ development in manufacturing and construction industries},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic comparison of software agents and digital twins:
Differences, similarities, and synergies in industrial production.
<em>JIM</em>, <em>36</em>(2), 765–800. (<a
href="https://doi.org/10.1007/s10845-023-02278-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve a highly agile and flexible production, a transformational shift is envisioned whereby industrial production systems evolve to be more decentralized, interconnected, and intelligent. Within this vision, production assets collaborate with each other, exhibiting a high degree of autonomy. Furthermore, information about individual production assets is accessible throughout their entire life-cycles. To realize this vision, the use of advanced information technology is required. Two commonly applied software paradigms in this context are Software Agents (referred to as Agents) and Digital Twins (DTs). This work presents a systematic comparison of Agents and DTs in industrial applications. The goal of the study is to determine the differences, similarities, and potential synergies between the two paradigms. The comparison is based on the purposes for which Agents and DTs are applied, the properties and capabilities exhibited by these software paradigms, and how they can be allocated within the Reference Architecture Model Industry 4.0. The comparison reveals that Agents are commonly employed in the collaborative planning and execution of production processes, while DTs are generally more applied to monitor production resources and process information. Although these observations imply characteristic sets of capabilities and properties for both Agents and DTs, a clear and definitive distinction between the two paradigms cannot be made. Instead, the analysis indicates that production assets utilizing a combination of Agents and DTs would demonstrate high degrees of intelligence, autonomy, sociability, and fidelity. To achieve this, further standardization is required, particularly in the field of DTs.},
  archive      = {J_JIM},
  author       = {Reinpold, Lasse M. and Wagner, Lukas P. and Gehlhoff, Felix and Ramonat, Malte and Kilthau, Maximilian and Gill, Milapji S. and Reif, Jonathan T. and Henkel, Vincent and Scholz, Lena and Fay, Alexander},
  doi          = {10.1007/s10845-023-02278-y},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {765-800},
  shortjournal = {J. Intell. Manuf.},
  title        = {Systematic comparison of software agents and digital twins: Differences, similarities, and synergies in industrial production},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of cutting force via machine learning: State of
the art, challenges and potentials. <em>JIM</em>, <em>36</em>(2),
703–764. (<a href="https://doi.org/10.1007/s10845-023-02260-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cutting force is a critical factor that reflects the machining states and affects tool wear, cutting stability, and the quality of the machined surface. Accurate prediction of cutting force has been the subject of extensive research in machining technology for decades. Generally, the predicting methods are based on the physical principles of metal cutting processes and they can be divided into two main categories: calculation of cutting forces by using analytical models and numerical simulation of cutting forces with finite element analysis. With the advance of artificial intelligence and machine learning (ML), various algorithms have been developed to predict cutting force with high accuracy and high efficiency. This paper provides a comprehensive review of force prediction methods, with a focus on ML-based algorithms. The mechanisms and characteristics of various force prediction methods, such as analytical models and finite element analysis, as well as different ML-based algorithms, are introduced in detail. The challenges of current algorithms and their potential in long-term and real-time prediction are discussed. The review highlights the potential of ML-based algorithms in improving the accuracy and efficiency of cutting force prediction and emphasizes the need for further research to address the current challenges and advance the field of force prediction in metal-cutting processes.},
  archive      = {J_JIM},
  author       = {Liu, Meng and Xie, Hui and Pan, Wencheng and Ding, Songlin and Li, Guangxian},
  doi          = {10.1007/s10845-023-02260-8},
  journal      = {Journal of Intelligent Manufacturing},
  month        = {2},
  number       = {2},
  pages        = {703-764},
  shortjournal = {J. Intell. Manuf.},
  title        = {Prediction of cutting force via machine learning: State of the art, challenges and potentials},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jirs---42">JIRS - 42</h2>
<ul>
<li><details>
<summary>
(2025). Monocular depth estimation applied to global localization
over 2D floor plans using free space density. <em>JIRS</em>,
<em>111</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10846-024-02131-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor global localization is a critical aspect of autonomous robotic navigation. The increasing demand for service consumer-grade robots that require self-localization calls for research on methods that work with easy setup and low-cost sensors. In this paper, we propose a monocular camera-based localization of a motorized wheeled robot using a 2D floor plan as a reference map. The innovation of our method lies in using depth maps estimated from monocular images to compute the free space around the robot to be used as a measurement model in a particle filter strategy. The estimated free space density is compared to the free space density extracted from particles in the 2D floor plan. Due to the inherent imperfections of estimated depth maps, we also propose a new particle weighting approach to account for uncertainties in the depth estimation from the monocular camera. Experiments performed using real-world scenario sequences of images comparing the proposed method with RGB-D camera-based approaches demonstrate the effectiveness of the method, even for imperfect depth maps obtained with the monocular depth estimation model.},
  archive      = {J_JIRS},
  author       = {Lopes, Cristian and Maffei, Renan and Kolberg, Mariana},
  doi          = {10.1007/s10846-024-02131-1},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Monocular depth estimation applied to global localization over 2D floor plans using free space density},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human factors and AI in UAV systems: Enhancing operational
efficiency through AHP and real-time physiological monitoring.
<em>JIRS</em>, <em>111</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10846-024-02188-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Artificial Intelligence (AI) into Unmanned Aerial Vehicle (UAV) operations has advanced efficiency, safety, and decision-making. This study addresses critical gaps in UAV methods, including insufficient integration of human factors, operator variability, and the lack of systematic error analysis. To overcome these challenges, a novel approach combines the Analytic Hierarchy Process (AHP) with three core human factors models: the Observe-Orient-Decide-Act (OODA) loop, the Human Factors Analysis and Classification System (HFACS), and the SHELL model. An online survey was conducted across diverse UAV operator groups to prioritize critical factors within each model. Additionally, real-time monitoring of heart rate (HR), heart rate variability (HRV), and respiratory rate (RR) was conducted during UAV operations at various automation levels with different experience levels. Visualization through boxplots and percentage change matrices provided insights into operator stress and workload across automation levels. Integrating AHP findings and physiological data revealed significant differences in operator prioritization, highlighting the need for tailored AI-UAV strategies. This research combines survey data with real-time physiological monitoring, offering visions into optimizing human-AI interaction in UAV operations and providing a foundation for improving AI integration and operator strategies.},
  archive      = {J_JIRS},
  author       = {Alharasees, Omar and Kale, Utku},
  doi          = {10.1007/s10846-024-02188-y},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-34},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Human factors and AI in UAV systems: Enhancing operational efficiency through AHP and real-time physiological monitoring},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and dynamic analysis of fish robot with soft
fluidic actuation. <em>JIRS</em>, <em>111</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10846-024-02196-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A biomimetic robotic fish tailored for environmental monitoring and hydrographic exploration is examined in this study. Traditional fish robots, driven by tail oscillations, often grapple with constrained maneuverability in a single plane. This article focuses on mitigating these limitations through an exploration of the dynamic behavior of a bioinspired soft robotic fish. Unlike prior designs that employed similar actuators for the robot&#39;s tail, which either lacked the ability for out-of-plane motion or relied on other propulsion systems, the single propulsion design proposed in our model enables movement along three-dimensional trajectories, leading to improved efficiency and maneuverability due to tail oscillation dynamics. The proposed design integrates strategically positioned nozzles for out-of-plane movements, alongside parallel fluid channels on the tail&#39;s neutral plate. Actuation is achieved by manipulating the internal fluid pressure within these channels. To precisely model tail deflection, we introduce a novel method utilizing Euler–Bernoulli beam theory considering nonlinear characteristics arising from internal fluid stress. For instance, following the proposed approximate analytical method, we optimize the fluidic actuator, considering that the soft tail deformation increases by 65% as the channel shape transitions from a semicircular to a square cross-section. The comprehensive comparison with analytical nonlinear method, finite element method, and experimental-driven analytical method extends our approach as an effective tool in terms of accuracy and computation time, demonstrating the effectiveness of validation processes. This study unveils a simplified and robust fish robot design, outperforming traditional mechanisms in efficacy. Despite its simplicity, the proposed design delivers comparable performance, presenting an effective alternative for achieving requisite functionality in fish robots.},
  archive      = {J_JIRS},
  author       = {Bamdad, Mahdi and Karimi, Ahmad and Sina, Seyedali and Cruz, Francisco},
  doi          = {10.1007/s10846-024-02196-y},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Modeling and dynamic analysis of fish robot with soft fluidic actuation},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path planning for the robotic manipulator in dynamic
environments based on a deep reinforcement learning method.
<em>JIRS</em>, <em>111</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10846-024-02205-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative and autonomous robots are increasingly important in meeting the demands of a faster and more cost-effective market. To ensure production efficiency and safety, robots must swiftly respond to the presence of human operators or other dynamic obstacles, avoiding potential collisions by quickly planning alternative paths. Deep Reinforcement Learning (DRL) based methods have shown great potential in path planning due to their rapid response capabilities. However, existing DRL-based planners lack a safety verification system to evaluate the feasibility of actions generated by neural models, and they cannot guarantee 100% collision-free paths. This paper presents an enhanced DRL-based path planning system incorporating a robust safety verification mechanism. This system predicts potential collisions and generates alternative collision-free paths as necessary. We analyzed the essential elements of trajectory planning using the DRL method and proposed improvements to accelerate planning speed. The results demonstrate that our planner consistently generates paths for typical reaching tasks with an average planning time of 12.1 ms, a notable improvement over traditional algorithms. Moreover, the paths produced by our method are nearly optimal, akin to those generated by Optimization-based algorithms.},
  archive      = {J_JIRS},
  author       = {Liu, Jie and Yap, Hwa Jen and Khairuddin, Anis Salwa Mohd},
  doi          = {10.1007/s10846-024-02205-0},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Path planning for the robotic manipulator in dynamic environments based on a deep reinforcement learning method},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). JIRS editorial, 4th quarter 2024. <em>JIRS</em>,
<em>111</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s10846-024-02208-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JIRS},
  author       = {Valavanis, Kimon P.},
  doi          = {10.1007/s10846-024-02208-x},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-2},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {JIRS editorial, 4th quarter 2024},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive velocity control for UAV boat landing: A neural
network and particle swarm optimization approach. <em>JIRS</em>,
<em>111</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10846-024-02201-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving precise landing of Unmanned Aerial Vehicles (UAVs) onto moving platforms, such as Autonomous Surface Vehicles (ASVs), is challenging, particularly in GPS-denied environments with dynamic disturbances. Conventional methods often rely on high-level waypoint navigation, extensive manual tuning, and expensive sensors. In this work, we propose an adaptive Proportional-Integral-Derivative (PID) controller optimization using a Neural Network-Particle Swarm Optimization (NN-PSO) algorithm. The algorithm dynamically tunes the PID controller, significantly reducing manual tuning effort, while relying solely on a low-cost camera and altitude sensor. The NN-PSO algorithm allows the UAV to land with an average error of 5 cm on static platforms and 10 cm on moving boats, based on multiple test flights. Our method also increases the maximum landing speed to 80.9% of the UAV’s top flight speed, a considerable improvement over existing systems. Our approach not only optimizes landing precision but also introduces techniques for ensuring soft landings, reducing oscillations, and preventing target misses. These enhancements make the method robust across varying flight altitudes and ASV speeds. Furthermore, this approach is applicable to a variety of GPS-denied scenarios, including rescue missions, package deliveries, and workspace inspections, without requiring costly equipment or extensive parameter tuning. Field experiments confirm the precision and stability of the proposed system, validating its performance in real-world conditions. [Video]},
  archive      = {J_JIRS},
  author       = {Wu, Li-Fan and Wang, Zihan and Rastgaar, Mo and Mahmoudian, Nina},
  doi          = {10.1007/s10846-024-02201-4},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Adaptive velocity control for UAV boat landing: A neural network and particle swarm optimization approach},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe multi-agent reinforcement learning via approximate
hamilton-jacobi reachability. <em>JIRS</em>, <em>111</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10846-024-02156-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) promises to address the challenges of cooperation and competition among multiple agents, often involving safety-critical scenarios. However, realizing safe MARL remains a domain of limited progress. Current works extend single-agent safe learning approaches, employing shielding or backup policies to ensure safety satisfaction. Nevertheless, these approaches require good cooperation among multiple agents, and weakly distributed approaches with centralized shielding become infeasible when agents encounter complex situations such as non-cooperative agents and coordination failures. In this paper, we integrate the Hamilton-Jacobi (HJ) reachability theory and present a Centralized Training and Decentralized Execution (CTDE) framework for Safe MARL. Our framework enables the learning of safety policies without the need for system model or shielding layer pre-training. Additionally, we enhance adaptability to varying levels of cooperation through a conservative approximation estimation of the value function. Experimental results validate the efficacy of our proposed method, demonstrating its ability to ensure safety while successfully achieving target tasks under cooperative conditions. Furthermore, our approach exhibits robustness in the face of non-cooperative behaviors induced by complex disturbance factors.},
  archive      = {J_JIRS},
  author       = {Zhu, Kai and Lan, Fengbo and Zhao, Wenbo and Zhang, Tao},
  doi          = {10.1007/s10846-024-02156-6},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Safe multi-agent reinforcement learning via approximate hamilton-jacobi reachability},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RDynaSLAM: Fusing 4D radar point clouds to visual SLAM in
dynamic environments. <em>JIRS</em>, <em>111</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10846-024-02204-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of visual SLAM systems, in terms of both robustness and accuracy, can be affected by the presence of dynamic objects in dynamic environments. The utilization of learning-based dynamic SLAM algorithms introduces additional challenges, such as increased power consumption and computing requirements, particularly on mobile platforms. Millimeter wave radar has the capability to directly detect and measure the relative velocity between objects and the radar system. Therefore, this paper presents a novel SLAM system that aims to integrate millimeter wave radar point clouds into visual SLAM in a dynamic environment. First, a real-time dynamic cluster extraction method was developed using Doppler information obtained from 4D radar. It effectively distinguishes between static background points and dynamic points by employing the RANSAC algorithm. The dynamic radar points are subsequently grouped together to create dynamic clusters. Then, the clusters are projected onto the image and expand to produce dynamic masks, taking into account the distribution characteristics. Finally, dynamic masks are employed to eliminate dynamic keypoints during the camera pose estimation, allowing for the estimation to be based solely on static keypoints. Experiments conducted in various daily dynamic scenarios have demonstrated the robustness of RDynaSLAM in operating effectively within dynamic environments. In comparison to ORBSLAM3, RDynaSLAM exhibits a notable reduction in the Root Mean Square Error (RMSE) of Absolute Pose Error (APE) and Relative Pose Error (RPE) in high dynamic environments. The method proposed in this paper has the capability to operate in real-time, without the need for GPU utilization.},
  archive      = {J_JIRS},
  author       = {Zhu, Dongying and Yang, Guanci},
  doi          = {10.1007/s10846-024-02204-1},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {RDynaSLAM: Fusing 4D radar point clouds to visual SLAM in dynamic environments},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of unmanned vehicle control with adaptive dynamic
programming implementations. <em>JIRS</em>, <em>111</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10846-024-02207-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this review, the optimal control designs via adaptive dynamic programming (ADP) of unmanned vehicles are investigated. Various complex tasks in unmanned systems are addressed as fundamental optimal regulation and tracking control problems related to the position and attitude of vehicles. The optimal control can be obtained by solving the Hamilton-Jacobi-Bellman equation using ADP-based control methods. Neural network implementations and policy iterative ADP algorithms are common approaches in ADP-based control methods, enabling online updates and partially model-free control for unmanned vehicles with various structures. For complexities and uncertain disturbances in unmanned vehicle dynamics, robust ADP-based control methods are proposed, including robust ADP control for matched and unmatched uncertainties, robust guaranteed cost control with ADP, and ADP-based $$H_\infty $$ control. In order to reduce communication and computational costs in unmanned vehicle operations, a preliminary discussion on event-triggered optimal control using ADP-based control methods is presented.},
  archive      = {J_JIRS},
  author       = {Liu, Hao and Yi, Xinning and Liu, Deyuan and Valavanis, Kimon P.},
  doi          = {10.1007/s10846-024-02207-y},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {A review of unmanned vehicle control with adaptive dynamic programming implementations},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous drone detection and classification using computer
vision and prony algorithm-based frequency feature extraction.
<em>JIRS</em>, <em>111</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10846-024-02216-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a practical and automated system for high-accuracy drone detection and classification using acoustic signals. Our approach leverages a novel frequency feature extraction method based on the Prony algorithm, which enables efficient detection and classification of drones. To assess the effectiveness of our proposed method, we conducted experiments on a new suitable database of recorded drone audio in natural environments and under different conditions, which we meticulously prepared. Furthermore, we compared the performance of our proposed method against conventional audio features, such as Mel Frequency Cepstral Coefficients (MFCCs), Gamma Tone Cepstral Coefficients (GTCCs), and Fast Fourier Transform (FFT). Our experimental results demonstrate that our proposed method achieves a remarkable accuracy of more than 97.7% for detection and 93.6% for classification, outperforming traditional audio features, which provide less than 80% accuracy for classification. This classification accuracy is crucial for designing a suitable system to manage drones. Additionally, we highlight the crucial advantage and efficiency of our proposed method for an unknown drone, which is particularly valuable in practical applications where the drone type is not known in advance. Notably, our method also exhibits suitable time for drone detection in practical applications, making it an effective solution for real-world scenarios.},
  archive      = {J_JIRS},
  author       = {Najafi, Jafar and Mirzakuchaki, Sattar and Shamaghdari, Saeed},
  doi          = {10.1007/s10846-024-02216-x},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Autonomous drone detection and classification using computer vision and prony algorithm-based frequency feature extraction},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive tracking control for a class of uncertain MIMO
nonlinear systems with input constraints. <em>JIRS</em>,
<em>111</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10846-024-02218-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of adaptive tracking control for a class of uncertain multi-input and multi-output nonlinear systems in the presence of asymmetric input constraints and external disturbance. In order to address the different action ranges of input signals in asymmetric dead-zone and saturation models, an adaptive backstepping control method related to asymmetric parameters is designed. A state-dependent upper bound of uncertainty is proposed instead of a constant upper bound. This avoids the problem of state constraints caused by the boundness of uncertainty before obtaining closed-loop characteristics. Certain positive results are emerged in this paper where an innovative adaptive control methodology is demonstrated to cope with system uncertainty. The proposed controller does not require a priori knowledge on the bound of them. By means of the Lyapunov stability theory, the close-loop system is proven to be uniformly ultimately bounded, the system states converge to a domain containing the origin, and the output tracks the reference signal commendably. Simulation examples are presented to show the effectiveness of the proposed control method.},
  archive      = {J_JIRS},
  author       = {Feng, Xingkai and Wang, Congqing},
  doi          = {10.1007/s10846-024-02218-9},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Adaptive tracking control for a class of uncertain MIMO nonlinear systems with input constraints},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nezha-d: Dynamic characteristics and design of a ducted
HAUV. <em>JIRS</em>, <em>111</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10846-024-02133-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid aerial underwater vehicles (HAUVs) can operate in water and air and are qualified for complex missions on the air-water interfaces. The unique working conditions put forward higher requirements for the propulsion systems of the vehicles. Present HAUVs’ propulsion systems mostly use propellers, but normal aerial propellers work inefficiently underwater, constraining the underwater applications of HAUVs. Besides, in-depth research of the thrusters, especially during the water-crossing process, is limited. These are two major factors that affect the further development of HAUVs. This paper aims to evaluate the feasibility of the ducted fan propulsion system and design a ducted HAUV with better underwater working capacity. Ducted fans have promising applications on HAUVs for higher underwater efficiency, but the outer ducts can lead to significant thrust loss during the water-crossing process. A novel experimental platform is firstly developed with the capability of collecting dynamic data of the thruster during water- air transition. The new water-crossing strategy and overall design of the HAUV are proposed based on the test results. The ducted HAUV is supposed to accelerate underwater and rush out with a certain speed to overcome thrust loss. A centroid adjustment mechanism has also been designed to realize the underwater motion switch for better efficiency. A prototype named “Nezha-D” is fabricated, and outfield tests are conducted to verify the feasibility of the new design.},
  archive      = {J_JIRS},
  author       = {Xie, Hongfei and Jin, Yufei and Bi, Yuanbo and Zeng, Zheng},
  doi          = {10.1007/s10846-024-02133-z},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Nezha-D: Dynamic characteristics and design of a ducted HAUV},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative multi-agent planning framework for fuel
constrained UAV-UGV routing problem. <em>JIRS</em>, <em>111</em>(1),
1–17. (<a href="https://doi.org/10.1007/s10846-024-02209-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs), adept at aerial surveillance, are often constrained by their limited battery capacity. Refueling on slow-moving Unmanned Ground Vehicles (UGVs) can significantly enhance UAVs’ operational endurance. This paper explores the computationally complex problem of cooperative UAV-UGV routing for vast area surveillance, considering speed and fuel constraints. It presents a sequential multi-agent planning framework aimed at achieving feasible and optimally satisfactory solutions. By considering the UAV fuel limit and utilizing a minimum set cover algorithm, we determine UGV refueling stops. This, in turn, facilitates UGV route planning as the first step. Through a task allocation technique and energy-constrained vehicle routing problem modeling with time windows (E-VRPTW), we then achieve the UAV route in the second step of the framework. The effectiveness of our multi-agent strategy is demonstrated through the implementation on 30 different task scenarios across three different scales. This work provides significant insight into the collaborative advantages of UAV-UGV systems and introduces heuristic approaches to bypass computational challenges and swiftly reach high-quality solutions.},
  archive      = {J_JIRS},
  author       = {Mondal, Md Safwan and Ramasamy, Subramanian and Humann, James D. and Dotterweich, James M. and Reddinger, Jean-Paul F. and Childers, Marshal A. and Bhounsule, Pranav A.},
  doi          = {10.1007/s10846-024-02209-w},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Cooperative multi-agent planning framework for fuel constrained UAV-UGV routing problem},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent saturation power limit load distribution
algorithm (ISPLLDA) for cooperative manipulators applications.
<em>JIRS</em>, <em>111</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10846-024-02210-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative manipulators face challenges related to an inadequate distribution of external loads and a decrease in Dynamic Load Carrying Capacity (DLCC). Understanding the impact of optimal load distribution on power consumption, load carrying capacity, and gripper error is crucial. This paper presents the Intelligent Saturation Power Limit Load Distribution Algorithm (ISPLLDA), a novel method that achieves optimal external load distribution. ISPLLDA dynamically distributes the external load among manipulators based on torque-bearing capacity and actuator position. Additionally, nonlinearity in system dynamics introduces uncertainties, leading to incorrect DLCC evaluation, increased error, and higher actuator power consumption. To address this, a Radial Basis Function Neural Network (RBFNN) accurately determines actuator saturation limits in the presence of uncertainty, enabling correct estimation of system dynamics and external disturbances. ISPLLDA ensures near-simultaneous saturation of all manipulators&#39; actuators, maximizing their capacity utilization. The proposed method is validated through simulations and experimental tests on cooperative manipulators. Results demonstrate a 17% increase in load-carrying capacity, as well as more than 35% improvement in error and torque indexes compared to the Lagrange multipliers method.},
  archive      = {J_JIRS},
  author       = {Korayem, Moharam Habibnejad and Kia, Ali Parsai and Lademakhi, Naeim Yousefi},
  doi          = {10.1007/s10846-024-02210-3},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Intelligent saturation power limit load distribution algorithm (ISPLLDA) for cooperative manipulators applications},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance comparison of ROS2 middlewares for multi-robot
mesh networks in planetary exploration. <em>JIRS</em>, <em>111</em>(1),
1–20. (<a href="https://doi.org/10.1007/s10846-024-02211-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Multi-Robot Systems (MRS) and mesh network technologies pave the way for innovative approaches to explore extreme environments. The Artemis Accords, a series of international agreements, have further catalyzed this progress by fostering cooperation in space exploration, emphasizing the use of cutting-edge technologies. In parallel, companies across various sectors’ widespread adoption of the Robot Operating System 2 (ROS 2) underscores its robustness and versatility. This paper evaluates the performances of available ROS 2 MiddleWare (RMW), such as FastRTPS, CycloneDDS and Zenoh, over a mesh network with a dynamic topology. The final choice of RMW is determined by the one that would most fit the scenario: an exploration of the extreme extra-terrestrial environment using a Multi-Robot Systems (MRS). The conducted study in a real environment highlights Zenoh as a potential solution for future applications, showing a reduced delay, reachability, data overhead and CPU usage while being competitive on the RAM usage over a dynamic mesh topology.},
  archive      = {J_JIRS},
  author       = {Chovet, Loïck Pierre and Garcia, Gabriel Manuel and Bera, Abhishek and Richard, Antoine and Yoshida, Kazuya and Olivares-Mendez, Miguel Angel},
  doi          = {10.1007/s10846-024-02211-2},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Performance comparison of ROS2 middlewares for multi-robot mesh networks in planetary exploration},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards planning urban air mobility (UAM) landing
trajectories in emergencies. <em>JIRS</em>, <em>111</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10846-024-02213-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ground transportation in dense urban environments has been facing challenges for many years (e.g., congestion and resilience) and the problem of congestion in urban environments will become more significant with the growth of populations and urbanization. In the past few years, the industry and the scientific communities have invested resources towards creating new ideas to improve urban transportation performance, such as the Urban Air Mobility (UAM) concept. Therefore, emergencies are considered a pivotal aspect to be managed appropriately to ensure safe UAM operations. Although normal operations represent a challenge nowadays (e.g., performance and social acceptance), emergencies are even more challenging due to the safety-critical risks. Thereupon, the main goal of this research is to propose the Landing Trajectory Planner for Emergencies in UAM Operations (LTPE), using Parallel Metaheuristics and considering autonomous vehicles’ presence. This trajectory planning method aims to design landing trajectories for multiple Electrical Vertical Take-off and Landing (eVTOL) vehicles in normal conditions and in emergencies. LTPE considers different eVTOL configurations in piloting systems (i.e., piloted vehicles, remotely piloted vehicles, and fully autonomous vehicles) as well as different vehicle priorities. Three landing modes are used for vehicles in emergency conditions: (i) land at the originally designed skyport; (ii) land at the nearest skyport; and (iii) land on the ground) and all metaheuristics include an early stopping feature. The experiments showed that LTPE can propose safe and efficient solutions for several scenarios with a short response time.},
  archive      = {J_JIRS},
  author       = {Pinto Neto, Euclides Carlos and Baum, Derick and Almeida Jr., Jorge Rady de and Camargo Jr., João Batista and Cugnasca, Paulo Sergio},
  doi          = {10.1007/s10846-024-02213-0},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Towards planning urban air mobility (UAM) landing trajectories in emergencies},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Planning aggressive drone manoeuvres: A geometric backwards
integration approach. <em>JIRS</em>, <em>111</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10846-024-02214-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of performing aggressive manoeuvres by using multirotor vehicles that include passing through any specific point within the full state space of the vehicle. To this end, the design of optimal trajectories considers the dynamical model of the vehicles by numerically integrating it backwards in time, in the manifold where the dynamics evolve, and dividing the manoeuvres into three distinct phases to accommodate any combination of initial, desired, and final states. In the first phase, the vehicles fly from an initial to a launch configuration to achieve the necessary momenta to reach the desired one in the second phase. To ensure the feasibility of executing the second phase, the relation between snap and body torques is exploited by commanding the vehicles to track geodesic curves on SO(3) during the backwards integration. The vehicles are then driven to a final configuration in the third phase. Most existing solutions to execute aggressive and precise manoeuvres with these rotorcraft focus either on the attitude control problem, leaving the position in open-loop, or use different controllers for different sections of the manoeuvre. In this work, a single tracking controller is considered to validate the proposed trajectory planning strategy in a realistic simulation environment, which involves the PX4 firmware, and in a controlled experimental setup. The results demonstrate that accurate tracking of the designed trajectories enables the vehicles to perform 360-degree loops at great speed and manoeuvres that facilitate the exchange of a parcel between two multirotor vehicles during flight.},
  archive      = {J_JIRS},
  author       = {Pinto, João and Guerreiro, Bruno J. and Cunha, Rita},
  doi          = {10.1007/s10846-024-02214-z},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Planning aggressive drone manoeuvres: A geometric backwards integration approach},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-bandwidth contact state estimation with only joint
angle feedback for legged robots. <em>JIRS</em>, <em>111</em>(1), 1–14.
(<a href="https://doi.org/10.1007/s10846-024-02215-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the contact between robot feet and the ground is unpredictable in unstructured terrains, robots require fast and accurate contact detection capability. This paper presents a contact detection method through estimating external force that benefits from compensating friction torque and driving torque effectively. For estimating external force, a new estimation method for joint angular velocity/acceleration is proposed. Furthermore, filter parameters are selected theoretically based on the leg control model, which achieves the optimal solution of the parameters. A control compensation strategy of the low-velocity zone of the motor is also implemented for optimizing the estimation of contact force. In contrast to the classic generalized momentum method, the proposed method allows high bandwidth estimation. The proposed contact detection method and the angular velocity/acceleration estimation method using only discrete position feedback information are verified on the developed quadruped robot platform SCIT Dog.},
  archive      = {J_JIRS},
  author       = {Yang, Junjie and Sun, Hao and Jia, Yinghao and Wang, Changhong},
  doi          = {10.1007/s10846-024-02215-y},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {High-bandwidth contact state estimation with only joint angle feedback for legged robots},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-sum differential games guidance law accounting for
impact-angle-constrained using adaptive dynamic programming.
<em>JIRS</em>, <em>111</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10846-024-02217-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To intercept a maneuvering target with a predetermined impact angle, a computational intelligence guidance law was proposed in this paper. Based on the theory of two-player zero-sum differential games, this problem is resolved efficiently by solving the Hamilton–Jacobi–Isaacs (HJI) equation. The Nash equilibrium solution of HJI equation can be solved with a policy iteration (PI) algorithm. Instead of using the offline PI algorithm, an online PI algorithm is introduced, in which the disturbance and control policies can be updated simultaneously. It can be proved that the online PI algorithm is a replacement for Newton’s iterative algorithm, the convergence of which is ensured by Kantorovich’s theorem. In the scenario of missiles intercepting targets, an adaptive critic structure based on a neural network (NN) is proposed to implement the online PI algorithm. Only one critic NN approximator is used in the PI algorithm to calculate a value function and the approximate Nash equilibrium solution. It is not necessary to acquire the exact internal dynamics information of nonlinear systems on the basis of online data sampling. The effectiveness of the computational intelligence guidance law is proven by simulation results.},
  archive      = {J_JIRS},
  author       = {Zhang, Xue and Wang, Qi},
  doi          = {10.1007/s10846-024-02217-w},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Zero-sum differential games guidance law accounting for impact-angle-constrained using adaptive dynamic programming},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theoretical foundation for erroneous behavior in
human–robot interaction. <em>JIRS</em>, <em>111</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10846-025-02221-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of mass customization has precipitated a need within the industry for the implementation of collaborative robots, which facilitate the integration of human cognitive capabilities with the speed and repeatability of robots. This coupling, however, engenders a closer collaboration between the partners, thereby necessitating collective synergy to achieve optimal scheduling while circumventing musculoskeletal disorders. It is imperative to study and analyze the behavior of humans and robots in interaction, as the current paradigm strives to achieve an optimal interaction between the two partners with the objective of ensuring productivity, safety, cognitive ergonomics and preventing musculoskeletal disorders. However, human behavior is variable and can, on occasion, give rise to anomalies in the interaction. Consequently, it is imperative that the robot partner exhibits precise behavior, whether proactive or reactive. This paper puts forth a unified perspective on robot behavior when confronted with human abnormal behavior during interaction on the factory floor. This systematic literature review and meta-analysis employs the PRISMA methodology to examine the literature on human and robot behavior in human–robot interaction in an industrial context, with a particular focus on robot behavior when confronted with human abnormal behavior during interaction. A systematic search of nearly 2,609 papers yielded 133 for inclusion in this systematic review. In light of the findings presented in this review, it can be concluded that the selection of robot actions based on human behavior represents a novel area of research that requires further investigation, particularly with regard to proactive online behavioral approaches. Indeed, there is a vast array of robot behavior modalities in response to typical human behavior (e.g., command input). However, there is currently no prescribed robot reaction based on atypical human behavior (e.g., misplacement in the factory floor, repetition of tasks, etc.). This lack of definition complicates the deployment of such technology in the smart factory. Consequently, it is essential to define new decision strategies based, for instance, on artificial intelligence approaches.},
  archive      = {J_JIRS},
  author       = {Tchane Djogdom, Gilde Vanel and Otis, Martin J.-D. and Meziane, Ramy},
  doi          = {10.1007/s10846-025-02221-8},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {A theoretical foundation for erroneous behavior in Human–Robot interaction},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of rapidly-exploring random tree
algorithms applied to ship trajectory planning and behavior generation.
<em>JIRS</em>, <em>111</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10846-025-02222-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly Exploring Random Tree (RRT) algorithms, notably used for nonholonomic vehicle navigation in complex environments, are often not thoroughly evaluated for their specific challenges. This paper presents a first such comparison study of the variants Potential-Quick RRT* (PQ-RRT*), Informed RRT* (IRRT*), RRT*, and RRT, in maritime single-query nonholonomic motion planning. Additionally, the practicalities of using these algorithms in maritime environments are discussed and outlined. We also contend that these algorithms are beneficial not only for trajectory planning in Collision Avoidance Systems (CAS) but also for CAS verification when used as vessel behavior generators. Optimal RRT variants tend to produce more distance-optimal paths but require more computational time due to complex tree wiring and nearest neighbor searches. Our findings, supported by Welch’s t-test at a significance level of $$\alpha =0.05$$ , indicate that PQ-RRT* slightly outperform IRRT* and RRT* in achieving shorter trajectory length but at the expense of higher tuning complexity and longer run-times. Based on the results, we argue that these RRT algorithms are better suited for smaller-scale problems or environments with low obstacle congestion ratio. This is attributed to the curse of dimensionality, and trade-off with available memory and computational resources.},
  archive      = {J_JIRS},
  author       = {Tengesdal, Trym and Pedersen, Tom Arne and Johansen, Tor Arne},
  doi          = {10.1007/s10846-025-02222-7},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {A comparative study of rapidly-exploring random tree algorithms applied to ship trajectory planning and behavior generation},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiDAR loop closure detection using semantic graphs with
graph attention networks. <em>JIRS</em>, <em>111</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10846-025-02223-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel loop closure detection algorithm that uses graph attention neural networks to encode semantic graphs to perform place recognition and then use semantic registration to estimate the 6 DoF relative pose constraint. Our place recognition algorithm has two key modules, namely, a semantic graph encoder module and a graph comparison module. The semantic graph encoder employs graph attention networks to efficiently encode spatial, semantic and geometric information from the semantic graph of the input point cloud. We then use self-attention mechanism in both node-embedding and graph-embedding steps to create distinctive graph vectors. The graph vectors of the current scan and a keyframe scan are then compared in the graph comparison module to identify a possible loop closure. Specifically, employing the difference of the two graph vectors showed a significant improvement in performance, as shown in ablation studies. Lastly, we implemented a semantic registration algorithm that takes in loop closure candidate scans and estimates the relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive evaluation on public datasets shows that our model is more accurate and robust, achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset, when compared to the baseline semantic graph algorithm. For the benefit of the community, we open-source the complete implementation of our proposed algorithm and custom implementation of semantic registration at https://github.com/crepuscularlight/SemanticLoopClosure .},
  archive      = {J_JIRS},
  author       = {Yang, Liudi and Mascaro, Ruben and Alzugaray, Ignacio and Prakhya, Sai Manoj and Karrer, Marco and Liu, Ziyuan and Chli, Margarita},
  doi          = {10.1007/s10846-025-02223-6},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {LiDAR loop closure detection using semantic graphs with graph attention networks},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-based mapless navigation of a hybrid aerial-underwater
vehicle using prioritized deep reinforcement learning. <em>JIRS</em>,
<em>111</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10846-024-02206-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Reinforcement Learning (RL) has made promising progress in several areas, such as control tasks and video games, by using simple, low-dimensional data. However, it struggles when it needs to process more complex, high-dimensional inputs like raw pixel images, offering results that are not as good as those that use information from laser sensors, as many robotics applications demand. This paper introduces a new technique called Contrastive Unsupervised Prioritized Representations in Reinforcement Learning (CUPRL) for mobile robotics. This innovative approach combines RL and Contrastive Learning to effectively handle high-dimensional observations, an area not fully explored. This is crucial for navigating complex environments, especially for hybrid robots, such as the Hybrid Unmanned Aerial-Underwater Vehicles (HUAUVs) that experience strong changes in light when moving between air and water. Our approach excels in taking important information from depth maps and RGB images during training, aiming to improve the ability of RL agents to navigate without a map in the context of HUAUVs. This field has much to be explored. Our tests in a robot simulator show that CUPRL, which uses learning from both RGB and depth images, performs better than current methods that rely only on pixel data. This is especially true for 3D navigation without maps, where we use only RGB images during tests. This proves that CUPRL could be useful for making decisions in HUAUVs. We believe our work not only offers improved solutions for navigation but also encourages further research into the use of high-dimensional data in RL, presenting a more efficient and adaptable method in complex environments compared to earlier strategies.},
  archive      = {J_JIRS},
  author       = {Costa de Jesus, Junior and Kich, Victor Augusto and Kolling, Alisson Henrique and Grando, Ricardo Bedin and da Silva Guerra, Rodrigo and Drews-Jr, Paulo Lilles Jorge},
  doi          = {10.1007/s10846-024-02206-z},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Image-based mapless navigation of a hybrid aerial-underwater vehicle using prioritized deep reinforcement learning},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CROW: A self-supervised crop row navigation algorithm for
agricultural fields. <em>JIRS</em>, <em>111</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10846-025-02219-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compact robots operating beneath the crop canopy present great potential for a range of autonomous and remote tasks, including phenotyping, soil analysis, and cover cropping. Under-canopy navigation presents unique challenges, such as the need for a navigation system that can traverse diverse crop types, navigate despite sensory obstructions, and manage sensory noise effectively. Aiming to solve this problem in a scalable manner, we present a novel navigation method that uses a self-supervised neural network tailored for row-following in under-canopy plantations for mobile robots. Our method, termed CROW (Crop-ROW navigation), integrates perception, waypoint generator, and control components, and is capable of handling variations in luminosity, topology, types of plantations, and plant growth stages. By using a Deep Learning-based approach to interpret LiDAR scans, we convert the detected rows of crops into lines, establishing waypoints for the controller based on fundamental geometric principles. To address the computational complexity inherent in standard Model Predictive Controller solvers, we employ a Constrained Iterative Linear Quadratic approach. Our system has been validated in both simulated and real-world environments, demonstrating successful navigation through 115-meter corn rows with little to no intervention, i.e., requiring only $$3 \pm 3$$ interventions per row experiment.},
  archive      = {J_JIRS},
  author       = {Affonso, Francisco and Tommaselli, Felipe Andrade G. and Capezzuto, Gianluca and Gasparino, Mateus V. and Chowdhary, Girish and Becker, Marcelo},
  doi          = {10.1007/s10846-025-02219-2},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {CROW: A self-supervised crop row navigation algorithm for agricultural fields},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic model-based stationing: Robust total station
localization without known control points. <em>JIRS</em>,
<em>111</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10846-025-02224-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based stationing refers to the process of registering a set of measurements to a model. Specifically, in the surveying context, this refers to the process of determining the station, i.e., the position and orientation, of a total station given a user-provided building floor plan and a series of polar measurements. Traditional methods compute the station using a set of known control points. We propose an automatic workflow which uses a novel registration method that does not require any known control points in order to find the station with high accuracy. Our registration algorithm relies on angle and distance measurements only; therefore, it is not limited to modern image-assisted total stations. In addition, the proposed workflow comprises a modeling phase to deal with model inaccuracies and produces reliable and accurate results. Quantitative and qualitative tests on synthetic and real-world scenarios demonstrate the performance and robustness of our automatic workflow and registration method.},
  archive      = {J_JIRS},
  author       = {Reyes-Aviles, Fernando and Gloor, Thomas and Arth, Clemens},
  doi          = {10.1007/s10846-025-02224-5},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Automatic model-based stationing: Robust total station localization without known control points},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large workspace frontal human following for mobile robots
utilizing 2D LiDAR. <em>JIRS</em>, <em>111</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10846-025-02225-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots following humans is an efficient and practical feature, particularly in the context of service robotics. However, the majority of existing research has focused on the robot following the human from behind, with relatively little attention given to the robot operating in front of the human. This following-in-front approach, where the robot remains within the user’s field of view, is more reassuring and facilitates further human-robot interaction. Unlike traditional following methods, frontal following requires awareness of the user’s orientation and intentions. New challenges will arise in developing a tracker that can accurately estimate a user’s pose based on a knee height 2D LiDAR, especially when the legs frequently occlude each other. There is also a need to ensure the safety of the user while addressing how the robot can keep up with the user in situations where it falls behind. Our contribution lies in proposing a novel 2D LiDAR-based frontal human following method that accommodates various motion patterns of the target user. Specifically, inspired by human walking gait, we develop an accurate and robust human pose tracker that takes into account leg occlusion and the data association problem. We build a large workspace velocity field that enables a holonomic mobile robot to follow and come gradually and safely in front of the user regardless of their relative position. We evaluate the performance of our approach through a rich set of experimental scenarios, and demonstrate its effectiveness in achieving reliable frontal human following. Our studies suggest this approach has potential applications in warehouses, industrial factories or for visually impaired persons.},
  archive      = {J_JIRS},
  author       = {Gao, Zhenyu and Wang, Ze and Saint-Bauzel, Ludovic and Ben Amar, Faïz},
  doi          = {10.1007/s10846-025-02225-4},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Large workspace frontal human following for mobile robots utilizing 2D LiDAR},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a microcontroller-based recurrent neural
network predictive system for lower limb exoskeletons. <em>JIRS</em>,
<em>111</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10846-025-02226-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical deployments of exoskeletons can often be limited by cost, limiting access to their usage by those that would benefit from them. Minimising cost whilst not harming effectiveness is therefore desirable for exoskeleton development. For Control Systems governing assistive and rehabilitative exoskeletons that react to the wearer’s movements, there will inevitably be some delay between when their wearer intends to move and when the exoskeleton can assist with this movement. This can lead to situations where a user may be limited by their own assistive exoskeleton, reducing their ability to move freely. A potential solution to this is to provide a proactive method of control, where the most likely path of the wearer’s movement is predicted ahead of the wearer making the motion themselves. This can be used to give the user assistance immediately as they are walking, as well as potentially pre-emptively adjust their gait if they suffer from predictable gait deficiencies. The purpose of this paper is to investigate the Data Collection, Implementation, and Effectiveness of an LSTM Recurrent Neural Network dynamically predicting future movement based off of prior movement. These methods were developed to use off the shelf, Low-Cost Microcontrollers as to minimise their Financial, Weight, and Power Impact on an overall Low-Cost exoskeleton design, as well as to evaluate how effective such an implementation would be when compared to running such a Neural Network on a more powerful processor. The created model was capable of achieving similar accuracies to far more powerful models on High-Powered Laptops.},
  archive      = {J_JIRS},
  author       = {Slucock, T. and Howells, G. and Hoque, S. and Sirlantzis, K.},
  doi          = {10.1007/s10846-025-02226-3},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Development of a microcontroller-based recurrent neural network predictive system for lower limb exoskeletons},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design optimisation of fully actuated UAVs using hybrid
optimisation. <em>JIRS</em>, <em>111</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10846-025-02227-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past few years, there has been a rise in novel fully actuated multirotor UAV configurations, each customised to perform a different task. These configurations must be optimised to extract their full potential for the different use cases. The main issue with existing optimisation methods is the computational cost required to produce a design when the optimisation includes both continuous and discrete variables, such as off-the-shelf components. We propose a hybrid optimisation method using continuous surrogate methods with localised parameter sweep. The continuous stage aims at finding the optimal value for the continuous design variables and reduces the search space for the discrete design variables. Empirical models for the off-the-shelf components are used to reduce the size of the continuous stage of the optimisation. The localised search method consists of a parameter sweep of the discrete design variables within a certain threshold of the optimal parameters from the first stage. Three case studies confirm the method’s capabilities with different configurations and optimisation setups, comparing optimality, success rate and computational cost. The optimal design from the hybrid method is consistent with the baseline methods used for comparison within each case study, with a minimum success rate of 30% while reducing cost by 98%. Compared to specialised discrete methods, the improvement in computational cost is inconsistent; however, it achieves a reduction of 99.3% with certain design requirements. A comparison to another hybrid method was also performed, with the proposed method maintaining its cost, optimality and success rate better than the SQP-DSS method when increasing method complexity.},
  archive      = {J_JIRS},
  author       = {Al-zubaidi, Salim and Stol, Karl},
  doi          = {10.1007/s10846-025-02227-2},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Design optimisation of fully actuated UAVs using hybrid optimisation},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motor speed control of four-wheel differential drive robots
using a new hybrid moth-flame particle swarm optimization (MFPSO)
algorithm. <em>JIRS</em>, <em>111</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10846-025-02228-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speed control of DC motors is essential for automated vehicles and four-wheel differential drive (4WD) cars, which are distinct by their high level of maneuverability. The PID controller is one of the most popular techniques for controlling speed, but tuning its parameters is challenging. This paper presents a novel hybrid algorithm, the Moth-Flame Particle Swarm Optimization (MFPSO), which combines moth-flame optimization (MFO) and particle swarm optimization (PSO) to address the slow convergence of MFO and the premature convergence of PSO. The MFPSO is deployed for real-time interactive tuning of the PID controller to control the speed of DC motors in a 4WD car. Additionally, a novel practical procedure is proposed to build a robust four-wheel differential drive and maintain the synchronization of the four DC motors. Simulation results and statistical analysis demonstrate the superior performance of the MFPSO compared with the PSO, MFO, and other hybrid variants (HMFPSO and HyMFPSO), with MFPSO ranking first in the Friedman test on CEC2020/2021 and engineering optimization benchmark problems. Practical results and the transient response analysis of the speed control revealed that MFPSO significantly outperformed the traditional Ziegler-Nichols (ZN) method, MFO, PSO, HMFPSO, and HyMFPSO algorithms. Specifically, the MFPSO algorithm reduced settling time by 34.83%, 21.20%, 20.75%, 22.97%, and 31.59%, and overshoot by 86.11%, 64.99%, 71.02%, 74.37%, and 60.58% compared to the ZN, MFO, PSO, HMFPSO, and HyMFPSO algorithms, respectively. The source code of the proposed algorithm is available at https://github.com/MohamedRedaMu/MFPSO-Algorithm .},
  archive      = {J_JIRS},
  author       = {Reda, Mohamed and Onsy, Ahmed and Haikal, Amira Y. and Ghanbari, Ali},
  doi          = {10.1007/s10846-025-02228-1},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Motor speed control of four-wheel differential drive robots using a new hybrid moth-flame particle swarm optimization (MFPSO) algorithm},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent path planning based on conflict-based search
(CBS) variations for heterogeneous robots. <em>JIRS</em>,
<em>111</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10846-025-02229-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel Multi-agent path planning scheme based on Conflict Based Search (CBS) for heterogeneous holonomic and non-holonomic agents, designated as Heterogeneous CBS (HCBS). The proposed methodology employs a hybrid $$A^*$$ algorithm for non-holonomic car-like robots and a conventional $$A^*$$ algorithm for holonomic robots. Following this, a body conflict detection strategy is utilized to construct the conflict tree, bridging the initial path planning with the resolution of conflicts among agents. Moreover, we present two variants of HCBS: the Enhanced Conflict-Based Search (EHCBS) and the Depth-First Conflict-Based Search (DFHCBS). We evaluate the efficacy of our proposed algorithms—HCBS, EHCBS, and DFHCBS—against a standard prioritized planning algorithm, focusing on success rates and computational efficiency in environments with varying numbers of agents and obstacles. The empirical results demonstrate that EHCBS exhibits superior computational efficiency in small, dense environments, while DFHCBS performs well in larger-scale environments. This highlights the adaptability of our proposed approaches in various settings, proving the computational advantage of EHCBS and DFHCBS over traditional methods.},
  archive      = {J_JIRS},
  author       = {Bai, Yifan and Kotpalliwar, Shruti and Kanellakis, Christoforos and Nikolakopoulos, George},
  doi          = {10.1007/s10846-025-02229-0},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Multi-agent path planning based on conflict-based search (CBS) variations for heterogeneous robots},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lower limb exoskeleton adaptive control method based on
model-free reinforcement learning and improved dynamic movement
primitives. <em>JIRS</em>, <em>111</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10846-025-02230-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in lower limb exoskeleton control have predominantly focused on enhancing walking capabilities across diverse terrains, such as level ground, stairs, and ramps. However, achieving seamless transitions between these terrains remains a significant challenge due to the unpredictability of the environment, which hampers adaptive control. In this paper, we propose a Hierarchical Interactive Learning (HIL) control method based on gait phase and locomotion pattern recognition. The method comprises two layers: high-level learning and low-level control. The high-level learning is based on gait phase and locomotion pattern recognition, utilizing the Dynamic Movement Primitives (DMP) to piecewise learn the desired joint torque curves. The low-level control utilizes the learned DMP to output torque based on the gait phase and locomotion pattern, while reinforcement learning is employed to dynamically adjust the control parameters of DMP in real-time with the goal of minimizing human-exoskeleton interaction forces. The experiments collected gait data of lower limb movement from active exoskeletons. The results show that our method significantly reduces human-exoskeleton interaction forces across diverse terrains. In order to verify the feasibility and effectiveness of the proposed method, 15 healthy subjects were tested with the lower limb exoskeleton of these 3 generations. The experimental results show that the proposed HIL control method gives a valuable tool for smooth transitions among different terrains, reduces the reliance on accurate dynamic models and the average oxygen consumption decreased by about 12%, underscoring its potential to improve exoskeleton-assisted mobility.},
  archive      = {J_JIRS},
  author       = {Huang, Liping and Zheng, Jianbin and Gao, Yifan and Song, Qiuzhi and Liu, Yali},
  doi          = {10.1007/s10846-025-02230-7},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {A lower limb exoskeleton adaptive control method based on model-free reinforcement learning and improved dynamic movement primitives},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technologies and applications of robots in dementia care: A
systematic review. <em>JIRS</em>, <em>111</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10846-025-02232-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Socially assistive robotics (SARs) can provide social interventions for people living with dementia. Although several SARs have been introduced, the absence of their evaluation and discussion has restricted the development and use of robots in dementia care. This paper systematically reviews robot technologies, explores their applications, and provides systematic information for technology development, selection, and implementation for robots in dementia care. Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, articles from January 1990 to May 2023 were retrieved from seven databases and 38 eligible articles were identified. Nineteen SARs were identified and reviewed, encompassing physical features, sensing capabilities, perception, user modeling, and interaction technologies. The robot personality framework, which classified SARs as animal-like, human-like, and artificial-being, was employed to compare technologies, applications, and clinical outcomes. In addition, the experimental methodologies and study quality of existing studies were compared and discussed. This study found that technologies shape the robot’s personality and contribute to its applications. Future studies could be based on their application purpose, which could guide the selection, development, and implementation of robot technologies, thereby promoting SAR applications in dementia care. In addition, studies with large sample sizes, rigorous study designs, and detailed intervention descriptions are recommended, which could enhance study quality and promote robot technologies and applications in dementia care.},
  archive      = {J_JIRS},
  author       = {Wu, Dongjun and Pu, Lihui and Jo, Jun and Moyle, Wendy},
  doi          = {10.1007/s10846-025-02232-5},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Technologies and applications of robots in dementia care: A systematic review},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new vertical plane motion control method based on the
framework consisting of control law and control allocation for
underwater vehicle with bow and stern elevators. <em>JIRS</em>,
<em>111</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10846-025-02233-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the vertical plane motion performance of an underwater vehicle with bow and stern elevators under actuator saturation and external disturbance conditions, a new control method is proposed in this paper. Firstly, for simplifying the analysis of the coupled kinetics equation of the underwater vehicle, virtual control variables are introduced to decompose the kinetics equation into two cascaded components. Then, a framework consisting of control law and control allocation is presented according to the cascaded components. In the aspect of control law, a double closed-loop control scheme is designed. The outer loop realizes the depth control by adjusting the expected heave velocity and pitch of the underwater vehicle with an S-plane regulator and a subsection function. Meanwhile, an Augmented Linear Quadratic Regulator (ALQR) is employed in the inner loop to control the heave velocity and pitch without steady-state error. Considering the physical constraints of elevators, a pitching-priority idea is adopted in the design of the control allocation to prioritize the pitch control requirement of the underwater vehicle. Finally, based on the framework, disturbances are estimated and compensated to enhance the anti-interference performance, as well as, auxiliary functions are constructed for reducing the adverse effect caused by saturation. Simulation results show that the new control method performs perfectly under various conditions, it is also conducive to the navigation safety of the underwater vehicle because of its superior pitch control capability.},
  archive      = {J_JIRS},
  author       = {Sun, Gongwu and Sima, Can and Yuan, Shouzheng and Ma, Xiangneng and Zhang, Wanyuan and Jiao, Huifeng},
  doi          = {10.1007/s10846-025-02233-4},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {A new vertical plane motion control method based on the framework consisting of control law and control allocation for underwater vehicle with bow and stern elevators},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An inpainting SLAM approach for detecting and recovering
regions with dynamic objects. <em>JIRS</em>, <em>111</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10846-025-02234-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) is a cornerstone capability for intelligent mobile robots, enabling them to accurately estimate their positions in unknown environments. However, most of the state-of-the-art visual SLAM systems rely on the assumption of static scenes, leading to significantly reduced accuracy and robustness in dynamic environments. In this paper, a novel RGB-D SLAM system termed Inpainting SLAM is proposed in the ORB-SLAM2 framework. Our Inpainting SLAM defines two new modules: one is the dynamic objects detection module, which combines segmentation and depth information to segment dynamic objects. Additionally, a new method is also proposed to determine whether movable objects are classified as dynamic. The other is an image inpainting module to restore static regions that are occluded by dynamic objects, with a new rectified approach introduced to determine the inpainting regions that can enhance the performance of the SLAM system. With these two modules, the accuracy and robustness of the SLAM system in dynamic scenes are expected to be improved. Our method is tested on the public TUM dataset, demonstrating its effectiveness and reliability. The improvements on ORB-SLAM2 in RTE, RRE, and ATE are 97.45%, 99.88%, and 97.90%, respectively. In comparison with other advanced dynamic SLAM methods, our approach also demonstrates superiority.},
  archive      = {J_JIRS},
  author       = {Zhang, Longxin and Xu, Benlian and Chen, Siwen and Nener, Brett and Zhou, Xu and Lu, Mingli and Li, Xinya and Le, Shuting},
  doi          = {10.1007/s10846-025-02234-3},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {An inpainting SLAM approach for detecting and recovering regions with dynamic objects},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic algorithm-based control of a two-wheeled
self-balancing robot. <em>JIRS</em>, <em>111</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10846-025-02236-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile robots are becoming increasingly popular in a wide array of applications: industrial, item delivery, search and rescue, space, social, and entertainment. A two-wheeled self-balancing mobile robot is a statically unstable non-linear system with strong coupling dynamics. Common practices in the development of control systems for such robots are either to linearise the region of application to be used with linear controllers or to use complex nonlinear controllers such as fuzzy logic, sliding mode, and neural networks. However, self-balancing robots are still restricted by the travelling distance needed to regain an upright stance, the length of settling time, high overshoot and lack of resilience to external disturbance. In this paper, we are proposing a novel genetic algorithm-based switching control to evolve more effective control parameters and increase autonomy. Differently from previous work a genetic algorithm has been used to select the parameters in a sliding mode control and a switching-algorithm-based controller of a two-wheeled self-balancing mobile robot. The performance of the proposed controllers is assessed in simulations using the CoppeliaSim environment. The tests used dynamic criteria (distance travelled, maximum angular deviation), control criteria (settling time, % overshoot). The results showed that the genetic algorithm-based control has better performance in the 55 degree recovery, impulse response and variable inclination tests and that switching algorithm-based control shows better performance in step response tests. The results produced by the evolutionary algorithm are often able to perform better than their analytic counterparts. This shows the potential of meta-heuristic algorithms to obtain solutions for optimization problems encountered by statically unstable non-linear systems in unstructured and fast-changing environments.},
  archive      = {J_JIRS},
  author       = {Papadimitriou, Kleon Dimitrios and Murasovs, Nikita and Giannaccini, Maria Elena and Aphale, Sumeet},
  doi          = {10.1007/s10846-025-02236-1},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Genetic algorithm-based control of a two-wheeled self-balancing robot},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-sensitive autonomous exploration of unknown
environments: A deep reinforcement learning perspective. <em>JIRS</em>,
<em>111</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10846-025-02235-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study provides a thorough investigation into autonomous exploration within unknown environments, with a focus on minimizing exploration time and so fuel consumption. This research utilizes a 2D simulation environment to collect training data efficiently, facilitating the evaluation of the proposed methods’ efficiency, adaptability, and generalizability through various experiments. Single-robot autonomous exploration policies using advanced Deep Reinforcement Learning (DRL) algorithms are developed. The main novelty of this paper is the development of risk-sensitive policies, in contrast to traditional risk-neutral approaches in DRL, to enhance exploration efficiency. Additionally, this research presents the development of an adaptive autonomous exploration policy that dynamically adjusts the Conditional Value-at-Risk (CVaR) based on the exploration percentage. The results demonstrate a significant improvement in autonomous exploration efficiency compared to well-known traditional RL and classical single-robot exploration policies, validating the effectiveness of the suggested novel autonomous exploration strategies.},
  archive      = {J_JIRS},
  author       = {Sarfi, Mohammad Hossein and Bisheban, Mahdis},
  doi          = {10.1007/s10846-025-02235-2},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Risk-sensitive autonomous exploration of unknown environments: A deep reinforcement learning perspective},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal consistency as pretext task in unsupervised domain
adaptation for semantic segmentation. <em>JIRS</em>, <em>111</em>(1),
1–15. (<a href="https://doi.org/10.1007/s10846-025-02220-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent and autonomous robots (and vehicles) largely adopt computer vision systems to help in localization, navigation and obstacle avoidance tasks. By integrating different deep learning techniques, such as Object Detection and Image Semantic Segmentation, these systems achieve high accuracy in the domain they were trained on. Nonetheless, robustly operating in different domains still poses a major challenge to vision-based perception. In this sense, Unsupervised Domain Adaptation (UDA) has recently gained momentum due to its importance to real-world applications. Specifically, it leverages the prompt availability of unlabeled data to design auxiliary sources of supervision to guide the knowledge transfer between domains. The advantages of such an approach are two-fold: avoiding going through exhaustive labeling processes, and enhancing adaptation performance. In this scenario, exploring temporal correlations in unlabeled video data stands as an interesting alternative, which has not yet been explored to its full potential. In this work, we propose a Self-supervised learning framework that employs Temporal Consistency from unlabeled video sequences as a pretext task for improving UDA for Semantic Segmentation (UDASS). A simple yet effective strategy, it has shown promising results in a real-to-real adaptation setting. Our results and discussions are expected to benefit both new and experienced researchers on the subject.},
  archive      = {J_JIRS},
  author       = {Barbosa, Felipe and Osório, Fernando},
  doi          = {10.1007/s10846-025-02220-9},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Temporal consistency as pretext task in unsupervised domain adaptation for semantic segmentation},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid physics-infused deep learning for enhanced real-time
prediction of human upper limb movements in collaborative robotics.
<em>JIRS</em>, <em>111</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10846-025-02237-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot collaboration is crucial in various industries, making accurate prediction of human arm movements essential for seamless interaction. This paper presents a significant advancement in collaborative robotics by developing a hybrid model that enhances the accuracy and interpretability of human motion predictions. By integrating a Physics-Infused Model with Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks, our approach effectively captures intricate temporal dependencies while incorporating physical constraints, leading to more robust and realistic predictions. The hybrid model was successfully implemented on an ABB IRB 120 robot, demonstrating its practical applicability in real-world scenarios. Our results show that this model outperforms conventional methods, particularly in predicting human arm positions during collaborative tasks. The key contribution of this work lies in the integration of deep learning with physics-based principles, setting a new benchmark for predictive accuracy in human–robot collaboration. This research not only enhances the performance of collaborative robots but also opens the door for similar hybrid models to be applied in other fields where accurate motion prediction is critical.},
  archive      = {J_JIRS},
  author       = {Halim, Mina Yousry and Awad, Mohammed Ibrahim and Maged, Shady A.},
  doi          = {10.1007/s10846-025-02237-0},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Hybrid physics-infused deep learning for enhanced real-time prediction of human upper limb movements in collaborative robotics},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of GMDH and perceptron controllers for mobile
robot obstacle following/avoidance with hardware-in-the-loop validation.
<em>JIRS</em>, <em>111</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10846-025-02239-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the effectiveness of using the Group Method of Data Handling (GMDH) and Perceptron neural controllers for a mobile robot obstacle/following avoidance application. The paper evaluates the performance of these controllers in different scenarios, analyzing parameters such as settling time, steady-state error, and overshoot. In addition, we investigate different hardware implementations of the proposed controllers using SoC FPGAs, tailored for small mobile robot platforms, offering high computational performance and low power consumption. To train the neural controllers, four bio-inspired optimization algorithms were used, and hypothesis tests were conducted to select the best neural controller. A Hardware-in-the-Loop (HIL) simulation was conducted in an AMD-Xilinx SoC FPGA Zynq 7020 to attain the best compromise between the controllers’ performance, numerical precision, hardware resources consumption, and power dissipation. The findings underscored the effectiveness of both GMDH and Perceptron controllers in stabilizing the robot amidst disturbances and adeptly navigating obstacle-following and avoidance tasks across various unknown scenarios. However, the Perceptron controller exhibits several advantages in terms of hardware resources and power consumption.},
  archive      = {J_JIRS},
  author       = {Pastrana Triana, Mario Andrés and Santana, Mateus Souza and Mendoza Peñaloza, Jose Alfredo and Nunes de Oliveira, Luiz Henrique and Muñoz Arboleda, Daniel Mauricio},
  doi          = {10.1007/s10846-025-02239-y},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Comparison of GMDH and perceptron controllers for mobile robot obstacle Following/Avoidance with hardware-in-the-loop validation},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian network-based threat assessment and TARAPSO
algorithm for UUV path planning in complex environments. <em>JIRS</em>,
<em>111</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10846-025-02240-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is crucial for unmanned underwater vehicles (UUVs) navigating in complex marine environments, which are often affected by multiple threat factors. Traditional path planning methods often fail to account for these factors, suffer from local optimal solutions, and lack adequate exploration and exploitation capabilities. To overcome these limitations, this paper proposes a target attraction rule-based adaptive particle swarm optimization (TARAPSO) algorithm, driven by threat assessment. Firstly, a 3D model of the complex marine environment is established, incorporating multiple threat factors. Secondly, a Bayesian network (BN)-based threat assessment model is developed to perform probabilistic inference on the states of potential threats encountered by the UUV. Based on the results of this assessment, a cost function incorporating the threat degree index is designed to evaluate the impact of various threat factors on the UUV’s path. Finally, the TARAPSO algorithm is introduced and applied to solve the multi-objective optimization problem of path planning, incorporating the threat assessment-driven cost function. Simulation results show that the TARAPSO algorithm, when combined with the BN-based threat assessment strategy, demonstrates superior efficiency in evaluating multiple threat factors, generates competitive paths with shorter lengths, and effectively ensures safe navigation of the UUV.},
  archive      = {J_JIRS},
  author       = {Zhang, Xun and Li, Wenguo and Wang, Ziqi},
  doi          = {10.1007/s10846-025-02240-5},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Bayesian network-based threat assessment and TARAPSO algorithm for UUV path planning in complex environments},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). JIRS editorial, 1st quarter 2025. <em>JIRS</em>,
<em>111</em>(1), 1. (<a
href="https://doi.org/10.1007/s10846-025-02242-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JIRS},
  author       = {Valavanis, Kimon P.},
  doi          = {10.1007/s10846-025-02242-3},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {JIRS editorial, 1st quarter 2025},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter identification of a flexible-joint robot axis
using sinusoidal position tracking. <em>JIRS</em>, <em>111</em>(1),
1–19. (<a href="https://doi.org/10.1007/s10846-025-02244-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for identifying the mechanical parameters of flexible-joint robot axes using sinusoidal position tracking control. Accurate knowledge of mechanical parameters, such as inertia, coupling stiffness, and friction components, is important for designing effective controllers in robotic systems. These parameters are determined from integral values derived from the torque, speed, and position measurements of both the motor and load sides, leveraging the 90 $$^{\circ }$$ phase relationship between position, velocity, and acceleration terms. A robust sinusoidal position controller was developed, and the speed and position measurements of both the motor and load sides were utilized to implement the proposed method. When compared with parameters identified using standard methods, the proposed method shows an absolute percentage error ranging from 3.55% to 14.6% for the inertias and coupling stiffness, and 10.76% to 19% for the friction coefficients. The straightforward implementation and effectiveness of this method make it suitable for applications in industrial robotic arms, where precise control is essential for enhancing performance and operational efficiency.},
  archive      = {J_JIRS},
  author       = {Hafez, Ishaq and Dhaouadi, Rached},
  doi          = {10.1007/s10846-025-02244-1},
  journal      = {Journal of Intelligent &amp; Robotic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Intell. Robot. Syst.},
  title        = {Parameter identification of a flexible-joint robot axis using sinusoidal position tracking},
  volume       = {111},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jmiv---9">JMIV - 9</h2>
<ul>
<li><details>
<summary>
(2025). Curvature-guided color image restoration by saturation-value
total variation. <em>JMIV</em>, <em>67</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10851-024-01218-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel curvature-guided saturation-value total variation model for color image restoration. Specifically, we incorporate the curvature prior into the traditional variational model to guide the evolution in the direction that maintains the curvature information. Theoretically, we investigate the properties of the proposed model and give a detailed discussion based on the mathematical foundation about the existence of the solution. Numerically, we formulate an effective and efficient algorithm to solve the proposed minimization problem based on the framework of alternating direction method of multipliers. Numerical examples are presented to demonstrate that the performance of the proposed model is better than that of other testing methods for several testing color images.},
  archive      = {J_JMIV},
  author       = {Wang, Wei and Wang, Jingjie and Ng, Michael K.},
  doi          = {10.1007/s10851-024-01218-6},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Curvature-guided color image restoration by saturation-value total variation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the axial symmetry of 2D star-shaped sets. <em>JMIV</em>,
<em>67</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10851-024-01222-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential aspect of the study of shapes is the symmetry because of its importance from a theoretical point of view and its applicability in multiple real-life problems. In this manuscript, the axial symmetry of 2D star-shaped sets is analyzed. For such a purpose, different measures of axial symmetry of a star-shaped set are proposed and the concept of a best symmetry axis is also introduced. By means of them, families of symmetry measures for star-shaped sets quantifying the degree of symmetry of a set of that class are introduced. All of them are discussed in detail, providing their main properties and the existence of at least a best axis of symmetry, which could be not unique, for any star-shaped set. Some examples illustrate the concepts and results of the manuscript.},
  archive      = {J_JMIV},
  author       = {López-Díaz, María Concepción and López-Díaz, Miguel},
  doi          = {10.1007/s10851-024-01222-w},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {On the axial symmetry of 2D star-shaped sets},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A structured l-BFGS method with diagonal scaling and its
application to image registration. <em>JMIV</em>, <em>67</em>(1), 1–20.
(<a href="https://doi.org/10.1007/s10851-024-01215-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise an L-BFGS method for optimization problems in which the objective is the sum of two functions, where the Hessian of the first function is computationally unavailable while the Hessian of the second function has a computationally available approximation that allows for cheap matrix–vector products. This is a prototypical setting for many inverse problems. The proposed L-BFGS method exploits the structure of the objective to construct a more accurate Hessian approximation than in standard L-BFGS. In contrast with existing works on structured L-BFGS, we choose the first part of the seed matrix, which approximates the Hessian of the first function, as a diagonal matrix rather than a multiple of the identity. We derive two suitable formulas for the coefficients of the diagonal matrix and show that this boosts performance on real-life image registration problems, which are highly non-convex inverse problems. The new method converges globally and linearly on non-convex problems under mild assumptions in a general Hilbert space setting, making it applicable to a broad class of inverse problems. An implementation of the method is freely available.},
  archive      = {J_JMIV},
  author       = {Mannel, Florian and Aggrawal, Hari Om},
  doi          = {10.1007/s10851-024-01215-9},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {A structured L-BFGS method with diagonal scaling and its application to image registration},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cartoon–texture image decomposition using least squares and
low-rank regularization. <em>JMIV</em>, <em>67</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10851-024-01216-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel model for the decomposition of cartoon–texture images, which integrates the edge-aware weighted least squares (WLS) with low-rank regularization. Unlike conventional methodologies that depend on total variation-based penalty functions, our model represents cartoon images using an edge-preserving WLS penalty. This approach effectively enhances edges and suppresses texture through iterative updates of an edge-preserving weight matrix. For the texture component, we introduce a low-rank penalty function to capture the structured regularity of texture patterns. By leveraging the repetitive nature of texture, our low-rank models can accurately represent these components. We employ a prediction–correction approach based on a three-block separable alternating direction multiplier method to solve the minimization problem, providing closed-form solutions for all subproblems. We also provide a convergence proof for the proposed algorithm. Numerical experiments validate the efficacy of our proposed method in successfully separating cartoon and texture components while preserving edges.},
  archive      = {J_JMIV},
  author       = {Li, Kexin and Wen, You-wei and Chan, Raymond H.},
  doi          = {10.1007/s10851-024-01216-8},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Cartoon–Texture image decomposition using least squares and low-rank regularization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy circularity: A new fuzzy shape-based descriptor of the
object. <em>JMIV</em>, <em>67</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10851-024-01217-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new family of fuzzy shape measures, called fuzzy circularity, to evaluate the degree to which a considered fuzzy shape matches a fuzzy disk. A new family of fuzzy shape-based measures ranges the interval (0, 1] where a maximum value equal to 1 is reached if and only if the shape under consideration is a fuzzy disk. This family is theoretically well grounded having the behavior that corresponds to human perception and can be predicted in advance. Additionally, a new family of fuzzy shape-based measures is invariant to rotation, translation, and scaling of the considered fuzzy shape. Various experiments on both synthetically generated and real images are included to provide a better understanding of the behavior of the new measures and to confirm the theoretically proven results. The performance of the new family of fuzzy circularity is extensively tested on several standard, well-known image datasets such as MPEG-7 CE-1, Animal, Swedish Leaf, and Galaxy Zoo datasets. Experimental evaluations also illustrate the effectiveness and advantages of the new shape descriptors in various object classification and recognition tasks by comparing them with other known analysis approaches.},
  archive      = {J_JMIV},
  author       = {Ilić, Vladimir and Ralević, Nebojša M.},
  doi          = {10.1007/s10851-024-01217-7},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Fuzzy circularity: A new fuzzy shape-based descriptor of the object},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind image deconvolution: When patch-wise minimal pixels
prior meets fractional-order method. <em>JMIV</em>, <em>67</em>(1),
1–20. (<a href="https://doi.org/10.1007/s10851-024-01221-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image deconvolution is a challenging issue in image processing. In blind image deconvolution, the typical approach involves iteratively estimating both the blur kernel and latent image until convergence to the blur kernel of the observed image is achieved. Recently, several approaches have been attempted to develop a sophisticated regularization to obtain the clean image. However, existing methods often struggle to effectively handle ringing artifacts and local blur. To overcome these limitations, we introduce a fractional-order variational model. This model alleviates the ringing artifacts through the selection of an optimal derivative. Subsequently, to refine the latent image further, we leverage the local prior, namely patch-wise minimal pixels (PMP) prior. Since the PMP prior of clean images blocks is much sparser than that of blurred ones, it is capable of discriminating between clean and blurred image blocks. We illustrate the effective integration of the fractional-order operations and the PMP prior within our proposed approach. Moreover, the convergence of our algorithm has been proved as the values of the objective function monotonically decrease. Extensive experiments on different datasets demonstrate the superiority of the proposed method compared with other methods in terms of reconstruction quality for blind deconvolution.},
  archive      = {J_JMIV},
  author       = {Wu, Tingting and Wan, Shaojie and Feng, Chenchen and Zhang, Hao and Zeng, Tieyong},
  doi          = {10.1007/s10851-024-01221-x},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Blind image deconvolution: When patch-wise minimal pixels prior meets fractional-order method},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic diffusion in riemannian colour geometry.
<em>JMIV</em>, <em>67</em>(1), 1–10. (<a
href="https://doi.org/10.1007/s10851-024-01223-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anisotropic diffusion has long been an important tool in image processing. More recently, it has also found its way to colour imaging. Until now, mainly Euclidean colour spaces have been considered in this context, but recent years have seen a renewed interest in and importance of non-Euclidean colour geometry. The main contribution of this paper is the derivation of the equations for anisotropic diffusion in Riemannian colour geometry. It is demonstrated that it contains several well-known solutions such as Perona–Malik diffusion and Tschumperlé–Deriche diffusion as special cases. Furthermore, it is shown how it is non-trivially connected to Sochen’s general framework for low-level vision. The main significance of the method is that it decouples the coordinates used for solving the diffusion equation from the ones that define the metric of the colour manifold, and thus directs the magnitude and direction of the diffusion through the diffusion tensor. It also enables the use of non-Euclidean colour manifolds and metrics for applications such as denoising, inpainting, and demosaicing, based on anisotropic diffusion.},
  archive      = {J_JMIV},
  author       = {Farup, Ivar and Rivertz, Hans Jakob},
  doi          = {10.1007/s10851-024-01223-9},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {Anisotropic diffusion in riemannian colour geometry},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New properties for full convex sets and full convex hulls.
<em>JMIV</em>, <em>67</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10851-024-01225-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Full convexity has been recently proposed as an alternative definition of digital convexity in $$\mathbb {Z}^n$$ . In contrast to classical definitions, fully convex sets are always connected and even simply connected whatever the dimension, while remaining digitally convex in the usual sense. In order to better understand the properties of full convexity, we present here two new and radically different characterizations of full convexity. The first one mimics the usual continuous convexity via segments inclusion. We show an equivalence of full convexity with this segment convexity in dimensions 1 and 2, and counterexamples starting from dimension 3. If we now ask that the cells touched by all d-simplices (instead of just 2-simplices aka segments) are within the cells touched by the digital set, we achieve an equivalence in arbitrary dimension d. The second characterization is recursive with respect to the dimension and relies on convexity of its axis-aligned projections. We provide several applications of these characterizations: the full convexity of balls and subsets of the hypercube, a natural measure of full convexity for digital sets, a new and faster algorithm to check the full convexity of digital sets. Finally, we study the main drawback of full convexity: Its envelope operator may not be an increasing operator. We characterize fully convex sets that have anti-monotonous subsets, and we show that they must be thin in a precise sense.},
  archive      = {J_JMIV},
  author       = {Feschet, Fabien and Lachaud, Jacques-Olivier},
  doi          = {10.1007/s10851-024-01225-7},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {New properties for full convex sets and full convex hulls},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new insight into the epipole from four point
correspondences in two calibrated views. <em>JMIV</em>, <em>67</em>(1),
1–12. (<a href="https://doi.org/10.1007/s10851-024-01227-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new insight into the epipole from four given image point correspondences in two calibrated views. Firstly, we propose an algebraic constraint on the relation between the epipole and a plane-induced homography. Secondly, we show a novel algorithm for determining the 10-degree curve about possible epipoles from four image point correspondences in two calibrated views by using algebraic method directly. In particular, we show that the problem of determining the epipole has at most four distinct real solutions when the left image plane is parallel to a certain face of the tetrahedron consisting of four control points. Furthermore, we also confirm that the upper bound of four distinct physically valid solutions is attainable. Lastly, we give some examples to validate our results.},
  archive      = {J_JMIV},
  author       = {Guo, Yang and Qu, Yingqi},
  doi          = {10.1007/s10851-024-01227-5},
  journal      = {Journal of Mathematical Imaging and Vision},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Math. Imaging Vis.},
  title        = {A new insight into the epipole from four point correspondences in two calibrated views},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jmui---7">JMUI - 7</h2>
<ul>
<li><details>
<summary>
(2025). Impact of communication modalities on social presence and
regulation processes in a collaborative game. <em>JMUI</em>,
<em>19</em>(1), 101–118. (<a
href="https://doi.org/10.1007/s12193-024-00450-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, leveraging communication technologies to foster collaborative learning is of utmost importance. This study explores the impact of different communication modalities, such as text, audio and video, on social presence and regulation processes within a computer-supported collaborative learning (CSCL) environment. Using learning analytics, we examine the influences of these modalities on collaboration and derive recommendations for their optimized use in the design of future CSCL environments. Our findings reveal a significant impact of communication modalities on the sense of social presence and regulation of collaborative activities. Audio communication results in enhanced co-presence, psychobehavioral accessibility, and better regulation processes compared to video and text modalities, indicating that audio is the most suitable modality in collaborative virtual environments for decision-making tasks. Conversely, video communication still facilitated strategic planning and enhanced self-regulation. Chat communication showed the lowest sense of social presence, yet improvements over time suggest that participants adapt to this modality, enhancing their collaborative efficiency.},
  archive      = {J_JMUI},
  author       = {Basille, Anthony and Lavoué, Élise and Serna, Audrey},
  doi          = {10.1007/s12193-024-00450-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {101-118},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Impact of communication modalities on social presence and regulation processes in a collaborative game},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vibration feedback reduces perceived difficulty of
virtualized fine motor task. <em>JMUI</em>, <em>19</em>(1), 93–99. (<a
href="https://doi.org/10.1007/s12193-024-00449-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has been increasingly used in the development or rehabilitation of sensorimotor skills as it provides a safe, personalized, repeatable, realistic, and interactive environment. However, the use of VR technology to simulate fine motor interactions is still rather limited. This study evaluated the performance and user experience of a virtualized fine motor task and the potential impact of vibration feedback to complement the VR simulation. The Nine Hole Peg test (NHPT), which is widely used in health care to assess hand motor functions, was considered. 100 healthy subjects were recruited to compare the performance of the conventional, VR-based, and VR-based with vibration feedback (VR+vibration) implementation of the NHPT. Results demonstrated a significant increase in the task execution time (about 50% increase) in VR-based and VR+vibration conditions as compared to the conventional condition (Kruskal Wallis test, Bonferroni correction, p &lt; 0.0001). Participants reported a significant decrease in perceived difficulty of the VR+vibration condition as compared to the VR-based condition (Wilcoxon signed-rank test, p &lt; 0.05). Another interesting finding was the gender effect - female participants spent significantly more time completing the task in VR as compared to their male counterparts. These results indicate that vibration feedback enhances the usability of virtualized fine motor tasks.},
  archive      = {J_JMUI},
  author       = {Park, Wanjoo and Jamil, Muhammad Hassan and Eid, Mohamad},
  doi          = {10.1007/s12193-024-00449-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {93-99},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Vibration feedback reduces perceived difficulty of virtualized fine motor task},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pointing gestures accelerate collaborative problem-solving
on tangible user interfaces. <em>JMUI</em>, <em>19</em>(1), 75–92. (<a
href="https://doi.org/10.1007/s12193-024-00448-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaboration is included in the Learning and Innovation skills of the 21st Century. Collaborative problem-solving represents the interaction of two dimensions i) complex problem-solving and ii) collaboration. Technology-based assessment of collaborative problem-solving should focus on both dimensions. We ran user studies with 66 participants at three secondary schools in Luxembourg and Belgium to observe the gestural user behaviour of triads while solving a collaborative problem on a tangible user interface (TUI). Social interactions and embodiment by using gestures are important collaboration channels. Our main objective is the relation between the usage of gestures with collaboration and complex problem-solving performance. Our apparatus to test collaborative problem-solving is a tangible tabletop and a micro-world about power plants. We analysed the videos manually and found correlations between gestures, complex problem-solving performance, and user experience. The results showed that pointing gestures and adaptors significantly correlate with response time of problem-solving. Our results on user experience showed that the use of a TUI was regarded as a novel and straightforward solution that many people could learn to use very quickly. We suggest gesture performance to be considered as one indicator of collaboration.},
  archive      = {J_JMUI},
  author       = {Anastasiou, Dimitra and Maquil, Valérie},
  doi          = {10.1007/s12193-024-00448-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {75-92},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Pointing gestures accelerate collaborative problem-solving on tangible user interfaces},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented conversations: AR face filters for facilitating
comfortable in-person interactions. <em>JMUI</em>, <em>19</em>(1),
57–74. (<a href="https://doi.org/10.1007/s12193-024-00446-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals with social anxiety often experience heightened anxiety during face-to-face conversations due to their fear of negative judgments when perceiving neutral facial expressions from others. This research aims to alleviate this anxiety by introducing a novel approach that involves overlaying visual effects onto the conversation partner via an augmented reality head-mounted display. We developed an AR application for HoloLens 2, allowing users to overlay either an anime-style avatar or a smiling face photo during in-person interactions. We conducted a user study where participants engaged in dyadic conversations using our AR application. 29 participants compared three conditions: control, anime-style avatar, and smiling face photo. The findings reveal two significant outcomes: (1) overlaying an anime-style avatar onto the conversation partner enhances conversational comfort, and (2) individuals with pronounced social interaction anxiety and intense fear of negative evaluation benefit from our AR-based system. This research presents possibilities for practical solutions that could improve the well-being of individuals with social anxiety during in-person conversations.},
  archive      = {J_JMUI},
  author       = {Yoneyama, Juri and Fujimoto, Yuichiro and Okazaki, Kosuke and Sawabe, Taishi and Kanbara, Masayuki and Kato, Hirokazu},
  doi          = {10.1007/s12193-024-00446-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {57-74},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Augmented conversations: AR face filters for facilitating comfortable in-person interactions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effects of haptic, visual and olfactory augmentations on
food consumed while wearing an extended reality headset. <em>JMUI</em>,
<em>19</em>(1), 37–55. (<a
href="https://doi.org/10.1007/s12193-024-00447-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current food production system is unsustainable, necessitating a shift towards plant-based diets. Nutritious options fulfill basic needs but may not satisfy hedonic ones. Our novel approach is to promote healthier eating habits without compromising on the pleasantness of eating by using extended reality technologies and multimodal interaction. We present a multisensory augmentation system integrating augmentations in olfaction, touch, and vision. We studied the experience of eating plant-based balls and meatballs. In an experiment with 40 participants, haptic and visual augmentations were found to have significant effects: augmented meatballs and plant-based balls were perceived as bigger and heavier compared to non-augmented versions. However, olfactory augmentation did not produce a similar effect: participants did not notice a stronger aroma with augmented balls compared to non-augmented balls, and the augmented plant-based version had a less appealing scent than its non-augmented counterpart. Moreover, the findings of the study indicate that our multisensory augmentation system had no significant effect on taste perception.},
  archive      = {J_JMUI},
  author       = {Karhu, Natalia and Rantala, Jussi and Farooq, Ahmed and Sand, Antti and Pennanen, Kyösti and Lappi, Jenni and Nayak, Mohit and Sozer, Nesli and Raisamo, Roope},
  doi          = {10.1007/s12193-024-00447-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {37-55},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The effects of haptic, visual and olfactory augmentations on food consumed while wearing an extended reality headset},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and testing of (a)MICO: A multimodal feedback system
to facilitate the interaction between cobot and human operator.
<em>JMUI</em>, <em>19</em>(1), 21–36. (<a
href="https://doi.org/10.1007/s12193-024-00444-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work describes the design, development and testing of a multimodal feedback system, named (A)MICO, with visual and acoustic feedback designed to facilitate the interaction of workers with collaborative robots (cobots) in production lines. The feedback is designed to make the human operator more aware of the cobot’s ongoing and future activities, and therefore gain more control over the situation. The ultimate goal is to obtain a new intuitive mode for transferring information through the combination of lights and sounds, not only to facilitate the flow of communication from the cobot to the operator, but also to make the interaction more accessible to neurodivergent groups, such as people with autism spectrum disorders. The design process focused on the evaluation of the human–robot interaction to select the situations where additional information is needed, and which is the best way to transfer messages as intuitively as possible. Potential end-users were actively involved during all stages of the design and development process. Five volunteers with high functioning autism participated in a preliminary co-design to identify the issues related to the interaction with the cobot and the logic of the multimodal signals. Then, to assess the system’s adaptability to several needs and the level of usability in providing information, validation tests were carried out involving a wider group of participants with ASD. The results suggest that the adoption of a multimodal communication strategy can be useful for making the workplace accessible and improving the well-being of all workers.},
  archive      = {J_JMUI},
  author       = {Dei, Carla and Meregalli Falerni, Matteo and Cilsal, Turgut and Redaelli, Davide Felice and Lavit Nicora, Matteo and Chiappini, Mattia and Storm, Fabio Alexander and Malosio, Matteo},
  doi          = {10.1007/s12193-024-00444-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {21-36},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Design and testing of (A)MICO: A multimodal feedback system to facilitate the interaction between cobot and human operator},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of comparative evaluation techniques for signing
agents: A study with deaf adults. <em>JMUI</em>, <em>19</em>(1), 1–19.
(<a href="https://doi.org/10.1007/s12193-024-00442-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign languages are considered fully-fledged and complete natural languages that are utilized by individuals who are deaf or hard of hearing as a means of communication within the visual-gestural modality. The utilization of virtual avatars as virtual assistants has witnessed a notable surge over the course of the previous fifteen years. Research on sign language recognition has already shown significant potential in achieving reliable and efficient automatic sign language recognition. Nevertheless, the development of physiologically believable (naturally looking) sign language synthesis and generation techniques is currently in its nascent stages. Moreover, traditional models often are rule-based, rely on manually programmed commands, and require the expertise of proficient interpreters, whereas data-driven approaches have the potential to offer more advanced solutions. In addition to the advancement of sign language systems, scholarly investigations indicate a notable lack in the signing systems evaluation by individuals who utilize sign language (deaf signers and interpreters). In this study, we introduce a sign language interpreting avatar based on data-driven techniques. Additionally, we conduct a subjective evaluation of the avatar’s performance. This paper presents the findings of a study conducted with deaf signers, which aimed to compare three different signing agents to a highly skilled sign language human interpreter. The study utilized well-known metrics that are considered to provide valuable insights into participants’ perceptions of signing agents, also their respective advantages and limitations.},
  archive      = {J_JMUI},
  author       = {Imashev, Alfarabi and Oralbayeva, Nurziya and Baizhanova, Gulmira and Sandygulova, Anara},
  doi          = {10.1007/s12193-024-00442-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Assessment of comparative evaluation techniques for signing agents: A study with deaf adults},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="joh---17">JOH - 17</h2>
<ul>
<li><details>
<summary>
(2025). Lagrangian-based heuristics for production planning with
perishable products, scarce resources, and sequence-dependent setup
times. <em>JOH</em>, <em>31</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s10732-024-09539-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a lot-sizing and scheduling problem apparent in the food industry that stemmed originally from the Brazilian meat production sector. More specifically, we consider a production environment in which various production lines share a set of scarce production resources. Therefore, only a subset of the existing production lines can simultaneously operate in each period under the limitations of the availability of resources. Moreover, we consider sequence-dependent setup times and costs, significant inventory holding costs, backlogging, and perishable products. The problem is formulated as a mixed integer programming model, and we propose four Lagrangian-based heuristics to find high-quality solutions for challenging instances. A computational study shows that proposed approaches are very competitive in solving the problem, outperforming methods already established in the literature.},
  archive      = {J_JOH},
  author       = {Soler, Willy A. Oliveira and Santos, Maristela O. and Akartunalı, Kerem},
  doi          = {10.1007/s10732-024-09539-w},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-35},
  shortjournal = {J. Heuristics},
  title        = {Lagrangian-based heuristics for production planning with perishable products, scarce resources, and sequence-dependent setup times},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridizing constraint programming and meta-heuristics for
multi-mode resource-constrained multiple projects scheduling problem.
<em>JOH</em>, <em>31</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10732-024-09540-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Mode Resource-Constrained Multiple Projects Scheduling Problem (MMRCMPSP) is an important combinatorial optimization problem for both real-world situations in industry and academic research. Its objective is to find the best schedule for activities across multiple projects that can be executed in different modes. The schedule must consider shared resource availability and satisfy precedence and time constraints. To tackle this problem, we propose a hybrid approach that combines constraint programming (CP) with meta-heuristic algorithms. We introduce and assess a CP model that incorporates all MMRCMPSP constraints. By leveraging the strengths of CP and meta-heuristics, our approach yields new upper bounds for various MMRCMPSP benchmark instances. Additionally, we evaluate our method using existing benchmark instances for single-project scheduling problems with multiple modes and provide improved solutions for many of them.},
  archive      = {J_JOH},
  author       = {Ahmeti, Arben and Musliu, Nysret},
  doi          = {10.1007/s10732-024-09540-3},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Heuristics},
  title        = {Hybridizing constraint programming and meta-heuristics for multi-mode resource-constrained multiple projects scheduling problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-echelon van-robot routing problem with sharing-curbside
satellites. <em>JOH</em>, <em>31</em>(1), 1–35. (<a
href="https://doi.org/10.1007/s10732-024-09541-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High population density and commercial-activity density in urban areas make land use for urban logistics systems even more challenging. Herein, a concept named sharing-curbside satellite (SS) is involved in two-echelon city logistics systems. Traditional vans are deployed in the 1st-echelon network, whereas ground-based robots are employed in the 2nd-echelon network. As a type of nondedicated satellites, the SS shares curbside spaces with the local traffic flow, and each SS can have multiple time windows for direct transshipment between vans and robots. The SS can provide a new mode for urban deliveries through temporary and nondedicated satellites at the neighborhood level. In this study, the two-echelon van-robot routing problem with SSs (2ERP-SS) is defined. The SS synchronization involves vans being used as part of SSs, each SS has multiple time windows, cargoes are transshipped directly between vans and robots, and the available transshipment capacity decreases over time. We develop a mixed-integer linear programming model. We provide a large neighborhood search (LNS) combined with a beam search algorithm, and employ an adaptive LNS (ALNS) for comparison. The effectiveness of the mathematical formulation and heuristics are evaluated through computational experiments, and practical management insights are elucidated.},
  archive      = {J_JOH},
  author       = {Li, Hongqi and Wang, Feilong and Xiong, Hanxi and Wang, Zhiqi},
  doi          = {10.1007/s10732-024-09541-2},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-35},
  shortjournal = {J. Heuristics},
  title        = {Two-echelon van-robot routing problem with sharing-curbside satellites},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptative multi-objective scatter search for solving the
dynamic bin packing problem. <em>JOH</em>, <em>31</em>(1), 1–69. (<a
href="https://doi.org/10.1007/s10732-024-09537-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the dynamic multi-objective bin packing problem, a combinatorial optimization problem within the cutting/packing problems family. A solution to this problem involves packing cooling cookies into boxes while following a specific production process. In such a case, items, each with its own capacity, arrive in batches at racks. The objective is to optimize (i) the number of boxes used, (ii) the average initial heat associated with each box, and (iii) the maximum time taken to move all boxes to the storefront. This problem is addressed through an adaptive multi-objective scatter search applied to a special dynamic bin packing problem. The method begins by constructing an initial Pareto front set, specifically, the initial reference set containing diversified solutions based on the three objective functions related to the studied problem. The subsequent stages of the method operate in three collaborative phases: (i) improving the reference set by optimizing a highlighted objective function, (ii) refining the final reference set by prioritizing the optimization of average initial heat, and (iii) addressing the objective related to the maximum time to establish the final solution. Hence, to facilitate the transition between the aforementioned three phases, a hybridization between the solution combination method and the reference update method is introduced. Finally, the proposed method’s performance is evaluated using benchmark and newly generated instances and compared with recent methods. The study includes both qualitative and quantitative analyses. To identify the best-performing method among those tested, three statistical tests are conducted: the Student’s t-test, the Sign test, and the Wilcoxon signed-rank test. Additionally, performance metrics such as the $$\varepsilon $$ -test, the binary coverage measure, and the net front contribution indicator are used for evaluation. The results achieved by the proposed method surpass those published in the literature, highlighting the method’s strength and effectiveness, and establishing it as a competitive solution for the dynamic multi-objective bin-packing problem.},
  archive      = {J_JOH},
  author       = {Aïder, Méziane and Boulebene, Sabrin and Hifi, Mhand},
  doi          = {10.1007/s10732-024-09537-y},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-69},
  shortjournal = {J. Heuristics},
  title        = {An adaptative multi-objective scatter search for solving the dynamic bin packing problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient backtracking heuristic for the resource
allocation problem with compatibility and exclusivity constraints.
<em>JOH</em>, <em>31</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10732-024-09538-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a resource allocation problem featuring specific constraints, called exclusivity constraints, in addition to regular compatibility constraints. Resources are to be allocated to independent modules, each having a list of compatible resources. A resource can be allocated to several modules. However, some modules exhibit exclusivity constraints, requiring each of them to be allocated to one dedicated compatible resource, not shared with any other module. Such a resource allocation problem arises in the deployment of simulation modules on computational resources in a distributed simulation platform, where the simulation requester may require some modules to be allocated to dedicated resources for a better soft real-time execution, or for instrumentation purposes. In this paper, we introduce the problem of resource allocation with compatibility and exclusivity constraints and show it reduces to the list-coloring problem in a threshold graph. We deduce that our problem is NP-complete in the general case, while it can be solved in polynomial time, in two special cases. We propose a heuristic backtracking algorithm enhanced by pruning rules and exploiting the subproblems’ special structure. Compared to four list coloring heuristics adapted to our problem, our heuristic algorithm can be considered as the method of choice to find high-quality solutions in short computing times.},
  archive      = {J_JOH},
  author       = {Khassiba, Ahmed},
  doi          = {10.1007/s10732-024-09538-x},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Heuristics},
  title        = {An efficient backtracking heuristic for the resource allocation problem with compatibility and exclusivity constraints},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 30 years of the journal of heuristics: A bibliometric
analysis. <em>JOH</em>, <em>31</em>(1), 1–55. (<a
href="https://doi.org/10.1007/s10732-024-09542-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Journal of Heuristics is an international journal that provides a forum to solve complex problems using heuristic solution tools. The journal was created in 1995, and in 2025, celebrates its 30th anniversary. Motivated by this special event, this article presents a bibliometric analysis of the journal. The article examines the journal publication and citation structure using the Scopus database, considering a wide range of issues including the most cited documents, productive authors, institutions, and countries. The work also develops a graphical visualization of the bibliographic data by using the VOS viewer software. This approach is studied with different bibliometric measures such as bibliographic coupling, co-citation, and co-occurrence of keywords. The results show that the Journal of Heuristics has maintained a solid quality of its publications over the years. Currently, the most popular topics are connected to heuristics, metaheuristics, local search, and tabu search. Researchers from the USA are the most productive. But countries such as France, the UK, Spain and Canada, have also a significant productivity in the journal.},
  archive      = {J_JOH},
  author       = {Flores-Sosa, Martha and Merigó, José M. and Sanchez-Valenzuela, Kenia},
  doi          = {10.1007/s10732-024-09542-1},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-55},
  shortjournal = {J. Heuristics},
  title        = {30 years of the journal of heuristics: A bibliometric analysis},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A memetic algorithm for the flexible periodic vehicle
routing problem. <em>JOH</em>, <em>31</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10732-024-09536-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexible periodic vehicle routing problem (FPVRP) is an extension of the classic VRP in which customers are visited periodically in the course of a given time horizon. Periodicity and flexibility in providing services are two main features of the FPVRP, which introduce new challenges in solving this problem. To the best of our knowledge, there is only one single-solution-based algorithm in the literature for solving the FPVRP. In this paper, another technique called the MA-FPVRP is proposed for tackling this problem. Our population-based approach is a memetic algorithm made up of two main components: a genetic procedure that aims to find a suitable sequence of visits for each route, and a local search procedure consisting of four moves whose prime goal is to guide the search toward the promising areas. Considering 45 standard benchmark instances with unknown optimal solutions, the MA-FPVRP outperforms the preceding method in terms of both the solution quality and execution time. Using this evolutionary scheme, the average relative percent deviation (RPD) to the best-known solution values decreases from 0.52% to 0.15%, and the average execution time improves by approximately 31%. For all 10 large instances of the FPVRP, new best results are found using our algorithm. Considering the remaining 35 small and medium-size instances, our approach is superior to the previous method in terms of the solution quality, and it is more than two times faster. Besides, using the MA-FPVRP, new best solutions are obtained for 5 out of these 35 instances.},
  archive      = {J_JOH},
  author       = {Amiri, Banafsheh and Ziarati, Koorush and Sohrabi, Somayeh},
  doi          = {10.1007/s10732-024-09536-z},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Heuristics},
  title        = {A memetic algorithm for the flexible periodic vehicle routing problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming last mile delivery with heterogeneous
assistants: Drones and delivery robots. <em>JOH</em>, <em>31</em>(1),
1–42. (<a href="https://doi.org/10.1007/s10732-024-09543-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid global expansion of e-commerce and the increasing number of online shoppers, logistics service providers (LSPs) are exploring sustainable solutions to meet the rising demand. Thanks to developments in automation and robotic technologies, LSPs have now the opportunity to enhance their operations through the deployment of autonomous delivery solutions like drones and delivery robots. This paper investigates a practical delivery system to integrate these emerging technologies simultaneously into conventional van-only delivery system. Additionally, the effects of various assistant characteristics on operations are examined through broader assumptions. We introduce a mathematical model aiming to minimize delivery makespan and explore various valid inequalities to mitigate its complexity. A new hybrid metaheuristic algorithm combining genetic algorithm and large neighborhood search algorithm is also proposed for large scale instances. A three-layer coding and encoding method is also introduced for genetic algorithm to manage the complex structure of the problem. Finally, extensive numerical experiments are conducted to show the effectiveness of valid inequalities and the algorithm. The sensitivity analyses provide comparisons of various delivery configurations and offer valuable insights for the logistics industry to integrate these innovative delivery solutions into their daily operations. In our experiments, using a single drone reduces total delivery times by up to 23.57%, while a single robot contributes to a 37.19% improvement in the objective. The heterogeneous configuration offers a substantial 49.71% improvement compared to using only vans for deliveries.},
  archive      = {J_JOH},
  author       = {Chen, Cheng and Demir, Emrah and Hu, Xisheng and Huang, Hainan},
  doi          = {10.1007/s10732-024-09543-0},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-42},
  shortjournal = {J. Heuristics},
  title        = {Transforming last mile delivery with heterogeneous assistants: Drones and delivery robots},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary optimization of the area under precision-recall
curve for classifying imbalanced multi-class data. <em>JOH</em>,
<em>31</em>(1), 1–66. (<a
href="https://doi.org/10.1007/s10732-024-09544-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of imbalanced multi-class data is still so far one of the most challenging issues in machine learning and data mining. This task becomes more serious when classes containing fewer instances are located in overlapping regions. Several approaches have been proposed through the literature to deal with these two issues such as the use of decomposition, the design of ensembles, the employment of misclassification costs, and the development of ad-hoc strategies. Despite these efforts, the number of existing works dealing with the imbalance in multi-class data is much reduced compared to the case of binary classification. Moreover, existing approaches still suffer from many limits. These limitations include difficulties in handling imbalances across multiple classes, challenges in adapting sampling techniques, limitations of certain classifiers, the need for specialized evaluation metrics, the complexity of data representation, and increased computational costs. Motivated by these observations, we propose a multi-objective evolutionary induction approach that evolves a population of NLM-DTs (Non-Linear Multivariate Decision Trees) using the $$\theta $$ -NSGA-III ( $$\theta $$ -Non-dominated Sorting Genetic Algorithm-III) as a search engine. The resulting algorithm is termed EMO-NLM-DT (Evolutionary Multi-objective Optimization of NLM-DTs) and is designed to optimize the construction of NLM-DTs for imbalanced multi-class data classification by simultaneously maximizing both the Macro-Average-Precision and the Macro-Average-Recall as two possibly conflicting objectives. The choice of these two measures as objective functions is motivated by a recent study on the appropriateness of performance metrics for imbalanced data classification, which suggests that the mAURPC (mean Area Under Recall Precision Curve) satisfies all necessary conditions for imbalanced multi-class classification. Moreover, the NLM-DT adoption as a baseline classifier to be optimized allows the generation non-linear hyperplanes that are well-adapted to the classes ‘boundaries’ geometrical shapes. The statistical analysis of the comparative experimental results on more than twenty imbalanced multi-class data sets reveals the outperformance of EMO-NLM-DT in building NLM-DTs that are highly effective in classifying imbalanced multi-class data compared to seven relevant and recent state-of-the-art methods.},
  archive      = {J_JOH},
  author       = {Chabbouh, Marwa and Bechikh, Slim and Mezura-Montes, Efrén and Ben Said, Lamjed},
  doi          = {10.1007/s10732-024-09544-z},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-66},
  shortjournal = {J. Heuristics},
  title        = {Evolutionary optimization of the area under precision-recall curve for classifying imbalanced multi-class data},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A q-learning-based algorithm for the block relocation
problem. <em>JOH</em>, <em>31</em>(1), 1–41. (<a
href="https://doi.org/10.1007/s10732-024-09545-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Block Relocation Problem (BRP), also known as the Container Relocation Problem, is a challenging combinatorial optimization problem in block stacking systems and has many applications in real-world scenarios such as logistics and manufacturing industry. The BRP is about finding the optimal way to retrieve blocks from a storage area with the objective of minimizing the number of relocations. The BRPs have been studied for a long time, and have been solved primarily using conventional optimization techniques, including mathematical programming models, as well as both exact and heuristic algorithms. For the first time, this paper tackles the problem using a reinforcement learning method. We focus on one of the major variants of the BRP—the restricted BRP with duplicate priorities (RBRP-dup). We first model the RBRP-dup as a Markov decision process and then propose a Q-learning-based algorithm to solve the problem. The Q-learning-based algorithm contains two phases. In the learning phase, two innovative mechanisms: an optimal rule-integrated behaviour policy and a heuristic-based dynamic initialization method, are incorporated into the Q-learning model to reduce the size of the state-action space and accelerate convergence. In the optimization phase, the insights obtained in the learning phase are combined with a heuristic algorithm to improve decision-making. The performance of our proposed method is evaluated against the state-of-the-art exact algorithm and a commonly used heuristic algorithm based on benchmark instances from the literature. The computational experiments demonstrate the superiority of our proposed method regarding solution quality in large and complex instances.},
  archive      = {J_JOH},
  author       = {Liu, Liqun and Feng, Yuanjun and Zeng, Qingcheng and Chen, Zhijun and Li, Yaqiu},
  doi          = {10.1007/s10732-024-09545-y},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-41},
  shortjournal = {J. Heuristics},
  title        = {A Q-learning-based algorithm for the block relocation problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heuristic algorithm to improving the coil slitting process
in the steel industry. <em>JOH</em>, <em>31</em>(1), 1–38. (<a
href="https://doi.org/10.1007/s10732-024-09546-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The steel industry is constantly facing problems and challenges that require optimisation to improve the production process. We present an algorithm to address a major challenge, the slitting problem, for a specific Spanish company. This problem arises when large steel coils need to be cut into smaller strips. Given the highly heterogeneous stock (coils come from previous operations), selecting the most suitable coils and defining the cutting patterns become very complicated due to operational and customer constraints. The company aims to reduce the leftovers and increase the service level (the difference between the weight requested by the customer and the weight supplied). The algorithm is currently in production and was validated using the company’s data and compared with an exact model. Results significantly improved the company’s operations, achieving a 50% reduction in leftovers and a much better service level in minutes, as opposed to the hours the company previously required. Although there are Mixed Integer Linear Optimization models that provide an optimal solution in small cases, they are not a viable alternative for the company because they require excessive computational time (even, in some cases, to obtain feasible solutions) and use overly expensive commercial solvers.},
  archive      = {J_JOH},
  author       = {Soto-Sánchez, Óscar and Sierra-Paradinas, María and Gallego, Micael and Alonso-Ayuso, Antonio and Gortázar, Francisco},
  doi          = {10.1007/s10732-024-09546-x},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-38},
  shortjournal = {J. Heuristics},
  title        = {A heuristic algorithm to improving the coil slitting process in the steel industry},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A local branching-based solution for the multi-period
cutting stock problem with tardiness, earliness, and setup costs.
<em>JOH</em>, <em>31</em>(1), 1–57. (<a
href="https://doi.org/10.1007/s10732-025-09547-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the Multi-Period Cutting Stock Problem with Due Dates and Setups (MPCSPDDS), an extension of the classical one-dimensional Cutting Stock Problem (CSP). The MPCSPDDS considers the due dates specified in cutting orders’ requests and setups required for transitioning between different cutting patterns. The challenge lies in minimizing tardiness and earliness during production, considering these as detrimental factors. Additionally, the proposed model assumes that a setup is necessary for the cutting machine when switching patterns. The contribution of this paper includes the proposition of an integer mathematical programming model and a matheuristic solution approach for two variants of the MPCSPDDS, employing column generation, a round-up heuristic, and the local branching matheuristic. Computational experiments show that our proposed solution method consistently yields, on average, high-quality feasible solutions compared to employing column generation and solving the problem with the generated columns using the CPLEX solver while maintaining a low computational cost.},
  archive      = {J_JOH},
  author       = {de Araújo Silva Oliveira, Elisama and Wanner, Elizabeth and de Sá, Elisangela Martins and de Souza, Sérgio Ricardo},
  doi          = {10.1007/s10732-025-09547-4},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-57},
  shortjournal = {J. Heuristics},
  title        = {A local branching-based solution for the multi-period cutting stock problem with tardiness, earliness, and setup costs},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristics for the run-length encoded burrows–wheeler
transform alphabet ordering problem. <em>JOH</em>, <em>31</em>(1), 1–29.
(<a href="https://doi.org/10.1007/s10732-025-09548-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Burrows–Wheeler Transform (BWT) is a string transformation technique widely used in areas such as bioinformatics and file compression. Many applications combine a run-length encoding (RLE) with the BWT in a way which preserves the ability to query the compressed data efficiently. However, these methods may not take full advantage of the compressibility of the BWT as they do not modify the alphabet ordering for the sorting step embedded in computing the BWT. Indeed, any such alteration of the alphabet ordering can have a considerable impact on the output of the BWT, in particular on the number of runs. For an alphabet $$\Sigma $$ containing $$\sigma $$ characters, the space of all alphabet orderings is of size $$\sigma !$$ . While for small alphabets an exhaustive investigation is possible, finding the optimal ordering for larger alphabets is not feasible. Therefore, there is a need for a more informed search strategy than brute-force sampling the entire space, which motivates a new heuristic approach. In this paper, we explore the non-trivial cases for the problem of minimizing the size of a run-length encoded BWT (RLBWT) via selecting a new ordering for the alphabet. We show that random sampling of the space of alphabet orderings usually gives sub-optimal orderings for compression and that a local search strategy can provide a large improvement in relatively few steps. We also inspect a selection of initial alphabet orderings, including ASCII, letter appearance, and letter frequency. While this alphabet ordering problem is computationally hard we demonstrate gain in compressibility.},
  archive      = {J_JOH},
  author       = {Major, Lily and Clare, Amanda and Daykin, Jacqueline W. and Mora, Benjamin and Zarges, Christine},
  doi          = {10.1007/s10732-025-09548-3},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Heuristics},
  title        = {Heuristics for the run-length encoded Burrows–Wheeler transform alphabet ordering problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring the effectiveness and efficiency of simulation
optimization metaheuristic algorithms. <em>JOH</em>, <em>31</em>(1),
1–21. (<a href="https://doi.org/10.1007/s10732-025-09549-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms have proven capable as general-purpose algorithms for solving simulation optimization problems. Researchers and practitioners often compare different metaheuristic algorithms by examining one or more measures that are derived through empirical analysis. This paper presents a single measure that can be used to empirically compare different metaheuristic algorithms for optimization problems. This measure incorporates both the effectiveness and efficiency of the metaheuristic algorithm, which is especially important in simulation optimization applications because the number of simulation runs available to the analyst (i.e., the run budget) can vary significantly with each simulation study. Therefore, the trade-off between the effectiveness and efficiency of a metaheuristic algorithm must be examined. This single measure is especially useful for multi-objective optimization problems; however, determining this measure is non-trivial for two or more objective functions. Additional details for calculating this measure for multi-objective optimization problems are provided as well as a procedure for comparing two or more metaheuristic algorithms. Finally, computational results are presented and analyzed to compare the performance of metaheuristic algorithms using knapsack problems, pure binary integer programs, traveling salesman problems, and the average results obtained across a diverse set of optimization problems that include simulation and multi-objective optimization problems.},
  archive      = {J_JOH},
  author       = {Thengvall, Benjamin G. and Hall, Shane N. and Deskevich, Michael P.},
  doi          = {10.1007/s10732-025-09549-2},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Heuristics},
  title        = {Measuring the effectiveness and efficiency of simulation optimization metaheuristic algorithms},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection for high-dimensional data using a
multivariate search space reduction strategy based scatter search.
<em>JOH</em>, <em>31</em>(1), 1–33. (<a
href="https://doi.org/10.1007/s10732-025-09550-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In feature selection, the increasing of the dimensionality and the complexity of feature interactions make the problem challenging. Furthermore, searching for an optimal subset of features from a high-dimensional feature space is known to be an $$\mathcal{N}\mathcal{P}$$ -hard problem. To improve the efficiency and effectiveness of the search algorithm, feature grouping has emerged as a way to reduce the search space by clustering features according to a measure. In this work we propose to reduce the search space by applying a greedy algorithm, called Multivariate Greedy Predominant Groups Generator (MGPGG). MGPGG extends the idea of the Greedy Predominant Groups Generator (GPGG) algorithm by taking into account feature interaction among three or more features. For this purpose, MGPGG uses the Multivariate Symmetrical Uncertainty (MSU) to group features that share information about the class label. We also propose a Scatter Search strategy that integrates MGPGG to find small subsets of features with high predictive power. The proposed algorithm, called Multivariate Predominant Group-based Scatter Search (MPGSS), is tested on high-dimensional data from biomedical and text-mining fields. The proposal is compared with state-of-the-art feature selection strategies. Results show that MPGSS is competitive since it is capable of finding small subsets of features while keeping high predictive classification models.},
  archive      = {J_JOH},
  author       = {Garcia-Torres, Miguel},
  doi          = {10.1007/s10732-025-09550-9},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. Heuristics},
  title        = {Feature selection for high-dimensional data using a multivariate search space reduction strategy based scatter search},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristic algorithm for integrated ship scheduling, routing
and stowage problem in multi-vessel roll-on/roll-off shipping.
<em>JOH</em>, <em>31</em>(1), 1–40. (<a
href="https://doi.org/10.1007/s10732-025-09551-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roll-on/roll-off (RoRo) ships offer distinct advantages in the maritime industry when it comes to transporting wheeled cargos and super-large vehicles. As the scale of RoRo fleets continues to grow, RoRo shipping companies face the challenge of efficiently organizing multiple ships to meet transportation demands across various regions, ensuring order fulfillment, and minimizing costs. In light of these challenges, we introduced and explored the multi-vessel RoRo ship scheduling, routing and stowage problem (m-RSRSSP), and proposed a mixed-integer linear programming (MILP) model to address this problem. Compared with previous studies, this paper enriches fleet&#39;s decision-making and address scenarios where multiple cargos are considered at one port on the basis of integrating ship scheduling, routing and stowage problem of ro-ro ship, which is better aligned with the requirements of certain practical scenarios. Given the intricate nature of this model, we developed a heuristic algorithm rooted in tabu search, incorporating a nested greedy approach. Furthermore, we presented a case study involving deep-sea RoRo transportation between Northeast Asia and Europe. The experimental results validate the efficiency and reliability of the proposed heuristic algorithm in solving large-scale problems, and provide valuable strategies for the formulation of the RoRo fleet operation schemes.},
  archive      = {J_JOH},
  author       = {Zhao, Yuzhe and Peng, Peiyun and Zhou, Jingmiao and Wang, Yadong},
  doi          = {10.1007/s10732-025-09551-8},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-40},
  shortjournal = {J. Heuristics},
  title        = {Heuristic algorithm for integrated ship scheduling, routing and stowage problem in multi-vessel roll-on/roll-off shipping},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective population-based approach for the partial set
covering problem. <em>JOH</em>, <em>31</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s10732-025-09552-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The partial set covering problem (PSCP) is a significant combinatorial optimization problem that finds applications in numerous real-world scenarios. The objective of PSCP is to encompass a minimum number of subsets while ensuring the coverage of at least n elements. Due to its NP-hard nature, solving large-scale PSCP efficiently remains a critical issue in computational intelligence. To effectively tackle this challenge, we delve into a population-based approach that incorporates a modified tabu search, thereby striking a delicate balance between exploration and exploitation. To further enhance its efficacy, we employ the multiple path-relinking strategy and the fix-and-optimize process. Finally, the dynamic resource allocation scheme is utilized to save computing efforts. Comparative experiments of the proposed algorithm were conducted against three state-of-the-art competitors, across two distinct categories, encompassing 150 instances. The results significantly underscore the profound effectiveness of our proposed algorithm, as evidenced by the updating of 67 best-known solutions. Moreover, we conduct an in-depth analysis of the key components inherent to the algorithm, shedding light on their respective influences on the whole performance.},
  archive      = {J_JOH},
  author       = {Zhang, Ye and He, Jinlong and Zhou, Yupeng and Hu, Shuli and Cai, Dunbo and Tian, Naiyu and Yin, Minghao},
  doi          = {10.1007/s10732-025-09552-7},
  journal      = {Journal of Heuristics},
  month        = {3},
  number       = {1},
  pages        = {1-32},
  shortjournal = {J. Heuristics},
  title        = {An effective population-based approach for the partial set covering problem},
  volume       = {31},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jota---19">JOTA - 19</h2>
<ul>
<li><details>
<summary>
(2025). Optimal control of several motion models. <em>JOTA</em>,
<em>205</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10957-025-02610-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to studying the dynamic optimization of several controlled crowd motion models in general planar settings. A set of necessary optimality conditions for optimal control problems involving crowd motion models with multiple agents and obstacles was derived and thoroughly analyzed. The analysis provides valuable insights into the primal and dual elements, as well as the degeneracy phenomena. The paper proposes several effective algorithms based on these necessary optimality conditions and presents various nontrivial illustrative examples along with their simulations. The implementation of all the considered motion models can be found via the link: https://github.com/tancao1128/Optimal_Control_of_Several_Motion_Models .},
  archive      = {J_JOTA},
  author       = {Cao, Tan H. and Chapagain, Nilson and Lee, Haejoon and Phung, Thi and Thieu, Nguyen Nang},
  doi          = {10.1007/s10957-025-02610-x},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-36},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Optimal control of several motion models},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto game of stochastic differential system with terminal
state constraint. <em>JOTA</em>, <em>205</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10957-025-02612-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on a type of Pareto game of stochastic differential equation with terminal state constraint. Firstly, we transform equivalently a nonlinear Pareto game problem with convex control space and terminal state constraint into a constrained stochastic optimal control problem. By virtue of duality theory and stochastic maximum principle, a necessary condition for Pareto efficient strategy is established. With some convex assumptions, we also give a sufficient condition for Pareto efficient strategy. Secondly, we consider a linear-quadratic Pareto game with terminal state constraint, and a feedback representation for Pareto efficient strategy is derived. Finally, as an application, we solve a government debt stabilization problem and give some numerical results.},
  archive      = {J_JOTA},
  author       = {Huang, Pengyan and Wang, Guangchen and Wang, Shujun},
  doi          = {10.1007/s10957-025-02612-9},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-30},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Pareto game of stochastic differential system with terminal state constraint},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degenerate drifted wave equations in nondivergence form:
Nonlinear stabilization. <em>JOTA</em>, <em>205</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10957-025-02613-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the stabilization of degenerate 1-D wave equations in non divergence form with drift. The degeneracy takes place in one boundary point and the stabilization is obtained by a nonlinear damping in the nondegeneracy one.},
  archive      = {J_JOTA},
  author       = {Fragnelli, Genni and Mugnai, Dimitri},
  doi          = {10.1007/s10957-025-02613-8},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-36},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Degenerate drifted wave equations in nondivergence form: Nonlinear stabilization},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A euclidean distance matrix model for convex clustering.
<em>JOTA</em>, <em>205</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10957-025-02616-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering has been one of the most basic and essential problems in unsupervised learning due to various applications in many critical fields. The recently proposed sum-of-norms (SON) model by Pelckmans et al. (in: PASCAL workshop on statistics and optimization of clustering, 2005), Lindsten et al. (in: IEEE statistical signal processing workshop, 2011) and Hocking et al. (in: Proceedings of the 28th international conference on international conference on machine learning, 2011) has received a lot of attention. The advantage of the SON model is the theoretical guarantee in terms of perfect recovery, established by Sun et al. (J Mach Learn Res 22(9):1–32, 2018). It also provides great opportunities for designing efficient algorithms for solving the SON model. The semismooth Newton based augmented Lagrangian method by Sun et al. (2018) has demonstrated its superior performance over the alternating direction method of multipliers and the alternating minimization algorithm. In this paper, we propose a Euclidean distance matrix model based on the SON model. Exact recovery property is achieved under proper assumptions. An efficient majorization penalty algorithm is proposed to solve the resulting model. Extensive numerical experiments are conducted to demonstrate the efficiency of the proposed model and the majorization penalty algorithm.},
  archive      = {J_JOTA},
  author       = {Wang, Z. W. and Liu, X. W. and Li, Q. N.},
  doi          = {10.1007/s10957-025-02616-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-22},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A euclidean distance matrix model for convex clustering},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimality conditions for interval-valued optimization
problems on riemannian manifolds under a total order relation.
<em>JOTA</em>, <em>205</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s10957-025-02618-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores fundamental properties of convex interval-valued functions defined on Riemannian manifolds. The study employs generalized Hukuhara directional differentiability to derive KKT-type optimality conditions for an interval-valued optimization problem on Riemannian manifolds. Based on the type of functions involved in optimization problems, we consider the following cases: objective function as well as constraints are real-valued; objective function is interval-valued and constraints are real-valued; objective function as well as constraints are interval-valued. The whole theory is justified with the help of examples. The order relation that we use throughout the paper is a total order relation defined on the collection of all closed and bounded intervals in $$\mathbb {R}$$ .},
  archive      = {J_JOTA},
  author       = {Bhat, Hilal Ahmad and Iqbal, Akhlad and Aftab, Mahwash},
  doi          = {10.1007/s10957-025-02618-3},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-29},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Optimality conditions for interval-valued optimization problems on riemannian manifolds under a total order relation},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random descent steps in a probability maximization scheme.
<em>JOTA</em>, <em>205</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10957-025-02619-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient computation of multivariate distribution functions calls for considerable effort. Hence coordinate descent and derivative-free approaches are attractive. This paper deals with constrained convex problems. We perform random descent steps in an approximation scheme that is an inexact cutting-plane method from a dual viewpoint. We prove that the scheme converges and present a computational study comparing different descent methods applied in the approximation scheme.},
  archive      = {J_JOTA},
  author       = {Csizmás, Edit and Drenyovszki, Rajmund and Szántai, Tamás and Fábián, Csaba I.},
  doi          = {10.1007/s10957-025-02619-2},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Random descent steps in a probability maximization scheme},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why study spherical convexity of non-homogeneous quadratics
and what makes it surprising? <em>JOTA</em>, <em>205</em>(1), 1–26. (<a
href="https://doi.org/10.1007/s10957-025-02620-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes necessary, sufficient, and equivalent conditions for the spherical convexity of non-homogeneous quadratic functions. By examining criteria for determining spherical convexity, we identified unique properties that differentiate spherically convex quadratic functions from their geodesically convex counterparts in both hyperbolic and Euclidean spaces. Since spherically convex functions over the entire sphere are constant, our analysis focuses on proper spherically convex subsets of the sphere. Our primary results concern non-homogeneous quadratic functions on the spherically convex set of unit vectors with positive coordinates. We also extend our findings to more general spherically convex sets. Additionally, the paper explores special cases of non-homogeneous quadratic functions where the defining matrix is of a specific type, such as positive, diagonal, or a Z-matrix. This study not only provides useful criteria for spherical convexity but also reveals surprising characteristics of spherically convex quadratic functions, contributing to a deeper understanding of convexity in spherical geometries.},
  archive      = {J_JOTA},
  author       = {Bolton, Ryan and Németh, Sándor Zoltán},
  doi          = {10.1007/s10957-025-02620-9},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Why study spherical convexity of non-homogeneous quadratics and what makes it surprising?},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic ISTA/FISTA adaptive step search algorithms for
convex composite optimization. <em>JOTA</em>, <em>205</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10957-025-02621-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and analyze stochastic variants of ISTA and a full backtracking FISTA algorithms (Beck and Teboulle in SIAM J Imag Sci 2(1):183–202, 2009; Scheinberg et al. in Found Comput Math 14(3):389–417, 2014) for composite optimization without the assumption that stochastic gradient is an unbiased estimator. This work extends analysis of inexact fixed step ISTA/FISTA in Schmidt et al. (Convergence rates of inexact proximal-gradient methods for convex optimization, 2022. arXiv:1109.2415 ) to the case of stochastic gradient estimates and adaptive step-size parameter chosen by backtracking. It also extends the framework for analyzing stochastic line-search method in Cartis and Scheinberg (Math Program 169(2):337-375, 2018) to the proximal gradient framework as well as to the accelerated first order methods.},
  archive      = {J_JOTA},
  author       = {Nguyen, Lam M. and Scheinberg, Katya and Tran, Trang H.},
  doi          = {10.1007/s10957-025-02621-8},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Stochastic ISTA/FISTA adaptive step search algorithms for convex composite optimization},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relaxed two-step inertial tseng’s extragradient method for
nonmonotone variational inequalities. <em>JOTA</em>, <em>205</em>(1),
1–27. (<a href="https://doi.org/10.1007/s10957-025-02622-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel inertial projection method for solving the variational inequality (VI) without imposing the restrictive assumption of monotonicity on the cost operator. We establish global convergence of the proposed method under the condition that the solution set of the associated Minty VI with it is non-empty. Our results improve upon and extend many important related results in this research direction, providing a more general and flexible framework for tackling non-monotone variational inequalities. To demonstrate the practical efficacy of our method, we give some numerical illustrations and apply the proposed algorithm to solve a network equilibrium flow problem, which is a fundamental problem in transportation infrastructure modeling. We also compare the performance of our algorithm with those of existing ones.},
  archive      = {J_JOTA},
  author       = {Viet Thong, Duong and Ky Anh, Pham and Tien Dung, Vu},
  doi          = {10.1007/s10957-025-02622-7},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Relaxed two-step inertial tseng’s extragradient method for nonmonotone variational inequalities},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum principle for optimal control of mean-field backward
doubly SDEs with delay. <em>JOTA</em>, <em>205</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10957-025-02624-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the control problems of mean-field backward doubly stochastic differential equations with delay in the form of an integral with respect to a finite regular measure. Using the standard variational method, we introduce a new type of anticipated mean-field doubly stochastic differential equations as adjoint equations and derive a necessary condition in form of the maximum principle for optimal control. Under appropriate assumptions, the sufficiency of the maximum principle is also established. Our results can be applied to a certain class of linear quadratic control problems and be used to study the mean-field game for a pension fund model with delayed surplus.},
  archive      = {J_JOTA},
  author       = {Wang, Meng},
  doi          = {10.1007/s10957-025-02624-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-24},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Maximum principle for optimal control of mean-field backward doubly SDEs with delay},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gevrey regularity for a fluid–structure interaction model.
<em>JOTA</em>, <em>205</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10957-025-02625-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A result of Gevrey regularity is ascertained for a semigroup which models a fluid–structure interaction problem. In this model, the fluid evolves in a piecewise smooth or convex geometry $$\mathcal {O}$$ . On a portion of the boundary, a fourth order plate equation is coupled with the fluid through pressure and matching velocities. The key to obtaining the conclusion of Gevrey regularity is an appropriate estimation of the resolvent of the associated $$C_0$$ -semigroup operator. Moreover, a numerical scheme and example is provided which empirically demonstrates smoothing of the fluid–structure semigroup.},
  archive      = {J_JOTA},
  author       = {Avalos, George and McKnight, Dylan and McKnight, Sara},
  doi          = {10.1007/s10957-025-02625-4},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-17},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Gevrey regularity for a Fluid–Structure interaction model},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for uncertain linear optimization problems through
polytopic approximation of the uncertainty set. <em>JOTA</em>,
<em>205</em>(1), 1–42. (<a
href="https://doi.org/10.1007/s10957-025-02626-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a globally convergent iterative method to solve uncertain constrained linear optimization problems. Due to the nondeterministic nature of such a problem, we use the min-max approach to convert the given problem into a deterministic one. We show that the robust feasible sets of the problem corresponding to the uncertainty set and the convex hull of the uncertainty set are identical. This result helps to reduce the number of inequality constraints of the problem drastically; often, this result reduces the semi-infinite programming problem of the min-max robust counterpart into a problem with a finite number of constraints. Following this, we provide a necessary and sufficient condition for the boundedness of the robust feasible set of the problem. Moreover, we explicitly identify the robust feasible set of the problem for polytopic and ellipsoidal uncertainty sets. We present an algorithm to construct an inner polytope of the convex hull of a general uncertainty set under a certain assumption. This algorithm provides a point-wise inner polytopic approximation of the convex hull with arbitrarily small precision. We employ this inner polytopic approximation corresponding to the uncertainty set and the infeasible interior-point technique to derive an iterative approach to solve general uncertain constrained linear optimization problems. Global convergence for the proposed method is reported. Numerical experiments illustrate the practical behaviour of the proposed method on discrete, star-shaped, disc-shaped, and ellipsoidal uncertainty sets.},
  archive      = {J_JOTA},
  author       = {Raushan, Ravi and Ghosh, Debdas and Zhao, Yong and Wei, Zhou},
  doi          = {10.1007/s10957-025-02626-3},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-42},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {A method for uncertain linear optimization problems through polytopic approximation of the uncertainty set},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Output-based receding horizon stabilizing control for linear
parabolic equations. <em>JOTA</em>, <em>205</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10957-025-02628-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A receding horizon control framework is coupled with a Luenberger observer to construct an output-based control input stabilizing parabolic equations. The actuators and sensors are indicator functions of small subdomains, representing localized actuation and localized measurements. It is shown that, for a class of explicitly given sets of actuators and sensors, we can guarantee the stabilizing property of the constructed input. Results of numerical simulations are presented validating the theoretical findings.},
  archive      = {J_JOTA},
  author       = {Azmi, Behzad and Rodrigues, Sérgio S.},
  doi          = {10.1007/s10957-025-02628-1},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-34},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Output-based receding horizon stabilizing control for linear parabolic equations},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Facial approach for constructing stationary
points for mathematical programs with cone complementarity constraints.
<em>JOTA</em>, <em>205</em>(1), 1. (<a
href="https://doi.org/10.1007/s10957-025-02629-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOTA},
  author       = {Madariaga, Javier I. and Ramírez, Héctor},
  doi          = {10.1007/s10957-025-02629-0},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Correction: Facial approach for constructing stationary points for mathematical programs with cone complementarity constraints},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intersection theorem for a pair of set-valued maps and
its applications. <em>JOTA</em>, <em>205</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10957-025-02632-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an intersection theorem for a pair of set-valued maps under relaxed closedness and coercivity conditions is investigated. This theorem leads to minimax inequality and variational relation results, enabling us to solve such problems even when the usual closedness and compactness conditions are not satisfied.},
  archive      = {J_JOTA},
  author       = {Lotfipour, Maryam},
  doi          = {10.1007/s10957-025-02632-5},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {An intersection theorem for a pair of set-valued maps and its applications},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Existence and uniqueness of solutions of generalized mixed
variational inequalities. <em>JOTA</em>, <em>205</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s10957-025-02636-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the generalized mixed variational inequality, which encompasses both the generalized variational inequality and the mixed variational inequality. The core contribution of this paper is twofold. Firstly, by utilizing the principles of degree theory, we establish certain sufficient conditions for the existence of solutions to the generalized mixed variational inequality. Additionally, we formulate a sufficient condition that ensures the uniqueness of these solutions. Secondly, we recognize that the conditions outlined in our theorem are inapplicable to the generalized mixed polynomial variational inequality, a subclass within the broader family of generalized mixed variational inequalities. To address this, we employ an exceptional family of elements and establish an existence and uniqueness theorem specifically tailored for the generalized mixed polynomial variational inequality.},
  archive      = {J_JOTA},
  author       = {Liu, Jian-Xun and Lan, Zhao-Feng and Huang, Zheng-Hai},
  doi          = {10.1007/s10957-025-02636-1},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-21},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Existence and uniqueness of solutions of generalized mixed variational inequalities},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solution existence and compactness analysis for nonsmooth
optimization problems. <em>JOTA</em>, <em>205</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s10957-025-02637-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the analysis of geometrical properties and behaviors of the optimal value and global optimal solutions for a class of nonsmooth optimization problems. We provide conditions under which the solution set of a nonsmooth and nonconvex optimization problem is non-empty and/or compact. We also examine related properties such as the compactness of the sublevel sets, the boundedness from below and the coercivity of the objective function to characterize the non-emptiness and the compactness of the solution set of the underlying optimization problem under the unboundedness of its associated feasible set.},
  archive      = {J_JOTA},
  author       = {Hung, Nguyen Canh and Chuong, Thai Doan and Anh, Nguyen Le Hoang},
  doi          = {10.1007/s10957-025-02637-0},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-25},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Solution existence and compactness analysis for nonsmooth optimization problems},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualitative properties of robust benson efficient solutions
of uncertain vector optimization problems. <em>JOTA</em>,
<em>205</em>(1), 1–37. (<a
href="https://doi.org/10.1007/s10957-025-02638-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider both unconstrained and constrained uncertain vector optimization problems involving free disposal sets, and study the qualitative properties of their robust Benson efficient solutions. First, we discuss necessary and sufficient optimality conditions for the robust Benson efficient solutions of these problems using the linear scalarization method. Then, by utilizing this approach, we investigate the semicontinuity properties of the solution maps when the problem data is perturbed by parameters given in parameter spaces. Finally, we suggest concepts of approximate robust Benson efficient solutions and investigate Hausdorff well-posedness conditions for such problems with respect to these approximate solutions. Several examples are provided to illustrate the applicability and novelty of the results obtained in this study.},
  archive      = {J_JOTA},
  author       = {Anh, Lam Quoc and Thuy, Vo Thi Mong and Zhao, Xiaopeng},
  doi          = {10.1007/s10957-025-02638-z},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-37},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Qualitative properties of robust benson efficient solutions of uncertain vector optimization problems},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Second-order sufficient optimality conditions in the
calculus of variations. <em>JOTA</em>, <em>205</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s10957-025-02639-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some classic second-order sufficient optimality conditions in the calculus of variations are shown to be equivalent, while also introducing a new equivalent second-order condition which is extremely easy to apply: simply integrate a linear second-order initial value problem and check that the solution is positive over the problem domain.},
  archive      = {J_JOTA},
  author       = {Hager, William W.},
  doi          = {10.1007/s10957-025-02639-y},
  journal      = {Journal of Optimization Theory and Applications},
  month        = {4},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Optim. Theory Appl.},
  title        = {Second-order sufficient optimality conditions in the calculus of variations},
  volume       = {205},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jrtip---12">JRTIP - 12</h2>
<ul>
<li><details>
<summary>
(2025). CFP-PSPNet: A lightweight unmanned vessel water segmentation
algorithm. <em>JRTIP</em>, <em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01603-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate water segmentation is the first prerequisite for unmanned vessels to navigate safely and perform other operations. Aiming at the problems of low utilization rate of unmanned boat water image features and low accuracy of contour edge segmentation in complex inland river scenarios, this paper proposes a lightweight water segmentation algorithm: channel feature pyramid-pyramid scene parsing network (CFP-PSPNet), which realizes efficient and accurate segmentation of water area in complex scenarios. First, a cross transformation-channel feature pyramid (CT-CFP) is proposed, which improves the utilization of the original feature information by cross-fertilizing the feature information between different layers and realizes the improvement of the water segmentation accuracy; Secondly, a parallel semantic segmentation network CFP-PSPNet is designed, which extracts the image information by pyramid pooling module (PPM) and CT-CFP dual pyramid, which solves the problem of loss of detail information and edge information, so as to achieve the purpose of improving the accuracy; finally, Mobilenetv2 after the introduction of encoder-context-attention (ECA) is used as a feature extraction network, which reduces the number of parameters and computation of the network without affecting the segmentation accuracy and realizes the lightweight design of the network. Experiments are conducted on the open-source dataset USVInland, and the experimental results show that our CFP-PSPNet algorithm has a significant reduction in the number of parameters, an increase in detection speed by 81FPS, and mean intersection over union (MIoU) and accuracy rates of 97.71% and 98.75%, respectively, which are 1.41% and 0.74% higher than that of the original network. It is superior to other classical semantic segmentation algorithms.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Song, Yijing and He, Lintao and Xue, Hang and Dong, Zhonghua and Zhang, Qingyun},
  doi          = {10.1007/s11554-024-01603-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CFP-PSPNet: A lightweight unmanned vessel water segmentation algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECDet: Efficient oriented object detection on the aerial
image with cross-layer attention. <em>JRTIP</em>, <em>22</em>(1), 1–13.
(<a href="https://doi.org/10.1007/s11554-024-01617-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in horizontal object detection models, oriented object detection models have also made significant strides. Yet existing rotated object detectors often struggle to maintain high accuracy while processing high-resolution remote sensing images in real-time. To address these challenges, we propose a new lightweight model specifically for oriented object detection, named the efficient cross-layer attention detector (ECDet). ECDet integrates several efficient modules, including an efficient reparameterized Transformer-like backbone (ERepViT) to reduce computational costs, and the efficient cross-layer fusion neck (CLF-Neck), a lightweight alternative to traditional pyramid networks for feature fusion with attention mechanism. Additionally, we introduce the lightweight task interaction decoupled (LTID) head, which enhances task-specific performance by providing more detailed, task-aligned information for classification and regression with minimal computational cost. Furthermore, an ensemble loss combined with the phase shifting coder (PSC) mitigates the angle discontinuity issue in regression-based methods. Evaluations on the DOTAv1 and HRSC datasets show that ECDet runs 32% faster than RTMDet-S with higher accuracy, demonstrating its strong potential for practical application. The source code will be release at https://github.com/tianlianghai/ECDet .},
  archive      = {J_JRTIP},
  author       = {Lyu, Xueqiang and Tian, Lianghai and Teng, Shangzhi},
  doi          = {10.1007/s11554-024-01617-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ECDet: Efficient oriented object detection on the aerial image with cross-layer attention},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-level attention based data compression for spike
camera. <em>JRTIP</em>, <em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-024-01618-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of autonomous driving applications, there is an increasing demand for high-speed vision sensors like spike cameras. However, the data transmission and storage requirements are becoming increasingly burdensome due to the high temporal resolution, such as 40,000 Hz. To resolve this problem, we propose a pixel-level attention-based data compression method for the spike camera. First the input spike data are partitioned into two block types by pixel-level attention-based method. Then, the two blocks are condensed using different methods and side information marking is transmitted for spike decoding. Finally, the condensed spike and side information marking are compressed into a binary stream for storage. The experimental results show that our method achieves higher compression efficiency than conventional methods. The decompressed spike can also reconstruct the image with better visual quality.},
  archive      = {J_JRTIP},
  author       = {Li, Yansong and Huang, Xiaofeng and Li, Shangqia and Cui, Yan and Zhou, Yang and Song, Jian and Yin, Haibing},
  doi          = {10.1007/s11554-024-01618-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Pixel-level attention based data compression for spike camera},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rocknet: Lightweight network for real-time segmentation of
martian rocks. <em>JRTIP</em>, <em>22</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-024-01619-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rock segmentation on the Martian is particularly critical for rover navigation, obstacle avoidance, and scientific target detection. We propose a lightweight network for real-time semantic segmentation of Martian rocks (RockNet). First, we propose the cross-dimension channel attention (CDCA) model to replace traditional downsample and upsample operation, which gives more weight to the channels with more useful information by adjusting the weight of each channel. Second, we modify the short-term dense concatenate model, we adopt dilated convolution to learn the feature with a larger receptive field, and through the skip connection structure, the degradation of the network can be reduced. Finally, we propose a feature fusion module (FFM) to fully fuse different levels of features. With only 0.86M parameters, our model gets 82.37% mIoU and 105.7 FPS running speed on the dataset of TWMARS.},
  archive      = {J_JRTIP},
  author       = {Wei, Pengfei and Sun, Zezhou and Tian, He},
  doi          = {10.1007/s11554-024-01619-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rocknet: Lightweight network for real-time segmentation of martian rocks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time underwater target detection based on improved
YOLOv7. <em>JRTIP</em>, <em>22</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s11554-025-01621-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target detection is crucial for ocean exploration, but existing methods struggle to achieve satisfactory results due to the complexity of the underwater environment. To enhance the accuracy and real-time performance of underwater detection models, we propose an improved YOLOv7 model. We introduce a multi-granularity feature attention method based on the Efficient Channel Attention (ECA) to help the model better adapt to the diverse conditions in the underwater environment, reducing focus on redundant information. Utilizing coordinate convolution provides the network with spatial awareness of input image coordinates, enabling more effective localization of target objects and reducing interference from similar background elements. To accommodate the features of small and dense underwater targets, we use normalized Wasserstein distance to measure the similarity of bounding boxes. On the Underwater Robot Picking Contest 2019 (URPC 2019) dataset, the mean Average Precision (mAP) of our improved network has reached 86.19%, which represents a 1.57% increase compared to the original YOLOv7 network. Additionally, the frames per second (fps) has achieved 124, surpassing the performance of the original network. This improvement is significantly superior to conventional target detection models, providing a faster and more accurate advantage for underwater target detection tasks in complex underwater environments.},
  archive      = {J_JRTIP},
  author       = {Wu, Qingqi and Cen, Lihui and Kan, Shichao and Zhai, Yongping and Chen, Xiaofang and Zhang, Hong},
  doi          = {10.1007/s11554-025-01621-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time underwater target detection based on improved YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LKR-DETR: Small object detection in remote sensing images
based on multi-large kernel convolution. <em>JRTIP</em>, <em>22</em>(1),
1–14. (<a href="https://doi.org/10.1007/s11554-025-01622-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in remote sensing imagery remains a challenging problem in computer vision. To address the inherent limitations of aerial imagery, such as densely packed objects with insufficient detail and occlusions caused by complex backgrounds, this study proposes LKR-DETR, an innovative object detection in remote sensing imagery framework based on RT-DETR. We propose a lightweight and efficient feature extraction module with large kernel convolution, which expands the receptive field while reducing parameters and computational costs. Furthermore, we present a novel multi-scale feature fusion structure based on wavelet transform convolution that effectively utilizes low-frequency information from low-level feature maps. Additionally, we introduce a lightweight image restoration module utilizing large kernel convolutions, which effectively recovers previously undetected details of small objects. To improve bounding box regression accuracy, the original GIoU loss is replaced with a Focaler-DIoU loss function. Compared to the benchmark model RT-DETR, the LKR-DETR model achieves a 2.5% improvement in $$mAP_{0.5}$$ and a 2.0% improvement in $$mAP_{0.5:0.95}$$ on the VisDrone2019-DET dataset, a 1.7% and 4.4% improvement on the DOTAv1.5 dataset, and a 3.4% and 2.4% improvement on the HIT-UAV dataset, while also reducing the parameter count and model size. Relative to other cutting-edge models, LKR-DETR attains superior detection accuracy while maintaining relatively low computational complexity, establishing it as an efficient solution for small object detection in remote sensing imagery.},
  archive      = {J_JRTIP},
  author       = {Dong, Ying and Xu, Fucheng and Guo, Jiahao},
  doi          = {10.1007/s11554-025-01622-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LKR-DETR: Small object detection in remote sensing images based on multi-large kernel convolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DHNet: A surface defect detection model utilizing
multi-scale convolutional kernels. <em>JRTIP</em>, <em>22</em>(1), 1–15.
(<a href="https://doi.org/10.1007/s11554-025-01623-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting micro-defects in densely populated printed circuit boards (PCBs) with complex backgrounds is a critical challenge. To address the problem, the DHNet, a small object detection network based on YOLOv8 employing multi-scale convolutional kernels is proposed for feature extraction and fusion. The lightweight VOVGSHet module is designed for feature fusion and a pyramid structure to efficiently leverage feature map relationships while minimizing model complexity and parameters. Otherwise, to optimize the original extraction structure and enhance multi-scale defect detection, convolutional kernels of varying sizes process the same input channels. Additionally, the incorporation of the Wise-IoU loss function improves small defect detection accuracy and efficiency. Moreover, extensive experiments on a custom PCB dataset demonstrate DHNet&#39;s effectiveness, achieving an outstanding mean Average Precision (mAP) of 84.5%, surpassing the original YOLOv8 network by 4.0%, with parameters only of 2.85 M. Model demonstrates a latency of 3.6 ms on NVIDIA 4090. However, YOLOv8n has a latency of 4.4 ms. Validation on public DeepPCB and NEU datasets further confirms DHNet&#39;s superiority, which can reach 99.1% and 79.9% mAP, respectively. Finally, successful deployment on the NVIDIA Jetson Nano platform validates DHNet&#39;s suitability for real-time defect detection in industrial applications.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yingying and Wang, Shuo and Wang, Jinhai and Zhao, Yu and Chen, Zhiwei},
  doi          = {10.1007/s11554-025-01623-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DHNet: A surface defect detection model utilizing multi-scale convolutional kernels},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSBF-YOLO: A lightweight model for tomato ripeness detection
in natural environments. <em>JRTIP</em>, <em>22</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-025-01624-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate tomato ripeness detection is essential for optimizing harvest timing and maximizing yield. Deep learning-based object detection has proven effective in this task. However, many existing algorithms have numerous parameters and substantial computational demands, making them unsuitable for agricultural environments with limited computational resources. Additionally, accurate detection becomes challenging with overlapping fruits, leaf occlusion, or complex backgrounds. To address these issues, this paper proposes a lightweight detection model, GSBF-YOLO. This model designs the GSim module to reduce parameters while maintaining detection accuracy. The C3Ghost module further reduces parameter count by replacing the traditional C3 module. The PANet multi-scale feature fusion network in the neck is replaced with the Bi-directional Feature Pyramid Network (BiFPN), which adjusts weights based on the importance of input features. Lastly, the fine-tuned FocalEIOU Loss function is used to calculate the bounding box regression loss, enhancing the model’s ability to adjust the weights of high-quality anchor boxes for better detection of targets in occlusion scenarios. Experimental results show that GSBF-YOLO reduces parameters and computational load by 42% and 45%, respectively, while mean Average Precision (mAP) increases by 1.9% and 1.6% on two datasets. The model achieves 110 Frames Per Second (FPS), meeting real-time detection requirements, and has fewer parameters and higher accuracy compared to models like YOLOv8. The research indicates that the proposed lightweight model can effectively detect tomato ripeness in natural environments.},
  archive      = {J_JRTIP},
  author       = {Hao, Fengqi and Zhang, Zuyao and Ma, Dexin and Kong, Hoiio},
  doi          = {10.1007/s11554-025-01624-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GSBF-YOLO: A lightweight model for tomato ripeness detection in natural environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time and general method for converting offline
skeleton-based action recognition to online ones. <em>JRTIP</em>,
<em>22</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11554-025-01625-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many motion sensors can directly acquire human skeletal data, and then extract features on the skeletal data through GCNs (graph convolutional networks) to perform action recognition. However, almost all state-of-the-art (SOTA) methods are offline methods, cannot perform online inference, wasting computational resources. The existing approach to transforming offline action recognition into online action recognition is to reconstruct the network structure of the offline method. This requires developers to have a deep understanding of the algorithm’s network structure and make extensive modifications, which results in slow development. To address the above issue, this paper points out that to convert offline methods to online ones, the key is removing outdated frame features and fusing new frame features. Furthermore, we propose a general and simple model called Encode One Frame (EOF), which achieves feature removal and fusion by a correlation matrix and the guidance of a teacher model. The EOF model has online inference capabilities, requiring only the input of the new frame of the current sample and the features encoded from the old sample. Based on the EOF model, we further propose the You Only Encode One Frame (YOEOF) algorithm to correct the cumulative errors generated during EOF model online inference. By coupling these proposals, YOEOF achieves online inference and outperforms some SOTA methods on public datasets. The deployment at the application level indicates that our method meets the requirements of high accuracy and real-time performance for dangerous action recognition.},
  archive      = {J_JRTIP},
  author       = {Dong, Liheng and He, Guiqing and Zhang, Zhaoxiang and Xu, Yuelei and Hui, Tian and Xu, Xin and Tao, Chengyang and Li, Huafeng},
  doi          = {10.1007/s11554-025-01625-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and general method for converting offline skeleton-based action recognition to online ones},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid architecture for video frame prediction:
Combining convolutional LSTM and 3D CNN. <em>JRTIP</em>, <em>22</em>(1),
1–18. (<a href="https://doi.org/10.1007/s11554-025-01626-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame prediction represents a fundamental challenge in computer vision, necessitating precise modeling of both spatial and temporal dynamics within video sequences. This computational task holds substantial implications across diverse domains, including video compression optimization, robust object tracking systems, and advanced motion forecasting applications. In this investigation, we present a novel hybrid architecture that synthesizes the complementary strengths of Convolutional Long Short-Term Memory (ConvLSTM) networks and three-dimensional Convolutional Neural Networks (3D CNN) for enhanced frame prediction capabilities. Our methodological framework incorporates a ConvLSTM component that fundamentally augments the traditional LSTM architecture through the integration of convolutional operations, thereby facilitating sophisticated modeling of sequential dependencies. Concurrently, the 3D CNN component employs volumetric convolutional layers to extract rich spatio-temporal features from the input sequences. Rigorous empirical evaluation demonstrates the superior performance of the ConvLSTM architecture, which consistently yields reduced validation errors and elevated coefficients of determination. Specifically, the ConvLSTM model achieves a validation Mean Squared Error (MSE) of 0.0237 and an $${\textrm{R}}^{2}$$ value of 0.6951, substantially outperforming the 3D CNN model, which exhibits a validation MSE of 0.0471 and an $${\textrm{R}}^{2}$$ value of 0.3939. These empirical findings substantiate the efficacy of the ConvLSTM architecture in addressing the inherent complexities of video frame prediction, while simultaneously illuminating its considerable potential for deployment across various video processing and predictive modeling applications. The results provide compelling evidence for the advantages of incorporating convolutional operations within recurrent architectures for sequential visual data processing.},
  archive      = {J_JRTIP},
  author       = {Aravinda, C. V. and Al-Shehari, Taher and Alsadhan, Nasser A. and Shetty, Shashank and Padmajadevi, G. and Reddy, K. R. Udaya Kumar},
  doi          = {10.1007/s11554-025-01626-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel hybrid architecture for video frame prediction: Combining convolutional LSTM and 3D CNN},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMAL-YOLOv8: Real-time detection algorithm for insulator
defects based on optimized feature fusion. <em>JRTIP</em>,
<em>22</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s11554-025-01629-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of misdetection and missed detection caused by the diversity of insulator types and the complexity of defect textures in power transmission lines, and to meet the demands of collaborative inspection, we propose a real-time detection algorithm for insulator defects based on YOLOv8. First, considering the characteristics of the dataset sample sizes, we designed a lightweight OMAL-Neck structure that optimizes feature fusion, enhancing the utilization of feature information and improving detection performance for medium and large targets. Second, to address the issue of large parameter and computation requirements in the YOLOv8 detection head, we designed a lightweight and efficient detection head. This redesigned detection head incorporates PConv, further accelerating model inference speed. Lastly, to counteract the decline in detection accuracy due to model lightweighting, we integrated the C2f module with DySnakeConv, enhancing the feature extraction capability for tubular structures and complex textures, thereby preventing information loss. Experimental results demonstrate that compared to the baseline YOLOv8s, the proposed model increases FPS from 44 to 78 frames/s, reduces the number of parameters and computational complexity by 27 and 38%, respectively, and improves the mAP by 1.7%. The improved model offers significant advantages in both detection accuracy and real-time performance, enabling rapid and precise identification of insulators and their defects, thereby improving the efficiency of power line inspections.},
  archive      = {J_JRTIP},
  author       = {Ru, Hongfang and Zhang, Wenhao and Wang, Guoxin and Ding, Luyang},
  doi          = {10.1007/s11554-025-01629-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {OMAL-YOLOv8: Real-time detection algorithm for insulator defects based on optimized feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition method for PCB chip targets based on
YOLO-GSG. <em>JRTIP</em>, <em>22</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s11554-024-01616-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial settings, the identification of chips on PCB boards is crucial for quality control and efficiency. However, achieving both speed and accuracy in chip detection remains a significant challenge. To address this issue, we propose the YOLO-GSG deep network model, which incorporates several novel modifications to the standard YOLO architecture. The key innovations include the replacement of the ELAN module with the C3Ghostnet module in the backbone network, improving feature extraction and reducing model complexity, and the introduction of the SE attention mechanism to minimize feature loss. Additionally, the GSnet module and GSConv convolution are integrated into the neck network to enhance feature fusion. The experimental results indicate that the YOLO-GSG algorithm achieves a mAP of 99.014%, with precision and recall improvements of 1.080% and 1.446% over the baseline YOLOv7 model. Additionally, the improved model has 24.478M parameters, 61.4 GFLOPs, and a model size of 50.8 MB. The model achieves an FPS of 231.55, representing a 12.8% speedup over the baseline. These results indicate that the YOLO-GSG model offers a superior balance of speed and accuracy for chip identification in industrial applications. This study contributes to the advancement of deep learning applications in industrial environments, providing a more efficient and effective tool for quality control in PCB manufacturing.},
  archive      = {J_JRTIP},
  author       = {Yue, Zeang and Li, Xun and Zhou, Huilong and Wang, Gaopin and Wang, Wenjie},
  doi          = {10.1007/s11554-024-01616-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition method for PCB chip targets based on YOLO-GSG},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="kis---33">KIS - 33</h2>
<ul>
<li><details>
<summary>
(2025). Correction: Taxonomy of deep learning-based intrusion
detection system approaches in fog computing: A systematic review.
<em>KIS</em>, <em>67</em>(2), 2017. (<a
href="https://doi.org/10.1007/s10115-024-02206-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Najafli, Sepide and Toroghi Haghighat, Abolfazl and Karasfi, Babak},
  doi          = {10.1007/s10115-024-02206-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {2017},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: taxonomy of deep learning-based intrusion detection system approaches in fog computing: a systematic review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-class margin climbing with cost-sensitive learning in
neural network classification. <em>KIS</em>, <em>67</em>(2), 1993–2016.
(<a href="https://doi.org/10.1007/s10115-024-02279-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large margin stands as an intuitive indicator of reliable classifiers, reflecting classifier robustness and generalizability. However, due to the intricate nonlinear mapping, directly defining margin as the optimization objective for multi-layer neural network is always challenging. On the other hand, the cost sensitivity coefficients of individual samples hold promise for shaping decision boundaries of neural network classification. A question arises as to whether optimizing neural network classifier can be guided to achieve larger classification margin by varying instance-level cost sensitivity factor. Inspired by above question, this paper proposes a heuristic hard mining strategy designed to progressively identify challenging samples and amplify the output margin through cost-sensitive learning. The refinement process adjusts the sample distribution when optimization reaches a local minimum to ensure the sustainable optimization, ultimately leading to margin climbing. Two hard mining algorithms are designed for binary and multi-class classification problems, which utilize distinct margin definitions based on different decision-making scenarios. In the proposed method, we focus on establishing individualized margin between distinct categories to more accurately characterize the inter-class margin. Empirical results demonstrate that our proposed methodology enhances both the accuracy and robustness in neural network classification.},
  archive      = {J_KIS},
  author       = {Zhang, Siyuan and Xie, Linbo and Chen, Ying and Zhang, Shanxin},
  doi          = {10.1007/s10115-024-02279-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1993-2016},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Inter-class margin climbing with cost-sensitive learning in neural network classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evidence-based approach for open-domain question
answering. <em>KIS</em>, <em>67</em>(2), 1969–1991. (<a
href="https://doi.org/10.1007/s10115-024-02269-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain question answering (ODQA) stands at the forefront of advancing natural language understanding and information retrieval. Traditional ODQA systems, which predominantly utilize a two-step process of information retrieval followed by reading module, face significant challenges in aligning retrieved passages with the contextual nuances of user queries. This paper introduces a novel methodology that leverages a semi-structured knowledge graph to enhance both the accuracy and relevance of answers in ODQA systems. Our model employs a threefold approach: firstly, it extracts and ranks evidence from a textual knowledge graph, a semi-structured knowledge graph where the nodes are real-world entities and the edges are sentences that two entities co-occur in, based on the contextual relationships relevant to the question. Secondly, it utilizes this ranked evidence to re-rank initially retrieved passages, ensuring that they align more closely with the query’s context. Thirdly, it integrates this evidence into a generative reading component to construct precise and context-rich answers. We compare our model, termed contextual evidence-based question answering (CEQA), against traditional and state-of-the-art ODQA systems across several datasets, including TriviaQA, Natural Questions, and SQuAD Open. Our extensive experiments and ablation studies show that CEQA significantly outperforms existing methods by improving both the relevance of retrieved passages and the accuracy of the generated answers, thereby establishing a new benchmark in ODQA.},
  archive      = {J_KIS},
  author       = {Jafarzadeh, Parastoo and Ensan, Faezeh},
  doi          = {10.1007/s10115-024-02269-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1969-1991},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An evidence-based approach for open-domain question answering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised multi-hop heterogeneous hypergraph embedding
with informative pooling for graph-level classification. <em>KIS</em>,
<em>67</em>(2), 1945–1968. (<a
href="https://doi.org/10.1007/s10115-024-02259-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous graph analysis, existing self-supervised learning (SSL) methods face several key challenges. Primarily, these approaches are tailored for node-level tasks and fail to effectively capture global graph-level features, a crucial aspect for comprehensive graph understanding. Furthermore, they predominantly rely on meta-path-based techniques to unravel graph structures, a process that can be computationally intensive and often intractable for complex networks. Another significant limitation is their inability to account for nonpairwise relationships, a common characteristic in real-world networks like protein-protein interaction and collaboration networks, limiting their effectiveness in graph-level learning where high-order connectivity is essential. To address these issues, we propose an innovative SSL framework for heterogeneous hypergraph embedding, expressly designed to enhance graph-level classification. Our framework introduces multi-hop attention in hypergraph convolution, a significant leap from existing attention mechanisms specifically for hypergraphs that primarily focus on immediate neighborhoods. This multi-hop approach allows for an expansive capture of relational structures, both near and far, uncovering intricate patterns integral to accurate graph-level classification. Complementing this, we implement an informative graph-level attentive pooling mechanism that surpasses traditional aggregation methods. It intelligently synthesizes features, taking into account their structural and semantic importance within the hypergraph, thereby preserving critical contextual information. Furthermore, we refine our contrastive learning approach and introduce targeted negative sampling strategies, creating a more robust learning environment that excels at discerning nuanced graph-level features. Rigorous evaluation against established graph kernels, graph neural networks, and graph pooling methods on real-world datasets demonstrates our model’s superior performance, validating its effectiveness in addressing the complexities inherent in heterogeneous graph-level classification.},
  archive      = {J_KIS},
  author       = {Hayat, Malik Khizar and Xue, Shan and Yang, Jian},
  doi          = {10.1007/s10115-024-02259-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1945-1968},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised multi-hop heterogeneous hypergraph embedding with informative pooling for graph-level classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oversampling algorithm for high-dimensional imbalanced
learning with class overlapping. <em>KIS</em>, <em>67</em>(2),
1915–1943. (<a
href="https://doi.org/10.1007/s10115-024-02276-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing standard learning methods suffer from poor performance in high-dimensional imbalanced learning with class overlapping. To tackle this problem, we propose a novel oversampling algorithm that aims to generate a robust ensemble of manifold dimensionality reduction, grid clustering, and information entropy criteria. Instead of simply balancing positive and negative numbers, the algorithm considers the difference in information entropy for interclass, which first reduces the dimensionality by manifold reduction, and then group data utilize grid clustering. Subsequently, calculate the oversampling weight of each group by information entropy and find seed samples based on entropy and neighborhood. Finally, SMOTE based on Beta distribution combined with standard classifiers achieve the rapid and precise classification for high-dimensional imbalanced datasets with class overlapping. Extensive experimental results on 20 real-world imbalanced datasets and compared with eight popular oversampling algorithms show that our proposed algorithm, while achieving good performance in terms of F-measure, G-mean, and AUPRC, can lead to robust performance under high-dimensional and overlapping. It is worth noting that our algorithm substantially reduces the number of synthetic samples against the quantity-balanced oversampling algorithms, and significantly reduces the generation of class overlapping.},
  archive      = {J_KIS},
  author       = {Yang, Xu and Xue, Zhen and Zhang, Liangliang and Wu, Jianzhen},
  doi          = {10.1007/s10115-024-02276-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1915-1943},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An oversampling algorithm for high-dimensional imbalanced learning with class overlapping},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on federated learning system: A new
paradigm to machine learning. <em>KIS</em>, <em>67</em>(2), 1811–1914.
(<a href="https://doi.org/10.1007/s10115-024-02257-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a machine learning technique that permits clients to train the model at a local site in a collaborative manner. It builds a global shared model on the basis of updates of the local model without exchanging data among multiple devices. Federated learning was introduced in 2016 with the goal of enabling local training as well as distributed machine learning training at the edge node’s level. It plays a vital role in terms of preserving the privacy of data while training the machine learning model on multiple devices. However, the introduction of federated learning into real-world applications exposes certain challenges in the training process, which affect the overall efficacy and efficiency of the federated learning model in real-world scenarios. As a result, an increasing number of researchers are now focusing on tackling the issues of FL and exploring various efficient research approaches to overcome these current obstacles. This paper systematically provides a detailed overview of federated learning, covering its definition, the need behind its development, privacy concepts, characteristics, and brief knowledge regarding different system components of federated learning. Different open-source frameworks that are available and used for implementing and solving problems related to federated learning have also been addressed in this article. Beyond this, the taxonomy of federated learning systems and different architectures for the same have also been discussed. In this paper, a brief comparison of related concepts with federated learning and a comparison among existing and popular federated learning studies proposed in different articles in the area of federated learning have also been summarized. In addition to the above-stated information, this article also provides brief information and a summary of various application areas of federated learning. Lastly, this paper briefly addresses the different challenges and prospects of research that lead to progress in this field.},
  archive      = {J_KIS},
  author       = {Chaudhary, Rajesh Kumar and Kumar, Ravinder and Saxena, Nitin},
  doi          = {10.1007/s10115-024-02257-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1811-1914},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic review on federated learning system: A new paradigm to machine learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMNE: Link prediction using hypergraph motifs and network
embedding in social networks. <em>KIS</em>, <em>67</em>(2), 1787–1809.
(<a href="https://doi.org/10.1007/s10115-024-02255-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embeddings, which map nodes to low-dimensional vectors, facilitate link prediction, a pivotal aspect of complex network research. However, existing methods often overlook the complexities of hypergraphs and potent structures for modeling intricate relationships among multiple entities. This paper delves into link prediction within hypergraph motifs and network embedding (HMNE), crucial for diverse fields like knowledge graphs and bioinformatics. HMNE employs motifs to perform network embedding, representing nodes as hyper-nodes. HMNE utilizes the skip-gram model to get the embedding vectors by analyzing the sequence generated using a local random walk technique. Additionally, we consider hyper-motifs as super-nodes to highlight structural similarities between nodes. To further refine our methodology, we use the depth and breadth motif random walk strategy on the embedded network with hyper-nodes. This innovative approach enriches our understanding of network dynamics and enhances the predictive power of our model. We have thoroughly experimented the proposed method on several real-world datasets, and the results consistently demonstrate its usefulness.},
  archive      = {J_KIS},
  author       = {Zhang, Yichen and Lai, Shouliang and Peng, Zelu and Rezaeipanah, Amin},
  doi          = {10.1007/s10115-024-02255-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1787-1809},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HMNE: Link prediction using hypergraph motifs and network embedding in social networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label classification with label clusters.
<em>KIS</em>, <em>67</em>(2), 1741–1785. (<a
href="https://doi.org/10.1007/s10115-024-02270-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is the task of simultaneously predicting a set of labels for an instance, with global and local being the two predominant approaches. The global approach trains a single classifier to handle all classes simultaneously, while the local approach breaks down the problem into multiple binary problems. Despite extensive research, effectively capturing label correlations remains a challenge in both methods. In this paper, we introduce an approach that clusters the label space to create hybrid partitions (disjoint correlated label clusters), striking a balance between global and local strategies while leveraging both advantages. Our approach consists of (i) clustering the label space based on correlations, (ii) generating and validating the resulting hybrid partitions, (iii) selecting the best partitions, and (iv) evaluating their performance. We also compare our approach against an oracle, exhaustive search, and random search to assess how closely our hybrid partitions approximate the best possible partitions. The oracle selects the best partition using the test set, while the exhaustive approach relies on validation data. Experiments conducted on multiple multi-label datasets demonstrate that our method, along with random partitions, achieves results that are superior or competitive compared to traditional global and local approaches, as well as the state-of-the-art Ensemble of Classifier Chains. These findings suggest that conventional methods may not fully capture label correlations, and clustering the label space offers a promising solution.},
  archive      = {J_KIS},
  author       = {Gatto, Elaine Cecília and Ferrandin, Mauri and Cerri, Ricardo},
  doi          = {10.1007/s10115-024-02270-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1741-1785},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-label classification with label clusters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEGODE: A structure-enhanced graph neural ordinary
differential equation network model for temporal link prediction.
<em>KIS</em>, <em>67</em>(2), 1713–1740. (<a
href="https://doi.org/10.1007/s10115-024-02261-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of temporal link prediction is to forecast potential future connections in a network by analyzing its structural underpinnings and tracking its temporal dynamics. However, existing methods for temporal link prediction are overly reliant on the most recent snapshots of the network, thereby limiting their ability to uncover and utilize the fundamental evolutionary patterns for effective dynamical inference. As a result, the predictive prowess of these models tends to be heightened for proximate future scenarios, as opposed to those farther into the horizon. Furthermore, the majority of the current methodologies overlook the influence of intricate higher-order and overarching structural dynamics, which could potentially enhance predictive accuracy. To tackle these challenges, we introduce a structure-enhanced graph neural ordinary differential equation (SEGODE), a comprehensive framework that leverages neural ordinary differential equations integrated with attention mechanisms to facilitate dynamic inference. The framework enhances the ability to snatch higher-order and global structures. To substantiate the viability of our novel model, we embarked on a comprehensive set of experiments conducted on seven real datasets. The outcomes of these rigorous tests demonstrate that our SEGODE approach not just demonstrates commendable performance in the task of link prediction but additionally has good results even when data is sparse.},
  archive      = {J_KIS},
  author       = {Fu, Jiale and Guo, Xuan and Hou, Jinlin and Yu, Wei and Shi, Hongjin and Zhao, Yanxia},
  doi          = {10.1007/s10115-024-02261-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1713-1740},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SEGODE: A structure-enhanced graph neural ordinary differential equation network model for temporal link prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved affinity propagation clustering algorithms: A
PSO-based approach. <em>KIS</em>, <em>67</em>(2), 1681–1711. (<a
href="https://doi.org/10.1007/s10115-024-02260-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional clustering algorithms such as K-means need to input the number of clusters before the start of the algorithm. Affinity propagation (AP) clustering algorithm solves this problem by considering each data point as a prospective cluster head (exemplar) and finding a set of appropriate exemplars by message passing. However, the AP clustering algorithm requires two parameters: preference and damping factor. Providing the parameters in advance poses the same issue faced in the traditional clustering algorithm. Moreover, all data points are not equally relevant for becoming cluster heads. To overcome these problems, we propose two parameter-free particle swarm optimization-based algorithms, PSO-APver1 and PSO-APver2. Furthermore, we introduce a novel version of mutant PSO where two cluster validity indices are used to judge the quality of the clustering solution. In PSO-APver2, we consider the internal data distribution using the square wave function to determine the initial preference value of data points. We conducted experiments on 8 real-world datasets to show the efficacy of our proposed algorithms over classic algorithms and two AP-based algorithms. We conducted the Friedman test followed by post hoc analysis to exhibit the significance of our work.},
  archive      = {J_KIS},
  author       = {Sinha, Ankita and Jana, Prasanta K.},
  doi          = {10.1007/s10115-024-02260-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1681-1711},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved affinity propagation clustering algorithms: A PSO-based approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive consistency analysis for nested probabilistic
linguistic preference relations and its application in decision making.
<em>KIS</em>, <em>67</em>(2), 1651–1680. (<a
href="https://doi.org/10.1007/s10115-024-02231-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making problems often involve complex and uncertain evaluation criteria, making linguistic variables more practical than precise numerical values for describing information. Among various linguistic terms, the nested probabilistic linguistic term excels at expressing multidimensional information through its dual-layer structure, which captures both ordinal and nominal meanings. Using NPLTs to make expressions, this paper introduces nested probabilistic linguistic preference relations (NPLPRs) for effectively conveying preference information in decision-making scenarios and provides new operational rules for quantitative computations. To ensure the consistency of the preference relations, we define a consistency index and threshold to assess the additive consistency of NPLPRs and innovatively propose an automatic iteration algorithm to improve NPLPRs with unacceptable consistency. To reflect the evaluation focus and obtain the final results, we consider the weights of nominal terms when aggregating preferences with acceptable consistency. Finally, an illustrative example of selecting the most environmentally sustainable city demonstrates the application and advantages of our approach, supported by comparative analyses.},
  archive      = {J_KIS},
  author       = {Xiao, Jinglin and Wang, Xinxin and Xu, Zeshui},
  doi          = {10.1007/s10115-024-02231-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1651-1680},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Additive consistency analysis for nested probabilistic linguistic preference relations and its application in decision making},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning and similarity measures for
requirements-driven composition of semantic web services. <em>KIS</em>,
<em>67</em>(2), 1627–1649. (<a
href="https://doi.org/10.1007/s10115-024-02244-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The composition of web service is the best chance provided by Service-Oriented Computing and Service-Oriented Architecture as it gives real competitive benefits for some industrial and technological actors via presenting them with the probability to guarantee fast and inexpensive improvement of collaborative and distributed software applications. Here, a novel technique is introduced, which contains several phases for better web services. At first, the requirements specification phase is enabled with a set of requirements such as non-functional and functional requirements. Next to the requirements specification stage, the discovery stage is enabled to choose the suitable web services that have high-matching profiles with the developer’s requirement set. Here, for a semantic matching algorithm, a new hybrid similarity measure is developed. Additionally, among the group of candidate services that the discovery phase returned, the best service is selected during the selection step. Then, hybrid Squeeze_Long Short-Term Memory (Squeeze_LSTM) is used for choosing the best service and it is designed by the formation of SqueezeNet and LSTM. The Semantic Web Services are finally implemented. The efficiency of the Squeeze_LSTM is evaluated and has achieved a superior precision of 0.909, recall of 0.890, and response time of 6.461S.},
  archive      = {J_KIS},
  author       = {Bhuvaneswari, A. and Sumathi, K. and Sarveshwaran, Velliangiri and Sivasangari, A.},
  doi          = {10.1007/s10115-024-02244-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1627-1649},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid deep learning and similarity measures for requirements-driven composition of semantic web services},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proposed real-time decision support platform for moroccan
fixed mining production systems. <em>KIS</em>, <em>67</em>(2),
1597–1626. (<a
href="https://doi.org/10.1007/s10115-024-02271-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the competition in the mining markets and the rapid evolution of customer requirements, the Moroccan mining group OCP (Office Chérifien des Phosphates) has been forced to improve the performance of its production systems. Thus, continuous performance improvement and optimization of production processes are prerequisites to remain competitive. However, in Morocco, data analytics-based mining process improvements do not fully utilize the data generated during process execution. They lack prescriptive methodologies, which is the major goal of this work, to translate analytic results into improvement actions. Indeed, we propose a new platform for optimizing the production processes of a Moroccan mine based on knowledge extraction from data, allowing mine managers to rapidly and continuously improve the performance of their production chains. The platform will be an effective and efficient tool for mining companies to generate prescriptive action recommendations during the execution of the processes.},
  archive      = {J_KIS},
  author       = {Battas, Ilham and Behja, Hicham and El Ouazguiti, Mohamed},
  doi          = {10.1007/s10115-024-02271-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1597-1626},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A proposed real-time decision support platform for moroccan fixed mining production systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving density peak clustering on multi-dimensional time
series: Rediscover and subdivide. <em>KIS</em>, <em>67</em>(2),
1573–1596. (<a
href="https://doi.org/10.1007/s10115-024-02272-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peak clustering (DPC) algorithm identifies patterns in high-dimensional data and obtains robust outcomes across diverse data types with minimal hyperparameters. However, DPC may produce inaccurate pattern sizes in multi-dimensional datasets and exhibit poor performance in recognizing similar patterns. To solve these issues, we propose the rediscover and subdivide density peak clustering algorithm (RSDPC), which follows three key strategies. The first strategy, rediscover, iteratively uncovers prominent patterns within the existing data. The second strategy, subdivide, partitions patterns into several similar subclasses. The third strategy, re-sort, rectifies errors from the preceding steps by incorporating critical distance and nearest distance considerations. The experimental results show that RSDPC is feasible and effective in synthetic and practical datasets compared with state-of-the-art works.},
  archive      = {J_KIS},
  author       = {Wang, Huina and Liu, Bo and Zhao, Huaipu and Qu, Guangzhi},
  doi          = {10.1007/s10115-024-02272-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1573-1596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving density peak clustering on multi-dimensional time series: Rediscover and subdivide},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A customized balanced-objective genetic algorithm for task
scheduling in reconfigurable computing systems. <em>KIS</em>,
<em>67</em>(2), 1541–1571. (<a
href="https://doi.org/10.1007/s10115-024-02268-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfiguration and hardware implementation capabilities in reconfigurable computing (RC) systems make them more appropriate to recent computationally intensive applications. However, reaching optimal resource utilization remained as one of the main challenges in these systems. In order to implement more tasks in each reconfiguration interval, several decisive factors such as execution time, data communication cost, and required hardware resources must be analyzed simultaneously. In this paper, we proposed a novel balanced-objective task selector combined with a genetic algorithm to efficiently pick up the tasks of an application and occupy the resources as more as possible. The multi-objective fitness function of this algorithm adequately partitions the input application and provides the desirable intra and inter-cluster characteristics. Moreover, a new chromosome encoding technique has been developed to prevent precedence constraint violation of invalid solutions by removing forbidden regions in the search space. We classified the input applications with topological features such as first level parallel tasks (FLPT) and critical path length (CPL) for comprehensive evaluation. Several experiments are performed on randomly generated and real-world Directed Acyclic Graphs (DAGs), and the results are more satisfying in DAGs with more FLPTs and shorter CPLs where up to 28.63% makespan and 29.3% resource utilization improvement have been achieved in comparison with previous methods.},
  archive      = {J_KIS},
  author       = {Gholamrezanejad, Milad and Shahhoseini, Hadi Shahriar and Mohtavipour, Seyed Mehdi},
  doi          = {10.1007/s10115-024-02268-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1541-1571},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A customized balanced-objective genetic algorithm for task scheduling in reconfigurable computing systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSSN: A novel cache placement method based on adapted
shannon entropy and simple additive weighting method in named data
networking. <em>KIS</em>, <em>67</em>(2), 1507–1540. (<a
href="https://doi.org/10.1007/s10115-024-02266-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new strategy called PSSN to address the Cache Placement challenge in NDN. The paper first presents a novel clustering method based on the SAW decision-making approach and dynamically adapted Shannon weighting. Criteria such as the number of neighbors, hop count, router capacity, and CPU power are simultaneously considered for clustering and determining cluster heads. A key innovation in the proposed clustering is storing a copy of each content in each cluster to reduce duplicate content, increase content diversity, and consequently improve hit rate while reducing latency. Subsequently, for content placement, popularity of content, remaining router capacity, and hop count are analyzed concurrently using the SAW method adapted with the proposed approach. This ensures that popular content is placed closer to requesters. Throughout all stages of the method, the dynamic change in the status of content requests from users leads to a dynamic adjustment of the criteria weights. Simulation results using NDNsim demonstrate improvements in key parameters, with average enhancements of 17.8% and 9% for Hit rate and Delivery Time, respectively, as well as a 30.75% improvement in Load Balancing compared to recent methods.},
  archive      = {J_KIS},
  author       = {Soltani, Mohammad and Barekatain, Behrang and Hendessi, Faramarz and Beheshti, Zahra},
  doi          = {10.1007/s10115-024-02266-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1507-1540},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PSSN: A novel cache placement method based on adapted shannon entropy and simple additive weighting method in named data networking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-aware symmetric non-negative matrix factorization
for short text clustering. <em>KIS</em>, <em>67</em>(2), 1481–1506. (<a
href="https://doi.org/10.1007/s10115-024-02226-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text clustering is a significant yet challenging task, where short texts generated from the Internet are extremely sparse, noisy, and ambiguous. The sparse nature makes traditional clustering methods, e.g., k-means family and topic modeling, much less effective. Fortunately, recent arts of document distance, e.g., word mover’s distance, and document representation, e.g., BERT, can accurately measure the similarities of short texts, especially their nearest neighbors. Inspired by those arts and observations, we induce short text clusters by directly factorizing the informative affinity matrix of nearest neighbors into the product of the cluster assignment matrix, following the intuition that neighboring short texts tend to be assigned to the same cluster. However, due to the noisy nature of short texts, many of them can be regarded as outliers or near outliers, resulting in many noisy neighboring similarities within the affinity matrix. To further alleviate this problem, we enhance the affinity matrix factorization by (1) incorporating a sparse noisy matrix to directly capture noisy neighboring similarities and (2) regularizing the cluster assignment matrix by $$\ell _{2,1}$$ norm to eliminate hard-to-clustering short texts (called pseudo-outliers), so as to indirectly neglect noisy neighboring similarities corresponding to them. After this factorization for pre-clustering, we train a classifier over the resulting clusters and adopt it to assign each pseudo-outlier to one cluster finally. We call this novel clustering method as anomaly-aware symmetric non-negative matrix factorization ( $$\hbox {A}^{2}$$ snmf). Experimental results on benchmark short text datasets demonstrate that $$\hbox {A}^{2}$$ snmf performs very competitively with the existing baseline methods. The code is available at the website https://github.com/wizardbo/A3SNMF_functions .},
  archive      = {J_KIS},
  author       = {Li, Ximing and Guan, Yuanyuan and Fu, Bo and Luo, Zhongxuan},
  doi          = {10.1007/s10115-024-02226-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1481-1506},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anomaly-aware symmetric non-negative matrix factorization for short text clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CETra: Online cluster tracking for clustering of streaming
data sources. <em>KIS</em>, <em>67</em>(2), 1455–1479. (<a
href="https://doi.org/10.1007/s10115-024-02267-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream clustering tasks may be applied to cluster streaming data objects (clustering by examples) or to cluster streaming data sources based on their temporal behavior (clustering by variables). We focus on the latter problem and propose CETra (Cluster evolution tracker)—the first online cluster tracking technique designed to provide information regarding cluster evolution in a streaming scenario of clustering by variables with efficient processing suitable for real-time problems. CETra can trace different intra and inter-cluster changes by considering not only statistics of interest but also the clusters’ membership, thus allowing a deeper understanding of the clustering results. Experimental evaluation using synthetic datasets and real data from meteorological sensors shows that CETra can track abrupt and gradual cluster transitions, while the competing method misses most of the gradual changes. Furthermore, CETra performs efficiently in a clustering environment for multiple streaming data sources, twice as fast as the related method.},
  archive      = {J_KIS},
  author       = {Sousa Lima, Afonso Matheus and de Sousa, Elaine Parros Machado},
  doi          = {10.1007/s10115-024-02267-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1455-1479},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CETra: Online cluster tracking for clustering of streaming data sources},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path-based distance computation for non-convexity with
applications in clustering. <em>KIS</em>, <em>67</em>(2), 1415–1453. (<a
href="https://doi.org/10.1007/s10115-024-02275-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms are essential in data analysis, but evaluating their performance is challenging when the true labels are not available, especially for non-convex clusters. Traditional performance evaluation metrics struggle to identify clustering quality, often assigning higher scores for linearly separated clusters than the true clusters. We propose an original approach to distance computation that accounts for the data structure, thus improving the clustering quality evaluation for non-convex clusters without affecting other shapes of clusters. We also showcase the applicability of this method through a modified version of K-Means using the proposed method that is capable of correctly separating non-convex clusters. The validation included the analysis of performance and time complexity of 3 traditional clustering quality evaluation metrics and the K-Means clustering algorithm against their augmented versions with the proposed approach. This analysis conducted on 7 benchmark synthetic datasets and 6 real datasets with various numbers of examples and features of diverse characteristics and joint complexities: simple convex clusters, overlapped and imbalanced clusters, and non-convex clusters. Through these analyses, we show the ineffectiveness of traditional methods and that the proposed approach overcomes the weaknesses of traditional methods.},
  archive      = {J_KIS},
  author       = {Ardelean, Eugen-Richard and Portase, Raluca Laura and Potolea, Rodica and Dînșoreanu, Mihaela},
  doi          = {10.1007/s10115-024-02275-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1415-1453},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A path-based distance computation for non-convexity with applications in clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier detection in classification based on
feature-selection-based regression. <em>KIS</em>, <em>67</em>(2),
1399–1414. (<a
href="https://doi.org/10.1007/s10115-024-02264-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outlier is a datum that is far from other data points in which it occurs. The appearance of outliers results in a complexity to obtain an accurate classification; numerous statistical and machine learning methods have been proposed to identify the outliers. This paper devotes a regression-based algorithm to the detection and identification of outlier before selecting a suitable classifier. The problem is firstly converted to an high-dimensional regression, then a novel method, based on the combination of multiple-correlation-coefficient-based feature selection for dimensional reduction and t-test for sparsification, is proposed, and an iterated algorithm is also given. Performance on simulated numerical data, low-dimensional iris data and high-dimensional DBWorld E-mail data demonstrate the superiority of the proposed method in outlier identification for classification.},
  archive      = {J_KIS},
  author       = {Su, Jinxia and Liu, Qiwen and Cui, Jingke},
  doi          = {10.1007/s10115-024-02264-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1399-1414},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Outlier detection in classification based on feature-selection-based regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An energy-aware migration framework using metaheuristic
algorithm in cloud computing. <em>KIS</em>, <em>67</em>(2), 1373–1398.
(<a href="https://doi.org/10.1007/s10115-024-02224-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive computing requires dynamic, energy-efficient cloud architecture to deploy services in VMs to distributed computing nodes. This mapping must ensure the service level agreement (SLA) runs VMs without disruption. This paper presents an energy-efficient metaheuristic Rock Hyrax algorithm-based cloud VM migration architecture. The male Rock Hyraxes find food and ensure the colony’s safety. His behavior has been mimicked in our proposed algorithm for finding energy-efficient compute nodes. A multi-objective VM migration function considers job submission deadlines. The proposed method reduces SLA violations and energy utilization while optimizing resource utilization. When evaluating the project, makespan, energy efficiency, and SLA violations were considered. The proposed algorithm is simulated on the CloudSim simulator considering both resources and jobs dynamic in nature. The suggested strategy outperforms ant colony optimization, particle swarm optimization, cuckoo optimization, and modified gray wolf optimization. The migration technique improves resource utilization by 18%, makespan time by 5%, SLA violation by 13%, and energy usage by 15%.},
  archive      = {J_KIS},
  author       = {Singhal, Saurabh and Sharma, Ashish},
  doi          = {10.1007/s10115-024-02224-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1373-1398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An energy-aware migration framework using metaheuristic algorithm in cloud computing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A timely personalized comments generation assistant based on
LSTM-SNP. <em>KIS</em>, <em>67</em>(2), 1351–1372. (<a
href="https://doi.org/10.1007/s10115-024-02198-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posting personalized comments on an upcoming hot topic in time is very meaningful, which not only likely to attract more users to participate, but also affects other users’ point of view. LSTM-SNP is a variant of long short-term memory (LSTM) inspired by the nonlinear spiking mechanism in nonlinear spiking neural systems. In order to improve the efficiency and diversity of user-edited comments, we design a novel assistant based on the LSTM-SNP model. The assistant consists of two modules, one for predicting topic hotness and the other for generating comments with personalized expression features based on blog post and user information. Experimental results show that, this novel assistant not only predicts the upcoming hot topics accurately, but also outperforms the baseline model in terms of automatic evaluation and human discernment of comment generation. More importantly, the generated comments excel in terms of timeliness and personalization.},
  archive      = {J_KIS},
  author       = {Li, Yixiao and Wu, Yue and Li, Wenjia and Chen, Hui and Chen, Qi and Li, Yuehui},
  doi          = {10.1007/s10115-024-02198-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1351-1372},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A timely personalized comments generation assistant based on LSTM-SNP},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IC-SNI: Measuring nodes’ influential capability in complex
networks through structural and neighboring information. <em>KIS</em>,
<em>67</em>(2), 1309–1350. (<a
href="https://doi.org/10.1007/s10115-024-02262-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influential nodes are the important nodes that most efficiently control the propagation process throughout the network. Among various structural-based methods, degree centrality, k-shell decomposition, or their combination identify influential nodes with relatively low computational complexity, making them suitable for large-scale network analysis. However, these methods do not necessarily explore nodes’ underlying structure and neighboring information, which poses a significant challenge for researchers in developing timely and efficient heuristics considering appropriate network characteristics. In this study, we propose a new method (IC-SNI) to measure the influential capability of the nodes. IC-SNI minimizes the loopholes of the local and global centrality and calculates the topological positional structure by considering the local and global contribution of the neighbors. Exploring the path structural information, we introduce two new measurements (connectivity strength and effective distance) to capture the structural properties among the neighboring nodes. Finally, the influential capability of a node is calculated by aggregating the structural and neighboring information of up to two-hop neighboring nodes. Evaluated on nine benchmark datasets, IC-SNI demonstrates superior performance with the highest average ranking correlation of 0.813 with the SIR simulator and a 34.1% improvement comparing state-of-the-art methods in identifying influential spreaders. The results show that IC-SNI efficiently identifies the influential spreaders in diverse real networks by accurately integrating structural and neighboring information.},
  archive      = {J_KIS},
  author       = {Nandi, Suman and Curado Malta, Mariana and Maji, Giridhar and Dutta, Animesh},
  doi          = {10.1007/s10115-024-02262-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1309-1350},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {IC-SNI: Measuring nodes’ influential capability in complex networks through structural and neighboring information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised feature selection with minimal redundancy
based on group optimization strategy for multi-label data. <em>KIS</em>,
<em>67</em>(2), 1271–1308. (<a
href="https://doi.org/10.1007/s10115-024-02258-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of intelligence technology, high-dimensional multi-label data exist in practical applications, which makes multi-label learning a significant challenge. Feature selection can obtain more distinguishable features to enhance recognition ability to address high-dimensional problems. Nowadays, most researchers usually evaluate the relevance between labels and features and the similarity between samples. They only focus on the local characteristics of samples without considering the global characteristics. To solve the above problems, in this paper, a novel feature selection approach for semi-supervised learning with minimal redundancy and group optimization strategy (SFGR) in multi-label scenario is proposed. First, a measure based on the Laplacian score and constrain score is utilized to evaluate the relevance between each feature and label. Meanwhile, the global structure of the data is considered via the creation of graphs and a priori information to obtain the feature subset with the highest relevance. Secondly, an optimization iteration algorithm based on a regularization term combining $$\text {l}_1$$ -norm and $$\text {l}_2$$ -norm is employed to ensure the sparsity of the feature weight matrix and minimize the redundancy. Moreover, a group optimal strategy is applied as a global search approach to fusion the feature subsets to obtain an approximate globally optimal feature subset. Eventually, experimental results on various multi-labeled datasets show that SFGR can perform better than other algorithms.},
  archive      = {J_KIS},
  author       = {Qing, Depeng and Zheng, Yifeng and Zhang, Wenjie and Ren, Weishuo and Zeng, Xianlong and Li, Guohe},
  doi          = {10.1007/s10115-024-02258-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1271-1308},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semi-supervised feature selection with minimal redundancy based on group optimization strategy for multi-label data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All is attention for multi-label text classification.
<em>KIS</em>, <em>67</em>(2), 1249–1270. (<a
href="https://doi.org/10.1007/s10115-024-02253-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification(MLTC) is a key task in natural language processing. Its challenge is to extract latent semantic features from text and effectively exploit label-associated features. This work proposes an MLTC model driven solely by attention mechanisms, which includes Graph Attention(GA), Class-Specific Attention(CSA), and Multi-Head Attention(MHA) modules. The GA module examines and records label dependencies by considering label semantic features as attributes of graph nodes. It uses graph embedding to maintain structural relationships within the label graph. Meanwhile, the CSA module produces distinctive features for each category by utilizing spatial attention scores, thereby improving classification accuracy. Then, the MHA module facilitates extensive feature interactions, enhancing the expressiveness of text features and supporting the handling of long-range dependencies. Experimental evaluations conducted on two MLTC datasets show that our proposed model outperforms existing MLTC algorithms, achieving state-of-the-art performance. These results highlight the effectiveness of our attention-based approach in tackling the complexity of MLTC tasks.},
  archive      = {J_KIS},
  author       = {Liu, Zhi and Huang, Yunjie and Xia, Xincheng and Zhang, Yihao},
  doi          = {10.1007/s10115-024-02253-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1249-1270},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {All is attention for multi-label text classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoRBS: A dynamic storytelling algorithm using a novel
contextualization approach for documents utilizing BERT features.
<em>KIS</em>, <em>67</em>(2), 1213–1248. (<a
href="https://doi.org/10.1007/s10115-024-02263-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is the process of connecting documents one after another representing the evolution of an event. Existing algorithms for storytelling connect events based on content overlaps between consecutive documents ignoring the role of the same term in different documents and the contemporary contexts (e.g., dynamic embeddings) of documents and terms. Due to the lack of role and contemporary contexts in the designs of the existing storytelling methods, the resultant stories frequently jump to documents with the keywords to form a chain but not a meaningful one. In this paper, we present a novel storytelling algorithm—Contextual Role-Based Storytelling (CoRBS)—that generates a chain of documents explaining the evolution of an event, addressing role and contemporary context issues of existing methods. CoRBS starts with a given document and moves forward temporally, stitching together role and context-driven documents to represent the evolution of the events that appear in the first document. We define the role of a term in a document as a distribution of similarities of the nearest neighbors of the term based on BERT embeddings of all terms of that document. Contemporary contexts are incorporated as a mechanism to discover a coherent next document while the story progresses. Our experiments demonstrate that CoRBS generates more meaningful stories compared to other baseline storytelling techniques.},
  archive      = {J_KIS},
  author       = {Nouri, Alireza and Hossain, M. Shahriar},
  doi          = {10.1007/s10115-024-02263-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1213-1248},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CoRBS: A dynamic storytelling algorithm using a novel contextualization approach for documents utilizing BERT features},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact lossy compression of tensors via neural tensor-train
decomposition. <em>KIS</em>, <em>67</em>(2), 1169–1211. (<a
href="https://doi.org/10.1007/s10115-024-02252-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TensorCodec, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions.TensorCodec incorporates three key ideas. The first idea is neural tensor-train decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be exploited by NTTD for improved approximation. In addition, we extend TensorCodec to enable the lossy compression of tensors with missing entries, often found in real-world datasets. Our analysis and experiments on 8 real-world datasets demonstrate that TensorCodec is (a) Concise: it gives up to $$7.38 \times $$ more compact compression than the best competitor with similar reconstruction error, (b) Accurate: given the same budget for compressed size, it yields up to $$3.33\times $$ more accurate reconstruction than the best competitor, (c) Scalable: Its empirical compression time is linear in the number of tensor entries, and it reconstructs each entry in logarithmic time. Our code and datasets are available at https://github.com/kbrother/TensorCodec .},
  archive      = {J_KIS},
  author       = {Kwon, Taehyung and Ko, Jihoon and Jung, Jinhong and Jang, Jun-Gi and Shin, Kijung},
  doi          = {10.1007/s10115-024-02252-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1169-1211},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Compact lossy compression of tensors via neural tensor-train decomposition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book recommendation using sentiment analysis and ensembling
hybrid deep learning models. <em>KIS</em>, <em>67</em>(2), 1131–1168.
(<a href="https://doi.org/10.1007/s10115-024-02250-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An immense volume of user-generated content exists online due to the exponential growth of internet usage among individuals. However, this abundance presents substantial challenges for recommendation systems and online customer services. The diverse data, encompassing consumer emails, reviews, and comments, contain a broad spectrum of information, making it challenging to decipher the underlying emotions and nuances. In social media analytics, sentiment analysis has emerged as a pivotal tool to unveil the emotional context embedded within textual data, offering deeper insights into user attitudes, opinions, and sentiments. This study introduces a novel strategy to strengthen the performance and precision of sentiment analysis-based book recommendation systems through ensemble learning on hybrid deep learning models. These book recommendation models leverage customer ratings and reviews from a vast Amazon books dataset as input. Initially, we used TextBlob to assess the polarity of customer reviews, categorizing them into neutral, negative, and positive sentiments. Subsequently, the input data underwent preprocessing, tokenization, and word embedding using bidirectional encoder representations from transformers (BERT). To effectively analyze and filter processed review comments and ratings, the proposed ensemble model integrates a diverse array of hybrid deep learning architectures, including long short-term memory (LSTM), bidirectional LSTM (BiLSTM), gated recurrent unit (GRU), and convolutional neural network (CNN). Extensive experimentation validated the superiority of the proposed ensemble model, achieving an impressive accuracy and F1-score of 98.21%. The significance of the approach lies in its ability to provide more accurate and contextually relevant book recommendations by considering the nuanced emotions expressed in customer reviews. This contributes to enhancing user satisfaction and engagement with recommendation systems, ultimately improving the overall quality of personalized book suggestions. Evaluation metrics further validate the efficacy of the proposed model, underscoring its practical utility in real-world applications of sentiment-based book recommendation systems.},
  archive      = {J_KIS},
  author       = {Devika, P. and Milton, A.},
  doi          = {10.1007/s10115-024-02250-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1131-1168},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Book recommendation using sentiment analysis and ensembling hybrid deep learning models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion pattern mining. <em>KIS</em>, <em>67</em>(2),
1101–1129. (<a
href="https://doi.org/10.1007/s10115-024-02254-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a diffusion network, some nodes exhibit similar diffusion patterns as they have analogous influence reachabilities to the other nodes. When these nodes are selected as initially infected nodes, they tend to yield similar infection results. Mining diffusion patterns of nodes is of practical significance in various applications, such as online marketing and epidemic prevention. Nonetheless, few existing work has effectively addressed this problem. In this work, we investigate how to find out which nodes in a diffusion network share similar diffusion pattern based only on historical infection results. Toward this, we first reconstruct the structure of influence relationships in the network, and then infer the infection propagation probability on each influence relationship, based on which the influence reachability of each node can be estimated. We present a diffusion pattern similarity metric to quantify the similarity of influence reachabilities, and group nodes that share similar influence reachabilities via hierarchical clustering. Extensive experimental results on both synthetic and real-world networks verify the effectiveness and efficiency of our approach.},
  archive      = {J_KIS},
  author       = {Yan, Qian and Yang, Yulan and Yin, Kai and Gan, Ting and Huang, Hao},
  doi          = {10.1007/s10115-024-02254-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1101-1129},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diffusion pattern mining},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward deep multi-view document clustering using enhanced
semantic embedding and consistent context semantics. <em>KIS</em>,
<em>67</em>(2), 1073–1100. (<a
href="https://doi.org/10.1007/s10115-024-02249-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view document clustering (MVDC) is a sophisticated approach in natural language processing that leverages multiple representations or views of data to improve clustering performance. Existing solutions are challenging due to inconsistency of document views, high dimensions, and sparseness in text documents. On the other hand, existing MVDC-based methods often depend on the performance of bag-of-words and pretrained language models. However, these models usually do not consider contextual semantics and are suitable for single-view document clustering. This paper addresses these challenges by proposing a deep MVDC model that utilizes enhanced semantic embedding and consistent context semantics (SECS). SECS uses semantic embedding to address high-dimensional challenges by considering complementary semantic information. Meanwhile, SECS takes advantage of the potential benefits of view-consistent context semantics based on pretrained language models. The proposed model captures intricate semantic relationships between words and documents through advanced embedding techniques, ensuring a richer and more nuanced representation of textual content. Furthermore, by incorporating consistent context semantics, SECS maintains contextual integrity across multiple views, leading to more coherent and meaningful clusters. Experimental results on benchmark datasets demonstrate the superiority of our model over state-of-the-art MVDC methods, highlighting its effectiveness in improving clustering quality and interpretability.},
  archive      = {J_KIS},
  author       = {Du, Yongsheng and Sun, Hongwei and Abdollahi, MohammadJavad},
  doi          = {10.1007/s10115-024-02249-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1073-1100},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Toward deep multi-view document clustering using enhanced semantic embedding and consistent context semantics},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twitter sentiment analysis using ensemble of multi-channel
model based on machine learning and deep learning techniques.
<em>KIS</em>, <em>67</em>(2), 1045–1071. (<a
href="https://doi.org/10.1007/s10115-024-02256-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People post a lot of comments on the websites these days, as social media and the Internet are major parts of modern life. With so much information available online related to services, products, politics, stocks, etc., thus, using artificial intelligence to understand the emotions in these comments is highly beneficial for understanding public opinion. In particular, detecting sentiment polarity in customer reviews is crucial for businesses to make informed decisions. Despite the vast amount of information available on the internet, understanding the underlying emotions in user comments remains a challenge. Our work aims to bridge this gap by proposing a sophisticated sentiment analysis model that leverages state-of-the-art deep learning techniques. In this study, we present a sentiment analysis model that combines advanced deep learning neural networks: convolutional neural network, long short-term memory networks (LSTM), Bidirectional LSTM (BiLSTM), and Bidirectional Encoder Representations from Transformers (BERT). Accurate feature extraction plays a pivotal role in sentiment analysis applications. By merging pre-trained BERT with sophisticated neural networks, the devised model achieves an impressive accuracy of 94.95%. We evaluated the proposed model on a publicly available Twitter Sentiment Analysis dataset. The proposed ensemble multi-channel model outperforms several deep learning and machine learning techniques in sentiment analysis. Hence, we suggest the use of the ensemble model to accurately determine sentiments from tweets and other textual data.},
  archive      = {J_KIS},
  author       = {Tembhurne, Jitendra V. and Lakhotia, Kirtan and Agrawal, Anant},
  doi          = {10.1007/s10115-024-02256-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1045-1071},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Twitter sentiment analysis using ensemble of multi-channel model based on machine learning and deep learning techniques},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic regex synthesis methods for english: A comparative
analysis. <em>KIS</em>, <em>67</em>(2), 1013–1043. (<a
href="https://doi.org/10.1007/s10115-024-02232-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular expressions (short form regex) find their application in program script synthesis, machine translation, information extraction and web applications, such as input validations. Their expressiveness and flexibility make them decidedly the best tool for many challenging text extraction tasks. Writing regex manually has been labeled as a laborious, time consuming and error prone task even for skilled programmers. An abundance of regex generation from text queries at online platforms mainly Stackoverflow and Quora signifies the automatic regex synthesis problem. Despite their popularity, a criminal lack of comprehensive literature study on the problem has also been observed. We intend to perform a detailed review of a variety of methods available for regex synthesis, repair, and learn beneficial lessons for appropriate datasets with one earnest goal: to synthesize resource efficient and correct regexes for given textual description.},
  archive      = {J_KIS},
  author       = {Tariq, Sadia and Rana, Toqir Ahmad},
  doi          = {10.1007/s10115-024-02232-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1013-1043},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automatic regex synthesis methods for english: A comparative analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chromosome segmentation and classification: An updated
review. <em>KIS</em>, <em>67</em>(2), 977–1011. (<a
href="https://doi.org/10.1007/s10115-024-02243-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Karyotyping is a study of chromosomes to identify various chromosomal aberrations related to structure and number. Chromosome image analysis involves challenging issues related to overlapping and touching of chromosomes. Chromosome segmentation and classification generally focus on separating overlapping and touching chromosomes. The analysis methods start from conventional image processing methods to advanced machine learning techniques. These methods are broadly classified into low-level and high-level methods. The low-level methods are thresholding-based approaches, edge detection, feature extraction techniques like active contours and watershed approaches and machine learning for classification. The high-level methods are deep learning algorithms like convolutional neural networks (CNNs), U-Net, autoencoder architectures. These methods help in improving accuracy and automate the process of chromosome segmentation and classification. High-level approaches can handle complexity in chromosome overlaps which provides better segmentation results. The approach learns complicated patterns and structures of chromosome images, which helps in achieving better classification accuracy. The challenges are: (i) working on large and annotated dataset for training deep learning models and (ii) suffer issues with new dataset even in they perform better during training phase. The solution for all these can be a hybrid approach that combines conventional method with modern approaches. This survey gives readers a basic understanding of automated karyotyping and future direction in this domain.},
  archive      = {J_KIS},
  author       = {Somasundaram, Devaraj and Madian, Nirmala and Goh, Kam Meng and Suresh, S.},
  doi          = {10.1007/s10115-024-02243-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {977-1011},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chromosome segmentation and classification: An updated review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mam---12">MAM - 12</h2>
<ul>
<li><details>
<summary>
(2025). Effective human oversight of AI-based systems: A signal
detection perspective on the detection of inaccurate and unfair outputs.
<em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09701-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legislation and ethical guidelines around the globe call for effective human oversight of AI-based systems in high-risk contexts – that is oversight that reliably reduces the risks otherwise associated with the use of AI-based systems. Such risks may relate to the imperfect accuracy of systems (e.g., inaccurate classifications) or to ethical concerns (e.g., unfairness of outputs). Given the significant role that human oversight is expected to play in the operation of AI-based systems, it is crucial to better understand the conditions for effective human oversight. We argue that the reliable detection of errors (as an umbrella term for inaccuracies and unfairness) is crucial for effective human oversight. We then propose that Signal Detection Theory (SDT) offers a promising framework for better understanding what affects people’s sensitivity (i.e., how well they are able to detect errors) and response bias (i.e., the tendency to report errors given a perceived evidence of an error) in detecting errors. Whereas an SDT perspective on the detection of inaccuracies is straightforward, we demonstrate its broader applicability by detailing the specifics for an SDT perspective on unfairness detection, including the need to choose a standard for (un)fairness. Additionally, we illustrate that an SDT perspective helps to better understand the conditions for effective error detection by showing examples of task-, system-, and person-related factors that may affect the sensitivity and response bias of humans tasked with detecting unfairness associated with the use of AI-based systems. Finally, we discuss future research directions for an SDT perspective on error detection.},
  archive      = {J_MAM},
  author       = {Langer, Markus and Baum, Kevin and Schlicker, Nadine},
  doi          = {10.1007/s11023-024-09701-0},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Effective human oversight of AI-based systems: A signal detection perspective on the detection of inaccurate and unfair outputs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How ChatGPT changed the media’s narratives on AI: A
semi-automated narrative analysis through frame semantics. <em>MAM</em>,
<em>35</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11023-024-09705-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform a mixed-method frame semantics-based analysis on a dataset of more than 49,000 sentences collected from 5846 news articles that mention AI. The dataset covers the twelve-month period centred around the launch of OpenAI’s chatbot ChatGPT and is collected from the most visited open-access English-language news publishers. Our findings indicate that during the six months succeeding the launch, media attention rose tenfold—from already historically high levels. During this period, discourse has become increasingly centred around experts and political leaders, and AI has become more closely associated with dangers and risks. A deeper review of the data also suggests a qualitative shift in the types of threat AI is thought to represent, as well as the anthropomorphic qualities ascribed to it.},
  archive      = {J_MAM},
  author       = {Ryazanov, Igor and Öhman, Carl and Björklund, Johanna},
  doi          = {10.1007/s11023-024-09705-w},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Minds Mach.},
  title        = {How ChatGPT changed the media’s narratives on AI: A semi-automated narrative analysis through frame semantics},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical learning theory and occam’s razor: The core
argument. <em>MAM</em>, <em>35</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11023-024-09703-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning theory is often associated with the principle of Occam’s razor, which recommends a simplicity preference in inductive inference. This paper distills the core argument for simplicity obtainable from statistical learning theory, built on the theory’s central learning guarantee for the method of empirical risk minimization. This core “means-ends” argument is that a simpler hypothesis class or inductive model is better because it has better learning guarantees; however, these guarantees are model-relative and so the theoretical push towards simplicity is checked by our prior knowledge.},
  archive      = {J_MAM},
  author       = {Sterkenburg, Tom F.},
  doi          = {10.1007/s11023-024-09703-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Minds Mach.},
  title        = {Statistical learning theory and occam’s razor: The core argument},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and global justice.
<em>MAM</em>, <em>35</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s11023-024-09708-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a philosophically informed and robust account of the global justice implications of Artificial Intelligence (AI). We first discuss some of the key theories of global justice, before justifying our focus on the Capabilities Approach as a useful framework for understanding the context-specific impacts of AI on low- to middle-income countries. We then highlight some of the harms and burdens facing low- to middle-income countries within the context of both AI use and the AI supply chain, by analyzing the extraction of materials, which includes mineral extraction and the environmental harms associated with it, and the extraction of labor, which includes unethical labor practices, low wages, and the trauma experienced by some AI workers. We then outline some of the potential harms and benefits that AI poses, how these are distributed, and what global justice implications this has for low- to middle-income countries. Finally, we articulate the global justice significance of AI by utilizing the Capabilities Approach. We argue that AI must be considered from a global justice perspective given that, globally, AI puts significant downward pressure on several elements of well-being thereby making it harder for people to achieve threshold levels of the central human capabilities needed for a life of dignity.},
  archive      = {J_MAM},
  author       = {Sahebi, Siavosh and Formosa, Paul},
  doi          = {10.1007/s11023-024-09708-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence (AI) and global justice},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Standards for belief representations in LLMs. <em>MAM</em>,
<em>35</em>(1), 1–25. (<a
href="https://doi.org/10.1007/s11023-024-09709-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.},
  archive      = {J_MAM},
  author       = {Herrmann, Daniel A. and Levinstein, Benjamin A.},
  doi          = {10.1007/s11023-024-09709-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {Standards for belief representations in LLMs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cheaper spaces. <em>MAM</em>, <em>35</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11023-024-09704-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity spaces are standardly constructed by collecting pairwise similarity judgments and subjecting those to a dimension-reduction technique such as multidimensional scaling or principal component analysis. While this approach can be effective, it has some known downsides, most notably, it tends to be costly and has limited generalizability. Recently, a number of authors have attempted to mitigate these issues through machine learning techniques. For instance, neural networks have been trained on human similarity judgments to infer the spatial representation of unseen stimuli. However, these newer methods are still costly and fail to generalize widely beyond their initial training sets. This paper proposes leveraging prebuilt semantic vector spaces as a cheap alternative to collecting similarity judgments. Our results suggest that some of those spaces can be used to approximate human similarity judgments at low cost and high speed.},
  archive      = {J_MAM},
  author       = {Moullec, Matthieu and Douven, Igor},
  doi          = {10.1007/s11023-024-09704-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Cheaper spaces},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in algorithmic profiling: The AMAS case.
<em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-024-09706-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a controversial application of algorithmic profiling in the public sector, the Austrian AMAS system. AMAS was supposed to help caseworkers at the Public Employment Service (PES) Austria to allocate support measures to job seekers based on their predicted chance of (re-)integration into the labor market. Shortly after its release, AMAS was criticized for its apparent unequal treatment of job seekers based on gender and citizenship. We systematically investigate the AMAS model using a novel real-world dataset of young job seekers from Vienna, which allows us to provide the first empirical evaluation of the AMAS model with a focus on fairness measures. We further apply bias mitigation strategies to study their effectiveness in our real-world setting. Our findings indicate that the prediction performance of the AMAS model is insufficient for use in practice, as more than 30% of job seekers would be misclassified in our use case. Further, our results confirm that the original model is biased with respect to gender as it tends to (incorrectly) assign women to the group with high chances of re-employment, which is not prioritized in the PES’ allocation of support measures. However, most bias mitigation strategies were able to improve fairness without compromising performance and thus may form an important building block in revising profiling schemes in the present context.},
  archive      = {J_MAM},
  author       = {Achterhold, Eva and Mühlböck, Monika and Steiber, Nadia and Kern, Christoph},
  doi          = {10.1007/s11023-024-09706-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Fairness in algorithmic profiling: The AMAS case},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Submarine cables and the risks to digital
sovereignty. <em>MAM</em>, <em>35</em>(1), 1. (<a
href="https://doi.org/10.1007/s11023-024-09707-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Ganz, Abra and Camellini, Martina and Hine, Emmie and Novelli, Claudio and Roberts, Huw and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09707-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {Minds Mach.},
  title        = {Correction: Submarine cables and the risks to digital sovereignty},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An app a day will (probably not) keep the doctor away: An
evidence audit of health and medical apps available on the apple app
store. <em>MAM</em>, <em>35</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s11023-025-09710-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are more than 350,000 health apps available in public app stores. The extolled benefits of health apps are numerous and well documented. However, there are also concerns that poor-quality apps, marketed directly to consumers, threaten the tenets of evidence-based medicine and expose individuals to the risk of harm. This study addresses this issue by assessing the overall quality of evidence publicly available to support the effectiveness claims of health apps marketed directly to consumers. To assess the quality of evidence available to the public to support the effectiveness claims of health apps marketed directly to consumers, an audit was conducted of a purposive sample of apps available on the Apple App Store. We find the quality of evidence available to support the effectiveness claims of health apps marketed directly to consumers to be poor. Less than half of the 220 apps (44%) we audited state that they have evidence to support their claims of effectiveness and, of these allegedly evidence-based apps, more than 70% rely publicly on either very low or low-quality evidence. For the minority of app developers that do publish studies, significant methodological limitations are commonplace. Finally, there is a pronounced tendency for apps—particularly mental health and diagnostic apps—to either borrow evidence generated in other (typically offline) contexts or to rely exclusively on unsubstantiated, unpublished user metrics as evidence to support their effectiveness claims. Health apps represent a significant opportunity for individual consumers and healthcare systems. Nevertheless, this opportunity will be missed if the health apps market continues to be flooded by poor quality, poorly evidenced, and potentially unsafe apps. It must be accepted that a continuing lag in generating high-quality publicly available evidence of app effectiveness and safety is not inevitable: it is a choice. Just because it will be challenging to raise the quality of the evidence base publicly available to support the claims of health apps, this does not mean that the bar for evidence quality should be lowered. Innovation for innovation’s sake must not be prioritized over public health and safety.},
  archive      = {J_MAM},
  author       = {Morley, Jessica and Laitila, Joel and Ross, Joseph S. and Schamroth, Joel and Zhang, Joe and Floridi, Luciano},
  doi          = {10.1007/s11023-025-09710-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {An app a day will (Probably not) keep the doctor away: An evidence audit of health and medical apps available on the apple app store},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT-4 in the turing test. <em>MAM</em>, <em>35</em>(1),
1–10. (<a href="https://doi.org/10.1007/s11023-025-09711-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been considerable optimistic speculation on how well ChatGPT-4 would perform in a Turing Test. However, no minimally serious implementation of the test has been reported to have been carried out. This brief note documents the results of subjecting ChatGPT-4 to 10 Turing Tests, with different interrogators and participants. The outcome is tremendously disappointing for the optimists. Despite ChatGPT reportedly outperforming 99.9% of humans in a Verbal IQ test, it falls short of passing the Turing Test. In 9 out of the 10 tests conducted, the interrogators successfully identified ChatGPT-4 and the human participant. The probability of obtaining this result from a process in which the interrogator is really no better than chance at correct identification is calculated to be less than 1%. An additional question was posed to the interrogators at the end of each test: What led them to distinguish between the human and the machine? The interrogators, who effectively filtered out ChatGPT-4 from passing the Turing Test for intelligence, stated that they could identify the machine because it, in effect, responded more intelligently than the human. Subsequently, ChatGPT-4 was tasked with differentiating syntax from semantics and self-corrected when falling for the fallacy of equivocation. The curious situation is arrived at that passing the Turing Test for intelligence remains a challenge that ChatGPT-4 has yet to overcome, precisely because, as per the interrogators, its intellectual abilities surpass those of individual humans.},
  archive      = {J_MAM},
  author       = {Restrepo Echavarría, Ricardo},
  doi          = {10.1007/s11023-025-09711-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Minds Mach.},
  title        = {ChatGPT-4 in the turing test},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On twelve shades of green: Assessing the levels of
environmental protection in the artificial intelligence act.
<em>MAM</em>, <em>35</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11023-025-09713-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper examines twelve legal regimes related to the governance and regulation of both the environmental risks and opportunities brought forth by the use of AI systems and AI models in the Artificial Intelligence Act (‘AIA’) of EU law. The assessment of risks and opportunities of AI related to the environment includes the high-risk management procedures under Art. 9 of the AIA, the “fundamental rights impact assessment” of Art. 27, and the codes of conduct of Art. 95. These provisions are supplemented by further regulatory regimes, such as the proposal of EU directive on sustainable consumption and green claims, and Reg. (EU) 2023/588 on environmental and space sustainability, among others. The aim of the analysis is to specify which are the less or the more environmentally friendly regulatory regimes set up with the AIA. The claim is that Art. 9, 27 and 95 are among the less green pieces of the whole legislation.},
  archive      = {J_MAM},
  author       = {Pagallo, Ugo},
  doi          = {10.1007/s11023-025-09713-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Minds Mach.},
  title        = {On twelve shades of green: Assessing the levels of environmental protection in the artificial intelligence act},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The testimony gap: Machines and reasons. <em>MAM</em>,
<em>35</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11023-025-09712-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most people who have considered the matter have concluded that machines cannot be moral agents. Responsibility for acting on the outputs of machines must always rest with a human being. A key problem for the ethical use of AI, then, is to ensure that it does not block the attribution of responsibility to humans or lead to individuals being unfairly held responsible for things over which they had no control. This is the “responsibility gap”. In this paper, we argue that the claim that machines cannot be held responsible for their actions has unacknowledged implications for the conditions under which the outputs of AI can serve as reasons for belief. Following Robert Brandom, we argue that, because the assertion of a claim is an action, moral agency is a necessary condition for the giving and evaluating of reasons in discourse. Thus, the same considerations that suggest that machines cannot be held responsible for their actions suggest that they cannot be held to account for the epistemic value — or lack of value — of their outputs. If there is a responsibility gap, there is also a “testimony gap.” An under-recognised problem with the use of AI, then, is to ensure that it does not block the attribution of testimony to human beings or lead to individuals being held responsible for claims that they have not asserted. More generally, the “assertions” of machines are only capable of serving as justifications for belief or action where one or more people accept responsibility for them.},
  archive      = {J_MAM},
  author       = {Sparrow, Robert and Flenady, Gene},
  doi          = {10.1007/s11023-025-09712-5},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Minds Mach.},
  title        = {The testimony gap: Machines and reasons},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="meco---8">MECO - 8</h2>
<ul>
<li><details>
<summary>
(2025). An adaptive matrix-based evolutionary computation framework
for EEG feature selection. <em>MECO</em>, <em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s12293-024-00434-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) plays a significant role in emotion recognition because it contains abundant information. However, due to the highly correlated EEG channels, a lot of redundant EEG features exist, which not only potentially degrade the emotion recognition accuracy, but also bring high computational costs. To address this challenge, this paper proposes an adaptive matrix-based evolutionary computation framework (AMEC) to select as few informative EEG features as possible for effective emotion recognition. Unlike most existing EC algorithms that utilize vector-based operations, this framework leverages matrix-based operations to reduce feature redundancy and improve classification accuracy by dynamically adjusting the feature subset size according to the characteristics of the dataset. In such a way, the selection efficiency is largely improved. To verify the effectiveness and efficiency of this framework, the classical genetic algorithm, the typical particle swarm optimization algorithm, and the classical differential evolution algorithm, are respectively embedded into this framework for EEG feature selection, and then evaluated on three widely used public EEG datasets for emotion recognition. Compared with several state-of-the-art EEG feature selection algorithms, the devised framework is much more effective in terms of the classification accuracy and the computational efficiency. In addition, the experimental results further reveal that the selected feature subsets are very different for different genders. This indicates the demand of gender-sensitive EEG feature selection for emotion recognition.},
  archive      = {J_MECO},
  author       = {Duan, Danting and Sun, Bing and Yang, Qiang and Ye, Long and Zhang, Qin and Zhang, Jun},
  doi          = {10.1007/s12293-024-00434-2},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Memet. Comput.},
  title        = {An adaptive matrix-based evolutionary computation framework for EEG feature selection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated design of state transition rules in ant colony
optimization by genetic programming: A comprehensive investigation.
<em>MECO</em>, <em>17</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s12293-025-00435-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated design of Ant Colony Optimization (ACO) algorithms has become increasingly significant, particularly in addressing complex combinatorial optimization problems. Although existing methods have achieved some success, they still face limitations, particularly the high dependency on expert knowledge, pre-solved data, and challenges in interpretability. Genetic Programming (GP), as a proven technology, has shown potential in optimizing the automated design state transition rules of ACO. However, existing research on GP-ACO is insufficient, particularly in terms of experimental validation and systematic evaluation. To address these issues, this study conducts comprehensive experiments to explore several key questions: the generality of GP-ACO on homogeneously distributed maps, the impact of different ACO variants on the learning capabilities of GP-ACO, the effect of 2-opt local search on the learning capabilities of GP-ACO, the enhancement of learning capabilities through the addition of more global information, and the interpretability of GP-ACO. The findings indicate that GP-ACO exhibits robust generality; variations among ACO variants have minimal impact on learning performance; 2-opt local search can somewhat diminish the performance of GP-ACO in the Max–Min Ant System; additional global information can significantly enhance the learning capabilities of GP-ACO; and GP-ACO has good interpretability.},
  archive      = {J_MECO},
  author       = {Lin, Bo-Cheng and Mei, Yi and Zhang, Mengjie},
  doi          = {10.1007/s12293-025-00435-9},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Memet. Comput.},
  title        = {Automated design of state transition rules in ant colony optimization by genetic programming: A comprehensive investigation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event extraction based on self-data augmentation with large
language models. <em>MECO</em>, <em>17</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s12293-025-00436-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event extraction plays a crucial role in natural language processing (NLP), facilitating the transformation of unstructured text into structured representations. This conversion significantly enhances the performance of various applications, such as automated question answering and information retrieval systems. However, traditional event extraction methodologies often encounter challenges stemming from limited datasets, imbalanced sample distributions, the necessity for extra resources to annotate large datasets, and the potential for data quality degradation during the augmentation process. To surmount these obstacles, this study introduces an innovative self-data augmentation strategy that leverages a single large language model (LLM) to concurrently perform data augmentation and event extraction. By dynamically assessing and refining the quality of generated samples, this approach mitigates the inclusion of noisy data, ultimately bolstering the model’s performance. Demonstrable enhancements in precision, recall, and F1 scores across various model configurations underscore the efficacy of this strategy in managing small and imbalanced datasets. Furthermore, the incorporation of Logical Thoughts for Self-Data Augmentation (LoTSA) ensures the superior quality of augmented data, culminating in more accurate and reliable extraction outcomes.},
  archive      = {J_MECO},
  author       = {Yang, Lishan and Fan, Xi and Wang, Xiangyu and Wang, Xin and Chen, Qiuju},
  doi          = {10.1007/s12293-025-00436-8},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Memet. Comput.},
  title        = {Event extraction based on self-data augmentation with large language models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel efficient bi-objective evolutionary algorithm for
frequent and high utility itemsets mining. <em>MECO</em>,
<em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12293-025-00437-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining frequent and high utility itemsets (FHUIs) from transaction database is an important task in data mining. In order to overcome the difficulties of parameter setting and huge search space in traditional algorithms for mining FHUIs, the task of mining FHUIs was modeled as a bi-objective problem and then solved by multi-objective evolutionary algorithms (MOEAs) in previous works. However, MOEAs may be inefficient when the number of transactions and items in the transaction database becomes large. To address this problem, we propose a novel efficient bi-objective evolutionary algorithm for mining FHUIs (NBOEA-FHUI). In NBOEA-FHUI, a novel initialization strategy is proposed, which takes the support, utility, and diversity of the initial population into account. The proposed initial strategy can make the initial population have relative high utility and support values with high population diversity. To improve the quality of the offspring, a method for estimating the support and utility value of itemsets and an offspring generation strategy are proposed in NBOEA-FHUI. The support and utility values of itemsets which are roughly proportional to their true values can be calculated by the estimation method with little computation. The proposed offspring generation strategy can generate better offspring based on the estimated support and utility value. Experimental results on several real datasets demonstrate that the proposed algorithm has better performance than the state-of-the-art MOEAs in terms of the convergence speed, search efficiency, and solution accuracy in the task of mining FHUIs.},
  archive      = {J_MECO},
  author       = {Ma, Li and Li, Chongyang and Lu, Heng-yang and Fang, Wei and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s12293-025-00437-7},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Memet. Comput.},
  title        = {A novel efficient bi-objective evolutionary algorithm for frequent and high utility itemsets mining},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A constrained large-scale lever evolutionary algorithm for
white-box problems and its application in spectral-energy efficiency
tradeoff of massive MIMO. <em>MECO</em>, <em>17</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s12293-025-00438-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing constrained multi-objective evolutionary algorithms are not so efficient when handling constrained large-scale multi-objective problems (CLSMOPs). To overcome white-box CLSMOPs with definitive objective functions, a spatial–temporal lever evolutionary algorithm (STLEA), consisting of the lever evolutionary algorithm (LEA) and spatial–temporal preference strategy (STPS), is proposed. LEA ditches the thought of the mainstream algorithms for the similar problems, which changes the structure of the large-scale decision space, but handles the large-scale decision space by a certain small-scale decision space. Specifically, inspired by the lever principle, LEA explores the way to pry up the large-scale decision space, as the “load”, by the small-scale decision space, as the “force”. Meanwhile, LEA rotates the optimizations in between “load” and “force” for dual-balance: balance between objectives and constraints, and balance between convergence and diversity of solutions. STPS dynamically adjusts the proportion of optimizations in “load” and “force”. Different from existing preference strategies, which only consider the stage of the evolutionary procedure, STPS considers both stage, related to time, and varying scale of the decision space, related to space, for the comprehensive balance of feasibility, convergence, and diversity of solutions. Eleven representative and state-of-the-art constrained multi-objective evolutionary algorithms have been compared to the proposed STLEA to demonstrate its effectiveness through comparative experiments on through comparative experiments on CLSMOPs with equality and inequality constraints and 1000 decision variables and three typical MaMIMO-LU models with 1024 antennas and 128, 256, and 512 users. Experimental results show that STLEA achieves the best SE-EE tradeoff.},
  archive      = {J_MECO},
  author       = {Wang, Qingzhu and Li, Tianyang},
  doi          = {10.1007/s12293-025-00438-6},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Memet. Comput.},
  title        = {A constrained large-scale lever evolutionary algorithm for white-box problems and its application in spectral-energy efficiency tradeoff of massive MIMO},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clinical causal analysis via iterative active structure
learning. <em>MECO</em>, <em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12293-025-00439-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning has achieved considerable success in clinical applications such as image-based diagnostics, predictive modeling for patient outcomes, and personalized treatment planning. However, the black-box nature of deep neural networks often results in poor interpretability and reliability of predictions. Traditional neural network architectures, focusing primarily on correlations, fall short in elucidating underlying causal medical mechanisms. Addressing this, causal discovery, aimed at elucidating the structure of causal graphical models from observational or experimental data, is gaining prominence in clinical fields demanding high reliability. Nevertheless, the complexity of search algorithms, the scarcity of real-world data, and the challenges in identifying unique results significantly hinder the reliability of these approaches. To overcome these challenges, we propose an iterative active structure learning approach to ensure reliable clinical causal analysis. Our method begins with the recovery of a causal structure, guided by a set of prior causal presence, followed by an iterative process of active refinement to enhance the output reliability. This involves using violations of known clinical mechanisms as structural constraints to guide successive rounds of learning, thereby correcting and refining the model iteratively. The process continues until there is a convergence between expertise and the data-derived solutions. Our experiments on real-world clinical data demonstrate that Our approach can improve the quality of causal findings and discover new causal associations beyond the basis of expert knowledge. Furthermore, our approach has yielded novel and significant insights from various datasets, which we explore in our discussion.},
  archive      = {J_MECO},
  author       = {Tao, Zhenchao and Chi, Meiyan and Chen, Lyuzhou and Ban, Taiyu and Tu, Qiang and Gao, Fei and Wang, Wei},
  doi          = {10.1007/s12293-025-00439-5},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Memet. Comput.},
  title        = {Clinical causal analysis via iterative active structure learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simro-dino: A rotary positional DINO with siamese structure
for traffic object detection under adverse conditions. <em>MECO</em>,
<em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s12293-025-00440-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection on traffic road is crucial for enabling real-time analysis of road conditions and can be applied in intelligent transportation systems. However, in real world, we may encounter cases of low visibility, occlusion and lens contamination, where general methods of object detection usually degrade. In order to address this problem, we propose a Rotary Positional DINO with Siamese Structure (SimRo-DINO) framework, which efficiently overcomes the difficulty associated with object detection under adverse conditions. Specifically, to extract salient detail features and distinguish them from extraneous interference information, we leverage siamese representation learning along with random masking, which is named Mask Siamese Subnetwork, improving the robustness under adverse conditions. Furthermore, to enhance the connection between features scattered by various interferences and capture latent positional information under adverse conditions, we introduce Rotary Position Embedding into Co-DINO framework, an end-to-end detector with the capacity of capturing long-range dependency relationships within images. Extensive experiments have been conduct on UA-DETRAC and our self-built dataset, both under the adverse conditions. The results from our experiments indicate a substantial advancement in mean Average Precision (mAP) of 2.3 and 2.5% on these two datasets, respectively, compared to the Co-DINO baseline. The related codes are publicly available at https://github.com/xhzhou123/SimRo-DINO .},
  archive      = {J_MECO},
  author       = {Lei, Meng and Zhou, Xinghan and Zhao, Tianju and Xu, Shifan and Zou, Liang},
  doi          = {10.1007/s12293-025-00440-y},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Memet. Comput.},
  title        = {Simro-dino: A rotary positional DINO with siamese structure for traffic object detection under adverse conditions},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task assisted multi-objective optimization algorithm
for autonomous underwater vehicle path planning. <em>MECO</em>,
<em>17</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s12293-025-00441-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the application of multi-objective optimization algorithms to craft feasible paths considering multiple factors has garnered significant attention in handling path planning problems for autonomous underwater vehicles. However, the construction of appropriate multi-objective problem models coupled with efficient search strategies emerges as a pivotal determinant influencing the performance of multi-objective path planning algorithms. This paper introduces a multi-task assisted multi-objective optimization algorithm (MAMO) tailored to address autonomous underwater vehicle path planning problems. The proposed multi-task framework encompasses two tasks: the original path planning task and a devised simple task. These two tasks have different decision spaces due to distinct encoding strategies. Additionally, two different yet interconnected multi-objective problem models are deployed in the above two tasks. Furthermore, two knowledge transfer strategies, domain mapping-based and reconstruction-based knowledge transfer strategies, are introduced to leverage the knowledge from the simple task to assist the original task. The efficacy of the proposed MAMO is compared against eight counterparts and evaluated on three autonomous underwater vehicle path planning cases with different numbers of obstacles. The empirical findings corroborate the efficacy of the algorithm proffered.},
  archive      = {J_MECO},
  author       = {Liu, Tianyu and Wu, Yu and Xu, He},
  doi          = {10.1007/s12293-025-00441-x},
  journal      = {Memetic Computing},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Memet. Comput.},
  title        = {Multi-task assisted multi-objective optimization algorithm for autonomous underwater vehicle path planning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ml---32">ML - 32</h2>
<ul>
<li><details>
<summary>
(2025). An empirical study on impact of label noise on synthetic
tabular data generation. <em>ML</em>, <em>114</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s10994-024-06629-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data has been actively used for various machine learning-based tasks due to its benefits such as massive reproducibility and privacy enhancement compared to using the original data. The quality of the generated synthetic dataset crucially depends on the quality of the original data, and the latter is often corrupted by label noise. While there have been studies on feature noise, how label noise affects synthetic data generation is under-explored. In this paper, we evaluate the impact of the noisy label on synthetic data generation with a focus on tabular data. One challenge is how to evaluate the quality of synthetic data under label noise. To this end, we design comprehensive experiments to measure the impact of label noise on synthetic data generation in different aspects: synthetic data quality, data utility, and convergence for training synthesizers and machine learning models for downstream tasks. The empirical results cover wide aspects of synthetic data generation under label noise and they show quality and utility degrades with higher noise levels while there is no significant effect on the synthesizer convergence observed.},
  archive      = {J_ML},
  author       = {Kim, Jeonghoon and Huang, Chao and Liu, Xin},
  doi          = {10.1007/s10994-024-06629-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {An empirical study on impact of label noise on synthetic tabular data generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing cross-validation variance through seed blocking in
hyperparameter tuning. <em>ML</em>, <em>114</em>(4), 1–48. (<a
href="https://doi.org/10.1007/s10994-024-06630-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross-validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) is commonly employed to reduce the variability of CV errors. This study investigates the efficacy of blocking cross-validation partitions and algorithm initialization seeds during hyperparameter tuning. The proposed approach, termed Controlled Cross-Validation (CCV), reduces variability in error estimates, enabling fairer and more reliable comparisons of predictive model performance. We provide both theoretical and empirical evidence to demonstrate that this blocking approach lowers the variance of the estimates compared to RCV. Our experiments indicate that the algorithm’s internal random behavior often does not significantly affect CV error variability. We present extensive examples using real-world datasets to compare the effectiveness and efficiency of blocking the CV partitions when tuning the hyperparameters of different supervised predictive learning algorithms.},
  archive      = {J_ML},
  author       = {Merola, Giovanni Maria},
  doi          = {10.1007/s10994-024-06630-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-48},
  shortjournal = {Mach. Learn.},
  title        = {Reducing cross-validation variance through seed blocking in hyperparameter tuning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive learning of robot task knowledge from raw data and
online expert feedback. <em>ML</em>, <em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06636-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios. In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user’s feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.},
  archive      = {J_ML},
  author       = {Meli, Daniele and Fiorini, Paolo},
  doi          = {10.1007/s10994-024-06636-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Inductive learning of robot task knowledge from raw data and online expert feedback},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient projection-free online convex optimization using
stochastic gradients. <em>ML</em>, <em>114</em>(4), 1–61. (<a
href="https://doi.org/10.1007/s10994-024-06640-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Online Convex Optimization (OCO) problems subject to a compact convex set. An important class of projection-free online methods known as Frank–Wolfe-type (FW-type) methods have attracted considerable attention in the machine learning community, as they eschew the expensive projection operation and only require a simple linear minimization oracle in each round. Recently, the stochastic gradient technique has been integrated in FW-type online methods to circumvent the expensive full gradient computation and further reduce the per-round computational cost. However, these methods generally have high regret bounds due to high variance in gradient estimation. Although adopting a large minibatch in stochastic gradients can reduce the variance, it would in turn increase the per-round computational cost. In this paper, we develop efficient FW-type methods that only need stochastic gradients with small minibatch and achieve nearly optimal regret bounds with low per-round costs. We first explore the similarity between gradients of decision variables in consecutive rounds, and construct a lightweight variance-reduced estimator by utilizing historical gradient information. Based on this estimator, we propose a method named OFWRG for smooth problems in the stochastic setting. We prove that OFWRG achieves a nearly optimal regret bound with the lowest $$\mathcal {O}(1)$$ per-round computational cost. OFWRG is the first method with such nearly optimal result in this setting. We further extend OFWRG to OCO problems in other settings, including smooth problems in the adversarial setting and a class of non-smooth problems in the stochastic and adversarial settings. Our theoretical analyses show that these extensions of OFWRG achieve nearly optimal regret bounds and low per-round computational costs under mild conditions. Experimental results demonstrate the efficiency of our methods.},
  archive      = {J_ML},
  author       = {Xie, Jiahao and Zhang, Chao and Shen, Zebang and Qian, Hui},
  doi          = {10.1007/s10994-024-06640-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-61},
  shortjournal = {Mach. Learn.},
  title        = {Efficient projection-free online convex optimization using stochastic gradients},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated explanations for regression. <em>ML</em>,
<em>114</em>(4), 1–34. (<a
href="https://doi.org/10.1007/s10994-024-06642-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) methods are an integral part of modern decision support systems. The best-performing predictive models used in AI-based decision support systems lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature’s importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations, previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is below an arbitrary threshold. The extension for regression keeps all the benefits of Calibrated Explanations, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of feature importance, and allows both factual and counterfactual explanations. Calibrated Explanations for regression provides fast, reliable, stable, and robust explanations. Calibrated Explanations for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model, allowing dynamic selection of thresholds. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available on GitHub and for installation using both pip and conda, making the results in this paper easily replicable.},
  archive      = {J_ML},
  author       = {Löfström, Tuwe and Löfström, Helena and Johansson, Ulf and Sönströd, Cecilia and Matela, Rudy},
  doi          = {10.1007/s10994-024-06642-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Calibrated explanations for regression},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforced logical reasoning over KGs for interpretable
recommendation system. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06646-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various domains, traditional recommendation systems have demonstrated significant benefits. However, their &quot;black box&quot; mechanisms have led to a crisis of trust among users. Interpretable recommendation systems have emerged as a solution by providing explanations for recommended items, thus enhancing transparency and user confidence. Another challenge to interpretable recommendation systems is data sparsity, which causes subpar recommendation performance. Addressing the challenges of model interpretability and data sparsity, this paper introduces the Knowledge Graphs-based Logic Reasoning Recommendation (KG-LRR) method, structured around an &quot;encoder-decoder&quot; architecture. The KG-LRR method tackles these issues by leveraging a knowledge graph for items to enhance the representation of users and items during the encoding process. It introduces a propositional logic reasoning model for decoding, rendering explanations in a more comprehensible manner. This dual approach ensures a balance between the recommendation system’s efficiency and interpretability. The KG-LRR method employs a neural network to simulate human-like propositional logical reasoning. This not only mitigates data sparsity issues but also explicates users’ interest in items. It provides deeper insights into users’ preferences and delivers robust interpretability. Experimental results across three public datasets-Yelp2018, Amazon-book, and Amazon-electronics-demonstrate that the KG-LRR model outperforms existing methods in terms of Recall and nDCG in top-k ranking recommendation scenarios. This validates its superior performance compared to prevailing interpretable recommendation techniques. In summary, the KG-LRR method offers a novel approach to enhance transparency and performance through an innovative &quot;encoder-decoder&quot; architecture. Its integration of knowledge graphs and propositional logic reasoning showcases promising outcomes in addressing current challenges within interpretable recommendation systems. Our code is available at https://github.com/siri-ya/KG-LRR .},
  archive      = {J_ML},
  author       = {Wang, Shirui and Xie, Bohan and Ding, Ling and Chen, Jianting and Xiang, Yang},
  doi          = {10.1007/s10994-024-06646-4},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Reinforced logical reasoning over KGs for interpretable recommendation system},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified convergence analysis for adaptive optimization with
moving average estimator. <em>ML</em>, <em>114</em>(4), 1–51. (<a
href="https://doi.org/10.1007/s10994-024-06650-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adaptive optimization algorithms have been successful in many applications, there are still some mysteries in terms of convergence analysis that have not been unraveled. This paper provides a novel non-convex analysis of adaptive optimization to uncover some of these mysteries. Our contributions are three-fold. First, we show that an increasing or large enough momentum parameter for the first-order moment used in practice is sufficient to ensure the convergence of adaptive algorithms whose adaptive scaling factors of the step size are bounded. Second, our analysis gives insights for practical implementations, e.g., increasing the momentum parameter in a stage-wise manner in accordance with stagewise decreasing step size would help improve the convergence. Third, the modular nature of our analysis allows its extension to solving other optimization problems, e.g., compositional, min–max and bilevel problems. As an interesting yet non-trivial use case, we present algorithms for solving non-convex min–max optimization and bilevel optimization that do not require using large batches of data to estimate gradients or double loops as the literature do. Our empirical studies corroborate our theoretical results.},
  archive      = {J_ML},
  author       = {Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  doi          = {10.1007/s10994-024-06650-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-51},
  shortjournal = {Mach. Learn.},
  title        = {Unified convergence analysis for adaptive optimization with moving average estimator},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the usefulness of the fit-on-test view on evaluating
calibration of classifiers. <em>ML</em>, <em>114</em>(4), 1–75. (<a
href="https://doi.org/10.1007/s10994-024-06652-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibrated uncertainty estimates are essential for classifiers used in safety-critical applications. If a classifier is uncalibrated, then there is a unique way to calibrate its uncertainty using the idealistic true calibration map corresponding to this classifier. Although the true calibration map is typically unknown in practice, it can be estimated with many post-hoc calibration methods which fit some family of potential calibration functions on a validation dataset. This paper examines the connection between such post-hoc calibration methods and calibration evaluation. Despite the negative connotations of fitting on test data in machine learning, we claim that fitting calibration maps on test data as part of the calibration evaluation process is a method worth considering, and we refer to this view as fit-on-test. This view enables the usage of any post-hoc calibration method as an evaluation measure, unlocking missed opportunities in development of evaluation methods. We prove that even ECE, which is the most common calibration evaluation method, is actually a fit-on-test measure. This observation leads us to a new method of tuning the number of bins in ECE with cross-validation. Fitting on test data can lead to test-time overfitting, and therefore, we discuss the limitations and concerns with the fit-on-test view. Our contributions also include: (1) enhancement of reliability diagrams with diagonal filling; (2) development of new calibration map families PL and PL3; and (3) an experimental study of which families perform strongly both as post-hoc calibrators and calibration evaluators.},
  archive      = {J_ML},
  author       = {Kängsepp, Markus and Valk, Kaspar and Kull, Meelis},
  doi          = {10.1007/s10994-024-06652-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-75},
  shortjournal = {Mach. Learn.},
  title        = {On the usefulness of the fit-on-test view on evaluating calibration of classifiers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum causal entropy inverse constrained reinforcement
learning. <em>ML</em>, <em>114</em>(4), 1–44. (<a
href="https://doi.org/10.1007/s10994-024-06653-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements specific to that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide a practical implementation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and environments, and it is able to handle problems with stochastic dynamics and a continuous state-action space.},
  archive      = {J_ML},
  author       = {Baert, Mattijs and Mazzaglia, Pietro and Leroux, Sam and Simoens, Pieter},
  doi          = {10.1007/s10994-024-06653-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-44},
  shortjournal = {Mach. Learn.},
  title        = {Maximum causal entropy inverse constrained reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A prompt-driven framework for multi-domain knowledge
tracing. <em>ML</em>, <em>114</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10994-024-06660-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) models students’ knowledge states to predict future performance based on historical interactions. Due to data privacy concerns and budget constraints, the availability of high-quality student data differs across domains and it is essential to effectively utilize KT data from multiple domains. In this work, we propose a novel prompt-enhanced paradigm, i.e., promptKT, to utilize student data from multiple domains to improve KT performance simultaneously. Specifically, a unified Transformer based backbone model is first pre-trained using data from all the KT domains to capture the commonality across domains. Then, we design a novel soft domain prompt module to capture the distinctions among various domains and users. Our promptKT is evaluated on six public real-world educational datasets. The results demonstrate that our approach outperforms the majority of existing KT models in terms of AUC and accuracy. Furthermore, empirical analysis shows the decent transferability and adaptation of promptKT across multiple KT domains. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit .},
  archive      = {J_ML},
  author       = {Liu, Zitao and Huang, Shuyan and Guo, Teng and Hou, Mingliang and Liang, Qianru},
  doi          = {10.1007/s10994-024-06660-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A prompt-driven framework for multi-domain knowledge tracing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capturing the context-aware code change via dynamic control
flow graph for commit message generation. <em>ML</em>, <em>114</em>(4),
1–23. (<a href="https://doi.org/10.1007/s10994-024-06671-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commit messages that summarize code changes of each commit in natural language help developers understand code changes without digging into implementation details, thus playing an essential role in comprehending software evolution. In constructing models for automatic commit message generation, prior research has focused on extracting information from the changed code hunks (i.e., code difference), while ignoring the unchanged code hunks (i.e., code context). However, most studies often neglect the fact that the code change is context-aware, that is the semantics of the code difference are heavily dependent on its code context. To take the code context into account, a key challenge arises: the extensive code context may overshadow the minuscule code difference in capturing the changed semantics, which is a disadvantage to commit message generation. In this paper, we propose the dynamic control flow graph (DCFG), which combines both the code contexts and code differences into one dynamic global–local structure. Based on DCFG, we design a novel framework termed capturing the context-aware code change for commit message generation ( $${\text {C}^4\text {MG}}$$ ), which attempts to model the changed semantics of the code change based on the relevant code context, while avoiding being misled by the overwhelming amount of unchanged code context. Extensive experiments demonstrate that benefiting from modeling the context-aware code change, $${\text {C}^4\text {MG}}$$ outperforms not only the state-of-the-art open-source models but also the large language models (e.g., LLaMA3, GPT-4o, and Gemini) on the commit message generation.},
  archive      = {J_ML},
  author       = {Du, Yali and Li, Ying and Ma, Yi-Fan and Li, Ming},
  doi          = {10.1007/s10994-024-06671-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Capturing the context-aware code change via dynamic control flow graph for commit message generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nettop: A light-weight network of orthogonal-plane features
for image recognition. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06672-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current light-weight CNN-based networks, convolutional operators are principally utilized to extract feature maps for image representation. However, such conventional operation can lead to lack of informative patterns for the learning process. It is because the operators have just been allocated to convolute on the spatial side of an input tensor. To deal with this deficiency, we propose a competent model to efficiently exploit the full-side features of a tensor. The proposed model is based on three novel concepts as follows. i) A novel grouped-convolutional operator is defined to produce complementary features in consideration of three plane-based volumes that have been correspondingly partitioned subject to three orthogonal planes (TOP) of a given tensor. ii) An effective perceptron block is introduced to take into account the TOP-based operator for orthogonal-plane feature extraction. iii) A light-weight backbone of TOP-based blocks (named NetTOP) is proposed to take advantage of the full-side informative patterns for image representation. Experimental results for image recognition on benchmark datasets have proved the prominent performance of the proposals. The code of NetTOP is available at https://github.com/nttbdrk25/NetTOP .},
  archive      = {J_ML},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10994-024-06672-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Nettop: A light-weight network of orthogonal-plane features for image recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HorNets: Learning from discrete and continuous signals with
routing neural networks. <em>ML</em>, <em>114</em>(4), 1–23. (<a
href="https://doi.org/10.1007/s10994-024-06673-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction of neural network architectures suitable for learning from both continuous and discrete tabular data is challenging, as contemporary high-dimensional tabular data sets are often characterized by a relatively small set of instances and the request for efficient learning. We propose HorNets (Horn Networks), a neural network architecture with state-of-the-art performance on synthetic and real-life data sets from scarce-data tabular domains. HorNets are based on a clipped polynomial-like activation function, extended by a custom discrete-continuous routing mechanism that decides which part of the neural network to optimize based on the input’s cardinality. By explicitly modeling parts of the feature combination space or combining whole space in a linear attention-like manner, HorNets dynamically decide which mode of operation is the most suitable for a given piece of data with no explicit supervision. This architecture is one of the few approaches that reliably retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art classification performance on 14 real-life biomedical high-dimensional data sets. HorNets are made freely available under a permissive license alongside a synthetic generator of categorical benchmarks.},
  archive      = {J_ML},
  author       = {Koloski, Boshko and Lavrač, Nada and Škrlj, Blaž},
  doi          = {10.1007/s10994-024-06673-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {HorNets: Learning from discrete and continuous signals with routing neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TCR: Topologically consistent reweighting for XGBoost in
regression tasks. <em>ML</em>, <em>114</em>(4), 1–52. (<a
href="https://doi.org/10.1007/s10994-024-06704-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient boosted tree ensembles (GBTEs) such as XGBoost continue to outperform other machine learning models on tabular data. However, the plethora of adjustable hyperparameters can exacerbate optimisation, especially in regression tasks with no intuitive performance measures such as accuracy and confidence. Automated machine learning frameworks alleviate the hyperparameter search for users, but if the optimisation procedure ends prematurely due to resource constraints, it is questionable whether users receive good models. To tackle this problem, we introduce a cost-efficient method to retrofit previously optimised XGBoost models by retraining them with a new weight distribution over the training instances. We base our approach on topological results, which allows us to infer model-agnostic weights for specific regions of the data distribution where the targets are more susceptible to input perturbations. By linking our theory to the training procedure of XGBoost regressors, we then establish a topologically consistent reweighting scheme, which is independent of the specific model instance. Empirically, we verify that our approach improves prediction performance, outperforms other reweighting methods and is much faster than a hyperparameter search. To enable users to find the optimal weights for their data, we provide guides based on our findings on 20 datasets. Our code is available at: https://github.com/montymaxzuehlke/tcr .},
  archive      = {J_ML},
  author       = {Zühlke, Monty-Maximilian and Kudenko, Daniel},
  doi          = {10.1007/s10994-024-06704-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {TCR: Topologically consistent reweighting for XGBoost in regression tasks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intramodal consistency in triplet-based cross-modal learning
for image retrieval. <em>ML</em>, <em>114</em>(4), 1–29. (<a
href="https://doi.org/10.1007/s10994-024-06710-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval requires building a common latent space that captures and correlates information from different data modalities, usually images and texts. Cross-modal training based on the triplet loss with hard negative mining is a state-of-the-art technique to address this problem. This paper shows that such approach is not always effective in handling intra-modal similarities. Specifically, we found that this method can lead to inconsistent similarity orderings in the latent space, where intra-modal pairs with unknown ground-truth similarity are ranked higher than cross-modal pairs representing the same concept. To address this problem, we propose two novel loss functions that leverage intra-modal similarity constraints available in a training triplet but not used by the original formulation. Additionally, this paper explores the application of this framework to unsupervised image retrieval problems, where cross-modal training can provide the supervisory signals that are otherwise missing in the absence of category labels. Up to our knowledge, we are the first to evaluate cross-modal training for intra-modal retrieval without labels. We present comprehensive experiments on MS-COCO and Flickr30k, demonstrating the advantages and limitations of the proposed methods in cross-modal and intra-modal retrieval tasks in terms of performance and novelty measures. We also conduct a case study on the ROCO dataset to assess the performance of our method on medical images and present an ablation study on one of our approaches to understanding the impact of the different components of the proposed loss function. Our code is publicly available on GitHub https://github.com/MariodotR/FullHN.git .},
  archive      = {J_ML},
  author       = {Mallea, Mario and Ñanculef, Ricardo and Araya, Mauricio},
  doi          = {10.1007/s10994-024-06710-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Intramodal consistency in triplet-based cross-modal learning for image retrieval},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain generalization via content factors isolation: A
two-level latent variable modeling approach. <em>ML</em>,
<em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-024-06717-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of domain generalization is to develop models that exhibit a higher degree of generality, meaning they perform better when evaluated on data coming from previously unseen distributions. Models obtained via traditional methods often cannot distinguish between label-specific and domain-related features in the latent space. To confront this difficulty, we propose formulating a novel data generation process using a latent variable model and postulating a partition of the latent space into content and style parts while allowing for statistical dependency to exist between them. In this model, the distribution of content factors associated with observations belonging to the same class depends on only the label corresponding to that class. In contrast, the distribution of style factors has an additional dependency on the domain variable. We derive constraints that suffice to recover the collection of content factors block-wise and the collection of style factors component-wise while guaranteeing the isolation of content factors. This allows us to produce a stable predictor solely relying on the latent content factors. Building upon these theoretical insights, we propose a practical and efficient algorithm for determining the latent variables under the variational auto-encoder framework. Our simulations with dependent latent variables produce results consistent with our theory, and real-world experiments show that our method outperforms the competitors.},
  archive      = {J_ML},
  author       = {Gao, Erdun and Bondell, Howard and Huang, Shaoli and Gong, Mingming},
  doi          = {10.1007/s10994-024-06717-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Domain generalization via content factors isolation: A two-level latent variable modeling approach},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing exchangeability in the batch mode with e-values and
markov alternatives. <em>ML</em>, <em>114</em>(4), 1–27. (<a
href="https://doi.org/10.1007/s10994-024-06720-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of this paper is testing the assumption of exchangeability, which is the standard assumption in mainstream machine learning. The common approaches are online testing by betting (such as conformal testing) and the older batch testing using p-values (as in classical hypothesis testing). The approach of this paper is intermediate in that we are interested in batch testing by betting; as a result, p-values are replaced by e-values. As a first step in this direction, this paper concentrates on the Markov model as alternative. The null hypothesis of exchangeability is formalized as a Kolmogorov-type compression model, and the Bayes mixture of the Markov model w.r. to the uniform prior is taken as simple alternative hypothesis. Using e-values instead of p-values leads to a computationally efficient testing procedure. Two appendixes discuss connections with the algorithmic theory of randomness; in particular, the test proposed in this paper can be interpreted as a poor man’s version of Kolmogorov’s deficiency of randomness.},
  archive      = {J_ML},
  author       = {Vovk, Vladimir},
  doi          = {10.1007/s10994-024-06720-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Testing exchangeability in the batch mode with e-values and markov alternatives},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel density estimation for multiclass quantification.
<em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-024-06726-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. Quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. The distribution-matching (DM) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. Current DM approaches model the involved populations using histograms of posterior probabilities. In this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. We propose a new representation mechanism based on multivariate densities that we model via kernel density estimation (KDE). The experiments we have carried out show our method, dubbed KDEy, yields superior quantification performance compared to previous DM approaches and other state-of-the-art quantification systems.},
  archive      = {J_ML},
  author       = {Moreo, Alejandro and González, Pablo and del Coz, Juan José},
  doi          = {10.1007/s10994-024-06726-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Kernel density estimation for multiclass quantification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical risk minimization in the interpolating regime with
application to neural network learning. <em>ML</em>, <em>114</em>(4),
1–52. (<a href="https://doi.org/10.1007/s10994-025-06738-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common strategy to train deep neural networks (DNNs) is to use very large architectures and to train them until they (almost) achieve zero training error. Empirically observed good generalization performance on test data, even in the presence of lots of label noise, corroborate such a procedure. On the other hand, in statistical learning theory it is known that over-fitting models may lead to poor generalization properties, occurring in e.g. empirical risk minimization (ERM) over too large hypotheses classes. Inspired by this contradictory behavior, so-called interpolation methods have recently received much attention, leading to consistent and optimally learning methods for, e.g., some local averaging schemes with zero training error. We extend this analysis to ERM-like methods for least squares regression and show that for certain, large hypotheses classes called inflated histograms, some interpolating empirical risk minimizers enjoy very good statistical guarantees while others fail in the worst sense. Moreover, we show that the same phenomenon occurs for DNNs with zero training error and sufficiently large architectures.},
  archive      = {J_ML},
  author       = {Mücke, Nicole and Steinwart, Ingo},
  doi          = {10.1007/s10994-025-06738-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {Empirical risk minimization in the interpolating regime with application to neural network learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression and restoration: Exploring elasticity in
continual test-time adaptation. <em>ML</em>, <em>114</em>(4), 1–32. (<a
href="https://doi.org/10.1007/s10994-025-06739-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation is a task that a pre-trained source model is updated during inference with given test data from target domains with different distributions. However, frequent updates in a long time without resetting the model will bring two main problems, i.e., error accumulation and catastrophic forgetting. Although some recent methods have alleviated the problems by designing new loss functions or update strategies, they are still very fragile to hyperparameters or suffer from storage burden. Besides, most methods treat each target domain equally, neglecting the characteristics of each target domain and the situation of the current model, which will mislead the update direction of the model. To address the above issues, we first leverage the mean cosine similarity per test batch between the features output by the source and updated models to measure the change of target domains. Then we summarize the elasticity of the mean cosine similarity to guide the model to update and restore adaptively. Motivated by this, we propose a frustratingly simple yet efficient method called Elastic-Test-time ENTropy Minimization (E-TENT) to dynamically adjust the mean cosine similarity based on the built relationship between it and the momentum coefficient. Combined with the extra three minimal improvements, E-TENT exhibits significant performance gains and strong robustness on CIFAR10-C, CIFAR100-C and ImageNet-C along with various practical scenarios.},
  archive      = {J_ML},
  author       = {Li, Jingwei and Liu, Chengbao and Bai, Xiwei and Tan, Jie and Chu, Jiaqi and Wang, Yudong},
  doi          = {10.1007/s10994-025-06739-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Compression and restoration: Exploring elasticity in continual test-time adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep errors-in-variables using a diffusion model.
<em>ML</em>, <em>114</em>(4), 1–25. (<a
href="https://doi.org/10.1007/s10994-025-06744-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Errors-in-Variables is the statistical concept used to explicitly model input variable errors caused, for example, by noise. While it has long been known in statistics that not accounting for such errors can produce a substantial bias, the vast majority of deep learning models have thus far neglected Errors-in-Variables approaches. Reasons for this include a significant increase of the numerical burden and the challenge in assigning an appropriate prior in a Bayesian treatment. To date, the attempts made to use Errors-in-Variables for neural networks do not scale to deep networks or are too simplistic to enhance the prediction performance. This work shows for the first time how Bayesian deep Errors-in-Variables models can increase the prediction performance. We present a scalable variational inference scheme for Bayesian Errors-in-Variables and demonstrate a significant increase in prediction performance for the case of image classification. Concretely, we use a diffusion model as input posterior to obtain a distribution over the denoised image data. We also observe that training the diffusion model on an unnoisy surrogate dataset can suffice to achieve an improved prediction performance on noisy data.},
  archive      = {J_ML},
  author       = {Faller, Josua and Martin, Jörg and Elster, Clemens},
  doi          = {10.1007/s10994-025-06744-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Deep errors-in-variables using a diffusion model},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover the balanced geometry in long-tailed contrastive
language-image pretraining. <em>ML</em>, <em>114</em>(4), 1–33. (<a
href="https://doi.org/10.1007/s10994-025-06745-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Contrastive Language-Image Pretraining (CLIP) has become the de facto standard for vision-language pretraining tasks, the exploration on the inherent long-tailed pretraining data distribution remains limited. From a neural collapse perspective, we show in principle that the vanilla CLIP training can be vulnerable to the long-tailed distributions, which might distort the representations with reduced inter-class separation and poor discriminative ability. To combat this issue, we propose an improved method, termed as Geometry-Balanced CLIP (GeoCLIP), which automatically constructs pseudo clusters and aligns them with a predefined equiangular geometric structure, thereby enjoying the theoretical merits of better maintaining the uniformity at the semantic level. Furthermore, we enhance GeoCLIP’s generality for real-world complex distributions by incorporating harmonized clusters that integrate both empirically observed data structures and theoretically optimal geometry. Extensive experiments across various benchmarks demonstrate the consistent superiority of GeoCLIP in achieving robust and transferable representation under long-tailed distributions. The source code will be publicly available.},
  archive      = {J_ML},
  author       = {Zhou, Zhihan and Ye, Yushi and Hong, Feng and Zhao, Peisen and Yao, Jiangchao and Zhang, Ya and Tian, Qi and Wang, Yanfeng},
  doi          = {10.1007/s10994-025-06745-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Uncover the balanced geometry in long-tailed contrastive language-image pretraining},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning with pre-trained conditional generative
models. <em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-025-06748-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) Source and target task label spaces overlap, (ii) Source datasets are available, and (iii) Target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a target architecture with an artificial dataset synthesized by using conditional source generative models. P-SSL applies SSL algorithms to labeled target data and unlabeled pseudo samples, which are generated by cascading the source classifier and generative models to condition them with target samples. Our experimental results indicate that our method can outperform the baselines of scratch training and knowledge distillation.},
  archive      = {J_ML},
  author       = {Yamaguchi, Shin’ya and Kanai, Sekitoshi and Kumagai, Atsutoshi and Chijiwa, Daiki and Kashima, Hisashi},
  doi          = {10.1007/s10994-025-06748-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Transfer learning with pre-trained conditional generative models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model for intelligible interaction between agents that
predict and explain. <em>ML</em>, <em>114</em>(4), 1–40. (<a
href="https://doi.org/10.1007/s10994-025-06750-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has emerged as a powerful form of data modelling with widespread applicability beyond its roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. In this paper we view interaction between humans and ML systems within the broader context of communication between agents capable of prediction and explanation. We formalise the interaction model by taking agents to be automata with some special characteristics and define a protocol for communication between such agents. We define One- and Two-Way Intelligibility as properties that emerge at run-time by execution of the protocol. The formalisation allows us to identify conditions under which run-time sequences are bounded, and identify conditions under which the protocol can correctly implement an axiomatic specification of intelligible interaction between a human and an ML system. We also demonstrate using the formal model to: (a) identify instances of One- and Two-Way Intelligibility in literature reports on humans interacting with ML systems providing logic-based explanations, as is done in Inductive Logic Programming (ILP); and (b) map interactions between humans and machines in an elaborate natural-language based dialogue-model to One- or Two-Way Intelligible interactions in the formal model.},
  archive      = {J_ML},
  author       = {Baskar, A. and Srinivasan, Ashwin and Bain, Michael and Coiera, Enrico},
  doi          = {10.1007/s10994-025-06750-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {A model for intelligible interaction between agents that predict and explain},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A contrastive neural disentanglement approach for query
performance prediction. <em>ML</em>, <em>114</em>(4), 1–21. (<a
href="https://doi.org/10.1007/s10994-025-06752-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach, referred to as contrastive disentangled representation for query performance prediction (CoDiR-QPP), to estimate search query performance by disentangling query content semantics from query difficulty. Our proposed approach leverages neural disentanglement to isolate the information need expressed in search queries from the complexities that affect retrieval performance. Motivated by empirical observations that varying query formulations for the same information need can significantly impact retrieval outcomes, we hypothesize that separating content semantics from query difficulty can enhance query performance prediction. Utilizing contrastive learning, CoDiR-QPP distinguishes between well-performing and poorly performing query variants, facilitating the estimation of a given query’s performance. Our extensive experiments on four standard benchmark datasets demonstrate that CoDiR-QPP outperforms state-of-the-art baselines in predicting query performance, offering improved semantic similarity computation and higher correlation metrics such as Kendall $$\tau$$ , Spearman $$\rho$$ , and scaled Mean Absolute Ranking Error (sMARE).},
  archive      = {J_ML},
  author       = {Salamat, Sara and Arabzadeh, Negar and Seyedsalehi, Shirin and Bigdeli, Amin and Zihayat, Morteza and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-025-06752-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A contrastive neural disentanglement approach for query performance prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Likelihood-ratio-based confidence intervals for neural
networks. <em>ML</em>, <em>114</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10994-024-06639-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.},
  archive      = {J_ML},
  author       = {Sluijterman, Laurens and Cator, Eric and Heskes, Tom},
  doi          = {10.1007/s10994-024-06639-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Likelihood-ratio-based confidence intervals for neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pairwise learning to rank by neural networks revisited:
Reconstruction, theoretical analysis and practical performance.
<em>ML</em>, <em>114</em>(4), 1–28. (<a
href="https://doi.org/10.1007/s10994-024-06644-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reevaluate the pairwise learning to rank approach based on neural nets, called RankNet, and present a theoretical analysis of its architecture. We show mathematically that the model can, under certain conditions, learn reflexive, antisymmetric, and transitive relations, enabling simplified training and improved performance. Experimental results on the LETOR MSLR-WEB10K, MQ2007 and MQ2008 datasets show that the model outperforms numerous state-of-the-art methods (including a listwise approach), while being inherently simpler in structure and using a pairwise approach only.},
  archive      = {J_ML},
  author       = {Köppel, Marius and Segner, Alexander and Wagener, Martin and Pensel, Lukas and Karwath, Andreas and Kramer, Stefan},
  doi          = {10.1007/s10994-024-06644-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Pairwise learning to rank by neural networks revisited: Reconstruction, theoretical analysis and practical performance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on self-supervised methods for visual
representation learning. <em>ML</em>, <em>114</em>(4), 1–56. (<a
href="https://doi.org/10.1007/s10994-024-06708-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.},
  archive      = {J_ML},
  author       = {Uelwer, Tobias and Robine, Jan and Wagner, Stefan Sylvius and Höftmann, Marc and Upschulte, Eric and Konietzny, Sebastian and Behrendt, Maike and Harmeling, Stefan},
  doi          = {10.1007/s10994-024-06708-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-56},
  shortjournal = {Mach. Learn.},
  title        = {A survey on self-supervised methods for visual representation learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring individual direct causal effects under
heterogeneous peer influence. <em>ML</em>, <em>114</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s10994-024-06729-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference is central to understanding the effectiveness of policies and designing personalized interventions. Causal inference involves estimating the causal effects of treatments on outcomes of interest after modeling appropriate assumptions. Most causal inference approaches assume that a unit’s outcome is independent of the treatments or outcomes of other units. However, this assumption is unrealistic when inferring causal effects in networks where a unit’s outcome can be influenced by the treatments and outcomes of its neighboring nodes, a phenomenon known as interference. Causal inference in networks should explicitly account for interference. In interference settings, the direct causal effect measures the impact of the unit’s own treatment while controlling for the treatments of peers. Existing solutions to estimating direct causal effects under interference consider either homogeneous influence from peers or specific heterogeneous influence mechanisms (e.g., based on local neighborhood structure). In this work, we define heterogeneous peer influence (HPI) as the general interference that occurs when a unit’s outcome may be influenced differently by different peers based on their attributes and relationships, or when each network node may have a different susceptibility to peer influence. This paper presents IDE-Net, a framework for estimating individual, i.e., unit-level, direct causal effects in the presence of HPI where the mechanism of influence is not known a priori. We first propose a structural causal model for networks that can capture different possible assumptions about network structure, interference conditions, and causal dependence and that enables reasoning about causal effect identifiability and discovery of potential heterogeneous contexts. We then propose a novel graph neural network-based estimator to estimate individual direct causal effects. We show empirically that state-of-the-art methods for individual direct effect estimation produce biased results in the presence of HPI, and that our proposed estimator is robust.},
  archive      = {J_ML},
  author       = {Adhikari, Shishir and Zheleva, Elena},
  doi          = {10.1007/s10994-024-06729-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Inferring individual direct causal effects under heterogeneous peer influence},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end explainability framework for spatio-temporal
predictive modeling. <em>ML</em>, <em>114</em>(4), 1–47. (<a
href="https://doi.org/10.1007/s10994-024-06733-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising adoption of AI models in real-world applications characterized by sensor data creates an urgent need for inference explanation mechanisms to support domain experts in making informed decisions. Explainable AI (XAI) opens up a new opportunity to extend black-box deep learning models with such inference explanation capabilities. However, existing XAI approaches for tabular, image, and graph data are ineffective in contexts with spatio-temporal data. In this paper, we fill this gap by proposing a XAI method specifically tailored for spatio-temporal data in sensor networks, where observations are collected at regular time intervals and at different locations. Our model-agnostic masking meta-optimization method for deep learning models uncovers global salient factors influencing model predictions, and generates explanations taking into account multiple analytical views, such as features, timesteps, and node locations. Our qualitative and quantitative experiments with real-world forecasting datasets show that our approach effectively extracts explanations of model predictions, and is competitive with state-of-the-art approaches.},
  archive      = {J_ML},
  author       = {Altieri, Massimiliano and Ceci, Michelangelo and Corizzo, Roberto},
  doi          = {10.1007/s10994-024-06733-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-47},
  shortjournal = {Mach. Learn.},
  title        = {An end-to-end explainability framework for spatio-temporal predictive modeling},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient descent fails to learn high-frequency functions and
modular arithmetic. <em>ML</em>, <em>114</em>(4), 1–30. (<a
href="https://doi.org/10.1007/s10994-025-06747-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function. A set of functions of the form $$x\rightarrow ax \bmod p$$ , where a is taken from $${{\mathbb {Z}}}_p$$ , has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of p-periodic functions on $${{\mathbb {Z}}}$$ and is tightly connected with a class of high-frequency periodic functions on the real line. We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base p is large. This in turn prevents such a learning algorithm from being successful.},
  archive      = {J_ML},
  author       = {Takhanov, Rustem and Tezekbayev, Maxat and Pak, Artur and Bolatov, Arman and Assylbekov, Zhenisbek},
  doi          = {10.1007/s10994-025-06747-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Gradient descent fails to learn high-frequency functions and modular arithmetic},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized median of means principle for bayesian
inference. <em>ML</em>, <em>114</em>(4), 1–38. (<a
href="https://doi.org/10.1007/s10994-025-06754-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of robustness is experiencing a resurgence of interest in the statistical and machine learning communities. In particular, robust algorithms making use of the so-called median of means estimator were shown to satisfy strong performance guarantees for many problems, including estimation of the mean, covariance structure as well as linear regression. In this work, we propose an extension of the median of means principle to the Bayesian framework, leading to the notion of the robust posterior distribution. In particular, we (a) quantify robustness of this posterior to outliers, (b) show that it satisfies a version of the Bernstein-von Mises theorem that connects Bayesian credible sets to the traditional confidence intervals, and (c) demonstrate that our approach performs well in applications.},
  archive      = {J_ML},
  author       = {Minsker, Stanislav and Yao, Shunan},
  doi          = {10.1007/s10994-025-06754-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Generalized median of means principle for bayesian inference},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mp---20">MP - 20</h2>
<ul>
<li><details>
<summary>
(2025). On the directional asymptotic approach in optimization
theory. <em>MP</em>, <em>209</em>(1), 859–937. (<a
href="https://doi.org/10.1007/s10107-024-02089-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a starting point of our research, we show that, for a fixed order $$\gamma \ge 1$$ , each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order 1), satisfies stationarity conditions in terms of a coderivative construction of order $$\gamma $$ , or is asymptotically stationary with respect to a critical direction as well as order $$\gamma $$ in a certain sense. By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders 1 and $$\gamma $$ . These abstract findings are carved out for the broad class of geometric constraints and $$\gamma :=2$$ , and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization. As a byproduct of the particular setting $$\gamma :=1$$ , our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers. We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization. Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings. It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity. Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity. For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data.},
  archive      = {J_MP},
  author       = {Benko, Matúš and Mehlitz, Patrick},
  doi          = {10.1007/s10107-024-02089-w},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {859-937},
  shortjournal = {Math. Program.},
  title        = {On the directional asymptotic approach in optimization theory},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asynchronous proximal bundle method. <em>MP</em>,
<em>209</em>(1), 825–857. (<a
href="https://doi.org/10.1007/s10107-024-02088-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a fully asynchronous proximal bundle method for solving non-smooth, convex optimization problems. The algorithm can be used as a drop-in replacement for classic bundle methods, i.e., the function must be given by a first-order oracle for computing function values and subgradients. The algorithm allows for an arbitrary number of master problem processes computing new candidate points and oracle processes evaluating functions at those candidate points. These processes share information by communication with a single supervisor process that resembles the main loop of a classic bundle method. All processes run in parallel and no explicit synchronization step is required. Instead, the asynchronous and possibly outdated results of the oracle computations can be seen as an inexact function oracle. Hence, we show the convergence of our method under weak assumptions very similar to inexact and incremental bundle methods. In particular, we show how the algorithm learns important structural properties of the functions to control the inaccuracy induced by the asynchronicity automatically such that overall convergence can be guaranteed.},
  archive      = {J_MP},
  author       = {Fischer, Frank},
  doi          = {10.1007/s10107-024-02088-x},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {825-857},
  shortjournal = {Math. Program.},
  title        = {An asynchronous proximal bundle method},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stackelberg risk preference design. <em>MP</em>,
<em>209</em>(1), 785–823. (<a
href="https://doi.org/10.1007/s10107-024-02083-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk measures are commonly used to capture the risk preferences of decision-makers (DMs). The decisions of DMs can be nudged or manipulated when their risk preferences are influenced by factors such as the availability of information about the uncertainties. This work proposes a Stackelberg risk preference design (STRIPE) problem to capture a designer’s incentive to influence DMs’ risk preferences. STRIPE consists of two levels. In the lower level, individual DMs in a population, known as the followers, respond to uncertainties according to their risk preference types. In the upper level, the leader influences the distribution of the types to induce targeted decisions and steers the follower’s preferences to it. Our analysis centers around the solution concept of approximate Stackelberg equilibrium that yields suboptimal behaviors of the players. We show the existence of the approximate Stackelberg equilibrium. The primitive risk perception gap, defined as the Wasserstein distance between the original and the target type distributions, is important in estimating the optimal design cost. We connect the leader’s optimality compromise on the cost with her ambiguity tolerance on the follower’s approximate solutions leveraging Lipschitzian properties of the lower level solution mapping. To obtain the Stackelberg equilibrium, we reformulate STRIPE into a single-level optimization problem using the spectral representations of law-invariant coherent risk measures. We create a data-driven approach for computation and study its performance guarantees. We apply STRIPE to contract design problems under approximate incentive compatibility. Moreover, we connect STRIPE with meta-learning problems and derive adaptation performance estimates of the meta-parameters.},
  archive      = {J_MP},
  author       = {Liu, Shutian and Zhu, Quanyan},
  doi          = {10.1007/s10107-024-02083-2},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {785-823},
  shortjournal = {Math. Program.},
  title        = {Stackelberg risk preference design},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding global minima via kernel approximations.
<em>MP</em>, <em>209</em>(1), 703–784. (<a
href="https://doi.org/10.1007/s10107-024-02081-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the global minimization of smooth functions based solely on function evaluations. Algorithms that achieve the optimal number of function evaluations for a given precision level typically rely on explicitly constructing an approximation of the function which is then minimized with algorithms that have exponential running-time complexity. In this paper, we consider an approach that jointly models the function to approximate and finds a global minimum. This is done by using infinite sums of square smooth functions and has strong links with polynomial sum-of-squares hierarchies. Leveraging recent representation properties of reproducing kernel Hilbert spaces, the infinite-dimensional optimization problem can be solved by subsampling in time polynomial in the number of function evaluations, and with theoretical guarantees on the obtained minimum. Given n samples, the computational cost is $$O(n^{3.5})$$ in time, $$O(n^2)$$ in space, and we achieve a convergence rate to the global optimum that is $$O(n^{-m/d + 1/2 + 3/d})$$ where m is the degree of differentiability of the function and d the number of dimensions. The rate is nearly optimal in the case of Sobolev functions and more generally makes the proposed method particularly suitable for functions with many derivatives. Indeed, when m is in the order of d, the convergence rate to the global optimum does not suffer from the curse of dimensionality, which affects only the worst-case constants (that we track explicitly through the paper).},
  archive      = {J_MP},
  author       = {Rudi, Alessandro and Marteau-Ferey, Ulysse and Bach, Francis},
  doi          = {10.1007/s10107-024-02081-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {703-784},
  shortjournal = {Math. Program.},
  title        = {Finding global minima via kernel approximations},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A normal fan projection algorithm for low-rank optimization.
<em>MP</em>, <em>209</em>(1), 681–702. (<a
href="https://doi.org/10.1007/s10107-024-02079-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a method for minimizing a low-rank quasiconcave objective function over a polytope by first projecting the polytope’s normal fan, then using the projected fan to obtain candidate solutions. When the polytope’s maximal number of nonparallel edges is bounded by a polynomial in its dimension, our method solves the problem in time that is polynomial in the number of variables and exponential in the rank of the objective function. We discuss several problems from previous literature that can be solved efficiently using this method. In all cases, our proposed algorithm matches or improves on the running time of existing problem-specific algorithms, while providing the first polynomial-time algorithm we know of for finding a spanning tree on a graph with multiple edge weight types, such that the product of the different weight types is minimized.},
  archive      = {J_MP},
  author       = {Scott, James R. and Geunes, Joseph},
  doi          = {10.1007/s10107-024-02079-y},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {681-702},
  shortjournal = {Math. Program.},
  title        = {A normal fan projection algorithm for low-rank optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample complexity analysis for adaptive optimization
algorithms with stochastic oracles. <em>MP</em>, <em>209</em>(1),
651–679. (<a href="https://doi.org/10.1007/s10107-024-02078-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several classical adaptive optimization algorithms, such as line search and trust-region methods, have been recently extended to stochastic settings where function values, gradients, and Hessians in some cases, are estimated via stochastic oracles. Unlike the majority of stochastic methods, these methods do not use a pre-specified sequence of step size parameters, but adapt the step size parameter according to the estimated progress of the algorithm and use it to dictate the accuracy required from the stochastic oracles. The requirements on the stochastic oracles are, thus, also adaptive and the oracle costs can vary from iteration to iteration. The step size parameters in these methods can increase and decrease based on the perceived progress, but unlike the deterministic case they are not bounded away from zero due to possible oracle failures, and bounds on the step size parameter have not been previously derived. This creates obstacles in the total complexity analysis of such methods, because the oracle costs are typically decreasing in the step size parameter, and could be arbitrarily large as the step size parameter goes to 0. Thus, until now only the total iteration complexity of these methods has been analyzed. In this paper, we derive a lower bound on the step size parameter that holds with high probability for a large class of adaptive stochastic methods. We then use this lower bound to derive a framework for analyzing the expected and high probability total oracle complexity of any method in this class. Finally, we apply this framework to analyze the total sample complexity of two particular algorithms, STORM (Blanchet et al. in INFORMS J Optim 1(2):92–119, 2019) and SASS (Jin et al. in High probability complexity bounds for adaptive step search based on stochastic oracles, 2021. https://doi.org/10.48550/ARXIV.2106.06454 ), in the expected risk minimization problem.},
  archive      = {J_MP},
  author       = {Jin, Billy and Scheinberg, Katya and Xie, Miaolan},
  doi          = {10.1007/s10107-024-02078-z},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {651-679},
  shortjournal = {Math. Program.},
  title        = {Sample complexity analysis for adaptive optimization algorithms with stochastic oracles},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perseus: A simple and optimal high-order method for
variational inequalities. <em>MP</em>, <em>209</em>(1), 609–650. (<a
href="https://doi.org/10.1007/s10107-024-02075-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper settles an open and challenging question pertaining to the design of simple and optimal high-order methods for solving smooth and monotone variational inequalities (VIs). A VI involves finding $$x^\star \in {\mathcal {X}}$$ such that $$\langle F(x), x - x^\star \rangle \ge 0$$ for all $$x \in {\mathcal {X}}$$ . We consider the setting in which $$F: {\mathbb {R}}^d \rightarrow {\mathbb {R}}^d$$ is smooth with up to $$(p-1)^{\text {th}}$$ -order derivatives. For $$p = 2$$ , the cubic regularization of Newton’s method has been extended to VIs with a global rate of $$O(\epsilon ^{-1})$$ (Nesterov in Cubic regularization of Newton’s method for convex problems with constraints, Tech. rep., Université catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2006). An improved rate of $$O(\epsilon ^{-2/3}\log \log (1/\epsilon ))$$ can be obtained via an alternative second-order method, but this method requires a nontrivial line-search procedure as an inner loop. Similarly, the existing high-order methods based on line-search procedures have been shown to achieve a rate of $$O(\epsilon ^{-2/(p+1)}\log \log (1/\epsilon ))$$ (Bullins and Lai in SIAM J Optim 32(3):2208–2229, 2022; Jiang and Mokhtari in Generalized optimistic methods for convex–concave saddle point problems, 2022; Lin and Jordan in Math Oper Res 48(4):2353–2382, 2023). As emphasized by Nesterov (Lectures on convex optimization, vol 137, Springer, Berlin, 2018), however, such procedures do not necessarily imply the practical applicability in large-scale applications, and it is desirable to complement these results with a simple high-order VI method that retains the optimality of the more complex methods. We propose a $$p^{\text {th}}$$ -order method that does not require any line search procedure and provably converges to a weak solution at a rate of $$O(\epsilon ^{-2/(p+1)})$$ . We prove that our $$p^{\text {th}}$$ -order method is optimal in the monotone setting by establishing a lower bound of $$\Omega (\epsilon ^{-2/(p+1)})$$ under a generalized linear span assumption. A restarted version of our $$p^{\text {th}}$$ -order method attains a linear rate for smooth and $$p^{\text {th}}$$ -order uniformly monotone VIs and another restarted version of our $$p^{\text {th}}$$ -order method attains a local superlinear rate for smooth and strongly monotone VIs. Further, the similar $$p^{\text {th}}$$ -order method achieves a global rate of $$O(\epsilon ^{-2/p})$$ for solving smooth and nonmonotone VIs satisfying the Minty condition. Two restarted versions attain a global linear rate under additional $$p^{\text {th}}$$ -order uniform Minty condition and a local superlinear rate under additional strong Minty condition.},
  archive      = {J_MP},
  author       = {Lin, Tianyi and Jordan, Michael I.},
  doi          = {10.1007/s10107-024-02075-2},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {609-650},
  shortjournal = {Math. Program.},
  title        = {Perseus: A simple and optimal high-order method for variational inequalities},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-convex scenario optimization. <em>MP</em>,
<em>209</em>(1), 557–608. (<a
href="https://doi.org/10.1007/s10107-024-02074-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scenario optimization is an approach to data-driven decision-making that has been introduced some fifteen years ago and has ever since then grown fast. Its most remarkable feature is that it blends the heuristic nature of data-driven methods with a rigorous theory that allows one to gain factual, reliable, insight in the solution. The usability of the scenario theory, however, has been restrained thus far by the obstacle that most results are standing on the assumption of convexity. With this paper, we aim to free the theory from this limitation. Specifically, we focus on the body of results that are known under the name of “wait-and-judge” and show that its fundamental achievements maintain their validity in a non-convex setup. While optimization is a major center of attention, this paper travels beyond it and into data-driven decision making. Adopting such a broad framework opens the door to building a new theory of truly vast applicability.},
  archive      = {J_MP},
  author       = {Garatti, Simone and Campi, Marco C.},
  doi          = {10.1007/s10107-024-02074-3},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {557-608},
  shortjournal = {Math. Program.},
  title        = {Non-convex scenario optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated first-order methods for a class of semidefinite
programs. <em>MP</em>, <em>209</em>(1), 503–556. (<a
href="https://doi.org/10.1007/s10107-024-02073-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new storage-optimal first-order method, CertSDP, for solving a special class of semidefinite programs (SDPs) to high accuracy. The class of SDPs that we consider, the exact QMP-like SDPs, is characterized by low-rank solutions, a priori knowledge of the restriction of the SDP solution to a small subspace, and standard regularity assumptions such as strict complementarity. Crucially, we show how to use a certificate of strict complementarity to construct a low-dimensional strongly convex minimax problem whose optimizer coincides with a factorization of the SDP optimizer. From an algorithmic standpoint, we show how to construct the necessary certificate and how to solve the minimax problem efficiently. Our algorithms for strongly convex minimax problems with inexact prox maps may be of independent interest. We accompany our theoretical results with preliminary numerical experiments suggesting that CertSDP significantly outperforms current state-of-the-art methods on large sparse exact QMP-like SDPs.},
  archive      = {J_MP},
  author       = {Wang, Alex L. and Kılınç-Karzan, Fatma},
  doi          = {10.1007/s10107-024-02073-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {503-556},
  shortjournal = {Math. Program.},
  title        = {Accelerated first-order methods for a class of semidefinite programs},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sum-of-squares relaxations for polynomial min–max problems
over simple sets. <em>MP</em>, <em>209</em>(1), 475–501. (<a
href="https://doi.org/10.1007/s10107-024-02072-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider min–max optimization problems for polynomial functions, where a multivariate polynomial is maximized with respect to a subset of variables, and the resulting maximal value is minimized with respect to the remaining variables. When the variables belong to simple sets (e.g., a hypercube, the Euclidean hypersphere, or a ball), we derive a sum-of-squares formulation based on a primal-dual approach. In the simplest setting, we provide a convergence proof when the degree of the relaxation tends to infinity and observe empirically that it can be finitely convergent in several situations. Moreover, our formulation leads to an interesting link with feasibility certificates for polynomial inequalities based on Putinar’s Positivstellensatz.},
  archive      = {J_MP},
  author       = {Bach, Francis},
  doi          = {10.1007/s10107-024-02072-5},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {475-501},
  shortjournal = {Math. Program.},
  title        = {Sum-of-squares relaxations for polynomial min–max problems over simple sets},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence rates for sums-of-squares hierarchies with
correlative sparsity. <em>MP</em>, <em>209</em>(1), 435–473. (<a
href="https://doi.org/10.1007/s10107-024-02071-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work derives upper bounds on the convergence rate of the moment-sum-of-squares hierarchy with correlative sparsity for global minimization of polynomials on compact basic semialgebraic sets. The main conclusion is that both sparse hierarchies based on the Schmüdgen and Putinar Positivstellensätze enjoy a polynomial rate of convergence that depends on the size of the largest clique in the sparsity graph but not on the ambient dimension. Interestingly, the sparse bounds outperform the best currently available bounds for the dense hierarchy when the maximum clique size is sufficiently small compared to the ambient dimension and the performance is measured by the running time of an interior point method required to obtain a bound on the global minimum of a given accuracy.},
  archive      = {J_MP},
  author       = {Korda, Milan and Magron, Victor and Ríos-Zertuche, Rodolfo},
  doi          = {10.1007/s10107-024-02071-6},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {435-473},
  shortjournal = {Math. Program.},
  title        = {Convergence rates for sums-of-squares hierarchies with correlative sparsity},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyhedral properties of RLT relaxations of nonconvex
quadratic programs and their implications on exact relaxations.
<em>MP</em>, <em>209</em>(1), 397–433. (<a
href="https://doi.org/10.1007/s10107-024-02070-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study linear programming relaxations of nonconvex quadratic programs given by the reformulation–linearization technique (RLT), referred to as RLT relaxations. We investigate the relations between the polyhedral properties of the feasible regions of a quadratic program and its RLT relaxation. We establish various connections between recession directions, boundedness, and vertices of the two feasible regions. Using these properties, we present a complete description of the set of instances that admit an exact RLT relaxation. We then give a thorough discussion of how our results can be converted into simple algorithmic procedures to construct instances of quadratic programs with exact, inexact, or unbounded RLT relaxations.},
  archive      = {J_MP},
  author       = {Qiu, Yuzhou and Yıldırım, E. Alper},
  doi          = {10.1007/s10107-024-02070-7},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {397-433},
  shortjournal = {Math. Program.},
  title        = {Polyhedral properties of RLT relaxations of nonconvex quadratic programs and their implications on exact relaxations},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The chvátal–gomory procedure for integer SDPs with
applications in combinatorial optimization. <em>MP</em>,
<em>209</em>(1), 323–395. (<a
href="https://doi.org/10.1007/s10107-024-02069-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the well-known Chvátal–Gomory (CG) procedure for the class of integer semidefinite programs (ISDPs). We prove several results regarding the hierarchy of relaxations obtained by iterating this procedure. We also study different formulations of the elementary closure of spectrahedra. A polyhedral description of the elementary closure for a specific type of spectrahedra is derived by exploiting total dual integrality for SDPs. Moreover, we show how to exploit (strengthened) CG cuts in a branch-and-cut framework for ISDPs. Different from existing algorithms in the literature, the separation routine in our approach exploits both the semidefinite and the integrality constraints. We provide separation routines for several common classes of binary SDPs resulting from combinatorial optimization problems. In the second part of the paper we present a comprehensive application of our approach to the quadratic traveling salesman problem (QTSP). Based on the algebraic connectivity of the directed Hamiltonian cycle, two ISDPs that model the QTSP are introduced. We show that the CG cuts resulting from these formulations contain several well-known families of cutting planes. Numerical results illustrate the practical strength of the CG cuts in our branch-and-cut algorithm, which outperforms alternative ISDP solvers and is able to solve large QTSP instances to optimality.},
  archive      = {J_MP},
  author       = {de Meijer, Frank and Sotirov, Renata},
  doi          = {10.1007/s10107-024-02069-0},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {323-395},
  shortjournal = {Math. Program.},
  title        = {The Chvátal–Gomory procedure for integer SDPs with applications in combinatorial optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized minimum 0-extension problem and discrete
convexity. <em>MP</em>, <em>209</em>(1), 279–322. (<a
href="https://doi.org/10.1007/s10107-024-02064-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a fixed finite metric space $$(V,\mu )$$ , the minimum 0-extension problem, denoted as $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ , is equivalent to the following optimization problem: minimize function of the form $$\min \nolimits _{x\in V^n} \sum _i f_i(x_i) + \sum _{ij} c_{ij}\hspace{0.5pt}\mu (x_i,x_j)$$ where $$f_i:V\rightarrow \mathbb {R}$$ are functions given by $$f_i(x_i)=\sum _{v\in V} c_{vi}\hspace{0.5pt}\mu (x_i,v)$$ and $$c_{ij},c_{vi}$$ are given nonnegative costs. The computational complexity of $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ has been recently established by Karzanov and by Hirai: if metric $$\mu $$ is orientable modular then $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ can be solved in polynomial time, otherwise $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ is NP-hard. To prove the tractability part, Hirai developed a theory of discrete convex functions on orientable modular graphs generalizing several known classes of functions in discrete convex analysis, such as $$L^\natural $$ -convex functions. We consider a more general version of the problem in which unary functions $$f_i(x_i)$$ can additionally have terms of the form $$c_{uv;i}\hspace{0.5pt}\mu (x_i,\{u,v\})$$ for $$\{u,\!\hspace{0.5pt}\hspace{0.5pt}v\}\in F$$ , where set $$F\subseteq \left( {\begin{array}{c}V\\ 2\end{array}}\right) $$ is fixed. We extend the complexity classification above by providing an explicit condition on $$(\mu ,F)$$ for the problem to be tractable. In order to prove the tractability part, we generalize Hirai’s theory and define a larger class of discrete convex functions. It covers, in particular, another well-known class of functions, namely submodular functions on an integer lattice. Finally, we improve the complexity of Hirai’s algorithm for solving $$\mathtt{0\hbox {-}Ext}[{\mu }]$$ on orientable modular graphs.},
  archive      = {J_MP},
  author       = {Dvorak, Martin and Kolmogorov, Vladimir},
  doi          = {10.1007/s10107-024-02064-5},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {279-322},
  shortjournal = {Math. Program.},
  title        = {Generalized minimum 0-extension problem and discrete convexity},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized nash equilibrium problems with mixed-integer
variables. <em>MP</em>, <em>209</em>(1), 231–277. (<a
href="https://doi.org/10.1007/s10107-024-02063-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider generalized Nash equilibrium problems (GNEPs) with non-convex strategy spaces and non-convex cost functions. This general class of games includes the important case of games with mixed-integer variables for which only a few results are known in the literature. We present a new approach to characterize equilibria via a convexification technique using the Nikaido–Isoda function. To any given instance of the GNEP, we construct a set of convexified instances and show that a feasible strategy profile is an equilibrium for the original instance if and only if it is an equilibrium for any convexified instance and the convexified cost functions coincide with the initial ones. We develop this convexification approach along three dimensions: We first show that for quasi-linear models, where a convexified instance exists in which for fixed strategies of the opponent players, the cost function of every player is linear and the respective strategy space is polyhedral, the convexification reduces the GNEP to a standard (non-linear) optimization problem. Secondly, we derive two complete characterizations of those GNEPs for which the convexification leads to a jointly constrained or a jointly convex GNEP, respectively. These characterizations require new concepts related to the interplay of the convex hull operator applied to restricted subsets of feasible strategies and may be interesting on their own. Note that this characterization is also computationally relevant as jointly convex GNEPs have been extensively studied in the literature. Finally, we demonstrate the applicability of our results by presenting a numerical study regarding the computation of equilibria for three classes of GNEPs related to integral network flows and discrete market equilibria.},
  archive      = {J_MP},
  author       = {Harks, Tobias and Schwarz, Julian},
  doi          = {10.1007/s10107-024-02063-6},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {231-277},
  shortjournal = {Math. Program.},
  title        = {Generalized nash equilibrium problems with mixed-integer variables},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hessian barrier algorithms for non-convex conic
optimization. <em>MP</em>, <em>209</em>(1), 171–229. (<a
href="https://doi.org/10.1007/s10107-024-02062-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key problem in mathematical imaging, signal processing and computational statistics is the minimization of non-convex objective functions that may be non-differentiable at the relative boundary of the feasible set. This paper proposes a new family of first- and second-order interior-point methods for non-convex optimization problems with linear and conic constraints, combining logarithmically homogeneous barriers with quadratic and cubic regularization respectively. Our approach is based on a potential-reduction mechanism and, under the Lipschitz continuity of the corresponding derivative with respect to the local barrier-induced norm, attains a suitably defined class of approximate first- or second-order KKT points with worst-case iteration complexity $$O(\varepsilon ^{-2})$$ (first-order) and $$O(\varepsilon ^{-3/2})$$ (second-order), respectively. Based on these findings, we develop new path-following schemes attaining the same complexity, modulo adjusting constants. These complexity bounds are known to be optimal in the unconstrained case, and our work shows that they are upper bounds in the case with complicated constraints as well. To the best of our knowledge, this work is the first which achieves these worst-case complexity bounds under such weak conditions for general conic constrained non-convex optimization problems.},
  archive      = {J_MP},
  author       = {Dvurechensky, Pavel and Staudigl, Mathias},
  doi          = {10.1007/s10107-024-02062-7},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {171-229},
  shortjournal = {Math. Program.},
  title        = {Hessian barrier algorithms for non-convex conic optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated tight lyapunov analysis for first-order methods.
<em>MP</em>, <em>209</em>(1), 133–170. (<a
href="https://doi.org/10.1007/s10107-024-02061-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a methodology for establishing the existence of quadratic Lyapunov inequalities for a wide range of first-order methods used to solve convex optimization problems. In particular, we consider (i) classes of optimization problems of finite-sum form with (possibly strongly) convex and possibly smooth functional components, (ii) first-order methods that can be written as a linear system on state-space form in feedback interconnection with the subdifferentials of the functional components of the objective function, and (iii) quadratic Lyapunov inequalities that can be used to draw convergence conclusions. We present a necessary and sufficient condition for the existence of a quadratic Lyapunov inequality within a predefined class of Lyapunov inequalities, which amounts to solving a small-sized semidefinite program. We showcase our methodology on several first-order methods that fit the framework. Most notably, our methodology allows us to significantly extend the region of parameter choices that allow for duality gap convergence in the Chambolle–Pock method when the linear operator is the identity mapping.},
  archive      = {J_MP},
  author       = {Upadhyaya, Manu and Banert, Sebastian and Taylor, Adrien B. and Giselsson, Pontus},
  doi          = {10.1007/s10107-024-02061-8},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {133-170},
  shortjournal = {Math. Program.},
  title        = {Automated tight lyapunov analysis for first-order methods},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex hulls of monomial curves, and a sparse
positivstellensatz. <em>MP</em>, <em>209</em>(1), 113–131. (<a
href="https://doi.org/10.1007/s10107-024-02060-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the closed convex hull K of a monomial curve given parametrically as $$(t^{m_1},\ldots ,t^{m_n})$$ , with the parameter t varying in an interval I. We show, using constructive arguments, that K admits a lifted semidefinite description by $$\mathcal {O}(d)$$ linear matrix inequalities (LMIs), each of size $$\left\lfloor \frac{n}{2} \right\rfloor +1$$ , where $$d= \max \{m_1,\ldots ,m_n\}$$ is the degree of the curve. On the dual side, we show that if a univariate polynomial p(t) of degree d with at most $$2k+1$$ monomials is non-negative on $${\mathbb {R}}_+$$ , then p admits a representation $$p = t^0 \sigma _0 + \cdots + t^{d-k} \sigma _{d-k}$$ , where the polynomials $$\sigma _0,\ldots ,\sigma _{d-k}$$ are sums of squares and $$\deg (\sigma _i) \le 2k$$ . The latter is a univariate positivstellensatz for sparse polynomials, with non-negativity of p being certified by sos polynomials whose degree only depends on the sparsity of p. Our results fit into the general attempt of formulating polynomial optimization problems as semidefinite problems with LMIs of small size. Such small-size descriptions are much more tractable from a computational viewpoint.},
  archive      = {J_MP},
  author       = {Averkov, Gennadiy and Scheiderer, Claus},
  doi          = {10.1007/s10107-024-02060-9},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {113-131},
  shortjournal = {Math. Program.},
  title        = {Convex hulls of monomial curves, and a sparse positivstellensatz},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of smooth parametrizations on nonconvex
optimization landscapes. <em>MP</em>, <em>209</em>(1), 63–111. (<a
href="https://doi.org/10.1007/s10107-024-02058-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new tools to study landscapes in nonconvex optimization. Given one optimization problem, we pair it with another by smoothly parametrizing the domain. This is either for practical purposes (e.g., to use smooth optimization algorithms with good guarantees) or for theoretical purposes (e.g., to reveal that the landscape satisfies a strict saddle property). In both cases, the central question is: how do the landscapes of the two problems relate? More precisely: how do desirable points such as local minima and critical points in one problem relate to those in the other problem? A key finding in this paper is that these relations are often determined by the parametrization itself, and are almost entirely independent of the cost function. Accordingly, we introduce a general framework to study parametrizations by their effect on landscapes. The framework enables us to obtain new guarantees for an array of problems, some of which were previously treated on a case-by-case basis in the literature. Applications include: optimizing low-rank matrices and tensors through factorizations; solving semidefinite programs via the Burer–Monteiro approach; training neural networks by optimizing their weights and biases; and quotienting out symmetries.},
  archive      = {J_MP},
  author       = {Levin, Eitan and Kileel, Joe and Boumal, Nicolas},
  doi          = {10.1007/s10107-024-02058-3},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {63-111},
  shortjournal = {Math. Program.},
  title        = {The effect of smooth parametrizations on nonconvex optimization landscapes},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Level constrained first order methods for function
constrained optimization. <em>MP</em>, <em>209</em>(1), 1–61. (<a
href="https://doi.org/10.1007/s10107-024-02057-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new feasible proximal gradient method for constrained optimization where both the objective and constraint functions are given by summation of a smooth, possibly nonconvex function and a convex simple function. The algorithm converts the original problem into a sequence of convex subproblems. Formulating those subproblems requires the evaluation of at most one gradient-value of the original objective and constraint functions. Either exact or approximate subproblems solutions can be computed efficiently in many cases. An important feature of the algorithm is the constraint level parameter. By carefully increasing this level for each subproblem, we provide a simple solution to overcome the challenge of bounding the Lagrangian multipliers and show that the algorithm follows a strictly feasible solution path till convergence to the stationary point. We develop a simple, proximal gradient descent type analysis, showing that the complexity bound of this new algorithm is comparable to gradient descent for the unconstrained setting which is new in the literature. Exploiting this new design and analysis technique, we extend our algorithms to some more challenging constrained optimization problems where (1) the objective is a stochastic or finite-sum function, and (2) structured nonsmooth functions replace smooth components of both objective and constraint functions. Complexity results for these problems also seem to be new in the literature. Finally, our method can also be applied to convex function constrained problems where we show complexities similar to the proximal gradient method.},
  archive      = {J_MP},
  author       = {Boob, Digvijay and Deng, Qi and Lan, Guanghui},
  doi          = {10.1007/s10107-024-02057-4},
  journal      = {Mathematical Programming},
  month        = {1},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Math. Program.},
  title        = {Level constrained first order methods for function constrained optimization},
  volume       = {209},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mpc---5">MPC - 5</h2>
<ul>
<li><details>
<summary>
(2025). Minimizing total tardiness for the single-machine
identical-jobs order scheduling problem with a learning effect.
<em>MPC</em>, <em>17</em>(1), 141–171. (<a
href="https://doi.org/10.1007/s12532-024-00271-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a single machine identical-jobs order scheduling problem with a position-dependent learning effect (SIOSLE) to minimize the total tardiness. A learning effect is common in the identical-jobs order manufacturing, such as clothing, bicycles, shoes, and so on, but its impact on the order scheduling problem has not been studied, especially for orders with different numbers of the same type of jobs. A mixed integer programming (MIP) model is first formulated for SIOSLE and serves as a benchmark. A new branch-and-bound algorithm was developed to handle computational complexity based on the Dominance, Split, Elimination, and Decomposition rules revised from the traditional job scheduling problem and new lower and upper bounds. Numerical experiments demonstrate that the proposed branch-and-bound algorithm is computationally better than the performance of using Gurobi, a popular commercial solver, to solve the MIP. The experiments for large-sized problems found that the proposed branch-and-bound algorithm can solve instances with up to 120 orders. The algorithm is more efficient for instances with a strong or weak learning effect, with tight or loose due dates, or with heterogeneous due dates. The effectiveness of the Dominance, Split, Elimination, and Decomposition rules varies with parameter settings. In addition, the proposed branch-and-bound algorithm can yield better solutions than traditional meta-heuristic algorithms but may require longer run time for large instances.},
  archive      = {J_MPC},
  author       = {Hu, Jinchang and Jin, Mingzhou},
  doi          = {10.1007/s12532-024-00271-x},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {141-171},
  shortjournal = {Math. Program. Comput.},
  title        = {Minimizing total tardiness for the single-machine identical-jobs order scheduling problem with a learning effect},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fix-propagate-repair heuristic for mixed integer
programming. <em>MPC</em>, <em>17</em>(1), 111–139. (<a
href="https://doi.org/10.1007/s12532-024-00269-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a diving heuristic framework based on constraint propagation for mixed integer linear programs. The proposed approach is an extension of the common fix-and-propagate scheme, with the addition of solution repairing after each step. The repair logic is loosely based on the WalkSAT strategy for boolean satisfiability. Different strategies for variable ranking and value selection, as well as other options, yield different diving heuristics. The overall method is relatively inexpensive, as it is basically LP-free: the full linear programming relaxation is solved only at the beginning (and only for the ranking strategies that make use of it), while additional, typically much smaller, LPs are only used to compute values for the continuous variables (if any), once at the bottom of a dive. While individual strategies are not very robust in finding feasible solutions on a heterogeneous testbed, a portfolio approach proved quite effective. In particular, it could consistently find feasible solutions in 189 out of 240 instances from the public MIPLIB 2017 benchmark testbed, in a matter of a few seconds of runtime. The framework has also been implemented inside the commercial MIP solver Xpress and shown to give a small performance improvement in time to optimality on a large internal heterogeneous testbed.},
  archive      = {J_MPC},
  author       = {Salvagnin, Domenico and Roberti, Roberto and Fischetti, Matteo},
  doi          = {10.1007/s12532-024-00269-5},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {111-139},
  shortjournal = {Math. Program. Comput.},
  title        = {A fix-propagate-repair heuristic for mixed integer programming},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCQPow: A solver for linear complementarity quadratic
programs. <em>MPC</em>, <em>17</em>(1), 81–109. (<a
href="https://doi.org/10.1007/s12532-024-00272-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce an open-source software package written in C++ for efficiently finding solutions to quadratic programming problems with linear complementarity constraints. These problems arise in a wide range of applications in engineering and economics, and they are challenging to solve due to their structural violation of standard constraint qualifications, and highly nonconvex, nonsmooth feasible sets. This work extends a previously presented algorithm based on a sequential convex programming approach applied to a standard penalty reformulation. We examine the behavior of local convergence and introduce new algorithmic features. Competitive performance profiles are presented in comparison to state-of-the-art solvers and solution variants in both existing and new benchmarks.},
  archive      = {J_MPC},
  author       = {Hall, Jonas and Nurkanović, Armin and Messerer, Florian and Diehl, Moritz},
  doi          = {10.1007/s12532-024-00272-w},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {81-109},
  shortjournal = {Math. Program. Comput.},
  title        = {LCQPow: A solver for linear complementarity quadratic programs},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate maximum likelihood estimators for linear
regression with independent component-wise design matrix uncertainty.
<em>MPC</em>, <em>17</em>(1), 53–79. (<a
href="https://doi.org/10.1007/s12532-024-00268-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider regression problems subject to noise in the operator or design matrix. This characterization appropriately models many physical phenomena with uncertainty in the regressors. Although the problem has been studied extensively for ordinary/total least squares, and via models that implicitly or explicitly assume Gaussianity, less attention has been paid to improving estimation for regression problems under general independent component-wise uncertainty in the design matrix. To address difficulties encountered when dealing with distributions of sums of random variables, we rely on the saddle point method to estimate densities and form an approximate log-likelihood to maximize. We show that the proposed method performs favorably against other classical methods.},
  archive      = {J_MPC},
  author       = {Clancy, Richard J. and Becker, Stephen},
  doi          = {10.1007/s12532-024-00268-6},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {53-79},
  shortjournal = {Math. Program. Comput.},
  title        = {Approximate maximum likelihood estimators for linear regression with independent component-wise design matrix uncertainty},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the b-differential of the componentwise minimum of two
affine vector functions. <em>MPC</em>, <em>17</em>(1), 1–52. (<a
href="https://doi.org/10.1007/s12532-024-00266-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the description and computation of the B-differential of the componentwise minimum of two affine vector functions. This issue arises in the reformulation of the linear complementarity problem with the Min C-function. The question has many equivalent formulations and we identify some of them in linear algebra, convex analysis and discrete geometry. These formulations are used to state some properties of the B-differential, like its symmetry, condition for its completeness, its connectivity, bounds on its cardinality, etc. The set to specify has a finite number of elements, which may grow exponentially with the range space dimension of the functions, so that its description is most often algorithmic. We first present an incremental-recursive approach avoiding to solve any optimization subproblem, unlike several previous approaches. It is based on the notion of matroid circuit and the related introduced concept of stem vector. Next, we propose modifications, adapted to the problem at stake, of an algorithm introduced by Rada and Černý (SIAM J Discret Math 32(1):455-473, 2018, https://doi.org/10.1137/15M1027930 ) to determine the cells of an arrangement in the space of hyperplanes having a point in common. Measured in CPU time on the considered test-problems, the mean acceleration ratios of the proposed algorithms, with respect to the one of Rada and Černý, are in the range 15..31, and this speed-up can exceed 100, depending on the problem, the approach and the chosen linear optimization and matroid solvers.},
  archive      = {J_MPC},
  author       = {Dussault, Jean-Pierre and Gilbert, Jean Charles and Plaquevent-Jourdain, Baptiste},
  doi          = {10.1007/s12532-024-00266-8},
  journal      = {Mathematical Programming Computation},
  month        = {3},
  number       = {1},
  pages        = {1-52},
  shortjournal = {Math. Program. Comput.},
  title        = {On the B-differential of the componentwise minimum of two affine vector functions},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mva---23">MVA - 23</h2>
<ul>
<li><details>
<summary>
(2025). Adversarial learning for unguided single depth map
completion of indoor scenes. <em>MVA</em>, <em>36</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s00138-024-01652-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single depth map completion in the absence of any guidance from color images is a challenging, ill-posed problem in computer vision. Most of the conventional depth map completion approaches rely on information extracted from the corresponding color image and require heavy computations and optimization-based postprocessing functions, which cannot yield results in real time. Successful application of generative adversarial networks has led to significant progress in several computer vision problems including, color image inpainting. However, contrasting local and non-local features of depth maps compared to color images prevents the direct application of deep learning models designed for color image inpainting to depth map completion. Motivated by these challenges, in this work we propose to use deep adversarial learning to derive plausible estimates of missing depth information in a single degraded observation without any guidance from the corresponding RGB frame and any postprocessing. Different types of depth map degradations, such as simulated random and textual missing pixels as well as contiguous large holes found in Kinect depth maps, are effectively handled to reconstruct clean depth maps. An ablation study is also performed to investigate the contribution of our adversarial network architecture towards the recovery of missing scene depth information. We carry out an illustrative experimental analysis on the NYU-Depth V2 dataset and perform zero-shot generalization on the Middlebury and Matterport3D datasets, comparing our proposed method with several state-of-the-art algorithms. The experimental results demonstrate robustness and efficacy of the proposed approach.},
  archive      = {J_MVA},
  author       = {Medhi, Moushumi and Ranjan Sahay, Rajiv},
  doi          = {10.1007/s00138-024-01652-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial learning for unguided single depth map completion of indoor scenes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computer vision system for recognition and defect
detection for reusable containers. <em>MVA</em>, <em>36</em>(2), 1–19.
(<a href="https://doi.org/10.1007/s00138-024-01636-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small load carriers (SLCs) are standardized reusable containers used to transport and protect customer goods in many manufacturers. Throughout the life cycle of the SLCs, they will be collected, manually checked for defects (wear, cracks, and residue on the surface), and cleaned by specialized logistic companies. Human operators in small to medium-sized companies manually evaluate the defects due to the variety and degree of possible defects and varying customer needs. This manual evaluation is not scalable and prone to errors. This work aims to fill this gap by proposing a computer vision system that can recognize the SLC type for inventory management and perform defect detection automatically. First, we develop a camera portal, consisting of standard components, that capture the relevant surfaces of the SLC. A labeled dataset of 17,530 images of 34 different SLCs with their defect status was recorded using this camera portal. We trained a classification model (ConvNeXt) using our dataset to predict the different types of SLCs achieving 100% class prediction accuracy. For defect detection, we explore eight state-of-the-art (SOTA) anomaly detection models that achieved high rankings in the MVTec industrial anomaly detection benchmark. These models are trained using default hyperparameters and the two highest-scoring models were chosen and fine-tuned. The best-fine-tuned models based on “Area under the Receiver Operating Characteristic Curve (AUROC)” are PatchCore (0.811) and DRAEM (0.748). These results indicate that there is still potential for improvement in the automation of defect detection of SLCs.},
  archive      = {J_MVA},
  author       = {Wahyudi, Vincent and Ziegler, Cedric C. and Frieß, Matthias and Schramm, Stefan and Lang, Constantin and Eberhardt, Lars and Freund, Fabian and Dobhan, Alexander and Storath, Martin},
  doi          = {10.1007/s00138-024-01636-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A computer vision system for recognition and defect detection for reusable containers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-enhanced feature mapping network for
visible-infrared person re-identification. <em>MVA</em>, <em>36</em>(2),
1–17. (<a href="https://doi.org/10.1007/s00138-024-01646-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI-ReID) plays a pivotal role in surveillance systems, enabling the accurate identification of individuals across varying times and locations. Traditional methods struggle in low-light conditions, which motivates our research. We introduce an Attention-Enhanced Feature Mapping Network (AEFMNet) that addresses both intra-modal and inter-modal discrepancies. Our AEFMNet employs an Attention-based Feature Fusion Module (AFFM) to enhance global feature representation and a GCN-based Feature Mapping Module (GFMM) to reduce cross-modal feature gaps. The proposed network is further strengthened by a Joint Training Algorithm (JTA) that integrates multi-scale local and global features, enhancing cross-modal matching accuracy. Our approach achieves advanced performance on three large-scale data sets, demonstrating its effectiveness and robustness.},
  archive      = {J_MVA},
  author       = {Liu, Shuaiyi and Han, Ke},
  doi          = {10.1007/s00138-024-01646-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Attention-enhanced feature mapping network for visible-infrared person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Region gradient-guided diffusion model for underwater image
enhancement. <em>MVA</em>, <em>36</em>(2), 1–24. (<a
href="https://doi.org/10.1007/s00138-024-01647-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is a critical challenge in marine visual perception and underwater robotics due to complex aquatic environments that severely degrade image quality. This paper introduces the region gradient-guided diffusion model (RGGDM), a novel framework that addresses the fundamental trade-off between local detail preservation and global consistency in UIE. RGGDM innovatively integrates a region gradient-guided mechanism with a hybrid Swin-ConvNeXt architecture, introducing a spatially adaptive denoising process governed by gradient discrepancies between input and target images. We propose a learnable parameter $$\delta $$ that dynamically modulates denoising intensity, focusing computational resources on semantically salient regions. Our approach is underpinned by rigorous mathematical analysis, demonstrating convergence properties under mild assumptions and providing theoretical guarantees for the model’s stability and effectiveness. The synergistic combination of Swin Transformer and ConvNeXt enhances feature representation, significantly improving both perceptual quality and pixel-level accuracy. Extensive experiments on benchmark datasets demonstrate RGGDM’s superior performance, consistently outperforming state-of-the-art methods across multiple evaluation metrics. Notably, RGGDM achieves a peak signal-to-noise ratio (PSNR) of 25.48 dB and an underwater image quality measure (UIQM) of 4.37 on the UIEB dataset. Furthermore, enhanced images show substantial improvements in downstream tasks such as SIFT feature matching, with an average increase of 132.19% in matching points. These results underscore RGGDM’s potential in advancing underwater visual perception and its broader implications for marine robotics and environmental monitoring applications.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-024-01647-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Region gradient-guided diffusion model for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-core token mixer: A novel approach for underwater
image enhancement. <em>MVA</em>, <em>36</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01651-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is critical in various applications, including marine biology research, underwater archaeology, and autonomous underwater vehicle (AUV) navigation. The unpredictable nature of underwater environments frequently leads to degradation in contrast, color, and perceptual visual quality. Previous methods using the single receptive field to extract features are not capable of handling varying light conditions, which hinders detail preservation, color correction, and image quality improvement. To address these challenges, we propose Multi Core Token Mixer (MCTM) by introducing a distinctive multi-core mechanism. This mechanism is adept at extracting varied receptive fields, thereby enabling the model to capture the degradation at different scales caused by inhomogeneous underwater conditions. We performed experiments on three datasets (UIEB, EUVP, and UFO-120), and MCTM consistently outperforms existing models in image enhancement, color correction, and perceptual visual quality. Our work sets a new standard in the field and emphasizes the promise held by task-specific architectures that harness the power of Transformer models to tackle domain-specific challenges, particularly in UIE.},
  archive      = {J_MVA},
  author       = {Xu, Tianrun and Xu, Shiyuan and Chen, Xue and Chen, Feng and Li, Hongjue},
  doi          = {10.1007/s00138-024-01651-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-core token mixer: A novel approach for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A concept-aware explainability method for convolutional
neural networks. <em>MVA</em>, <em>36</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01653-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolutional Neural Networks (CNN) outperform the classical models in a wide range of Machine Vision applications, their restricted interpretability and their lack of comprehensibility in reasoning, generate many problems such as security, reliability, and safety. Consequently, there is a growing need for research to improve explainability and address their limitations. In this paper, we propose a concept-based method, called Concept-Aware Explainability (CAE) to provide a verbal explanation for the predictions of pre-trained CNN models. A new measure, called detection score mean, is introduced to quantify the relationship between the filters of the model and a set of pre-defined concepts. Based on the detection score mean values, we define sorted lists of Concept-Aware Filters (CAF) and Filter-Activating Concepts (FAC). These lists are used to generate explainability reports, where we can explain, analyze, and compare models in terms of the concepts embedded in the image. The proposed explainability method is compared to the state-of-the-art methods to explain Resnet18 and VGG16 models, pre-trained on ImageNet and Places365-Standard datasets. Two popular metrics, namely, the number of unique detectors and the number of detecting filters, are used to make a quantitative comparison. Superior performances are observed for the suggested CAE, when compared to Network Dissection (NetDis) (Bau et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017), Net2Vec (Fong and Vedaldi, in: Paper presented at IEEE conference on computer vision and pattern recognition (CVPR), 2018), and CLIP-Dissect (CLIP-Dis) (Oikarinen and Weng, in: The 11th international conference on learning representations (ICLR), 2023) methods.},
  archive      = {J_MVA},
  author       = {Gurkan, Mustafa Kagan and Arica, Nafiz and Yarman Vural, Fatos T.},
  doi          = {10.1007/s00138-024-01653-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A concept-aware explainability method for convolutional neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end unsupervised learning of latent-space clustering
for image segmentation via fully dense-UNet and fuzzy c-means loss.
<em>MVA</em>, <em>36</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s00138-024-01654-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental unsupervised approach in machine learning for grouping tasks. Image segmentation is one of the main applications of clustering and a preliminary requirement for most high-level applications in computer vision and scene understanding. However, parameter tuning requirements of conventional unsupervised image segmentation approaches limit their application. Deep learning approaches are capable of diverse and discriminate feature learning, however supervised learning paradigm and computational complexity of deep neural networks (DNNs) induces bottlenecks for real-time applications. We present unsupervised learning paradigm for fully dense-UNet (FDU-Net) model training with loss constraints: Semantic loss, Fuzzy C-means Clustering (FCM) loss, and Total Variation (TV) loss. Semantic loss works by selecting maximum activation class for each pixel spatial location and Simple Linear Iterative Clustering (SLIC)-based spatial refinement provides a coherent feature representation for model optimisation. FCM loss is based on the objective function of the conventional unsupervised Fuzzy C-means algorithm loss function. TV loss computes and minimises the spatial discontinuities in the FDU-Net activation maps. Loss constraints operate in tandem to ensure the control of false positives and false negatives. We conduct extensive experiments to compare our proposed method with unsupervised conventional and contemporary deep learning-driven (DL) methods. We experimentally demonstrate that the proposed method yields competitive quantitative and, most importantly, qualitative segmentation results, on the unseen images from the BSDS500 benchmark dataset. During inference, the segmentation quality of the proposed approach results is more significant than the contemporary DL-based and conventional clustering methods while reducing the computation cost by several folds.},
  archive      = {J_MVA},
  author       = {Khan, Zubair and Khan, Tehreem and Sattar, Mohsin and Yang, Jie},
  doi          = {10.1007/s00138-024-01654-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {End-to-end unsupervised learning of latent-space clustering for image segmentation via fully dense-UNet and fuzzy C-means loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WTT: Combining wavelet transform with transformer for remote
sensing image super-resolution. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01655-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most deep learning-based super-resolution techniques primarily operate in the spatial domain, utilizing similar methods to process high- and low-frequency information in images. However, this often results in edge blurring. To address this issue, this paper introduces a novel structure that integrates wavelet transform and transformer mechanisms. The proposed method effectively segregates high- and low-frequency image information via discrete wavelet transform (DWT) and learns their correlations through a self-attention mechanism to enhance super-resolution outcomes. Specifically, the input image/feature is decomposed into four frequency domain components using DWT, which are concatenated to form a full-frequency domain feature map. A high-frequency feature map is constructed from three of these components. A new feature map is then generated using multi-head self-attention, with the full-frequency domain feature map serving as the query and value, and the high-frequency feature map as the key. The output feature map is produced by applying inverse DWT, with the new feature map serving as the low-frequency component and the original high-frequency components retained. Additionally, a parallel 1 × 1 convolution filter is employed to minimize information loss. Furthermore, a super-resolution network for remote sensing images is constructed by combining wavelet transform and transformer, incorporating hierarchical residual connections to enable the network to focus on learning high-frequency information. Experimental results on a publicly available remote sensing dataset demonstrate the superiority of the proposed method compared to existing approaches.},
  archive      = {J_MVA},
  author       = {Liu, Jingyi and Yang, Xiaomin},
  doi          = {10.1007/s00138-024-01655-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WTT: Combining wavelet transform with transformer for remote sensing image super-resolution},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environmental factors-aware two-stream GCN for
skeleton-based behavior recognition. <em>MVA</em>, <em>36</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s00138-024-01656-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of human behavior recognition, modeling human skeletons as spatio-temporal graphs using Graph Convolutional Networks (GCNs) has achieved outstanding performance. Existing GCN-based methods typically focus on the two-dimensional or three-dimensional features of the skeleton. However, the same action may represent different behaviors in different environments, making it suboptimal to consider only skeleton features for behavior recognition tasks with diverse scenarios. To simultaneously account for both skeleton features and environmental factors that influence human behaviors, this study proposed a novel two-stream Graph Convolutional Network, 2S-EGCN, which incorporates environmental factors for human behavior recognition. In this network, we designed an innovative environmental factor sampling strategy that samples fixed-scale environmental factors from variable-scale feature maps. To better integrate environmental factors with skeleton features, we further developed a Skeleton-Environment Interaction Module. This module uses a specific feature fusion method to combine environmental factors with skeleton features, allowing for the modeling of both pure skeleton information and skeleton information fused with environmental factors, thus improving behavior recognition accuracy. Extensive experiments conducted on the large Kinetics dataset demonstrate that our model outperforms the state-of-the-art, improving top-1 accuracy by 1.71–55.61% and achieving top-5 accuracy of 93.41%.},
  archive      = {J_MVA},
  author       = {Li, Zhuoran and Yan, Lianshan and Li, Hua and Wang, Yu},
  doi          = {10.1007/s00138-024-01656-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Environmental factors-aware two-stream GCN for skeleton-based behavior recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry-induced ambiguity in orientation estimation from
RGB images. <em>MVA</em>, <em>36</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s00138-024-01657-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of object orientation from RGB images is a core component in many modern computer vision pipelines. Traditional techniques mostly predict a single orientation per image, learning a one-to-one mapping between images and rotations. However, when objects exhibit rotational symmetries, they can appear identical from multiple viewpoints. This induces ambiguity in the estimation problem, making images map to rotations in a one-to-many fashion. In this paper, we explore several ways of addressing this problem. In doing so, we specifically consider algorithms that can map an image to a range of multiple rotation estimates, accounting for symmetry-induced ambiguity. Our contributions are threefold. Firstly, we create a data set with annotated symmetry information that covers symmetries induced through self-occlusion. Secondly, we compare and evaluate various learning strategies for multiple-hypothesis prediction models applied to orientation estimation. Finally, we propose to model orientation estimation as a binary classification problem. To this end, based on existing work from the field of shape reconstruction, we design a neural network that can be sampled to reconstruct the full range of ambiguous rotations for a given image. Quantitative evaluation on our annotated data set demonstrates its performance and motivates our design choices.},
  archive      = {J_MVA},
  author       = {Bertens, Tijn and Caasenbrood, Brandon and Saccon, Alessandro and Jalba, Andrei},
  doi          = {10.1007/s00138-024-01657-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Symmetry-induced ambiguity in orientation estimation from RGB images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ipdm: Identity preserving diffusion model for face sketch
and photo synthesis. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-024-01658-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face sketch and photo synthesis is widely applied in industry and information fields, such as entertainment business and heterogeneous face retrieval. The key challenge lies in completing a face transformation with both good visual effects and face identity preservation. However, existing methods are still difficult to obtain a good synthesis due to the large model gap between the two different face domains. Recently, diffusion models have achieved great success in image synthesis, which allows us to extend its application in such a face generation task. Thus, we propose IPDM, which constructs a mapping of latent representation for domain-adaptive face features. The other proposed IDP utilizes auxiliary features to correct the latent features through their directions and supplementary identity information, so that the generation can keep face identity unchanged. The various evaluation results show that our method is superior to state-of-the-art methods in both identity preservation and visual effects.},
  archive      = {J_MVA},
  author       = {Tang, Duoxun and Jiang, Xinhang and Zhang, Ying and Dai, Yuhang and Lin, Ye},
  doi          = {10.1007/s00138-024-01658-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ipdm: Identity preserving diffusion model for face sketch and photo synthesis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personvit: Large-scale self-supervised vision transformer
for person re-identification. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01659-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (ReID) aims to retrieve relevant individuals in non-overlapping camera images and has a wide range of applications in the field of public safety. In recent years, with the development of Vision Transformer (ViT) and self-supervised learning techniques, the performance of person ReID based on self-supervised pre-training has been greatly improved. Person ReID requires extracting highly discriminative local fine-grained features of the human body, while traditional ViT is good at extracting context-related global features, making it difficult to focus on local human body features. To this end, this article introduces the recently emerged Masked Image Modeling (MIM) self-supervised learning method into person ReID, and effectively extracts high-quality global and local features through large-scale unsupervised pre-training by combining masked image modeling and discriminative contrastive learning, and then conducts supervised fine-tuning training in the person ReID task. This person feature extraction method based on ViT with masked image modeling (PersonViT) has the good characteristics of unsupervised, scalable, and strong generalization capabilities, overcoming the problem of difficult annotation in supervised person ReID, and achieves state-of-the-art results on publicly available benchmark datasets, including MSMT17, Market1501, DukeMTMC-reID, and Occluded-Duke. The code and pre-trained models of the PersonViT method are released at https://github.com/hustvl/PersonViT to promote further research in the person ReID field.},
  archive      = {J_MVA},
  author       = {Hu, Bin and Wang, Xinggang and Liu, Wenyu},
  doi          = {10.1007/s00138-025-01659-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Personvit: Large-scale self-supervised vision transformer for person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversified image style transfer—approaches, new methods and
directed variability control. <em>MVA</em>, <em>36</em>(2), 1–12. (<a
href="https://doi.org/10.1007/s00138-025-01660-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image style transfer is to automatically redraw an input image in the style of another image, such as an artist’s painting. The disadvantage of conventional stylization algorithms is the uniqueness of result. If the user is not satisfied with the way the style was transferred, he has no option to remake the stylization. The paper provides an overview of existing style transfer methods that generate diverse results after each run and proposes two new methods. The first method enables diversity by concatenating a random vector into inner image representation inside the neural network and by reweighting image features accordingly in the loss function. The second method allows diverse stylizations by passing the stylized image through orthogonal transformations, which impact the way the target style is transferred. These blocks are trained to replicate patterns from additional pattern images, which serve as additional input and provide an interpretable way to control stylization variability for the end user. Qualitative and quantitative comparisons demonstrate that both methods are capable to generate different stylizations with higher variability achieved by the second method. The code of both methods is available on github.},
  archive      = {J_MVA},
  author       = {Ustyuzhanin, Alexander and Kitov, Victor and Kitov, Vladimir},
  doi          = {10.1007/s00138-025-01660-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Diversified image style transfer—approaches, new methods and directed variability control},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional cascaded multimodal attention for multiple
choice visual question answering. <em>MVA</em>, <em>36</em>(2), 1–16.
(<a href="https://doi.org/10.1007/s00138-025-01661-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a rapidly advancing field that aims to develop systems capable of answering questions based on image content. Performance of a VQA model largely depends on the effective integration of multimodal data. A sparsity-based Bidirectional Cascaded Multimodal Attention network has been proposed in this paper. This model leverages bidirectional attention between image and text modalities, enabling a deeper contextual understanding of one modality through the other. To encourage the focus of attention mechanism on the most relevant regions in the input, sparsity has been introduced in these interactions. In multiple choice VQA, answer options contain important context, and incorporating them with multimodal features using attention results in a comprehensive feature representation. The performance of the proposed model is assessed using the multiple-choice Visual7W dataset. To test the generalizability of the model, a modified VQAv2 dataset is prepared and evaluated. Through extensive experiments, the model demonstrates competitive performance, effectively handling diverse question types such as “what”, “where”, “who”, “why”, and “how”. A detailed analysis of attention maps for different question types highlights how the model focuses on various input regions. Visualizations of image, text, and cross-modal attention maps reveal the key areas that contributed to the model’s decision-making process.},
  archive      = {J_MVA},
  author       = {Upadhyay, Sushmita and Tripathy, Sanjaya Shankar},
  doi          = {10.1007/s00138-025-01661-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Bidirectional cascaded multimodal attention for multiple choice visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CorFormer: A hybrid transformer-CNN architecture for
corrosion segmentation on metallic surfaces. <em>MVA</em>,
<em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01663-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of periodic corrosion inspection in steel structures cannot be overstated. However, current manual inspection approaches are fraught with challenges: they are time-consuming, subjective, and pose risks. To address these limitations, extensive research has been conducted over the past decade gauging the feasibility of Convolutional Neural Networks (CNNs) for automation of corrosion inspection. Meanwhile, Transformer networks have recently emerged as powerful tools in computer vision due to their ability to model intricate global relationships. In this paper, a novel hybrid architecture, dubbed CorFormer, is proposed for effective and efficient automation of corrosion inspection. The CorFormer network fuses Transformer and CNN layers at different stages of the encoder, which captures global context through Transformer layers while leveraging the inherent inductive bias of CNNs. To bridge the semantic gap between features generated by Transformer and CNN layers, a Semantic Gap Merger (SGM) module is introduced after each feature merge operation. The encoder is complemented by a hierarchical decoder, able to decrypt complex features at large and small scales. CorFormer is compared against state-of-the-art CNN and Transformer architectures for corrosion segmentation, and is found to outperform the best alternative by 2.7% in terms of Intersection over Union (IoU) across 10 validation data splits. Furthermore, it enables real-time inspection at an impressive rate of 28 frames per second. Rigorous statistical tests provide support for the findings presented in this study, and an extensive ablation study validates all design choices.},
  archive      = {J_MVA},
  author       = {Subedi, Abhishek and Qian, Cheng and Sadeghian, Reza and Jahanshahi, Mohammad R.},
  doi          = {10.1007/s00138-025-01663-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CorFormer: A hybrid transformer-CNN architecture for corrosion segmentation on metallic surfaces},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretability of fingerprint presentation attack
detection systems: A look at the “representativeness” of samples against
never-seen-before attacks. <em>MVA</em>, <em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01666-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, fingerprint Presentation Attack Detection systems (PADs) are primarily based on deep learning architectures subjected to massive training. However, their performance decreases to never-seen-before attacks. With the goal of contributing to explaining this issue, we hypothesized that this limited ability to generalize is due to the lack of &quot;representativeness&quot; of the samples available for the PAD training. &quot;Representativeness&quot; is treated here from a geometrical perspective: the spread of samples into the feature space, especially near the decision boundaries. In particular, we explored the possibility of adopting three-dimensionality reduction methods to make the problem affordable through visual inspection. These methods enable visual inspection and interpretation by projecting data into two-dimensional spaces, facilitating the identification of weak areas in the decision regions estimated after the training phase. Our analysis delineates the benefits and drawbacks of each dimensionality reduction method and leads us to make substantial recommendations in the crucial phase of the training design.},
  archive      = {J_MVA},
  author       = {Carta, Simone and Casula, Roberto and Orrù, Giulia and Micheletto, Marco and Marcialis, Gian Luca},
  doi          = {10.1007/s00138-025-01666-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Interpretability of fingerprint presentation attack detection systems: A look at the “representativeness” of samples against never-seen-before attacks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Depthanything and SAM for UIE: Exploring large model
information contributes to underwater image restoration. <em>MVA</em>,
<em>36</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s00138-025-01662-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) remains a fundamental yet challenging problem in computer vision due to the complex physics of light propagation in aquatic environments. Traditional physics-based or learning-driven approaches often need more prior knowledge and representational capacity to generalize across diverse underwater conditions. This paper presents a novel theoretical framework for leveraging large-scale pre-trained models in UIE, explicitly addressing the fundamental limitations in existing methods through principled integration of depth and semantic priors. Our key contribution is twofold: First, we establish a rigorous information-theoretic foundation that quantifies how auxiliary features from foundation models enhance the representational capacity of UIE systems, providing theoretical guarantees through PAC-Bayesian bounds on generalization performance. Second, we propose a Feature Enhancement Strategy that optimally combines depth information from DepthAnything and semantic priors from the Segment Anything Model, guided by underwater optical physics. We introduce CAB-USRI, a physics-based algorithm for both baseline and theoretical validation. Our extensive experimentation on multiple benchmark datasets demonstrates that our approach consistently outperforms state-of-the-art methods by significant margins while maintaining theoretical interpretability. Our ablation studies reveal the crucial role of depth priors in underwater scenarios, establishing a clear connection between theoretical bounds and empirical performance. This work bridges the gap between foundation models and domain-specific tasks, providing theoretical insights and practical solutions for complex image restoration problems in challenging environments.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-025-01662-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Depthanything and SAM for UIE: Exploring large model information contributes to underwater image restoration},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based power line cables and pylons detection for low
flying aircraft. <em>MVA</em>, <em>36</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s00138-025-01664-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power lines are dangerous for low-flying aircraft, especially in low-visibility conditions. Thus, a vision-based system able to analyze the aircraft’s surroundings and to provide the pilots with a “second pair of eyes” can contribute to enhancing their safety. To this end, we develop a deep learning approach to jointly detect power line cables and pylons from images captured at distances of several hundred meters by aircraft-mounted cameras. In doing so, we combine a modern convolutional architecture with transfer learning and a loss function adapted to curvilinear structure delineation. We use a single network for both detection tasks and demonstrate its performance on two benchmarking datasets. We have also integrated it within an onboard system and run it inflight. We show with our experiments that it outperforms the prior distant cable detection method by Stambler et al. (in: International Conference on Robotics and Automation, 2019) on both datasets, while also successfully detecting pylons, given their annotations are available for the data.},
  archive      = {J_MVA},
  author       = {Gwizdała, Jakub and Oner, Doruk and Roy, Soumava Kumar and Shah, Mian Akbar and Eberhard, Ad and Egorov, Ivan and Krüsi, Philipp and Yakushev, Grigory and Fua, Pascal},
  doi          = {10.1007/s00138-025-01664-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Vision-based power line cables and pylons detection for low flying aircraft},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D face parsing based on 2D CPFNet: Conformal parameterized
face parsing network. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01667-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing is a fundamental component of many advanced face technologies, which assigns labels to each pixel on the face data. Although three-dimensional (3D) face parsing has the potential to outperform its two-dimensional (2D) counterpart, it remains challenging due to the high cost of processing 3D mesh data. Recent works have introduced various methods for 3D surface segmentation, but their performance is still limited and they consume large amounts of memory and computation. In this paper, we propose a “3D–2D–3D” strategy for 3D face parsing. First, we transform 3D face data into a topological disk-like 2D face image containing spatial and textural information via conformal parameterization. Subsequently, we use a specific 2D deep learning network called CPFNet to achieve 2D face image semantic segmentation with multiscale technology and feature aggregation. Finally, the 2D semantic result is inversely remapped to the 3D face data to achieve 3D face parsing. Experimental results show that both CPFNet and our “3D–2D–3D” strategy accomplish high-quality 3D face parsing and outperform some 2D networks and 3D methods in both qualitative and quantitative comparisons.},
  archive      = {J_MVA},
  author       = {Yang, M. and Sun, W. and Wang, Y. and Zhou, G. and Tong, J. and Zhou, P.},
  doi          = {10.1007/s00138-025-01667-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D face parsing based on 2D CPFNet: Conformal parameterized face parsing network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic volume measurement using nonlinear count-lines.
<em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01668-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic volume measurements are increasingly recognized as important for improving society’s infrastructure, by functions such as managing congestion and enhancing logistics. Dedicated traffic data capture devices that use sensors embedded in the road enable accurate measurements but have the problems of high cost and limited installation locations, which make it difficult to expand the coverage of traffic volume measurements. To address this issue, the approach that combines already deployed Closed-Circuit Television (CCTV) cameras with image recognition technology has attracted attention and offers practical performance in ordinary situations. One remaining problem is that accuracy is degraded by the presence of headlight flare at nighttime and occlusion by large vehicles on busy roads. In this paper, we propose a method for measuring traffic volume that automatically sets count-lines using the Kernel Support Vector Machine (Kernel SVM) at optimal positions less affected by these issues. In addition, to make the proposal robust to illumination changes and occlusion we introduce nonlinear count-lines. Extensive experiments on Japanese road video footage shows that our method improves accuracy by $$5.9\%$$ at night and $$2.1\%$$ in situations prone to occlusion compared to the most basic fixed count-line method. Additionally, experiments on a public dataset, UA-DETRAC, demonstrate the proposal’s effectiveness in countries other than Japan.},
  archive      = {J_MVA},
  author       = {Iwao, Yuwa and Yamamoto, Yota and Yaginuma, Hideki and Taniguchi, Yukinobu},
  doi          = {10.1007/s00138-025-01668-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Traffic volume measurement using nonlinear count-lines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting few-shot learning via selective patch embedding by
comprehensive sample analysis. <em>MVA</em>, <em>36</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s00138-025-01669-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of limited data samples, few-shot learning continues to pose a significant challenge. A prevalent strategy in recent times has been to pre-train models on extensive datasets and subsequently transfer them to downstream few-shot tasks, which has demonstrated efficacy in enhancing performance. However, a persistent challenge lies in the inadequacy of pre-trained models to capture the essential features of the new downstream dataset. This issue is particularly acute in images containing multiple entities, where crucial features are often overlooked yet play a pivotal role in image classification. To address this challenge, we propose an innovative local information enhancement strategy that harnesses information from all samples to capture important local features in images and integrates them with global features. The objective of our strategy is to enhance class differentiation by ensuring distinct class prototypes in the embedding space through the incorporation of local information. By integrating local information, query samples exhibit closer alignment with the prototype of their respective classes, ultimately resulting in improved classification accuracy. To further bolster the performance of few-shot classification, we have refined the pre-trained model approach and augmented the dataset. Comprehensive ablation experiments demonstrate the specific impact of our approach on enhancing the accuracy of few-shot classification.},
  archive      = {J_MVA},
  author       = {Yang, Juan and Zhang, Yuliang and Wang, Ronggui and Xue, Lixia},
  doi          = {10.1007/s00138-025-01669-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boosting few-shot learning via selective patch embedding by comprehensive sample analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced normal estimation of point clouds via fine-grained
geometric information learning. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01671-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud normal estimation is a fundamental task in 3D computer graphics, essential for downstream applications such as surface reconstruction and semantic segmentation. While recent advances in deep learning have significantly improved normal estimation accuracy, existing methods often struggle with capturing fine-grained geometric details. In this study, we propose a novel encoder that integrates a local gradient attention module and positional encoding to better capture subtle geometric variations. By introducing the gradient attention module, we effectively capture fine-grained information along the z-axis, while positional encoding using sine and cosine functions further amplifies these variations. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art methods, achieving up to a 2.53% improvement in accuracy on PCPNet dataset. Our work not only advances normal estimation but also demonstrates its potential for surface reconstruction tasks. The code is available at https://github.com/ABc90/gam-net-normal-main .},
  archive      = {J_MVA},
  author       = {Jin, Wei and Zhou, Jun and Wang, Mingjie and Li, Nannan and Wang, Weixiao and Liu, Xiuping},
  doi          = {10.1007/s00138-025-01671-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced normal estimation of point clouds via fine-grained geometric information learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNFR: Salient neighbor decoding and text feature refining
for scene text recognition. <em>MVA</em>, <em>36</em>(2), 1–13. (<a
href="https://doi.org/10.1007/s00138-025-01672-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition methods are broadly categorized into serial and parallel. Serial methods achieve superior accuracy but are slower in speed. Parallel methods offer faster speed but may sacrifice accuracy. Current methods struggle to strike a balance between accuracy and inference speed, particularly facing challenges in both accuracy and speed. Therefore, we propose a new scene text recognizer called SNFR. It includes a simple yet efficient decoder, Salient Neighbor Decoder (SND), which achieves high accuracy recognition with lower computational cost for attention map calculation. SND generates a neighbor matrix by selecting salient positions, which guides the generation of all the character attention maps. We also propose a Text Feature Refining Module (TFRM) to capture the contextual relationship of text sequences, enhancing the overall feature representation of scene text. The experimental results demonstrate that our method achieves competitive performance on standard datasets and also shows superior performance on long text recognition.},
  archive      = {J_MVA},
  author       = {Lu, Tongwei and Fan, Huageng and Chen, Yuqian and Shao, Pengyan},
  doi          = {10.1007/s00138-025-01672-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SNFR: Salient neighbor decoding and text feature refining for scene text recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="naco---11">NACO - 11</h2>
<ul>
<li><details>
<summary>
(2025). Self-organizing nest migration dynamics synthesis for ant
colony systems. <em>NACO</em>, <em>24</em>(1), 163–172. (<a
href="https://doi.org/10.1007/s11047-022-09923-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we synthesize a novel dynamical approach for ant colonies enabling them to migrate to new nest sites in a self-organizing fashion. In other words, we realize ant colony migration as a self-organizing phenotype-level collective behavior. For this purpose, we first segment the edges of the graph of ants’ pathways. Then, each segment, attributed to its own pheromone profile, may host an ant. So, multiple ants may occupy an edge at the same time. Thanks to this segment-wise edge formulation, ants have more selection options in the course of their pathway determination, thereby increasing the diversity of their colony’s emergent behaviors. In light of the continuous pheromone dynamics of segments, each edge owns a spatio-temporal piece-wise continuous pheromone profile in which both deposit and evaporation processes are unified. The passive dynamics of the proposed migration mechanism is sufficiently rich so that an ant colony can migrate to the vicinity of a new nest site in a self-organizing manner without any external supervision. In particular, we perform extensive simulations to test our migration dynamics applied to a colony including 500 ants traversing a pathway graph comprising 200 nodes and 4000 edges which are segmented based on various resolutions. The obtained results exhibit the effectiveness of our strategy.},
  archive      = {J_NACO},
  author       = {Macktoobian, Matin},
  doi          = {10.1007/s11047-022-09923-0},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {163-172},
  shortjournal = {Nat. Comput.},
  title        = {Self-organizing nest migration dynamics synthesis for ant colony systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling and evaluating restricted ESNs on single- and
multi-timescale problems. <em>NACO</em>, <em>24</em>(1), 149–161. (<a
href="https://doi.org/10.1007/s11047-024-10004-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir Computing is a computing model ideal for performing computation on varied physical substrates. However, these physical reservoirs can be difficult to scale up. We propose joining various reservoirs together as an approach to solving this problem, simulating physical reservoirs with Echo State Networks (ESNs). We investigate various methods of combining ESNs to form larger reservoirs, including a method that we dub Restricted ESNs. We provide a notation for describing Restricted ESNs, and use it to benchmark a standard ESN against restricted ones. We investigate two methods to keep the weight matrix density consistent when comparing a Restricted ESN to a standard one, which we call overall consistency and patch consistency. We benchmark restricted ESNs on NARMA10 and the sunspot prediction benchmark, and find that restricted ESNs perform similarly to standard ones. We present some application scenarios in which restricted ESNs may offer advantages over standard ESNs. We then test restricted ESNs on a version of the multi-timescale Multiple Superimposed Sines tasks, in order to establish a baseline performance that can be improved upon in further work. We conclude that we can scale up reservoir performance by linking small homogeneous subreservoirs together without significant loss in performance over a single large reservoir, justifying future work on using heterogeneous subreservoirs for greater flexibility.},
  archive      = {J_NACO},
  author       = {Wringe, Chester and Stepney, Susan and Trefzer, Martin A.},
  doi          = {10.1007/s11047-024-10004-7},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {149-161},
  shortjournal = {Nat. Comput.},
  title        = {Modelling and evaluating restricted ESNs on single- and multi-timescale problems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-shuffle card-based protocol with eight cards per gate
and its extensions. <em>NACO</em>, <em>24</em>(1), 131–147. (<a
href="https://doi.org/10.1007/s11047-024-10006-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Card-based cryptography allows us to securely compute arbitrary functions using a deck of physical cards. Its performance is mainly measured by the number of used cards and shuffles, and there is a line of work that aims to reduce either of them. One seminal work is the card-based garbled circuit technique by Shinagawa and Nuida (Discret Appl Math 289:248–261, 2021, https://doi.org/10.1016/j.dam.2020.10.013 ), which allows the construction of a card-based protocol for any Boolean function with a single shuffle. Their construction requires $$2n + 24g$$ cards for an n-input Boolean function that is represented by g logical gates. In this paper, we reduce the number of cards to $$2n + 8g$$ for arbitrary functions while keeping it working with only one shuffle. In addition, we propose two types of extensions to support numerical encoding and multi-input gates. In the extended scheme, the free-ADD technique, obtained by generalizing the free-XOR technique by Manabe and Shinagawa (Deng J, Kolesnikov V, Schwarzmann AA (eds) CANS 2023, LNCS, vol 14342. Springer, Singapore, pp 232–248, 2023, https://doi.org/10.1007/978-981-99-7563-1-11 ), is available. The free-ADD technique allows our scheme to evaluate any n-input symmetric Boolean function using $$2n^2+6n+2$$ cards.},
  archive      = {J_NACO},
  author       = {Tozawa, Kazunari and Morita, Hiraku and Mizuki, Takaaki},
  doi          = {10.1007/s11047-024-10006-5},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {131-147},
  shortjournal = {Nat. Comput.},
  title        = {Single-shuffle card-based protocol with eight cards per gate and its extensions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of the relative effects of connectivity and
coupling interactions on spin networks emulating the d-wave 2000Q
quantum annealer. <em>NACO</em>, <em>24</em>(1), 113–129. (<a
href="https://doi.org/10.1007/s11047-024-10001-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From available data, we show strong positive spatial correlations in the qubits of a D-Wave 2000Q quantum annealing chip that are connected to qubits outside their own unit cell. Then, by simulating the dynamics of three different spin networks and two different initial conditions, we then show that correlation between nodes is affected by a number of factors. The different connectivity of qubits within the network means that information transfer is not straightforward even when all the qubit-qubit couplings have equal weighting. Connected nodes behave even more dissimilarly when the couplings’ strength is scaled according to the physical length of the connections (here to simulate dipole-dipole interactions). This highlights the importance of understanding the architectural features and potentially unprogrammed interactions/connections that can divert the performance of a quantum system away from the idealised model of identical qubits and couplings across the chip.},
  archive      = {J_NACO},
  author       = {Park, Jessica and Stepney, Susan and D’Amico, Irene},
  doi          = {10.1007/s11047-024-10001-w},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {113-129},
  shortjournal = {Nat. Comput.},
  title        = {An analysis of the relative effects of connectivity and coupling interactions on spin networks emulating the D-wave 2000Q quantum annealer},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating particle swarm optimization and various
mobility algorithms for autonomous navigation in flying ad-hoc networks.
<em>NACO</em>, <em>24</em>(1), 95–112. (<a
href="https://doi.org/10.1007/s11047-024-10009-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flying ad-hoc networks (FANETs) have gained significant attention within the research community due to the widespread availability of unmanned aerial vehicles (UAVs) and the electronic components required for their control and connectivity. Various applications, such as 3D mapping, construction inspection, and emergency response operations, stand to benefit from leveraging a swarm of UAVs instead of a single UAV. This necessitates the establishment of an ad-hoc network for communication and coordination. An important aspect of implementing FANETs involves autonomously determining the optimal position of the UAVs to ensure communications with the ground nodes while maximizing coverage and connectivity to a remote server. In this research, an application of particle swarm optimization (PSO) algorithm is proposed to achieve optimal positioning of the UAVs facilitating air-to-ground communications to several ground nodes whose positions within the grid are unknown. The performance of the PSO algorithm is compared with fixed and hybrid models across varying grid sizes (1000 × 1000 and 1500 × 1500), numbers of UAVs (N = 3, …, 9), and numbers of sensor nodes (n = 10, 20, and 30). The log-normal propagation model is considered to account for channel fading effects resulting from multipath propagation.},
  archive      = {J_NACO},
  author       = {Paredes, W. D. and Kaushal, Hemani and Prodanoff, Z. and Vakilinia, I.},
  doi          = {10.1007/s11047-024-10009-2},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {95-112},
  shortjournal = {Nat. Comput.},
  title        = {Investigating particle swarm optimization and various mobility algorithms for autonomous navigation in flying ad-hoc networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proving new directed tile complexity lower bounds at
temperature 1 by folding between 2D and just-barely 3D self-assembly.
<em>NACO</em>, <em>24</em>(1), 79–93. (<a
href="https://doi.org/10.1007/s11047-024-09979-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of determining the size of the smallest tile set that uniquely self-assembles into a given target shape in Winfree’s abstract Tile Assembly Model (aTAM), an elegant theoretical model of DNA tile self-assembly. This problem is also known as the “directed tile complexity” problem. We prove two main results related to the directed tile complexity problem within a variant of the aTAM in which the minimum binding strength threshold (temperature) is set to 1. For our first result, self-assembly happens in a “just-barely 3D” setting, where self-assembling unit cubes are allowed to be placed in the $$z=0$$ and $$z=1$$ planes. This is the same setting in which Furcy, Summers and Withers (DNA 2021) recently proved lower and upper bounds on the directed tile complexity of a just-barely 3D $$k \times N$$ rectangle at temperature 1 of $$\Omega \left( N^{\frac{1}{k}}\right) $$ and $$O\left( N^{\frac{1}{k-1}}+\log N\right) $$ , respectively, the latter of which does not hold for $$k=2$$ . Our first result closes this gap for $$k=2$$ by proving an asymptotically tight bound of $$\Theta (N)$$ on the directed tile complexity of a just-barely 3D $$2 \times N$$ rectangle at temperature 1. Our proof uses a novel process by which a just-barely 3D assembly sequence is “unfolded” to an equivalent 2D assembly sequence. For our second result, we use the aforementioned lower bound by Furcy, Summers and Withers and a novel process that is complementary-in-spirit to our 3D-to-2D unfolding process, by which we “fold” a 2D tile assembly to an equivalent just-barely 3D assembly to prove a new lower bound on the directed tile complexity of a 2D $$k \times N$$ rectangle at temperature 1 of $$\Omega \left( \frac{N^{\frac{2}{k + (k \bmod 2)}}}{k} \right) $$ . For fixed k, our new bound gives a nearly quadratic improvement over, and matches for general even values of $$k &lt; \frac{\log N}{\log \log N - \log \log \log N}$$ the state of the art lower bound on the directed tile complexity of a $$k \times N$$ rectangle at temperature 1 by Furcy, Summers and Wendlandt (DNA 2019) of $$\Omega \left( N^{\frac{1}{k}}\right) $$ . While both of our results represent improvements over previous corresponding state of the art results, the proofs thereof are facilitated by novel examples of reasoning about tile self-assembly happening in 2D (just-barely 3D) as though it is happening in just-barely 3D (2D).},
  archive      = {J_NACO},
  author       = {Furcy, David and Summers, Scott M. and Vadnais, Hailey},
  doi          = {10.1007/s11047-024-09979-0},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {79-93},
  shortjournal = {Nat. Comput.},
  title        = {Proving new directed tile complexity lower bounds at temperature 1 by folding between 2D and just-barely 3D self-assembly},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Abstract geometrical computation 12: Generating
representation of infinite countable linear orderings. <em>NACO</em>,
<em>24</em>(1), 67–78. (<a
href="https://doi.org/10.1007/s11047-024-10005-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Any countable (infinite or not) linear (total) ordering can be represented by displaying all its elements on an axis in increasing order. Such a representation can be generated using only geometrical constructions based on coloured line segment extensions and rules to handle segment intersections. After a bounded time, the construction segments disappear and only the representation remains. The process starts with finitely many segments, so that unbounded acceleration effects are used to generate infinitely many segments for the representation. There is no outside machinery nor operator: any needed computation has to be carried out through the drawing. After providing some illustrative examples with ad hoc constructions, we prove our main results. One rational signal machine (bounded to use only rational coordinates) can generate the representation of any decidable linear ordering (i.e. the order between two elements is decidable by a Turing machine). In the general case, there is a signal machine able to generate the representation of any countable linear ordering (encoded in a real number).},
  archive      = {J_NACO},
  author       = {Durand-Lose, Jérôme},
  doi          = {10.1007/s11047-024-10005-6},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {67-78},
  shortjournal = {Nat. Comput.},
  title        = {Abstract geometrical computation 12: Generating representation of infinite countable linear orderings},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sandpiles prediction and crossover on <span
class="math display">ℤ<sup>2</sup></span> within moore neighborhood.
<em>NACO</em>, <em>24</em>(1), 29–66. (<a
href="https://doi.org/10.1007/s11047-024-10002-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational complexity of predicting sandpiles on $$\mathbb {Z}^2$$ is not settled yet, neither for von Neumann nor for Moore neighborhood (is it in $${\textsf{NC}}$$ ? is it $${\textsf{P}}$$ -complete?). In this work we study the sandpile model considering all the 256 possible sub-neighborhoods within the Moore neighborhood. Surprisingly, we found that 12 of them have a $${\textsf{P}}$$ -complete prediction problem, while for the remaining 244 neighborhoods, we prove that they do not admit a crossover gate, i.e., for them, it is impossible to cross information, if the bit of information is the presence (or absence) of an avalanche.},
  archive      = {J_NACO},
  author       = {Concha-Vega, Pablo and Goles, Eric and Montealegre, Pedro and Perrot, Kévin},
  doi          = {10.1007/s11047-024-10002-9},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {29-66},
  shortjournal = {Nat. Comput.},
  title        = {Sandpiles prediction and crossover on $$\mathbb {Z}^2$$ within moore neighborhood},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extraction rates of algorithmically random continuous
functionals. <em>NACO</em>, <em>24</em>(1), 17–28. (<a
href="https://doi.org/10.1007/s11047-024-10000-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the extraction rate, or output/input rate, of continuous functionals on the Cantor space $$2^\omega$$ , in particular for algorithmically random functionals. It is shown that random functionals have an average extraction rate over all inputs corresponding to the rate of producing a single bit of output, and that this average rate is attained for any sufficiently random input. We also examine functionals computed by discrete distribution generating trees, where we calculate the expected extraction rate and show that this rate is attained for any sufficiently random input.},
  archive      = {J_NACO},
  author       = {Cenzer, Douglas and Fraize, Cameron and Porter, Christopher},
  doi          = {10.1007/s11047-024-10000-x},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {17-28},
  shortjournal = {Nat. Comput.},
  title        = {Extraction rates of algorithmically random continuous functionals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniform robot relocation is hard in only two directions even
without obstacles. <em>NACO</em>, <em>24</em>(1), 3–16. (<a
href="https://doi.org/10.1007/s11047-024-10007-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given n unit-sized robots contained within a square grid surrounded by four walls, we ask the question of whether it is possible to move a particular robot a to a specific grid location b by performing a sequence of global step operations in which all robots move one grid step in the same cardinal direction (if not blocked by a wall or other blocked robots). We show this problem is NP-complete when restricted to just two directions (south and west). This answers the simplest fundamental problem in uniform global unit tilt swarm robotics. We then consider a relaxed version of this problem called row relocation in which the goal is to move a robot a to a specific row regardless of its horizontal placement. We show that if asking about the first row of the square grid (bottom-most), then this version of the problem is solvable in polynomial time. Finally, we discuss several areas for future research and open problems.},
  archive      = {J_NACO},
  author       = {Caballero, David and Cantu, Angel A. and Gomez, Timothy and Luchsinger, Austin and Schweller, Robert and Wylie, Tim},
  doi          = {10.1007/s11047-024-10007-4},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {3-16},
  shortjournal = {Nat. Comput.},
  title        = {Uniform robot relocation is hard in only two directions even without obstacles},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preface. <em>NACO</em>, <em>24</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s11047-025-10012-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NACO},
  author       = {Genova, Daniela and Kari, Jarkko},
  doi          = {10.1007/s11047-025-10012-1},
  journal      = {Natural Computing},
  month        = {3},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Nat. Comput.},
  title        = {Preface},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="nca---28">NCA - 28</h2>
<ul>
<li><details>
<summary>
(2025). Retraction note: Research on mining collaborative behaviour
patterns of dynamic supply chain network from the perspective of big
data. <em>NCA</em>, <em>37</em>(10), 7457–7458. (<a
href="https://doi.org/10.1007/s00521-025-11023-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Leng, Kaijun and Jing, Linbo and Lin, I.-Ching and Chang, Sheng-Hung and Lam, Anthony},
  doi          = {10.1007/s00521-025-11023-w},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7457-7458},
  shortjournal = {Neural Comput. Appl.},
  title        = {Retraction note: Research on mining collaborative behaviour patterns of dynamic supply chain network from the perspective of big data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED ARTICLE: Deep learning-based algorithm for optimum
cluster head selection in sustainable wireless communication system.
<em>NCA</em>, <em>37</em>(10), 7455. (<a
href="https://doi.org/10.1007/s00521-023-08861-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Revanesh, M. and Mary, S. A. Sahaaya Arul and Gnaneswari, G. and Jones, G. Maria and Kanimozhi, K. V. and Kamalam, G. K.},
  doi          = {10.1007/s00521-023-08861-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7455},
  shortjournal = {Neural Comput. Appl.},
  title        = {RETRACTED ARTICLE: Deep learning-based algorithm for optimum cluster head selection in sustainable wireless communication system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED ARTICLE: Energy-efficient and sustainable
communication in optical networks for eliminating path reservation
criteria and providing guaranteed packet transmission between nodes.
<em>NCA</em>, <em>37</em>(10), 7453. (<a
href="https://doi.org/10.1007/s00521-023-08866-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Asha, P. and Kalaavathi, B. and Shantha Kumari, K. and Malarvizhi, K. and Kishore Kumar, A. and Sobitha Ahila, S.},
  doi          = {10.1007/s00521-023-08866-6},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7453},
  shortjournal = {Neural Comput. Appl.},
  title        = {RETRACTED ARTICLE: Energy-efficient and sustainable communication in optical networks for eliminating path reservation criteria and providing guaranteed packet transmission between nodes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED ARTICLE: Analysis of complex cognitive task and
pattern recognition using distributed patterns of EEG signals with
cognitive functions. <em>NCA</em>, <em>37</em>(10), 7451. (<a
href="https://doi.org/10.1007/s00521-020-05439-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Zhao, Jianyu and Li, Ke and Xi, Xi and Wang, Shanshan and Saravanan, Vijayalakshmi and Samuel, R. Dinesh Jackson},
  doi          = {10.1007/s00521-020-05439-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7451},
  shortjournal = {Neural Comput. Appl.},
  title        = {RETRACTED ARTICLE: Analysis of complex cognitive task and pattern recognition using distributed patterns of EEG signals with cognitive functions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Leveraging large language models for word sense
disambiguation. <em>NCA</em>, <em>37</em>(10), 7449–7450. (<a
href="https://doi.org/10.1007/s00521-025-11082-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Yae, Jung H. and Skelly, Nolan C. and Ranly, Neil C. and LaCasse, Phillip M.},
  doi          = {10.1007/s00521-025-11082-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7449-7450},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Leveraging large language models for word sense disambiguation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Securing IIoT operations with recurrent
federated network-based enhanced local search grasshopper. <em>NCA</em>,
<em>37</em>(10), 7447. (<a
href="https://doi.org/10.1007/s00521-024-10907-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Alassafi, Madini O.},
  doi          = {10.1007/s00521-024-10907-7},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7447},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Securing IIoT operations with recurrent federated network-based enhanced local search grasshopper},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Software effort estimation using convolutional
neural network and fuzzy clustering. <em>NCA</em>, <em>37</em>(10),
7445. (<a href="https://doi.org/10.1007/s00521-024-10906-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Azzeh, Mohammad and Alkhateeb, Abedalrhman and Nassif, Ali Bou},
  doi          = {10.1007/s00521-024-10906-8},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7445},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Software effort estimation using convolutional neural network and fuzzy clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: An artificial intelligence strategy for the
deployment of future microservice-based applications in 6G networks.
<em>NCA</em>, <em>37</em>(10), 7443. (<a
href="https://doi.org/10.1007/s00521-024-10754-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Ssemakula, John Bosco and Gorricho, Juan-Luis and Kibalya, Godfrey and Serrat-Fernandez, Joan},
  doi          = {10.1007/s00521-024-10754-6},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7443},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: An artificial intelligence strategy for the deployment of future microservice-based applications in 6G networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CONELPABO: Composite networks learning via parallel bayesian
optimization to predict remaining useful life in predictive maintenance.
<em>NCA</em>, <em>37</em>(10), 7423–7441. (<a
href="https://doi.org/10.1007/s00521-025-10995-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining equipment and machinery in industries is imperative for maximizing operational efficiency and prolonging their lifespan. The adoption of predictive maintenance enhances resource allocation, productivity, and product quality by proactively identifying and addressing potential equipment anomalies through rigorous data analysis before they escalate into critical issues. Consequently, these measures strengthen market competitiveness and generate favorable economic outcomes. In many applications, sensors operate at high frequencies or capture data over extended periods. This work introduces CONELPABO (Composite Networks Learning via Parallel Bayesian Optimization), a framework for analyzing long time series data, particularly for predicting the remaining useful life of a system or component. It uses a divide-and-conquer strategy to manage the exponential growth in the hyperparameter search space during Bayesian Optimization and to accelerate model training by 50%. Additionally, this strategy enables the training of deeper networks with limited resources. The usefulness of the framework is demonstrated through two case studies, in which it achieves state-of-the-art results, showing that CNN-CNN and RNN-RNN architectures are highly effective for long time-series data. These architectures outperform many existing approaches and challenge the common academic focus on CNN-RNN hybrids.},
  archive      = {J_NCA},
  author       = {Solís-Martín, David and Galán-Páez, Juan and Borrego-Díaz, Joaquín},
  doi          = {10.1007/s00521-025-10995-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7423-7441},
  shortjournal = {Neural Comput. Appl.},
  title        = {CONELPABO: Composite networks learning via parallel bayesian optimization to predict remaining useful life in predictive maintenance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic economic dispatch with uncertain wind power
generation using an enhanced artificial hummingbird algorithm.
<em>NCA</em>, <em>37</em>(10), 7397–7422. (<a
href="https://doi.org/10.1007/s00521-025-10982-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimum scheduling of the conventional thermal generators for three different dynamic test systems is percolated in this article. In pursuit of this objective, a developed version of a recent optimization algorithm, denoted as the leader artificial hummingbird algorithm, is introduced. The profile with the largest penetration of wind energy is obtained by calculating wind power from hourly wind speed using the Weibull distribution density function. After that, the test system and the wind profiles were connected to carry out dynamic economic dispatch (DED). The DED problem with wind uncertainty poses important challenges because of its complication, considered by multiple constraints including ramp rate limits and the valve-point effects (VPEs), nonconvexity, and nonlinearity, as well as the uncertainty of the wind energy. These complications make it critical to discover innovative optimization algorithms to find optimum solutions for the DED problem. First, in order to demonstrate the validity of the suggested LAHA approach in comparison with four contemporary techniques, simulations are run on 23 benchmark functions. Next, the 5-unit, 10-unit with/without transmission losses, 15-unit, modified 10-unit with transmission losses, and wind power test systems are used to evaluate the LAHA’s performance. The numerical results demonstrate how competitive the suggested approach is in reaching reduced total generation cost when compared to the other documented optimization algorithms.},
  archive      = {J_NCA},
  author       = {Hassan, Mohamed H. and Mohamed, Ehab Mahmoud and Kamel, Salah and Eslami, Mahdiyeh},
  doi          = {10.1007/s00521-025-10982-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7397-7422},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic economic dispatch with uncertain wind power generation using an enhanced artificial hummingbird algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoML for shape-writing biometrics. <em>NCA</em>,
<em>37</em>(10), 7379–7396. (<a
href="https://doi.org/10.1007/s00521-025-10983-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape-writing is a text entry method that allows users to type words on mobile devices by gliding their finger across the keyboard from one character to the next. This creates a trajectory of touch coordinates that contains rich information about the user. Previous work exploited this information to create Machine Learning (ML) models to predict demographic and behavioral targets, such as age, nationality, or handedness. However, previous work used pseudo-grid search, which is a bit tedious and rather inefficient. We show how to find better models with Automated Machine Learning (AutoML), by completely automating the architecture design process, outperforming all models reported in previous work. Our study suggests that researchers should incorporate AutoML to their training pipelines, as classification performance will likely be better than manually designing the model architecture. Taken together, our results show that it is possible to decode user’s latent information from shape-writing trajectories with higher performance than previously reported.},
  archive      = {J_NCA},
  author       = {Weber, Louis and Leiva, Luis A.},
  doi          = {10.1007/s00521-025-10983-3},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7379-7396},
  shortjournal = {Neural Comput. Appl.},
  title        = {AutoML for shape-writing biometrics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing facial expression recognition in uncontrolled
environment: A lightweight CNN approach with pre-processing.
<em>NCA</em>, <em>37</em>(10), 7363–7378. (<a
href="https://doi.org/10.1007/s00521-025-10974-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions play a key role in human non-verbal type of communication, providing key insights into emotions and intentions. These expressions serve as universal signals, helping individuals convey their internal states across various personal and social contexts. With the growing interest in automatic facial emotion recognition, deep neural networks have emerged as a popular approach for detecting human emotions, even under challenging, real-world conditions. However, external factors can affect the system&#39;s performance, degrading the quality of facial features and making emotion detection more difficult. In the presented paper, we propose a highly optimized lightweight convolutional neural network (LCNN) for emotion recognition in controlled and uncontrolled environments. The proposed model is designed to learn hidden nonlinear patterns from facial images. The proposed convolutional neural network consisting a series of convolutional layers followed by max-pooling layers. The model&#39;s performance is evaluated with and without pre-processing steps to highlight the importance of pre-processing in improving detection accuracy. The LCNN achieves 65% accuracy on the FER-2013 dataset and 98% on the CK + dataset.},
  archive      = {J_NCA},
  author       = {Grover, Richa and Bansal, Sandhya},
  doi          = {10.1007/s00521-025-10974-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7363-7378},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing facial expression recognition in uncontrolled environment: A lightweight CNN approach with pre-processing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing electroencephalogram signal quality in epileptic
patients using bidirectional stochastic long short-term memory network.
<em>NCA</em>, <em>37</em>(10), 7339–7361. (<a
href="https://doi.org/10.1007/s00521-025-10977-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artifacts frequently disrupt electroencephalogram (EEG) signal recordings, originating from diverse sources such as eye-blinks and muscle twitches. These artifacts present significant challenges when employing automated systems for diagnosing neurological disorders. In this research, we introduce an innovative architectural solution designed to effectively eliminate these artifacts from EEG signals acquired from individuals with epilepsy. Our proposed framework combines bidirectional long short-term memory networks with bidirectional stochastic configuration networks (BSCN). This integration empowers the model to discern intricate patterns within both past and future time steps of the EEG signal. Furthermore, the non-iterative training characteristic of the BSCN-based classifier enhances training efficiency. To assess the effectiveness of our approach, we conducted experiments on four epilepsy datasets and a sleep dataset. The performance of our novel technique was evaluated using a range of performance metrics, and the results unequivocally indicate its superiority over existing artifact removal methods.},
  archive      = {J_NCA},
  author       = {Pandey, Anviti and Singh, Sanjay Kumar and Udmale, Sandeep S. and Shukla, K. K.},
  doi          = {10.1007/s00521-025-10977-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7339-7361},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing electroencephalogram signal quality in epileptic patients using bidirectional stochastic long short-term memory network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel human actions recognition and classification using
semantic segmentation with deep learning techniques. <em>NCA</em>,
<em>37</em>(10), 7321–7337. (<a
href="https://doi.org/10.1007/s00521-024-10962-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel method for recognizing human actions through the semantic segmentation of images. The aim is to enhance action-motion dynamics by directing attention toward regions crucial for action recognition. The proposed approach utilizes a SegNet model with an incorporated attention mechanism and modified bidirectional gated recurrent unit (BiGRU) backbone. The process begins with the generation of binary masks for each frame in a video dataset, achieved through a combination of grayscale conversion, Gaussian blurring, and adaptive thresholding. The emphasis on crucial regions for action recognition and capturing temporal variations is heightened through the application of the frame-ranking method. In our experiments, we observed that the proposed method significantly enhances the dynamics of the action-motion representation. The SegNet architecture was designed for semantic segmentation tasks and features an encoder-decoder architecture. In this structure, the model performs hierarchical feature extraction from the input image via the encoder, whereas the decoder focuses on reconstructing the segmented output. Attention is paid to the encoded feature maps, augmenting the model&#39;s capability to capture dependencies over extensive spatial ranges. A bidirectional GRU layer is employed to capture the sequential dependencies in the concatenated feature maps. The integration of the SegNet model with the attention mechanism and a BiGRU backbone, featuring an encoder-decoder architecture for feature extraction, classification, and segmentation, demonstrated superior performance in capturing nuanced spatiotemporal features. The proposed method demonstrated an accuracy of 98.52% for UCF101 and 84.25% for HMDB51. The findings reveal that the model achieves state-of-the-art results in human action recognition tasks, outperforming the existing methods in terms of accuracy. The combination of semantic segmentation and BiGRU-based temporal modeling proved effective in discerning intricate patterns of human motion, showcasing its potential for real-world applications in video analysis and surveillance systems.},
  archive      = {J_NCA},
  author       = {Jayamohan, M. and Yuvaraj, S.},
  doi          = {10.1007/s00521-024-10962-0},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7321-7337},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel human actions recognition and classification using semantic segmentation with deep learning techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi-synchronization analysis of heterogeneous neural
networks with multiple delays under impulsive control. <em>NCA</em>,
<em>37</em>(10), 7303–7319. (<a
href="https://doi.org/10.1007/s00521-024-10948-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the quasi-synchronization of impulsive controlled heterogeneous dynamic neutral networks with time-varying delay, distributed delays and proportional delay is discussed. Compared with the existing literature, the significant advantage of this paper is that all three types of delays are taken into account. Here we consider time-varying delay depending on probability distribution conditions, so the results of this paper also rely on the problem of probability distribution of time-varying delay. By establishing a suitable comparison system, creating a new kind of impulsive delay inequality and applying Bernoulli distributions and Lyapunov theory, some conditions to realize quasi-synchronization of heterogeneous neural networks are studied. Finally we illustrate the validity of our theorem with numerical examples.},
  archive      = {J_NCA},
  author       = {Wang, Qing and Guo, Yingxin and Zhang, Chuan and Fu, Jianting},
  doi          = {10.1007/s00521-024-10948-y},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7303-7319},
  shortjournal = {Neural Comput. Appl.},
  title        = {Quasi-synchronization analysis of heterogeneous neural networks with multiple delays under impulsive control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced model for abstractive arabic text summarization
using natural language generation and named entity recognition.
<em>NCA</em>, <em>37</em>(10), 7279–7301. (<a
href="https://doi.org/10.1007/s00521-024-10949-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of Arabic digital content, effective summarization methods are essential. Current Arabic text summarization systems face challenges such as language complexity and vocabulary limitations. We introduce an innovative framework using Arabic Named Entity Recognition to enhance abstractive summarization, crucial for NLP applications like question answering and knowledge graph construction. Our model, based on natural language generation techniques, adapts to diverse datasets. It identifies key information, synthesizes it into coherent summaries, and ensures grammatical accuracy through deep learning. Evaluated on the EASC dataset, our model achieved a 74% ROUGE1 score and a 97.6% accuracy in semantic coherence, with high readability and relevance scores. This sets a new standard for Arabic text summarization, greatly improving NLP information processing.},
  archive      = {J_NCA},
  author       = {Essa, Nada and El-Gayar, M. M. and El-Daydamony, Eman M.},
  doi          = {10.1007/s00521-024-10949-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7279-7301},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced model for abstractive arabic text summarization using natural language generation and named entity recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-light few-shot object detection via curve contrast
enhancement and flow-encoder-based variational autoencoder.
<em>NCA</em>, <em>37</em>(10), 7261–7278. (<a
href="https://doi.org/10.1007/s00521-024-10885-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of insufficient samples in low-light object detection in some environments, a low-light few-shot object detection method based on curve contrast enhancement and flow-encoder-based variational autoencoder (CCEFVAE) is proposed. Our approach involves designing a CCE module to enhance the detailed features and contrast of low-light images by deriving a relationship expression between the enhanced image and the low-light image through the recursive relationship of high-order curves. The lumination estimation module in the CCE module estimates the parameters of the expression to calculate the pixel values of the enhanced image. Moreover, we propose an FVAE module to improve the decoupling of support features by combining the flow model encoder with the variational autoencoder, facilitating subsequent feature aggregation and classification. To ensure the consistency of the loss function of the flow model with the few-shot object detection loss, we design a negative Jacobian determinant transformation function. This enables direct addition of the two losses, allowing for unified optimization. Experimental results demonstrate that our proposed algorithm outperforms mainstream few-shot object detection models by an average of 13.1–23% in average after training on the low-light dataset (ExDark), and shows an average improvement of 5.8% compared to the state-of-the-art (SOTA) few-shot object detection model VFA. When trained on the normal lighting dataset (PASCAL VOC), the proposed algorithm exhibits a 1.7% improvement in average compared to VFA.},
  archive      = {J_NCA},
  author       = {Jiang, Zetao and Jin, Xin and Kang, Junjie},
  doi          = {10.1007/s00521-024-10885-w},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7261-7278},
  shortjournal = {Neural Comput. Appl.},
  title        = {Low-light few-shot object detection via curve contrast enhancement and flow-encoder-based variational autoencoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supercell thunderstorm algorithm (STA): A nature-inspired
metaheuristic algorithm for engineering optimization. <em>NCA</em>,
<em>37</em>(10), 7207–7260. (<a
href="https://doi.org/10.1007/s00521-024-10848-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an optimization algorithm called supercell thunderstorm algorithm (STA) is proposed. STA draws inspiration from the strategies employed by storms, such as spiral motion, tornado formation, and the jet stream. It is a computational algorithm specifically designed to simulate and model the behavior of supercell thunderstorms. These storms are known for their rotating updrafts, strong wind shear, and potential for generating tornadoes. The optimization procedures of the STA algorithm are based on three distinct approaches: exploring a divergent search space using spiral motion, exploiting a convergent search space through tornado formation, and navigating through the search space with the aid of the jet stream. To evaluate the effectiveness of the proposed STA algorithm in achieving optimal solutions for various optimization problems, a series of test sequences were conducted. Initially, the algorithm was tested on a set of 23 well-established functions. Subsequently, the algorithm’s performance was assessed on more complex problems, including ten CEC2019 test functions, in the second experimental sequence. Finally, the algorithm was applied to five real-world engineering problems to validate its effectiveness. The experimental results of the STA algorithm were compared to those of contemporary metaheuristic methods. The analysis clearly demonstrates that the developed STA algorithm outperforms other methods in terms of performance.},
  archive      = {J_NCA},
  author       = {Hassan, Mohamed H. and Kamel, Salah},
  doi          = {10.1007/s00521-024-10848-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7207-7260},
  shortjournal = {Neural Comput. Appl.},
  title        = {Supercell thunderstorm algorithm (STA): A nature-inspired metaheuristic algorithm for engineering optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond administrative reports: A deep learning framework for
classifying and monitoring crime and accidents leveraging large-scale
online news. <em>NCA</em>, <em>37</em>(10), 7183–7205. (<a
href="https://doi.org/10.1007/s00521-024-10833-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating prevalence of violent crimes and accidents underscores the urgent need for efficient and timely monitoring systems. Traditional methods reliant on administrative reports often suffer from significant delays. This paper proposes CRIMSON, a novel framework that leverages large-scale online news to provide real-time insights into crime and accident trends. CRIMSON utilizes a multi-label classification technique that leverages a fine-tuned, pre-trained, cross-lingual language model to accurately categorize news articles. Our experimental results, conducted on a substantial dataset of Thai news articles, demonstrate superior performance, achieving an average F1 score of 86%. Beyond classification, CRIMSON aggregates categorized news into real-time statistics, revealing strong correlations between news-reported incidents and official crime data. This study pioneers online news as a reliable and timely crime and accident monitoring source, offering valuable insights for law enforcement, policymakers, and researchers.},
  archive      = {J_NCA},
  author       = {Tuarob, Suppawong and Tatiyamaneekul, Phonarnun and Pongpaichet, Siripen and Tawichsri, Tanisa and Noraset, Thanapon},
  doi          = {10.1007/s00521-024-10833-8},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7183-7205},
  shortjournal = {Neural Comput. Appl.},
  title        = {Beyond administrative reports: A deep learning framework for classifying and monitoring crime and accidents leveraging large-scale online news},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GP-PSENet: A group-related dilated and a parallel
extensional dilation-wise residual encoder for scene text detection.
<em>NCA</em>, <em>37</em>(10), 7159–7181. (<a
href="https://doi.org/10.1007/s00521-024-10688-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, scene text detection is grabbing more and more attention as an offshoot of machine vision. However, due to the existing long types of text instances and complex background context, less exact localization and higher missed detection cases still remain in text detection domain. Accordingly, with the aim of tackling these two issues, we propose a text detector named GP-PSENet that comprises a combination of a group-related dilated encoder, a parallel extensional dilation-wise residual encoder and a mixed upsample. Firstly, feature maps of the lowest level processed by the backbone network are sent to a dilated encoder with group linkage. And the group residual module provides stratification to join group coefficients and dilated factors. This module can enhance the correctness of predictions about longer boundary boxes. Secondly, semantic information from the highest level is fed into a parallel extensional dilation-wise residual encoder. The extensional dilation-wise module is capable of obtaining diverse receptive fields by more parallel branches. And it can alleviate error detection from interfering material in the background. Thirdly, the feature maps processed in the second step are given to the mixed upsample module for transforming so as to the next fuse. Finally, the processed two-level feature maps are fused and sent to the progressive scale expansion algorithm for the final post-processing to gain the predicted coordinate points. Ablation experiments are conducted on CTW1500, ICDAR15, MSRA-TD500 and Total-Text datasets to confirm the availability of the proposed method. The values of precision on these datasets reach 86.24%, 87.84%, 73.98% and 90.48%. The proposed method is also competitive with other scene detection methods.},
  archive      = {J_NCA},
  author       = {Huang, Liwen and Yang, Wenyuan},
  doi          = {10.1007/s00521-024-10688-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7159-7181},
  shortjournal = {Neural Comput. Appl.},
  title        = {GP-PSENet: A group-related dilated and a parallel extensional dilation-wise residual encoder for scene text detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time optimal energy management of microgrid based on
multi-agent proximal policy optimization. <em>NCA</em>, <em>37</em>(10),
7145–7157. (<a
href="https://doi.org/10.1007/s00521-024-10654-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to achieve economic operation of the microgrid (MG), energy management problem (EMP) has attracted attention from scholars worldwide. In order to overcome the lack of flexibility when coping with uncertainties and topology changes, a multi-agent based proximal policy optimization algorithm (MAPPO) is proposed in this paper. Different from the offline training and online implementing mode, the proposed decentralized MAPPO algorithm has the characteristic of online training and online application, which can get higher optimization efficiency and lower communication burden. Taking into account users’ satisfaction, renewable energy utilization rate and operating costs, an optimization model is established. Aiming at the difficulty on satisfying the power balance constraint in EMPU using reinforcement learning (RL), a novel power imbalance penalty is designed. Compared with the traditional penalty function, the proposed penalty function can effectively avoid the phenomenon of power imbalance. Finally, 24-hour energy management results are provided to verify the effectiveness of the proposed algorithm. Moreover, the proposed MAPPO is compared with several popular multi-agent based RL algorithms. Simulation results show that the proposed algorithm has higher efficiency and can obtain better energy management strategies.},
  archive      = {J_NCA},
  author       = {Wang, Danlu and Sun, Qiuye and Su, Hanguang},
  doi          = {10.1007/s00521-024-10654-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7145-7157},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time optimal energy management of microgrid based on multi-agent proximal policy optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal consistent loss diffusion model for sentinel-3
single image super resolution. <em>NCA</em>, <em>37</em>(10), 7121–7143.
(<a href="https://doi.org/10.1007/s00521-024-10573-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of Earth observation, the trade-off between spatial, spectral, and temporal resolution often limits the versatility of remote sensing images in many important applications. In response, this paper introduces a novel deep learning diffusion model, specifically tailored to improve the spatial resolution of the optical products acquired by the Sentinel-3 (S3) satellite. Our framework employs a diffusion probabilistic model, benefiting from the higher spatial resolution of the Sentinel-2 satellite during training via a new multi-modal loss formulation. This ensures consistency with the original S3 images while enhancing the spatial details. Two distinct conditional low-resolution encoders were experimented with, providing insights into their respective contributions to the diffusion process. The efficacy of the proposed model is demonstrated through extensive ablation studies and comparisons with state-of-the-art methods, using both synthetic and real S3 products. The findings indicate that our model successfully improves spatial resolution while maintaining the integrity of the spectral information, contributing to the field of remote sensing single-image super-resolution.},
  archive      = {J_NCA},
  author       = {Ibañez, Damian and Fernandez-Beltran, Ruben and Pla, Filiberto and Yokoya, Naoto and Xia, Junshi},
  doi          = {10.1007/s00521-024-10573-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7121-7143},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-modal consistent loss diffusion model for sentinel-3 single image super resolution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic generative r-CNN. <em>NCA</em>, <em>37</em>(10),
7107–7120. (<a
href="https://doi.org/10.1007/s00521-024-10739-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different techniques have been developed for object detection and recognition. These techniques can be divided into single-shot and two-shot methods. Single-shot methods focus on real-time applications, while two-shot methods are used in applications requiring higher accuracy. However, different versions of the two-shot techniques produce limited results in terms of accuracy and speed, or both. Therefore, this study proposes a novel model called dynamic generative R-CNN (DGR-CNN) that reduces the number of proposed regions using a dynamic programming model that applies the graph similarity method over graph-based image segmentation. Additionally, the proposed model employs DCGAN technique to improve detection performance. DGR-CNN reduces the overall detection and classification time and enhances the detection accuracy. The PASCAL VOC2007 and MS COCO datasets were utilized to evaluate the model. The results showed that DGR-CNN significantly reduces the number of candidate regions compared to the selective search algorithm employed in R-CNN and fast R-CNN. Although fast R-CNN utilizes 2000 regions and faster R-CNN utilizes 300 regions, DGR-CNN reduces the number of regions to approximately 130. The mean average precision of the proposed method was 75.1% on the PASCAL VOC2007, while fast and faster R-CNN scored 66.9% and 69.9%, respectively. Moreover, the DGR-CNN model significantly improved the classification accuracy when tested on the MS COCO dataset, achieving an MAP of 68.76%, compared with 32.64% and 42.3% for fast and faster R-CNN. This increase in accuracy was achieved without significantly compromising the speed compared with faster R-CNN.},
  archive      = {J_NCA},
  author       = {Saffarini, Rasha and Khamayseh, Faisal and Awwad, Yousef and Sabha, Muath and Eleyan, Derar},
  doi          = {10.1007/s00521-024-10739-5},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7107-7120},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic generative R-CNN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing a continuous action learning automata (CALA)
optimizer for training artificial neural networks. <em>NCA</em>,
<em>37</em>(10), 7089–7105. (<a
href="https://doi.org/10.1007/s00521-024-10546-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep artificial neural networks (ANNs) get bigger, deeper, and used in more challenging applications, the need for non-gradient based training methods becomes more desirable. This paper explores a new non-gradient-based method to train ANNs and deep ANNs, the Continuous Action Learning Automata (CALA) optimizer. The CALA optimizer assigns a Learning Automata agent to every weight in a neural network and uses game theory to coordinate actions of the agents. We show that the CALA optimizer is computationally efficient, that it converges to a desired error rate faster than current gradient-based methods like stochastic gradient descent (SGD) and show how one could use a Finite Action Learning Automata (FALA) algorithm to find optimal values for the hyper-parameters required to optimize the CALA controller. The CALA method contrasts itself against other non-gradient methods in that it approaches the computational efficiency of top gradient descent methods like SGD. The CALA method converges fast, and there is any easy-to-follow algorithm to tune the hyper-parameters of the algorithm. These advantages address weaknesses that other non-gradient methods suffer from. Therefore, the CALA controller has the potential to see far greater implementation than other non-gradient-based optimization methods for training deep ANNs.},
  archive      = {J_NCA},
  author       = {Lindsay, James and Givigi, Sidney},
  doi          = {10.1007/s00521-024-10546-y},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7089-7105},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing a continuous action learning automata (CALA) optimizer for training artificial neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive dual-weighted feature network for insulator
detection in transmission lines. <em>NCA</em>, <em>37</em>(10),
7067–7087. (<a
href="https://doi.org/10.1007/s00521-024-10957-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of electrical power applications, high-voltage insulators necessitate routine inspection to assure the security and stability of the whole electric power system operation. Accurately positioning the insulator is extremely crucial for proceeding to the insulator defect detection. However, during UAV electrical line inspection, the presence of the electric power line magnetic field engenders a reduction in the pixel representation of the insulator within the image data, thereby diminishing the accuracy of insulator detection. In response to the prevailing issues, we present the creation of the adaptive dual-weighted feature network in this paper. Simultaneously, we create an insulator dataset to substantiate the effectiveness of enhanced model in detecting small insulators. Firstly, the integration of context fusion network is employed to capture comprehensive contextual features for each effective feature map. In addition, a cross-scale residual perception network is incorporated into the neck prior to three concatenation modules, facilitating the collection of diverse information across levels. Finally, a Dual-Weighted Feature Fusion module is designed to replace the conventional concatenation pattern within the neck, thus achieving a more precise representation of object features. Experiments are conducted on the insulator dataset, the RSOD dataset and the NWPU VHR-10 dataset to evaluate the designed model, resulting in mAP values that were 3.92%, 1.55% and 2.39% higher than the YOLOv7, respectively.},
  archive      = {J_NCA},
  author       = {Zhang, Jie and Wang, Xiabing and Li, Yinhua and Li, Dailin and Wang, Fengxian and Li, Linwei and Zhang, Huanlong and Shi, Xiaoping},
  doi          = {10.1007/s00521-024-10957-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7067-7087},
  shortjournal = {Neural Comput. Appl.},
  title        = {An adaptive dual-weighted feature network for insulator detection in transmission lines},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward better semantic segmentation by retaining spectral
information using matched wavelet pooling. <em>NCA</em>,
<em>37</em>(10), 7049–7066. (<a
href="https://doi.org/10.1007/s00521-025-11008-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling operations, such as average pooling, strided convolution, and max pooling, have become fundamental components of convolutional neural networks (CNNs) due to their ability to capture local features, expand receptive fields, and reduce computational costs. However, in the context of semantic segmentation, these pooling techniques can lead to the loss of crucial spatial details that are necessary for accurate pixel-level predictions. To tackle this issue, extensive research has focused on refining deep CNN models through architectural adaptations and novel training methods. Recent studies have demonstrated the importance of pooling layers, exemplified by innovations like the introduction of wavelet pooling. In our study, we highlight the value of incorporating our previously proposed matched wavelet pooling (MWP) into CNNs to enhance semantic segmentation pipelines. The core concept of MWP challenges the notion that including all sub-bands generated from wavelet decomposition consistently improves accuracy. Instead, we advocate for selecting specific sub-bands for the pooling process in each image during both training and testing. This approach introduces sub-band selection protocols customized for image-specific pooling, designed specifically for semantic segmentation CNN architectures, with a particular focus on the UNet and SegNet models. Across three widely used datasets, our proposed MWP- based pipeline, featuring the MWP-UNet architecture, consistently outperforms conventional pooling methods. It achieves a significant average improvement in intersection over union (IoU) of over 25% compared to recent literature. Additionally, our MWP-SegNet model outperformed the standard SegNet by 12.5% mIoU, further demonstrating the effectiveness of our matched wavelet pooling approach across different network architectures.},
  archive      = {J_NCA},
  author       = {El-Khamy, Said and El-Bana, Shimaa and Al-Kabbany, Ahmad and Elragal, Hassan},
  doi          = {10.1007/s00521-025-11008-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7049-7066},
  shortjournal = {Neural Comput. Appl.},
  title        = {Toward better semantic segmentation by retaining spectral information using matched wavelet pooling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on arabic text augmentation:
Approaches, challenges, and applications. <em>NCA</em>, <em>37</em>(10),
7015–7048. (<a
href="https://doi.org/10.1007/s00521-025-11020-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arabic is a linguistically complex language with a rich structure and valuable syntax that pose unique challenges for natural language processing (NLP), primarily due to the scarcity of large, reliable annotated datasets essential for training models. The varieties of dialects and mixtures of more than one language within a single conversation further complicate the development and efficacy of deep learning models targeting Arabic. Data augmentation (DA) techniques have emerged as a promising solution to tackle data scarcity and improve model performance. However, implementing DA in Arabic NLP presents its challenges, particularly in maintaining semantic integrity and adapting to the language’s intricate morphological structure. This survey comprehensively examines various aspects of Arabic data augmentation techniques, covering strategies for model training, methods for evaluating augmentation performance, understanding the effects and applications of augmentation on data, studying NLP downstream tasks, addressing augmentation problems, proposing solutions, conducting in-depth literature reviews, and drawing conclusions. Through detailed analysis of 75 primary and 9 secondary papers, we categorize DA methods into diversity enhancement, resampling, and secondary approaches, each targeting specific challenges inherent in augmenting Arabic datasets. The goal is to offer insights into DA effectiveness, identify research gaps, and suggest future directions for advancing NLP in Arabic.},
  archive      = {J_NCA},
  author       = {ElSabagh, Ahmed Adel and Azab, Shahira Shaaban and Hefny, Hesham Ahmed},
  doi          = {10.1007/s00521-025-11020-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7015-7048},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comprehensive survey on arabic text augmentation: Approaches, challenges, and applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding silent speech: A machine learning perspective on
data, methods, and frameworks. <em>NCA</em>, <em>37</em>(10), 6995–7013.
(<a href="https://doi.org/10.1007/s00521-024-10456-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the nexus of signal processing and machine learning (ML), silent speech recognition (SSR) has evolved as a game-changing technology that allows for communication without audible voice. This study offers a thorough overview of SSR, tracing its evolution from early waveform analysis to the most recent ML methods. We start by examining current SSR techniques using ML and determining the essential conditions for efficient SSR systems. After that, we look at the datasets and data collection techniques currently employed in SSR research, highlighting the difficulties posed by the variety of articulatory movements and the scarcity of data. Examining state-of-the-art SSR frameworks, the paper covers important topics such signal processing, feature extraction, ML techniques for decoding and optimizing and assessing the performance of SSR models. We emphasize how deep learning (DL) and ML models have evolved to increase SSR resilience and accuracy. The field&#39;s proposed procedures are examined, with an emphasis on sophisticated feature extraction and classification methods. Modern SSR techniques are compared in terms of performance, highlighting the advantages and disadvantages of different models. There is also discussion of ethical issues, especially those pertaining to privacy and consent. The integration of multimodal information—visual cues, electromyography signals, and neuroimaging data—to improve SSR systems is covered in this work. We investigate the functions of transfer learning and domain adaptation in handling cross-subject variability. Lastly, the study offers suggestions and future prospects for SSR research, providing practitioners, engineers, and academics with a road map. As SSR continues to push the frontiers of human–machine interaction, our study aims to increase our collective understanding of the technological advances and societal effects of SSR in the ML age.},
  archive      = {J_NCA},
  author       = {Chowdhury, Adiba Tabassum and Newaz, Mehrin and Saha, Purnata and AbuHaweeleh, Mohannad Natheef and Mohsen, Sara and Bushnaq, Diala and Chabbouh, Malek and Aljindi, Raghad and Pedersen, Shona and Chowdhury, Muhammad E. H.},
  doi          = {10.1007/s00521-024-10456-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {6995-7013},
  shortjournal = {Neural Comput. Appl.},
  title        = {Decoding silent speech: A machine learning perspective on data, methods, and frameworks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="npl---22">NPL - 22</h2>
<ul>
<li><details>
<summary>
(2025). HGBL: A fine granular hierarchical multi-label text
classification model. <em>NPL</em>, <em>57</em>(1), 1–28. (<a
href="https://doi.org/10.1007/s11063-024-11713-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical multi-label text classification is vital for natural language processing (NLP). However, existing research rarely makes full use of the interaction between labels and text features that are crucial to hierarchical multi-label text classification. To address this issue, a novel model named hierarchy-guided BiLSTM guided contrastive learning classification (HGBL) is proposed, which successfully enhances the interaction between labels and text features by incorporating global context and embedding the idea of contrastive learning into this model. During modeling, Graphormer is adopted to model the dependencies between labels, and the bidirectional recurrent network (BiLSTM) is used to integrate global context including label features. Afterwards, the contrastive learning module embeds hierarchical awareness into the fine-tuned bidirectional encoder representations from transformers (BERT) by training the value of the loss. Experimental results on NYT, WOS and RCV1-V2 datasets show that HGBL exhibits significant competitive advantages compared with 19 competitors in terms of several indicators and can be used effectively for hierarchical multi-label text classification problems.},
  archive      = {J_NPL},
  author       = {Zhang, Chaoqun and Dai, Linlin and Liu, Chengxing and Zhang, Longhao},
  doi          = {10.1007/s11063-024-11713-x},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Neural Process. Lett.},
  title        = {HGBL: A fine granular hierarchical multi-label text classification model},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual learning in medicine: A systematic literature
review. <em>NPL</em>, <em>57</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11063-024-11709-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual Learning (CL) is a novel AI paradigm in which tasks and data are made available over time; thus, the trained model is computed on the basis of a stream of data. CL-based approaches are able to learn new skills and knowledge without forgetting the previous ones, with no guaranteed access to previously encountered data, and mitigating the so-called “catastrophic forgetting” phenomenon. Interestingly, by making AI systems able to learn and improve over time without the need for large amounts of new data or computational resources, CL can help at reducing the impact of computationally-expensive and energy-intensive activities; hence, CL can play a key role in the path towards more green AIs, enabling more efficient and sustainable uses of resources. In this work, we describe different methods proposed in the literature to solve CL tasks; we survey different applications, highlighting strengths and weaknesses, with a particular focus on the biomedical context. Furthermore, we discuss how to make the methods more robust and suitable for a wider range of applications.},
  archive      = {J_NPL},
  author       = {Bruno, Pierangela and Quarta, Alessandro and Calimeri, Francesco},
  doi          = {10.1007/s11063-024-11709-7},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Neural Process. Lett.},
  title        = {Continual learning in medicine: A systematic literature review},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LPM-net: A data-driven resource-efficient predictive motion
planner for mobile robots. <em>NPL</em>, <em>57</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s11063-024-11671-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-driven predictive motion planner for mobile robots, referred to as LPM-Net, has been proposed in this paper. Conventional predictive motion planners are computationally expensive, often resulting in insufficient throughput on mobile robot hardware. LPM-Net is an imitation learning-assisted local predictive non-holonomic motion planner that is capable of learning from conventional motion planners regarded as paradigm models and replicating their behavior while satisfying the same kinodynamic constraints. In addition, LPM-Net is compatible with GPU and TPU hardware, allowing for faster and more efficient processing. LPM-Net uses convolutional and recurrent long short-term memory deep neural networks to predict steering commands. This has improved computational efficiency which allows autonomous vehicles to be equipped with more cost-effective computers. In the present study, LPM-Net was tuned to mimic the behavior of a model predictive controller paradigm model. Measurements in this study demonstrate that the proposed mimic planner, LPM-Net, consumes approximately half the processing power of the conventional predictive planner, albeit with a slight increase in hesitation when reaching goals.},
  archive      = {J_NPL},
  author       = {Amirhosseini, Fakhreddin and Nilforoushan, Zahra and Leili Mirtaheri, Seyedeh},
  doi          = {10.1007/s11063-024-11671-4},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Neural Process. Lett.},
  title        = {LPM-net: A data-driven resource-efficient predictive motion planner for mobile robots},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LieCConv: An image classification algorithm based on lie
group convolutional neural network. <em>NPL</em>, <em>57</em>(1), 1–21.
(<a href="https://doi.org/10.1007/s11063-024-11691-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Lie group convolutional neural networks (LG-CNNs), the calculation and storage of Lie group distances have quadratic space complexity. In order to improve the memory utilization efficiency of LG-CNNs, a novel Lie group convolutional neural network called LieCConv is proposed. LieCConv utilizes an innovative sampling algorithm and a linear space complexity calculation and storage approach for Lie group distances, substantially enhancing network memory efficiency. Firstly, LieCConv employs a novel sampling algorithm called array-neighborhood sampling (ANS) in the downsampling stage. ANS only requires neighborhood information to obtain an excellent sample set with a low threshold of use. The sample set generated by ANS reflects the distribution of the original set. Then, LieCConv adopts a batch calculation and storage scheme for Lie group distances, which effectively declines the space complexity of calculating and storing Lie group distances from quadratic complexity to linear complexity, reducing the memory consumption during training. Finally, the contrast between ANS and farthest point sampling was presented, demonstrating that ANS better captures the distribution characteristics of the original dataset. The memory usage of LieCConv and LieConv was compared, revealing that LieCConv reduces the memory usage for calculating and storing Lie group distances to less than 500 MB. And the performance of LieCConv was evaluated on RotMNIST, RotFashionMNIST and TT100K, validating that LieCConv is universal and effective.},
  archive      = {J_NPL},
  author       = {Zhang, Yunjie and Luo, Xizhao and Tao, Chongben and Qin, Bo and Yang, Anjia and Cao, Feng},
  doi          = {10.1007/s11063-024-11691-0},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Neural Process. Lett.},
  title        = {LieCConv: An image classification algorithm based on lie group convolutional neural network},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harborfront anomaly detection. <em>NPL</em>, <em>57</em>(1),
1–16. (<a href="https://doi.org/10.1007/s11063-024-11696-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating high-quality datasets for the task of video anomaly detection is challenging due to a subjective anomaly definition and the rarity of anomalies, which oust the possibility of obtaining statistically significant data. This results in datasets where anomalies are placed in a single category, and are often considered less relevant from a security standpoint. Instead, we propose to create video anomaly datasets based on a framework utilizing object annotations to ease the annotation process and allow users to decide on the anomaly definition. Furthermore, this allows for a fine-grained evaluation w.r.t. anomaly types, which represents a novelty in the area of video anomaly detection. The framework is demonstrated using the existing thermal long-term drift (LTD) dataset, identifying and evaluating five different types of anomalies (appearance, motion, localization, density, and tampering) on six test sets. State-of-the-art anomaly detection methods are evaluated and found to underperform on the thermal anomaly detection dataset, which emphasizes a need for an adjustable anomaly definition in order to produce better anomaly datasets and models that generalize towards practical use. We share the code of the proposed framework to extract anomaly types along with object annotations for the LTD dataset at https://github.com/jagob/harborfront-vad .},
  archive      = {J_NPL},
  author       = {Dueholm, Jacob V. and Siemon, Mia and Ionescu, Radu T. and Moeslund, Thomas B. and Nasrollahi, Kamal},
  doi          = {10.1007/s11063-024-11696-9},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Neural Process. Lett.},
  title        = {Harborfront anomaly detection},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring dual coupledness for effective pruning in object
detection. <em>NPL</em>, <em>57</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s11063-024-11697-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning offers an efficient approach to compressing models deployed on resource-constrained devices. In this paper, we introduce a novel method called Dual-Coupledness Object Detection Pruning (DCODP), specifically designed for object detection models. Taking into account the complexity of model coupling, our algorithm utilizes a depth-first search approach to identify interlayer coupling within the model. It then groups sublayers with the same parent layer together. Filters corresponding to feature maps with strong coupling are pruned within the layer, and the same pruning operation is applied to the corresponding indices in other coupled layers. In order to prove the validity of our method, extensive experiments are conducted on PASCAL VOC2007, PASCAL VOC2012 and MS COCO2017. The results show that our DCODP achieves a significant reduction of 50% in parameters and an average of more than 70% impressive score.},
  archive      = {J_NPL},
  author       = {Xiaohui, Guan and Wenzhuo, Huang and Yaguan, Qian and Xinxin, Sun},
  doi          = {10.1007/s11063-024-11697-8},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {Exploring dual coupledness for effective pruning in object detection},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixed-time synchronization of caputo/conformable
fractional-order inertial cohen-grossberg neural networks via
event-triggered one/two-phase hybrid impulsive control. <em>NPL</em>,
<em>57</em>(1), 1–57. (<a
href="https://doi.org/10.1007/s11063-024-11703-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the fixed-time synchronization (FXS) problems of Caputo/conformable fractional-order inertial Cohen-Grossberg neural networks (FOICGNNs) by one/two-phase hybrid impulsive control (HIC) through event-triggered update strategies. By utilizing the properties of fractional calculus, several novel inequalities regarding the fixed-time convergence of hybrid impulsive systems (HIS) are obtained. We especially discuss and compare the cases of Caputo and conformable fractional order to gain deep insight into fractional calculus. By applying the Lyapunov stability theory, two hybrid controllers, which consist of event-triggered continuous controllers and impulsive controllers, are designed to realize the FXS of FOICGNNs. It’s worth pointing out that, we unprecedentedly study and compare the differences of the one-phase HIC and two-phase HIC, where a novel nonlinear impulsive controller is proposed and designed to obtain fixed-time convergence in the impulsive control phase. In addition, the exclusion of Zeno behavior is proved for the designed event-triggered strategy. Finally, several numerical examples are provided to illustrate the feasibility of the proposed control approach and the correctness of the theoretical results.},
  archive      = {J_NPL},
  author       = {Xiong, Yao and Li, Yesheng and Lv, Haifei and Wu, Wei and Xie, Songhua and Chen, Mengwei and Hu, Changkui and Li, Min},
  doi          = {10.1007/s11063-024-11703-z},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-57},
  shortjournal = {Neural Process. Lett.},
  title        = {Fixed-time synchronization of Caputo/Conformable fractional-order inertial cohen-grossberg neural networks via event-triggered One/Two-phase hybrid impulsive control},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network clustering for multi-task learning. <em>NPL</em>,
<em>57</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s11063-024-11712-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multi-Task Learning (MTL) technique has been widely studied by worldwide researchers. The majority of current MTL studies adopt the hard parameter sharing structure, where hard layers tend to learn general representations over all tasks and specific layers are prone to learn specific representations for each task. Since the specific layers directly follow the hard layers, the MTL model needs to estimate this direct change (from general to specific) as well. To alleviate this problem, we introduce the novel cluster layer, which groups tasks into clusters during training procedures. In a cluster layer, the tasks in the same cluster are further required to share the same network. By this way, the cluster layer produces the general presentation for the same cluster, while produces relatively specific presentations for different clusters. The cluster layers are used as transitions between the hard layers and the specific layers. Thus, the MTL model can learn general representations to specific representations gradually. We evaluate our model with MTL document classification, and the results demonstrate the cluster layer is quite efficient in MTL.},
  archive      = {J_NPL},
  author       = {Mu, Zhiying and Gao, Dehong and Guo, Sensen},
  doi          = {10.1007/s11063-024-11712-y},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Neural Process. Lett.},
  title        = {Network clustering for multi-task learning},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly learning type-aware relations and inter-aspect with
graph convolutional networks for aspect sentiment analysis.
<em>NPL</em>, <em>57</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11063-024-11715-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for aspect-level sentiment analysis by leveraging the relationships between dependent types and aspects. The proposed method involves simplifying the type-aware graph convolutional network and designing a graph convolution module specifically for extracting relations between aspect words. The process begins with constructing an ordinary dependency graph for each sentence using a dependency tree. This graph is then refined by considering syntactic dependencies between context words and aspect-specific words, resulting in an aspect-focused graph. The aspect-focused graph, along with the corresponding embedding matrices, is fed into the aspect-focused GCN to capture the essential aspects and context words. Moreover, an inter-aspect GCN is employed to extract the dependencies between aspect words and other aspect words, utilizing the representations learned by the focused aspect GCN based on the inter-aspect graph. The L-layer of the GCN incorporates a bidirectional attentional mechanism to extract interrelationships, thus enhancing sentiment polarity judgment. Through interactive learning of aspect-specific affective features, the model acquires an understanding of the relationships between important text and aspect words, as well as the relationships among aspect words. Experimental results on five benchmark datasets demonstrate the superior performance of our proposed method compared to state-of-the-art approaches, exhibiting a significant improvement over the regular GCN model.},
  archive      = {J_NPL},
  author       = {Zong, Liansong and Hu, Dongfeng and Gui, Qingchi and Zhang, Pengfei and Wang, Jie},
  doi          = {10.1007/s11063-024-11715-9},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Neural Process. Lett.},
  title        = {Jointly learning type-aware relations and inter-aspect with graph convolutional networks for aspect sentiment analysis},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic evaluation of english translation based on
multi-granularity interaction fusion. <em>NPL</em>, <em>57</em>(1),
1–17. (<a href="https://doi.org/10.1007/s11063-025-11716-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest neural machine translation automatic evaluation method uses pre-trained context word vectors to extract semantic features and directly concatenates them into the neural network to predict translation quality. However, the direct operation can easily lead to a lack of interaction between features, and the layer-by-layer prediction is prone to losing fine-grained matching information. To address these issues, we propose a multi-granularity interactive fusion English translation automatic evaluation, which introduces middle and late information fusion methods. First, we use a bilinear attention distribution to capture high-order cross language feature interactions. By stacking multiple high-order interaction blocks and equipping them with an index linear unit without parameters for middle fusion in a parameter-free manner. Second, we use fine-grained accurate matching sentence shift distance and sentence-level cosine similarity for late fusion. The experimental results on the WMT’21 Metrics Task benchmark dataset show that the proposed method can effectively improve its correlation with human evaluation and achieve comparable performance with the best participating system.},
  archive      = {J_NPL},
  author       = {Chen, Xibo and Yang, Yonghe and Hu, Haize},
  doi          = {10.1007/s11063-025-11716-2},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Neural Process. Lett.},
  title        = {Automatic evaluation of english translation based on multi-granularity interaction fusion},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature enhancement-based few-shot bearing surface defect
image classification method. <em>NPL</em>, <em>57</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11063-025-11720-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of intelligent bearing surface defect classification based on deep neural networks remains challenging in real factories, due to the scarcity of defect samples. Under real-working conditions, with less training samples, the paper proposes a few-shot bearing defect image classification network which can recognize different bearing surface defects image, including notch, reddish rust, scratching, incising, conformity, pitting and mill scale. Based on general metric learning neural network framework, a local feature extraction layer is designed, which calculates the auto-correlation vector of global feature in a sliding region to enhance detail features. Additionally, a similar feature attention module emphasizes the the regions of similarity between the query set and the class prototype center to overcome the influence of background noise on classification. To validate the effectiveness of the proposed network, comparative experiments were conducted using the benchmark dataset miniImageNet, achieving classification accuracies of 59% in the 5-way 1-shot setting and 76% in the 5-way 5-shot setting respectively. Furthermore, to assess its performance in a real-factory condition, a self-made dataset of bearing defects from a factory was employed. The proposed network achieved a remarkable classification accuracy of 88% in the 5-way 5-shot setting. These experimental results confirm the practical application value of our few-shot bearing surface defect image classification network, demonstrating its ability to accurately recognize various bearing defects with limited training samples.},
  archive      = {J_NPL},
  author       = {Cang, Yan and Zhang, Xuanshang},
  doi          = {10.1007/s11063-025-11720-6},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {Feature enhancement-based few-shot bearing surface defect image classification method},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional feature interaction for conversational
aspect-based quadruple sentiment analysis. <em>NPL</em>, <em>57</em>(1),
1–19. (<a href="https://doi.org/10.1007/s11063-025-11721-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational aspect-level quadruple sentiment analysis (DiaASQ) is proposed as a new task that aims to extract target-aspect-opinion-sentiment quadruples in dialogues. However, this task faces the problem of complex context matching and multiple utterance feature modeling, which creates difficulties in extracting quadruples from multiple intersecting utterances. To address this problem, this paper proposes a Multi-dimensional Dialogue Feature Interaction (MDFI) approach. This method models dialogue features through an interactive network structure to capture interactions between utterance features. The approach adds two layers of ResNet to achieve deep association fusion based on multi-head self-attention. It superimposes the associated features of replies, speakers, and dialogue threads layer by layer and enhances the capability of conversation representation through linear augmentation. Our model outperforms the DiaASQ benchmark model in global utterance, intra-utterance, and cross-utterance quadruple extraction. In particular, the ZH dataset shows an improvement of 7.42 in global utterance and 9.66 in cross-utterance.},
  archive      = {J_NPL},
  author       = {Zhao, Zhongyang and Zhang, Long and Zheng, Qiusheng and Zhang, Junshuai},
  doi          = {10.1007/s11063-025-11721-5},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Neural Process. Lett.},
  title        = {Multi-dimensional feature interaction for conversational aspect-based quadruple sentiment analysis},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Value creation for healthcare ecosystems through artificial
intelligence applied to physician-to-physician communication: A
systematic review. <em>NPL</em>, <em>57</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s11063-025-11725-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study reviews the role of artificial intelligence (AI) in enhancing healthcare through an analysis of physician-to-physician communication. It seeks to identify the best practices for extracting value from professional medical chats (PMCs) and assess the impact of AI on patient outcomes and healthcare systems, emphasizing the integration of ethical and responsible AI practices. We conducted an extensive systematic literature review using the Web of Science Core Collection. Searches encompassed English-language articles published between January 2019 and July 2023 using keywords related to AI, machine learning, natural language processing, and physician communication. Of the 247 articles screened, 13 met the inclusion criteria given their in-depth analysis of AI in healthcare communication, methodological soundness, and relevance to clinical outcomes. The review provides insights into interprofessional communication dynamics, the advancement of NLP and deep learning in medical dialogues, and strategies for effective human-machine collaboration. Ethical considerations and the need for transparency in AI applications are key to these central findings. This study highlights the untapped potential of physician-generated real-world data in creating value for healthcare ecosystems. It advocates for a multidisciplinary strategy encompassing communication, education, and collaboration to advance AI in healthcare responsibly. Moreover, it suggests that by combining existing techniques in the AI discipline, including neural networks, generative AI, and genetic algorithms, as well as keeping a “physician in the loop” when building AI systems, we can have a significant impact on healthcare delivery and medical research.},
  archive      = {J_NPL},
  author       = {Rubinstein, Beny and Matos, Sergio},
  doi          = {10.1007/s11063-025-11725-1},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Neural Process. Lett.},
  title        = {Value creation for healthcare ecosystems through artificial intelligence applied to physician-to-physician communication: A systematic review},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AUMEs: AU detection-based dual-stream multi-task 3DCNN for
micro-expression recognition. <em>NPL</em>, <em>57</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s11063-025-11726-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions are brief, involuntary facial movements that can reveal real emotions. However, their short duration and low intensity pose a challenge for feature extraction and learning of neural networks. To overcome this challenge, we propose AUMEs, a 3DCNN-based multi-task learning framework that utilizes deep learning-based Lagrangian motion magnification and optical flow computation methods to enhance spatio-temporal features of micro-expressions, thus solving the problem of weak micro-expression motion intensity. AUMEs also use AU detection as a parallel task to improve the accuracy of micro-expression recognition by transferring knowledge from the AU detection task, and focal loss is utilized in model training to handle category imbalance in the micro-expression dataset. AUMEs achieve competitive results compared with existing SOTA methods on the CASMEII and SAMM datasets, achieving accuracy (Acc.) of 81.05% and 79.85%, UF1 score reaches 0.8880 and 0.7450 on the five-category task, and on the three-category UAR reached 89.02% and 75.86% and 0.8880 and 0.7450 for UF1. Furthermore, in both dataset analyses, the multi-task approach surpassed the single-task method across both the five-category and three-category classifications.},
  archive      = {J_NPL},
  author       = {Shi, Hu and Wang, Yanxia and Wang , Renjie and Liu, Dan},
  doi          = {10.1007/s11063-025-11726-0},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Neural Process. Lett.},
  title        = {AUMEs: AU detection-based dual-stream multi-task 3DCNN for micro-expression recognition},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot object detection based on global domain adaptation
strategy. <em>NPL</em>, <em>57</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11063-025-11727-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to detect novel objects from only a few annotated samples, few-shot object detection (FSOD) has undergone remarkable development. Previous works rarely pay attention to the perspective of gradient propagation to optimize existing methods, therefore failing to make full use of information for novel objects in gradient propagation. We propose a method to solve this problem based on two-stage fine-tuning. A domain adaptation module with multi-constraints is used to promote the spread of gradients, a classification promotion network is used to improve the effect of classification, and a multi-path mask head is added to enrich RoI features. Experiments on PASCAL VOC and COCO datasets show that our model significantly raises the performance compared with previous methods (up to 1–5 $$\%$$ in average).},
  archive      = {J_NPL},
  author       = {Gong, Xiaolin and Cai, Youpeng and Wang, Jian and Liu, Daqing and Ma, Yongtao},
  doi          = {10.1007/s11063-025-11727-z},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Neural Process. Lett.},
  title        = {Few-shot object detection based on global domain adaptation strategy},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GrapHisto: A robust representation of graph-structured data
for graph convolutional networks. <em>NPL</em>, <em>57</em>(1), 1–27.
(<a href="https://doi.org/10.1007/s11063-025-11728-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning from graphs is an established branch of AI research motivated by the relevance of applications that involve graph-structured data. The most popular instance is the graph neural network (GNN). On the other hand, due to the promising results of deep learning models in the most diverse fields of application, several efforts have been made to replicate these successes when dealing with graphical data. A prominent specimen of the kind is the graph convolutional network (GCN). Along these lines, the paper propose a novel approach for processing graphs that exploits the capabilities of convolutional neural networks (CNNs) to learn from images. This is achieved by means of a new representation of graphs, called GrapHisto, that portrays graphs in the form of characteristic “pictures”. The GrapHisto is in the form of graph-specific, unique tensors encapsulating the graph topology and its features (i.e., the labels associated with vertexes and edges). This representation is fed to a CNN, and the resulting machine is termed GrapHisto-CNN. The paper provides some theoretical investigations of the properties of the approach, and proposes solutions to some practical issues. An experimental evaluation of the GrapHisto-CNN is reported, revolving around two setups: classification of synthetically-generated graphs, and molecule classification form the dataset QM9. The results show that the approach is effective and robust, and that it compares favorably with GNNs and GCNs.},
  archive      = {J_NPL},
  author       = {Benini, Marco and Bongini, Pietro and Trentin, Edmondo},
  doi          = {10.1007/s11063-025-11728-y},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Neural Process. Lett.},
  title        = {GrapHisto: A robust representation of graph-structured data for graph convolutional networks},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECTTLNER: An effective cross-task transferring learning
method for low-resource named entity recognition. <em>NPL</em>,
<em>57</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s11063-025-11729-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition is a fundamental task in natural language processing that significantly impacts the performance of its downstream tasks. Cross-task transfer learning methods are more naturally suited for low-resource named entity recognition compared to cross-language and cross-domain transfer learning methods. Existing cross-task transfer learning methods improve the performance of the low-resource named entity recognition by leveraging relevant information from other auxiliary tasks, such as sentence-level and token-level information. However, these methods do not fully exploit token-level information of entities, leaving room for improvement in low-resource named entity recognition. To futher improve the performance of the low-resource named entity recognition, this paper proposes a simple and effective cross-task transfer learning method called ECTTLNER, which introduces Sentence Contains Entities, Sentence Entity Number, Token Is Entity, and Token Boundary Label prediction tasks into named entity recognition and performs multi-task learning together with the main sequence labeling task. Experimental results on three NER datasets demonstrate that ECTTLNER outperforms a set of state-of-the-art baseline models, and achieves more than a 2.6% improvement in F1-score over these baseline models, particularly in low-resource scenarios.},
  archive      = {J_NPL},
  author       = {Xu, Yiwu and Chen, Yun},
  doi          = {10.1007/s11063-025-11729-x},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Neural Process. Lett.},
  title        = {ECTTLNER: An effective cross-task transferring learning method for low-resource named entity recognition},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power analysis attacks on NVM crossbar-based neuromorphic
systems. <em>NPL</em>, <em>57</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s11063-025-11730-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new adversarial attack strategy against neuromorphic systems using analysis of power consumption. Specifically, we show that neuromorphic designs based on non-volatile memory crossbars can leak important information about loss sensitivity in their power profile. Adversaries can use this information to craft evasion attacks even if they don’t know the dataset that the model was trained on. In our experiments, we show that these types of attacks are effective against both single-layer and multilayer neuromorphic implementations of neural networks, and they can be made query-efficient through Bayesian optimization. We also provide theoretical insights into the relationship between the loss sensitivity and the power consumption measurements, showing that, for single-layer networks, the correlation coefficient of these two metrics scales inversely with the square root of the input size. Finally, this paper proposes that low bitwidth quantization could be an effective defense strategy against the class of attacks discussed herein.},
  archive      = {J_NPL},
  author       = {Merkel, Cory and Su, Allen},
  doi          = {10.1007/s11063-025-11730-4},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Neural Process. Lett.},
  title        = {Power analysis attacks on NVM crossbar-based neuromorphic systems},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent emerging techniques in explainable artificial
intelligence to enhance the interpretable and understanding of AI models
for human. <em>NPL</em>, <em>57</em>(1), 1–32. (<a
href="https://doi.org/10.1007/s11063-025-11732-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Explainable Artificial Intelligence (XAI) aim to bridge the gap between complex artificial intelligence (AI) models and human understanding, fostering trust and usability in AI systems. However, challenges persist in comprehensively interpreting these models, hindering their widespread adoption. This study addresses these challenges by exploring recently emerging techniques in XAI. The primary problem addressed is the lack of transparency and interpretability in AI models to humanity for institution-wide use, which undermines user trust and inhibits their integration into critical decision-making processes. Through an in-depth review, this study identifies the objectives of enhancing the interpretability of AI models and improving human understanding of their decision-making processes. Various methodological approaches, including post-hoc explanations, model transparency methods, and interactive visualization techniques, are investigated to elucidate AI model behaviours. We further present techniques and methods to make AI models more interpretable and understandable to humans including their strengths and weaknesses to demonstrate promising advancements in model interpretability, facilitating better comprehension of complex AI systems by humans. In addition, we provide the application of XAI in local use cases. Challenges, solutions, and open research directions were highlighted to clarify these compelling XAI utilization challenges. The implications of this research are profound, as enhanced interpretability fosters trust in AI systems across diverse applications, from healthcare to finance. By empowering users to understand and scrutinize AI decisions, these techniques pave the way for more responsible and accountable AI deployment.},
  archive      = {J_NPL},
  author       = {Mathew, Daniel Enemona and Ebem, Deborah Uzoamaka and Ikegwu, Anayo Chukwu and Ukeoma, Pamela Eberechukwu and Dibiaezue, Ngozi Fidelia},
  doi          = {10.1007/s11063-025-11732-2},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Neural Process. Lett.},
  title        = {Recent emerging techniques in explainable artificial intelligence to enhance the interpretable and understanding of AI models for human},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Master–slave finite-time synchronization of chaotic
fractional-order neural networks under hybrid sampled-data control: An
LMI approach. <em>NPL</em>, <em>57</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s11063-025-11733-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hybrid controller with a sampled data control is investigated to achieve finite-time master–slave synchronization of delayed fractional-order neural networks (DFONNs). A Lyapunov-Krasovskii functional is constructed to obtain the sufficient conditions that incorporate delay information. For the first time, the asymptotic stability of the error system is guaranteed in a finite-time using the inequality technique and a sampled-data hybrid controller. The obtained conditions are expressed via linear matrix inequality. Notably, the proposed approach outperforms existing methods, demonstrating improved results in a comparative analysis. An explicit formula is utilized to calculate the settling time, which is significantly influenced by the fractional order $$0&lt;\beta \le 1$$ . The superior performance of the proposed control method is evident, showcasing its effectiveness through numerical simulations and addressing the synchronization problem in DFONNs.},
  archive      = {J_NPL},
  author       = {Kiruthika, R. and Manivannan, A.},
  doi          = {10.1007/s11063-025-11733-1},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Neural Process. Lett.},
  title        = {Master–Slave finite-time synchronization of chaotic fractional-order neural networks under hybrid sampled-data control: An LMI approach},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parkinsons detection from gait time series classification
using modified metaheuristic optimized long short term memory.
<em>NPL</em>, <em>57</em>(1), 1–29. (<a
href="https://doi.org/10.1007/s11063-025-11735-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative conditions are defined by the progressive deterioration and death of nerve cells in the core neural system. Most neurodegenerative conditions are not curable. While there have been significant improvements and techniques used to treat these diseases early diagnosis continues to play a crucial role in the entire approach. Conditions are often diagnosed only once they start negatively impacting the daily life of those affected. Early detection and timely preventative treatment can help improve patient subjective well-being. This study examines the application of a non-invasive gait analysis technique for the detection of Parkinson’s disease. Publicly available data collected from patients suffering from Parkinson’s along with control groups is utilized and combined with long-short-term neural networks to construct models capable of detecting signs on Parkinson’s disorder. However, because of the significant reliance of models on appropriate parameters selection, metaheuristic algorithms are used to fine tune the selection process, and a modified variation of the strongly founded PSO algorithm was proposed. Several contemporary optimizers are compared based on their ability to optimize model performance. This suggested approach achieved the superior outcomes with an accuracy of 89.92%. The constructed models have been evaluated to determine feature importance using game theory based methods.},
  archive      = {J_NPL},
  author       = {Markovic, Filip and Jovanovic, Luka and Spalevic, Petar and Kaljevic, Jelena and Zivkovic, Miodrag and Simic, Vladimir and Shaker, Hotefa and Bacanin, Nebojsa},
  doi          = {10.1007/s11063-025-11735-z},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Neural Process. Lett.},
  title        = {Parkinsons detection from gait time series classification using modified metaheuristic optimized long short term memory},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: LPM-net: A data-driven resource-efficient
predictive motion planner for mobile robots. <em>NPL</em>,
<em>57</em>(1), 1. (<a
href="https://doi.org/10.1007/s11063-025-11736-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NPL},
  author       = {Amirhosseini, Fakhreddin and Nilforoushan, Zahra and Mirtaheri, Seyedeh Leili},
  doi          = {10.1007/s11063-025-11736-y},
  journal      = {Neural Processing Letters},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {Neural Process. Lett.},
  title        = {Correction: LPM-net: a data-driven resource-efficient predictive motion planner for mobile robots},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="paaa---38">PAAA - 38</h2>
<ul>
<li><details>
<summary>
(2025). Plant leaf image segmentation in natural scenes: A
multi-layer graph queries propagation approach. <em>PAAA</em>,
<em>28</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s10044-024-01380-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate leaf segmentation is crucial for optimizing plant recognition and enhancing leaf identification precision. However, leaf segmentation encounters challenges when working with images captured in natural scenes. These images often contain intricate backgrounds with soil artifacts, overlapping leaves, plant elements, shadows, and variations in lighting. To address these issues, we propose an approach for segmenting leaf images using a multi-layer graph-based propagation method. The process begins with spatial localization of the leaf, aiding in detecting the foreground template, describing the central area of the leaf. Subsequently, a multi-level decomposition of the image into homogeneous regions is accomplished to capture image details at different scales. We then construct a graph based on this structure, connecting each region to its neighbors with weighted edges based on shared areas or edges across different resolutions. This graph is used to rank regional similarities to the leaf by propagating ranking scores from the foreground template to the image boundaries. As a result, we obtain a saliency map, which is used to extract the leaf from its surroundings. Finally, the resulting binary mask is refined using random forests to achieve optimal separation between the leaf and the background. Experiments conducted on a widely used dataset demonstrate that our method outperforms several state-of-the-art segmentation methods.},
  archive      = {J_PAAA},
  author       = {Lyasmine, Adada and Idir, Filali and Samia, Bouzefrane},
  doi          = {10.1007/s10044-024-01380-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Plant leaf image segmentation in natural scenes: A multi-layer graph queries propagation approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A partitioning incremental algorithm using adaptive
mahalanobis fuzzy clustering and identifying the most appropriate
partition. <em>PAAA</em>, <em>28</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01360-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of determining the most appropriate number of clusters in a fuzzy Mahalanobis partition. First, a new fuzzy Mahalanobis incremental algorithm is constructed to search for an optimal fuzzy Mahalanobis partition with $$2,\,3,\ldots$$ clusters. Among these partitions, selecting the one with the most appropriate number of clusters is based on appropriately modified existing fuzzy indexes. In addition, the Fuzzy Mahalanobis Minimal Distance index is defined as a natural extension of the recently proposed Mahalanobis Minimal Distance index for non-fuzzy clustering. The new fuzzy Mahalanobis incremental algorithm was tested on several artificial data sets and the color image segmentation problems from real-world applications: art images, nature photography images, and medical images. The algorithm includes multiple usage of the global optimization algorithm DIRECT. But unlike previously known fuzzy Mahalanobis indexes, the proposed Fuzzy Mahalanobis Minimal Distance index ensures accurate results even when applied to complex real-world applications. A possible disadvantage could be the need for longer CPU time. Furthermore, besides effective identification of the partition with the most appropriate number of clusters, it is shown how to use the proposed Fuzzy Mahalanobis Minimal Distance index to search for an acceptable partition, which proved particularly useful in the above-mentioned real-world applications.},
  archive      = {J_PAAA},
  author       = {Scitovski, Rudolf and Sabo, Kristian and Grahovac, Danijel and Martínez-Álvarez, Francisco and Ungar, Sime},
  doi          = {10.1007/s10044-024-01360-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A partitioning incremental algorithm using adaptive mahalanobis fuzzy clustering and identifying the most appropriate partition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive optimization of low rank decomposition and its
application on fabric defect detection. <em>PAAA</em>, <em>28</em>(1),
1–15. (<a href="https://doi.org/10.1007/s10044-024-01363-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications of fabric defect detection, low-rank decomposition is the effective method. Sparse matrices represent defect results, so sparse terms are the focus of this application. Because the characteristics of each observation matrix differ, the weight of sparse term also differ. Therefore, this paper proposes adaptive weight for the model, allowing it to find suitable weight for different observation matrices and thereby improving the accuracy of model. During the matrix separation process of the model, elements that should belong to the sparse matrix may be separated into the noise matrix. To address this, this paper establishes new constraints to achieve a deeper separation between the two. While establishing the corresponding algorithmic framework, this paper also considers the fluctuations in the model’s solution process and proposes a new definition for the penalty factors. This aims to improve algorithm efficiency and reduce CPU time. This paper also provides a convergence analysis of the proposed method. In the dataset of fabric defects, it was shown that the star and dot types had the best results in TPR and F-measure, with TPR of 85.15% and 81.56%, and f-measure of 70.51% and 65.40%, respectively. Indicating that the method proposed in this paper has the fastest calculation speed.},
  archive      = {J_PAAA},
  author       = {Shi, Wenya and Chen, Zhixiang and Liang, Jiuzhen and Jiang, Daihong},
  doi          = {10.1007/s10044-024-01363-z},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive optimization of low rank decomposition and its application on fabric defect detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse embeddings learning for multi-view clustering.
<em>PAAA</em>, <em>28</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01364-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which improves clustering performance by exploring complementarity and consistency among multiple distinct feature sets, is attracting more and more researchers due to its wide applications in various fields e.g., pattern recognition and data mining. Traditional approaches usually explore above characteristics by mapping different views to a unified embedding through view-specific mapping matrices or neural networks. Then the unified embedding is fed into conventional single view clustering algorithms for final clustering results. However, a unified embedding is not enough to model distinct or even conflict multiple view characteristics due to their diverse representation abilities. Moreover, clustering and embedding learning are divided into two separate parts, which may bring in a gap between the class label and the learned embedding. To alleviate above problems, both unified and view-specific embeddings are learned, and a shared operator tensor and view-specific latent variables are introduced for their relationship modeling. Besides, a Kullback-Liebler divergence based objective is developed as a clustering oriented constraint, which leads to more clustering friendly embedding learned. Extensive experiments are conducted on six widely used datasets, achieving better results compared with several state-of-the-art approaches.},
  archive      = {J_PAAA},
  author       = {Li, Yongzhen and Liao, Husheng},
  doi          = {10.1007/s10044-024-01364-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Diverse embeddings learning for multi-view clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA-DETR: Saliency attention-based DETR for salient object
detection. <em>PAAA</em>, <em>28</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01379-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researches on the Salient Object Detection (SOD) task have made many advances based on deep learning methods. However, most methods have focused on predicting a fine mask rather than finding the most salient objects. Most datasets for the SOD task also focus on evaluating pixel-wise accuracy rather than “saliency”. In this study, we used the Salient Objects in Clutter (SOC) dataset to conduct research that focuses more on the saliency of objects. We propose a architecture that extends the cross-attention mechanism of Transformer to the DETR architecture to learn the relationship between the global image semantics and the objects. We extended module with Saliency Attention (SA) to the network, namely SA-DETR, to detect salient objects based on object-level saliency. Our proposed method with cross- and saliency-attentions shows superior results in detecting salient objects among multiple objects compared to other methods. We demonstrate the effectiveness of our proposed method by showing that it outperforms the state-of-the-art performance of the existing SOD method by 4.7% and 0.2% in MAE and mean E-measure, respectively.},
  archive      = {J_PAAA},
  author       = {Nam, Kwangwoon and Kim, Jeeheon and Kim, Heeyeon and Chung, Minyoung},
  doi          = {10.1007/s10044-024-01379-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {SA-DETR: Saliency attention-based DETR for salient object detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stgcn-pad: A spatial-temporal graph convolutional network
for detecting abnormal pedestrian motion patterns at grade crossings.
<em>PAAA</em>, <em>28</em>(1), 1–17. (<a
href="https://doi.org/10.1007/s10044-024-01382-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Spatial-Temporal Graph Convolutional Network-based Pedestrians’ behaviors Anomaly Detection system (STGCN-PAD) for grade crossings. The behaviors of pedestrians are represented in a structured manner by skeleton trajectories that are generated using a pose estimation model. The ST-GCN components are sequentially applied to capture the spatial dependencies between skeleton key points within a single video frame and the temporal relationships for each of them. Based on these features, the system reconstructs input trajectories with a constant sliding window size, and the reconstruction error is used to distinguish abnormal behaviors from those normal. To accelerate the processing of extracted multi-dimensional feature maps, an MLP-Mixer model-based reconstruction network is developed as an alternative to the traditional convolution neural network. Only trajectories of normal walking behavior are included for model training. Anomalies, such as lingering and squatting activities, can be identified as outliers by observing the magnitude of reconstruction errors. The case studies demonstrate the salient feasibility and efficiency of the proposed system, which achieves at least comparable performance (approximately 88% in the AUC evaluation metric) with several state-of-the-art approaches while using the MLP-Mixer model accelerates model inference by 10× relative to our previous effort (Song et al. in Appl Intell 53:21676–21691, 2023).},
  archive      = {J_PAAA},
  author       = {Song, Ge and Qian, Yu and Wang, Yi},
  doi          = {10.1007/s10044-024-01382-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Stgcn-pad: A spatial-temporal graph convolutional network for detecting abnormal pedestrian motion patterns at grade crossings},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Appropriateness of distances in nearest neighbour
classification: A monometric perspective. <em>PAAA</em>, <em>28</em>(1),
1–23. (<a href="https://doi.org/10.1007/s10044-024-01373-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the non-parametric classification methods, the nearest neighbour classifier (NNC) holds a pre-eminent position. Given a training or sample set $${\mathcal {S}}$$ the choice one needs to make is on the value of k and the distance function d to be employed. Towards improving the efficacy of an NNC, there are many works—both theoretical and empirical—that help in choosing a suitable value of k. However, works that deal with the appropriateness of a distance d for a given $${\mathcal {S}}$$ are largely empirical. In this work, we address the following two posers for a given $${\mathcal {S}}$$ : (1) How to identify a potentially appropriate distance d? (2) What qualities should an appropriate d possess? Our investigations show that every distance function d determines a landscape on the underlying data space and only if the class boundaries align with this landscape can this d be appropriate. In view of this, we construct a relational graph $${\mathcal {G}}_{{\mathcal {S}},d}$$ , in fact, a poset, on the given $${\mathcal {S}}$$ using d. With the help of $${\mathcal {G}}_{{\mathcal {S}},d}$$ , we choose a $${\mathcal {T}} \subset {\mathcal {S}}$$ to be used in a condensed-NN algorithm. Terming it the NEN algorithm, firstly, we show empirically that the training error of this NEN algorithm is reflective of the appropriateness of d. Towards providing a theoretical justification to our claims based on empiricism, we investigate the problem of classification in the setting of monometric spaces, wherein it emerges that the suitability of d is essentially related to the embeddability of $${\mathcal {G}}_{{\mathcal {S}},d}$$ in the monometric space ( $${\mathcal {X}}, \preceq _d,d$$ ).},
  archive      = {J_PAAA},
  author       = {Gupta, Megha and Jayaram, Balasubramaniam},
  doi          = {10.1007/s10044-024-01373-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Appropriateness of distances in nearest neighbour classification: A monometric perspective},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class preserving projections and data augmentation for
appearance-based face recognition. <em>PAAA</em>, <em>28</em>(1), 1–12.
(<a href="https://doi.org/10.1007/s10044-024-01388-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer Vision and Biometrics benefit from the recent advances in Pattern Recognition and Artificial Intelligence, which tends to make model-based face recognition more efficient. Also, deep learning combined with data augmentation tends to enrich the training sets used for learning tasks. Nevertheless, face recognition still is challenging, especially because of imaging issues that occur in practice, such as changes in lighting, appearance, head posture and facial expression. In order to increase the reliability of face recognition, we propose a novel supervised appearance-based face recognition method which creates a low-dimensional orthogonal subspace that enforces the face class separability. The proposed approach uses data augmentation to mitigate the problem of training sample scarcity. Unlike most face recognition approaches, the proposed approach is capable of handling efficiently grayscale and color face images, as well as low and high-resolution face images. Moreover, proposed supervised method presents better class structure preservation than typical unsupervised approaches, and also provides better data preservation than typical supervised approaches as it obtains an orthogonal discriminating subspace that is not affected by the singularity problem that is common in such cases. Furthermore, a soft margins Support Vector Machine classifier is learnt in the low-dimensional subspace and tends to be robust to noise and outliers commonly found in practical face recognition. To validate the proposed method, an extensive set of face identification experiments was conducted on three challenging public face databases, comparing the proposed method with methods representative of the state-of-the-art. The proposed method tends to present higher recognition rates in all databases. In addition, the experiments suggest that data augmentation also plays an essential role in the appearance-based face recognition, and that the CIELAB color space (L*a*b) is generally more efficient than RGB for face recognition as it attenuates lighting variations.},
  archive      = {J_PAAA},
  author       = {Soldera, John and Scharcanski, Jacob},
  doi          = {10.1007/s10044-024-01388-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Class preserving projections and data augmentation for appearance-based face recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeFormer: Local patch-based edge detection transformer on
point clouds. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01386-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge points on 3D point clouds can clearly convey 3D geometry and surface characteristics, therefore, edge detection is widely used in many vision applications with high industrial and commercial demands. However, the fine-grained edge features are difficult to detect effectively as they are generally densely distributed or exhibit small-scale surface gradients. To address this issue, we present a learning-based edge detection network, named EdgeFormer, which mainly consists of two stages. Based on the observation that spatially neighboring points tend to exhibit high correlation, forming the local underlying surface, we convert the edge detection of the entire point cloud into a point classification based on local patches. Therefore, in the first stage, we construct local patch feature descriptors that describe the local neighborhood around each point. In the second stage, we classify each point by analyzing the local patch feature descriptors generated in the first stage. Due to the conversion of the point cloud into local patches, the proposed method can effectively extract the finer details. The experimental results show that our model demonstrates competitive performance compared to six baselines.},
  archive      = {J_PAAA},
  author       = {Xie, Yifei and Tu, Zhikun and Yang, Tong and Zhang, Yuhe and Zhou, Xinyu},
  doi          = {10.1007/s10044-024-01386-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {EdgeFormer: Local patch-based edge detection transformer on point clouds},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive dual-branch network for long-tailed visual
recognition. <em>PAAA</em>, <em>28</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01387-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, deep learning techniques have been widely applied to visual tasks, leading to remarkable breakthroughs. However, data in real-world scenarios often exhibit a long-tailed distribution, where head classes contain significantly more samples than tail classes. Models trained on such imbalanced data tend to bias the head classes, resulting in poor performance on tail classes. Moreover, due to the scarcity of samples in the tail classes, it is challenging for models to learn robust representations for those classes. Inspired by the success of contrastive learning in representation learning, we propose a Contrastive Dual-Branch Network (CDBN) for long-tailed visual recognition. CDBN integrates an imbalance learning branch and a contrastive learning branch to address the challenges of imbalanced data. The imbalance learning branch leverages traditional methods to address data imbalance, while the contrastive learning branch follows the principles of contrastive learning. Specifically, it uses two distinct data augmentation techniques to process the same batch of samples, generating positive sample pairs for enhanced learning. A contrastive auxiliary loss is then introduced to minimize the distance between these pairs in the normalized embedding space. Furthermore, we propose a Cumulative Fusion Strategy (CFS) to guide the model in progressively prioritizing tail classes throughout training. We conducted extensive experiments on the CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets and compared our method with various advanced algorithms. The results demonstrate that our method substantially enhances performance across all datasets, achieving state-of-the-art results on several benchmarks. Our code is available at https://github.com/mmzbyxx/CDBN .},
  archive      = {J_PAAA},
  author       = {Miao, Jie and Zhai, Junhai and Han, Ling},
  doi          = {10.1007/s10044-024-01387-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Contrastive dual-branch network for long-tailed visual recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting optimized forgery representation space for
general fake face detection. <em>PAAA</em>, <em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01391-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery has become more realistic with deep learning in computer vision, posing a significant challenge to trustworthy face identification. Existing works have achieved considerable accuracy within the dataset by formulating the detection as a binary classification problem. These methods attempt to amplify the category differences between real and fake faces but ignore the optimization of representation space for learning the specific forgery information within samples, which results in the intra-class distribution collapse and poor generalization in unseen domains. To mitigate this issue, we propose a novel forgery detection framework that combines contrastive learning with supervised learning, named Contrastive Learning Against face Forgery (CLAF). Specifically, a dual branch learning framework is involved in extracting the consistent forgery feature distribution first. Then, we consider the similarity, variance, and covariance constraint term for the representation space, which can better preserve the specific forgery information within each sample for generalization detection. The generalization performance is confirmed on FaceForensics++, Celeb-DF, and DFDC. Extensive experiment results demonstrate the effectiveness of our framework in improving generalization.},
  archive      = {J_PAAA},
  author       = {Yang, Gaoming and Zuo, Bang and Fang, Xianjin and Zhang, Ji},
  doi          = {10.1007/s10044-024-01391-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Exploiting optimized forgery representation space for general fake face detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving distantly supervised named entity recognition by
emphasizing uncertain examples. <em>PAAA</em>, <em>28</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s10044-024-01392-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distantly supervised named entity recognition (DS-NER) aims to acquire knowledge from noisy labels. Recently, label re-weighting and label correction based frameworks have been recognized as promising approaches for DS-NER. These methods mainly handle easy or hard examples, yet neglect the impact of uncertain examples that are predicted correctly sometimes and incorrectly some other times during optimization. In this paper, we propose UE-NER, an Uncertainty Estimation method for DS-NER, which estimates the uncertainty of training examples and emphasizes uncertain ones, thus leads to more accurate and robust performance. To enable uncertainty reasoning, we formulate DS-NER as a span-level classification problem and the variance in predicted probability of the correct class across iterations of minibatch SGD is taken as the uncertainty measure. We further design an enhanced encoder to combine the power of the named entity and other spans in the sentence to boost recognition performance. Experimental results on two benchmark datasets demonstrate the superiority of the proposed UE-NER over existing DS-NER methods.},
  archive      = {J_PAAA},
  author       = {Nie, Binling and Shao, Yiming and Wang, Yigang},
  doi          = {10.1007/s10044-024-01392-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Improving distantly supervised named entity recognition by emphasizing uncertain examples},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multi-target vehicle trajectory prediction based
on multi-scale graph convolution. <em>PAAA</em>, <em>28</em>(1), 1–17.
(<a href="https://doi.org/10.1007/s10044-024-01396-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target vehicle trajectory prediction holds significant importance in the field of autonomous driving. Accurate trajectory prediction can enhance the safety and efficiency of autonomous vehicles, reducing traffic accidents and congestion. However, existing methods often fall short when dealing with complex traffic scenarios. Traditional approaches typically rely on single-scale spatiotemporal feature extraction, which struggles to fully capture the complex dynamics of traffic across different temporal and spatial scales, especially in high-density traffic environments. To address these challenges, this thesis proposes a multi-target vehicle trajectory prediction method based on a Multi-Scale Graph Convolutional Network (MSGCN). This method integrates high-definition semantic maps and employs a spatiotemporal multi-head attention mechanism alongside an adaptive dynamic weighting module to achieve efficient multi-target vehicle trajectory prediction. Specifically, this thesis constructs a dynamic feature repository using vehicle subgraphs and lane subgraphs to stabilize model weight fluctuations, thereby more accurately reflecting actual traffic conditions. Experimental results on the Argoverse dataset demonstrate the effectiveness of our method. Specifically, our approach reduces the average displacement error (mADE) by 7% and enhances the final displacement error (mFDE) by 19% when compared to existing state-of-the-art models. Our code is made available at https://github.com/Garegreen/EfficientMSGCN .},
  archive      = {J_PAAA},
  author       = {Gu, Xiang and Wang, Jing and Cheng, Dengyang and Li, Chao and Huang, Qiwei},
  doi          = {10.1007/s10044-024-01396-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Efficient multi-target vehicle trajectory prediction based on multi-scale graph convolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning learning curves. <em>PAAA</em>, <em>28</em>(1),
1–13. (<a href="https://doi.org/10.1007/s10044-024-01394-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning curves depict how a model’s expected performance changes with varying training set sizes, unlike training curves, showing a gradient-based model’s performance with respect to training epochs. Extrapolating learning curves can be useful for determining the performance gain with additional data. Parametric functions, that assume monotone behaviour of the curves, are a prevalent methodology to model and extrapolate learning curves. However, learning curves do not necessarily follow a specific parametric shape: they can have peaks, dips, and zigzag patterns. These unconventional shapes can hinder the extrapolation performance of commonly used parametric curve-fitting models. In addition, the objective functions for fitting such parametric models are non-convex, making them initialization-dependent and brittle. In response to these challenges, we propose a convex, data-driven approach that extracts information from available learning curves to guide the extrapolation of another targeted learning curve. Our method achieves this through using a learning curve database. Using the initial segment of the observed curve, we determine a group of similar curves from the database and reduce the dimensionality via Functional Principle Component Analysis FPCA. These principal components are used in a semi-parametric kernel ridge regression (SPKR) model to extrapolate targeted curves. The solution of the SPKR can be obtained analytically and does not suffer from initialization issues. To evaluate our method, we create a new database of diverse learning curves that do not always adhere to typical parametric shapes. Our method performs better than parametric non-parametric learning curve-fitting methods on this database for the learning curve extrapolation task.},
  archive      = {J_PAAA},
  author       = {Turan, O. Taylan and Tax, David M. J. and Viering, Tom J. and Loog, Marco},
  doi          = {10.1007/s10044-024-01394-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning learning curves},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning with deep laplacian support vector
machine. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01395-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is a rapidly growing field that can effectively extract latent features from data and use them to make predictions based on the learned features, but most models just sum the loss of each sample without considering the relationship between samples. On the other hand, the traditional Laplacian Support Vector Machine (LapSVM) can effectively utilize samples and the relationship between samples by constructing a Laplacian graph, and performs well on semi-supervised data. In this paper, we combine LapSVM and deep learning and propose Deep Laplacian Support Vector Machine. Our approach is to first use a Deep Neural Network to extract the latent features from the image, then based on the extracted feature information and a small amount of original label information, we use LapSVM for classification, build a loss function, and finally iteratively update the two parts together. We evaluate our method on several benchmark datasets and demonstrate that it outperforms other semi-supervised learning methods.},
  archive      = {J_PAAA},
  author       = {Chen, Hangyu and Xie, Xijiong and Li, Di},
  doi          = {10.1007/s10044-024-01395-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Semi-supervised learning with deep laplacian support vector machine},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust approach for outlier detection based on the ratio
of number of reverse neighbors to neighbors. <em>PAAA</em>,
<em>28</em>(1), 1–30. (<a
href="https://doi.org/10.1007/s10044-024-01372-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is an important issue in data mining, which has a wide range of applications in medicine, economics, video search, and credit card fraud detection. Many outlier detection methods have recently been developed. Most of the existing methods act based on the distance or density. Since each of these methods has its inherent disadvantage, we proposed a method which has the advantages of both distance-based and density-based methods. The proposed method is inspired by the basic idea that outliers are usually more distant neighbors to their nearest neighbors. The proposed method consists of three different parts. Each of these parts considers the distance, density, or location of objects, and finally we reach an optimal and efficient algorithm by combining these parts. Our algorithm is based on k nearest neighbor; in addition, we also use another kind of adaptive and extended neighborhood in order to provide more accurate results. Furthermore, the proposed method is robust and has little sensitivity to changes in parameter k. Numerical experiments and comparing with well-known algorithms are performed on both synthetic and real datasets in order to prove the efficiency and robustness of the proposed method.},
  archive      = {J_PAAA},
  author       = {Heydari-Gharaei, Reza and Sharifi, Rasoul and Kashef, Shima and Nezamabadi-pour, Hossein},
  doi          = {10.1007/s10044-024-01372-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A robust approach for outlier detection based on the ratio of number of reverse neighbors to neighbors},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFM-UNET: Enhancing medical image segmentation with
multi-scale and multi-view frequency fusion. <em>PAAA</em>,
<em>28</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01384-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation benefits greatly from accurate and efficient models. Although CNNs and Transformer-based models are widely regarded as foundational methods in the realm of medical image segmentation, each has inherent drawbacks: Convolutional Neural Networks (CNNs) frequently face challenges when it comes to accurately capturing long-range relationships because of their limited receptive fields. Conversely, Transformers excel at capturing long-range relationships but come with a high computational cost. To address these challenges, State Space Models (SSMs) like Mamba have emerged as a promising alternative, providing an effective method to represent long-range interactions while maintaining a linear complexity. In this study, we present the Multi-Scale and Multi-View Frequency Mamba UNet (MSFM-UNet), a model specifically designed to leverage Mamba’s unique strengths for improving medical image segmentation. Additionally, the Multi-Scale Feature Aggregation (MSFA) effectively merges the feature outputs generated by each encoder block with those from the decoder. Furthermore, the Multi-View Frequency Enhancement (MVFA) is employed to simultaneously capture global and local perspectives, combining frequency domain attributes to improve the representation of features across multiple scales. We performed a comprehensive evaluation of MSFM-UNet on four widely recognized public datasets: ISIC17, ISIC18, Synapse, and ACDC. The experimental results clearly demonstrate that MSFM-UNet outperforms the current leading models in medical image segmentation. The code is made publicly available at https://github.com/qczggaoqiang/MSFM-UNet .},
  archive      = {J_PAAA},
  author       = {Gao, Qiang and Wang, Yi and Zhou, Feiyan and Wen, Jing and Li, Yong and Fang, Bin and Chen, Peng and Du, Lan and Chen, Cunjian},
  doi          = {10.1007/s10044-024-01384-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSFM-UNET: Enhancing medical image segmentation with multi-scale and multi-view frequency fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent sparse subspace learning and visual domain
classification via balanced distribution alignment and hilbert–schmidt
metric. <em>PAAA</em>, <em>28</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10044-024-01390-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Getting machine learning (ML) to perform accurate prediction needs a sufficient number of labeled samples. However, due to the either lack or small number of labeled samples in most domains, it is often beneficial to use domain adaptation (DA) and transfer learning (TL) to leverage a related auxiliary source domain to optimize the performance on target domain. In fact, the purpose of TL and DA is to use the labeled sample information (i.e., samples and the corresponding labels) for training the classifier to categorize the unlabeled samples. In this paper, we aim to propose a novel semi-supervised transfer learning method entitled “Latent Sparse subspace learning and visual domain classification via Balanced distribution alignment and Hilbert–Schmidt metric (LSBH)”. LSBH uses the latent sparse domain transfer learning for visual adaptation (LSDT) to adapt the samples with different distributions or feature spaces across domains and prevent the creation of local common subspace for source and target domains via the simultaneous learning of latent space and sparse reconstruction. LSBH proposes a novel robust classifier which maintains performance and accuracy even when faced with variations across the source and target domains. To this end, it utilizes the following two criteria in the optimization problem: maximum mean discrepancy and Hilbert–Schmidt independence criterion to reduce the marginal and conditional distribution disparities of domains and increase the dependency between samples and labels at the classification step. LSBH obtains the optimal coefficients for the classifier, which results in the minimum error in the loss function by solving the optimization problem. Thus, the error minimizing of the loss function is a part of the optimization problem. Also, to maintain the geometric structure of data in the classification step, the neighborhood graph of samples is used. The efficiency of the proposed method has been evaluated on different visual datasets and has been compared with new and prominent methods of domain adaptation and transfer learning. The results induce the superior performance of LSBH compared to the other state-of-the-art methods in label prediction.},
  archive      = {J_PAAA},
  author       = {Noori Saray, Shiva and Balafar, Mohammad-Ali and Tahmoresnezhad, Jafar},
  doi          = {10.1007/s10044-024-01390-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Latent sparse subspace learning and visual domain classification via balanced distribution alignment and Hilbert–Schmidt metric},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gated normalization unit for image restoration.
<em>PAAA</em>, <em>28</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10044-024-01393-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration has been an integral part of image processing research with the goal of converting degraded images into clear ones. While some networks have achieved state-of-the-art results through architecture and module design, little attention has been paid to the adaptation of normalization methods in image restoration tasks. Normalization methods are crucial in deep learning. In this work, we attempt to combine gating mechanisms with normalization methods. Gated mechanisms are popular in feature extraction and information filtering, and combining them with normalization methods has potential for designing image restoration algorithms. Firstly, we propose a Simple Gated Attention Unit (SGAU), a block using a simple gating mechanism to validate the potential of gating mechanisms. Then, we propose a new normalization block, Gated Instance Normalization (GIN), and introduce a new normalization method, Global Response Normalization (GRN), for image restoration tasks. Both GIN and GRN combine gating mechanisms with normalization methods for feature extraction, fusion, and integration. Finally, we propose a two-stage network, Gated Normalization Network (GNNet), utilizing GIN and GRN as blocks to effectively extract and filter information. Deep separable convolutions are used in the deep layers to reduce parameters while preserving spatial information, improving local feature perception. An improved cross-stage feature fusion (ICSFF) block is used for feature information transfer between stages, and a supervised attention module (SAM) is used as input to the second stage network from the first stage output. Through various image restoration tasks, we achieve 32.93 dB PSNR on GoPro, 30.42 dB PSNR on HIDE for image deblurring, 39.94 dB PSNR on SIDD for real-world denoising, and good performance in Gaussian white noise denoising and image deraining tasks. Moreover, the GIN and GRN only generated a small number of gated weight and bias parameters, and compared to other multi-stage networks, the model size is reduced, and computational complexity is well balanced.},
  archive      = {J_PAAA},
  author       = {Wang, Qingyu and Wang, Haitao and Zang, Luyang and Jiang, Yi and Wang, Xinyao and Liu, Qiang and Huang, Dehai and Hu, Binding},
  doi          = {10.1007/s10044-024-01393-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Gated normalization unit for image restoration},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-state perception consistency constraints network for
person re-identification. <em>PAAA</em>, <em>28</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s10044-024-01398-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) remains challenging due to pose variations and scale changes across non-overlapping camera views. In this work, we propose a Multi-state Perception Consistency Constraints Network (MPCC-Net) that extracts discriminative and robust features for person Re-ID. MPCC-Net consists of three primary components. First, a multi-state fused backbone network processes multi-scale and multi-view information. Second, perception consistency constraints enhance feature stability. Third, partition attention modules focus on different body parts to improve local discrimination. Comprehensive experiments on benchmark datasets demonstrate MPCC-Net’s competitive performance, effectively addressing pose and scale variations for accurate person Re-ID. Our source code will also be publicly available at: https://github.com/sesamecandy/MPCC-Net},
  archive      = {J_PAAA},
  author       = {Zhou, Mengting and Lian, Guoyun and Ouyang, Xinyu and Du, Jingyu and Song, Qiqi and Yang, Jinfeng},
  doi          = {10.1007/s10044-024-01398-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-state perception consistency constraints network for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-weighted subspace clustering via adaptive rank
constrained graph embedding. <em>PAAA</em>, <em>28</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10044-024-01405-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, subspace clustering methods have attracted wide attention in partitioning high-dimensional data from a union of underlying subspaces, in which the data distribution is mainly explored to compensate for the absence of label information. However, for practical applications, subspace clustering still suffers from redundant and noisy features, which brings about disturbed reconstruction loss and restricts trustworthy graph learning. In this paper, we propose a robust subspace clustering framework via Self-weighted feature learning and adaptive rank constrained graph embedding (SWARG) to address the limitations of existing graph-based subspace clustering models. Specifically, a feature self-weighted learning term is introduced to the sparse subspace clustering framework to alleviate the disturbed contributions from the noisy and redundant features. As such, a few discriminative features will act as remarkable contributions in representing data samples. Meanwhile, the profile-based graph embedding term further preserving the contribution behavior information of data samples that distributed around the same subspace. Moreover, the adaptive rank-constraint graph embedding method is considered to guarantee discriminative structure for different components of representation matrix with flexible entropy-based similarity preserving. To solve the proposed model, we then develop an efficient alternative direction updating algorithm, together with convergence and complexity analysis. Finally, experimental results on toy databases and benchmark databases demonstrate the effectiveness of the proposed SWARG model compared to a series of state-of-the-art models. Our code is available at http://github.com/ty-kj/SAWRG .},
  archive      = {J_PAAA},
  author       = {Jiang, Kun and Yang, Zhihai and Sun, Qindong},
  doi          = {10.1007/s10044-024-01405-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Self-weighted subspace clustering via adaptive rank constrained graph embedding},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ST-HViT: Spatial-temporal hierarchical vision transformer
for action recognition. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-024-01407-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) is an important task in the field of computer vision, and its primary goal is to analyze and understand human activities in videos. In addition to containing the spatial information of static images, videos also contain unique temporal information, which makes the information contained in the videos even richer. However, the training cost required to fully learn the spatial-temporal information of the videos is quite expensive for a model. In light of this, we propose a novel two-stream network structure to effectively capture the spatial-temporal information in video data. We perform masked autoencoders (MAE) pre-training, aiming to reduce the training burden of the model through this asymmetric encoder-decoder pre-training method. In addition, we propose a new multi-scale decoder component that combines transposed convolutional upsampling and convolutional downsampling. It fully utilizes the multi-scale features of the encoder to achieve excellent performance. On two challenging video datasets, Kinetics 400 (K400) and Something-Something-v2 (SSv2), we achieve state-of-the-art performance with 85.9 $$\%$$ and 75.3 $$\%$$ Top-1 accuracy, respectively.},
  archive      = {J_PAAA},
  author       = {Xia, Limin and Fu, Weiye},
  doi          = {10.1007/s10044-024-01407-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ST-HViT: Spatial-temporal hierarchical vision transformer for action recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional spatio-temporal generative adversarial network
for video super-resolution. <em>PAAA</em>, <em>28</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10044-024-01409-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial and periodic training method plays an essential role in video super-resolution, which can generate spatial high frequency detail and temporal consistency relation. However, this approach is based on unidirectional loop whose first frame can only use its own feature information, while the last frame can use all feature information of whole sequence. The biggest problem caused by this information imbalance is that early video frames are poorly reconstructed. To address these issues, we propose a novel video super-resolution model, called Bidirectional Spatio-Temporal Generative Adversarial Network (Bi-STGAN), to generate fine detail and temporal consistency video by explicitly introducing the backward branch. Specifically, Bi-STGAN adopts an elaborately designed bidirectional branch structure so that the high-resolution frames estimated from front to back can be used as input for subsequent iterations from back to front. The advantage of Bi-STGAN is to enhance information gathering by utilizing information from past and future frames which can be cyclically passed through the time series. The experimental results show that compared with the baselines, Bi-STGAN achieves competitive improvement of 15.20% for LPIPS and 2.87dB for PSNR on the REDS4 dataset, thereby demonstrating the superiority of our state-of-the-art model on video super-resolution.},
  archive      = {J_PAAA},
  author       = {Yang, Peng and Chen, Zhangquan and Sun, Yuankang and Hu, Zhongjian and Li, Bing},
  doi          = {10.1007/s10044-024-01409-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Bidirectional spatio-temporal generative adversarial network for video super-resolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced clustering contrastive learning for long-tailed
visual recognition. <em>PAAA</em>, <em>28</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s10044-025-01410-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world deep learning training data often follow a long-tailed (LT) distribution, where a few classes (head classes) have the most samples and many classes (tail classes) have very few samples. Models trained on LT datasets typically achieve high accuracy on head classes, but suffer from poor performance on tail classes. To address this challenge, strategies based on supervised contrastive learning have been explored. However, existing methods often focus on either reducing the dominance of head class features or expanding the feature space of tail classes, but rarely achieve a balanced feature distribution across both. In this paper, we propose Balanced clustering contrastive learning (BCCL) to balance the feature space between the head and tail classes more effectively. The proposed approach introduces two main components. First, we employ queue-based clustering to extract multiple centroids. This addresses the intra-minibatch class absence issue and maintains intra-class balance. Second, we expand the feature space of tail classes based on class frequency to enhance their expressiveness. An evaluation of four LT datasets, CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018, demonstrates that BCCL consistently outperforms the existing methods. These results establish the ability of BCCL to maintain a balanced feature space in diverse environments. Our code is available at https://github.com/GGTINE/BCCL .},
  archive      = {J_PAAA},
  author       = {Kim, Byeong-il and Ko, Byoung Chul},
  doi          = {10.1007/s10044-025-01410-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Balanced clustering contrastive learning for long-tailed visual recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separability and scatteredness (s&amp;s) ratio-based
efficient SVM regularization parameter, kernel, and kernel parameter
selection. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-025-01411-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning a regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels. In this case a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through various forms of deterministic or probabilistic grid-search. The existing methods rely heavily on exhaustive searches and provide very limited insight into the underlying data characteristics, resulting in excessive computational complexity. This work addresses this issue by proposing a statistical framework that directly relates the dataset’s separability and scatteredness to the choice of optimal hyperparameters. By stochastically analyzing the behavior of the regularization parameter, the method shows that the SVM performance can be modeled as a function of the newly defined separability and scatteredness (S&amp;S) ratio of the data. The Separability is a measure of the distance between classes, and the scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost function, an S&amp;S ratio-based table provides the optimum RP. The data S&amp;S ratio is a powerful value that can automatically evaluate linear or non-linear separability before using the SVM algorithm. The provided lookup S&amp;S ratio-based table can also provide the optimum kernel and its parameters before using the SVM algorithm. Consequently, the computational complexity of the CV grid-search is reduced to only the computational complexity of one-time use of the SVM. The simulation results on the real dataset confirm the superiority of the proposed approach in the sense of efficiency and computational complexity over the grid-search methods. The method performs better or comparable to the existing state of-the-art methods with a significantly reduced computational cost.},
  archive      = {J_PAAA},
  author       = {Shamsi, Mahdi and Beheshti, Soosan},
  doi          = {10.1007/s10044-025-01411-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Separability and scatteredness (S&amp;S) ratio-based efficient SVM regularization parameter, kernel, and kernel parameter selection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ultra-fast computation of fractal dimension for RGB images.
<em>PAAA</em>, <em>28</em>(1), 1–22. (<a
href="https://doi.org/10.1007/s10044-025-01415-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractal dimension (FD) is a quantitative parameter widely used to analyze digital images in many application fields such as image segmentation, feature extraction, object recognition, texture analysis, and image compression and denoising, among many others. A variety of algorithms have been previously proposed for estimating the FD, however most of them are limited to binary or gray-scale images only. In recent years, several authors have proposed algorithms for computing the FD of color images. Nevertheless, almost all these methods are computationally inefficient when analyzing large images. Nowadays, color images can be very large in size, and there is a growing trend toward even larger datasets. This implies that the time required to calculate the FD of such datasets can become extremely long. In this paper we present a very efficient GPU algorithm, implemented in CUDA, for computing the FD of RGB color images. Our solution is an extension to RGB of the differential box-counting (DBC) algorithm for gray-scale images. Our implementation simplifies the box-counting computation to very simple operations which are easily combined across iterations. We evaluated our algorithm on two distinct hardware/software platforms using a set of images of increasing size. The performance of our method was compared against two recent FD algorithms for RGB images: a fast box-merging GPU algorithm, and the most advanced approach based on extending the DBC method. The results showed that our GPU algorithm performed very well and achieved speedups of up to 7.9× and 6172.6× regarding these algorithms, respectively. In addition, our algorithm achieved average error rates similar to those obtained by the two reference algorithms when estimating the FD for synthetic images with known FD values, and even outperformed them when processing large images. These results suggest that our GPU algorithm offers a highly reliable and ultra-fast solution for estimating the FD of color images.},
  archive      = {J_PAAA},
  author       = {Ruiz de Miras, Juan and Li, Yurong and León, Alejandro and Arroyo, Germán and López, Luis and Torres, Juan Carlos and Martín, Domingo},
  doi          = {10.1007/s10044-025-01415-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Ultra-fast computation of fractal dimension for RGB images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DE-NAF: Decoupled neural attenuation fields for sparse-view
CBCT reconstruction. <em>PAAA</em>, <em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-025-01416-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cone-beam computed tomography (CBCT) acquires three-dimensional internal images, particularly effective for high-mineral density structures like bones. However, especially in sparse view scenarios, its ability to visualize low-density soft tissues is limited, restricting its clinical applications. To address this problem, this study proposes a method called Decoupled Neural Attenuation Fields (DE-NAF). Specifically, DE-NAF utilizes an Adaptive Hybrid Encoder that includes both hash encoding and 3D feature grid encoding methods. This approach decouples the CBCT reconstruction into two components. Hash encoding is used for high-mineral density structures, such as bones, owing to its superior encoding quality and ability to retain features of high-mineral density structures during hash conflict resolution. The 3D feature grid is employed for low-density soft tissues, such as muscles, as it effectively preserves the feature information of low-density soft tissues. The Adaptive Hybrid Encoder extracts these features, which are then decoded by a multilayer perceptron (MLP) decoder to predict X-ray attenuation values for precise reconstruction. In addition, a loss of structural perception was introduced to enhance tissue contrast and detail, further aiding CBCT reconstruction. Extensive experiments demonstrated that DE-NAF effectively addresses the limitations of CBCT in imaging low-density soft tissues, maintaining complete structural integrity and exceeding other methods in reconstruction quality.},
  archive      = {J_PAAA},
  author       = {Zhao, Tianning and Ding, Guoping and Liu, Zhenyang and Hu, Peng and Wei, Hangping and Tan, Min and Ding, Jiajun},
  doi          = {10.1007/s10044-025-01416-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DE-NAF: Decoupled neural attenuation fields for sparse-view CBCT reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task model with attribute-specific heads for person
re-identification. <em>PAAA</em>, <em>28</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-025-01421-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) has become an important task in digital surveillance for enhancing security, efficient monitoring, and enabling various applications in smart cities and public safety systems. Person ReID with attributes is a challenging task due to different camera views create significant difficulties in capturing each person’s unique identity and detailed attributes. In this work, we propose a multi-task model that not only performs unique person ReID but also simultaneously predicts attributes. Our model jointly utilizes a shared backbone network, which can be either ResNet50 or EfficientNet, along with generalized mean (GeM) pooling to achieve efficient feature extraction. It also applies attribute-specific heads to predict various characteristics such as gender, age, type of clothes, color, and alongside the ReID classification. This multi-task approach utilizes the shared features across tasks, gives comprehensive attribute predictions, and may further contribute to identification in surveillance scenarios. We evaluate our model on two commonly used publicly available datasets, Market1501 and DukeMTMC-reID, demonstrating how our approach can improve both in ReID accuracy and give reliable attribute predictions. These results reveal that our multi-task model can be competitive, providing a holistic solution for practical applications in surveillance where both identification and attributes are important. The approach has shown the potential of unifying ReID with attribute prediction to develop more robust and advanced surveillance systems. The code of this experiment is publicly accessible at https://github.com/TripleTheGreatDali/ReIDMTMASH .},
  archive      = {J_PAAA},
  author       = {Ahmed, Md Foysal and Oyshee, Adiba An Nur},
  doi          = {10.1007/s10044-025-01421-0},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-task model with attribute-specific heads for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new method for tuning the CNN pre-trained models as a
feature extractor for malware detection. <em>PAAA</em>, <em>28</em>(1),
1–19. (<a href="https://doi.org/10.1007/s10044-024-01381-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in Android malware detection, current approaches face notable challenges, particularly in handling obfuscation techniques, achieving high detection accuracy, and maintaining computational efficiency. Traditional static and dynamic analysis methods often struggle with evolving malware tactics and providing lightweight execution, which necessitates models that can dynamically adapt to these challenges. To address these needs, this study presents TuneDroid, a novel approach that optimizes CNN model configurations for both improved detection rates and resilience to obfuscation. By leveraging image-based visualization of code, TuneDroid enables CNNs to recognize high-level visual patterns that remain consistent even with code modifications, thereby enhancing robustness against common evasion tactics. TuneDroid utilizes Bayesian optimization for dynamically tuning pre-trained Convolutional Neural Network (CNN) models. This optimization process selects optimal pre-trained models, layer configurations, and positions, significantly enhancing detection performance. Using a dataset of 3000 benign and 3000 malicious apps, where DEX code is converted into images, TuneDroid achieved accuracy rates of 99.44% on the validation set and 98.00% on the testing set. In comparison, static end-to-end models without hyperparameter tuning yielded lower accuracies, not exceeding 90.50% and 91.17%. The robustness of TuneDroid’s performance is demonstrated through extensive experiments, including precision, recall, F1-score, and comparisons with baseline models. These results highlight the importance of dynamic tuning in maximizing the effectiveness of CNN-based malware detection. This work stands out by focusing on the dynamic tuning of deep learning models for Android app security, demonstrating substantial improvements in detection accuracy and showcasing the potential of Bayesian optimization in this context.},
  archive      = {J_PAAA},
  author       = {Bakır, Halit},
  doi          = {10.1007/s10044-024-01381-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new method for tuning the CNN pre-trained models as a feature extractor for malware detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFFNet: Multi-scale feature fusion network with semantic
optimization for crowd counting. <em>PAAA</em>, <em>28</em>(1), 1–16.
(<a href="https://doi.org/10.1007/s10044-024-01385-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the crowd-counting system has strong strength to ensure safety in public places with the practical necessity of dense crowd analysis. However, until now, it has been difficult to obtain high-quality density maps due to complex background interference, congested crowd and large-scale variations. To address this issue, this paper presents a multi-scale feature fusion network (MSFFNet) based on CNN (convolution neural network), which is capable of detecting enough semantic features to understand crowds in sparse and highly congested scenes. In this method, a large majority of encoded features are fused adaptively rather than separated extraction components. Therefore, it enhances the ability to extend the range of receptive field sizes and reduce computation cost. MSSFNet is consists of three modules: grouped feature extractor, fusion block and decoder. The feature extractor is based on first 13 convolution layers of VGG16, which extract low-level features from crowd images. The fusion block computes the weights in each group from the contrast features and average them from convolutional layers later concatenated pooling layer with a feature map. The decoder capably extracts relevant information while enduring spatial resolution. Additionally, we designed two-stream module and semantic optimization module (SOM) with decoder which instantaneously enhance the crowd head positions and reduce background noises by re-weighting features. Extensive experiments on four public datasets (ShanghaiTech Part_A and Part_B, UCF_CC_50, UCF_QNRF and JHU-Crowd++), validate that MSFFNet can perform efficiently in complex background noises and capture head sizes in sparse, congested and various weather situation.},
  archive      = {J_PAAA},
  author       = {Rohra, Avinash and Yin, Baoqun and Bilal, Hazrat and Kumar, Aakash and Ali, Munawar and Li, Yang},
  doi          = {10.1007/s10044-024-01385-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSFFNet: Multi-scale feature fusion network with semantic optimization for crowd counting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight multidimensional feature network for small
object detection on UAVs. <em>PAAA</em>, <em>28</em>(1), 1–24. (<a
href="https://doi.org/10.1007/s10044-024-01389-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV small object detection has essential applications in the military, search and rescue, and smart cities, providing critical support for target recognition in complex environments. However, the existing UAV small object detection models usually have many parameters and high computational complexity, limiting their deployment and application in practical scenarios to some extent. In this study, we propose a UAV detector with Lightweight Multidimensional Feature Network (LMF-UAV), aiming to reduce the number of parameters and computation of the model while guaranteeing accuracy, which constructs the Lightweight Multidimensional Feature Network (LMF-Net) for lightweight feature extraction, and Efficient Expressive Network (EENet) for efficient feature fusion. Neural architecture search utilizes the Dual-branch Cross-stage Universal Inverted Bottleneck to enable the network to select the most suitable structure at different layers according to requirements, thereby improving the computational efficiency of LMF-Net while maintaining performance. EENet uses the Channel-wise Partial Convolution Stage to reduce redundant computation and memory access and fuse spatial features more effectively. First, LMF-Net extracts features from the images collected by UAV and obtains three multi-scale feature maps. Second, EENet performs feature fusion on three feature maps of different scales to obtain three feature representatives. Finally, the decoupled head detects the feature map and outputs the final result. The bounding box regression loss function uses Wasserstein distance to evaluate box similarity and enhance the model’s sensitivity to small targets. The experimental results demonstrate that on the VisDrone dataset, mAP50-95 of LMF-UAV reaches 24.6%, while parameters are only 14.7M, FLOPs are only 61.8G, showing a good balance between performance and efficiency.},
  archive      = {J_PAAA},
  author       = {Yang, Wenyuan and He, Qihan and Li, Zhongxu},
  doi          = {10.1007/s10044-024-01389-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A lightweight multidimensional feature network for small object detection on UAVs},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilize trajectory information for small target
classification. <em>PAAA</em>, <em>28</em>(1), 1–9. (<a
href="https://doi.org/10.1007/s10044-024-01397-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small target recognition has always been challenging in image processing systems. When targets are far from the camera, target features tend to have low quality, limiting the amount of useful information for detection systems. Consequently, classic Detection Before Tracking (DBT) algorithms find great difficulty in separating targets from their background based on their visual properties. In this study, we proposed a Track Before Detect (TBD) approach that tracks potential targets in multiple frames, reducing the false alarm rate and enhancing the detection robustness to clutter. Then, we utilize target trajectory information to distinguish actual targets from any background noise. The proposed approach reframes the classic target image classification challenge to a multivariate time series classification problem, using target trajectory coordinates (x, y) as features. The proposed approach achieved a remarkable 97% accuracy in classifying targets from noise using only ten data points (half a second of tracking). Furthermore, it successfully classified targets into specific categories (airplane, drone, bird) with a 96% accuracy rate over a 1.5 s window (30 data points).},
  archive      = {J_PAAA},
  author       = {Alkentar, Saad and Assalem, Abdulkarim and Alsahwa, Bassem},
  doi          = {10.1007/s10044-024-01397-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Utilize trajectory information for small target classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved nearest neighbour classifier. <em>PAAA</em>,
<em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01399-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A windowed version of the Nearest Neighbour (WNN) classifier for images is described. While its construction is inspired by the architecture of Artificial Neural Networks, the underlying theoretical framework is based on approximation theory. We illustrate WNN on the datasets MNIST and EMNIST of images of handwritten digits. In order to calibrate the parameters of WNN, we first study it on MNIST. We then apply WNN with these parameters to EMNIST resulting in an error rate of 0.76% which significantly outperforms traditional classification methods like Support Vector Machines. By expansions of the training set, an error rate down to 0.42% is achieved.},
  archive      = {J_PAAA},
  author       = {Setterqvist, Eric and Kruglyak, Natan and Forchheimer, Robert},
  doi          = {10.1007/s10044-024-01399-1},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An improved nearest neighbour classifier},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture discrimination via hilbert curve path based
information quantifiers. <em>PAAA</em>, <em>28</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s10044-024-01400-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of the spatial arrangement of colors and roughness/smoothness of figures is relevant due to its wide range of applications. This paper proposes a texture characterization method that extracts data from images using the Hilbert curve. Three information theory quantifiers are then computed: permutation entropy, permutation complexity, and Fisher information measure. The proposal exhibits some important properties: (i) it allows discrimination between figures according to varying degrees of correlations (as measured by the Hurst exponent), (ii) it is invariant to rotation and symmetry transformations, (iii) it is invariant to image scaling, (iv) it can be used for both black and white and color images. Validations have been performed not only using synthetic images but also using the well-known Brodatz image database.},
  archive      = {J_PAAA},
  author       = {Bariviera, Aurelio F. and Hansen, Roberta and Pastor, Verónica E.},
  doi          = {10.1007/s10044-024-01400-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Texture discrimination via hilbert curve path based information quantifiers},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A light-weight backbone to adapt with extracting grouped
dilation features. <em>PAAA</em>, <em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01401-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing grouped dilation features (GDFs) improved the learning ability of MobileNetV1 in image representation. However, the computational complexity is still at a high level, while the performance is a modest degree. This expensive cost is principally caused by the backbone of MobileNetV1 taking deep feature maps in several latest layers. To mitigate these issues, we propose a light-weight network (called CGDF-Net) with an adaptative architecture to effectively extract grouped dilation features. CGDF-Net is structured by two main contributions: (i) Its backbone is improved by simply replacing several latest layers of MobileNetV1 with a pointwise convolutional layer for reducing the computational complexity; (ii) Embedding an attention mechanism into the GDF block to form a completed GDF perceptron (CGDF) that directs the learning process into the significant properties of objects in images instead of the trivial ones. Experimental results on benchmark datasets for image recognition have validated that the proposed CGDF-Net network obtained good performance with a small computational cost in comparison with MobileNets and other light-weight models. For instance, CGDF-Net obtained 60.86% with 3.53M learnable parameters on Stanford Dogs, up to 6% better than MoblieNetV1-GDF (54.9%, 3.39M) and 9% versus MoblieNetV1 (51.6%, 3.33M). Meantime, the performance of CGDF-Net on ImageNet-100 is 85.22%, about 6% $$\sim$$ 8% higher than MobileNetV1-GDF’s (79.14%) and MobileNetV1’s (77.01%), respectively. The code of CGDF-Net is available at https://github.com/nttbdrk25/CGDFNet .},
  archive      = {J_PAAA},
  author       = {Nguyen, Thanh Tuan and Pham, Hoang Anh and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10044-024-01401-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A light-weight backbone to adapt with extracting grouped dilation features},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMIHCT: Improved multi-stage image inpainting with hybrid
CNN and transformer. <em>PAAA</em>, <em>28</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s10044-024-01402-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at problems such as the poor effect of large-hole image inpainting, the limitation of local information reconstruction of the convolutional neural network, and a surge in parameters caused by stacking a large number of convolutional modules in the network model. We make full use of the advantages of convolutional neural network and transformer and propose an improved multi-stage inpainting method with hybrid CNN and transformer. The method achieves a balance between performance and parameters. Specifically, rough results are first generated using a coarse inpainting network with skip connections and lightweight Taylor transformer modules. Then, a local refinement network with coordinate attention is used to perform local refinement, optimizing local details while weakening the influence of unreasonable content in the distance. Finally, to compensate for the inability of local refinement networks to refine the overall pattern over long distances, global refinement is performed using a network that is consistent with the structure of the coarse inpainting network to make the reconstructed image more realistic and natural. Results show that the method outperforms the state of the arts on three publicly available datasets. The code is made available at https://github.com/Sheeran2000/IMIHCT .},
  archive      = {J_PAAA},
  author       = {Ning, Tao and Wang, Xingfang and Ding, Hongwei},
  doi          = {10.1007/s10044-024-01402-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {IMIHCT: Improved multi-stage image inpainting with hybrid CNN and transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMAM-NET: Ultrasound thyroid nodule malignancy grading
network based on multi-subnet attention mechanism. <em>PAAA</em>,
<em>28</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s10044-024-01404-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and Objectives: Thyroid nodules are one of the most common thyroid diseases, and their incidence has been on the rise in recent years. Ultrasound imaging, due to its low cost and no ionizing radiation, has become the preferred method for imaging thyroid nodules. Accurate assessment of the malignancy grade of thyroid nodules is crucial to ensure the accuracy of subsequent examination and treatment. Texture and shape are key features for determining the nature of thyroid nodules. Despite the excellent performance of convolutional neural networks (CNNs) in image feature extraction and aggregation, the low resolution and high noise characteristics of ultrasound images still pose challenges for existing CNN models in identifying texture and shape. Methods: To address this challenge, we propose a thyroid nodule malignancy grading network based on a multi-subnet attention mechanism (UMAM-NET). In the feature extraction stage, we innovatively introduce the multi-subnet attention module. The module designs two parallel subnets, aiming to enhance the model’s attention to the texture and shape of thyroid nodules. Results: Compared to other deep learning models, the proposed UMAM-NET performs better in the malignant grading task of thyroid nodules. It demonstrates excellent performance on public datasets, achieving the best results in Recall (93.1%), F1-score (95.4%), and Accuracy (98.4%). Similarly, it also shows outstanding performance on the sub-collected dataset, with Recall (91.8%), F1-score (92.0%), and Accuracy (94.4%). Conclusion: Our proposed UMAM-NET, based on multi-subnet attention mechanism, provides a promising approach for accurate assessment of thyroid nodule malignancy grade, which can be of great value in clinical practice.},
  archive      = {J_PAAA},
  author       = {Bi, Hui and Wang, Fan and Xiong, YuHao and Dong, ZhaoHui and Jiang, Yibo and Zhao, Tong and Zheng, Yineng},
  doi          = {10.1007/s10044-024-01404-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {UMAM-NET: Ultrasound thyroid nodule malignancy grading network based on multi-subnet attention mechanism},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSFE-YOLO: A traffic sign detection algorithm with
pixel-wise spatial feature enhancement. <em>PAAA</em>, <em>28</em>(1),
1–15. (<a href="https://doi.org/10.1007/s10044-024-01406-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection is essentially important for intelligent driving. Existing detection algorithms typically incorporate self-attention mechanisms to model the dependencies among image elements, such as patches or pixels. When using a patch as a token, the positional information within the patch could be lost. When using a pixel as a token, an increase in the number of tokens can lead to a significant increase in computational complexity. To balance these two extreme situations, a pixel only needs to focus on pixels from the surrounding area. Therefore, we propose a local attention module termed Pixel-wise Spatial Feature Enhancement (PSFE), which uses pixels as tokens to enhance the spatial information of feature maps, and each pixel’s self-attention only acts on a local region to reduce computational complexity. Furthermore, we design a Bidirectional Res2Net (BR) module that generates multiple feature maps with different channel numbers from an input feature map, and then restores them to one feature map with the original input size through bidirectional fusion, greatly enriching the receptive field information contained in the feature map. We conducted experiments on the GTSDB, TT100K, and CCTSDB 2021 datasets to comprehensively evaluate our method, and the experimental results showed that our method has superior performance.},
  archive      = {J_PAAA},
  author       = {Zhang, Jianming and Wang, Zulou and Yi, Yao and Kuang, Li-Dan and Zhang, Jin},
  doi          = {10.1007/s10044-024-01406-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PSFE-YOLO: A traffic sign detection algorithm with pixel-wise spatial feature enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ptrf---12">PTRF - 12</h2>
<ul>
<li><details>
<summary>
(2025). The dynamical ising-kac model in 3D converges to <span
class="math display"><em>Φ</em><sub>3</sub><sup>4</sup></span>.
<em>PTRF</em>, <em>191</em>(1), 671–778. (<a
href="https://doi.org/10.1007/s00440-024-01316-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the Glauber dynamics of a ferromagnetic Ising-Kac model on a three-dimensional periodic lattice of size $$(2 N + 1)^3$$ , in which the flipping rate of each spin depends on an average field in a large neighborhood of radius $$\gamma ^{-1}&lt;\!\!&lt; N$$ . We study the random fluctuations of a suitably rescaled coarse-grained spin field as $$N \rightarrow \infty $$ and $$\gamma \rightarrow 0$$ ; we show that near the mean-field value of the critical temperature, the process converges in distribution to the solution of the dynamical $$\Phi ^4_3$$ model on a torus. Our result settles a conjecture from Giacomin et al. (1999). The dynamical $$\Phi ^4_3$$ model is given by a non-linear stochastic partial differential equation (SPDE) which is driven by an additive space-time white noise and which requires renormalisation of the non-linearity. A rigorous notion of solution for this SPDE and its renormalisation is provided by the framework of regularity structures (Hairer in Invent Math 198(2):269–504, 2014. https://doi.org/10.1007/s00222-014-0505-4 ). As in the two-dimensional case (Mourrat and Weber in Commun Pure Appl Math 70(4):717–812, 2017), the renormalisation corresponds to a small shift of the inverse temperature of the discrete system away from its mean-field value.},
  archive      = {J_PTRF},
  author       = {Grazieschi, P. and Matetski, K. and Weber, H.},
  doi          = {10.1007/s00440-024-01316-x},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {671-778},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {The dynamical ising-kac model in 3D converges to $$\Phi ^4_3$$},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative stochastic homogenization for random
conductance models with stable-like jumps. <em>PTRF</em>,
<em>191</em>(1), 627–669. (<a
href="https://doi.org/10.1007/s00440-024-01354-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider random conductance models with long range jumps on $$\mathbb {Z}^d$$ , where the one-step transition probability from x to y is proportional to $$w_{x,y}|x-y|^{-d-\alpha }$$ with $$\alpha \in (0,2)$$ . Assume that $$\{w_{x,y}\}_{(x,y)\in E}$$ are independent, identically distributed and uniformly bounded non-negative random variables with $$\mathbb {E}w_{x,y}=1$$ , where E is the set of all unordered pairs on $$\mathbb {Z}^d$$ . We obtain a quantitative version of stochastic homogenization for these random walks, with explicit polynomial rates up to logarithmic corrections.},
  archive      = {J_PTRF},
  author       = {Chen, Xin and Chen, Zhen-Qing and Kumagai, Takashi and Wang, Jian},
  doi          = {10.1007/s00440-024-01354-5},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {627-669},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Quantitative stochastic homogenization for random conductance models with stable-like jumps},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluctuations in the logarithmic energy for zeros of random
polynomials on the sphere. <em>PTRF</em>, <em>191</em>(1), 569–626. (<a
href="https://doi.org/10.1007/s00440-024-01334-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smale’s Seventh Problem asks for an efficient algorithm to generate a configuration of n points on the sphere that nearly minimizes the logarithmic energy. As a candidate starting configuration for this problem, Armentano, Beltrán and Shub considered the set of points given by the stereographic projection of the roots of the random elliptic polynomial of degree n and computed the expected logarithmic energy. We study the fluctuations of the logarithmic energy associated to this random configuration and prove a central limit theorem. Our approach shows that all cumulants of the logarithmic energy are asymptotically linear in n, and hence the energy is well-concentrated on the scale of $$\sqrt{n}$$ .},
  archive      = {J_PTRF},
  author       = {Michelen, Marcus and Yakir, Oren},
  doi          = {10.1007/s00440-024-01334-9},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {569-626},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Fluctuations in the logarithmic energy for zeros of random polynomials on the sphere},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularity of laws via dirichlet forms: Application to
quadratic forms in independent and identically distributed random
variables. <em>PTRF</em>, <em>191</em>(1), 523–567. (<a
href="https://doi.org/10.1007/s00440-024-01332-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the regularity of the law of a quadratic form Q(X, X), evaluated in a sequence $$X = (X_{i})$$ of independent and identically distributed random variables, when $$X_{1}$$ can be expressed as a sufficiently smooth function of a Gaussian field. This setting encompasses a large class of important and frequently used distributions, such as, among others, Gaussian, Beta, for instance uniform, Gamma distributions, or else any polynomial transform of them. Let us present an emblematic application. Take $$X = (X_{i})$$ a sequence of independent and identically distributed centered random variables, with unit variance, following such distribution. Consider also $$(Q_{n})$$ a sequence of quadratic forms, with associated symmetric Hilbert–Schmidt operators $$({\textsf{A}}^{(n)})$$ . Assume that $${{\,\textrm{Tr}\,}}[ ({\textsf{A}}^{(n)})^{2} ] = 1/2$$ , $${\textsf{A}}^{(n)}_{ii} =0$$ , and the spectral radius of $${\textsf{A}}^{(n)}$$ tends to 0. Then, $$(Q_{n}(X))$$ converges in a strong sense to the standard Gaussian distribution. Namely, all derivatives of the densities, which are well-defined for n sufficiently large, converge uniformly on $$\mathbb {R}$$ to the corresponding derivatives of the standard Gaussian density. While classical methods, from Malliavin calculus or $$\Gamma $$ -calculus, generally consist in bounding negative moments of the so-called carré du champ operator $$\Gamma (Q(X),Q(X))$$ , we provide a new paradigm through a second-order criterion involving the eigenvalues of a Hessian-type matrix related to Q(X). This Hessian is built by iterating twice a tailor-made gradient, the sharp operator $$\sharp $$ , obtained via a Gaussian representation of the carré du champ. We believe that this method, recently developed by the authors in the current paper and Herry et al. (Ann Probab 52(3):1162–1200, 2024), is of independent interest and could prove useful in other settings.},
  archive      = {J_PTRF},
  author       = {Herry, Ronan and Malicet, Dominique and Poly, Guillaume},
  doi          = {10.1007/s00440-024-01332-x},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {523-567},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Regularity of laws via dirichlet forms: Application to quadratic forms in independent and identically distributed random variables},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of three-dimensional loop-erased random walk in
the natural parametrization. <em>PTRF</em>, <em>191</em>(1), 421–521.
(<a href="https://doi.org/10.1007/s00440-024-01338-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we consider loop-erased random walk (LERW) and its scaling limit in three dimensions, and prove that 3D LERW parametrized by renormalized length converges to its scaling limit parametrized by some suitable measure with respect to the uniform convergence topology in the lattice size scaling limit. Our result greatly improves the work (Kozma in Acta Math 199:29-152, 2007) of Gady Kozma which establishes the weak convergence of the rescaled trace of 3D LERW towards a random compact set with respect to the Hausdorff distance.},
  archive      = {J_PTRF},
  author       = {Li, Xinyi and Shiraishi, Daisuke},
  doi          = {10.1007/s00440-024-01338-5},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {421-521},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Convergence of three-dimensional loop-erased random walk in the natural parametrization},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From ABC to KPZ. <em>PTRF</em>, <em>191</em>(1), 361–420.
(<a href="https://doi.org/10.1007/s00440-024-01314-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the equilibrium fluctuations of an interacting particle system evolving on the discrete ring with $$N\in {\mathbb {N}}$$ points, denoted by $${\mathbb {T}}_N$$ , and with three species of particles that we name A, B and C, but such that at each site there is only one particle. We prove that proper choices of density fluctuation fields (that match those from nonlinear fluctuating hydrodynamics theory) associated to the (two) conserved quantities converge, in the limit $$N\rightarrow \infty $$ , to a system of stochastic partial differential equations, that can either be the Ornstein–Uhlenbeck equation or the Stochastic Burgers equation. To understand the cross interaction between the two conserved quantities, we derive a general version of the Riemann–Lebesgue lemma which is of independent interest.},
  archive      = {J_PTRF},
  author       = {Cannizzaro, G. and Gonçalves, P. and Misturini, R. and Occelli, A.},
  doi          = {10.1007/s00440-024-01314-z},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {361-420},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {From ABC to KPZ},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The fuzzy potts model in the plane: Scaling limits and arm
exponents. <em>PTRF</em>, <em>191</em>(1), 287–359. (<a
href="https://doi.org/10.1007/s00440-024-01319-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a critical Fortuin–Kasteleyn (FK) percolation with cluster weight $$q \in [1,4)$$ in the plane, and color its clusters in red (respectively blue) with probability $$r \in (0,1)$$ (respectively $$1-r$$ ), independently of each other. We study the resulting fuzzy Potts model, which corresponds to the critical Ising model in the special case $$q=2$$ and $$r=1/2$$ . We show that under the assumption that the critical FK percolation converges to a conformally invariant scaling limit (which is known to hold for the FK-Ising model,i.e. $$q=2$$ ), the obtained coloring converges to variants of Conformal Loop Ensembles constructed, described and studied by Miller, Sheffield and Werner. Based on discrete considerations, we also show that the arm exponents for this coloring in the discrete model are identical to the ones of the continuum model. Using the values of these arm exponents in the continuum, we determine the arm exponents for the fuzzy Potts model.},
  archive      = {J_PTRF},
  author       = {Köhler-Schindler, Laurin and Lehmkuehler, Matthis},
  doi          = {10.1007/s00440-024-01319-8},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {287-359},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {The fuzzy potts model in the plane: Scaling limits and arm exponents},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geodesic lévy flights and expected stopping time for random
searches. <em>PTRF</em>, <em>191</em>(1), 235–285. (<a
href="https://doi.org/10.1007/s00440-024-01327-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give an analytic description for the infinitesimal generator constructed in Applebaum and Estrade (Ann Probab 28(1):166-184, 2000) for Lévy flights on a broad class of closed Riemannian manifolds including all negatively-curved manifolds, the flat torus and the sphere. Various properties of the associated semigroup and the asymptotics of the expected stopping time for Lévy flight based random searches for small targets, also known as the “narrow capture problem&quot;, are then obtained using our newfound understanding of the infinitesimal generator. Our study also relates to the Lévy flight foraging hypothesis in the field of biology as we compute the expected time for finding a small target by using the Lévy flight random search. Compared to the random search time for Brownian motion on surfaces done in Nursultanov et al. ( arXiv:2209.12425 ), our result suggests that Lévy flight may not always be the optimal strategy, consistent with the conclusion obtained in Palyulin et al. (Proc Natl Acad Sci 111(8):2931-2936, 2014) for the one dimensional case.},
  archive      = {J_PTRF},
  author       = {Chaubet, Yann and Bonthonneau, Yannick Guedes and Lefeuvre, Thibault and Tzou, Leo},
  doi          = {10.1007/s00440-024-01327-8},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {235-285},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Geodesic lévy flights and expected stopping time for random searches},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equivalence of approximate message passing and low-degree
polynomials in rank-one matrix estimation. <em>PTRF</em>,
<em>191</em>(1), 181–233. (<a
href="https://doi.org/10.1007/s00440-024-01322-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating an unknown parameter vector $${\varvec{\theta }}\in {{\mathbb {R}}}^n$$ , given noisy observations $${{\varvec{Y}}}= {\varvec{\theta }}{\varvec{\theta }}^{\textsf{T}}/\sqrt{n}+{{\varvec{Z}}}$$ of the rank-one matrix $${\varvec{\theta }}{\varvec{\theta }}^{\textsf{T}}$$ , where $${{\varvec{Z}}}$$ has independent Gaussian entries. When information is available about the distribution of the entries of $${\varvec{\theta }}$$ , spectral methods are known to be strictly sub-optimal. Past work characterized the asymptotics of the accuracy achieved by the optimal estimator. However, no polynomial-time estimator is known that achieves this accuracy. It has been conjectured that this statistical-computation gap is fundamental, and moreover that the optimal accuracy achievable by polynomial-time estimators coincides with the accuracy achieved by certain approximate message passing (AMP) algorithms. We provide evidence towards this conjecture by proving that no estimator in the (broader) class of constant-degree polynomials can surpass AMP.},
  archive      = {J_PTRF},
  author       = {Montanari, Andrea and Wein, Alexander S.},
  doi          = {10.1007/s00440-024-01322-z},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {181-233},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Equivalence of approximate message passing and low-degree polynomials in rank-one matrix estimation},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric bayesian estimation in a multidimensional
diffusion model with high frequency data. <em>PTRF</em>,
<em>191</em>(1), 103–180. (<a
href="https://doi.org/10.1007/s00440-024-01317-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nonparametric Bayesian inference in a multidimensional diffusion model with reflecting boundary conditions based on discrete high-frequency observations. We prove a general posterior contraction rate theorem in $$L^2$$ -loss, which is applied to Gaussian priors. The resulting posteriors, as well as their posterior means, are shown to converge to the ground truth at the minimax optimal rate over Hölder smoothness classes in any dimension. Of independent interest and as part of our proofs, we show that certain frequentist penalized least squares estimators are also minimax optimal.},
  archive      = {J_PTRF},
  author       = {Hoffmann, Marc and Ray, Kolyan},
  doi          = {10.1007/s00440-024-01317-w},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {103-180},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Nonparametric bayesian estimation in a multidimensional diffusion model with high frequency data},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rearranged stochastic heat equation. <em>PTRF</em>,
<em>191</em>(1), 41–102. (<a
href="https://doi.org/10.1007/s00440-024-01335-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this work is to provide an explicit construction of a strong Feller semigroup on the space of probability measures over the real line that additionally maps bounded measurable functions into Lipschitz continuous functions, with a Lipschitz constant that blows up in an integrable manner in small time. Our construction relies on a rearranged version of the stochastic heat equation on the circle driven by a coloured noise. Formally, this stochastic equation writes as a reflected equation in infinite dimension. Under the action of the rearrangement, the solution is forced to live in a space of quantile functions that is isometric to the space of probability measures on the real line. We prove the equation to be solvable by means of an Euler scheme in which we alternate flat dynamics in the space of random variables on the circle with a rearrangement operation that projects back the random variables onto the subset of quantile functions. A first challenge is to prove that this scheme is tight. A second one is to provide a consistent theory for the limiting reflected equation and in particular to interpret in a relevant manner the reflection term. The last step in our work is to establish the aforementioned Lipschitz property of the semigroup by adapting earlier ideas from the Bismut–Elworthy–Li formula.},
  archive      = {J_PTRF},
  author       = {Delarue, François and Hammersley, William R. P.},
  doi          = {10.1007/s00440-024-01335-8},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {41-102},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Rearranged stochastic heat equation},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local central limit theorem for gradient field models.
<em>PTRF</em>, <em>191</em>(1), 1–40. (<a
href="https://doi.org/10.1007/s00440-024-01330-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the gradient field model in $$\left[ -N,N\right] ^{2}\cap {\mathbb {Z}}^{2}$$ with a uniformly convex interaction potential. Naddaf–Spencer (Comm Math Phys 183(1):55–84, 1997) and Miller (Comm Math Phys 908(3):591–639, 2011) proved that the macroscopic averages of linear statistics of the field converge to a continuum Gaussian free field. In this paper we prove the distribution of $$\phi (0)/\sqrt{\log N}$$ converges uniformly in $${\mathbb {R}}$$ to a Gaussian density, with a Berry-Esseen type bound. This implies the distribution of $$\phi (0)$$ is sufficiently ‘Gaussian like’ between $$[-\sqrt{\log N}, \sqrt{\log N}]$$ .},
  archive      = {J_PTRF},
  author       = {Wu, Wei},
  doi          = {10.1007/s00440-024-01330-z},
  journal      = {Probability Theory and Related Fields},
  month        = {2},
  number       = {1},
  pages        = {1-40},
  shortjournal = {Probab. Theory Relat. Fields},
  title        = {Local central limit theorem for gradient field models},
  volume       = {191},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="sac---29">SAC - 29</h2>
<ul>
<li><details>
<summary>
(2025). Penalized empirical likelihood estimation and EM algorithms
for closed-population capture–recapture models. <em>SAC</em>,
<em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-024-10557-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capture–recapture experiments are widely used to estimate the abundance of a finite population. Based on capture–recapture data, the empirical likelihood (EL) method has been shown to outperform the conventional conditional likelihood (CL) method. However, the current literature on EL abundance estimation ignores behavioral effects, and the EL estimates may not be stable, especially when the capture probability is low. We make three contributions in this paper. First, we extend the EL method to capture–recapture models that account for behavioral effects. Second, to overcome the instability of the EL method, we propose a penalized EL (PEL) estimation method that penalizes large abundance values. We then investigate the asymptotics of the maximum PEL estimator and the PEL ratio statistic. Third, we develop standard expectation–maximization (EM) algorithms for PEL to improve its practical performance. The EM algorithm is also applicable to EL and CL with slight modifications. Our simulation and a real-world data analysis demonstrate that the PEL method successfully overcomes the instability of the EL method and the proposed EM algorithm produces more reliable results than existing optimization algorithms.},
  archive      = {J_SAC},
  author       = {Liu, Yang and Li, Pengfei and Liu, Yukun},
  doi          = {10.1007/s11222-024-10557-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Penalized empirical likelihood estimation and EM algorithms for closed-population capture–recapture models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal distributed subsampling under heterogeneity.
<em>SAC</em>, <em>35</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-024-10558-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed subsampling approaches have been proposed to process massive data in a distributed computing environment, where subsamples are taken from each site and then analyzed collectively to address statistical problems when the full data is not available. In this paper, we consider that each site involves a common parameter and site-specific nuisance parameters and then formulate a unified framework of optimal distributed subsampling under heterogeneity for general optimization problems with convex loss functions that could be nonsmooth. By establishing the consistency and asymptotic normality of the distributed subsample estimators for the common parameter of interest, we derive the optimal subsampling probabilities and allocation sizes under the A- and L-optimality criteria. A two-step algorithm is proposed for practical implementation and the asymptotic properties of the resultant estimator are established. For nonsmooth loss functions, an alternating direction method of multipliers method and a random perturbation procedure are proposed to obtain the subsample estimator and estimate the covariance matrices for statistical inference, respectively. The finite-sample performance of linear regression, logistic regression and quantile regression models is demonstrated through simulation studies and an application to the National Longitudinal Survey of Youth Dataset is also provided.},
  archive      = {J_SAC},
  author       = {Shao, Yujing and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-024-10558-7},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Optimal distributed subsampling under heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCA-uCPD: An ensemble method for multiple change-point
detection in moderately high-dimensional data. <em>SAC</em>,
<em>35</em>(2), 1–17. (<a
href="https://doi.org/10.1007/s11222-024-10553-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-point detection (CPD) receives extensive studies due to its wide applications in various fields. However, CPD remains a challenging problem for complex data with medium or high dimensions, high correlations, outliers or heavy-tailed distribution. This article proposes an integrated change-point detection method called PCA-uCPD, which utilizes principal components analysis (PCA) to project the original data series into uncorrelated principal components (PCs). Subsequently, we apply existing univariate change-point detection methods to the mapped PCs, followed by a proposed refining technique to obtain the ultimate change-point estimates for the original data sequences. The proposed method admits a flexible architecture that is thus capable of dealing with complex data. Theoretical justifications have been provided to guarantee the feasibility of the proposed methods. Moreover, we conduct simulations to assess performance across various data-generating scenarios. The efficacy of PCA-uCPD is further demonstrated through applications in both genetic and financial datasets.},
  archive      = {J_SAC},
  author       = {Qin, Shanshan and Tan, Zhenni and Wei, Weidong and Wu, Yuehua},
  doi          = {10.1007/s11222-024-10553-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power enhancing probability subsampling using side
information. <em>SAC</em>, <em>35</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10556-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the subsampling technique for hypothesis testing in generalized linear models with large-scale datasets, focusing on testing simple null hypotheses against composite linear alternatives. We propose a subsample-based test statistic and show that it converges to non-central chi-square distributions under Pitman’s local alternatives. The optimal subsampling distribution that maximizes power requires iterative calculations on the full data, which is computationally infeasible. Furthermore, it depends on the true parameter, which cannot be consistently estimated under Pitman’s local alternatives. We maximize a lower bound of the non-central parameter to define the power enhancing probability and utilize side information under the alternative to replace the true parameter. Extensive simulations and an application to a real dataset on flight delays and cancellations show that the proposed method offers a computationally viable solution for hypothesis testing in the realm of big data.},
  archive      = {J_SAC},
  author       = {Gao, Junzhuo and Wang, Lei and Wang, Haiying},
  doi          = {10.1007/s11222-024-10556-9},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Power enhancing probability subsampling using side information},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference issue in multiscale geographically and temporally
weighted regression. <em>SAC</em>, <em>35</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-024-10559-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically and temporally weighted regression (GTWR) models assume that all of the varying coefficients operate at a same spatiotemporal scale, which reduces the flexibility of local regression models. Multiscale geographically and temporally weighted regression (MGTWR) models increase flexibility, enhance interpretability, and provide more comprehensive information by allowing regression coefficients to vary across different spatiotemporal scales. However, a limitation of the MGTWR models is that, to date, statistical inference regarding the local coefficient estimates has not been feasible. Formally, “hat matrix”, which projects the observed response variable vector onto the fitting response variable, was not available in the MGTWR model. This paper settles this limitation by reformulating the GTWR model into a Generalized Additive Model, extending this framework to the MGTWR model and then deriving standard deviations for the local coefficient estimates in the MGTWR model. In addition, we also obtain the number of effective parameters for the overall fit of the MGTWR model and for each covariate within the model. This statistic is crucial for comparing the goodness of fit between MGTWR, GTWR and classical global regression models, as well as for adjusting multiple tests. Simulation studies and a real-world example demonstrate these advances to the MGTWR framework.},
  archive      = {J_SAC},
  author       = {Hong, Zhimin and Wang, Ruoxuan and Wang, Zhiwen and Du, Wala},
  doi          = {10.1007/s11222-024-10559-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Inference issue in multiscale geographically and temporally weighted regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constrained least squares simplicial-simplicial regression.
<em>SAC</em>, <em>35</em>(2), 1–16. (<a
href="https://doi.org/10.1007/s11222-024-10560-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model’s advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The strategy behind the formulation of the new model implemented is related to an existing methodology, that is of the same spirit, showcasing how other similar models can be employed as well. Finally, the performance of the proposed technique and its comparison to the existing methodology takes place using simulation studies and real data examples.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail},
  doi          = {10.1007/s11222-024-10560-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Constrained least squares simplicial-simplicial regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical investigations of boosting with pseudo-outcome
imputation for missing responses. <em>SAC</em>, <em>35</em>(2), 1–18.
(<a href="https://doi.org/10.1007/s11222-024-10561-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting techniques have gained increasing interest in both machine learning and statistical research. However, many of these methods are primarily designed for complete datasets, which limits their applicability to handle incomplete data such as missing observations. In this paper, we propose the pseudo-outcome strategy to account for missingness effects and describe a functional gradient descent algorithm. Numerical studies demonstrate the satisfactory performance of the proposed method in finite sample settings. Furthermore, we apply the proposed method to analyze the KLIPS Data.},
  archive      = {J_SAC},
  author       = {Bian, Yuan and Yi, Grace Y. and He, Wenqing},
  doi          = {10.1007/s11222-024-10561-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Empirical investigations of boosting with pseudo-outcome imputation for missing responses},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of the modality and flexibility of the inverse
stereographic normal distribution. <em>SAC</em>, <em>35</em>(2), 1–12.
(<a href="https://doi.org/10.1007/s11222-025-10563-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular data arises in various fields including robotics, biology, geology and material sciences. Modelling such data requires flexible distribution families on the hypertorus. Common choices are the von Mises and the wrapped normal distributions. In this work we investigate the inverse stereographic normal distribution as an alternative distribution family both from a theoretical and applied perspective. We first generalize unimodality results to arbitrary dimensions by proving that the inverse stereographic normal distribution is unimodal if and only if all eigenvalues of the covariance matrix are less than or equal to 0.5. We then analyze the flexibility by fitting mixtures of shifted inverse stereographic normal distributions via gradient descent to several examples of toroidal data.},
  archive      = {J_SAC},
  author       = {Hinz, Florian B. and Mahmoud, Amr H. and Lill, Markus A.},
  doi          = {10.1007/s11222-025-10563-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {An analysis of the modality and flexibility of the inverse stereographic normal distribution},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic post-selection inference for regularized
graphical models. <em>SAC</em>, <em>35</em>(2), 1–30. (<a
href="https://doi.org/10.1007/s11222-025-10564-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotically valid inference is obtained for graphical model edge parameters after selection using the same data set as the one used for inference. We consider Gaussian and (trans)elliptical graphical models, where the edge selection and consequent sparse estimation is operated by applying the $$\ell _1$$ , elastic net, smoothly clipped absolute deviation, or minimax concave penalty to an appropriately regular loss function. The polyhedral lemma is used to carry out conditional inference which is asymptotically valid in the (possibly wrong) selected graphical model. Simulation studies show how the method yields valid inference in a variety of finite-sample settings.},
  archive      = {J_SAC},
  author       = {Guglielmini, Sofia and Claeskens, Gerda},
  doi          = {10.1007/s11222-025-10564-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Asymptotic post-selection inference for regularized graphical models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient modeling of quasi-periodic data with seasonal
gaussian process. <em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10565-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-periodicity refers to a pattern in a function where it appears periodic at a certain frequency but exhibits evolving amplitudes over time. This is often the case in practical settings such as the modeling of case counts of infectious disease or the population dynamics of species over time. In this paper, we consider a class of Gaussian processes, called seasonal Gaussian Processes (sGP), for model-based inference of such quasi-periodic behavior. We illustrate that the exact sGP can be efficiently fitted using its state space representation for equally spaced time points. However, for large datasets with irregular spacing, the exact approach becomes computationally inefficient and unstable. To address this, we develop a continuous finite dimensional approximation for sGP using the seasonal B-spline (sB-spline) basis constructed by damping B-splines with sinusoidal functions. We prove the covariance convergence rate of the proposed approximation to the true sGP as the number of basis functions increases, and show its superior approximation quality through numerical studies. We also provide a unified and interpretable way to define priors for the sGP, based on the notion of predictive standard deviation. Finally, we implement the proposed inference method on several real data examples to illustrate its practical usage.},
  archive      = {J_SAC},
  author       = {Zhang, Ziang and Brown, Patrick and Stafford, Jamie},
  doi          = {10.1007/s11222-025-10565-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Efficient modeling of quasi-periodic data with seasonal gaussian process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy k-expectiles clustering. <em>SAC</em>, <em>35</em>(2),
1–19. (<a href="https://doi.org/10.1007/s11222-025-10566-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the Fuzzy K-expectiles clustering model is proposed. The model takes into account the asymmetry inherent in the data distribution, extending its applicability to a broader spectrum of data than the Fuzzy K-means. To achieve this, the Fuzzy K-expectiles clustering model introduces the cluster centroid expectile, and assigns data points based on expectile distances. An adaptive asymmetry parameter is specified for each variable and for each cluster The performance of the adaptive Fuzzy K-expectiles model is compared with other clustering models suggested in the literature. To show the performances of the proposed model three simulation studies and three applications to real datasets are presented.},
  archive      = {J_SAC},
  author       = {D’Urso, Pierpaolo and De Giovanni, Livia and Federico, Lorenzo and Vitale, Vincenzina},
  doi          = {10.1007/s11222-025-10566-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Fuzzy K-expectiles clustering},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geographically weighted quantile regression for count data.
<em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10568-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a methodological framework known as geographically weighted quantile regression (GWQR) has emerged for spatial data analysis. This framework offers the abilities to simultaneously explore spatial heterogeneity or nonstationarity in regression relationships and to estimate various conditional quantile functions. However, the current configuration of GWQR is limited to the analysis of continuous dependent variables. Discrete count data are observed in many disciplines. Whenever modeling such outcomes is necessary, the conventional GWQR approach is inadequate and fails to provide comprehensive insights into count data. To address this issue, this study aims to extend the GWQR framework originally designed for continuous dependent variables to accommodate count outcomes. We introduce an approach called geographically weighted count quantile regression (GWCQR), wherein the model specification is based on the smoothing of count responses through a jittering procedure. A semiparametric counterpart that allows for the inclusion of both spatially varying and invariant coefficients is also discussed. Finally, the proposed techniques are applied to a dataset of dengue fever in Taiwan as an empirical illustration.},
  archive      = {J_SAC},
  author       = {Chen, Vivian Yi-Ju and Wang, Shi-Ting},
  doi          = {10.1007/s11222-025-10568-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Geographically weighted quantile regression for count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic multidimensional smoothing using bayesian tensor
product p-splines. <em>SAC</em>, <em>35</em>(2), 1–14. (<a
href="https://doi.org/10.1007/s11222-025-10569-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a highly efficient fully Bayesian approach for anisotropic multidimensional smoothing. The main challenge in this context is the Markov chain Monte Carlo (MCMC) update of the smoothing parameters as their full conditional posterior comprises a pseudo-determinant that appears to be intractable at first sight. As a consequence, existing implementations are computationally feasible only for the estimation of two-dimensional tensor product smooths, which is, however, too restrictive for many applications. In this paper, we break this barrier and derive closed-form expressions for the log-pseudo-determinant and its first and second order partial derivatives. These expressions are valid for arbitrary dimension and very fast to evaluate, which allows us to set up an efficient MCMC sampler with derivative-based Metropolis–Hastings (MH) updates for the smoothing parameters. We derive simple formulas for low-dimensional slices and averages to facilitate visualization and investigate hyperprior sensitivity. We show that our new approach outperforms previous suggestions in the literature in terms of accuracy, scalability and computational cost and demonstrate its applicability through an illustrating temperature data example from spatio-temporal statistics.},
  archive      = {J_SAC},
  author       = {Bach, Paul and Klein, Nadja},
  doi          = {10.1007/s11222-025-10569-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Anisotropic multidimensional smoothing using bayesian tensor product P-splines},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logit unfolding choice models for binary data. <em>SAC</em>,
<em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10570-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete choice models with non-monotonic response functions are important in many areas of application, especially political sciences and marketing. This paper describes a novel unfolding model for binary data that allows for heavy-tailed shocks to the underlying utilities. One of our key contributions is a Markov chain Monte Carlo algorithm that requires little or no parameter tuning, fully explores the support of the posterior distribution, and can be used to fit various extensions of our core model that involve (Bayesian) hypothesis testing on the latent construct. Our empirical evaluations of the model and the associated algorithm suggest that they provide better complexity-adjusted fit to voting data from the United States House of Representatives.},
  archive      = {J_SAC},
  author       = {Lei, Rayleigh and Rodriguez, Abel},
  doi          = {10.1007/s11222-025-10570-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Logit unfolding choice models for binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle gibbs for likelihood-free inference of stochastic
volatility models. <em>SAC</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10571-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic volatility models (SVMs) are widely used in finance and econometrics for analyzing and interpreting volatility. Real financial data are often observed to have heavy tails, which violate a Gaussian assumption and may be better modeled using the stable distribution. However, the intractable density of the stable distribution hinders the use of common computational methods such as Markov chain Monte Carlo (MCMC) for parameter inference of SVMs. In this paper, we propose a new particle Gibbs sampler as a strategy to handle SVMs with intractable likelihoods in the approximate Bayesian computation (ABC) setting. The proposed sampler incorporates a conditional auxiliary particle filter, which can help mitigate the weight degeneracy often encountered when using ABC. Simulation studies demonstrate the efficacy of our sampler for inferring SVM parameters when compared to existing particle Gibbs samplers based on the conditional bootstrap filter, and for inferring both SVM and stable distribution parameters when compared to existing particle MCMC samplers. As a real data application, we apply the proposed sampler for fitting an SVM to S&amp;P 500 Index time-series data during the 2008–2009 financial crisis.},
  archive      = {J_SAC},
  author       = {Hou, Zhaoran and Wong, Samuel W. K.},
  doi          = {10.1007/s11222-025-10571-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Particle gibbs for likelihood-free inference of stochastic volatility models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time inference for smoothing quantile regression on
streaming datasets with heterogeneity detection. <em>SAC</em>,
<em>35</em>(2), 1–40. (<a
href="https://doi.org/10.1007/s11222-025-10572-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data is generated at high speed and in large quantities over time, and it calls for online learning to deal with it. In this paper, a new online updating method is established for smoothing quantile regression to make inferences in real-time. The renewable estimators are updated only by the current dataset and summary statistics of historical datasets. This method is adapted to the streaming datasets containing small samples. Theoretically, it is proved that renewable estimators have consistency and asymptotic normality and equivalence to pooled offline estimators based on all datasets. The dynamic bandwidth selection is applied to estimate the asymptotic covariance matrix in an online manner, which is theoretically highly asymptotically efficient. In particular, the renewable estimator provides asymptotic confidence intervals that are asymptotically smaller than those generated by existing methods, thereby improving the accuracy of interval estimation. Additionally, our approach addresses the common assumption of homogeneous models by accommodating non-parametric heterogeneity and detecting and removing anomalous data batches through an online screening process. Meanwhile, numerical simulations verify the theoretical results and outcomes on real datasets illustrate that our method is adapted to real streaming data situations.},
  archive      = {J_SAC},
  author       = {Wang, Yidan and Zhang, Lingyun and Gai, Yujie},
  doi          = {10.1007/s11222-025-10572-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-40},
  shortjournal = {Stat. Comput.},
  title        = {Real-time inference for smoothing quantile regression on streaming datasets with heterogeneity detection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploratory analysis of dynamic networks using latent
functions. <em>SAC</em>, <em>35</em>(2), 1–18. (<a
href="https://doi.org/10.1007/s11222-025-10573-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks, which capture the evolving interactions among entities, have flourished in various scientific fields. We propose a framework for exploratory analysis of these dynamic networks by representing them as latent functions. This framework comprises several visualization tools based on functional data analysis, specifically tailored for addressing typical tasks such as community detection, central node identification, and change point discovery. Besides, we develop a computationally efficient algorithm to obtain the latent functions. Through comprehensive simulation studies conducted under commonly investigated settings, we demonstrate the effectiveness of these tools. Furthermore, we apply the proposed tools to three representative and intriguing real-world networks, yielding enlightening discoveries. An R package for implementing the proposed methods, along with supplementary materials for this article, is available online.},
  archive      = {J_SAC},
  author       = {Shi, Haosheng and Dai, Wenlin},
  doi          = {10.1007/s11222-025-10573-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Exploratory analysis of dynamic networks using latent functions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference and goodness-of-fit test in functional
data via error distribution function. <em>SAC</em>, <em>35</em>(2),
1–16. (<a href="https://doi.org/10.1007/s11222-025-10574-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A kernel distribution estimator (KDE) is proposed for the error distribution in the functional data, which is computed from the residuals of the B-spline trajectories over all the measurements. The maximal stochastic process between the KDE and the error distribution is shown to converge to a Gaussian process with mean zero and specified covariance function under some mild conditions. Thus, a simultaneous confidence band (SCB) is constructed for the error distribution based on the KDE in the dense functional data. The proposed SCB is applicable in not only the independent functional data but also the functional time series. In addition, the symmetric test is proposed for the error distribution, as well as a goodness-of-fit test for mean function by using the bootstrap method. Simulation studies examine the finite sample performance of the SCB and show the bootstrap method performs well in numerical studies. The proposed theory is illustrated by the electroencephalogram (EEG) functional data.},
  archive      = {J_SAC},
  author       = {Zhong, Chen},
  doi          = {10.1007/s11222-025-10574-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference and goodness-of-fit test in functional data via error distribution function},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EM algorithm for fitting matrix-variate normal
distributions on interval-censored and missing data. <em>SAC</em>,
<em>35</em>(2), 1–11. (<a
href="https://doi.org/10.1007/s11222-025-10575-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix-variate distributions are powerful tools for modeling three-way datasets that often arise in longitudinal and multidimensional spatio-temporal studies. However, observations in these datasets can be missing or subject to some detection limits because of the restriction of the experimental apparatus. Here, we develop an efficient EM-type algorithm for maximum likelihood estimation of parameters, in the context of interval-censored and/or missing data, utilizing the matrix-variate normal distribution. This algorithm provides closed-form expressions that rely on truncated moments, offering a reliable approach to parameter estimation under these conditions. Results obtained from the analysis of both simulated data and real case studies concerning water quality monitoring are reported to demonstrate the effectiveness of the proposed method.},
  archive      = {J_SAC},
  author       = {Lachos, Victor H. and Tomarchio, Salvatore D. and Punzo, Antonio and Ingrassia, Salvatore},
  doi          = {10.1007/s11222-025-10575-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {An EM algorithm for fitting matrix-variate normal distributions on interval-censored and missing data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing common degree-correction parameters of multilayer
networks. <em>SAC</em>, <em>35</em>(2), 1–22. (<a
href="https://doi.org/10.1007/s11222-025-10576-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common properties of the networks. In this paper, we study whether multilayer networks share the same degree-correction parameters, which is a special case of the widely studied common invariant subspace problem. We first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same degree-correction parameters, and under the alternative hypothesis, there exist at least two networks that have different degree-correction parameters. We propose a weighted degree difference test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.},
  archive      = {J_SAC},
  author       = {Yuan, Mingao and Yao, Qianqian},
  doi          = {10.1007/s11222-025-10576-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Testing common degree-correction parameters of multilayer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online structural break detection in financial durations.
<em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10577-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Durations between events of interest such as intra-day transactions of assets can reflect the volatility of asset prices in financial markets. The diverse dynamics of these intervals, which we refer to as financial durations, offer valuable insights into market behavior for investors. Inspection of streaming price data for structural breaks and timely and accurate detection of transitions between different duration patterns within a trading day enables practitioners to update parameters of suitable duration models. In this article, an innovative Ensemble Penalized Estimating Function (E-PEF) approach is proposed to effectively detect change points in the logarithmic autoregressive conditional duration models for financial durations. As a quasi-score-based online detection approach, this methodology leverages Mahalanobis distances and the block bootstrap sampling method to compute critical values for finite sample time series. The online structural break detection rule is informed by comparing observed quasi-scores in the monitoring period with pre-calculated critical values from training data in an ensemble manner. Extensive simulations demonstrate that the E-PEF method has fast structural break detection performance, while effectively controlling the probability of false detection. In the real data application, we applied our method to identify structural breaks for four assets, explored their relationships with summarized changes in volatility patterns, and noted several considerations for practitioners in the financial market.},
  archive      = {J_SAC},
  author       = {Wang, Yanzhao and Zhang, Yaohua and Zou, Jian and Ravishanker, Nalini},
  doi          = {10.1007/s11222-025-10577-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Online structural break detection in financial durations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function-on-function regression models with nonlinear
dynamic effect and linear concurrent effect. <em>SAC</em>,
<em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10578-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a function-on-function regression model that predicts a functional response by both a nonlinear dynamic effect of a functional predictor and a linear concurrent effect of another functional predictor. The nonlinear dynamic effect is characterized by taking an integral of a time-dependent two-dimensional smooth surface and the linear concurrent effect is modeled through a time-varying coefficient. The model structure combines the flexibility of nonlinear modeling with the interpretability of the linear concurrent effect. To approximate the two-dimensional surface, we use tensor product basis expansions, and for the time-varying coefficient in the concurrent effect, we employ B-spline expansions. The expansion parameters for each effect are estimated iteratively to account for the mutual dependencies between these two estimated effects. Each iteration of parameter estimation involves solving a penalized least squares problem. We establish the asymptotic properties of our estimator. The numerical performance of the proposed method is illustrated by simulation studies and two real data applications.},
  archive      = {J_SAC},
  author       = {Jia, Shifan and Shi, Haolun and Guan, Tianyu},
  doi          = {10.1007/s11222-025-10578-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Function-on-function regression models with nonlinear dynamic effect and linear concurrent effect},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian functional principal component analysis.
<em>SAC</em>, <em>35</em>(2), 1–25. (<a
href="https://doi.org/10.1007/s11222-025-10580-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a robust Bayesian functional principal component analysis (RB-FPCA) method that utilizes the skew elliptical class of distributions to model functional data, which are observed over a continuous domain. This approach effectively captures the primary sources of variation among curves, even in the presence of outliers, and provides a more robust and accurate estimation of the covariance function and principal components. The proposed method can also handle sparse functional data, where only a few observations per curve are available. We employ annealed sequential Monte Carlo for posterior inference, which offers several advantages over conventional Markov chain Monte Carlo algorithms. To evaluate the performance of our proposed model, we conduct simulation studies, comparing it with well-known frequentist and conventional Bayesian methods. The results show that our method outperforms existing approaches in the presence of outliers and performs competitively in outlier-free datasets. Finally, we demonstrate the effectiveness of our method by applying it to environmental and biological data to identify outlying functional observations. The implementation of our proposed method and applications are available at https://github.com/SFU-Stat-ML/RBFPCA .},
  archive      = {J_SAC},
  author       = {Zhang, Jiarui and Cao, Jiguo and Wang, Liangliang},
  doi          = {10.1007/s11222-025-10580-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Robust bayesian functional principal component analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on diffusion processes related to a general growth
model. <em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10562-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers two stochastic diffusion processes associated with a general growth curve that includes a wide family of growth phenomena. The resulting processes are lognormal and Gaussian, and for them inference is addressed by means of the maximum likelihood method. The complexity of the resulting system of equations requires the use of metaheuristic techniques. The limitation of the parameter space, typically required by all metaheuristic techniques, is also provided by means of a suitable strategy. Several simulation studies are performed to evaluate to goodness of the proposed methodology, and an application to real data is described.},
  archive      = {J_SAC},
  author       = {Albano, Giuseppina and Barrera, Antonio and Giorno, Virginia and Torres-Ruiz, Francisco},
  doi          = {10.1007/s11222-025-10562-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Inference on diffusion processes related to a general growth model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density regression via dirichlet process mixtures of normal
structured additive regression models. <em>SAC</em>, <em>35</em>(2),
1–20. (<a href="https://doi.org/10.1007/s11222-025-10567-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within Bayesian nonparametrics, dependent Dirichlet process mixture models provide a flexible approach for conducting inference about the conditional density function. However, several formulations of this class make either restrictive modelling assumptions or involve intricate algorithms for posterior inference. We propose a flexible and computationally convenient approach for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. We assume an additive structure for the mean of each mixture component and incorporate the effects of continuous covariates through smooth functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. Our method also seamlessly accommodates categorical covariates, linear effects of continuous covariates, varying coefficient terms, and random effects, which is why we refer our model as a Dirichlet process mixture of normal structured additive regression models. A notable feature of our method is the simplicity of posterior simulation using Gibbs sampling, as closed-form full conditional distributions for all model parameters are available. Results from a simulation study demonstrate that our approach successfully recovers the true conditional densities and other regression functionals in challenging scenarios. Applications to three real datasets further underpin the broad applicability of our method. An R package, DDPstar, implementing the proposed method is provided.},
  archive      = {J_SAC},
  author       = {Rodríguez-Álvarez, María Xosé and Inácio, Vanda and Klein, Nadja},
  doi          = {10.1007/s11222-025-10567-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Density regression via dirichlet process mixtures of normal structured additive regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel latent class models for cross-classified
categorical data: Model definition and estimation through stochastic EM.
<em>SAC</em>, <em>35</em>(2), 1–15. (<a
href="https://doi.org/10.1007/s11222-025-10579-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension of the multilevel latent class model for dealing with multilevel cross-classified categorical data. Cross-classified data structures arise when observations are simultaneously nested within two or more groups, for example, children nested within both schools and neighborhoods. More specifically, we propose extending the standard hierarchical latent class model, which contains mixture components at two levels, say for children and schools, by including a separate set of mixture components for each of the higher-level crossed classifications, say for schools and neighborhoods. Because of the complex dependency structure arising from the cross-classified nature of the data, it is no longer possible to obtain maximum likelihood estimates of the model parameters, for example, using the EM algorithm. As a solution to the estimation problem, we propose an approximate estimation approach using a stochastic version of the EM algorithm. The performance of this approach, which resembles Gibbs sampling, was investigated through a set of simulation studies. Moreover, the application of the new model is illustrated using an Italian dataset on the quality of university experience at degree programme level, with degree programmes nested in both universities and fields of study.},
  archive      = {J_SAC},
  author       = {Columbu, S. and Piras, N. and Vermunt, J. K.},
  doi          = {10.1007/s11222-025-10579-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel latent class models for cross-classified categorical data: Model definition and estimation through stochastic EM},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control charts for monitoring weibull quantile under
generalized hybrid and progressive censoring schemes. <em>SAC</em>,
<em>35</em>(2), 1–20. (<a
href="https://doi.org/10.1007/s11222-025-10581-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose control charts to monitor quantiles of Weibull distribution for Type-I generalized hybrid censoring scheme and Type-II progressive censoring scheme. Parametric bootstrap method is employed to derive the control limits. Monte Carlo simulations are carried out to assess the in-control and out-of-control performance of the proposed charts. The phase-I analysis evaluates the performance of the charts through average run lengths for various combinations of quantile, false-alarm rate, sample size, and censoring parameters. Chart performance is thoroughly investigated in the phase-II analysis for several choices of shifts in the scale and shape parameters of Weibull distribution along with different censoring schemes. The charts for both censoring schemes have been demonstrated to be highly effective in detecting out-of-control signals, both in terms of magnitude and speed. The proposed charts are illustrated through applications in reliability and clinical practices. While both charts show similar performance in terms of speed, the chart with the optimal Type-II progressive censoring scheme outperforms the one with the Type-I generalized hybrid censoring scheme in terms of magnitude in both examples.},
  archive      = {J_SAC},
  author       = {Modok, Bidhan and Chowdhury, Shovan and Kundu, Amarjit},
  doi          = {10.1007/s11222-025-10581-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Control charts for monitoring weibull quantile under generalized hybrid and progressive censoring schemes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using prior-data conflict to tune bayesian regularized
regression models. <em>SAC</em>, <em>35</em>(2), 1–19. (<a
href="https://doi.org/10.1007/s11222-025-10582-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional regression models, variable selection becomes challenging from a computational and theoretical perspective. Bayesian regularized regression via shrinkage priors like the Laplace or spike-and-slab prior are effective methods for variable selection in $$p&gt;n$$ scenarios provided the shrinkage priors are configured adequately. We propose an empirical Bayes configuration using checks for prior-data conflict: tests that assess whether there is disagreement in parameter information provided by the prior and data. We apply our proposed method to the Bayesian LASSO and spike-and-slab shrinkage priors in the linear regression model and assess the variable selection performance of our prior configurations through a high-dimensional simulation study. Additionally, we apply our method to proteomic data collected from patients admitted to the Albany Medical Center in Albany NY in April of 2020 with COVID-like respiratory issues. Simulation results suggest our proposed configurations may outperform competing models when the true regression effects are small.},
  archive      = {J_SAC},
  author       = {Biziaev, Timofei and Kopciuk, Karen and Chekouo, Thierry},
  doi          = {10.1007/s11222-025-10582-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Using prior-data conflict to tune bayesian regularized regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Directional data analysis: Spherical cauchy or poisson
kernel-based distribution? <em>SAC</em>, <em>35</em>(2), 1–21. (<a
href="https://doi.org/10.1007/s11222-025-10583-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, two novel distributions for the analysis of directional data were introduced: the spherical Cauchy distribution and the Poisson kernel-based distribution. This paper provides a detailed exploration of both distributions within various analytical frameworks. To enhance the practical utility of these distributions, alternative parametrizations that offer advantages in numerical stability and parameter estimation are presented, such as implementation of the Newton–Raphson algorithm for parameter estimation, while facilitating a more efficient and simplified approach in the regression framework. Additionally, a two-sample location test based on the log-likelihood ratio test is introduced. This test is designed to assess whether the location parameters of two populations can be assumed equal. The maximum likelihood discriminant analysis framework is developed for classification purposes, and finally, the problem of clustering directional data is addressed, by fitting finite mixtures of Spherical Cauchy or Poisson kernel-based distributions. Empirical validation is conducted through comprehensive simulation studies and real data applications, wherein the performance of the spherical Cauchy and Poisson kernel-based distributions is systematically compared.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail and Papastamoulis, Panagiotis and Kato, Shogo},
  doi          = {10.1007/s11222-025-10583-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Directional data analysis: Spherical cauchy or poisson kernel-based distribution?},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="si---3">SI - 3</h2>
<ul>
<li><details>
<summary>
(2025). The viability of domain constrained coalition formation for
robotic collectives. <em>SI</em>, <em>19</em>(1), 55–96. (<a
href="https://doi.org/10.1007/s11721-024-00242-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications, such as military and disaster response, can benefit from robotic collectives’ ability to perform multiple cooperative tasks (e.g., surveillance, damage assessments) efficiently across a large spatial area. Coalition formation algorithms can potentially facilitate collective robots’ assignment to appropriate task teams; however, most coalition formation algorithms were designed for smaller multiple robot systems (i.e., 2–50 robots). Collectives’ scale and domain-relevant constraints (i.e., distribution, near real-time, minimal communication) make coalition formation more challenging. This manuscript identifies the challenges inherent to designing coalition formation algorithms for very large collectives (e.g., 1000 robots). A survey of multiple robot coalition formation algorithms finds that most are unable to transfer directly to collectives, due to the identified system differences; however, auctions and hedonic games may be the most transferable. A simulation-based evaluation of five total algorithms from two combinatorial auction families and one hedonic game family, applied to homogeneous and heterogeneous collectives, demonstrates that there are collective compositions for which no evaluated algorithm is viable; however, the experimental results and literature survey suggest paths forward.},
  archive      = {J_SI},
  author       = {Diehl, Grace and Adams, Julie A.},
  doi          = {10.1007/s11721-024-00242-x},
  journal      = {Swarm Intelligence},
  month        = {3},
  number       = {1},
  pages        = {55-96},
  shortjournal = {Swarm Intell.},
  title        = {The viability of domain constrained coalition formation for robotic collectives},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized traffic management of autonomous drones.
<em>SI</em>, <em>19</em>(1), 29–53. (<a
href="https://doi.org/10.1007/s11721-024-00241-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coordination of local and global aerial traffic has become a legal and technological bottleneck as the number of unmanned vehicles in the common airspace continues to grow. To meet this challenge, automation and decentralization of control is an unavoidable requirement. In this paper, we present a solution that enables self-organization of cooperating autonomous agents into an effective traffic flow state in which the common aerial coordination task—filled with conflicts—is resolved. Using realistic simulations, we show that our algorithm is safe, efficient, and scalable regarding the number of drones and their speed range, while it can also handle heterogeneous agents and even pairwise priorities between them. The algorithm works in any sparse or dense traffic scenario in two dimensions and can be made increasingly efficient by a layered flight space structure in three dimensions. To support the feasibility of our solution, we show stable traffic simulations with up to 5000 agents, and experimentally demonstrate coordinated aerial traffic of 100 autonomous drones within a 250 m wide circular area.},
  archive      = {J_SI},
  author       = {Balázs, Boldizsár and Vicsek, Tamás and Somorjai, Gergő and Nepusz, Tamás and Vásárhelyi, Gábor},
  doi          = {10.1007/s11721-024-00241-y},
  journal      = {Swarm Intelligence},
  month        = {3},
  number       = {1},
  pages        = {29-53},
  shortjournal = {Swarm Intell.},
  title        = {Decentralized traffic management of autonomous drones},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imprecise evidence in social learning. <em>SI</em>,
<em>19</em>(1), 1–27. (<a
href="https://doi.org/10.1007/s11721-024-00238-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social learning is a collective approach to decentralised decision-making and is comprised of two processes; evidence updating and belief fusion. In this paper we propose a social learning model in which agents’ beliefs are represented by a set of possible states, and where the evidence collected can vary in its level of imprecision. We investigate this model using multi-agent and multi-robot simulations and demonstrate that it is robust to imprecise evidence. Our results also show that certain kinds of imprecise evidence can enhance the efficacy of the learning process in the presence of sensor errors.},
  archive      = {J_SI},
  author       = {Liu, Zixuan and Crosscombe, Michael and Lawry, Jonathan},
  doi          = {10.1007/s11721-024-00238-7},
  journal      = {Swarm Intelligence},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Swarm Intell.},
  title        = {Imprecise evidence in social learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="soco---35">SOCO - 35</h2>
<ul>
<li><details>
<summary>
(2025). Improving recurrent deterministic policy gradient strategy
in autonomous driving. <em>SOCO</em>, <em>29</em>(3), 1931–1946. (<a
href="https://doi.org/10.1007/s00500-025-10442-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though autonomous driving has emerged as a prominent study topic, the conventional control systems for autonomous driving are often rule-based and need to be more adaptable to the flow and conditions of traffic that change over time. Recurrent deterministic policy gradient (RDPG) is a strategy for building autonomous driving control systems. Its performance has been shown to be better than some other methods. Consequently, in this study, we make use of the RDPG algorithm to implement our control strategies as well and further give more comprehensive considerations to the learning procedure to obtain better control performance in the testing procedure, e.g., various punishments to avoid vehicle collisions, different speed limitations to avoid slow-driving or fast-driving, distinct rewards to encourage the ego-vehicle to reach the destination, and so on. On the other hand, we also improve the training performance by focusing solely on the critical events during the training procedure. Namely, our training architecture is more efficient based on the same training time (training steps). The road scene and vehicular simulator, AirSim, has been selected as the experimental platform. The findings indicate that our design achieves more accurate and steady outcomes in control and faster convergence in learning compared to an existing RDPG control strategy for autonomous driving in the literature.},
  archive      = {J_SOCO},
  author       = {Ooi, Yee-Ming and Chang, Che-Cheng},
  doi          = {10.1007/s00500-025-10442-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1931-1946},
  shortjournal = {Soft Comput.},
  title        = {Improving recurrent deterministic policy gradient strategy in autonomous driving},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of discharge coefficient of submerged gates using
a stacking ensemble model. <em>SOCO</em>, <em>29</em>(3), 1911–1929. (<a
href="https://doi.org/10.1007/s00500-025-10518-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the precision of discharge coefficient (Cd) prediction is crucial for effective agricultural water management. However, existing methods for Cd calculation are often complex and dependent on specific assumptions. Therefore, there is a critical need for robust and automated models for Cd estimation. This study introduces a dual-stage ensemble model called EnsembleCNN, for Cd prediction using two distinct gate types under submerged flow conditions. The EnsembleCNN framework uniquely integrates machine learning (ML) models with a recurrent convolutional neural network (CNN) model to capture higher-order interactions and non-linearities. Five base ML models are employed to generate initial predictions. These predictions are subsequently processed by a CNN model embedded with long short-term memory (LSTM) layer, residual connection (RC) and an attention mechanism (ATM). This setup effectively manages the complexity of the combined predictions, seamlessly integrating the outputs from the base models. LSTM is exploited to aggregate the best features for prediction. ATM effectively prioritized high-performing base model outputs, while RC improved the gradient flow, collectively reducing the impact of irrelevant features. The proposed approach strategically weights the contributions of each base model, resulting in accurate Cd estimations. The proposed model achieved root mean square errors of 0.0552 and 0.0173 on vertical sluice gates and radial gates datasets, respectively. Additionally, EnsembleCNN outperformed the base and existing models in terms of prediction accuracy. The proposed system provides a robust tool for optimizing water resource management. Moreover, the adaptability to two field datasets further underscores the practical utility of our model in diverse irrigation scenarios.},
  archive      = {J_SOCO},
  author       = {Hosny, Mohamed and Abdelhaleem, Fahmy S. and Elshenhab, Ahmed M. and Ibrahim, Amir},
  doi          = {10.1007/s00500-025-10518-x},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1911-1929},
  shortjournal = {Soft Comput.},
  title        = {Prediction of discharge coefficient of submerged gates using a stacking ensemble model},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-specific multi-label text classification based on
dynamic graph convolutional networks. <em>SOCO</em>, <em>29</em>(3),
1897–1909. (<a
href="https://doi.org/10.1007/s00500-025-10446-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification is a key task in natural language processing, aiming to assign each text to multiple predefined categories simultaneously. Existing neural network models usually learn the same text representation for different labels, which limits the effectiveness of the models in capturing deep semantics and distinguishing between similar labels; moreover, these models tend to ignore inter-label correlation, leading to loss of information. To overcome these limitations, we propose a novel label-specific dynamic graph convolutional network (LDGCN). This network combines convolutional operations and BiLSTM to model text sequences and obtains label-specific text representations through a label attention mechanism. In addition, LDGCN improves the dynamic graph convolutional network by utilizing statistical label co-occurrence and label reconstruction maps to effectively capture inter-label dependencies and adaptive interactions between label-specific semantic components. Extensive experiments on the RCV1, AAPD, and EUR-Lex datasets show that our model achieves 96.92%, 86.30%, and 81.42% on the P@1 metrics, respectively, and demonstrates a significant advantage in dealing with tail labels.},
  archive      = {J_SOCO},
  author       = {Yan, Yaoyao and Liu, Fang‘ai and Liu, Kenan and Xu, Weizhi and Zhuang, Xuqiang},
  doi          = {10.1007/s00500-025-10446-w},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1897-1909},
  shortjournal = {Soft Comput.},
  title        = {Label-specific multi-label text classification based on dynamic graph convolutional networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Threats to medical diagnosis systems: Analyzing targeted
adversarial attacks in deep learning-based COVID-19 diagnosis.
<em>SOCO</em>, <em>29</em>(3), 1879–1896. (<a
href="https://doi.org/10.1007/s00500-025-10516-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep and machine learning models have become pivotal in medical image analysis, especially for diagnosing COVID-19 using X-rays and CT scans. While these models, including transfer learning-based approaches, have achieved high accuracy, they remain highly vulnerable to adversarial attacks, which can manipulate input data and cause misclassification, posing critical risks in clinical applications. This study introduces a novel approach to addressing this issue by systematically evaluating the impact of adversarial attacks on COVID-19 diagnosis models built with two leading architectures, VGG-16 and DenseNet-121, using the Fast Gradient Sign Method (FGSM). The FGSM attack causes a dramatic drop in accuracy, reducing VGG-16’s accuracy from 95.12 to 9.97% and DenseNet-121’s from 96.51 to 10.13%. To counter these vulnerabilities, we propose a novel defense mechanism that combines adversarial training with Gaussian noise data augmentation, a dynamic approach that generates perturbations across various epsilon values during the training phase. This innovative method significantly enhances model robustness, restoring accuracy to over 92% on adversarial examples. These findings emphasize the need for strong defense mechanisms in deep learning models for COVID-19 diagnosis, ensuring reliability and security against adversarial threats in clinical environments.},
  archive      = {J_SOCO},
  author       = {Haque, Sheikh Burhan Ul and Zafar, Aasim and Haq, Sheikh Riyaz Ul and Haque, Sheikh Moeen Ul and Ahmad, Mohassin and Roshan, Khushnaseeb},
  doi          = {10.1007/s00500-025-10516-z},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1879-1896},
  shortjournal = {Soft Comput.},
  title        = {Threats to medical diagnosis systems: Analyzing targeted adversarial attacks in deep learning-based COVID-19 diagnosis},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive multimodal approach for parkinson’s disease
classification using artificial intelligence: Insights and model
explainability. <em>SOCO</em>, <em>29</em>(3), 1845–1877. (<a
href="https://doi.org/10.1007/s00500-025-10463-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is a debilitating neurodegenerative disorder affecting millions worldwide. Early detection is vital for effective management, yet remains challenging. In this study, we investigated four distinct datasets for PD detection. Through comprehensive experimentation employing ensemble methods and feature selection, we achieved high classification accuracies across the datasets. For the Oxford Parkinson’s Disease Detection Dataset, an accuracy of 95.67%, precision of 97.59%, recall of 84.5%, specificity of 99.32%, and F1-score of 90.57% were achieved. For the Alzheimer Parkinson Diseases 3 Class Dataset, the “Stacking” approach surpasses individual models, reaching an accuracy of 99.85%, precision of 99.81%, recall of 99.81%, specificity of 99.86%, and F1 of 99.81%. For the NewHandPD dataset, Regarding the Spiral category, The “Base-P32-384” model surpasses others with an accuracy of 97.35%, precision of 96.50%, recall of 98.57%, and F1-score of 97.53%. The collective “Stacking” approach proves highly effective regarding the Circle category, achieving 100% across all performance metrics. Regarding the Meander category, the “Base-P16-224” model achieves an accuracy of 97.35%, precision of 99.26%, recall of 95.71%, specificity of 99.19%, and F1 of 97.45%. The Mobile Device Voice Recordings at King’s College London (MDVR-KCL) dataset contains two datasets. Regarding the “SpontaneousDialogue” dataset, accuracy, BAC, precision, recall, specificity, and F1-score were computed, resulting in values of 94.03%, 92.83%, 90.78%, 100.0%, and 85.67%, respectively. Regarding the “ReadText” dataset, accuracy, BAC, precision, recall, specificity, and F1-score were computed, resulting in values of 91.89%, 90.62%, 87.5%, 100.0%, and 81.25%, respectively. Our findings highlight the efficacy of leveraging diverse data sources and advanced machine learning techniques to enhance PD detection accuracy.},
  archive      = {J_SOCO},
  author       = {Balaha, Hossam Magdy and Hassan, Asmaa El-Sayed and Ahmed, Rawan Ayman and Balaha, Magdy Hassan},
  doi          = {10.1007/s00500-025-10463-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1845-1877},
  shortjournal = {Soft Comput.},
  title        = {Comprehensive multimodal approach for parkinson’s disease classification using artificial intelligence: Insights and model explainability},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing neural network predictions with finetuned numeric
embeddings for stock trend forecasting. <em>SOCO</em>, <em>29</em>(3),
1829–1844. (<a
href="https://doi.org/10.1007/s00500-025-10483-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The financial markets, particularly stock trading, offer a variety of profit-generating opportunities based on complex and volatile behaviour. Investors seek strategies to maximise returns, leading to an investigation of inherent market patterns. Converting OHLC (Open, High, Low, Close) data into transformers-based pre-trained language model compatible text is an innovative method for representing numeric data. Extending the language model’s utility to integrate stock market numeric time-series data incorporates its inherent numeracy in embeddings. Raw data are converted into a format compatible with the pre-trained language model through preprocessing and text templates. Using an ensemble of Bidirectional Encoder Representations from Transformers (BERT), FinBERT (BERT finetuned with the financial corpus), FLANG-BERT (BERT finetuned with the financial corpus) and FLANG-ELECTRA (ELECTRA finetuned with the financial corpus) as feature extractor, historical stock market data are utilised to generate an embedding matrix and fused with established neural network architectures, such as Backpropagation Neural Network (BPNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), to predict stock market trends. The simulation results demonstrate that the proposed integrated approach is preferable to previous methodologies. The significance of the findings is confirmed by statistical validation using the Wilcoxon signed-rank test (p value &lt; 0.01). This study offers a promising approach for improving stock market trend prediction by integrating the ensemble of language model-based numeric embeddings with neural networks.},
  archive      = {J_SOCO},
  author       = {Trivedi, Avinash and Sangeetha, S.},
  doi          = {10.1007/s00500-025-10483-5},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1829-1844},
  shortjournal = {Soft Comput.},
  title        = {Enhancing neural network predictions with finetuned numeric embeddings for stock trend forecasting},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-theoretic exploration with surplus profit-sharing in
a three-channel supply chain, featuring e-commerce dynamics.
<em>SOCO</em>, <em>29</em>(3), 1811–1827. (<a
href="https://doi.org/10.1007/s00500-025-10453-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a three-channel supply chain, coordination can be challenging especially when a manufacturer has to work with a retailer and an online platform. In such a scenario, sales efforts can be critical to the success of the supply chain. However, there is a risk of free riding behavior by either the retailer or the manufacturer, which can lead to suboptimal sales performance. This article will explore the centralized and the decentralized models by the use of game theory (Nash and Stackelberg) and eventually tries to coordinate the three-channel supply chain with the help of Operational Research (OR) to optimize the decision-making and create a win–win situation. Numerical examples are provided to prove the efficiency of the presented models. Finally, the models are evaluated through sensitivity analysis, and managerial insights are provided to enhance the applicability of the models for coordinating a three-channel supply chain.},
  archive      = {J_SOCO},
  author       = {Vatanara, Maryam and Rabbani, Masoud and Heydari, Jafar},
  doi          = {10.1007/s00500-025-10453-x},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1811-1827},
  shortjournal = {Soft Comput.},
  title        = {A game-theoretic exploration with surplus profit-sharing in a three-channel supply chain, featuring e-commerce dynamics},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forest fire rescue framework to jointly optimize
firefighting force configuration and facility layout: A case study of
digital-twin simulation optimization. <em>SOCO</em>, <em>29</em>(3),
1789–1810. (<a
href="https://doi.org/10.1007/s00500-025-10434-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pre-prevention stage, firefighting force configuration and facility layout play a critical role in reducing fire extinguishing time (FET) during the early-stage forest fire rescue. It is acknowledged that there is a scarcity of quantitative evaluation research establishing a connection between observed forest fire behaviors and pre-prevention research. Therefore, we propose a forest fire rescue framework to jointly optimize firefighting force configuration and facility layout. As an iterative optimization framework based on fire spread and suppression model (FSSM), firefighting force configuration and facility layout methods use differential-evolution-based algorithm and deep neural network to adjust the configuration funds of various firefighting forces and plan the spatial layout of multiple firefighting facilities. With iterations increasing, the proposed method can continue to find better solutions than before. Moreover, through the offensive and defensive procedures in FSSM, the best configuration and layout solution can mirror multi-rescue-resource interactions and mutual restraints. The performance of the proposed framework is validated through various maps and experiments in terms of FET, forest burned area, and uncontrolled fire rate, even under extreme wind-speed pressure conditions. This implies that the proposed framework demonstrates favorable adaptability. Furthermore, the proposed framework can be introduced into the related dynamic interactions and constraints optimization scenarios (e.g., smart factories, smart construction sites, and more), thereby opening the door of digital-twin simulation optimization.},
  archive      = {J_SOCO},
  author       = {Zhang, HongGuang and Ma, ShengWen and Li, Xiang and You, MingCan and Tao, YuXuan},
  doi          = {10.1007/s00500-025-10434-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1789-1810},
  shortjournal = {Soft Comput.},
  title        = {Forest fire rescue framework to jointly optimize firefighting force configuration and facility layout: A case study of digital-twin simulation optimization},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient collocation algorithm for third order
non-linear emden–fowler equation. <em>SOCO</em>, <em>29</em>(3),
1767–1788. (<a
href="https://doi.org/10.1007/s00500-025-10431-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study presents a novel algorithm for solving third-order non-linear equations (Emden–Fowler type), which can be applied to various physical models. The algorithm uses a quintic trigonometric B-spline collocation method and a quasilinearization technique to avoid the non-linearity term in the equation. The study established a comprehensive error analysis for the proposed algorithm and proved that it has fourth order, i.e., $$(\mathscr {O}(h^4))$$ convergent. The algorithm’s ability to handle singular behavior at the point $$x=0$$ and its faster rate of convergence exhibit a promising approach to solving such problems. The study also validates the theoretical results through numerical experiments and shows that the proposed algorithm has a faster rate of convergence in comparison to the existing methods.},
  archive      = {J_SOCO},
  author       = {Alam, Mohammad Prawesh and Khan, Arshad},
  doi          = {10.1007/s00500-025-10431-3},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1767-1788},
  shortjournal = {Soft Comput.},
  title        = {An efficient collocation algorithm for third order non-linear Emden–Fowler equation},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature extraction method for rotating machinery fault
diagnosis based on a multiscale entropy fusion strategy and GA-RL-LDA
model. <em>SOCO</em>, <em>29</em>(3), 1747–1765. (<a
href="https://doi.org/10.1007/s00500-025-10484-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of information loss, feature redundancy and unsatisfactory diagnosis accuracy when using traditional multiscale entropy methods and feature reduction methods to diagnose rotating machinery faults, a feature extraction method based on a multiscale entropy fusion strategy and a GA-RL-LDA model is proposed in this paper. Firstly, the multiscale fluctuation dispersion entropy (MFDE), the refined composite multiscale dispersion entropy (RCMDE) and the refined composite multiscale fluctuation dispersion entropy (RCMFDE) of the collected vibration signal are calculated to form an original feature set. Then, based on the ReliefF algorithm and Laplacian score (LS), an RL index is constructed for feature sensitivity evaluation. After that, combing the RL with Linear discriminant analysis (LDA) and using genetic algorithm (GA) to optimize the uncertain parameters, a GA-RL-LDA model is proposed for feature reduction. Finally, the reduced feature subset is input into support vector machine (SVM) for fault classification. The experiment utilized data from Unit 3 of the SK Hydropower Station and bearing data from Case Western Reserve University, achieving diagnostic accuracies of 95.2381% and 97.3333%, respectively. In the 105 test samples from Unit 3 of the SK Hydropower Station, only 5 samples were misclassified, while in the 150 test samples from Case Western Reserve University, only 4 samples were misclassified. Compared with different information entropy and optimization strategies, the results show that the proposed method can more effectively extract fault sensitive features and accurately diagnose rotating machinery faults even with a small number of training samples.},
  archive      = {J_SOCO},
  author       = {Lu, Na and Li, Zhongliang and Liu, Dong and Cao, Chaofan and Jiang, Shuangyun and Chen, Xudong and Wang, Peng},
  doi          = {10.1007/s00500-025-10484-4},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1747-1765},
  shortjournal = {Soft Comput.},
  title        = {A feature extraction method for rotating machinery fault diagnosis based on a multiscale entropy fusion strategy and GA-RL-LDA model},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning approach to analyse stress by using voice
and body posture. <em>SOCO</em>, <em>29</em>(3), 1719–1745. (<a
href="https://doi.org/10.1007/s00500-025-10441-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current scenario, where we can see young people struggling for their careers, they are even fighting a battle with their stress and tension. None of their work is done without stress to complete their task and compete with others. To overcome stress, one should have good emotional intelligence to cope with emotions and any upcoming stress. But at some point, due to lack of guidance, some people don’t know how to analyze the situations and how to handle them without taking the stress and end up with anxiety, depression, disappointment, suicide, heart attack, stroke etc. Due to the advancement of Human–Computer Interaction (HCI), medical science has leveled up to another peak. Machine Learning and Deep Learning played a major role in such interactions and predictions. Many applications have been developed in past years based on machine learning and deep learning. One of those applications is related to psychology and is still in research. These applications can be used for emotion and stress analysis among people, especially youngsters. Research in this field is being conducted using various verbal and non-verbal parameters. This paper addresses the research problem of improving emotion recognition accuracy and robustness to better analyze and manage stress. The primary objective is to develop an advanced Emotion Recognition System (ERS) that leverages deep learning algorithms to analyses both verbal and non-verbal cues—specifically, speech and body posture, including facial expressions. We have further integrated it with the Flask web framework to make an Emotion Recognition System that takes input in the form of video and audio to analyze Emotions and Stress. We have also compared our proposed ERS with existing ones and found that our ERS gives better results.},
  archive      = {J_SOCO},
  author       = {Gupta, Sumita and Gambhir, Sapna and Gambhir, Mohit and Majumdar, Rana and Shrivastava, Avinash K. and Pham, Hoang},
  doi          = {10.1007/s00500-025-10441-1},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1719-1745},
  shortjournal = {Soft Comput.},
  title        = {A deep learning approach to analyse stress by using voice and body posture},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced TOPSIS-CoCoSo framework for multi-attribute
decision-making with triangular fuzzy neutrosophic sets: “Effect
evaluation of intelligent technology empowering physical education
teaching” case. <em>SOCO</em>, <em>29</em>(3), 1703–1717. (<a
href="https://doi.org/10.1007/s00500-025-10411-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The history of human education development has proven that there is an interactive relationship between technological development and education and teaching. In the process of promoting education modernization and high-quality development, the widespread application of intelligent technology in the field of education is the trend, and intelligence is driving profound transformation and transformation in the field of education. The effect evaluation of intelligent technology empowering Physical Education teaching could be considered as multiple-attribute decision-making (MADM). Recently, the TOPSIS technique and Combined Compromise Solution (CoCoSo) technique was employed to deal with MADM. The triangular fuzzy neutrosophic sets (TFNSs) are employed as a better tool for expressing uncertain information during the effect evaluation of intelligent technology empowering Physical Education teaching. In this paper, the triangular fuzzy neutrosophic number TOPSIS-CoCoSo (TFNN-TOPSIS-CoCoSo) technique based on the TFNN relative closeness coefficient (TFNNRCC) technique is managed to cope with the MADM under TFNSs. The information entropy technique is employed to manage the weight values based on the TFNNRCC under TFNSs. Finally, a numerical example of effect evaluation of intelligent technology empowering Physical Education teaching is managed and some better comparisons are managed to verify the TFNN-TOPSIS-CoCoSo technique. The main contribution of this paper is outlined: (1)TFNN-TOPSIS-CoCoSo technique based on the TFNNRCC is constructed; (2) Entropy technique is employed to manage weight based on the TFNNRCC under TFNSs. (3) TFNN-TOPSIS-CoCoSo technique is founded to manage the MADM based on the TFNNRCC under TFNSs; (4) numerical example for effect evaluation of intelligent technology empowering Physical Education teaching and some comparative analysis is supplied to verify the proposed TFNN-TOPSIS-CoCoSo technique.},
  archive      = {J_SOCO},
  author       = {Xiao, Jie and Zhang, Yu},
  doi          = {10.1007/s00500-025-10411-7},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1703-1717},
  shortjournal = {Soft Comput.},
  title        = {Enhanced TOPSIS-CoCoSo framework for multi-attribute decision-making with triangular fuzzy neutrosophic sets: “effect evaluation of intelligent technology empowering physical education teaching” case},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lacunary statistical soft convergence in soft topology.
<em>SOCO</em>, <em>29</em>(3), 1691–1701. (<a
href="https://doi.org/10.1007/s00500-025-10479-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical convergence and some related types of convergence are a generalisation of topological convergence. Similarly, soft set theory, introduced by Molodtsov to deal with uncertainty in various scientific fields, is a generalisation of the classical concept of sets. Although both concepts have found extensive applications to various mathematical structures, the investigation of statistical convergence within soft topological spaces has not yet been undertaken. This study examines the lacunary statistical convergence of sequences of soft points in soft topological spaces, employing a density defined by an unbounded modulus function. Basic results and inclusion theorems concerning this convergence are presented.},
  archive      = {J_SOCO},
  author       = {Bayram, Erdal and Dervişoğlu, Melisa},
  doi          = {10.1007/s00500-025-10479-1},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1691-1701},
  shortjournal = {Soft Comput.},
  title        = {Lacunary statistical soft convergence in soft topology},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-InfNode: Ranking top-k influential nodes in complex
networks with random walk. <em>SOCO</em>, <em>29</em>(3), 1677–1690. (<a
href="https://doi.org/10.1007/s00500-025-10471-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex network is a symbolic representation of distinct real-world systems where information propagates through nodes. Its goal is to identify communities that represent the network’s structure. However, locating the influential node with the maximal range among various nodes and the ability to disseminate influence to a wide portion of the network is one of the most essential concerns in such a network. Centrality is a traditional metric for understanding the effect of nodes in a network, with numerous variants such as closeness, betweenness, degree centrality, and so on. The centrality metrics either work locally or globally to identify influential nodes. In this study, a proposed algorithm named k-InfNode, based on the characteristics of community structure, captures the dynamics of nodes. k-InfNode uses a random walk and combines local and global properties to figure out which nodes are important in a complex network. It was inspired by the idea of overlapping nodes that show how nodes and communities interact with each other across the network. In the beginning, the fuzzy c-means algorithm finds the overlapping nodes in the network. Next, the algorithm assigns an initial score to each node based on node and community information, and iteratively scores each node using the Random Walk with Restart (RWR) algorithm. Experiments performed using real and artificial networks have shown that the k-InfNode is effective.},
  archive      = {J_SOCO},
  author       = {Hasan, Ahmadi and Kamal, Ahmad and Kumar, Pawan},
  doi          = {10.1007/s00500-025-10471-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1677-1690},
  shortjournal = {Soft Comput.},
  title        = {K-InfNode: Ranking top-k influential nodes in complex networks with random walk},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A micro-level approach for modeling rumor propagation in
online social networks. <em>SOCO</em>, <em>29</em>(3), 1667–1675. (<a
href="https://doi.org/10.1007/s00500-025-10456-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks have become the major platforms for information dissemination in recent years. However, rapid propagation of rumors in these networks as a special form of information can greatly influences social lives. Hence, work on rumor propagation models and analysis is under great attention by the research communities. Previously, researchers have proposed various models to explore the dynamics of rumor propagation and analyze steady-state. However, most of them did not consider people’s behavior differences in the spreading or opposing rumor. To overcome this limitation, we assume that individuals have different probability of spreading rumor, spreading anti-rumor and stifling. In this paper we introduce a new model for rumor propagation in social networks considering these differences at micro-level. The proposed model which considered both types of rumor and anti-rumor messages on people decision is an agent-based model in terms of probabilistic automata network. To evaluate the proposed model, we conduct a number of Monte-Carlo simulation experiments on Barabasi-Albert model of social networks that show the accuracy of the proposed model. We also conduct interesting sensitivity analysis to see the effects of different model parameters on the dynamics of the rumor propagation.},
  archive      = {J_SOCO},
  author       = {Sahafizadeh, Ebrahim and Talatian Azad, Saeed},
  doi          = {10.1007/s00500-025-10456-8},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1667-1675},
  shortjournal = {Soft Comput.},
  title        = {A micro-level approach for modeling rumor propagation in online social networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Paw decompositions of diamond and some edge cycle graphs.
<em>SOCO</em>, <em>29</em>(3), 1659–1665. (<a
href="https://doi.org/10.1007/s00500-025-10541-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $${C}_{n}, {K}_{n},{W}_{n},{K}_{r,s}$$ denote a cycle, complete graph, wheel graph, complete bipartite graph respectively. An edge cycle graph of a graph $$G$$ is the graph $$G({C}_{k})$$ formed from one copy of $$G$$ and $$|E(G)|$$ copies of $${P}_{k},$$ where t he ends of the $${i}^{th}$$ edge are identified with the ends of $${i}^{th}$$ copy of $${P}_{k}$$ . In this article, we determine the necessary and sufficient conditions for the existence of paw- decompositions of the diamond graph $${Br}_{n}$$ and some edge cycle graphs like $${K}_{n}\left({C}_{3}\right), { W}_{n}\left({C}_{3}\right),{ K}_{r,s}\left({C}_{3}\right), { C}_{n}\circ \stackrel{\leftharpoonup}{{K}_{m}}({C}_{3})$$ and $${P}_{n}\circ \stackrel{\leftharpoonup}{{K}_{m}}({C}_{3})$$ where $$\circ $$ denotes the corona of graphs.},
  archive      = {J_SOCO},
  author       = {Esakkimuthu, Murugan and Rameshbabu, Sivaprakash Gunniya},
  doi          = {10.1007/s00500-025-10541-y},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1659-1665},
  shortjournal = {Soft Comput.},
  title        = {Paw decompositions of diamond and some edge cycle graphs},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On goal programming approach for interval-valued
intuitionistic fuzzy multi-objective transportation problems with an
application to tourism industry. <em>SOCO</em>, <em>29</em>(3),
1627–1657. (<a
href="https://doi.org/10.1007/s00500-025-10420-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation problems are inevitably affected by numerous imprecise factors like weather, fuel expenses, topography, etc. Hence, the use of crisp parameters to model transportation problems appears to be both insufficient and inaccurate. Consequently, transportation problems using fuzzy/ intuitionistic fuzzy (IF) numbers seem more effective. Interval-valued intuitionistic fuzzy (IVIF) numbers are further generalization of IF numbers where membership and non-membership degrees are closed sub-intervals of [0, 1]. This concept of allocating interval values helps in dealing with the hesitancy of decision-maker while assigning fixed values to membership and non-membership degrees. In this article, balanced transportation problems having multiple objectives under the IVIF environment are examined. To overcome inconsistencies in the existing approaches, novel linear as well as non-linear interval-valued membership and non-membership functions have been proposed. Subsequently, an improved IVIF programming approach is developed using these newly defined functions along with theoretical validation. In addition, when goals are associated with objective functions, the proposed approach has been further improvised as IVIF prioritized goal programming. Eventually, a trip planning problem in the tourism industry is exhibited to illustrate the proposed IVIF technique and later, it is amalgamated with prioritized goals to demonstrate the proposed IVIF goal programming approach.},
  archive      = {J_SOCO},
  author       = {Chauhan, Abhishek and Mahajan, Sumati},
  doi          = {10.1007/s00500-025-10420-6},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1627-1657},
  shortjournal = {Soft Comput.},
  title        = {On goal programming approach for interval-valued intuitionistic fuzzy multi-objective transportation problems with an application to tourism industry},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware coverage path planning for a swarm of UAVs
using mobile ground stations for battery-swapping. <em>SOCO</em>,
<em>29</em>(3), 1605–1625. (<a
href="https://doi.org/10.1007/s00500-025-10537-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of swarms of drones is expected to continue growing in the next years, particularly in dangerous scenarios, such as monitoring and rescue missions in hostile and disaster areas. Small-sized Unmanned Aerial Vehicles (UAVs) are highly suitable for use in such scenarios due to their agility and maneuverability. On the other hand, their limited battery capacity poses significant challenges, especially during missions requiring full coverage of large areas in a short time and extreme weather conditions. This work proposed an energy efficiency approach, which makes use of mobile ground-based battery-swapping stations (BSSes), to speed up the UAV’s battery replacement and reduce energy waste in the round trip to the charging station. Specifically, a Context-Aware Coverage Path Planning (CACPP) problem has been formulated to determine the complete coverage path of a large area by a swarm of UAVs, minimizing the path overlapping and UAV battery swapping. The model takes into account the need to continue re-planning the mission, depending on the weather conditions (i.e., temperature and wind), the presence of obstacles, and the residual energy levels of the drones, as well as the relative positions of the drones and mobile BSSes. To solve the CACPP problem, an iterative approach leveraging two synchronized optimization models for planning UAV paths and BSS routes has been presented. As the CACPP problem is NP-hard, a heuristic procedure for solving it has also been evaluated. Experimental results show that it can be appropriate for large instances of the problem.},
  archive      = {J_SOCO},
  author       = {Porcelli, Lorenzo and Ficco, Massimo and D’Angelo, Gianni and Palmieri, Francesco},
  doi          = {10.1007/s00500-025-10537-8},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1605-1625},
  shortjournal = {Soft Comput.},
  title        = {Context-aware coverage path planning for a swarm of UAVs using mobile ground stations for battery-swapping},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using deep forest regression and multi-layer state
transition algorithm to soft measuring modeling with small sample data.
<em>SOCO</em>, <em>29</em>(3), 1587–1603. (<a
href="https://doi.org/10.1007/s00500-025-10527-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In actual industrial operation process, some key performance indicators (KPIs) are tricky to detect online due to the characteristics of the detection equipment and the nature of the parameters. Moreover, these KPIs usually present small sample attributes. In this article, a stable and efficient soft measuring model for the KPIs of industrial processes is proposed using deep forest regression (DFR) and multi-layer state transition algorithm (STA). First, DFR is used to build soft measuring models for KPIs with random initial hyperparameters. Second, an improved dynamic STA (DSTA) is developed to optimize the DFR’s hyperparameters. Furthermore, the probability parameters of the DSTA structure are optimally selected using a STA. Finally, gradient refinement is utilized to fine-tune the state factor, which achieves a more accurate optimization process during the internal iteration process. The proposed algorithm is evaluated on the benchmark function, dataset, and an actual industrial problem. Results prove that the use of our method in soft measuring modeling can be effective.},
  archive      = {J_SOCO},
  author       = {Xia, Heng and Tang, Jian and Yu, Wen},
  doi          = {10.1007/s00500-025-10527-w},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1587-1603},
  shortjournal = {Soft Comput.},
  title        = {Using deep forest regression and multi-layer state transition algorithm to soft measuring modeling with small sample data},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A community-based simulated annealing approach with a new
structure-based neighborhood search to identify influential nodes in
social networks. <em>SOCO</em>, <em>29</em>(3), 1567–1585. (<a
href="https://doi.org/10.1007/s00500-025-10490-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying influential nodes has attracted the attention of many researchers in recent years. Because of the weak tradeoff between accuracy and running time, and ignoring the community structure by the proposed algorithms in the past research studies, further studies in this area are required. In this paper, we consider communities and also use a novel structure-based neighborhood search to improve exploration strategy of the simulated annealing (SA) algorithm. Moreover, we use the k-shell method for generating a better initial solution instead of random generation. In the proposed algorithm called Ckshell-SA, first, the communities are detected, then the k-shell method is used in each community to find initial candidate nodes locally. Finally, SA algorithm is applied with a neighborhood search that considers the structural properties of the network, and three centralities to find the influential nodes globally. A derivative of the Ckshell-SA method called kshell-SA is also introduced in this paper to examine the impact of considering communities. Unlike the Ckshell-SA, the community structure is neglected, and the k-shell is performed on the whole network in kshell-SA algorithm. Extensive experiments are conducted on eight real-world networks under Independent Cascade Model (IC) and Weighted Independent Cascade Model (WC). The results show that the Ckshell-SA and kshell-SA algorithms outperform the state-of-the-art algorithms concerning influence spread. Furthermore, the results show that Ckshell-SA is more efficient in networks like Facebook with a high Power Law exponent and higher modularity. On the contrary, kshell-SA is more successful in networks like Slashdot or Epinions with lower modularity.},
  archive      = {J_SOCO},
  author       = {Abyaneh, Farzaneh Rajaee and Charkari, Nasrollah Moghadam and Roayaei, Mehdy},
  doi          = {10.1007/s00500-025-10490-6},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1567-1585},
  shortjournal = {Soft Comput.},
  title        = {A community-based simulated annealing approach with a new structure-based neighborhood search to identify influential nodes in social networks},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid genetic search based approach for the generalized
vehicle routing problem. <em>SOCO</em>, <em>29</em>(3), 1553–1566. (<a
href="https://doi.org/10.1007/s00500-025-10507-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel meta-heuristic for addressing a variant of the classical Capacitated Vehicle Routing Problem (CVRP) known as the Generalized Vehicle Routing Problem (GVRP). In the GVRP, nodes are organized into clusters, with the constraint that only one node from each cluster must be visited. The proposed meta-heuristic is a Hybrid Genetic Search (HGS) that leverages recent advancements in CVRP methodologies, adapting successful strategies and techniques from CVRP to the GVRP context. To evaluate the performance of the HGS meta-heuristic, we perform an extensive computational analysis on numerous benchmark instances ranging from small to large sizes. To thoroughly analyze the algorithm’s average behavior, convergence profiles over time are reported for the considered instances. Results show that the proposed algorithm achieves 174 new best solutions out of the 498 instances considered. In only six instances out of 498, the algorithm is unable to reach or improve upon the best-known solution in the literature. These results suggest that the proposed meta-heuristic has significant potential in addressing real-world generalized vehicle routing challenges. Code available at: https://github.com/vlatorre847/HGSGVRP .},
  archive      = {J_SOCO},
  author       = {Latorre, Vittorio},
  doi          = {10.1007/s00500-025-10507-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1553-1566},
  shortjournal = {Soft Comput.},
  title        = {A hybrid genetic search based approach for the generalized vehicle routing problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient reconfigurable architecture to extract image
features for face recognition using local binary pattern. <em>SOCO</em>,
<em>29</em>(3), 1541–1552. (<a
href="https://doi.org/10.1007/s00500-025-10415-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of the face is a widely used method to detect human features. In various scenarios the face recognition speed becomes significant which necessitates to improve the critical delay of the architecture. In this paper, we propose Efficient FPGA architecture to extract image features using Local Binary pattern (LBP) for Face Recognition. The face image is converted into standard size (256 × 256) as pre-processing and the Gaussian filter is used to remove the high frequency components. These image is then applied to optimized LBP block to obtain the LBP features for both database sample and test sample are further compared to make the decision for face recognition. The proposed LBP architecture is designed using simple counter and comparators which leads to minimum complexity in turn improving the critical delay and hardware utilizations of the entire system. The simulation is performed for Olivetti Research Laboratory (ORL) dataset using MATLAB by showing False Acceptance Rate (FAR), False Rejection Rate (FRR) and Total Success Rate (TSR) values. The thresholding is performed based on Weighted Mean Square Difference and is varied for Total Success Rate (TSR) calculations tested for different combinations of Person in Database (PID) and Person Out of database (POD). Finally, the proposed architecture is synthesized on Spartan 6-xc651 × 4c-3csg432 Digilent FPGA board. It is observed that the recognition time of our architecture in hardware (FPGA) is 1.05 µS which is better compared to existing methods.},
  archive      = {J_SOCO},
  author       = {Bhavikatti, Sumangala and Bhairannawar, Satish},
  doi          = {10.1007/s00500-025-10415-3},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1541-1552},
  shortjournal = {Soft Comput.},
  title        = {Efficient reconfigurable architecture to extract image features for face recognition using local binary pattern},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposition matheuristics for last mile delivery using
public transportation systems. <em>SOCO</em>, <em>29</em>(3), 1511–1539.
(<a href="https://doi.org/10.1007/s00500-025-10513-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the potential of using public transportation systems for freight delivery, where we intend to utilize the spare capacities of public vehicles like buses, trams, metros, and trains, particularly during off-peak hours, to transport packages within the city instead of using dedicated delivery vehicles. The study contributes to the growing literature on innovative strategies for performing sustainable last mile deliveries. We study an operational level problem called the Three-Tier Delivery Problem on Public Transportation, where packages are first transported from the Consolidation and Distribution Center (CDC) to nearby public vehicle stations by delivery trucks, comprising the first tier of the problem. In the second tier, the public vehicles pick them up from the stops and transport them into the city area. The last leg, or the third tier of the delivery, is performed to deliver the packages to their respective customers using green vehicles or eco-friendly systems. We propose mixed-integer linear programming formulations to study the transport of packages from the CDC to the customers and employ decomposition-based matheuristics to solve them. We have three decomposition approaches based on the order of solving the tiers, resulting from the tier we start solving the problem from. We use a heuristic methodology to link the tiers by coordinating the flow of packages between them, and utilize CPLEX to solve the individual tiers. We provide numerical experiments to demonstrate the efficiency and effectiveness of the system. Our results show that this system has the potential to reduce the length of trips performed by traditional delivery trucks by 85.91%, thereby reducing the negative social and environmental impacts of existing last mile delivery systems.},
  archive      = {J_SOCO},
  author       = {Mandal, Minakshi Punam and Archetti, Claudia},
  doi          = {10.1007/s00500-025-10513-2},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1511-1539},
  shortjournal = {Soft Comput.},
  title        = {Decomposition matheuristics for last mile delivery using public transportation systems},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure quantum homomorphic encryption ciphertext retrieval
scheme. <em>SOCO</em>, <em>29</em>(3), 1497–1509. (<a
href="https://doi.org/10.1007/s00500-025-10454-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent paper (Gong et al. Quantum Inf Process 19:3, 2020), a novel ciphertext retrieval scheme based on the Grover algorithm and quantum homomorphic encryption was presented. In this scheme, when the server performs the operation of marking the solution on the user’s encrypted state in the Grover iteration, it needs to remove many gate-errors generated in the homomorphic evaluation of the T gate. And the server could judge this specific solution from the quantum circuit of marking the solution. It makes this scheme unable to achieve the low-cost and secure ciphertext retrieval. Therefore, we improve the Gong et al.’s scheme and propose a secure quantum homomorphic encryption ciphertext retrieval scheme. In our scheme, the trusted third party is introduced to cooperate with the server to execute the Grover algorithm. In each Grover iteration, the trusted third party can quickly mark the solution on the plaintext state, encrypt the marked state, and transmit it to the server. Then the server performs the remaining operations of this Grover iteration on the encrypted state. The trusted third party finally decrypts the iterated state. This cooperative approach ensures that the number of auxiliary qubits required and extra quantum gates executed in our scheme are lower than the Gong et al.’s scheme. By analyzing the security of our scheme, we confirm that the server and the trusted third party will not be informed of this solution. Thus, our scheme realizes the secure ciphertext retrieval with low computational overhead. We utilize IBM’s Qiskit framework to simulate our scheme, and the experimental result shows that our scheme is correct. It is worth noting that the low-cost and secure ciphertext retrieval will play a crucial role in modern information security and privacy protection.},
  archive      = {J_SOCO},
  author       = {Cheng, Zhen-Wen and Chen, Xiu-Bo and Xu, Gang and Chang, Yan and Miao, Li-Hua and Yang, Yi-Xian and Wang, Ya-Lan},
  doi          = {10.1007/s00500-025-10454-w},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1497-1509},
  shortjournal = {Soft Comput.},
  title        = {A secure quantum homomorphic encryption ciphertext retrieval scheme},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing signature scheme: The enhanced edward
elgamal extreme performance accumulate signature approach for IoT and
blockchain applications. <em>SOCO</em>, <em>29</em>(3), 1473–1496. (<a
href="https://doi.org/10.1007/s00500-025-10426-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital signatures, essential for establishing trust in the digital realm, have evolved in their application and importance alongside emerging technologies such as the Internet of Things (IoT), Blockchain, and cryptocurrency. These advancements necessitate improvements in performance, security, and efficiency. This article examines and compares the Elliptic Curve Digital Signature Algorithm with the Hyper Elliptic Curve Digital Signature Algorithm and the Edwards Curve Digital Signature Algorithm. We highlight its superior capabilities for blockchain and IoT applications and advocate for its potential to deliver immediate enhancements in security and performance. Our study introduces a novel digital signature scheme specifically designed to enhance non-repudiation in blockchain ecosystems. Utilizing the Optimized Extreme Performance Edwards Curve Accumulated Signature scheme, our approach significantly reduces signing and verification times by 10% and 13%, respectively, compared to traditional signatures. Additionally, it offers a 10% boost in transaction throughput and block validation efficiency. Experiments conducted within various blockchain-integrated IoT setups demonstrate the scheme&#39;s effectiveness, consistently achieving improvements across diverse IoT sensor data. This highlights the innovative contribution of our scheme to the efficiency and security of blockchain technology.},
  archive      = {J_SOCO},
  author       = {Anusha, R. and Saravanan, R.},
  doi          = {10.1007/s00500-025-10426-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1473-1496},
  shortjournal = {Soft Comput.},
  title        = {Revolutionizing signature scheme: The enhanced edward elgamal extreme performance accumulate signature approach for IoT and blockchain applications},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank decomposition optimization and its application in
fabric defects. <em>SOCO</em>, <em>29</em>(3), 1453–1472. (<a
href="https://doi.org/10.1007/s00500-025-10399-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank decomposition model is frequently employed in defect detection. It separates the target matrix into a low-rank component and a sparse component using the nuclear norm and the $$l_1$$ -norm, which aids in extracting the background and defects. However, the nuclear norm, derived from singular value decomposition, often fails to effectively extract the background of fabrics. This paper introduces a novel matrix norm, defined by integrating several key elementary functions, enhancing the separation of the low-rank and sparse matrices. The Alternating Direction Method of Multipliers (ADMM) typically solves the low-rank decomposition model with a fixed step size penalty factor. This study dynamically adjusts the penalty factor based on defect detection characteristics, thus enhancing the algorithm’s computational efficiency. Additionally, the convergence of the proposed algorithm is validated. Experimental results demonstrate that this new model not only precisely distinguishes the sparse matrix but also achieves higher computational efficiency, surpassing other existing methods in both accuracy and efficiency.},
  archive      = {J_SOCO},
  author       = {Chen, Zhixiang and Shi, Wenya and Liang, Jiuzhen and Liu, Hao},
  doi          = {10.1007/s00500-025-10399-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1453-1472},
  shortjournal = {Soft Comput.},
  title        = {Low-rank decomposition optimization and its application in fabric defects},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient COVID-19 detection using data mining algorithms: A
comparison of basic and hybrid approaches. <em>SOCO</em>,
<em>29</em>(3), 1437–1451. (<a
href="https://doi.org/10.1007/s00500-025-10538-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient diagnosis of COVID-19 remains a significant challenge due to the limitations of current detection methods, such as blood tests and chest scans, which can be time-consuming and error-prone. This study aims to compare the performance of basic and hybrid data mining algorithms in diagnosing COVID-19, using blood test results and clinical information to identify the most effective approach. A dataset of 200 records from suspected and infected COVID-19 patients, with 23 characteristics and one diagnostic class, was analysed. Nine data mining algorithms were tested: four basic algorithms (Naive Bayes, Support Vector Machine, Decision Tree, K-Nearest Neighbor) and five hybrid algorithms (Random Forest, AdaBoost, Majority Voting, XGBoost, Bagging). The study also integrated Response Surface Methodology (RSM) and Adaptive-Network-based Fuzzy Inference System (ANFIS) to enhance model performance. The Bagging algorithm demonstrated superior performance with an accuracy of 88%, sensitivity of 74%, and F-criterion of 78%. The integration of RSM and ANFIS further showed that a smart model could be developed for efficient pandemic crisis management, achieving up to 100% accuracy when considering key factors like AST, Albumin, and CRP. The findings suggest that Bagging and hybrid data mining algorithms can significantly improve COVID-19 detection, reducing time and errors in identifying exposed individuals. The study highlights the potential of combining machine learning techniques with RSM-ANFIS models for effective pandemic management and decision-making in medical settings.},
  archive      = {J_SOCO},
  author       = {Saidi, Mohammad and Gheibi, Mohammad and Ghazikhani, Adel and Lotfata, Aynaz and Chahkandi, Benyamin and Familsamavati, Sajad and Behzadian, Kourosh},
  doi          = {10.1007/s00500-025-10538-7},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1437-1451},
  shortjournal = {Soft Comput.},
  title        = {Efficient COVID-19 detection using data mining algorithms: A comparison of basic and hybrid approaches},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging variant of CAE with sparse convolutional
embedding and two-stage application-driven data augmentation for image
clustering. <em>SOCO</em>, <em>29</em>(3), 1419–1435. (<a
href="https://doi.org/10.1007/s00500-025-10500-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep clustering approaches often struggle with redundant feature learning, which limits their effectiveness. The primary goal of this study is to address these issues by developing a more robust deep clustering method. To achieve this, we propose a variant of the convolutional autoencoder (CAE) called SCDAC, which incorporates sparse convolutional embedding and a two-stage application-driven data augmentation approach. The proposed model operates in two main stages: pretraining and finetuning. In the pretraining stage, we employ application-driven data augmentation to train the CAE variant, focusing on learning robust features and constructing a foundational feature space using sparse convolutional embedding. During the finetuning stage, the model performs joint feature learning and cluster assignment. The feature learning task utilizes an augmented framework to control the input of both original and augmented data, preserving the local structure of images in the feature space. For cluster assignment, the framework controls the input of original data and uses the sparse convolutional embedding layer to obtain low-dimensional representations for soft cluster assignment. Experimental evaluations on six publicly available datasets demonstrate the effectiveness of the proposed model, with significant improvements in accuracy, particularly increases of $$3\%$$ and $$10.3\%$$ on the COIL20 and ORL datasets, respectively. In conclusion, our findings underscore the significance of the SCDAC approach in enhancing deep image clustering performance, offering a viable solution to the limitations of existing methods.},
  archive      = {J_SOCO},
  author       = {Liu, Yanming and Liu, Jinglei},
  doi          = {10.1007/s00500-025-10500-7},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1419-1435},
  shortjournal = {Soft Comput.},
  title        = {Leveraging variant of CAE with sparse convolutional embedding and two-stage application-driven data augmentation for image clustering},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatgpt and operations research: Evaluation on the shortest
path problem. <em>SOCO</em>, <em>29</em>(3), 1407–1418. (<a
href="https://doi.org/10.1007/s00500-025-10505-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ChatGPT tool, the large language model developed by OpenAI, is having a great impact among users, experts and scholars for its capabilities of answering questions and retrieving solutions automatically. Despite the short time since its release, it has already been employed in several application domains. However, to the best of our knowledge, it has not been studied in the field of operation research (OR). In this paper, we use ChatGPT to define solution strategies for addressing several variants of the shortest path problem. The results obtained by executing the solution approaches returned by the tool are compared, in terms of correctness and efficiency, to reference codes. They indicate that the proper utilization of this tool could represent a good aid for domain experts. In particular, the outputs provided by ChatGPT could represent not only a good base for more complex implementations, but also they represent a way to facilitate some tasks in order to reduce times to do certain activities, which in any case must involve human control, adaptation and supervision.},
  archive      = {J_SOCO},
  author       = {Luzzi, Martina and Guerriero, Francesca and Maratea, Marco and Greco, Gianluigi and Garofalo, Marco},
  doi          = {10.1007/s00500-025-10505-2},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1407-1418},
  shortjournal = {Soft Comput.},
  title        = {Chatgpt and operations research: Evaluation on the shortest path problem},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using past sample means in exponential ratio and regression
type estimators under a simple random sampling. <em>SOCO</em>,
<em>29</em>(3), 1389–1406. (<a
href="https://doi.org/10.1007/s00500-025-10408-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical sampling commonly employs auxiliary variables for the selection and estimation phases to improve efficiency of the estimators. However, existing estimators like ratio and product types display limitations under specific conditions. Regression-type estimators, known for their unbiasedness and efficiency, rely solely on current sample information. This highlights the need for more effective estimators capable of leveraging both past and current sample means to improve accuracy and applicability across diverse datasets. In this study, we introduce two novel memory-type estimators, drawing inspiration from Noor-ul-Amin&#39;s (2020) approach, which integrates past and current sample information using Hybrid Exponentially Weighted Moving Averages (HEWMA), particularly effective for time-based surveys. Through simulation studies and real data examples, we evaluate the performance of our estimators and identify crucial shortcomings in previous memory-type estimator studies. Furthermore, we highlight significant deficits in previous studies, particularly concerning the impact of sample sizes based on past means, correlation, number of past means, weight parameters and initial values of EWMA and HEWMA algorithms, and the distribution shape of the data on estimator efficiency. Our findings underscore the importance of parameter selection in HEWMA, a greater number of past means, and the significance of past sample sizes for optimizing the performance of the proposed memory-type estimators. By integrating HEWMA, our approach enhances the efficiency and applicability of these estimators, addressing essential gaps in the existing literature and laying the groundwork for more robust and efficient estimation techniques for future studies that use mean.},
  archive      = {J_SOCO},
  author       = {Koçyiğit, Eda Gizem},
  doi          = {10.1007/s00500-025-10408-2},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1389-1406},
  shortjournal = {Soft Comput.},
  title        = {Using past sample means in exponential ratio and regression type estimators under a simple random sampling},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection of multiple types of data: A PLS
approach. <em>SOCO</em>, <em>29</em>(3), 1369–1387. (<a
href="https://doi.org/10.1007/s00500-025-10531-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of data collection techniques in recent years, multiple types of data have emerged, including scalar data, functional data (curve-like), and compositional data (pie-like). While existing studies propose predictive models for multiple-type of data, few address the issue of variable selection. The challenge lies in the fact that different data types originate from different vector spaces, making it difficult to conduct variable selection at the variable level instead of selection at their sub-component level. This study leverages the group selection ability of gPLS (group Partial Least Squares) and gsPLS (group sparse Partial Least Squares) by regarding the functional and compositional variables as natural groups and proposes two variable selection approaches, named MD-gPLS and MD-gsPLS, after building a vector space for multiple types of data. Numerical studies and real-world examples verify the effectiveness of the proposed approaches. This study broadens the statistical modeling tools of multiple types of data analysis in terms of variable selection and also contributes to the literature by introducing the vector space of multiple types of data.},
  archive      = {J_SOCO},
  author       = {Kong, Boao and Wang, Huiwen and Lu, Shan},
  doi          = {10.1007/s00500-025-10531-0},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1369-1387},
  shortjournal = {Soft Comput.},
  title        = {Variable selection of multiple types of data: A PLS approach},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictor–corrector approach for the numerical solution of
fuzzy fractional differential equations and linear multiterm fuzzy
fractional equations. <em>SOCO</em>, <em>29</em>(3), 1347–1368. (<a
href="https://doi.org/10.1007/s00500-025-10401-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the modeling of fuzzy fractional differential equations (FFDEs) has been a very significant issue in many new applications in applied sciences and engineering, while a natural tool for modeling such dynamical systems is to use fuzzy fractional differential equations. We establish the existence and uniqueness of solutions for fuzzy fractional differential equations under sufficient assumptions and contraction principles and study numerical solutions of FFDEs. Our study is based on Caputo’s generalized Hukuhara differentiability. By applying Schauder’s fixed point theorem and a hypothetical condition, we explore the existence of the solutions. In addition, we show the uniqueness of the system&#39;s solution by using the contraction mapping theorem. We analyze the predictor–corrector approach (PCA) for FFDEs and multiterm FFDEs. We utilize the PCA to find the approximate solutions to linear multiterm FFDEs under the Caputo fuzzy derivative. After that, we present numerical solutions to initial value problems for solving two families of fuzzy fractional problems: fuzzy fractional differential equations (FFDEs) and multiterm fuzzy fractional differential equations (MFFDEs) utilizing the PCA. The method used in this paper has several advantages; first, it is significant and yields stable results without diverging as well as its ability to solve other mathematical, physical, and engineering problems; second, it is higher accuracy, needs less effort to achieve the results and works to reduces the error between exact and approximate solutions, as depicted in the utilized figures and tables. Finally, the accuracy of our suggested approach is demonstrated by solving some specific examples and analyzing the figures and tables, along with several suggestions for future research directions.},
  archive      = {J_SOCO},
  author       = {Al-Sadi, Wadhah and Wei, Zhouchao and Moroz, Irene and Abu Arqub, Omar and Abdullah, Tariq Q. S.},
  doi          = {10.1007/s00500-025-10401-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1347-1368},
  shortjournal = {Soft Comput.},
  title        = {Predictor–corrector approach for the numerical solution of fuzzy fractional differential equations and linear multiterm fuzzy fractional equations},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach data processing: Density-based spatial
clustering of applications with noise (DBSCAN) clustering using
game-theory. <em>SOCO</em>, <em>29</em>(3), 1331–1346. (<a
href="https://doi.org/10.1007/s00500-025-10405-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unpredictable growth of data in various fields, rapid clustering of big data is seriously needed in order to identify the hidden structure of data and discover the relationships between objects. Among clustering methods, density-based clustering methods have an acceptable processing speed for dealing with big data with high dimensions. However, some methods have fixed parameters that are certainly not optimized for all sections. In addition, the complexity of these clustering methods strongly depends on the number of objects. In this paper, a clustering method is presented in order to increase clustering performance and parameter sensitivity according to game-theory and using the concept of Nash equilibrium and dense games, the optimal parameter for clustering is selected and between noise and points clusters make a difference. This method includes (1) searching the grid with several spaces in which there is no cluster, (2) identifying the player through high density data points in order to determine the parameters and (3) combining the clusters to make the game and (4) merging the nearby clusters. The performance of the proposed method was evaluated in four big synthetic datasets, eight real datasets labeled and unlabeled. The obtained results indicate the superiority of the proposed method over SOM, K-means, DBSCAN, SCGPSC methods in terms of accuracy and purity in processing time.},
  archive      = {J_SOCO},
  author       = {Kazemi, Uranus and Soleimani, Seyfollah},
  doi          = {10.1007/s00500-025-10405-5},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1331-1346},
  shortjournal = {Soft Comput.},
  title        = {A new approach data processing: Density-based spatial clustering of applications with noise (DBSCAN) clustering using game-theory},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arithmetic optimization algorithm with cosine
transform-based two-dimensional composite chaotic mapping.
<em>SOCO</em>, <em>29</em>(3), 1289–1329. (<a
href="https://doi.org/10.1007/s00500-025-10412-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arithmetic optimization algorithm (AOA) is a newly developed meta-heuristic algorithm that draws inspiration from the combination of arithmetic operations. Since many scholars have widely used traditional one-dimensional chaotic mapping at home and abroad in function optimization, the AOA based on cosine transform two-dimensional composite chaotic mapping is proposed. Firstly, seven two-dimensional chaotic mappings are proposed to be embedded into the MOA and MOP in AOA. Secondly, one-dimensional chaotic systems based on the cosine transform are put forward. Then the proposed chaotic system based on the cosine transform is combined with the two-dimensional chaotic mapping to form the cosine transformed two-dimensional composite chaotic mapping. Finally, six more cosine transformed two-dimensional composite chaotic mappings are embedded into the MOA and MOP of the AOA to balance the algorithm&#39;s global and local searching ability and improve the algorithm&#39;s performance. The superiority of the improved algorithm is verified by employing 12 benchmark test functions in CEC2022. Then it is compared with the Coati Optimization Algorithm (COA), Prairie Dog Optimization (PDO), Butterfly Optimization Algorithm (BOA), Reptile Search Algorithm (RSA), Bat Algorithm (BAT), and Rat Swarm Optimization (RSO) to verify its convergence. Finally, four engineering design problems (tension/compression spring problem, pressure vessel problem, cantilever beam design problem, and slotted bulkhead design problem) were optimized to validate the efficiency of the improved algorithm. The simulation experiments demonstrate that the improved AOA exhibits superior performance in addressing both function and engineering optimization problems. It showcases remarkable optimization capabilities and improves convergence accuracy.},
  archive      = {J_SOCO},
  author       = {Li, Yi-Xuan and Wang, Jie-Sheng and Zhang, Si-Wen and Zhang, Shi-Hui and Guan, Xin-Yi and Ma, Xin-Ru},
  doi          = {10.1007/s00500-025-10412-6},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1289-1329},
  shortjournal = {Soft Comput.},
  title        = {Arithmetic optimization algorithm with cosine transform-based two-dimensional composite chaotic mapping},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some new construction methods of similarity measure on
picture fuzzy sets. <em>SOCO</em>, <em>29</em>(3), 1273–1287. (<a
href="https://doi.org/10.1007/s00500-025-10536-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Picture fuzzy sets address problems characterized by ambiguity, instability and inconsistent data. Similarity measures on picture fuzzy sets play an indispensable role in determining the relationships between two such sets. Consequently, the study of similarity measures for picture fuzzy sets has garnered significant attention from scholars, yielding fruitful results. Notably, the existing research on picture fuzzy set similarity has mainly focused on overcoming the limitations of certain existing similarity measures by proposing one or a few new ones, ignoring the construction methods for similarity measures. Therefore, this paper presents two novel construction methods for similarity measures on picture fuzzy sets. The first approach combines the differences among positive membership, neutral membership, negative membership, and refusal membership within picture fuzzy sets using a strictly monotonically decreasing function. Remarkably, this method not only integrates existing similarity measures but also generates novel ones, providing a unified framework for both. The second method employs a strictly decreasing binary function to aggregate the distance measures between two picture fuzzy sets. By varying the binary function and distance measures, we obtain a range of novel similarity measures. Additionally, we apply the newly developed similarity measures to pattern recognition and compare their performance against existing measures. Based on the identification results, it is evident that these novel similarity measures yield reasonable outcomes and exhibit a high degree of reliability.},
  archive      = {J_SOCO},
  author       = {Luo, Minxia and Gao, Jianlei and Li, Wenling},
  doi          = {10.1007/s00500-025-10536-9},
  journal      = {Soft Computing},
  month        = {2},
  number       = {3},
  pages        = {1273-1287},
  shortjournal = {Soft Comput.},
  title        = {Some new construction methods of similarity measure on picture fuzzy sets},
  volume       = {29},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
