<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JAIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jair---14">JAIR - 14</h2>
<ul>
<li><details>
<summary>
(2025). Enhanced recommendation systems with retrieval-augmented
large language model. <em>JAIR</em>, <em>82</em>, 1147–1173. (<a
href="https://doi.org/10.1613/jair.1.17809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have long struggled with challenges such as cold start and data sparsity, which can lead to poor recommendation performance. While previous approaches have attempted to address these issues by incorporating side information, they often introduce noise, lack flexibility for data expansion, and suffer from inconsistent data quality—factors that hinder accurate user preference inference and reduce recommendation performance. With the vast knowledge bases and advanced reasoning capabilities of large language models (LLMs), these models are particularly well-suited to supplement auxiliary information and capture implicit user intent. To address these challenges, we propose a novel framework, ER2ALM, which leverages the capabilities of LLMs enhanced by Retrieval-Augmented Generation (RAG) to improve recommendation outcomes. Our framework specifically addresses the challenges by flexibly and accurately augmenting auxiliary information and capturing users’ implicit preferences and interests. Additionally, to mitigate the risk of introducing noise, we incorporate a noise reduction strategy to ensure the reliability of the augmented information. Experimental validation on two real-world datasets demonstrates the efficacy of our approach, significantly enhancing both the accuracy and robustness of recommendations compared to state-of-the-art methods. This demonstrates the potential of our framework as a new paradigm for preference mining in recommendation systems.},
  archive      = {J_JAIR},
  author       = {Chuyuan Wei and Ke Duan and Shengda Zhuo and Hongchun Wang and Shuqiang Huang and Jie Liu},
  doi          = {10.1613/jair.1.17809},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {1147-1173},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Enhanced recommendation systems with retrieval-augmented large language model},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control by adding or deleting edges in graph-restricted
weighted voting games. <em>JAIR</em>, <em>82</em>, 1077–1145. (<a
href="https://doi.org/10.1613/jair.1.16940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-restricted weighted voting games generalize weighted voting games, a well-studied class of succinct simple games, by embedding them into a communication structure: a graph whose vertices are the players some of which are connected by edges. In such games, only sufficiently connected coalitions are taken into consideration for calculating the players&#39; power indices. Focusing on the probabilistic Penrose-Banzhaf index (which Dubey and Shapley proposed in 1979 as an alternative to the normalized Penrose-Banzhaf index) and the Shapley-Shubik index, we study control of these games by an agent who can add edges to or delete edges from the given graph. We determine upper and lower bounds on how much such control actions can change a distinguished player&#39;s power and we study the computational complexity of the related problems.},
  archive      = {J_JAIR},
  author       = {Joanna Kaczmarek and Jörg Rothe and Nimrod Talmon},
  doi          = {10.1613/jair.1.16940},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {1077–1145},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Control by adding or deleting edges in graph-restricted weighted voting games},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical game theoretic analysis: A survey. <em>JAIR</em>,
<em>82</em>, 1017–1076. (<a
href="https://doi.org/10.1613/jair.1.16146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the empirical approach to game-theoretic analysis (EGTA), the model of the game comes not from declarative representation, but is derived by interrogation of a procedural description of the game environment. The motivation for developing this approach was to enable game-theoretic reasoning about strategic situations too complex for analytic specification and solution. Since its introduction over twenty years ago, EGTA has been applied to a wide range of multiagent domains, from auctions and markets to recreational games to cyber-security. We survey the extensive methodology developed for EGTA over the years, organized by the elemental subproblems comprising the EGTA process. We describe key EGTA concepts and techniques, and the questions at the frontier of EGTA research. Recent advances in machine learning are accelerating progress in EGTA, and promise to significantly expand our capacities for reasoning about complex game situations.},
  archive      = {J_JAIR},
  author       = {Michael P. Wellman and Karl Tuyls and Amy Greenwald},
  doi          = {10.1613/jair.1.16146},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {1017-1076},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Empirical game theoretic analysis: A survey},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new literature review of 3D object detection on autonomous
driving. <em>JAIR</em>, <em>82</em>, 973–1015. (<a
href="https://doi.org/10.1613/jair.1.15961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the realm of computer vision has experienced a significant surge in the importance of 3D object detection, especially in the context of autonomous driving. The capability to precisely identify the locations, dimensions, and types of key 3D objects surrounding an autonomous vehicle is crucial, rendering 3D object detection a vital component of any advanced perception system. This review delivers an extensive overview of the emerging technologies in 3D object detection tailored for autonomous vehicles. It encompasses a thorough examination, evaluation, and integration of the current research landscape in this domain, staying up-to-date with the latest advancements in 3D object detection and suggesting prospective avenues for future research. Our survey begins by clarifying the principles of 3D object detection and addressing its present challenges in the 3D domain. We then introduce three distinct taxonomies: camera-based, point cloudbased, and multi-modality-based approaches, providing a comprehensive classification of contemporary 3D object detection methodologies from various angles. Diverging from previous reviews, this paper also highlights and scrutinizes common issues and solutions for specific scenarios (such as pedestrian detection, lane lines, roadside cameras, and weather conditions) in object detection. Furthermore, we conduct an in-depth analysis and comparison of different classifications and methods, utilizing various datasets and experimental outcomes. Conclusively, we suggest several potential research directions, offering valuable insights for the ongoing evolution of 3D object detection technology. This review aims to serve as a comprehensive resource for researchers and practitioners in the field, guiding future innovations in 3D object detection for autonomous driving.},
  archive      = {J_JAIR},
  author       = {Peng Zhang and Xin Li and Xin Lin and Liang He},
  doi          = {10.1613/jair.1.15961},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {973-1015},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A new literature review of 3D object detection on autonomous driving},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Promoting the responsible development of speech datasets for
mental health and neurological disorders research. <em>JAIR</em>,
<em>82</em>, 937–972. (<a
href="https://doi.org/10.1613/jair.1.16406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation, less so on data collection. However, recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability. These aspects are particularly impactful on sensitive domains such as mental health and neurological disorders, where speech data are used to develop AI applications for patients and healthcare providers. In this paper, we chart the landscape of available speech datasets for this domain, to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity. We present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into an actionable checklist focused on ethical concerns to foster more responsible research.},
  archive      = {J_JAIR},
  author       = {Eleonora Mancini and Ana Tanevska and Andrea Galassi and Alessio Galatolo and Federico Ruggeri and Paolo Torroni},
  doi          = {10.1613/jair.1.16406},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {937-972},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Promoting the responsible development of speech datasets for mental health and neurological disorders research},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A query-based constraint acquisition approach for enhanced
precision in program precondition inference. <em>JAIR</em>, <em>82</em>,
901–936. (<a href="https://doi.org/10.1613/jair.1.16206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program annotations in the form of function pre/postconditions play a crucial role in various software engineering and program verification tasks. However, the frequent unavailability of these annotations necessitates manual retrofitting. This paper shows how constraint acquisition, a learning framework derived from constraint programming and version space learning, can be extended for automatically inferring program preconditions. Our approach performs this inference in a black-box manner through automatic query generation and input-output observations of program executions. We introduce PreCA, the first-ever precondition inference framework leveraging query-based constraint acquisition. Notably, we specialize PreCA to handle memory-related preconditions on binary code, which pose significant challenges in data and information management systems. In contrast to prior black-box techniques, PreCA provides well-defined guarantees. Specifically, it employs a sound and complete method to generate preconditions consistent with all the observed input-output relationships of the program. Furthermore, empirical evaluations on our benchmark demonstrate that PreCA outperforms the results of state-of-the-art approaches, delivering comparable or superior results in 5s, as opposed to the 1-hour runtime of existing approaches on identical machines. We also present two successful use cases from the standard libc and the mbedtls cryptographic library. PreCA notably infers for the former one a more precise precondition than specified in the documentation.},
  archive      = {J_JAIR},
  author       = {Grégoire Menguy and Sébastien Bardin and Arnaud Gotlieb and Nadjib Lazaar},
  doi          = {10.1613/jair.1.16206},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {901-936},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A query-based constraint acquisition approach for enhanced precision in program precondition inference},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient ontology-mediated query answering: Extending
DL-liteR and linear ELH. <em>JAIR</em>, <em>82</em>, 851–899. (<a
href="https://doi.org/10.1613/jair.1.16401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The OWL 2 QL profile of the OWL 2 Web Ontology Language, based on the family of description logics called DL-Lite, is designed so that data stored in a standard relational database system (RDBMS) can be queried through an ontology via a rewriting mechanism, i.e., by rewriting the query into an SQL query that is then answered by the RDBMS system, without any changes to the data. In this paper we propose a language whose expressive power goes beyond that of DL-Lite while still allowing query answering via rewriting of queries into unions of conjunctive two-way regular path queries (UC2RPQs) instead of SQL queries. Our language is an extension of both OWL 2 QL and linear ELH: OWL 2 QL is extended by allowing qualified existential quantification on the left-hand side of concept inclusion axioms, and linear ELH by allowing inverses in role inclusion axioms. We identify a syntactic property of the extended language that guarantees UC2RPQ-rewritability. We propose a novel rewriting technique for conjunctive queries (CQs) under our ontology language that makes use of nondeterministic finite state automata. We show that CQ answering in our setting is NLOGSPACE-complete with respect to data complexity and NP-complete for combined complexity; we also show that answering instance queries is NLOGSPACE-complete for data complexity and in PTIME for combined complexity.},
  archive      = {J_JAIR},
  author       = {Mirko M. Dimartino and Peter T. Wood and Andrea Cali and Alexandra Poulovassilis},
  doi          = {10.1613/jair.1.16401},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {851-899},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient ontology-mediated query answering: Extending DL-liteR and linear ELH},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Value preferences estimation and disambiguation in hybrid
participatory systems. <em>JAIR</em>, <em>82</em>, 819–850. (<a
href="https://doi.org/10.1613/jair.1.14958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding citizens’ values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants’ choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that “valuing is deliberatively consequential.” That is, if a participant’s choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value preferences estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone. Then, we introduce a disambiguation strategy that combines Natural Language Processing and Active Learning to address the detected inconsistencies between choices and motivations. We evaluate the proposed methods on a dataset of a large-scale survey on energy transition. The results show that explicitly addressing inconsistencies between choices and motivations improves the estimation of an individual’s value preferences. The disambiguation strategy does not show substantial improvements when compared to similar baselines—however, we discuss how the novelty of the approach can open new research avenues and propose improvements to address the current limitations.},
  archive      = {J_JAIR},
  author       = {Enrico Liscio and Luciano C. Siebert and Catholijn M. Jonker and Pradeep K. Murukannaiah},
  doi          = {10.1613/jair.1.14958},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {819-850},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Value preferences estimation and disambiguation in hybrid participatory systems},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reasoning about decidability of strategic logics with
imperfect information and perfect recall strategies. <em>JAIR</em>,
<em>82</em>, 777–817. (<a
href="https://doi.org/10.1613/jair.1.17237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In logics for strategic reasoning the main challenge is represented by their verification in contexts of imperfect information and perfect recall strategies. In this work, we show the combination of two techniques to approximate the verification of Alternating-time Temporal Logic (ATL∗ ) under imperfect information and perfect recall, which is known to be undecidable. Given a model M and a formula φ, we propose a verification procedure that generates sub-models of M in which each sub-model M′ satisfies a sub-formula φ′ of φ and the verification of φ′ in M′ is decidable. Then, we use CTL∗ model checking to provide a verification result of φ on M. In case the previous step does not give a final result, we exploit a runtime verification mechanism to provide some intermediate result. We prove that our procedure is sound and in the same complexity class of ATL∗ model checking under perfect information and perfect recall. Moreover, we present a tool that uses our procedure and provide experimental results.},
  archive      = {J_JAIR},
  author       = {Davide Catta and Angelo Ferrando and Vadim Malvone},
  doi          = {10.1613/jair.1.17237},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {777-817},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Reasoning about decidability of strategic logics with imperfect information and perfect recall strategies},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Against the achilles’ heel: A survey on red teaming for
generative models. <em>JAIR</em>, <em>82</em>, 687–775. (<a
href="https://doi.org/10.1613/jair.1.17654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safe use as various vulnerabilities are exposed. In light of this, the field of red teaming is undergoing fast-paced growth, highlighting the need for a comprehensive survey covering the entire pipeline and addressing emerging topics. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the “searcher” framework to unify various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around LLM-based agents, overkill of harmless queries, and the balance between harmlessness and helpfulness. Warning: This paper contains examples that may be offensive, harmful, or biased.},
  archive      = {J_JAIR},
  author       = {Lizhi Lin and Honglin Mu and Zenan Zhai and Minghan Wang and Yuxia Wang and Renxi Wang and Junjie Gao and Yixuan Zhang and Wanxiang Che and Timothy Baldwin and Xudong Han and Haonan Li},
  doi          = {10.1613/jair.1.17654},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {687-775},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Against the achilles&#39; heel: A survey on red teaming for generative models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of SAT-based and ASP-based algorithms for
inconsistency measurement. <em>JAIR</em>, <em>82</em>, 563–685. (<a
href="https://doi.org/10.1613/jair.1.16888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present algorithms based on satisfiability problem (SAT) solving, as well as answer set programming (ASP), for solving the problem of determining inconsistency degrees in propositional knowledge bases. We consider six different inconsistency measures whose respective decision problems lie on the first level of the polynomial hierarchy. Namely, these are the contension, forgetting-based, hitting set, max-distance, sum-distance, and hit-distance inconsistency measures. In an extensive experimental analysis, we compare the SAT-based and ASP-based approaches with each other, as well as with a set of naive baseline algorithms. Our results demonstrate that, overall, both the SAT-based and the ASP-based approaches clearly outperform the naive baseline methods in terms of runtime. The results further show that the proposed ASP-based approaches perform superior to the SAT-based ones with regard to all six inconsistency measures considered in this work. Moreover, we conduct additional experiments to explain the aforementioned results in greater detail.},
  archive      = {J_JAIR},
  author       = {Isabelle Kuhlmann and Anna Gessler and Vivien Laszlo and Matthias Thimm},
  doi          = {10.1613/jair.1.16888},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {563-685},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Comparison of SAT-based and ASP-based algorithms for inconsistency measurement},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAC-chernoff bounds: Understanding generalization in the
interpolation regime. <em>JAIR</em>, <em>82</em>, 503–562. (<a
href="https://doi.org/10.1613/jair.1.17036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a distribution-dependent PAC-Chernoff bound that exhibits perfect tightness for interpolators, even within over-parameterized model classes. This bound, which relies on basic principles of Large Deviation Theory, defines a natural measure of the smoothness of a model, characterized by simple real-valued functions. Building upon this bound and the new concept of smoothness, we present an unified theoretical framework revealing why certain interpolators show an exceptional generalization, while others falter. We theoretically show how a wide spectrum of modern learning methodologies, encompassing techniques such as ℓ2-norm, distance-from-initialization and input-gradient regularization, in combination with data augmentation, invariant architectures, and over-parameterization, collectively guide the optimizer toward smoother interpolators, which, according to our theoretical framework, are the ones exhibiting superior generalization performance. This study shows that distribution-dependent bounds serve as a powerful tool to understand the complex dynamics behind the generalization capabilities of over-parameterized interpolators.},
  archive      = {J_JAIR},
  author       = {Andres R. Masegosa and Luis A. Ortega},
  doi          = {10.1613/jair.1.17036},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {503-562},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {PAC-chernoff bounds: Understanding generalization in the interpolation regime},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI reliance and decision quality: Fundamentals,
interdependence, and the effects of interventions. <em>JAIR</em>,
<em>82</em>, 471–501. (<a
href="https://doi.org/10.1613/jair.1.15873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In AI-assisted decision-making, a central promise of having a human-in-the-loop is that they should be able to complement the AI system by overriding its wrong recommendations. In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice. Different ways of relying on AI recommendations have immediate, yet distinct, implications for decision quality. Unfortunately, reliance and decision quality are often inappropriately conflated in the current literature on AI-assisted decision-making. In this work, we disentangle and formalize the relationship between reliance and decision quality, and we characterize the conditions under which human-AI complementarity is achievable. To illustrate how reliance and decision quality relate to one another, we propose a visual framework and demonstrate its usefulness for interpreting empirical findings, including the effects of interventions like explanations. Overall, our research highlights the importance of distinguishing between reliance behavior and decision quality in AI-assisted decision-making.},
  archive      = {J_JAIR},
  author       = {Jakob Schoeffer and Johannes Jakubik and Michael Vössing and Niklas Kühl and Gerhard Satzger},
  doi          = {10.1613/jair.1.15873},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {471–501},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {AI reliance and decision quality: Fundamentals, interdependence, and the effects of interventions},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oracle-guided approach to constrained policy synthesis
under uncertainty. <em>JAIR</em>, <em>82</em>, 433–469. (<a
href="https://doi.org/10.1613/jair.1.16593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with aleatoric uncertainty is key in many domains involving sequential decision making, e.g., planning in AI, network protocols, and symbolic program synthesis. This paper presents a general-purpose model-based framework to obtain policies operating in uncertain environments in a fully automated manner. The new concept of coloured Markov Decision Processes (MDPs) enables a succinct representation of a wide range of synthesis problems. A coloured MDP describes a collection of possible policy configurations with their structural dependencies. The framework covers the synthesis of (a) programmatic policies from probabilistic program sketches and (b) finite-state controllers representing policies for partially observable MDPs (POMDPs), including decentralised POMDPs as well as constrained POMDPs. We show that all these synthesis problems can be cast as exploring memoryless policies in the corresponding coloured MDP. This exploration uses a symbiosis of two orthogonal techniques: abstraction refinement—using a novel refinement method—and counter-example generalisation. Our approach outperforms dedicated synthesis techniques on some problems and significantly improves an earlier version of this framework.},
  archive      = {J_JAIR},
  author       = {Roman Andriushchenko and Milan Češka and Filip Macák and Sebastian Junges and Joost-Pieter Katoen},
  doi          = {10.1613/jair.1.16593},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {433-469},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An oracle-guided approach to constrained policy synthesis under uncertainty},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
