<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>acm_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="acm">ACM</h1>
<h2 id="cacm---21">CACM - 21</h2>
<ul>
<li><details>
<summary>
(2025). A heavenly host. <em>CACM</em>, <em>68</em>(2), 108–ff. (<a
href="https://doi.org/10.1145/3708556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708556},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {108-ff},
  shortjournal = {Commun. ACM},
  title        = {A heavenly host},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asleep at the keyboard? Assessing the security of GitHub
copilot’s code contributions. <em>CACM</em>, <em>68</em>(2), 96–105. (<a
href="https://doi.org/10.1145/3610721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described “AI pair programmer,” GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs—and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot’s code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, for example, those from MITRE’s “Top 25” Common Weakness Enumeration (CWE) list. We explore Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.},
  archive      = {J_CACM},
  doi          = {10.1145/3610721},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {96-105},
  shortjournal = {Commun. ACM},
  title        = {Asleep at the keyboard? assessing the security of GitHub copilot’s code contributions},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsafe code still a hurdle copilot must clear.
<em>CACM</em>, <em>68</em>(2), 95. (<a
href="https://doi.org/10.1145/3660529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3660529},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {95},
  shortjournal = {Commun. ACM},
  title        = {Unsafe code still a hurdle copilot must clear},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Negative-weight single-source shortest paths in near-linear
time. <em>CACM</em>, <em>68</em>(2), 87–94. (<a
href="https://doi.org/10.1145/3631536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the single-source shortest paths problem, the goal is to compute the shortest path tree from a designated source vertex in a weighted, directed graph. We present the first near-linear time algorithm for the problem that can also handle negative edge-weights; the runtime is O ( m log 8 ( n ) log W ) . In contrast to all recent developments that rely on sophisticated continuous optimization methods and dynamic algorithms, our algorithm is simple: it requires only a simple graph decomposition and elementary combinatorial tools. In fact, ours is the first combinatorial algorithm for negative-weight single-source shortest paths to break through the classic O ~ ( m n log W ) bound from over three decades ago (Gabow and Tarjan, SICOMP’89.)},
  archive      = {J_CACM},
  doi          = {10.1145/3631536},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {87-94},
  shortjournal = {Commun. ACM},
  title        = {Negative-weight single-source shortest paths in near-linear time},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shortening the path to designing efficient graph algorithms.
<em>CACM</em>, <em>68</em>(2), 86. (<a
href="https://doi.org/10.1145/3660528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3660528},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {86},
  shortjournal = {Commun. ACM},
  title        = {Shortening the path to designing efficient graph algorithms},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Program correctness through self-certification.
<em>CACM</em>, <em>68</em>(2), 74–84. (<a
href="https://doi.org/10.1145/3689624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3689624},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {74-84},
  shortjournal = {Commun. ACM},
  title        = {Program correctness through self-certification},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Justice, equity, diversity, and inclusion at UbiComp/ISWC:
Best practices for accessible and equitable computing conferences.
<em>CACM</em>, <em>68</em>(2), 64–73. (<a
href="https://doi.org/10.1145/3689820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3689820},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {64-73},
  shortjournal = {Commun. ACM},
  title        = {Justice, equity, diversity, and inclusion at UbiComp/ISWC: Best practices for accessible and equitable computing conferences},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta’s hyperscale infrastructure: Overview and insights.
<em>CACM</em>, <em>68</em>(2), 52–63. (<a
href="https://doi.org/10.1145/3701296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701296},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {52-63},
  shortjournal = {Commun. ACM},
  title        = {Meta’s hyperscale infrastructure: Overview and insights},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Questioning the criteria for evaluating non-cryptographic
hash functions. <em>CACM</em>, <em>68</em>(2), 46–51. (<a
href="https://doi.org/10.1145/3704255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3704255},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {46-51},
  shortjournal = {Commun. ACM},
  title        = {Questioning the criteria for evaluating non-cryptographic hash functions},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). It is time to standardize principles and practices for
software memory safety. <em>CACM</em>, <em>68</em>(2), 40–45. (<a
href="https://doi.org/10.1145/3708553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708553},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {40-45},
  shortjournal = {Commun. ACM},
  title        = {It is time to standardize principles and practices for software memory safety},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Life lessons from the first half-century of my career.
<em>CACM</em>, <em>68</em>(2), 36–39. (<a
href="https://doi.org/10.1145/3637905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3637905},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {36-39},
  shortjournal = {Commun. ACM},
  title        = {Life lessons from the first half-century of my career},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building on shaky ground. <em>CACM</em>, <em>68</em>(2),
34–35. (<a href="https://doi.org/10.1145/3707467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3707467},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {34-35},
  shortjournal = {Commun. ACM},
  title        = {Building on shaky ground},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical credit. <em>CACM</em>, <em>68</em>(2), 30–33. (<a
href="https://doi.org/10.1145/3690043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3690043},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {30-33},
  shortjournal = {Commun. ACM},
  title        = {Technical credit},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence then and now. <em>CACM</em>,
<em>68</em>(2), 24–29. (<a
href="https://doi.org/10.1145/3708554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708554},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {24-29},
  shortjournal = {Commun. ACM},
  title        = {Artificial intelligence then and now},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating research software engineering: Toward RSE
research. <em>CACM</em>, <em>68</em>(2), 20–23. (<a
href="https://doi.org/10.1145/3685265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3685265},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {20-23},
  shortjournal = {Commun. ACM},
  title        = {Investigating research software engineering: Toward RSE research},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain implants give people back what they lost.
<em>CACM</em>, <em>68</em>(2), 17–19. (<a
href="https://doi.org/10.1145/3701222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701222},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {17-19},
  shortjournal = {Commun. ACM},
  title        = {Brain implants give people back what they lost},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The evolution of computer science at the university level.
<em>CACM</em>, <em>68</em>(2), 14–16. (<a
href="https://doi.org/10.1145/3701223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701223},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {14-16},
  shortjournal = {Commun. ACM},
  title        = {The evolution of computer science at the university level},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can LLMs make robots smarter? <em>CACM</em>, <em>68</em>(2),
11–13. (<a href="https://doi.org/10.1145/3701227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701227},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {11-13},
  shortjournal = {Commun. ACM},
  title        = {Can LLMs make robots smarter?},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Go-ing to the cloud. <em>CACM</em>, <em>68</em>(2), 8–9. (<a
href="https://doi.org/10.1145/3701221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3701221},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {8-9},
  shortjournal = {Commun. ACM},
  title        = {Go-ing to the cloud},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fit for people, fit for purpose: Designing tech that
matters. <em>CACM</em>, <em>68</em>(2), 7. (<a
href="https://doi.org/10.1145/3708470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3708470},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {7},
  shortjournal = {Commun. ACM},
  title        = {Fit for people, fit for purpose: Designing tech that matters},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building safer and interoperable AI systems. <em>CACM</em>,
<em>68</em>(2), 5. (<a href="https://doi.org/10.1145/3709744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CACM},
  doi          = {10.1145/3709744},
  journal      = {Communications of the ACM},
  month        = {2},
  number       = {2},
  pages        = {5},
  shortjournal = {Commun. ACM},
  title        = {Building safer and interoperable AI systems},
  volume       = {68},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="csur---27">CSUR - 27</h2>
<ul>
<li><details>
<summary>
(2025). Explaining the explainers in graph neural networks: A
comparative study. <em>CSUR</em>, <em>57</em>(5), 1–37. (<a
href="https://doi.org/10.1145/3696444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following a fast initial breakthrough in graph-based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process. GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable or which explainer should be preferred in a given setting. In this survey we fill these gaps by devising a systematic experimental study, which tests 12 explainers on eight representative message-passing architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the choice and applicability of GNN explainers, we isolate key components that make them usable and successful and provide recommendations on how to avoid common interpretation pitfalls. We conclude by highlighting open questions and directions of possible future research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3696444},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {Explaining the explainers in graph neural networks: A comparative study},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy AI-based performance diagnosis systems for cloud
applications: A review. <em>CSUR</em>, <em>57</em>(5), 1–37. (<a
href="https://doi.org/10.1145/3701740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance diagnosis systems are defined as detecting abnormal performance phenomena and play a crucial role in cloud applications. An effective performance diagnosis system is often developed based on artificial intelligence (AI) approaches, which can be summarized into a general framework from data to models. However, the AI-based framework has potential hazards that could degrade the user experience and trust. For example, a lack of data privacy may compromise the security of AI models, and low robustness can be hard to apply in complex cloud environments. Therefore, defining the requirements for building a trustworthy AI-based performance diagnosis system has become essential. This article systematically reviews trustworthiness requirements in AI-based performance diagnosis systems. We first introduce trustworthiness requirements and extract six key requirements from a technical perspective, including data privacy, fairness, robustness, explainability, efficiency, and human intervention. We then unify these requirements into a general performance diagnosis framework, ranging from data collection to model development. Next, we comprehensively provide related works for each component and concrete actions to improve trustworthiness in the framework. Finally, we identify possible research directions and challenges for the future development of trustworthy AI-based performance diagnosis systems.},
  archive      = {J_CSUR},
  doi          = {10.1145/3701740},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {Trustworthy AI-based performance diagnosis systems for cloud applications: A review},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT authentication protocols: Challenges, and comparative
analysis. <em>CSUR</em>, <em>57</em>(5), 1–43. (<a
href="https://doi.org/10.1145/3703444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-evolving information technology landscape, the Internet of Things (IoT) is a groundbreaking concept that bridges the physical and digital worlds. It is the backbone of an increasingly sophisticated interactive environment, yet it is a subject of intricate security challenges spawned by its multifaceted manifestations. Central to securing IoT infrastructures is the crucial aspect of authentication, necessitating a comprehensive examination of its nuances, including benefits, challenges, opportunities, trends, and societal implications. In this article, we thoroughly review the IoT authentication protocols (Aps), addressing the main challenges such as privacy protection, scalability, and human factors that may impact security. Through exacting analysis, we evaluate the strengths and weaknesses of existing APs and conduct a comparative performance analysis to evaluate their effectiveness and scalability in securing IoT environments and devices. At the end of this study, we summarize the main findings and suggest ways to improve the security of IoT devices in the future.},
  archive      = {J_CSUR},
  doi          = {10.1145/3703444},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-43},
  shortjournal = {ACM Comput. Surv.},
  title        = {IoT authentication protocols: Challenges, and comparative analysis},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The internet of bio-nano things with insulin-glucose,
security and research challenges: A survey. <em>CSUR</em>,
<em>57</em>(5), 1–42. (<a
href="https://doi.org/10.1145/3703448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Bio-Nano Things (IoBNT) is collaborative cell biology and nanodevice technology interacting through Molecular Communication (MC). The IoBNT can be accomplished by using the Information and Communication Theory (ICT) study of biological networks. Various technologies such as the Internet of Nano Things (IoNT), the Internet of Bio-degradable Things (IoBDT), and the Internet of Ingestible Things (IoIT) contribute to the development of IoBNT. Our survey discussed the Bio-Nano network and the role of IoT-based technologies along with a comparative study from various literature. We surveyed the various applications of IoNT in which the drug delivery for Insulin-Glucose system is prominent. Our survey aims to provide information about the Insulin-Glucose system involving the IoBNT and MC. We described the details of various factors for a diabetes analysis. We surveyed the diffusion coefficients of Insulin, Glucose, and the various parameters that influence insulin production in the body. Our survey identifies the security aspects of IoBNT such as attacks in nanonetworks, bio-cyber interface, Insulin-Glucose system, and their possible mitigation techniques. Our survey also provides a hierarchical model of the Bio-Nano network collaborating all the related technologies. Finally, our survey includes the research challenges involved in the proper handling of the IoBNT.},
  archive      = {J_CSUR},
  doi          = {10.1145/3703448},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-42},
  shortjournal = {ACM Comput. Surv.},
  title        = {The internet of bio-nano things with insulin-glucose, security and research challenges: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of AI-generated content (AIGC). <em>CSUR</em>,
<em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3704262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Artificial Intelligence Generated Content (AIGC) has gained significant attention from society, especially with the rise of Generative AI (GAI) techniques such as ChatGPT, GPT-4 [ 165 ], DALL-E-3 [ 184 ], and Sora [ 137 ]. AIGC involves using AI models to create digital content, such as images, music, and natural language, with the goal of making the content creation process more efficient and accessible. Large-scale models have become increasingly important in AIGC as they provide better intent extraction and generation results. This survey provides a comprehensive review of the history of generative models and recent advances in AIGC, focusing on both unimodal and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, the survey discusses the existing open problems and future challenges in AIGC. Overall, this survey serves as a valuable resource for individuals interested in understanding the background and secrets behind the impressive performance of AIGC techniques.},
  archive      = {J_CSUR},
  doi          = {10.1145/3704262},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey of AI-generated content (AIGC)},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances of foundation language models-based
continual learning: A survey. <em>CSUR</em>, <em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3705725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing and computer vision. Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich common sense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this article, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models, large language models, and vision-language models. We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.},
  archive      = {J_CSUR},
  doi          = {10.1145/3705725},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {Recent advances of foundation language models-based continual learning: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wi-fi sensing techniques for human activity recognition:
Brief survey, potential challenges, and research directions.
<em>CSUR</em>, <em>57</em>(5), 1–30. (<a
href="https://doi.org/10.1145/3705893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in wireless communication technologies have made Wi-Fi signals indispensable in both personal and professional settings. The utilization of these signals for Human Activity Recognition (HAR) has emerged as a cutting-edge technology. By leveraging the fluctuations in Wi-Fi signals for HAR, this approach offers enhanced privacy compared to traditional visual surveillance methods. The essence of this technique lies in detecting subtle changes when Wi-Fi signals interact with the human body, which are then captured and interpreted by advanced algorithms. This article initially provides an overview of the key methodologies in HAR and the evolution of non-contact sensing, introducing sensor-based recognition, computer vision, and Wi-Fi signal based approaches, respectively. It then explores tools for Wi-Fi-based HAR signal collection and lists several high-quality datasets. Subsequently, the article reviews various sensing tasks enabled by Wi-Fi signal recognition, highlighting the application of deep learning networks in Wi-Fi signal detection. Experimental results are then presented that assess the capabilities of different networks. The findings indicate significant variability in the generalization capacities of neural networks and notable differences in test accuracy for various motion analyses.},
  archive      = {J_CSUR},
  doi          = {10.1145/3705893},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Comput. Surv.},
  title        = {Wi-fi sensing techniques for human activity recognition: Brief survey, potential challenges, and research directions},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient algorithms and systems of foundation
models: A survey. <em>CSUR</em>, <em>57</em>(5), 1–39. (<a
href="https://doi.org/10.1145/3706418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large foundation models, including large language models, vision transformers, diffusion, and large language model based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706418},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-39},
  shortjournal = {ACM Comput. Surv.},
  title        = {Resource-efficient algorithms and systems of foundation models: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial binaries: AI-guided instrumentation methods for
malware detection evasion. <em>CSUR</em>, <em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3706573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial binaries are executable files that have been altered without loss of function by an AI agent in order to deceive malware detection systems. Progress in this emergent vein of research has been constrained by the complex and rigid structure of executable files. Although prior work has demonstrated that these binaries deceive a variety of malware classification models which rely on disparate feature sets, a consensus as to the best approach has not been reached, either in terms of the optimization algorithms or the instrumentation methods. Although inconsistencies in the data sets, target classifiers, and functionality verification methods make head-to-head comparisons difficult, we extract lessons learned and make recommendations for future research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706573},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {Adversarial binaries: AI-guided instrumentation methods for malware detection evasion},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review of enterprise architecture
evaluation methods. <em>CSUR</em>, <em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3706582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise Architecture (EA) is a systematic and holistic approach to designing and managing an organization&#39;s information systems components, aiding in optimizing resources, managing risk, and facilitating change. It weighs different architectural quality attributes against each other to achieve the most advantageous architecture. However, the evaluation of EA lacks a systematic approach. This study employs a Systematic Literature Review, analyzing, in detail, 109 articles carefully selected from 3644 papers published since 2005. The key outcome of the research reveals that a crucial factor for the extensive worldwide adoption of EA evaluation methods lies in the automation of the assessment and architecture modeling processes, particularly emphasizing the facet of data collection. The automation of EA evaluation will empower organizations to streamline their processes, make data-driven decisions, and respond more effectively to change, ultimately contributing to their competitiveness and long-term success in the global market. The study identifies diverse evaluation methods, determines evaluation criteria, examines the extent to which these methods have been verified in practice, and provides directions for further research and advancement.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706582},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {A systematic literature review of enterprise architecture evaluation methods},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on privacy-preserving caching at network edge:
Classification, solutions, and challenges. <em>CSUR</em>,
<em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3706630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caching content at the edge network is a popular and effective technique widely deployed to alleviate the burden of network backhaul, shorten service delay, and improve service quality. However, there has been some controversy over privacy violations in caching content at the edge network. On the one hand, the multi-access open edge network provides an ideal entrance or interface for external attackers to obtain private data from edge caches by extracting sensitive information. On the other hand, privacy can be infringed on by curious edge caching providers through caching trace analysis targeting the achievement of better caching performance or higher profits. Therefore, an in-depth understanding of privacy issues in edge caching networks is vital and indispensable for creating a privacy-preserving caching service at the edge network. In this article, we are among the first to fill this gap by examining privacy-preserving techniques for caching content at the edge network. First, we provide an introduction to the background of privacy-preserving edge caching. Next, we summarize the key privacy issues and present a taxonomy for caching at the edge network from the perspective of private information. Additionally, we conduct a retrospective review of the state-of-the-art countermeasures against privacy leakage from content caching at the edge network. Finally, we conclude the survey and envision challenges for future research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3706630},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey on privacy-preserving caching at network edge: Classification, solutions, and challenges},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised semantic segmentation with image-level
labels: From traditional models to foundation models. <em>CSUR</em>,
<em>57</em>(5), 1–29. (<a
href="https://doi.org/10.1145/3707447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning has driven significant progress in image semantic segmentation—a fundamental task in computer vision. Semantic segmentation algorithms often depend on the availability of pixel-level labels (i.e., masks of objects), which are expensive, time consuming, and labor intensive. Weakly supervised semantic segmentation (WSSS) is an effective solution to avoid such labeling. It utilizes only partial or incomplete annotations and provides a cost-effective alternative to fully supervised semantic segmentation. In this article, our focus is on the WSSS with image-level labels, which is the most challenging form of WSSS. Our work has two parts. First, we conduct a comprehensive survey on traditional methods, primarily focusing on those presented at premier research conferences. We categorize them into four groups based on where their methods operate: pixel-wise, image-wise, cross-image, and external data. Second, we investigate the applicability of visual foundation models, such as the Segment Anything Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing scenarios: text prompting and zero-shot learning. We provide insights into the potential and challenges of deploying visual foundational models for WSSS, facilitating future developments in this exciting research area. Our code is provided at this link: https://github.com/zhaozhengChen/SAM_WSSS .},
  archive      = {J_CSUR},
  doi          = {10.1145/3707447},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-29},
  shortjournal = {ACM Comput. Surv.},
  title        = {Weakly-supervised semantic segmentation with image-level labels: From traditional models to foundation models},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial machine learning attacks and defences in
multi-agent reinforcement learning. <em>CSUR</em>, <em>57</em>(5), 1–35.
(<a href="https://doi.org/10.1145/3708320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) is susceptible to Adversarial Machine Learning (AML) attacks. Execution-time AML attacks against MARL are complex due to effects that propagate across time and between agents. To understand the interaction between AML and MARL, this survey covers attacks and defences for MARL, Multi-Agent Learning (MAL), and Deep Reinforcement Learning (DRL). This survey proposes a novel perspective on AML attacks based on attack vectors. This survey also proposes a framework that addresses gaps in current modelling frameworks and enables the comparison of different attacks against MARL. Lastly, the survey identifies knowledge gaps and future avenues of research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708320},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {Adversarial machine learning attacks and defences in multi-agent reinforcement learning},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed machine learning in edge computing: Challenges,
solutions and future directions. <em>CSUR</em>, <em>57</em>(5), 1–37.
(<a href="https://doi.org/10.1145/3708495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning on edges is widely used in intelligent transportation, smart home, industrial manufacturing, and underground pipe network monitoring to achieve low latency and real time data processing and prediction. However, the presence of a large number of sensing and edge devices with limited computing, storage, and communication capabilities prevents the deployment of huge machine learning models and hinders its application. At the same time, although distributed machine learning on edges forms an emerging and rapidly growing research area, there has not been a systematic survey on this topic. The article begins by detailing the challenges of distributed machine learning in edge environments, such as limited node resources, data heterogeneity, privacy, security issues, and summarizes common metrics for model optimization. We then present a detailed analysis of parallelism patterns, distributed architectures, and model communication and aggregation schemes in edge computing. we subsequently present a comprehensive classification and intensive description of node resource-constrained processing, heterogeneous data processing, attacks and protection of privacy. The article ends by summarizing the applications of distributed machine learning in edge computing and presenting problems and challenges for further research.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708495},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {Distributed machine learning in edge computing: Challenges, solutions and future directions},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on physical layer authentication
techniques: Categorization and analysis of model-driven and data-driven
approaches. <em>CSUR</em>, <em>57</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3708496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The open and broadcast nature of wireless mediums introduces significant security vulnerabilities, making authentication a critical concern in wireless networks. In recent years, Physical-Layer Authentication (PLA) techniques have garnered considerable research interest due to their advantages over Upper-Layer Authentication (ULA) methods, such as lower complexity, enhanced security, and greater compatibility. The application of signal processing techniques in PLA serves as a crucial link between the extraction of Physical-Layer Features (PLFs) and the authentication of received signals. Different signal processing approaches, even with the same PLF, can result in varying authentication performances and computational demands. Despite this, there remains a shortage of comprehensive overviews on state-of-the-art PLA schemes with a focus on signal processing approaches. This article presents the first thorough survey of signal processing in various PLA schemes, categorizing existing approaches into model-based and Machine Learning (ML)-based schemes. We discuss motivation and address key issues in signal processing for PLA schemes. The applications, challenges, and future research directions of PLA are discussed in Part 3 of the Appendix, which can be found in supplementary materials online.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708496},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {A comprehensive survey on physical layer authentication techniques: Categorization and analysis of model-driven and data-driven approaches},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards trustworthy machine learning in production: An
overview of the robustness in MLOps approach. <em>CSUR</em>,
<em>57</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3708497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708497},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {Towards trustworthy machine learning in production: An overview of the robustness in MLOps approach},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of geometric optimization for deep learning: From
euclidean space to riemannian manifold. <em>CSUR</em>, <em>57</em>(5),
1–37. (<a href="https://doi.org/10.1145/3708498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) has achieved remarkable success in tackling complex Artificial Intelligence tasks. The standard training of neural networks employs backpropagation to compute gradients and utilizes various optimization algorithms in the Euclidean space ℛ n . However, this optimization process faces challenges, such as the local optimal issues and the problem of gradient vanishing and exploding. To address these problems, Riemannian optimization offers a powerful extension to solve optimization problems in deep learning. By incorporating the prior constraint structure and the metric information of the underlying geometric information, Riemannian optimization-based DL offers a more stable and reliable optimization process, as well as enhanced adaptability to complex data structures. This article presents a comprehensive survey of applying geometric optimization in DL, including the basic procedure of geometric optimization, various geometric optimizers, and some concepts of the Riemannian manifold. In addition, it investigates various applications of geometric optimization in different DL networks for diverse tasks and discusses typical public toolboxes that implement optimization on the manifold. This article also includes a performance comparison among different deep geometric optimization methods in image recognition scenarios. Finally, this article elaborates on future opportunities and challenges in this field.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708498},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-37},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey of geometric optimization for deep learning: From euclidean space to riemannian manifold},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent generation of graphical game assets: A
conceptual framework and systematic review of the state of the art.
<em>CSUR</em>, <em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3708499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural content generation (PCG) can be applied to a wide variety of tasks in games, from narratives, levels, and sounds to trees and weapons. A large amount of game content is composed of graphical assets , such as clouds, buildings, or vegetation, that do not require gameplay function considerations. There is also a breadth of literature examining the procedural generation of such elements for purposes outside of games. The body of research, focused on specific methods for generating specific assets, provides a narrow view of the available possibilities. Hence, it is difficult to have a clear picture of all approaches and possibilities, with no guide for interested parties to discover possible methods and approaches for their needs and no facility to guide them through each technique or approach to map out the process of using them. Therefore, a systematic literature review has been conducted, yielding 239 accepted papers. This article explores state-of-the-art approaches to graphical asset generation, examining research from a wide range of applications, inside and outside of games. Informed by the literature, a conceptual framework has been derived to address the aforementioned gaps.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708499},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {Intelligent generation of graphical game assets: A conceptual framework and systematic review of the state of the art},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterization of android malwares and their families.
<em>CSUR</em>, <em>57</em>(5), 1–31. (<a
href="https://doi.org/10.1145/3708500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, smartphones have made our lives easier and have become essential gadgets for us. Apart from calling, mobiles are used for various purposes, such as banking, chatting, data storage, connecting to the internet, and running apps, that make life easier. Therefore, attackers are developing new methods or malware to steal smartphone data. Primarily, the study outlines various types of Android malware families, the evolution of Android malware and its effects on detection techniques over time. We report malware timelines and Android app datasets with their source web links. Data are collected from various recent studies and reported. In this study, we have reported 384 Android malware families and their year of discovery, i.e., from 2001 to 2020. According to the malfunctions they perform on the device, we categorized the families into 11 types. Information about datasets is divided into three categories, along with their source links, and is presented. The categorization and timeline of malware will make it easy for researchers to focus on upcoming trends according to the malware category and activities they perform. Various open issues and future challenges are also addressed for future researchers.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708500},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-31},
  shortjournal = {ACM Comput. Surv.},
  title        = {Characterization of android malwares and their families},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual content privacy protection: A survey. <em>CSUR</em>,
<em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3708501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision is the most important sense for people, and it is also one of the main ways of cognition. As a result, people tend to utilize visual content to capture and share their life experiences, which greatly facilitates the transfer of information. Meanwhile, it also increases the risk of privacy violations, e.g., an image or video can reveal different kinds of privacy-sensitive information. Scholars have persistently pursued the advancement of tailored privacy protection measures. Various surveys attempt to consolidate these efforts from specific viewpoints. Nevertheless, these surveys tend to focus on particular issues, scenarios, or technologies, hindering a comprehensive overview of existing solutions on a broader scale. In this survey, a framework that encompasses various concerns and solutions for visual privacy is proposed, which allows for a macro understanding of privacy concerns from a comprehensive level. It is based on the fact that privacy concerns have corresponding adversaries, and divides privacy protection into three categories, based on computer vision (CV) adversary, based on human vision (HV) adversary, and based on CV &amp; HV adversary. For each category, we analyze the characteristics of the main approaches to privacy protection, and then systematically review representative solutions. Open challenges and future directions for visual privacy protection are also discussed.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708501},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {Visual content privacy protection: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISP meets deep learning: A survey on deep learning methods
for image signal processing. <em>CSUR</em>, <em>57</em>(5), 1–44. (<a
href="https://doi.org/10.1145/3708516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entire Image Signal Processor (ISP) of a camera relies on several processes to transform the data from the Color Filter Array (CFA) sensor, such as demosaicing, denoising, and enhancement. These processes can be executed either by some hardware or via software. In recent years, Deep Learning (DL) has emerged as one solution for some of them or even to replace the entire ISP using a single neural network for the task. In this work, we investigated several recent pieces of research in this area and provide deeper analysis and comparison among them, including results and possible points of improvement for future researchers.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708516},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-44},
  shortjournal = {ACM Comput. Surv.},
  title        = {ISP meets deep learning: A survey on deep learning methods for image signal processing},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserved and responsible recommenders: From
conventional defense to federated learning and blockchain.
<em>CSUR</em>, <em>57</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3708982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RS) play an integral role in many online platforms. Exponential growth and potential commercial interests are raising significant concerns around privacy, security, fairness, and overall responsibility. The existing literature around responsible recommendation services is diverse and multidisciplinary. Most literature reviews cover a specific aspect or a single technology for responsible behavior, such as federated learning or blockchain. This study integrates relevant concepts across disciplines to provide a broader representation of the landscape. We review the latest advancements toward building privacy-preserved and responsible recommendation services for the e-commerce industry. The survey summarizes recent, high-impact works on diverse aspects and technologies that ensure responsible behavior in RS through an interconnected taxonomy. We contextualize potential privacy threats, practical significance, industrial expectations, and research remedies. From the technical viewpoint, we analyze conventional privacy defenses and provide an overview of emerging technologies including differential privacy, federated learning, and blockchain. The methods and concepts across technologies are linked based on their objectives, challenges, and future directions. In addition, we also develop an open source repository that summarizes a wide range of evaluation benchmarks, codebases, and toolkits to aid the further research. The survey offers a holistic perspective on this rapidly evolving landscape by synthesizing insights from both RS and responsible AI literature.},
  archive      = {J_CSUR},
  doi          = {10.1145/3708982},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Comput. Surv.},
  title        = {Privacy-preserved and responsible recommenders: From conventional defense to federated learning and blockchain},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An in-depth analysis of password managers and two-factor
authentication tools. <em>CSUR</em>, <em>57</em>(5), 1–32. (<a
href="https://doi.org/10.1145/3711117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passwords remain the primary authentication method in online services, a domain increasingly crucial in our digital age. However, passwords suffer from several well-documented security and usability issues. Addressing these concerns, password managers and two-factor authentication (2FA) have emerged as key solutions. This article examines these methods with a focus on enhancing password security without compromising usability. We utilize an adapted Bonneau et al. (IEEE S&amp;P 2012) framework tailored to the specific challenges of password managers and 2FA. This allows us to categorize and evaluate prominent solutions from both academic research and industry practice, with a focus on their security, privacy, and usability. A crucial aspect of our study involves evaluating the effectiveness of a combined PM+2FA system in balancing security and usability. This study not only examines current trends but also suggests potential areas for future research, offering valuable insights to both users and developers in the evolving landscape of digital security.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711117},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-32},
  shortjournal = {ACM Comput. Surv.},
  title        = {An in-depth analysis of password managers and two-factor authentication tools},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-centric artificial intelligence: A survey.
<em>CSUR</em>, <em>57</em>(5), 1–42. (<a
href="https://doi.org/10.1145/3711118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI . The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI .},
  archive      = {J_CSUR},
  doi          = {10.1145/3711118},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-42},
  shortjournal = {ACM Comput. Surv.},
  title        = {Data-centric artificial intelligence: A survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of heuristics for profile and wavefront reductions.
<em>CSUR</em>, <em>57</em>(5), 1–16. (<a
href="https://doi.org/10.1145/3711120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article surveys heuristic methods for profile and wavefront reductions. These graph layout problems represent a challenge for optimization methods and heuristics especially. The article presents the graph layout problems with their formal definition. It provides an ample perspective of techniques for designing heuristic methods for these graph layout problems but concentrates on the approaches and methodologies that yield high-quality solutions. Thus, this work references the most relevant studies in the associated literature and discusses the current state-of-the-art heuristics for these graph layout problems.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711120},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-16},
  shortjournal = {ACM Comput. Surv.},
  title        = {A survey of heuristics for profile and wavefront reductions},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can graph neural networks be adequately explained? A survey.
<em>CSUR</em>, <em>57</em>(5), 1–36. (<a
href="https://doi.org/10.1145/3711122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the barrier caused by the black-box nature of Deep Learning (DL) for practical deployment, eXplainable Artificial Intelligence (XAI) has emerged and is developing rapidly. While significant progress has been made in explanation techniques for DL models targeted to images and texts, research on explaining DL models for graph data is still in its infancy. As Graph Neural Networks (GNNs) have shown superiority over various network analysis tasks, their explainability has also gained attention from both academia and industry. However, despite the increasing number of GNN explanation methods, there is currently neither a fine-grained taxonomy of them, nor a holistic set of evaluation criteria for quantitative and qualitative evaluation. To fill this gap, we conduct a comprehensive survey on existing explanation methods of GNNs in this article. Specifically, we propose a novel four-dimensional taxonomy of GNN explanation methods and summarize evaluation criteria in terms of correctness, robustness, usability, understandability, and computational complexity. Based on the taxonomy and criteria, we thoroughly review the recent advances in GNN explanation methods and analyze their pros and cons. In the end, we identify a series of open issues and put forward future research directions to facilitate XAI research in the field of GNNs.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711122},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-36},
  shortjournal = {ACM Comput. Surv.},
  title        = {Can graph neural networks be adequately explained? a survey},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulating information and network security: Review and
challenges. <em>CSUR</em>, <em>57</em>(5), 1–38. (<a
href="https://doi.org/10.1145/3711124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of internet activities in daily life has elevated cyberattacks to a significant global threat. As a result, protecting the networks and systems of industries, organizations, and individuals against cybercrimes has become an increasingly critical challenge. This monograph provides a comprehensive review and analysis of national, international, and industry regulations on cybercrimes. It presents empirical evidence of the effectiveness of these regulatory measures and their impacts at the national, organizational, and individual levels. We also examine the challenges posed by emerging technologies to these regulations. Finally, the monograph identifies limitations in the current regulatory framework and proposes future directions to enhance the cybersecurity ecosystem.},
  archive      = {J_CSUR},
  doi          = {10.1145/3711124},
  journal      = {ACM Computing Surveys},
  month        = {1},
  number       = {5},
  pages        = {1-38},
  shortjournal = {ACM Comput. Surv.},
  title        = {Regulating information and network security: Review and challenges},
  volume       = {57},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="jacm---10">JACM - 10</h2>
<ul>
<li><details>
<summary>
(2025). Integer programs with bounded subdeterminants and two
nonzeros per row. <em>JACM</em>, <em>72</em>(1), 1–50. (<a
href="https://doi.org/10.1145/3695985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a strongly polynomial-time algorithm for integer linear programs defined by integer coefficient matrices whose subdeterminants are bounded by a constant and that contain at most two nonzero entries in each row. The core of our approach is the first polynomial-time algorithm for the weighted stable set problem on graphs that do not contain more than k vertex-disjoint odd cycles, where k is any constant. Previously, polynomial-time algorithms were only known for k =0 (bipartite graphs) and for k =1. We observe that integer linear programs defined by coefficient matrices with bounded subdeterminants and two nonzeros per column can be also solved in strongly polynomial-time, using a reduction to b -matching.},
  archive      = {J_JACM},
  doi          = {10.1145/3695985},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-50},
  shortjournal = {J. ACM},
  title        = {Integer programs with bounded subdeterminants and two nonzeros per row},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subsampling suffices for adaptive data analysis.
<em>JACM</em>, <em>72</em>(1), 1–45. (<a
href="https://doi.org/10.1145/3698104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst’s query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of adaptive data analysis was formalized in the seminal works of Dwork et al. (STOC 2015) and Hardt and Ullman (FOCS 2014). We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: the only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work. In addition to its simplicity, we demonstrate the utility of this framework by designing mechanisms for two foundational tasks: statistical queries and median finding. In particular, our mechanism for answering the broadly applicable class of statistical queries is both extremely simple and state of the art in many parameter regimes.},
  archive      = {J_JACM},
  doi          = {10.1145/3698104},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-45},
  shortjournal = {J. ACM},
  title        = {Subsampling suffices for adaptive data analysis},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orbit-finite linear programming. <em>JACM</em>,
<em>72</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3703909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An infinite set is orbit-finite if, up to permutations of atoms, it has only finitely many elements. We study a generalisation of linear programming where constraints are expressed by an orbit-finite system of linear inequalities. As our principal contribution we provide a decision procedure for checking if such a system has a real solution, and for computing the minimal/maximal value of a linear objective function over the solution set. We also show undecidability of these problems in case when only integer solutions are considered. Therefore orbit-finite linear programming is decidable, while orbit-finite integer linear programming is not.},
  archive      = {J_JACM},
  doi          = {10.1145/3703909},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {J. ACM},
  title        = {Orbit-finite linear programming},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of approximate diameter: Now for undirected graphs.
<em>JACM</em>, <em>72</em>(1), 1–32. (<a
href="https://doi.org/10.1145/3704631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating the graph diameter is a basic task of both theoretical and practical interest. A simple folklore algorithm can output a 2-approximation to the diameter in linear time by running BFS from an arbitrary vertex. It has been open whether a better approximation is possible in near-linear time. A series of articles on fine-grained complexity have led to strong hardness results for diameter in directed graphs, culminating in a recent tradeoff curve independently discovered by [Li, STOC’21] and [Dalirrooyfard and Wein, STOC’21], showing that under the Strong Exponential Time Hypothesis (SETH), for any integer k ≥ 2 and δ &gt; 0, a \(2-\frac{1}{k}-\delta\) approximation for diameter in directed m -edge graphs requires \(m^{1+1/(k-1)-o(1)}\) time. In particular, the simple linear time 2-approximation algorithm is optimal for directed graphs. In this article, we prove that the same tradeoff lower bound curve is possible for undirected graphs as well, extending results of [Roditty and Vassilevska W., STOC’13], [Li’20] and [Bonnet, ICALP’21] who proved the first few cases of the curve, k =2,3, and 4, respectively. Our result shows in particular that the simple linear time 2-approximation algorithm is conditionally optimal for undirected graphs. To obtain our result, we extract the core ideas in known reductions and introduce a unification and generalization that could be useful for proving SETH-based hardness for other problems in undirected graphs related to distance computation.},
  archive      = {J_JACM},
  doi          = {10.1145/3704631},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {J. ACM},
  title        = {Hardness of approximate diameter: Now for undirected graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative coding and complexity theory of continuous
data: Part i: Motivation, definition, consequences. <em>JACM</em>,
<em>72</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3705609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When encoding real numbers as (necessarily infinite) bit-strings, the naïve binary/decimal expansion is well-known [ doi:10.1112/plms/s2-43.6.544 ] computably “ un reasonable”, rendering, for example, tripling qualitatively discontinuous on Cantor’s sequence space. Encoding reals as sequences of (finite integer numerators and denominators, in binary, of) rational approximations does make common operations qualitatively computable, yet admits no bounds on their computational complexity/quantitative continuity. Dyadic approximations, on the other hand, are known polynomially, and signed binary expansions even linearly, “reasonable” in a rigorous sense recalled in the introduction of this work. But how to distinguish between un/suitable encodings of spaces common in Calculus beyond the reals, such as Banach or Sobolev? With respect to qualitative computability/continuity on topological spaces, the technical condition of admissibility had been identified [ doi:10.1016/0304-3975(85)90208-7 ] for an encoding over Cantor space (historically called a representation ) to be “reasonable” [ doi:10.1007/978-3-030-59234-9_9 ] . Roughly speaking, admissibility requires the representation to be (i) continuous, and to be (ii) maximal with respect to continuous reduction. Admissible representations exist for a large class of spaces. And for (precisely) these does the Kreitz–Weihrauch—sometimes aka Main —Theorem of Computable Analysis hold, which characterizes continuity of functions by continuity of mappings translating codes, so-called realizers . We refine qualitative computability/continuity on topological spaces to quantitative continuity/complexity on metric spaces by proposing a notion, and investigating the properties, of polynomially/linearly admissible representations. Roughly speaking, these are (i) close to “optimally” continuous, namely linearly/polynomially relative to the space’s entropy, and they are (ii) maximal with respect to relative linear/polynomial quantitatively continuous reductions defined in the main text. Quantitatively admissible representations are closed under composition over generalized ground spaces beyond Cantor’s. Such representations exhibit a quantitative strengthening of the qualitative Main Theorem , namely now characterizing quantitative continuity of functions by quantitative continuity of realizers. A large class of compact metric spaces is shown to admit polynomially admissible representations over compact ultra metric spaces, and some even a generalization of the linearly admissible signed binary encoding. Quantitative admissibility thus provides the desired criterion for complexity-theoretically “reasonable” encodings.},
  archive      = {J_JACM},
  doi          = {10.1145/3705609},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-39},
  shortjournal = {J. ACM},
  title        = {Quantitative coding and complexity theory of continuous data: part i: motivation, definition, consequences},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correct and complete type checking and certified erasure for
coq, in coq. <em>JACM</em>, <em>72</em>(1), 1–74. (<a
href="https://doi.org/10.1145/3706056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coq is built around a well-delimited kernel that performs type checking for definitions in a variant of the Calculus of Inductive Constructions ( CIC ). Although the metatheory of CIC is very stable and reliable, the correctness of its implementation in Coq is less clear. Indeed, implementing an efficient type checker for CIC is a rather complex task, and many parts of the code rely on implicit invariants which can easily be broken by further evolution of the code. Therefore, on average, one critical bug has been found every year in Coq . This article presents the first implementation of a type checker for the kernel of Coq (without the module system, template polymorphism and η-conversion), which is proven sound and complete in Coq with respect to its formal specification. Note that because of Gödel’s second incompleteness theorem, there is no hope to prove completely the soundness of the specification of Coq inside Coq (in particular strong normalization), but it is possible to prove the correctness and completeness of the implementation assuming soundness of the specification, thus moving from a trusted code base (TCB) to a trusted theory base (TTB) paradigm. Our work is based on the MetaCoq project which provides meta-programming facilities to work with terms and declarations at the level of the kernel. We verify a relatively efficient type checker based on the specification of the typing relation of the Polymorphic, Cumulative Calculus of Inductive Constructions ( PCUIC ) at the basis of Coq . It is worth mentioning that during the verification process, we have found a source of incompleteness in Coq ’s official type checker, which has then been fixed in Coq 8.14 thanks to our work. In addition to the kernel implementation, another essential feature of Coq is the so-called extraction mechanism: the production of executable code in functional languages from Coq definitions. We present a verified version of this subtle type and proof erasure step, therefore enabling the verified extraction of a safe type checker for Coq in the future.},
  archive      = {J_JACM},
  doi          = {10.1145/3706056},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-74},
  shortjournal = {J. ACM},
  title        = {Correct and complete type checking and certified erasure for coq, in coq},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow-augmentation i: Directed graphs. <em>JACM</em>,
<em>72</em>(1), 1–38. (<a
href="https://doi.org/10.1145/3706103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show a flow-augmentation algorithm in directed graphs: There exists a randomized polynomial-time algorithm that, given a directed graph G , two vertices s, t ∈ V(G) , and an integer k , adds (randomly) to G a number of arcs such that for every minimal st -cut Z in G of size at most k , with probability 2 −poly( k ) the set Z becomes a minimum st -cut in the resulting graph. We also provide a deterministic counterpart of this procedure. The directed flow-augmentation tool allows us to prove fixed-parameter tractability of a number of problems parameterized by the cardinality of the deletion set whose parameterized complexity status was repeatedly posed as open problems: Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set . By proving that Chain SAT is FPT, we confirm a conjecture of Chitnis, Egri, and Marx that, for any graph H , if the List H - Coloring problem is polynomial-time solvable, then the corresponding vertex-deletion problem is fixed-parameter tractable. Chain SAT , defined by Chitnis, Egri, and Marx [ESA’13, Algorithmica’17], a number of weighted variants of classic directed cut problems, such as Weighted st - Cut or Weighted Directed Feedback Vertex Set .},
  archive      = {J_JACM},
  doi          = {10.1145/3706103},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {J. ACM},
  title        = {Flow-augmentation i: Directed graphs},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric embeddability of complexes is ∃ℝ-complete.
<em>JACM</em>, <em>72</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3707201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the decision problem of determining whether a given (abstract simplicial) k -complex has a geometric embedding in ℝ d is complete for the Existential Theory of the Reals for all d ≥ 3 and k ∈ { d -1, d } by reducing from pseudoline stretchability. Consequently, the problem is polynomial time equivalent to determining whether a polynomial equation system has a real solution. Moreover, this implies NP-hardness and constitutes the first hardness result for the algorithmic problem of geometrically embedding (abstract simplicial) complexes. This complements recent breakthroughs for the computational complexity of piece-wise linear embeddability [Matoušek, Sedgwick, Tancer, and Wagner, J. ACM 2018, and de Mesmay, Rieck, Sedgwick and Tancer, J. ACM 2020] and establishes connections to computational topology.},
  archive      = {J_JACM},
  doi          = {10.1145/3707201},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {J. ACM},
  title        = {Geometric embeddability of complexes is ∃ℝ-complete},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient quantum factoring algorithm. <em>JACM</em>,
<em>72</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3708471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that n -bit integers can be factorized by independently running a quantum circuit with \(\tilde{O}(n^{3/2})\) gates for \(\sqrt {n}+4\) times, and then using polynomial-time classical post-processing. The correctness of the algorithm relies on a certain number-theoretic conjecture. It is currently not clear if the algorithm can lead to improved physical implementations in practice.},
  archive      = {J_JACM},
  doi          = {10.1145/3708471},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. ACM},
  title        = {An efficient quantum factoring algorithm},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelize single-site dynamics up to dobrushin criterion.
<em>JACM</em>, <em>72</em>(1), 1–33. (<a
href="https://doi.org/10.1145/3708558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-site dynamics are canonical Markov chain based algorithms for sampling from high-dimensional distributions, such as the Gibbs distributions of graphical models. We introduce a simple and generic parallel algorithm that faithfully simulates single-site dynamics. Under a much relaxed, asymptotic variant of the ℓ p -Dobrushin’s condition—where the Dobrushin’s influence matrix has a bounded ℓ p -induced operator norm for an arbitrary p ∈ [1, ∞]—our algorithm simulates N steps of single-site updates within a parallel depth of O ( N / n +log n ) on Õ( m ) processors, where n is the number of sites and m is the size of the graphical model. For Boolean-valued random variables, if the ℓ p -Dobrushin’s condition holds—specifically, if the ℓ p -induced operator norm of the Dobrushin’s influence matrix is less than 1—the parallel depth can be further reduced to O (log N + log n ), achieving an exponential speedup. These results suggest that single-site dynamics with near-linear mixing times can be parallelized into RNC sampling algorithms, independent of the maximum degree of the underlying graphical model, as long as the Dobrushin influence matrix maintains a bounded operator norm. We show the effectiveness of this approach with RNC samplers for the hardcore and Ising models within their uniqueness regimes, as well as an RNC SAT sampler for satisfying solutions of conjunctive normal form formulas in a local lemma regime. Furthermore, by employing non-adaptive simulated annealing, these RNC samplers can be transformed into RNC algorithms for approximate counting.},
  archive      = {J_JACM},
  doi          = {10.1145/3708558},
  journal      = {Journal of the ACM},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. ACM},
  title        = {Parallelize single-site dynamics up to dobrushin criterion},
  volume       = {72},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tap---1">TAP - 1</h2>
<ul>
<li><details>
<summary>
(2025). Toward understanding the effects of intelligence of a
virtual character during an immersive jigsaw puzzle co-solving task.
<em>TAP</em>, <em>22</em>(2), 1–28. (<a
href="https://doi.org/10.1145/3700822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality, creating intelligent virtual characters has been a long-lasting endeavor. However, while researchers have investigated several aspects of a virtual character’s intelligence, little attention has been paid to the impact of the implemented intelligence levels assigned to a virtual character during human–virtual character collaboration. Thus, we conducted a within-group study ( \(N=24\) ) to explore how three different intelligence levels (low vs. medium vs. high) assigned to a virtual character can impact how study participants perceive that virtual character and interact with the task they are instructed to complete. Specifically, for our study, we developed a jigsaw puzzle game and instructed our participants to solve it with the help of a virtual character. During the jigsaw puzzle solving process, we collected application logs related to how the participants executed the task and observed the virtual environment. Moreover, after each condition, we asked the participants to respond using a questionnaire that examined their social presence, how they perceived the character’s intelligence and compared it with their own, and how they rated the virtual character’s realism. Our results indicated that the different intelligence levels assigned to the virtual characters impacted participants’ responses on several variables, including co-presence, perceived intelligence, intelligence comparison, and character interaction and behavior realism. Moreover, based on the collected logged data, we found that the intelligence levels assigned to our virtual character significantly impacted the performance of our participants. Our results could be valuable to the research community for creating more engaging experiences with intelligent virtual characters for collaborative tasks in immersive environments.},
  archive      = {J_TAP},
  doi          = {10.1145/3700822},
  journal      = {ACM Transactions on Applied Perception},
  month        = {1},
  number       = {2},
  pages        = {1-28},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Toward understanding the effects of intelligence of a virtual character during an immersive jigsaw puzzle co-solving task},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tist---8">TIST - 8</h2>
<ul>
<li><details>
<summary>
(2025). Explaining neural news recommendation with attributions onto
reading histories. <em>TIST</em>, <em>16</em>(1), 1–25. (<a
href="https://doi.org/10.1145/3673233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important aspect of responsible recommendation systems is the transparency of the prediction mechanisms. This is a general challenge for deep-learning-based systems such as the currently predominant neural news recommender architectures, which are optimized to predict clicks by matching candidate news items against users’ reading histories. Such systems achieve state-of-the-art click-prediction performance, but the rationale for their decisions is difficult to assess. At the same time, the economic and societal impact of these systems makes such insights very much desirable. In this article, we ask the question to what extent the recommendations of current news recommender systems are actually based on content-related evidence from reading histories. We approach this question from an explainability perspective. Building on the concept of integrated gradients, we present a neural news recommender that can accurately attribute individual recommendations to news items and words in input reading histories while maintaining a top scoring click-prediction performance. Using our method as a diagnostic tool, we find that: (a), a substantial number of users’ clicks on news are not explainable from reading histories, and many history-explainable items are actually skipped; (b), while many recommendations are based on content-related evidence in histories, for others the model does not attend to reasonable evidence, and recommendations stem from a spurious bias in user representations. Our code is publicly available at https://github.com/lucasmllr/xnrs .},
  archive      = {J_TIST},
  doi          = {10.1145/3673233},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Explaining neural news recommendation with attributions onto reading histories},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aspect-enhanced explainable recommendation with multi-modal
contrastive learning. <em>TIST</em>, <em>16</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3673234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable recommender systems ( ERS ) aim to enhance users’ trust in the systems by offering personalized recommendations with transparent explanations. This transparency provides users with a clear understanding of the rationale behind the recommendations, fostering a sense of confidence and reliability in the system’s outputs. Generally, the explanations are presented in a familiar and intuitive way, which is in the form of natural language, thus enhancing their accessibility to users. Recently, there has been an increasing focus on leveraging reviews as a valuable source of rich information in both modeling user-item preferences and generating textual interpretations, which can be performed simultaneously in a multi-task framework. Despite the progress made in these review-based recommendation systems, the integration of implicit feedback derived from user-item interactions and user-written text reviews has yet to be fully explored. To fill this gap, we propose a model named SERMON (A s pect-enhanced E xplainable R ecommendation with M ulti-modal C o ntrast Lear n ing). Our model explores the application of multimodal contrastive learning to facilitate reciprocal learning across two modalities, thereby enhancing the modeling of user preferences. Moreover, our model incorporates the aspect information extracted from the review, which provides two significant enhancements to our tasks. Firstly, the quality of the generated explanations is improved by incorporating the aspect characteristics into the explanations generated by a pre-trained model with controlled textual generation ability. Secondly, the commonly used user-item interactions are transformed into user-item-aspect interactions, which we refer to as interaction triple, resulting in a more nuanced representation of user preference. To validate the effectiveness of our model, we conduct extensive experiments on three real-world datasets. The experimental results show that our model outperforms state-of-the-art baselines, with a 2.0% improvement in prediction accuracy and a substantial 24.5% enhancement in explanation quality for the TripAdvisor dataset.},
  archive      = {J_TIST},
  doi          = {10.1145/3673234},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Aspect-enhanced explainable recommendation with multi-modal contrastive learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness and bias in algorithmic hiring: A multidisciplinary
survey. <em>TIST</em>, <em>16</em>(1), 1–54. (<a
href="https://doi.org/10.1145/3696457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of , algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.},
  archive      = {J_TIST},
  doi          = {10.1145/3696457},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-54},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Fairness and bias in algorithmic hiring: A multidisciplinary survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extracting political interest model from interaction data
based on novel word-level bias assignment. <em>TIST</em>,
<em>16</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3702649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In democratic countries, political interest is deeply involved in people’s daily lives. Research in political consumerism shows that product purchase decision is also influenced by the political orientation of the consumer. In traditional recommendation system design, user interest in an item is provided by a unified model. Recently, interest disentanglement methods have been introduced. It is shown that by disentangling interest factors such as conformity and private interest, recommendation performance can be significantly improved. However, few studies attempt to disentangle political interest in purchase behavior, which is bipolar. In this article, we propose a method to extract political interest model from e-commerce interaction data, which is supported by a novel word-level political bias assignment. For the bias assignment part, we improved a political bias distilling method. For the political interest model extraction part, we extend a one-side bias method to make it support bipolar bias. We compare our method with state-of-the-art baseline methods in several evaluation settings, and the experimental results show that our method can achieve superior performance. Further investigation shows that our method is consistent with theories of political consumerism.},
  archive      = {J_TIST},
  doi          = {10.1145/3702649},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Extracting political interest model from interaction data based on novel word-level bias assignment},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge manipulations for the maximum vertex-weighted bipartite
b-matching. <em>TIST</em>, <em>16</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3702650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we explore the Mechanism Design aspects of the Maximum Vertex-Weighted \(b\) -matching (MVbM) problem on bipartite graphs \((A\cup T,E)\) . The set \(A\) comprises agents, while \(T\) represents tasks. The set \(E\) , which connects \(A\) and \(T\) , is the private information of either agents or tasks. In this framework, we investigate three mechanisms— \(\mathbb{M}_{BFS}\) , \(\mathbb{M}_{DFS}\) , and \(\mathbb{M}_{G}\) . We examine scenarios in which either agents or tasks are strategic and report their adjacent edges to one of the three mechanisms. In both cases, we assume that the strategic entities are bounded by their statements: They can hide edges, but they cannot report edges that do not exist. First, we consider the case in which agents can manipulate. In this framework, \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) are optimal but not truthful. By characterizing the Nash Equilibria induced by \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) , we reveal that both mechanisms have a Price of Anarchy ( \(PoA\) ) and Price of Stability ( \(PoS\) ) of \(2\) . These efficiency guarantees are tight; no deterministic mechanism can achieve a lower \(PoA\) or \(PoS\) . In contrast, the third mechanism, \(\mathbb{M}_{G}\) , is not optimal, but truthful and its approximation ratio is \(2\) . We demonstrate that this ratio is optimal; no deterministic and truthful mechanism can outperform it. We then shift our focus to scenarios where tasks can exhibit strategic behavior. In this case, \(\mathbb{M}_{BFS}\) , \(\mathbb{M}_{DFS}\) , and \(\mathbb{M}_{G}\) all maintain truthfulness, making \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) truthful and optimal mechanisms. In conclusion, we investigate the manipulability of \(\mathbb{M}_{BFS}\) and \(\mathbb{M}_{DFS}\) through experiments on randomly generated graphs. We observe that (i) \(\mathbb{M}_{BFS}\) is less prone to be manipulated by the first agent than \(\mathbb{M}_{DFS}\) , and (ii) \(\mathbb{M}_{BFS}\) is more manipulable on instances in which the total capacity of the agents is equal to the number of tasks. 1},
  archive      = {J_TIST},
  doi          = {10.1145/3702650},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Edge manipulations for the maximum vertex-weighted bipartite b-matching},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and accurate evacuation planning algorithm with
bayesian optimization. <em>TIST</em>, <em>16</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3704920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a method for generating an evacuation plan at a high speed to realize safe and swift evacuation in the event of a large-scale disaster such as an earthquake and its accompanying tsunami. Existing conventional methods have several problems. Simulation-based methods that use agents and methods that use existing time expansion networks have high computational costs, which makes it difficult for evacuation routes to be immediately changed according to the effects of disasters such as collapsed buildings and roads. Although heuristics with reduced calculation costs are also being researched, they may result in very long evacuation completion times and cannot generate optimal evacuation plans. We guarantee the optimal solution by reducing the number of maximum flow problem calculations, which constitute the bottleneck for methods using the existing time expansion network, through the use of the Bayesian optimization machine learning method. This reduces the calculation cost of the entire algorithm. The performance of our method is evaluated from the two viewpoints of the evacuation completion time, which indicates the quality of the evacuation plan, and the time required for the generation by the solution of the algorithm in computer experiments under multiple scenarios. In addition, the impact of the number of evacuees and the locations of the sinks are analyzed. We show that our method can quickly generate an optimal evacuation plan.},
  archive      = {J_TIST},
  doi          = {10.1145/3704920},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Fast and accurate evacuation planning algorithm with bayesian optimization},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph neural networks using self-supervised
reciprocally contrastive learning. <em>TIST</em>, <em>16</em>(1), 1–21.
(<a href="https://doi.org/10.1145/3706115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph neural network (HGNN) is a popular technique for modeling and analyzing heterogeneous graphs. Most existing HGNN-based approaches are supervised or semi-supervised learning methods requiring graphs to be annotated, which is costly and time-consuming. Self-supervised contrastive learning has been proposed to address the problem of requiring annotated data by mining intrinsic properties in the given data. However, the existing contrastive learning methods are not suitable for heterogeneous graphs because they construct contrastive views only based on data perturbation or pre-defined structural properties (e.g., meta-path) in graph data while ignoring noises in node attributes and graph topologies. We develop a robust heterogeneous graph contrastive learning approach, namely HGCL, which introduces two views on respective guidances of node attributes and graph topologies and integrates and enhances them by a reciprocally contrastive mechanism to better model heterogeneous graphs. In this new approach, we adopt distinct but suitable attribute and topology fusion mechanisms in the two views, which are conducive to mining relevant information in attributes and topologies separately. We further use both attribute similarity and topological correlation to construct high-quality contrastive samples. Extensive experiments on four large real-world heterogeneous graphs demonstrate the superiority and robustness of HGCL over several state-of-the-art methods.},
  archive      = {J_TIST},
  doi          = {10.1145/3706115},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Heterogeneous graph neural networks using self-supervised reciprocally contrastive learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tucker decomposition-enhanced dynamic graph convolutional
networks for crowd flows prediction. <em>TIST</em>, <em>16</em>(1),
1–19. (<a href="https://doi.org/10.1145/3706116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd flows prediction is an important problem for traffic management and public safety. Graph Convolutional Network (GCN), known for its ability to effectively capture and utilize topological information, has demonstrated significant advancements in addressing this problem. However, GCN-based models were often based on predefined crowd-flow graphs via historical movement behaviors of human beings and traffic vehicles, which ignored the abnormal changes in crowd flows. In this study, we propose a multi-scale fusion GCN-based framework with Tucker decomposition named mTDNet to enhance dynamic GCN for crowd flows prediction. Following the paradigm of extant methods, we also employ the predefined crowd-flow graphs as a part of mTDNet to effectively capture the historical movement behaviors of crowd flows. To capture the abnormal changes, we propose a Tucker decomposition-based network with the product of the adjacency matrix of historical movement pattern graphs and an Adaptive Learning Tensor ( ALT ) by reconstructing the crowd flows. Particularly, we utilize the Tucker decomposition scheme to decompose ALT , which enhances the dynamic learning of graph structures, allowing for effective capturing of the dynamic changes in crowd flow, including abnormal changes. Furthermore, a multi-scale 3DGCN is utilized to mine and fuse the multi-scale spatio-temporal information from crowd flows, to further boost the mTDNet prediction performance. Experiments conducted on two real-world datasets showed that the proposed mTDNet surpasses other crowd flow prediction methods.},
  archive      = {J_TIST},
  doi          = {10.1145/3706116},
  journal      = {ACM Transactions on Intelligent Systems and Technology},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  title        = {Tucker decomposition-enhanced dynamic graph convolutional networks for crowd flows prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tkdd---4">TKDD - 4</h2>
<ul>
<li><details>
<summary>
(2025). Modeling on-road trajectories with multi-task learning.
<em>TKDD</em>, <em>19</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3705005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of GPS modules, there are various urban applications such as car navigation relying on trajectory data modeling. In this work, we study the problem of modeling on-road trajectories, which is to predict the next road segment given a partial GPS trajectory. Existing methods that model trajectories with Markov chain or recurrent neural network suffer from various issues, including limited capability of sequential modeling, insufficiency of incorporating the road network context, and lack of capturing the underlying semantics of trajectories. In this article, we propose a new trajectory modeling framework called Multi-task Modeling for Trajectories (MMTraj+), which avoids these issues. Specifically, MMTraj+ uses multi-head self-attention networks for sequential modeling, captures the overall road network as the context information for road segment embedding, and performs an auxiliary task of predicting the trajectory destination information (namely the ID and bearing angle) to better guide the main trajectory modeling task (controlled by a carefully designed gating mechanism). In addition, we tailor MMTraj+ for the cases where the destination information is known by dropping its auxiliary task of predicting the trajectory destination information. Extensive experiments conducted on real-world datasets demonstrate the superiority of the proposed method over the baseline methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3705005},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling on-road trajectories with multi-task learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure amplification on multi-layer stochastic block
models. <em>TKDD</em>, <em>19</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3706111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of the complexity of social, biological, and engineering systems arises from the complicated interactions among the entities in the corresponding networks. A number of network analysis tools have been successfully used to discover latent structures termed communities in such networks. However, some communities with relatively weak structures can be difficult to uncover because they are obscured by other stronger connections. To cope with this situation, our previous work proposes an algorithm called HICODE to detect and amplify the dominant and hidden community structures. In this work, we conduct a comprehensive and systematic theoretical analysis on the impact of hidden community structure and the efficacy of the HICODE algorithm, as well as provide illustrations of the detection process and results. Specifically, we define a multi-layer stochastic block model and use this model to explain why the existence of hidden structure makes the detection of dominant structure harder than equivalent random noises, which can also explain why many community detection algorithms only focusing on the dominant structure do not work well as expected. We then provide theoretical analysis that the iterative reducing methods could help to enhance the discovery of hidden structure as well as the dominant structure in the multi-layer stochastic block model for the two cases of accurate and inaccurate detection. Finally, visual simulations and experimental results are presented to show the process of HICODE algorithm and the impact of different number of layers on the detection quality.},
  archive      = {J_TKDD},
  doi          = {10.1145/3706111},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Structure amplification on multi-layer stochastic block models},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending federated recommender systems against untargeted
attacks: A contribution-aware robust aggregation scheme. <em>TKDD</em>,
<em>19</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3706112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommender systems (FedRSs) effectively tackle the tradeoff between recommendation accuracy and privacy preservation. However, recent studies have revealed severe vulnerabilities in FedRSs, particularly against untargeted attacks seeking to undermine their overall performance. Defense methods employed in traditional recommender systems are not applicable to FedRSs, and existing robust aggregation schemes for other federated learning-based applications have proven ineffective in FedRSs. Building on the observation that malicious clients contribute negatively to the training process, we design a novel contribution-aware robust aggregation scheme to defend FedRSs against untargeted attacks, named contribution-aware Bayesian knowledge distillation aggregation (ConDA), comprising two key components for the defense. In the first contribution estimation component, we decentralize the estimation from the server side to the client side and propose an ensemble-based Shapley value to enable the efficient calculation of contributions, addressing the limitations of lacking auxiliary validation data and high computational complexity. In the second contribution-aware aggregation component, we merge the decentralized contributions via a majority voting mechanism and integrate the merged contributions into a Bayesian knowledge distillation aggregation scheme for robust aggregation, mitigating the impact of unreliable contributions induced by attacks. We evaluate the effectiveness and efficiency of ConDA on two real-world datasets from movie and music service providers. Through extensive experiments, we demonstrate the superiority of ConDA over the baseline robust aggregation schemes.},
  archive      = {J_TKDD},
  doi          = {10.1145/3706112},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Defending federated recommender systems against untargeted attacks: A contribution-aware robust aggregation scheme},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Margin-aware noise-robust contrastive learning for partially
view-aligned problem. <em>TKDD</em>, <em>19</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3707646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a challenging problem in contrastive learning when just a portion of data is aligned in multi-view dataset due to temporal, spatial, or spatio-temporal asynchronism across views. It is important to study partially view-aligned data since this type of data is common in real-world application and easily leads to data inconsistency among different views. Such a Partially View-aligned Problem (PVP) in contrastive learning has been relatively less touched so far, especially in downstream tasks, i.e., classification and clustering. In order to solve this problem, we introduce a flexible margin and propose margin-aware noise-robust contrastive learning to simultaneously identify the within-category counterparts from the other view of one data point based on the established cross-view correspondence and learn a shared representation. To be specific, the proposed learning framework is built on a novel margin-aware noise-robust contrastive loss. Since data pairs are used as input for the proposed margin-aware noise-robust contrastive learning, we build positive pairs according to the known correspondences and negative pairs in the manner of random sampling. Our margin-aware noise-robust contrastive learning framework is able to effectively reduce or remove the impacts caused by the possible existing noise for the constructed pairs in a margin-aware manner, i.e., false negative pairs led by random sampling in PVP. We relax the proposed margin-aware noise-robust contrastive loss and then give a detailed mathematical analysis for the effectiveness of our loss. As an instantiation, we construct an example under the proposed margin-aware noise-robust contrastive learning framework for validation in this work. To the best of our knowledge, this is the first attempt of extending contrastive learning to a margin-aware noise-robust version for dealing with PVP. We also enrich the learning paradigm when there is noise in the data. Extensive experiments on different datasets demonstrate the promising performance of the proposed method in the classification and clustering tasks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3707646},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Margin-aware noise-robust contrastive learning for partially view-aligned problem},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="todaes---8">TODAES - 8</h2>
<ul>
<li><details>
<summary>
(2025). STRIVE: Empowering a low power tensor processing unit with
fault detection and error resilience. <em>TODAES</em>, <em>30</em>(2),
1–25. (<a href="https://doi.org/10.1145/3705003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid growth in Deep Neural Network (DNN) workloads has increased the energy footprint of the Artificial Intelligence (AI) computing realm. For optimum energy efficiency, we propose operating a DNN hardware in the Low-Power Computing (LPC) region. However, operating at LPC causes increased delay sensitivity to Process Variation (PV). Delay faults are an intriguing consequence of PV. In this article, we demonstrate the vulnerability of DNNs to delay variations, substantially lowering the prediction accuracy. To overcome delay faults, we present STRIVE—a post-fabrication fault detection and reactive error reduction technique. We also introduce a time-borrow correction technique to ensure error-free DNN computation.},
  archive      = {J_TODAES},
  doi          = {10.1145/3705003},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {STRIVE: Empowering a low power tensor processing unit with fault detection and error resilience},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global placement exploiting soft 2D regularity.
<em>TODAES</em>, <em>30</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3705729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell placement is a step of paramount importance in chip physical design and requests relentless effort for continuous improvement. Recently, designs with two-dimensional (2D) processing element arrays have become popular primarily due to their deep neural network hardware applications. The 2D array regularity is similar to but different from the regularity of conventional datapath designs. To exploit the 2D array regularity, this work develops a new global placement technique, Placement of Arrays with SOft Regularity (PASOR), built upon RePlAce, the state-of-the-art placement framework. Experimental results from various designs show that the proposed approach can reduce global routing wirelength by 11% and 6% compared to RePlAce and a previous work on datapath driven placement, respectively.},
  archive      = {J_TODAES},
  doi          = {10.1145/3705729},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Global placement exploiting soft 2D regularity},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIMTAM: Generation diversity test programs for FPGA
simulation tools testing via timing area mutation. <em>TODAES</em>,
<em>30</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3705730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-Programmable Gate Array (FPGA) timing simulation is essential in electronic circuit design, allowing for the verification of timing characteristics like delays and clock frequencies. However, bugs in timing simulation tools can lead to inaccurate results, potentially causing designers to miss critical issues in chip performance. Traditional testing methods often fall short in thoroughly assessing these tools, as current FPGA testing primarily focuses on synthesis and behavioral simulation, neglecting timing aspects. To address this issue, we propose SIMTAM for testing timing simulation tools. Specifically, SIMTAM consists of three components: equivalent delay region construction, diversity program segment generation, and differential testing. Given a seed circuit design file written by hardware description language such as Verilog, the delay region construction component randomly identifies delay structures for inertial delay in the design file to construct equivalent delay sleep regions. In the sleep region, the simulator skips the signal pulse whose width is less than the specified delay, thus ensuring the equivalence of the variations. The diversity program segment generation component combines Verilog expressions using generation operators and injects them into the sleep region to generate diverse design files. The differential testing component compares the seed and variant design files to find compilation inconsistency issues. In 5 months, SIMTAM reported 16 bugs to developers in two popular timing simulation tools, Iverilog and Vivado, 10 of which are confirmed.},
  archive      = {J_TODAES},
  doi          = {10.1145/3705730},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {SIMTAM: Generation diversity test programs for FPGA simulation tools testing via timing area mutation},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed learning based multiphysics simulation for
fast transient TSV electromigration analysis. <em>TODAES</em>,
<em>30</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3706106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through Silicon Vias (TSVs) are vulnerable to electromigration (EM) degradation due to their high local current densities, thereby reducing the reliability of 3D ICs with stack dies and TSVs. Due to the broad application of 3D ICs, it is necessary to analyze the electromigration reliability of TSVs. To overcome the weakness of traditional method for EM modeling of TSVs, we propose a physics-informed learning approach for transient analysis of electromigration modeling in TSV by solving the conventional mass balance equation. The proposed method allows simultaneous consideration of atomic depletion and accumulation, effective resistance degradation, electric current evolution, and stress distribution. In particular, we propose a customized neural network to simulate the EM process in TSV without the need for fine grid meshing and temporal iteration in traditional methods. Considering that the loss function of the proposed model is a combination of different loss terms, we propose a modified self-adaptive loss balanced method to automatically adjust the weights of multiple loss terms to enhance network performance. Given the prediction uncertainty due to data randomness or model architecture constraints, Gaussian probabilistic model is constructed to define the self-adaptive weights and update the dynamic weights per epoch built on maximum likelihood estimation. Compared with the finite element method, the proposed physics informed neural network method can lead to a speedup with less than 0.1% mean square error. Experimental results also show that the proposed model achieves excellent performance over other competing methods and high robustness under values of initial weights, different numbers of hidden layers and neurons per layer.},
  archive      = {J_TODAES},
  doi          = {10.1145/3706106},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Physics-informed learning based multiphysics simulation for fast transient TSV electromigration analysis},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PACE: A piece-wise approximate floating-point divider with
runtime configurability and high energy efficiency. <em>TODAES</em>,
<em>30</em>(2), 1–23. (<a
href="https://doi.org/10.1145/3706634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing emerges as a viable solution to enhance energy efficiency in applications sensitive to human perception, particularly on edge devices. This work introduces a novel piece-wise approximate floating-point divider that boasts resource efficiency and runtime configurability. Our method leverages a piece-wise approximation algorithm for computing 1/ y by exploiting powers of 2, complemented by an error compensation technique grounded in thorough mathematical analysis. This approach facilitates the realization of a reciprocal-based floating-point divider devoid of multipliers, which not only mitigates hardware resource consumption but also reduces latency. Additionally, we unveil a multi-level runtime configurable hardware architecture that significantly improves flexibility across diverse application contexts. Compared to the existing state-of-the-art approximate dividers and truncated exact dividers, our proposed solution achieves a superior compromise between precision and resource efficiency. Application-level evaluations reveal that our design provides over 87.7% energy saving while maintaining a negligible impact on output quality.},
  archive      = {J_TODAES},
  doi          = {10.1145/3706634},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {PACE: A piece-wise approximate floating-point divider with runtime configurability and high energy efficiency},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the effectiveness of STLs for GPUs via bounded
model checking. <em>TODAES</em>, <em>30</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3706635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphics Processing Units (GPUs) are becoming widespread, even in safety-critical applications. In that case, it is imperative to guarantee that the probability of producing critical failures due to hardware faults is lower than a given threshold. To detect possible permanent hardware faults as soon as they appear during the operational phase (e.g., due to aging), Software Test Libraries (STLs) have gained significant traction as a widely adopted test solution due to their effectiveness in terms of fault detection capabilities, test application time, and flexibility. However, a major drawback of this solution is the lack of automation in the STL generation phase. As a result, high manual labor is required for their generation. This becomes even more arduous in complex architectures that require in-depth knowledge to cover hard-to-test faults. In this article, we introduce a methodology based on Bounded Model Checking to support the generation and improvement of stuck-at-oriented STLs for hard-to-test units in GPUs, showing that we can enhance the test coverage achieved by pre-existing STLs while also identifying a set of functionally untestable faults. To experimentally validate the proposed method’s effectiveness, we use the FlexGripPlus GPU model to target two hard-to-test units, one medium to low complexity sub-unit and one high complexity sub-unit, as study cases. For both units, we had pre-existing STLs written for the stuck-at model. Resorting to the proposed method, the STLs’ test coverage was increased by 9.57% and 2.19%, respectively. In addition, the method also identified a significant number of functionally untestable faults.},
  archive      = {J_TODAES},
  doi          = {10.1145/3706635},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Enhancing the effectiveness of STLs for GPUs via bounded model checking},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISOAcc: In-situ shift operation-based accelerator for
efficient in-SRAM multiplication. <em>TODAES</em>, <em>30</em>(2), 1–24.
(<a href="https://doi.org/10.1145/3707205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital SRAM-based CIM architectures must balance three critical factors: quantized neural network bitwidth, accuracy loss, and computational efficiency, each crucial to optimizing performance and efficiency. In Domain Specific Accelerators (DSAs), flexible and specific hardware design, when incorporated with tailored Power-of-2 (P-2) quantization schemes, addresses this issue. However, in CIMs, the absence of flexible and specific hardware to support dynamic switching between general and tailored quantization schemes hinders the adoption of efficient quantization methods. In this article, we propose the I n-situ S hift O peration based Acc elerator ( ISOAcc ) for efficient SRAM-based multiplication. The key idea is to introduce transmission gates near the SRAM array to enable the selection of bits from either the same or the neighbor line when data flows from one row to another. This functionally equals a shift operation. By configuring the transmission gates array in a cascade manner, ISOAcc can support 0 to 15-bit shift with a negligible overhead. The ISOAcc can directly leverage P-2 quantization schemes in hardware, thereby greatly reducing multiplication cycles. We have chosen five well-known neural networks to evaluate ISOAcc. The evaluations show that ISOAcc achieves an average performance improvement of 3.24× and an energy reduction of 75%, compared with the state-of-the-art (SOTA) SRAM-based CIM design, Bit-Parallel.},
  archive      = {J_TODAES},
  doi          = {10.1145/3707205},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {ISOAcc: In-situ shift operation-based accelerator for efficient in-SRAM multiplication},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing machine learning in dynamic thermal management in
embedded CPU-GPU platforms. <em>TODAES</em>, <em>30</em>(2), 1–32. (<a
href="https://doi.org/10.1145/3708890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing transistor density, modern heterogeneous embedded processors often exhibit high temperature gradients due to complex application scheduling scenarios which may have missed design considerations. In many use cases, off-chip ”active” cooling solutions are considered prohibitive in such reduced form factors. Core frequency throttling by existing dynamic thermal management techniques often compromises the Quality-of-Service (QoS) and violates real-time deadlines. This necessitates the adoption of intelligent resource management that simultaneously manages both thermal and latency performance. Coupled with the complexity of modern heterogeneous multi-cores, the periodic application updates that cater to ever-changing user requirements often render model-driven thermal-aware resource allocation approaches unsuitable for heterogeneous multi-core systems. For such application-architecture scenarios, we propose a novel self-learning based resource manager using Reinforcement Learning that intelligently manipulates core frequencies and task set mappings to fulfill thermal and latency objectives. Our framework employs a data-driven system modeling technique using Gaussian Process Regression to enable efficient offline training of this learning-based resource manager to avoid challenges associated with initial online training. We evaluate the approach on a heterogeneous embedded CPU-GPU platform with real workloads and observe a significant reduction in peak operating temperature when compared to the default onboard frequency governor as well as other learning-based state-of-the-art approaches.},
  archive      = {J_TODAES},
  doi          = {10.1145/3708890},
  journal      = {ACM Transactions on Design Automation of Electronic Systems},
  month        = {1},
  number       = {2},
  pages        = {1-32},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  title        = {Harnessing machine learning in dynamic thermal management in embedded CPU-GPU platforms},
  volume       = {30},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="tosem---4">TOSEM - 4</h2>
<ul>
<li><details>
<summary>
(2025). SimClone: Detecting tabular data clones using value
similarity. <em>TOSEM</em>, <em>34</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3676961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clones are defined as multiple copies of the same data among datasets. The presence of data clones between datasets can cause issues such as difficulties in managing data assets and data license violations when using datasets with clones to build AI software. However, detecting data clones is not trivial. The majority of the prior studies in this area rely on structural information to detect data clones (e.g., font size, column header). However, tabular datasets used to build AI software are typically stored without any structural information. In this article, we propose a novel method called SimClone for data clone detection in tabular datasets without relying on structural information. SimClone method utilizes value similarities for data clone detection. We also propose a visualization approach as a part of our SimClone method to help locate the exact position of the cloned data between a dataset pair. Our results show that our SimClone outperforms the current state-of-the-art method by at least 20% in terms of both F1-score and AUC. In addition, SimClone’s visualization component helps identify the exact location of the data clone in a dataset with a Precision@10 value of 0.80 in the top 20 true positive predictions.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3676961},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {SimClone: Detecting tabular data clones using value similarity},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the understandability of design-level security practices
in infrastructure-as-code scripts and deployment architectures.
<em>TOSEM</em>, <em>34</em>(1), 1–37. (<a
href="https://doi.org/10.1145/3691630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrastructure as Code (IaC) automates IT infrastructure deployment, which is particularly beneficial for continuous releases, for instance, in the context of microservices and cloud systems. Despite its flexibility in application architecture, neglecting security can lead to vulnerabilities. The lack of comprehensive architectural security guidelines for IaC poses challenges in adhering to best practices. We studied how developers interpret IaC scripts (source code) in two IaC technologies, Ansible and Terraform, compared to semi-formal IaC deployment architecture models and metrics regarding design-level security understanding. In a controlled experiment involving ninety-four participants, we assessed the understandability of IaC-based deployment architectures through source code inspection compared to semi-formal representations in models and metrics. We hypothesized that providing semi-formal IaC deployment architecture models and metrics as supplementary material would significantly improve the comprehension of IaC security-related practices, as measured by task correctness . Our findings suggest that semi-formal IaC deployment architecture models and metrics as supplementary material enhance the understandability of IaC security-related practices without significantly increasing duration . We also observed a significant correlation between task correctness and duration when models and metrics were provided.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3691630},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-37},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {On the understandability of design-level security practices in infrastructure-as-code scripts and deployment architectures},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision support model for selecting the optimal blockchain
oracle platform: An evaluation of key factors. <em>TOSEM</em>,
<em>34</em>(1), 1–35. (<a
href="https://doi.org/10.1145/3697011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contract-based applications are executed in a blockchain environment, and they cannot directly access data from external systems, which is required for the service provision of these applications. Instead, smart contracts use agents known as blockchain oracles to collect and provide data feeds to the contracts. The functionality and compatibility with smart contract applications need to be considered when selecting the best-fit oracle platform. As the number of oracle alternatives and their features increases, the decision-making process becomes increasingly complex. Selecting the wrong or sub-optimal oracle is costly and may lead to severe security risks. This article provides a decision support model for the oracle selection problem. The model supports smart contract decision-makers in selecting a secure, cost-effective, and feasible oracle platform for their applications. We interviewed oracle co-founders and smart contracts experts to refine and validate the decision model. Two real-world smart contract application case studies were used to evaluate the model. Our model prioritises and suggests more than one possible oracle platform based on the developer’s required criteria, security assessment and cost analysis. Moreover, this guided decision model serves to reveal issues that may go unnoticed if done haphazardly, reduce decision-making efforts and provide a cost-effective solution.},
  archive      = {J_TOSEM},
  doi          = {10.1145/3697011},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {Decision support model for selecting the optimal blockchain oracle platform: An evaluation of key factors},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiPri: Distance-based seed prioritization for greybox
fuzzing—RCR report. <em>TOSEM</em>, <em>34</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3701298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This replicated computational results (RCR) report describes how to (1) set up DiPri and (2) replicate the experimental results. The primary artifact is the C/C++ prototype of DiPri , which is essentially an extension of the state-of-the-art greybox fuzzer AFL++ (version 4.06). Other artifacts include the Java implementation of DiPri on Zest, the materials for integrating DiPri into FuzzBench and Magma, and the scripts for running docker and processing data. All artifacts can be found at our GitHub repository 1 and Zenodo archive. 2},
  archive      = {J_TOSEM},
  doi          = {10.1145/3701298},
  journal      = {ACM Transactions on Software Engineering and Methodology},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  title        = {DiPri: Distance-based seed prioritization for greybox Fuzzing—RCR report},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
