<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc---10">BIOMTC - 10</h2>
<ul>
<li><details>
<summary>
(2025). The subtype-free average causal effect for heterogeneous
disease etiology. <em>BIOMTC</em>, <em>81</em>(1), ujaf016. (<a
href="https://doi.org/10.1093/biomtc/ujaf016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have shown that the effect an exposure may have on a disease can vary for different subtypes of the same disease. However, existing approaches to estimate and compare these effects largely overlook causality. In this paper, we study the effect smoking may have on having colorectal cancer subtypes defined by a trait known as microsatellite instability (MSI). We use principal stratification to propose an alternative causal estimand, the Subtype-Free Average Causal Effect (SF-ACE). The SF-ACE is the causal effect of the exposure among those who would be free from other disease subtypes under any exposure level. We study non-parametric identification of the SF-ACE and discuss different monotonicity assumptions, which are more nuanced than in the standard setting. As is often the case with principal stratum effects, the assumptions underlying the identification of the SF-ACE from the data are untestable and can be too strong. Therefore, we also develop sensitivity analysis methods that relax these assumptions. We present 3 different estimators, including a doubly robust estimator, for the SF-ACE. We implement our methodology for data from 2 large cohorts to study the heterogeneity in the causal effect of smoking on colorectal cancer with respect to MSI subtypes.},
  archive      = {J_BIOMTC},
  author       = {Sasson, A and Wang, M and Ogino, S and Nevo, D},
  doi          = {10.1093/biomtc/ujaf016},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf016},
  shortjournal = {Biometrics},
  title        = {The subtype-free average causal effect for heterogeneous disease etiology},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiply robust difference-in-differences estimation of
causal effect curves for continuous exposures. <em>BIOMTC</em>,
<em>81</em>(1), ujaf015. (<a
href="https://doi.org/10.1093/biomtc/ujaf015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers commonly use difference-in-differences (DiD) designs to evaluate public policy interventions. While methods exist for estimating effects in the context of binary interventions, policies often result in varied exposures across regions implementing the policy. Yet, existing approaches for incorporating continuous exposures face substantial limitations in addressing confounding variables associated with intervention status, exposure levels, and outcome trends. These limitations significantly constrain policymakers’ ability to fully comprehend policy impacts and design future interventions. In this work, we propose new estimators for causal effect curves within the DiD framework, accounting for multiple sources of confounding. Our approach accommodates misspecification of a subset of intervention, exposure, and outcome models while avoiding any parametric assumptions on the effect curve. We present the statistical properties of the proposed methods and illustrate their application through simulations and a study investigating the heterogeneous effects of a nutritional excise tax under different levels of accessibility to cross-border shopping.},
  archive      = {J_BIOMTC},
  author       = {Hettinger, Gary and Lee, Youjin and Mitra, Nandita},
  doi          = {10.1093/biomtc/ujaf015},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf015},
  shortjournal = {Biometrics},
  title        = {Multiply robust difference-in-differences estimation of causal effect curves for continuous exposures},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Potential outcome simulation for efficient head-to-head
comparison of adaptive dose-finding designs. <em>BIOMTC</em>,
<em>81</em>(1), ujaf012. (<a
href="https://doi.org/10.1093/biomtc/ujaf012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dose-finding trials are a key component of the drug development process and rely on a statistical design to help inform dosing decisions. Triallists wishing to choose a design require knowledge of operating characteristics of competing methods. This is often assessed using a large-scale simulation study with multiple designs and configurations investigated, which can be time-consuming and therefore limits the scope of the simulation. We introduce a new approach to the design of simulation studies of dose-finding trials. The approach simulates all potential outcomes that individuals could experience at each dose level in the trial. Datasets are simulated in advance and then applied to each of the competing methods to enable a more efficient head-to-head comparison. Furthermore, individual trial datasets can be interrogated to understand when designs deviate in their decision making. In three case-studies, we show sizeable reductions in Monte Carlo error for comparing a performance metric between two competing designs. Efficiency gains depend on the similarity of the designs. Comparing two Phase I/II design variants, with high correlation of recommending the same optimal biologic dose, we show that the new approach requires a simulation study that is approximately 48 times smaller than the conventional approach. Furthermore, advance-simulated trial datasets can be reused to assess the performance of designs across multiple configurations. We recommend researchers consider this more efficient simulation approach in their dose-finding studies and we have updated the R package escalation to help facilitate implementation.},
  archive      = {J_BIOMTC},
  author       = {Sweeting, Michael and Slade, Daniel and Jackson, Dan and Brock, Kristian},
  doi          = {10.1093/biomtc/ujaf012},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf012},
  shortjournal = {Biometrics},
  title        = {Potential outcome simulation for efficient head-to-head comparison of adaptive dose-finding designs},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple and powerful method for large-scale composite null
hypothesis testing with applications in mediation analysis.
<em>BIOMTC</em>, <em>81</em>(1), ujaf011. (<a
href="https://doi.org/10.1093/biomtc/ujaf011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale mediation analysis has received increasing interest in recent years, especially in genome-wide epigenetic studies. The statistical problem in large-scale mediation analysis concerns testing composite null hypotheses in the context of large-scale multiple testing. The classical Sobel’s and joint significance tests are overly conservative and therefore are underpowered in practice. In this work, we propose a testing method for large-scale composite null hypothesis testing to properly control the type I error and hence improve the testing power. Our method is simple and essentially only requires counting the number of observed test statistics in a certain region. Non-asymptotic theories are established under weak assumptions and indicate that the proposed method controls the type I error well and is powerful. Extensive simulation studies confirm our non-asymptotic theories and show that the proposed method controls the type I error in all settings and has strong power. A data analysis on DNA methylation is also presented to illustrate our method.},
  archive      = {J_BIOMTC},
  author       = {Liu, Yaowu},
  doi          = {10.1093/biomtc/ujaf011},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf011},
  shortjournal = {Biometrics},
  title        = {A simple and powerful method for large-scale composite null hypothesis testing with applications in mediation analysis},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instrumental variable estimation of complier casual
treatment effects with interval-censored competing risks data.
<em>BIOMTC</em>, <em>81</em>(1), ujaf010. (<a
href="https://doi.org/10.1093/biomtc/ujaf010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the assessment of causal treatment effects on a time-to-event outcome, a crucial part of many scientific investigations. Although some methods have been developed for the problem, they are not applicable to situations where there exist both interval censoring and competing risks. We fill in this critical gap under a class of transformation models for cumulative incidence functions by developing an instrumented variable (IV) estimation approach. The IV is a valuable tool commonly used to mitigate the impact of endogenous treatment selection and to determine causal treatment effects in an unbiased manner. The proposed method is flexible as the model includes many commonly used models such as the sub-distributional proportional odds and hazards models (ie, the Fine–Gray model) as special cases. The resulting estimator for the regression parameter is shown to be consistent and asymptotically normal. A simulation study is conducted to evaluate finite sample performance of the proposed approach and suggests that it works well in practice. It is applied to a breast cancer screening study.},
  archive      = {J_BIOMTC},
  author       = {Lou, Yichen and Ma, Yuqing and Sun, Jianguo and Wang, Peijie and Ye, Zhisheng},
  doi          = {10.1093/biomtc/ujaf010},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf010},
  shortjournal = {Biometrics},
  title        = {Instrumental variable estimation of complier casual treatment effects with interval-censored competing risks data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite likelihood inference for space-time point
processes. <em>BIOMTC</em>, <em>81</em>(1), ujaf009. (<a
href="https://doi.org/10.1093/biomtc/ujaf009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics of a rain forest is extremely complex involving births, deaths, and growth of trees with complex interactions between trees, animals, climate, and environment. We consider the patterns of recruits (new trees) and dead trees between rain forest censuses. For a current census, we specify regression models for the conditional intensity of recruits and the conditional probabilities of death given the current trees and spatial covariates. We estimate regression parameters using conditional composite likelihood functions that only involve the conditional first order properties of the data. When constructing assumption lean estimators of covariance matrices of parameter estimates, we only need mild assumptions of decaying conditional correlations in space, while assumptions regarding correlations over time are avoided by exploiting conditional centering of composite likelihood score functions. Time series of point patterns from rain forest censuses are quite short, while each point pattern covers a fairly big spatial region. To obtain asymptotic results, we therefore use a central limit theorem for the fixed timespan—increasing spatial domain asymptotic setting. This also allows us to handle the challenge of using stochastic covariates constructed from past point patterns. Conveniently, it suffices to impose weak dependence assumptions on the innovations of the space-time process. We investigate the proposed methodology by simulation studies and an application to rain forest data.},
  archive      = {J_BIOMTC},
  author       = {Jalilian, Abdollah and Cuevas-Pacheco, Francisco and Xu, Ganggang and Waagepetersen, Rasmus},
  doi          = {10.1093/biomtc/ujaf009},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf009},
  shortjournal = {Biometrics},
  title        = {Composite likelihood inference for space-time point processes},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining experimental and observational data through a
power likelihood. <em>BIOMTC</em>, <em>81</em>(1), ujaf008. (<a
href="https://doi.org/10.1093/biomtc/ujaf008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials are the gold standard for causal inference and play a pivotal role in modern evidence-based medicine. However, the sample sizes they use are often too limited to provide adequate power for drawing causal conclusions. In contrast, observational data are becoming increasingly accessible in large volumes but can be subject to bias as a result of hidden confounding. Given these complementary features, we propose a power likelihood approach to augmenting randomized controlled trials with observational data to improve the efficiency of treatment effect estimation. We provide a data-adaptive procedure for maximizing the expected log predictive density (ELPD) to select the learning rate that best regulates the information from the observational data. We validate our method through a simulation study that shows increased power while maintaining an approximate nominal coverage rate. Finally, we apply our method in a real-world data fusion study augmenting the PIONEER 6 clinical trial with a US health claims dataset, demonstrating the effectiveness of our method and providing detailed guidance on how to address practical considerations in its application.},
  archive      = {J_BIOMTC},
  author       = {Lin, Xi and Tarp, Jens Magelund and Evans, Robin J},
  doi          = {10.1093/biomtc/ujaf008},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf008},
  shortjournal = {Biometrics},
  title        = {Combining experimental and observational data through a power likelihood},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature screening for metric space-valued responses based on
fréchet regression with its applications. <em>BIOMTC</em>,
<em>81</em>(1), ujaf007. (<a
href="https://doi.org/10.1093/biomtc/ujaf007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various applications, we need to handle more general types of responses, such as distributional data and matrix-valued data, rather than a scalar variable. When the dimension of predictors is ultrahigh, it is necessarily important to identify the relevant predictors for such complex types of responses. For example, in our Alzheimer’s disease neuroimaging study, we need to select the relevant single nucleotide polymorphisms out of 582 591 candidates for the distribution of voxel-level intensities in each of 42 brain regions. To this end, we propose a new sure independence screening (SIS) procedure for general metric space-valued responses based on global Fréchet regression, termed as Fréchet-SIS. The marginal general residual sum of squares is utilized to serve as a marginal utility for evaluating the importance of predictors, where only a distance between data objects is needed. We theoretically show that the proposed Fréchet-SIS procedure enjoys the sure screening property under mild regularity conditions. Monte Carlo simulations are conducted to demonstrate its excellent finite-sample performance. In Alzheimer’s disease neuroimaging study, we identify important genes that correlate with brain activity across different stages of the disease and brain regions. In addition, we also include an economic case study to illustrate our proposal.},
  archive      = {J_BIOMTC},
  author       = {Tian, Bing and Kang, Jian and Zhong, Wei},
  doi          = {10.1093/biomtc/ujaf007},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf007},
  shortjournal = {Biometrics},
  title        = {Feature screening for metric space-valued responses based on fréchet regression with its applications},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-observations for bivariate survival data.
<em>BIOMTC</em>, <em>81</em>(1), ujaf006. (<a
href="https://doi.org/10.1093/biomtc/ujaf006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pseudo-observations approach has been gaining popularity as a method to estimate covariate effects on censored survival data. It is used regularly to estimate covariate effects on quantities such as survival probabilities, restricted mean life, cumulative incidence, and others. In this work, we propose to generalize the pseudo-observations approach to situations where a bivariate failure-time variable is observed, subject to right censoring. The idea is to first estimate the joint survival function of both failure times and then use it to define the relevant pseudo-observations. Once the pseudo-observations are calculated, they are used as the response in a generalized linear model. We consider 2 common nonparametric estimators of the joint survival function: the estimator of Lin and Ying (1993) and the Dabrowska estimator (Dabrowska, 1988). For both estimators, we show that our bivariate pseudo-observations approach produces regression estimates that are consistent and asymptotically normal. Our proposed method enables estimation of covariate effects on quantities such as the joint survival probability at a fixed bivariate time point or simultaneously at several time points and, consequentially, can estimate covariate-adjusted conditional survival probabilities. We demonstrate the method using simulations and an analysis of 2 real-world datasets.},
  archive      = {J_BIOMTC},
  author       = {Travis-Lumer, Yael and Mandel, Micha and Betensky, Rebecca A},
  doi          = {10.1093/biomtc/ujaf006},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf006},
  shortjournal = {Biometrics},
  title        = {Pseudo-observations for bivariate survival data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Report of the editors—2024. <em>BIOMTC</em>, <em>81</em>(1),
ujaf004. (<a href="https://doi.org/10.1093/biomtc/ujaf004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1093/biomtc/ujaf004},
  journal      = {Biometrics},
  month        = {2},
  number       = {1},
  pages        = {ujaf004},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2024},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
