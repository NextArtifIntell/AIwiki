<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>informs_all</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h1 id="informs">INFORMS</h1>
<h2 id="deca---6">DECA - 6</h2>
<ul>
<li><details>
<summary>
(2025). Appreciation to referees, 2024. <em>DECA</em>,
<em>22</em>(1), 87–88. (<a
href="https://doi.org/10.1287/deca.2025.thanks.v22.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vicki Bier, the Editor-in-Chief of Decision Analysis , thanks the referees who generously provide expert counsel and guidance on a voluntary basis. Without them, the journal could not function. The following list acknowledges those individuals who acted as referees for papers considered from September 2023 to September 2024.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2025.thanks.v22.n1},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {87-88},
  shortjournal = {Decis. Anal.},
  title        = {Appreciation to referees, 2024},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision making in information security investments: Impact
of system vulnerability and investment timing on resource-sharing
platforms. <em>DECA</em>, <em>22</em>(1), 70–86. (<a
href="https://doi.org/10.1287/deca.2024.0190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study distinguishes enterprises into high- and low-type categories based on enterprise value and cost efficiency, examining their strategic behaviors in three investment timing games: move simultaneously, the high-type enterprise moves first, and the low-type enterprise moves first. By comparing the three games, we find that both types of enterprises would always exert more effort in the sequential game than in the simultaneous game, and the later-move advantage makes both types of enterprises prefer to become the follower in the game. We also find that the enhanced cost efficiency advantage or enterprise value gap possessed by the high-type enterprise would widen the effort gap between the two types of enterprises, and enhance the low-type enterprise’s incentive to be the follower. Moreover, the existence of system vulnerability not only causes both types of enterprises to reduce their security effort that generates free-riding behaviors but also can first discourage and then encourage enterprises from moving in advance. We further propose a liability-based mechanism to tackle the free-riding problem. We reveal an exact optimal liability coefficient, whether in the simultaneous or sequential game and find that the high-type enterprise should undertake more compensation when its dominant position becomes more obvious and the low-type enterprise could therefore undertake decreased liability. Last, we extend the model to multiple enterprises and show that the results are robust. Funding: This work was supported by the Shanghai Science and Technology Development Foundation [Grant 24692107700]; the Humanities and Social Science Fund of Ministry of Education of China [Grant 23YJA630100]; Fundamental Research Funds for the Central Universities; DHU Distinguished Young Professor Program; and the Shanghai Social Science Foundation [Grant 2022ZGL009]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/deca.2024.0190 .},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0190},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {70-86},
  shortjournal = {Decis. Anal.},
  title        = {Decision making in information security investments: Impact of system vulnerability and investment timing on resource-sharing platforms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on ecological restoration strategies for abandoned
mines based on ecology-oriented development. <em>DECA</em>,
<em>22</em>(1), 44–69. (<a
href="https://doi.org/10.1287/deca.2023.0132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ecological restoration of abandoned mines poses a significant challenge within China’s broader efforts toward the protection and restoration of the nation’s ecosystems. Based on the ecology-oriented development (EOD) model, a differential game framework for the ecological restoration of abandoned mines was constructed in this study, involving both government and corporate entities as players. Our analysis focused on various factors, including investment levels in ecological restoration quality, the quality of ecological restoration itself, investment levels in environmentally sensitive industry development, the quality of environmentally sensitive industries, environmentally sensitive product pricing strategies, and profit dynamics for both players across four distinct modes. Our findings reveal several key insights. First, extending the government’s concession period incentivizes enterprises to increase investment in both the quality of ecological restoration of abandoned mines and the development of environmentally sensitive industries. Second, the government should prefer the cooperative mode to introduce social capital for the ecology-oriented development of abandoned mines. Third, in noncooperative mode, for technology-intensive industries, the government’s industrial support policy is more mutually beneficial; conversely, for labor-intensive industries, the ecological restoration compensation policy yields higher mutual gains. Lastly, differentiated incentive policies implemented by the government across various modes prompt the input of enterprises in ecological restoration quality and environmentally sensitive industry development. Based on these findings, we offer specific policy recommendations to guide governmental efforts in promoting EOD for the ecological restoration of abandoned mines. Funding: This work was supported by the Sichuan Office of Philosophy and Social Science [Grant SC22C004] and the National Social Science Fund of China [Grant 22XKS015].},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2023.0132},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {44-69},
  shortjournal = {Decis. Anal.},
  title        = {Research on ecological restoration strategies for abandoned mines based on ecology-oriented development},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Satiation-based theory of frugal materialism. <em>DECA</em>,
<em>22</em>(1), 30–43. (<a
href="https://doi.org/10.1287/deca.2024.0192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frugal materialism is a tendency of consumer demand to become more elastic in product durability in response to a tightening budget constraint. This paper proposes a value function-based model of frugal materialism and establishes a close theoretical link between frugal materialism and the slope of value satiation as defined in the decision analytic literature; for both their absolute and relative versions, frugal materialism and increasing value satiation are nearly equivalent to each other. Only some shapes of the value function are thus compatible with frugal materialism, and the shape restrictions involve concepts familiar from models of decision making under risk, even though the proposed model of frugal materialism does not involve any risk. Under the additional assumption of relative risk neutrality, the demand for risky assets of a frugally materialistic consumer should exhibit well-known behavioral patterns associated with increasing risk aversion, and there is only a narrow possibility for frugal materialism to coexist with precautionary saving behavior.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0192},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {30-43},
  shortjournal = {Decis. Anal.},
  title        = {Satiation-based theory of frugal materialism},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why investors want risk. <em>DECA</em>, <em>22</em>(1),
14–29. (<a href="https://doi.org/10.1287/deca.2024.0206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We imagine investors taking shares in an exogenous lognormal cash payoff with known parameters. Using a power utility (constant relative risk aversion (CRRA)) replica of Lintner’s static payoffs -based constant absolute risk aversion-normal capital asset pricing model, we examine how an investor’s expected utility is affected by the payoff parameters and surrounding market conditions. The market clearing asset price falls as a proportion of wealth when market wealth is higher, implying that CRRA investors hold a lower (rather than fixed) proportion of wealth in the risky asset when they are wealthier. Investors prefer conditions where they can obtain more risk, either because the risky payoff is exogenously riskier or competing investors want less. The equilibrium asset price is “disproportionately” lower when the asset is riskier, or when there are fewer willing buyers, leaving a better opportunity with higher expected utility.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0206},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {14-29},
  shortjournal = {Decis. Anal.},
  title        = {Why investors want risk},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the value of information across decision problems.
<em>DECA</em>, <em>22</em>(1), 1–13. (<a
href="https://doi.org/10.1287/deca.2024.0187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The value of information is an important concept in decision analysis that has been quantified as the buying price ( BPI) for the information and the expected utility increase ( EUI) obtainable by using the information. These two measures rank information sources identically in a scalar-valued decision problem only when the utility function is linear or exponential. In contrast, this paper focuses on the value of information across scalar-valued decision problems sharing the same utility function such as different divisions within an organization exploring various information sources for their decisions using the same organizational utility function. In this context, it still makes sense to ask which sources are more informative. We show that BPI and EUI rank information sources identically in this context only when the utility function is linear . However, if the certainty equivalent increase is used instead of EUI , then identical ranking with BPI across problems is maintained for the broader class of linear or exponential utility functions. We discuss the importance of these results for distributed decision-making settings, where different departments within an organization may calculate the value of information separately. Our results advise against using EUI to measure information value in this context when risk attitude is important.},
  archive      = {J_DECA},
  doi          = {10.1287/deca.2024.0187},
  journal      = {Decision Analysis},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Decis. Anal.},
  title        = {On the value of information across decision problems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijaa---6">IJAA - 6</h2>
<ul>
<li><details>
<summary>
(2025). Optimizing mobility for elderly and disabled dutch citizens
using taxis. <em>INFORMS Journal on Applied Analytics</em>,
<em>55</em>(1), 66–82. (<a
href="https://doi.org/10.1287/inte.2024.0180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the Netherlands, 200,000 elderly and disabled citizens annually use subsidized taxi rides executed by Transvision. The day-to-day planning of up to 15,000 long-distance rides was previously a complex and daunting task split over dozens of subcontractors. Transvision, CQM, and Geodan developed an optimization solution that combines the rides into efficient taxi routes. Starting in January 2020, this solution significantly improved the mobility challenge for elderly and disabled citizens, including (1) increased punctuality and a 50% improvement in passenger satisfaction, (2) savings of 15 million driving kilometers per year, and (3) combined financial savings for all stakeholders of 60 million euros over the years 2019 to 2023 and another total of 30 million euros projected for 2024 and 2025, according to conservative estimates. Daily planning in a single batch can range from 1,000 to 15,000 rides. To construct high-quality ride plans in reasonable time for this massive-scale operations research problem, we applied classical operations research techniques viewed through a modern lens. In this paper, we explain how practical large-scale dial-a-ride problems can be solved using high-quality heuristics that exploit the power of parallel processing. Furthermore, we present new and efficient techniques to perform the required millions to billions of calculations to determine distances and driving times on the Dutch road network. We overcome several practical challenges such as (1) aligning the interests of a vulnerable passenger group and over 60 different taxi operators, (2) aligning the software that interfaces with the various companies, and (3) adapting to changing regulations and ad hoc COVID-19 measures.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0180},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {66-82},
  title    = {Optimizing mobility for elderly and disabled dutch citizens using taxis},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of continuous steel annealing operations using
model predictive control at tata steel, india. <em>INFORMS Journal on
Applied Analytics</em>, <em>55</em>(1), 48–65. (<a
href="https://doi.org/10.1287/inte.2024.0183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In steel manufacturing, continuous annealing is a crucial heat treatment applied to cold-rolled steel strips to achieve a prescribed temperature, which will ensure quality in terms of mechanical properties. However, controlling this process is challenging because of slow furnace temperature dynamics, significant time delays, frequent changes in the steel mass flow rate, target annealing temperature changes induced by steel grade transitions, and multivariable interactions within the furnace zones. To address these challenges, Tata Steel, India, with consultancy support from the Indian Institute of Technology Bombay, developed a novel model predictive control (MPC) technology-based solution for a continuous annealing furnace, which produces automotive grade steels. The dynamic model was developed using data from both perturbation trials and scraping historical processes. We then converted the model to a discrete-time state-space form and used it to formulate an optimal control problem over a moving time window. The solution generates optimal furnace setpoints by solving this finite-horizon optimal control problem each minute, ensuring smooth temperature transitions during steel grade changes while avoiding operational constraint violations. Tata Steel successfully implemented an MPC-based real-time supervisory optimal control solution, which became fully operational in January 2023. The implementation of the solution has led to a significant improvement in the proportion of annealed products meeting the premium quality band (±5°C of the target temperature), increasing from 30% (manually operated) to 50% (MPC operated), thereby ensuring better uniformity of properties. Furthermore, an 8% reduction in products outside the widest band (±15°C) has prevented the reprocessing of 13,000 tons of material from one line alone, annually. We have seen a consistent 8% reduction in specific fuel consumption per ton of steel. When considering Tata Steel’s current installations and those under commissioning, these improvements translate to savings of US$2.5 million and a reduction of 10,000 tons of CO 2 emissions annually.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0183},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {48-65},
  title    = {Optimization of continuous steel annealing operations using model predictive control at tata steel, india},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). McDonald’s china adopts operations research for network
design. <em>INFORMS Journal on Applied Analytics</em>, <em>55</em>(1),
36–47. (<a href="https://doi.org/10.1287/inte.2024.0179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The supply chain network design (SCND) problem is a typical optimization problem that determines the structure of a supply chain and affects its costs and operational performance. SCND deals with various decisions, such as determining the number, size, and location of facilities and the optimal material and product flows of the entire supply chain network. Therefore, SCND is one of the most crucial planning problems in supply chain management. In this paper, we present a practical approach in which we adopt a mixed-integer programming (MIP) mathematical model to solve a real industry SCND problem for McDonald’s China. As a result of this project, McDonald’s China has saved millions of dollars in logistics costs and reduced CO 2 emissions by more than 10%. In our approach, size-reduction techniques were successfully applied to deal with a large-scale model, making it possible to analyze hundreds of scenarios before coming to a consensus.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0179},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {36-47},
  title    = {McDonald’s china adopts operations research for network design},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven inventory control and integrated employee
involvement for special buys at ALDI SÜD germany. <em>INFORMS Journal on
Applied Analytics</em>, <em>55</em>(1), 22–35. (<a
href="https://doi.org/10.1287/inte.2024.0182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a subsidiary of the ALDI SOUTH Group, ALDI SÜD Germany launched the data-driven special buys project in response to pandemic-related supply chain disruptions, market shifts, and growing competition. This initiative combines advanced analytics and operations research with employee engagement to optimize product life cycles of special buys products. The solution components include (1) multiperiod mixed-integer optimization (MIP) models for product order decisions to warehouses; (2) XGBoost classification for store product allocation (SAM); (3) a proprietary algorithm (ANA) for just-in-time reallocations between stores; (4) a multiperiod dynamic programming model (DAVE) developed in 2021 for nationwide inventory clearance; (5) an advanced version for inventory clearance, the DAVE stochastic dynamic programming model (DAVE SDP), introduced in 2023; (6) a multivariate regression model for budget allocations for markdowns on small product leftover quantities for incorporating store employee involvement; and (7) a smartphone application (Market-Whispering) to engage 50,000 employees in product selection. This paper focuses on the MIP, which optimizes the decision on stock order quantities to warehouses. We examine this in conjunction with ANA and DAVE because of their significant influence on operational efficiency. For DAVE, we examine two variants: the reference model DAVE and DAVE SDP. The project faced coordination challenges and required a strategic shift from decentralized to centralized approaches. Proprietary software for the special buys product range improved operational efficiency, positively impacting the daily operations of 40,000 store employees, and led to annual savings of several million euros in Germany.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0182},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {22-35},
  title    = {Data-driven inventory control and integrated employee involvement for special buys at ALDI SÜD germany},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven at sea: Forecasting and revenue management at
molslinjen. <em>INFORMS Journal on Applied Analytics</em>,
<em>55</em>(1), 5–21. (<a
href="https://doi.org/10.1287/inte.2024.0177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Molslinjen, one of the world’s largest operators of fast-moving catamaran ferries, based in Denmark, adopted a focus on digitalization to profoundly change its operations and business practices. Molslinjen partnered with Halfspace, a data, analytics, and artificial intelligence (AI) company based in Copenhagen, Denmark, to support that transition. Halfspace and Molslinjen jointly developed and deployed a successful forecasting and revenue management toolbox for the data-driven operation of ferries in Denmark since 2020. This has resulted in $2.6–3.2 million yearly savings (and a total of $5 million savings as of December 2023), a significant reduction in the number of delayed departures and average delays, and a 3% reduction in fuel costs and emissions. This toolbox relies on some of the latest advances in machine learning for forecasting and in analytics approaches to revenue management. The potential for generalizing our toolbox to the global ferry industry is significant, with an impact on both revenues and environmental, societal, and governance criteria.},
  archive  = {J},
  doi      = {10.1287/inte.2024.0177},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {5-21},
  title    = {Data-driven at sea: Forecasting and revenue management at molslinjen},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction: 2024 franz edelman award for achievement in
advanced analytics, operations research, and management science.
<em>INFORMS Journal on Applied Analytics</em>, <em>55</em>(1), 1–4. (<a
href="https://doi.org/10.1287/inte.2024.intro.v55.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This special issue of the INFORMS Journal on Applied Analytics (formerly Interfaces ) is devoted to the finalists of the 2024 annual competition for the Franz Edelman Award for Achievement in Advanced Analytics, Operations Research, and Management Science, the profession’s most prestigious award for deployed work. As in previous years, the finalists this year cover a wide range of industries and functions.},
  archive  = {J},
  doi      = {10.1287/inte.2024.intro.v55.n1},
  journal  = {INFORMS Journal on Applied Analytics},
  month    = {1-2},
  number   = {1},
  pages    = {1-4},
  title    = {Introduction: 2024 franz edelman award for achievement in advanced analytics, operations research, and management science},
  volume   = {55},
  year     = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijds---6">IJDS - 6</h2>
<ul>
<li><details>
<summary>
(2025). A reduced modeling approach for making predictions with
incomplete data having blockwise missing patterns. <em>IJDS</em>,
<em>4</em>(1), 85–99. (<a
href="https://doi.org/10.1287/ijds.2022.9016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete data with blockwise missing patterns are commonly encountered in analytics, and solutions typically entail listwise deletion or imputation. However, as the proportion of missing values in input features increases, listwise or columnwise deletion leads to information loss, whereas imputation diminishes the integrity of the training data set. We present the blockwise reduced modeling (BRM) method for analyzing blockwise missing patterns, which adapts and improves on the notion of reduced modeling proposed by Friedman, Kohavi, and Yun in 1996 as lazy decision trees. In contrast to the original idea of reduced modeling of delaying model induction until a prediction is required, our method is significantly faster because it exploits the blockwise missing patterns to pretrain ensemble models that require minimum imputation of data. Models are pretrained over the overlapping subsets of an incomplete data set that contain only populated values. During prediction, each test instance is mapped to one of these models based on its feature-missing pattern. BRM can be applied to any supervised learning model for tabular data. We benchmark the predictive performance of BRM using simulations of blockwise missing patterns on three complete data sets from public repositories. Thereafter, we evaluate its utility on three data sets with actual blockwise missing patterns. We demonstrate that BRM is superior to most existing benchmarks in terms of predictive performance for linear and nonlinear models. It also scales well and is more reliable than existing benchmarks for making predictions with blockwise missing pattern data. History: Maytal Saar-Tsechansky served as the senior editor for this article. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/0274716/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.9016 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.9016},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {85-99},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {A reduced modeling approach for making predictions with incomplete data having blockwise missing patterns},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair collaborative learning (FairCL): A method to improve
fairness amid personalization. <em>IJDS</em>, <em>4</em>(1), 67–84. (<a
href="https://doi.org/10.1287/ijds.2024.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model personalization has attracted widespread attention in recent years. In an ideal situation, if individuals’ data are sufficient, model personalization can be realized by building models separately for different individuals using their own data. But, in reality, individuals often have data sets of varying sizes and qualities. To overcome this disparity, collaborative learning has emerged as a generic strategy for model personalization, but there is no mechanism to ensure fairness in this framework. In this paper, we develop fair collaborative learning (FairCL) that could potentially integrate a variety of fairness concepts. We further focus on two specific fairness metrics, the bounded individual loss and individual fairness, and develop a self-adaptive algorithm for FairCL and conduct both simulated and real-world case studies. Our study reveals that model fairness and accuracy could be improved simultaneously in the context of model personalization. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: This work was supported by the Breakthrough T1D Award [Grant 2-SRA-2022-1259-S-B]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/1331847/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0029 ). The real-world data, including the transportation demand management and surgical site infection data sets, are proprietary and not publicly available. Other results are available at https://github.com/ryanlif/FairCL .},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0029},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {67-84},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Fair collaborative learning (FairCL): A method to improve fairness amid personalization},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multilabel classification for fine-level event
extraction from aviation accident reports. <em>IJDS</em>, <em>4</em>(1),
51–66. (<a href="https://doi.org/10.1287/ijds.2022.0032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large numbers of accident reports are recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we must understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. To make the labeling process more efficient, many researchers have started developing algorithms to automatically identify the underlying events from accident reports. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem to be a hierarchical classification task, where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into the bidirectional encoder representations from transformers model. To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated using data collected by the National Transportation Safety Board. It has been shown that fine-level prediction accuracy is highly improved and that the regularization term can be beneficial to the rare event identification problem. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: The research reported in this paper was supported by funds from NASA University Leadership Initiative program (Contract No. NNX17AJ86A, Project Officer: Dr. Anupa Bajwa, Principal Investigator: Dr. Yongming Liu) and NSF DMS 1830363. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/9128124/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0032 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0032},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {51-66},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Hierarchical multilabel classification for fine-level event extraction from aviation accident reports},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank robust subspace tensor clustering for metro
passenger flow modeling. <em>IJDS</em>, <em>4</em>(1), 33–50. (<a
href="https://doi.org/10.1287/ijds.2022.0028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor clustering has become an important topic, specifically in spatiotemporal modeling, because of its ability to cluster spatial modes (e.g., stations or road segments) and temporal modes (e.g., time of day or day of the week). Our motivating example is from subway passenger flow modeling, where similarities between stations are commonly found. However, the challenges lie in the innate high-dimensionality of tensors and also the potential existence of anomalies. This is because the three tasks, that is, dimension reduction, clustering, and anomaly decomposition, are intercorrelated with each other, and treating them in a separate manner will render a suboptimal performance. Thus, in this work, we design a tensor-based subspace clustering and anomaly decomposition technique for simultaneous outlier-robust dimension reduction and clustering for high-dimensional tensors. To achieve this, a novel low-rank robust subspace clustering decomposition model is proposed by combining Tucker decomposition, sparse anomaly decomposition, and subspace clustering. An effective algorithm based on Block Coordinate Descent is proposed to update the parameters. Prudent experiments prove the effectiveness of the proposed framework via the simulation study, with a gain of +25% clustering accuracy over benchmark methods in a hard case. The interrelations of the three tasks are also analyzed via ablation studies, validating the interrelation assumption. Moreover, a case study in station clustering based on real passenger flow data is conducted, with quite valuable insights discovered. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: H. Yan is partially funded by DOE [DE-EE0009354] and NSF [CMMI 2316654]. F. Tsung is partially funded with the RGC [GRF 16201718 and 16216119]. The authors appreciate the help from the Hong Kong MTR Co. research, marketing, and customer service teams. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6536164/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0028 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0028},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {33-50},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Low-rank robust subspace tensor clustering for metro passenger flow modeling},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal time series forecasting using an iterative
kernel-based regression. <em>IJDS</em>, <em>4</em>(1), 20–32. (<a
href="https://doi.org/10.1287/ijds.2023.0019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal time series analysis is a growing area of research that includes different types of tasks, such as forecasting, prediction, clustering, and visualization. In many domains, like epidemiology or economics, time series data are collected to describe the observed phenomenon in particular locations over a predefined time slot and predict future behavior. Regression methods provide a simple mechanism for evaluating empirical functions over scattered data points. In particular, kernel-based regressions are suitable for cases in which the relationship between the data points and the function is not linear. In this work, we propose a kernel-based iterative regression model, which fuses data from several spatial locations for improving the forecasting accuracy of a given time series. In more detail, the proposed method approximates and extends a function based on two or more spatial input modalities coded by a series of multiscale kernels, which are averaged as a convex combination. The proposed spatio-temporal regression resembles ideas that are present in deep learning architectures, such as passing information between scales. Nevertheless, the construction is easy to implement, and it is also suitable for modeling data sets of limited size. Experimental results demonstrate the proposed model for solar energy prediction, forecasting epidemiology infections, and future number of fire events. The method is compared with well-known regression techniques and highlights the benefits of the proposed model in terms of accuracy and flexibility. The reliable outcome of the proposed model and its nonparametric nature yield a robust tool to be integrated as a forecasting component in wide range of decision support systems that analyze time series data. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This research was supported by the Israel Science Foundation [Grant 1144/20] and partly supported by the Ministry of Science and Technology, Israel [Grant 5614]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6417440/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0019 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0019},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {20-32},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Spatio-temporal time series forecasting using an iterative kernel-based regression},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking cost-sensitive classification in deep learning
via adversarial data augmentation. <em>IJDS</em>, <em>4</em>(1), 1–19.
(<a href="https://doi.org/10.1287/ijds.2022.0033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-sensitive classification is critical in applications where misclassification errors widely vary in cost. However, overparameterization poses fundamental challenges to the cost-sensitive modeling of deep neural networks (DNNs). The ability of a DNN to fully interpolate a training data set can render a DNN, evaluated purely on the training set, ineffective in distinguishing a cost-sensitive solution from its overall accuracy maximization counterpart. This necessitates rethinking cost-sensitive classification in DNNs. To address this challenge, this paper proposes a cost-sensitive adversarial data augmentation (CSADA) framework to make overparameterized models cost sensitive. The overarching idea is to generate targeted adversarial examples that push the decision boundary in cost-aware directions. These targeted adversarial samples are generated by maximizing the probability of critical misclassifications and used to train a model with more conservative decisions on costly pairs. Experiments on well-known data sets and a pharmacy medication image (PMI) data set, made publicly available, show that our method can effectively minimize the overall cost and reduce critical errors while achieving comparable overall accuracy. History: Nick Street served as the senior editor for this article. Funding: Research reported in this publication was supported by the National Library of medicine of the National Institutes of Health in the United States under award number R01LM013624. Data Ethics &amp; Reproducibility Note: This paper abides by data ethics requirements. Data used are publicly available online at https://deepblue.lib.umich.edu/data/concern/data_sets/6d56zw997 . Codes to replicate the results of this paper are available on Code Ocean at https://doi.org/10.24433/CO.2139841.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0033 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0033},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Rethinking cost-sensitive classification in deep learning via adversarial data augmentation},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijoc---11">IJOC - 11</h2>
<ul>
<li><details>
<summary>
(2025). Appreciation to reviewers. <em>IJOC</em>, <em>37</em>(1),
189–196. (<a
href="https://doi.org/10.1287/ijoc.2025.thx.v37.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On behalf of the Editorial Board, I would like to thank the hundreds of people who acted as reviewers for the INFORMS Journal on Computing during the past year. Reviewers are the cornerstone of the peer review system and give their time and expertise unselfishly. IJOC reviewers, please know you are greatly appreciated! Alice Smith, Editor-in-Chief},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2025.thx.v37.n1},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {189-196},
  shortjournal = {INFORMS J. Comput.},
  title        = {Appreciation to reviewers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum-inspired bilevel optimization algorithm for the
first responder network design problem. <em>IJOC</em>, <em>37</em>(1),
172–188. (<a href="https://doi.org/10.1287/ijoc.2024.0574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the aftermath of a sudden catastrophe, first responders (FRs) strive to reach and rescue immobile victims. Simultaneously, civilians use the same roads to evacuate, access medical facilities and shelters, or reunite with their relatives via private vehicles. The escalated traffic congestion can significantly hinder critical FR operations. A proposal from the Türkiye Ministry of Transportation and Infrastructure is to allocate a lane on specific road segments exclusively for FR use, mark them clearly, and precommunicate them publicly. For a successful implementation of this proposal, an FR path should exist from designated entry points to each FR demand point in the network. The reserved FR lanes along these paths will be inaccessible to evacuees, potentially increasing evacuation times. Hence, in this study, we aim to determine a subset of links along which an FR lane should be reserved and analyze the resulting evacuation flow under evacuees’ selfish routing behavior. We introduce this problem as the first responder network design problem (FRNDP) and formulate it as a mixed-integer nonlinear program. To efficiently solve FRNDP, we introduce a novel bilevel nested heuristic, the Graver augmented multiseed algorithm (GAMA) within GAMA, called GAGA. We test GAGA on synthetic graph instances of various sizes as well as scenarios related to a potential Istanbul earthquake. Our comparisons with a state-of-the-art exact algorithm for network design problems demonstrate that GAGA offers a promising alternative approach and highlights the need for further exploration of quantum-inspired computing to tackle complex real-world problems. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: S. Tayur and A. Tenneti acknowledge Raytheon BBN (RTX-BBN) for its support through a Carnegie Mellon University-BBN contract as part of a Defense Advanced Research Projects Agency project on quantum-inspired classical computing. A. Karahalios is supported by the National Science Foundation Graduate Research Fellowship Program [Grants DGE1745016, DGE2140739]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0574 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0574 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0574},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {172-188},
  shortjournal = {INFORMS J. Comput.},
  title        = {A quantum-inspired bilevel optimization algorithm for the first responder network design problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the instance dependence of parameter initialization for
the quantum approximate optimization algorithm: Insights via instance
space analysis. <em>IJOC</em>, <em>37</em>(1), 146–171. (<a
href="https://doi.org/10.1287/ijoc.2024.0564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantum approximate optimization algorithm (QAOA) tackles combinatorial optimization problems in a quantum computing context, where achieving globally optimal and exact solutions is not always feasible because of classical computational constraints or problem complexity. The performance of QAOA generally depends on finding sufficiently good parameters that facilitate competitive approximate solutions. However, this is fraught with challenges, such as “barren plateaus,” making the search for effective parameters a nontrivial endeavor. More recently, the question of whether such an optimal parameter search is even necessary has been posed, with some studies showing that optimal parameters tend to be concentrated on certain values for specific types of problem instances. However, these existing studies have only examined specific instance classes of Maximum Cut, so it is uncertain if the claims of instance independence apply to a diverse range of instances. In this paper, we use instance space analysis to study QAOA parameter initialization strategies for the first time, providing a comprehensive study of how instance characteristics affect the performance of initialization strategies across a diverse set of graph types and weight distributions. Unlike previous studies that focused on specific graph classes (e.g., d -regular or Erdős–Rényi), our work examines a much broader range of instance types, revealing insights about parameter transfer between different graph classes. We introduce and evaluate a new initialization strategy, quantum instance-based parameter initialization, that leverages instance-specific information, demonstrating its effectiveness across various instance types. Our analysis at higher QAOA depths ( p = 15) provides insights into the effectiveness of different initialization strategies beyond the low-depth circuits typically studied. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. This article is accepted for Special Issue. Funding: This research was supported by the Australian Research Council [Grant IC200100009 for the Australian Research Council Training Centre in Optimization Technologies, Integrated Methodologies and Applications]. V. Katial is supported by The University of Melbourne [Research Training Program Scholarship]. The authors gratefully acknowledge the information technology infrastructure support provided by The University of Melbourne’s Research Computing Services and the Petascale Campus Initiative. This research was also supported by The University of Melbourne through the establishment of the IBM Quantum Network Hub at the university. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0564 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0564 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0564},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {146-171},
  shortjournal = {INFORMS J. Comput.},
  title        = {On the instance dependence of parameter initialization for the quantum approximate optimization algorithm: Insights via instance space analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage estimation and variance modeling for
latency-constrained variational quantum algorithms. <em>IJOC</em>,
<em>37</em>(1), 125–145. (<a
href="https://doi.org/10.1287/ijoc.2024.0575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantum approximate optimization algorithm (QAOA) has enjoyed increasing attention in noisy, intermediate-scale quantum computing with its application to combinatorial optimization problems. QAOA has the potential to demonstrate a quantum advantage for NP-hard combinatorial optimization problems. As a hybrid quantum-classical algorithm, the classical component of QAOA resembles a simulation optimization problem in which the simulation outcomes are attainable only through a quantum computer. The simulation that derives from QAOA exhibits two unique features that can have a substantial impact on the optimization process: (i) the variance of the stochastic objective values typically decreases in proportion to the optimality gap, and (ii) querying samples from a quantum computer introduces an additional latency overhead. In this paper, we introduce a novel stochastic trust-region method derived from a derivative-free, adaptive sampling trust-region optimization method intended to efficiently solve the classical optimization problem in QAOA by explicitly taking into account the two mentioned characteristics. The key idea behind the proposed algorithm involves constructing two separate local models in each iteration: a model of the objective function and a model of the variance of the objective function. Exploiting the variance model allows us to restrict the number of communications with the quantum computer and also helps navigate the nonconvex objective landscapes typical in QAOA optimization problems. We numerically demonstrate the superiority of our proposed algorithm using the SimOpt library and Qiskit when we consider a metric of computational burden that explicitly accounts for communication costs. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This material is based upon work supported by the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers and the Office of Advanced Scientific Computing Research, Accelerated Research for Quantum Computing program under contract number DE-AC02-06CH11357. Y. Ha and S. Shashaani also gratefully acknowledge the U.S. National Science Foundation Division of Civil, Mechanical and Manufacturing Innovation Grant CMMI-2226347 and the U.S. Office of Naval Research [Grant N000142412398]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0575 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0575 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0575},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {125-145},
  shortjournal = {INFORMS J. Comput.},
  title        = {Two-stage estimation and variance modeling for latency-constrained variational quantum algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QHDOPT: A software for nonlinear optimization with quantum
hamiltonian descent. <em>IJOC</em>, <em>37</em>(1), 107–124. (<a
href="https://doi.org/10.1287/ijoc.2024.0587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an open-source, end-to-end software (named QHDOPT), which can solve nonlinear optimization problems using the quantum Hamiltonian descent (QHD) algorithm. QHDOPT offers an accessible interface and automatically maps tasks to various supported quantum backends (i.e., quantum hardware machines). These features enable users, even those without prior knowledge or experience in quantum computing, to utilize the power of existing quantum devices for nonlinear and nonconvex optimization tasks. In its intermediate compilation layer, QHDOPT employs SimuQ, an efficient interface for Hamiltonian-oriented programming, to facilitate multiple algorithmic specifications and ensure compatible cross-hardware deployment. The detailed documentation of QHDOPT is available at https://github.com/jiaqileng/QHDOPT . History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This work was supported by the U.S. Department of Energy’s Advanced Research Projects Agency–Energy [Grant DE-SC0020273], the Alfred P. Sloan Foundation, the Simons Foundation [Simons Investigator Award 825053], the Simons Quantum Postdoctoral Fellowship, the National Science Foundation [Grants CCF-1816695, CCF-1942837, and ECCS-2045978], the Unitary Fund, and the Air Force Office of Scientific Research [Grant FA95502110051]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0587 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0587 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0587},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {107-124},
  shortjournal = {INFORMS J. Comput.},
  title        = {QHDOPT: A software for nonlinear optimization with quantum hamiltonian descent},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Binary quantum control optimization with uncertain
hamiltonians. <em>IJOC</em>, <em>37</em>(1), 86–106. (<a
href="https://doi.org/10.1287/ijoc.2024.0560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the controls of quantum systems plays a crucial role in advancing quantum technologies. The time-varying noises in quantum systems and the widespread use of inhomogeneous quantum ensembles raise the need for high-quality quantum controls under uncertainties. In this paper, we consider a stochastic discrete optimization formulation of a discretized binary optimal quantum control problem involving Hamiltonians with predictable uncertainties. We propose a sample-based reformulation that optimizes both risk-neutral and risk-averse measurements of control policies, and solve these with two gradient-based algorithms using sum-up-rounding approaches. Furthermore, we discuss the differentiability of the objective function and prove upper bounds of the gaps between the optimal solutions to binary control problems and their continuous relaxations. We conduct numerical simulations on various sized problem instances based on two applications of quantum pulse optimization; we evaluate different strategies to mitigate the impact of uncertainties in quantum systems. We demonstrate that the controls of our stochastic optimization model achieve significantly higher quality and robustness compared with the controls of a deterministic model. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This work was supported by the US Department of Energy, Advanced Scientific Computing Research [Grants DE-AC02-06CH11357, DE-SC0018018]; Defense Sciences Office, DARPA [Grant IAA-8839-annex-130]; the US National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant 2041745]; and the US National Aeronautics and Space Administration (NASA) Ames Research Center [Grant 80ARC020D0010]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0560 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0560 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0560},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {86-106},
  shortjournal = {INFORMS J. Comput.},
  title        = {Binary quantum control optimization with uncertain hamiltonians},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel noise-aware classical optimizer for variational
quantum algorithms. <em>IJOC</em>, <em>37</em>(1), 63–85. (<a
href="https://doi.org/10.1287/ijoc.2024.0578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key component of variational quantum algorithms (VQAs) is the choice of classical optimizer employed to update the parameterization of an ansatz. It is well recognized that quantum algorithms will, for the foreseeable future, necessarily be run on noisy devices with limited fidelities. Thus, the evaluation of an objective function (e.g., the guiding function in the quantum approximate optimization algorithm (QAOA) or the expectation of the electronic Hamiltonian in variational quantum eigensolver (VQE)) required by a classical optimizer is subject not only to stochastic error from estimating an expected value but also to error resulting from intermittent hardware noise. Model-based derivative-free optimization methods have emerged as popular choices of a classical optimizer in the noisy VQA setting, based on empirical studies. However, these optimization methods were not explicitly designed with the consideration of noise. In this work we adapt recent developments from the “noise-aware numerical optimization” literature to these commonly used derivative-free model-based methods. We introduce the key defining characteristics of these novel noise-aware derivative-free model-based methods that separate them from standard model-based methods. We study an implementation of such noise-aware derivative-free model-based methods and compare its performance on demonstrative VQA simulations to classical solvers packaged in scikit-quant . History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This material is based upon work supported by the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers and the Office of Advanced Scientific Computing Research, Accelerated Research for Quantum Computing program under contract number DE-AC02-06CH11357. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0578 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0578 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0578},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {63-85},
  shortjournal = {INFORMS J. Comput.},
  title        = {A novel noise-aware classical optimizer for variational quantum algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multicommodity network flow model and branch and cut
for optimal quantum boolean circuit synthesis. <em>IJOC</em>,
<em>37</em>(1), 42–62. (<a
href="https://doi.org/10.1287/ijoc.2024.0562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new optimization model and a branch-and-cut approach for synthesizing optimal quantum circuits for reversible Boolean functions, which are pivotal components in quantum algorithms. Although heuristic algorithms have been extensively explored for quantum circuit synthesis, research on exact counterparts remains relatively limited. However, the need to design quantum circuits with guaranteed optimality is increasing, especially for improving computational fidelity on noisy intermediate-scale quantum devices. This study presents mathematical optimization as a viable option for optimal synthesis, with the potential to accommodate practical considerations arising in fast-evolving quantum technologies. We set a demonstrative problem to implement reversible Boolean functions using high-level gates known as multiple control Toffoli gates while minimizing a technology-based proxy called quantum cost—the number of low-level gates used to realize each high-level gate. To address this problem, we propose a discrete optimization model based on a multicommodity network and discuss potential future variations at an abstract level to incorporate technical considerations. A customized branch and cut is then developed upon different aspects of our model, including polyhedron integrality, surrogate constraints, and variable prioritization. Our experiments demonstrate the robustness of the proposed approach in finding cost-optimal circuits for all benchmark instances within a two-hour time frame. Furthermore, we present interesting intuitions from these experiments and compare our computational results with relevant studies, highlighting newly discovered circuits with the lowest quantum costs reported in this paper. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing. This paper has been accepted for the INFORMS Journal on Computing Special Issue on Quantum Computing. Funding: This research was supported by the Ministry of Science and Information and Communication Technology, South Korea [Grants 2017R1E1A1A0307098814 and 2020R1A4A307986411]. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0562 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0562 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0562},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {42-62},
  shortjournal = {INFORMS J. Comput.},
  title        = {A new multicommodity network flow model and branch and cut for optimal quantum boolean circuit synthesis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized noise suppression for quantum circuits.
<em>IJOC</em>, <em>37</em>(1), 22–41. (<a
href="https://doi.org/10.1287/ijoc.2024.0551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computation promises to advance a wide range of computational tasks. However, current quantum hardware suffers from noise and is too small for error correction. Thus, accurately utilizing noisy quantum computers strongly relies on noise characterization, mitigation, and suppression. Crucially, these methods must also be efficient in terms of their classical and quantum overhead. Here, we efficiently characterize and mitigate crosstalk noise, which is a severe error source in, for example, cross-resonance based superconducting quantum processors. For crosstalk characterization, we develop a simplified measurement experiment. Furthermore, we analyze the problem of optimal experiment scheduling and solve it for common hardware architectures. After characterization, we mitigate noise in quantum circuits by a noise-aware qubit routing algorithm. Our integer programming algorithm extends previous work on optimized qubit routing by swap insertion. We incorporate the measured crosstalk errors in addition to other, more easily accessible noise data in the objective function. Furthermore, we strengthen the underlying integer linear model by proving a convex hull result about an associated class of polytopes, which has applications beyond this work. We evaluate the proposed method by characterizing crosstalk noise for two chips with up to 127 qubits and leverage the resulting data to improve the approximation ratio of the Quantum Approximate Optimization Algorithm by up to 10% compared with other established noise-aware routing methods. Our work clearly demonstrates the gains of including noise data when mapping abstract quantum circuits to hardware native ones. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue on Quantum Computing. Funding: This work was supported by Bavarian state government; Bayerische Staatsministerium für Wirtschaft, Landesentwicklung und Energie. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0551 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0551 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0551},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {22-41},
  shortjournal = {INFORMS J. Comput.},
  title        = {Optimized noise suppression for quantum circuits},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient implementation of interior-point methods for
quantum relative entropy. <em>IJOC</em>, <em>37</em>(1), 3–21. (<a
href="https://doi.org/10.1287/ijoc.2024.0570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum relative entropy (QRE) programming is a recently popular and challenging class of convex optimization problems with significant applications in quantum computing and quantum information theory. We are interested in modern interior-point (IP) methods based on optimal self-concordant barriers for the QRE cone. A range of theoretical and numerical challenges associated with such barrier functions and the QRE cones have hindered the scalability of IP methods. To address these challenges, we propose a series of numerical and linear algebraic techniques and heuristics aimed at enhancing the efficiency of gradient and Hessian computations for the self-concordant barrier function, solving linear systems, and performing matrix-vector products. We also introduce and deliberate about some interesting concepts related to QRE such as symmetric quantum relative entropy. We design a two-phase method for performing facial reduction that can significantly improve the performance of QRE programming. Our new techniques have been implemented in the latest version (DDS 2.2) of the software package Domain-Driven Solver (DDS). In addition to handling QRE constraints, DDS accepts any combination of several other conic and nonconic convex constraints. Our comprehensive numerical experiments encompass several parts, including (1) a comparison of DDS 2.2 with Hypatia for the nearest correlation matrix problem, (2) using DDS 2.2 for combining QRE constraints with various other constraint types, and (3) calculating the key rate for quantum key distribution (QKD) channels and presenting results for several QKD protocols. History: Accepted by Giacomo Nannicini, Area Editor for Quantum Computing and Operations Research. Accepted for Special Issue. Funding: This work was supported by the National Science Foundation [Grant CMMI-2347120] and Discovery Grants from the Natural Sciences and Engineering Research Council of Canada. Supplemental Material: The software that supports the findings of this study is available within the paper and its Supplemental Information ( https://pubsonline.informs.org/doi/suppl/10.1287/ijoc.2024.0570 ) as well as from the IJOC GitHub software repository ( https://github.com/INFORMSJoC/2024.0570 ). The complete IJOC Software and Data Repository is available at https://informsjoc.github.io/ .},
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2024.0570},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {3-21},
  shortjournal = {INFORMS J. Comput.},
  title        = {Efficient implementation of interior-point methods for quantum relative entropy},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the special issue on quantum computing and
operations research. <em>IJOC</em>, <em>37</em>(1), 2. (<a
href="https://doi.org/10.1287/ijoc.2025.ed.v37.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJOC},
  doi          = {10.1287/ijoc.2025.ed.v37.n1},
  journal      = {INFORMS Journal on Computing},
  month        = {1-2},
  number       = {1},
  pages        = {2},
  shortjournal = {INFORMS J. Comput.},
  title        = {Introduction to the special issue on quantum computing and operations research},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ijoo---4">IJOO - 4</h2>
<ul>
<li><details>
<summary>
(2025). A polyhedral study of multivariate decision trees.
<em>IJOO</em>, <em>7</em>(1), 61–82. (<a
href="https://doi.org/10.1287/ijoo.2023.0017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are a widely used tool for interpretable machine learning. Multivariate decision trees employ hyperplanes at the branch nodes to route datapoints throughout the tree and yield more compact models than univariate trees. Recently, mixed-integer programming (MIP) has been applied to formulate the optimal decision tree problem. A key component of most MIP formulations is a specification of how to route datapoints in the tree from the root to the leaves. Our goal is to provide polyhedral characterizations of realizable routings, that is, routings that can be realized using multivariate branching rules. We first focus on shattering inequalities, a class of valid inequalities that can be used to strengthen almost any MIP formulation and that have been shown to be computationally effective. We prove that if all the feature vectors are in general position, then the shattering inequalities defined for the root of the tree are facet defining for the convex hull of the realizable routings. We then show that every facet-defining inequality of a depth one tree involving at least two variables is also facet defining for trees of arbitrary depth. Finally, we show that facet-defining inequalities characterizing realizable routings are also facet-defining for a complete MIP formulation. We validate our theoretical results by performing numerical experiments that specifically exploit shattering inequalities defined at the root node and we observe an improvement in performance for both numerical and categorical data sets, especially for decision trees of intermediate size. Funding: C. Michini gratefully acknowledges funding from the DoD, Air Force [Award FA9550-23-1-0487].},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2023.0017},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {61-82},
  shortjournal = {INFORMS J. Optim.},
  title        = {A polyhedral study of multivariate decision trees},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonconvex factorization and manifold formulations are almost
equivalent in low-rank matrix optimization. <em>IJOO</em>,
<em>7</em>(1), 38–60. (<a
href="https://doi.org/10.1287/ijoo.2022.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the geometric landscape connection of the widely studied manifold and factorization formulations in low-rank positive semidefinite (PSD) and general matrix optimization. We establish a sandwich relation on the spectrum of Riemannian and Euclidean Hessians at first-order stationary points (FOSPs). As a result of that, we obtain an equivalence on the set of FOSPs, second-order stationary points, and strict saddles between the manifold and factorization formulations. In addition, we show that the sandwich relation can be used to transfer more quantitative geometric properties from one formulation to another. Similarities and differences in the landscape connection under the PSD case and the general case are discussed. To the best of our knowledge, this is the first geometric landscape connection between the manifold and factorization formulations for handling rank constraints, and it provides a geometric explanation for the similar empirical performance of factorization and manifold approaches in low-rank matrix optimization observed in the literature. In the general low-rank matrix optimization, the landscape connection of two factorization formulations (unregularized and regularized ones) is also provided. By applying these geometric landscape connections (in particular, the sandwich relation), we are able to solve unanswered questions in the literature and establish stronger results in the applications on geometric analysis of phase retrieval, well-conditioned low-rank matrix optimization, and the role of regularization in factorization arising from machine learning and signal processing. Funding: This work was supported by the National Key R&amp;D Program of China [Grants 2020YFA0711900 and 2020YFA0711901], the National Natural Science Foundation of China [Grants 12271107 and 62141407], and the Shanghai Science and Technology Program [Grant 21JC1400600]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2022.0030 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0030},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {38-60},
  shortjournal = {INFORMS J. Optim.},
  title        = {Nonconvex factorization and manifold formulations are almost equivalent in low-rank matrix optimization},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function design for improved competitive ratio in online
resource allocation with procurement costs. <em>IJOO</em>,
<em>7</em>(1), 20–37. (<a
href="https://doi.org/10.1287/ijoo.2021.0012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of online resource allocation, where customers arrive sequentially, and the seller must irrevocably allocate resources to each incoming customer while also facing a prespecified procurement cost function over the total allocation. The objective is to maximize the reward obtained from fulfilling the customers’ requests sans the cumulative procurement cost. We analyze the competitive ratio of a primal-dual algorithm in this setting and develop an optimization framework for designing a surrogate function for the procurement cost to be used by the algorithm to improve the competitive ratio of the primal-dual algorithm. We use the optimal surrogate function for polynomial procurement cost functions to improve on previous bounds. For general procurement cost functions, our design method uses quasiconvex optimization to find optimal design parameters. We then implement the design techniques and show the improved performance of the algorithm in numerical examples. Finally, we extend the analysis by devising a posted pricing mechanism in which the algorithm does not require the customers’ preferences to be revealed. Funding: M. Fazel’s work was supported in part by the National Science Foundation [Awards 2023166, 2007036, and 1740551]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2021.0012 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0012},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {20-37},
  shortjournal = {INFORMS J. Optim.},
  title        = {Function design for improved competitive ratio in online resource allocation with procurement costs},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal computing budget allocation for data-driven ranking
and selection. <em>IJOO</em>, <em>7</em>(1), 1–19. (<a
href="https://doi.org/10.1287/ijoo.2024.0035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a fixed-budget ranking and selection (R&amp;S) problem, one aims to identify the best design among a finite number of candidates by efficiently allocating the given computing budget to evaluate design performance. Classical methods for R&amp;S usually assume the distribution of the randomness in the system is exactly known. In this paper, we consider the practical scenario where the true distribution is unknown but can be estimated from streaming input data that arrive in batches over time. We formulate the R&amp;S problem in this dynamic setting as a multistage problem where we adopt the Bayesian approach to estimate the distribution, and we formulate a stagewise optimization problem to allocate the computing budget. We characterize the optimality conditions for the stagewise problem by applying the large deviations theory to maximize the decay rate of the probability of false selection. Based on the optimality conditions and combined with the updating of distribution estimates, we design two sequential budget allocation procedures for R&amp;S under streaming input data. We theoretically guarantee the consistency and asymptotic optimality of the proposed procedures. We demonstrate the practical efficiency through numerical experiments in comparison with the equal allocation policy and an extension of the optimal computing budget allocation algorithm. Funding: The authors gratefully acknowledge the support of the Air Force Office of Scientific Research [Grant FA9550-22-1-0244], the National Science Foundation [Grant NSF-DMS2053489], and the NSF AI Institute for Advances in Optimization under [Grant NSF-2112533]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2024.0035 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2024.0035},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {1-19},
  shortjournal = {INFORMS J. Optim.},
  title        = {Optimal computing budget allocation for data-driven ranking and selection},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ited---10">ITED - 10</h2>
<ul>
<li><details>
<summary>
(2025). Case—prescriptive analytics for entrepreneurial growth:
Data-driven strategic decision making at iParty bangkok co., ltd.
<em>ITED</em>, <em>25</em>(2), 175–178. (<a
href="https://doi.org/10.1287/ited.2023.0028cs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0028cs},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {175-178},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Prescriptive analytics for entrepreneurial growth: Data-driven strategic decision making at iParty bangkok co., ltd.},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case article—prescriptive analytics for entrepreneurial
growth: Data-driven strategic decision making at iParty bangkok co.,
ltd. <em>ITED</em>, <em>25</em>(2), 169–174. (<a
href="https://doi.org/10.1287/ited.2023.0028ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case centers around the dilemma faced by the female owner of a balloon decoration business: whether to outsource transportation of balloon arrangements to third-party providers or handle the transportation of some orders in house. The case illustrates a path to answering the strategic business question through solving a sequence of vehicle routing problems (VRPs) of increasing complexity. The focus of the case is not only on solving VRPs, but also on teaching a practical framework for (1) framing a business problem as a prescriptive analytics problem; (2) researching appropriate concepts, prior publications, and tools; (3) setting up an optimization problem formulation, estimating the necessary inputs, and obtaining a solution; (4) deciding on the appropriate level of formulation complexity needed; and (5) mapping the results from the models to a response to the original business question. Realistic data and Excel models and are provided. The case is appropriate for use in courses in optimization, operations research, and business analytics at either the advanced undergraduate or master’s/MBA level. Supplemental Material: The Teaching Note and supplemental data are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0028ca},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {169-174},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Prescriptive analytics for entrepreneurial growth: Data-driven strategic decision making at iParty bangkok co., ltd.},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spreadsheet modeling and wrangling with python.
<em>ITED</em>, <em>25</em>(2), 152–168. (<a
href="https://doi.org/10.1287/ited.2023.0047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A staple of many spreadsheet-based management science courses is the use of Excel for activities such as model building, sensitivity analysis, goal seeking, and Monte-Carlo simulation. What might those things look like if carried out using Python? We describe a teaching module in which Python is used to do typical Excel-based modeling and data-wrangling tasks. In addition, students are exposed to basic software engineering principles, including project folder structures, version control, object-oriented programming, and other more advanced Python skills, creating deployable packages and documentation. The module is supported with Jupyter notebooks, Python scripts, course web pages that include numerous screencasts, and a few GitHub repositories. All of the supporting materials are permissively licensed and freely accessible. Supplemental Material: The supplemental files are available at https://doi.org/10.1287/ited.2023.0047 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0047},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {152-168},
  shortjournal = {Inf. Syst. Res.},
  title        = {Spreadsheet modeling and wrangling with python},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating how social justice framing for assessments
impacts technical learning. <em>ITED</em>, <em>25</em>(2), 136–151. (<a
href="https://doi.org/10.1287/ited.2022.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple studies call for engineering education to integrate social justice into classroom instruction. Yet, there is uncertainty regarding whether integrating these social topics into engineering curriculum will support or detract from the learning of technical concepts. This study focuses on evaluating how reframing technical assessments to include social justice concepts impacts student learning and investigates how well students integrate social justice into engineering decision making. Using a within-subject design, in which students were exposed to both conditions (questions with and without social justice context), we evaluate how social justice framing impacts overall student learning of technical topics. Social justice prompts are added to homework questions, and we assess students’ demonstration of knowledge of original technical content of the course, as well as their ability to consider social justice implications of engineering design. In the earlier homework assignment, the experimental group showed a significant decrease in learning when technical concepts were framed to include social justice. As the students became more familiar with social justice considerations, their learning of technical concepts became comparable to that of students who did not have the social justice components in their assignment. Their evaluation of the social implications of technical decisions also improved. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on DEI in ORMS Classrooms. Funding: This work was supported by the Carnegie Mellon University’s Wimmer Faculty Fellowship and the National Science Foundation [Grant 2053856]. D. Nock also acknowledges support from the Wilton E. Scott Institute for Energy Innovation, where she is an energy fellow. Supplemental Material: The online appendices are available at https://doi.org/10.1287/ited.2022.0030 .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0030},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {136-151},
  shortjournal = {Inf. Syst. Res.},
  title        = {Investigating how social justice framing for assessments impacts technical learning},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—racial bias in automated traffic law enforcement and
the price of unjustness. <em>ITED</em>, <em>25</em>(2), 128–135. (<a
href="https://doi.org/10.1287/ited.2023.0032cs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case study has been developed for students to practice their data analysis and optimization skills in a contemporary societal issue: that of injustice in automated traffic law enforcement. Specifically, this case study is for students of modern data analysis and statistical modeling courses that focus on hypothesis testing; it also has a component for students in optimization and mathematical modeling courses that focus on linear and network optimization. The case study has been used since Spring 2023 in a combination of two courses from the Industrial Engineering (Analysis of Data, an introduction to probability and statistics) and Civil Engineering (Transportation Systems, an introduction to mathematical modeling and optimization for civil engineers with a focus on transportation) curricula. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on DEI in ORMS Classrooms.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0032cs},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {128-135},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Racial bias in automated traffic law enforcement and the price of unjustness},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case article—racial bias in automated traffic law
enforcement and the price of unjustness. <em>ITED</em>, <em>25</em>(2),
122–127. (<a href="https://doi.org/10.1287/ited.2023.0032ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case study has been developed for students to practice their data analysis and optimization skills in a contemporary societal issue: that of injustice in automated traffic law enforcement. Specifically, this case study is for students of modern data analysis and statistical modeling courses that focus on hypothesis testing; it also has a component for students in optimization and mathematical modeling courses that focus on linear and network optimization. The case study has been used since Spring 2023 in a combination of two courses from the Industrial Engineering (Analysis of Data, an introduction to probability and statistics) and Civil Engineering (Transportation Systems, an introduction to mathematical modeling and optimization for civil engineers with a focus on transportation) curricula. History: This paper has been accepted for the INFORMS Transactions on Education Special Issue on DEI in ORMS Classrooms. Supplemental Material: The Teaching Note and data files are available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2023.0032ca},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {122-127},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Racial bias in automated traffic law enforcement and the price of unjustness},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How has the COVID-19 pandemic affected students’ online
social presence? <em>ITED</em>, <em>25</em>(2), 106–121. (<a
href="https://doi.org/10.1287/ited.2022.0054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus disease 2019 pandemic has affected higher education institutions worldwide as they had to switch from face-to-face to online teaching almost overnight. This abrupt change made a huge impact on teaching, learning, and particularly, student engagement. This paper focuses on online social presence as an element of student engagement, which represents how students feel under synchronous online teaching. A survey was conducted among 244 first-year students to evaluate the impact of online social interaction, online collaboration, online contact with staff, online engagement, and online active learning on online social presence. Structural equation modeling was used to test and evaluate these multivariate relationships. Our study illustrates that all variables have a significant positive relationship with online social presence. In particular, online social interaction and online collaboration show a more powerful relationship with student online social presence. Thus, digital technologies should be adopted in a way that encourages students to actively interact with their peers.},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0054},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {106-121},
  shortjournal = {Inf. Syst. Res.},
  title        = {How has the COVID-19 pandemic affected students’ online social presence?},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case—locating a truck terminal in texas. <em>ITED</em>,
<em>25</em>(2), 99–105. (<a
href="https://doi.org/10.1287/ited.2022.0042cs">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0042cs},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {99-105},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case—Locating a truck terminal in texas},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Case article—locating a truck terminal in texas.
<em>ITED</em>, <em>25</em>(2), 90–98. (<a
href="https://doi.org/10.1287/ited.2022.0042ca">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This case study introduces students to the decision-making process of a facility location problem and requires them to apply appropriate methods to recommend a desirable terminal location for CBT, a transportation provider in the United States. In response to the increasing number of drivers in the Texas market, CBT explored different options to better serve the rising demand, including a new and second terminal in a different geographic area. Students employ multi-criteria decision-making framework and optimization modeling to support their analysis using empirical data. While students use provided data to solve the problem in the base case, they need to research various publicly available databases in the case extension. This case can be used in undergraduate courses in logistics, transportation, operations, and supply chain management to apply facility location related concepts, or in a business analytics course to illustrate optimization modeling in a real-world context. Supplemental Material: Data are available at https://doi.org/10.1287/ited.2022.0042ca . The Teaching Note is available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials .},
  archive      = {J_ITED},
  doi          = {10.1287/ited.2022.0042ca},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {90-98},
  shortjournal = {Inf. Syst. Res.},
  title        = {Case Article—Locating a truck terminal in texas},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call for papers—INFORMS transactions on education special
issue on generative AI in teaching management science, operations
research, operations management, and analytics. <em>ITED</em>,
<em>25</em>(2), 89. (<a
href="https://doi.org/10.1287/ited.2024.cfp.v25.n2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ITED},
  doi          = {10.1287/ited.2024.cfp.v25.n2},
  journal      = {INFORMS Transactions on Education},
  month        = {1},
  number       = {2},
  pages        = {89},
  shortjournal = {Inf. Syst. Res.},
  title        = {Call for Papers—INFORMS transactions on education special issue on generative AI in teaching management science, operations research, operations management, and analytics},
  volume       = {25},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="mksc---14">MKSC - 14</h2>
<ul>
<li><details>
<summary>
(2025). Focus on authors. <em>MKSC</em>, <em>44</em>(1), 243–246.
(<a href="https://doi.org/10.1287/mksc.2025.focusonaus.v44.1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2025.focusonaus.v44.1},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {243-246},
  shortjournal = {Market. Sci.},
  title        = {Focus on authors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder: “Consumer uncertainty and purchase decision
reversals: Theory and evidence.” <em>MKSC</em>, <em>44</em>(1), 242. (<a
href="https://doi.org/10.1287/mksc.2024.1022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research found that providing uncertainty-reducing information can increase product returns or service cancellations when judgments are reference-dependent and losses loom larger than gains. This paper revisits the analytical proof to demonstrate that the results hold when an implicit assumption is made explicit. History: Olivier Toubia served as the senior editor for this article.},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.1022},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {242},
  shortjournal = {Market. Sci.},
  title        = {Rejoinder: “Consumer uncertainty and purchase decision reversals: theory and evidence”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Erratum on “purchase decision reversals” model by shulman et
al. (2015). <em>MKSC</em>, <em>44</em>(1), 240–241. (<a
href="https://doi.org/10.1287/mksc.2024.0837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shulman et al. (2015) [Consumer uncertainty and purchase decision reversals: Theory and evidence. Marketing Sci . 34(4): 590–605.] provide theory and evidence that uncertainty-reducing information provided before the purchase decision can actually increase the number of decision reversals. In its current form, readers may get the impression that the main result of the model holds for all distributions of a relevant parameter in the model. However, we show that the main result may not hold for all distributions of the parameter. We then provide two potential resolutions that can serve to preserve the main result. History: Olivier Toubia served as the senior editor for this article. Funding: C. Li’s work was supported by the National Natural Science Foundation of China [Grant 72072051].},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2024.0837},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {240-241},
  shortjournal = {Market. Sci.},
  title        = {Erratum on “Purchase decision reversals” model by shulman et al. (2015)},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling behavioral dynamics in digital content consumption:
An attention-based neural point process approach with applications in
video games. <em>MKSC</em>, <em>44</em>(1), 220–239. (<a
href="https://doi.org/10.1287/mksc.2020.0180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consumption of digital content products (e.g., video games and live streaming) is often associated with multifaceted, dynamically interacting consumer behavior that is subject to influence from pertinent external events. Inspired by these characteristics, we develop a novel attention-based neural point process approach to holistically capture the richness and complexity of consumer behavioral dynamics in modern digital content consumption. Our model features a new multirepresentational, continuous-time attention mechanism that can flexibly model dynamic interactions between different types of behavior under external influence. Using learned representations as sufficient statistics of past events, we build a marked point process to efficiently characterize the occurrence time, behavior combination, and consumption quantity of consumers’ future activities. We illustrate our model development and applications in the empirical context of a sports video game, showing its superior predictive performance over a wide range of baseline methods. Leveraging individual-level parameter estimates, we further demonstrate our model’s utility for conducting segmentation analysis and evaluating the effects of past events on consumers’ future engagement. Our model provides managers and practitioners with a powerful tool for developing more effective and targeted marketing strategies and gaining insights into consumer behavioral dynamics in digital content consumption. History: Yuxin Chen served as the senior editor. Funding: J. Yin was partly supported by the Adobe Digital Experience Research Award and the Amazon AWS Machine Learning Research Award. Y. (K.) Feng was supported by the Research Grants Council of Hong Kong [ECS Grant 25508819]. Y. Liu gratefully acknowledges the support from Bob Eckert, chairman of the board at Levi Struss and former CEO and chairman at Mattel, through the Robert A. Eckert endowed chair in marketing at the University of Arizona. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2020.0180 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2020.0180},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {220-239},
  shortjournal = {Market. Sci.},
  title        = {Modeling behavioral dynamics in digital content consumption: An attention-based neural point process approach with applications in video games},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theory-based explainable deep learning architecture for
music emotion. <em>MKSC</em>, <em>44</em>(1), 196–219. (<a
href="https://doi.org/10.1287/mksc.2022.0323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a theory-based, explainable deep learning convolutional neural network (CNN) classifier to predict the time-varying emotional response to music. We design novel CNN filters that leverage the frequency harmonics structure from acoustic physics known to impact the perception of musical features. Our theory-based model is more parsimonious, but it provides comparable predictive performance with atheoretical deep learning models while performing better than models using handcrafted features. Our model can be complemented with handcrafted features, but the performance improvement is marginal. Importantly, the harmonics-based structure placed on the CNN filters provides better explainability for how the model predicts emotional response (valence and arousal) because emotion is closely related to consonance—a perceptual feature defined by the alignment of harmonics. Finally, we illustrate the utility of our model with an application involving digital advertising. Motivated by YouTube’s midroll ads, we conduct a laboratory experiment in which we exogenously insert ads at different times within videos. We find that ads placed in emotionally similar contexts increase ad engagement (lower skip rates and higher brand recall rates). Ad insertion based on emotional similarity metrics predicted by our theory-based, explainable model produces comparable or better engagement relative to atheoretical models. History: Tat Chan served as the senior editor. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0323 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0323},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {196-219},
  shortjournal = {Market. Sci.},
  title        = {A theory-based explainable deep learning architecture for music emotion},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online causal inference for advertising in real-time bidding
auctions. <em>MKSC</em>, <em>44</em>(1), 176–195. (<a
href="https://doi.org/10.1287/mksc.2022.0406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time bidding systems, which utilize auctions to allocate user impressions to competing advertisers, continue to enjoy success in digital advertising. Assessing the effectiveness of such advertising remains a challenge in research and practice. This paper proposes a new approach to perform causal inference on advertising bought through such mechanisms. Leveraging the economic structure of first- and second-price auctions, we establish novel results that show how the effects of advertising are connected to and, hence, identified from optimal bids. Importantly, we also outline the precise conditions under which these relationships hold. Because these optimal bids are required to estimate the effects of advertising, we present an adapted Thompson Sampling algorithm to solve a multiarmed bandit problem that succeeds in recovering such bids and, consequently, the effects of advertising, while minimizing the costs of experimentation. We also show that a greedy variant of this algorithm can perform just as well, if not better, when exploiting the structure of the model we consider. We use data from real-time bidding auctions to show that it outperforms commonly used methods to estimate the effects of advertising. History: Olivier Toubia served as the senior editor for this article. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0406 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0406},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {176-195},
  shortjournal = {Market. Sci.},
  title        = {Online causal inference for advertising in real-time bidding auctions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The interactions of customer reviews and price and their
dual roles in conveying quality information. <em>MKSC</em>,
<em>44</em>(1), 155–175. (<a
href="https://doi.org/10.1287/mksc.2022.0380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer reviews help communicate product information, but their effectiveness may suffer from selection bias (i.e., depending on factors, such as the individual experience and price, not all consumers may voluntarily write reviews). Consequently, a seller may have to resort to additional means (e.g., signaling through price in the context of an experience good) to convey its quality. This paper develops an analytical model to investigate the interaction of customer reviews and price with the presence of selection bias in marketing an experience good with uncertain quality to consumers. Our analysis reveals the dual roles played by both customer reviews and price in communicating quality information. On one hand, customer reviews may either directly convey product information with unbiased distribution of reviews or facilitate price signaling when reviews are biased because of selection. On the other hand, price may be adjusted to mitigate the selection bias of reviews to make them more informative, and it may also signal quality directly in the presence of review bias. As a result, we show that bias in reviews may actually benefit consumers without compromising information communication as the incentive to reduce review selection bias makes it credible and profitable for the high-quality seller to signal its type by undercutting the price that would be set if it is of low quality. We then extend our analysis to examine the information, profits, and welfare impacts of several important design elements of a review system as well as the impact of consumers’ aversion to risk. Finally, the implications of our findings on the management of user-generated content and pricing are discussed. History: Anthony Dukes served as the senior editor. Funding: J. Du is grateful for financial support from the Research Grants Council (RGC) of Hong Kong [Grant GRF/17501021]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2022.0380 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0380},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {155-175},
  shortjournal = {Market. Sci.},
  title        = {The interactions of customer reviews and price and their dual roles in conveying quality information},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call me maybe: Does customer feedback seeking impact
nonsolicited customers? <em>MKSC</em>, <em>44</em>(1), 129–154. (<a
href="https://doi.org/10.1287/mksc.2023.0324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Academics and practitioners acknowledge the value of customer feedback in improving firm performance. Companies routinely solicit feedback from different customer subsets. However, the extent to which this feedback impacts nonsolicited customers depends on whether firms implement meaningful business-level changes that resonate with customers. This paper assesses customer feedback’s impact on firm learning and business improvements as well as its spillover effects on nonsolicited customers using a randomized, controlled field experiment conducted in Rwanda over two years. We hypothesize that private feedback seeking could operate through two broad mechanisms: (a) directly influencing solicited customers and/or (b) prompting firms to improve their offerings, leading to spillover effects on other customers. Our results demonstrate a 38.2% increase in recall and a 77.4% increase in purchases for customers not engaged in the feedback process. The analysis further suggests that business-level changes driven by customer feedback fuel these spillovers. Additionally, customer feedback seeking significantly improves treatment firm performance, resulting in a 62.0% revenue increase and 54.5% profit increase compared with control firms. Our study also introduces a basic customer feedback-seeking technology for small businesses to improve performance. These findings can guide firms in leveraging customer feedback to undertake business changes and generate greater revenues/profits. History: Olivier Toubia served as the senior editor for this article. Funding: This research was supported by grants from the Stanford King Center on Global Development, the Polsky Center for Entrepreneurship and Innovation (Chicago Booth), the Rustandy Center for Social Sector Innovation (Chicago Booth), the London School of Economics and Political Science, the Stanford Graduate School of Business, the John A. and Cynthia Fry Gunn Faculty Scholar award (Stanford), the UK Department for International Development (DFID) and Economic and Social Research Council’s (ESRC) joint Growth Research Program, Private Enterprise Development in Low-Income Countries (PEDL), and the Fama-Miller Center for Research in Finance. Further support is acknowledged from the Chicago Booth Kilts Center for Marketing and the Leonard L. Berry Chair in Services Marketing at Mays Business School. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0324 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0324},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {129-154},
  shortjournal = {Market. Sci.},
  title        = {Call me maybe: Does customer feedback seeking impact nonsolicited customers?},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating parameters of structural models using neural
networks. <em>MKSC</em>, <em>44</em>(1), 102–128. (<a
href="https://doi.org/10.1287/mksc.2022.0360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io . History: Manchanda Puneet served as the senior editor. Supplemental Material: The data files are available at https://doi.org/10.1287/mksc.2022.0360 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0360},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {102-128},
  shortjournal = {Market. Sci.},
  title        = {Estimating parameters of structural models using neural networks},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The value of platform endorsement. <em>MKSC</em>,
<em>44</em>(1), 84–101. (<a
href="https://doi.org/10.1287/mksc.2022.0226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many digital platforms with large product assortments endorse a select group of items to facilitate user choice. However, although it seems intuitive that such endorsement may increase the sales of endorsed items, little is known about its effect on unendorsed items and on the platform. Using data from a field experiment conducted by an online freelance platform, we examine the effect of exposure to platform endorsement on user search and purchase behavior. We find that exposure to platform endorsement increases user search and purchases not only for endorsed services but also, for unendorsed services. We link the increase in search and purchases to an increase in the perception of the quality of services offered on the platform. We further explore heterogeneity in the effect of platform endorsement and find that the effect of exposure to platform endorsement on purchase is more pronounced for users with a higher propensity to purchase. We discuss implications for platforms, merchants, and regulators. History: Olivier Toubia served as the senior editor. Funding: The authors acknowledge the support of London Business School’s Research and Materials Development Fund for this research. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2022.0226 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0226},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {84-101},
  shortjournal = {Market. Sci.},
  title        = {The value of platform endorsement},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymmetric impact of matching technology on influencer
marketing: Implications for platform revenue. <em>MKSC</em>,
<em>44</em>(1), 65–83. (<a
href="https://doi.org/10.1287/mksc.2023.0211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the impact of using advanced technology such as artificial intelligence (AI) to match marketers with social media influencers. We develop a theoretical model to examine how matching accuracy affects the competition between influencers and the profitability of a social media platform. Our findings show that improving matching accuracy may not always benefit the platform, especially for platforms with intermediate follower density. Two opposing effects of technology improvement affect the prices of influencer marketing campaigns: advanced technology, such as AI, enhances the matching between influencers and marketers and also intensifies competition between different types of influencers. The overall effect on prices can be negative for some influencers because of the asymmetric nature of such matching technology: the matching outcome for influencers with a narrower audience (niche influencers) is more sensitive to matching accuracy than that for those with a broader audience (general influencers). As a result, more niche influencers begin to participate in marketing campaigns when matching accuracy improves, which reduces the prices offered by sufficiently general influencers and may lead to a decline in platform revenue. Additionally, we find that adjusting commission rates in response to technology improvements could help mitigate the negative impact although it may not eliminate it entirely. Our findings offer valuable insights for social media platforms seeking to remain competitive in the influencer marketing landscape. History: Anthony Dukes served as the senior editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.0211 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0211},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {65-83},
  shortjournal = {Market. Sci.},
  title        = {Asymmetric impact of matching technology on influencer marketing: Implications for platform revenue},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What drives demand for playlists on spotify? <em>MKSC</em>,
<em>44</em>(1), 54–64. (<a
href="https://doi.org/10.1287/mksc.2022.0273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide estimates of the drivers of playlist followers on Spotify. We base our analysis on a unique panel data set for 30,000+ popular playlists and combine it with data on how prominently these playlists are featured in the Spotify app. Using two-way fixed effects and staggered synthetic difference in difference models, we compare the short-term effect of two important demand factors in our data—featuring playlists on Spotify’s Search Page and adding songs by exceptionally popular major label artists to playlists. We find that users prefer to follow playlists featured in the app. According to our estimates, being featured on the Search Page raises daily playlist followers by 0.95%—which is about two times larger than the effect on followers of including a song by an exceptionally popular major label artist (0.45%). Our examination of playlist demand has two important implications. First, Spotify can effectively guide user attention to certain playlists, supporting industry executives’ and artists’ concerns that the platform has the potential to favor some producers by selectively promoting their content. Second, popular artists signed with major labels play an important role in attracting followers to playlists on Spotify. History: Catherine Tucker served as the senior editor for this article. Funding: This work was supported by the Dutch Research Council [NWO 451-17-028] and the Tilburg Science Hub. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2022.0273 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0273},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {54-64},
  shortjournal = {Market. Sci.},
  title        = {What drives demand for playlists on spotify?},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can AI and AI-hybrids detect persuasion skills? Salesforce
hiring with conversational video interviews. <em>MKSC</em>,
<em>44</em>(1), 30–53. (<a
href="https://doi.org/10.1287/mksc.2023.0149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an AI and AI-human-based model for salesforce hiring using recordings of conversational video interviews that involve two-sided, back-and-forth interactions with messages conveyed through multiple modalities (text, voice, and body language). We derive objective, theory-backed measures of sales performance from these modalities, leveraging recent advances in body language analysis, conversational analysis, and large language models (LLMs). These measures serve as explanatory variables in our AI model. Our key contribution to the broader research on persuasion and influence is that we show how to use conversational videos to capture features related to (i) two-way conversational interactivity; (ii) real-time adaptation; and (iii) human body language, with minimal measurement error relative to extant survey-based approaches that suffer from recall biases. We use rubric-based scores by panels of sales professionals (correlated with hiring decisions) to isolate a candidate’s “latent sales ability” and use these as outcome variables to be predicted by the AI model. The AI model achieves reasonable predictive accuracy, yet incorporating human judgments into an AI-human hybrid model enhances its effectiveness—improving workforce quality by 67% over random selection. Although the content of what is spoken is most important in prediction, conversational interactivity, sellers’ real-time adaptation to the buyer, and body language also have good explanatory power. Finally, in terms of performance-cost trade-offs, the addition of just one human professional evaluation in the hiring loop in combination with AI is optimal. Further, using human input based on only the two early stages of the interview in a task-based hybrid model is the most cost-effective in improving performance. History: Tat Chan served as the senior editor. Funding: K. Chiong and H. Dover received financial support for this work from the NEC Foundation of America. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0149 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2023.0149},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {30-53},
  shortjournal = {Market. Sci.},
  title        = {Can AI and AI-hybrids detect persuasion skills? salesforce hiring with conversational video interviews},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommending for a multi-sided marketplace: A
multi-objective hierarchical approach. <em>MKSC</em>, <em>44</em>(1),
1–29. (<a href="https://doi.org/10.1287/mksc.2022.0238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems play a vital role in driving the long-term values for online platforms. However, developing recommender systems for multi-sided platforms faces two prominent challenges. First, recommending for multi-sided platforms typically involves a joint optimization of multiple, potentially conflicting objectives. Second, many platforms adopt hierarchical homepages, where items can either be individual products or groups of products. Off-the-shelf recommendation algorithms are not applicable in these settings. To address these challenges, we propose MOHR, a novel multi-objective hierarchical recommender. By combining machine learning, probabilistic hierarchical aggregation, and multi-objective optimization, MOHR efficiently solves the multi-objective ranking problem in a hierarchical setting through an innovative formulation of probabilistic consumer behavior modeling and constrained optimization. We implemented MOHR at Uber Eats, one of the world’s largest food delivery platforms. Online experiments showed significant improvements in consumer conversion, retention, and gross bookings, resulting in a $1.5 million weekly increase in revenue. Moreover, MOHR offers managers a mathematically principled tool to make quantifiable and interpretable trade-offs across multiple objectives. As a result, it has been deployed globally as the recommender system for Uber Eats’ app homepage. History: Puneet Manchanda served as the senior editor for this article. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mksc.2022.0238 .},
  archive      = {J_MKSC},
  doi          = {10.1287/mksc.2022.0238},
  journal      = {Marketing Science},
  month        = {1-2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Market. Sci.},
  title        = {Recommending for a multi-sided marketplace: A multi-objective hierarchical approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="moor---28">MOOR - 28</h2>
<ul>
<li><details>
<summary>
(2025). Steiner cut dominants. <em>MOOR</em>, <em>50</em>(1),
764–781. (<a href="https://doi.org/10.1287/moor.2022.0280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a subset T of nodes of an undirected graph G , a T-Steiner cut is a cut δ ( S ) with T ∩ S ≠ ø and T \ S ≠ ø . The T-Steiner cut dominant of G is the dominant CUT + ( G , T ) of the convex hull of the incidence vectors of the T -Steiner cuts of G . For T = { s , t } , this is the well-understood s - t -cut dominant. Choosing T as the set of all nodes of G , we obtain the cut dominant for which an outer description in the space of the original variables is still not known. We prove that for each integer τ , there is a finite set of inequalities such that for every pair ( G , T ) with | T | ≤ τ , the nontrivial facet-defining inequalities of CUT + ( G , T ) are the inequalities that can be obtained via iterated applications of two simple operations, starting from that set. In particular, the absolute values of the coefficients and of the right-hand sides in a description of CUT + ( G , T ) by integral inequalities can be bounded from above by a function of | T | . For all | T | ≤ 5 , we provide descriptions of CUT + ( G , T ) by facet-defining inequalities, extending the known descriptions of s - t -cut dominants.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0280},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {764-781},
  shortjournal = {Math. Oper. Res.},
  title        = {Steiner cut dominants},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniqueness of convex-ranged probabilities and applications
to risk measures and games. <em>MOOR</em>, <em>50</em>(1), 743–763. (<a
href="https://doi.org/10.1287/moor.2023.0015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit Marinacci’s uniqueness theorem for convex-ranged probabilities and its applications. Our approach does away with both the countable additivity and the positivity of the charges involved. In the process, we uncover several new equivalent conditions, which lead to a novel set of applications. These include extensions of the classic Fréchet–Hoeffding bounds as well as of the automatic Fatou property of law-invariant functionals. We also generalize existing results of the “collapse to the mean”-type concerning capacities and α -MEU preferences.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0015},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {743-763},
  shortjournal = {Math. Oper. Res.},
  title        = {Uniqueness of convex-ranged probabilities and applications to risk measures and games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-path large deviations for unbounded additive
functionals of the reflected random walk. <em>MOOR</em>, <em>50</em>(1),
711–742. (<a href="https://doi.org/10.1287/moor.2020.0094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove a sample-path large deviation principle (LDP) with sublinear speed for unbounded functionals of certain Markov chains induced by the Lindley recursion. The LDP holds in the Skorokhod space D [ 0 , 1 ] equipped with the M 1 ′ topology. Our technique hinges on a suitable decomposition of the Markov chain in terms of regeneration cycles. Each regeneration cycle denotes the area accumulated during the busy period of the reflected random walk. We prove a large deviation principle for the area under the busy period of the Markov random walk, and we show that it exhibits a heavy-tailed behavior. Funding: The research of B. Zwart and M. Bazhba is supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek [Grant 639.033.413]. The research of J. Blanchet is supported by the National Science Foundation (NSF) [Grants 1915967, 1820942, and 1838576] as well as the Defense Advanced Research Projects Agency [Grant N660011824028]. The research of C.-H. Rhee is supported by the NSF [Grant CMMI-2146530].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2020.0094},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {711-742},
  shortjournal = {Math. Oper. Res.},
  title        = {Sample-path large deviations for unbounded additive functionals of the reflected random walk},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact characterization of the jointly optimal restocking and
auditing policy in inventory systems with record inaccuracy.
<em>MOOR</em>, <em>50</em>(1), 656–710. (<a
href="https://doi.org/10.1287/moor.2022.0145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a continuous-time stochastic model of an inventory system with record inaccuracy. In this formulation, demand is modeled by a point process and is observable only when it leads to sales. In addition to demand that can reduce the stock, an unobservable stochastic loss process can also reduce the stock. The retailer’s goal is to identify the restocking and auditing policy that minimizes the expected discounted cost of carrying a product over an infinite horizon. We analytically characterize the optimal restocking and jointly optimal auditing policy. We prove that the optimal restocking policy is a threshold policy. Our proof of this result is based on a coupling argument that is valid for any demand and loss model. Unlike the optimal restocking policy, the jointly optimal auditing policy is not of threshold type. We show that a complete proof of this statement cannot be obtained by solely resorting to the first-order stochastic dominance property of the Bayesian shelf stock distribution induced by the demand and loss process. Instead, our characterization of the jointly optimal auditing policy is based on proving that the dynamics of the shelf stock distribution constitute a (strictly) sign-regular kernel. To our knowledge, this is the first paper that characterizes the optimal policy of a complex control problem by establishing sign regularity of its underlying Markovian dynamics. Our theoretical results lead to a fast algorithm for computing the exact jointly optimal auditing/restocking policy over the problem’s entire state space. This enables comparative statics analysis, which allows us to determine how inventory record inaccuracy affects the economic significance of various cost drivers. This, in turn, allows us to determine when or, better, under what conditions auditing can be an effective tool for reducing the total cost.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0145},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {656-710},
  shortjournal = {Math. Oper. Res.},
  title        = {Exact characterization of the jointly optimal restocking and auditing policy in inventory systems with record inaccuracy},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast rates for the regret of offline reinforcement learning.
<em>MOOR</em>, <em>50</em>(1), 633–655. (<a
href="https://doi.org/10.1287/moor.2021.0167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the regret of offline reinforcement learning in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted Q -iteration (FQI), suggest root- n convergence for regret, empirical behavior exhibits much faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function, the regret of the policy it defines converges at a rate given by the exponentiation of the estimate’s pointwise convergence rate, thus speeding up the rate. The level of exponentiation depends on the level of noise in the decision-making problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman residual minimization to establish the correct pointwise convergence guarantees. As specific cases, our results imply one-over- n rates in linear cases and exponential-in- n rates in tabular cases. We extend our findings to general function approximation by extending our results to regret guarantees based on L p -convergence rates for estimating the optimal quality function rather than pointwise rates, where L 2 guarantees for nonparametric estimation can be ensured under mild conditions. Funding: This work was supported by the Division of Information and Intelligent Systems, National Science Foundation [Grant 1846210].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0167},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {633-655},
  shortjournal = {Math. Oper. Res.},
  title        = {Fast rates for the regret of offline reinforcement learning},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal investment strategy for α-robust utility
maximization problem. <em>MOOR</em>, <em>50</em>(1), 606–632. (<a
href="https://doi.org/10.1287/moor.2023.0076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reality, investors are uncertain about the dynamics of risky asset returns. Therefore, investors prefer to make robust investment decisions. In this paper, we propose an α-robust utility maximization problem under uncertain parameters. The investor is allowed to invest in a financial market consisting of a risk-free asset and a risky asset. The uncertainty about the expected return rate is parameterized by a nonempty set. Different from most existing literature on robust utility maximization problems where investors are generally assumed to be extremely ambiguity averse because they tend to consider only expected utility in the worst-case scenario, we pay attention to the investors who are not only ambiguity averse but also ambiguity seeking. Under power utility, we provide the implicit function representations for the precommitted strategy, equilibrium strategy of the open-loop type, and equilibrium strategy of the closed-loop type. Some properties about the optimal trading strategies, the best-case and worst-case parameters under three different kinds of strategies, are provided. Funding: This work was supported by National Natural Science Foundation of China [Grants 12071147, 12171169, 12271171, 12371470, 71721001, 71931004, 72371256], the Shanghai Philosophy Social Science Planning Office Project [Grant 2022ZJB005], Fundamental Research Funds for the Central Universities [Grant 2022QKT001], the Excellent Young Team Project Natural Science Foundation of Guangdong Province of China [Grant 2023B1515040001], the Philosophy and Social Science Programming Foundation of Guangdong Province [Grant GD22CYJ17], the Nature Science Foundation of Guangdong Province of China [Grant 2022A1515011472], and the 111 Project [Grant B14019].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0076},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {606-632},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal investment strategy for α-robust utility maximization problem},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel langevin pathwise average for gibbs
approximation. <em>MOOR</em>, <em>50</em>(1), 573–605. (<a
href="https://doi.org/10.1287/moor.2021.0243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a new multilevel method for the numerical approximation of a Gibbs distribution π on R d , based on (overdamped) Langevin diffusions. This method relies on a multilevel occupation measure, that is, on an appropriate combination of R occupation measures of (constant-step) Euler schemes with respective steps γ r = γ 0 2 − r , r = 0 , … , R . We first state a quantitative result under general assumptions that guarantees an ε-approximation (in an L 2 -sense) with a cost of the order ε − 2 or ε − 2 | log ε | 3 under less contractive assumptions. We then apply it to overdamped Langevin diffusions with strongly convex potential U : R d → R and obtain an ε-complexity of the order O ( d ε − 2 log 3 ( d ε − 2 ) ) or O ( d ε − 2 ) under additional assumptions on U . More precisely, up to universal constants, an appropriate choice of the parameters leads to a cost controlled by ( λ ¯ U ∨ 1 ) 2 λ ¯ U − 3 d ε − 2 (where λ ¯ U and λ ¯ U respectively denote the supremum and the infimum of the largest and lowest eigenvalue of D 2 U ). We finally complete these theoretical results with some numerical illustrations, including comparisons to other algorithms in Bayesian learning and opening to the non–strongly convex setting. Funding: The authors are grateful to the SIRIC ILIAD Nantes-Angers program, supported by the French National Cancer Institute [INCA-DGOS-Inserm Grant 12558].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0243},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {573-605},
  shortjournal = {Math. Oper. Res.},
  title        = {Multilevel langevin pathwise average for gibbs approximation},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semidefinite approximations for bicliques and bi-independent
pairs. <em>MOOR</em>, <em>50</em>(1), 537–572. (<a
href="https://doi.org/10.1287/moor.2023.0046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate some graph parameters dealing with bi-independent pairs ( A , B ) in a bipartite graph G = ( V 1 ∪ V 2 , E ) , that is, pairs ( A , B ) where A ⊆ V 1 , B ⊆ V 2 , and A ∪ B are independent. These parameters also allow us to study bicliques in general graphs. When maximizing the cardinality | A ∪ B | , one finds the stability number α ( G ) , well-known to be polynomial-time computable. When maximizing the product | A | · | B | , one finds the parameter g ( G ), shown to be NP-hard by Peeters in 2003, and when maximizing the ratio | A | · | B | / | A ∪ B | , one finds h ( G ), introduced by Vallentin in 2020 for bounding product-free sets in finite groups. We show that h ( G ) is an NP-hard parameter and, as a crucial ingredient, that it is NP-complete to decide whether a bipartite graph G has a balanced maximum independent set. These hardness results motivate introducing semidefinite programming (SDP) bounds for g ( G ), h ( G ), and α bal ( G ) (the maximum cardinality of a balanced independent set). We show that these bounds can be seen as natural variations of the Lovász ϑ-number, a well-known semidefinite bound on α ( G ) . In addition, we formulate closed-form eigenvalue bounds, and we show relationships among them as well as with earlier spectral parameters by Hoffman and Haemers in 2001 and Vallentin in 2020. Funding: This work was supported by H2020 Marie Skłodowska-Curie Actions [Grant 813211 (POEMA)].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0046},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {537-572},
  shortjournal = {Math. Oper. Res.},
  title        = {Semidefinite approximations for bicliques and bi-independent pairs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-field multiagent reinforcement learning: A
decentralized network approach. <em>MOOR</em>, <em>50</em>(1), 506–536.
(<a href="https://doi.org/10.1287/moor.2022.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges for multiagent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial information of the entire system. Whereas exciting progress has been made to analyze decentralized MARL with the network of agents for social networks and team video games, little is known theoretically for decentralized MARL with the network of states for modeling self-driving vehicles, ride-sharing, and data and traffic routing. This paper proposes a framework of localized training and decentralized execution to study MARL with the network of states. Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that agents can execute afterward the learned decentralized policies, which depend only on agents’ current states. The theoretical analysis consists of three key components: the first is the reformulation of the MARL system as a networked Markov decision process with teams of agents, enabling updating the associated team Q-function in a localized fashion; the second is the Bellman equation for the value function and the appropriate Q-function on the probability measure space; and the third is the exponential decay property of the team Q-function, facilitating its approximation with efficient sample efficiency and controllable error. The theoretical analysis paves the way for a new algorithm LTDE-N eural -AC, in which the actor–critic approach with overparameterized neural networks is proposed. The convergence and sample complexity are established and shown to be scalable with respect to the sizes of both agents and states. To the best of our knowledge, this is the first neural network–based MARL algorithm with network structure and provable convergence guarantee. Funding: X. Wei is partially supported by NSFC no. 12201343. R. Xu is partially supported by the NSF CAREER award DMS-2339240.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0055},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {506-536},
  shortjournal = {Math. Oper. Res.},
  title        = {Mean-field multiagent reinforcement learning: A decentralized network approach},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Marginal values of a stochastic game. <em>MOOR</em>,
<em>50</em>(1), 482–505. (<a
href="https://doi.org/10.1287/moor.2023.0297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-sum stochastic games are parameterized by payoffs, transitions, and possibly a discount rate. In this article, we study how the main solution concepts, the discounted and undiscounted values, vary when these parameters are perturbed. We focus on the marginal values, introduced by Mills in 1956 in the context of matrix games—that is, the directional derivatives of the value along any fixed perturbation. We provide a formula for the marginal values of a discounted stochastic game. Further, under mild assumptions on the perturbation, we provide a formula for their limit as the discount rate vanishes and for the marginal values of an undiscounted stochastic game. We also show, via an example, that the two latter differ in general. Funding: This work was supported by Fondation CFM pour la Recherche; the European Research Council [Grant ERC-CoG-863818 (ForM-SMArt)]; and Agence Nationale de la Recherche [Grant ANR-21-CE40-0020].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0297},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {482-505},
  shortjournal = {Math. Oper. Res.},
  title        = {Marginal values of a stochastic game},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence and stability of coupled belief-strategy
learning dynamics in continuous games. <em>MOOR</em>, <em>50</em>(1),
459–481. (<a href="https://doi.org/10.1287/moor.2022.0161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning dynamics to model how strategic agents repeatedly play a continuous game while relying on an information platform to learn an unknown payoff-relevant parameter. In each time step, the platform updates a belief estimate of the parameter based on players’ strategies and realized payoffs using Bayes’ rule. Then, players adopt a generic learning rule to adjust their strategies based on the updated belief. We present results on the convergence of beliefs and strategies and the properties of convergent fixed points of the dynamics. We obtain sufficient and necessary conditions for the existence of globally stable fixed points. We also provide sufficient conditions for the local stability of fixed points. These results provide an approach to analyzing the long-term outcomes that arise from the interplay between Bayesian belief learning and strategy learning in games and enable us to characterize conditions under which learning leads to a complete information equilibrium. Funding: Financial support from the Air Force Office of Scientific Research [Project Building Attack Resilience into Complex Networks], the Simons Institute [research fellowship], and a Michael Hammer Fellowship is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0161},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {459-481},
  shortjournal = {Math. Oper. Res.},
  title        = {Convergence and stability of coupled belief-strategy learning dynamics in continuous games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A policy gradient algorithm for the risk-sensitive
exponential cost MDP. <em>MOOR</em>, <em>50</em>(1), 431–458. (<a
href="https://doi.org/10.1287/moor.2022.0139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the risk-sensitive exponential cost Markov decision process (MDP) formulation and develop a trajectory-based gradient algorithm to find the stationary point of the cost associated with a set of parameterized policies. We derive a formula that can be used to compute the policy gradient from (state, action, cost) information collected from sample paths of the MDP for each fixed parameterized policy. Unlike the traditional average cost problem, standard stochastic approximation theory cannot be used to exploit this formula. To address the issue, we introduce a truncated and smooth version of the risk-sensitive cost and show that this new cost criterion can be used to approximate the risk-sensitive cost and its gradient uniformly under some mild assumptions. We then develop a trajectory-based gradient algorithm to minimize the smooth truncated estimation of the risk-sensitive cost and derive conditions under which a sequence of truncations can be used to solve the original, untruncated cost problem. Funding: This work was supported by the Office of Naval Research Global [Grant N0001419-1-2566], the Division of Computer and Network Systems [Grant 21-06801], the Army Research Office [Grant W911NF-19-1-0379], and the Division of Computing and Communication Foundations [Grants 17-04970 and 19-34986].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0139},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {431-458},
  shortjournal = {Math. Oper. Res.},
  title        = {A policy gradient algorithm for the risk-sensitive exponential cost MDP},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric semidefinite programming: Geometry of the
trajectory of solutions. <em>MOOR</em>, <em>50</em>(1), 410–430. (<a
href="https://doi.org/10.1287/moor.2021.0097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, solutions of convex optimization problems are updated on-line, as functions of time. In this paper, we consider parametric semidefinite programs, which are linear optimization problems in the semidefinite cone whose coefficients (input data) depend on a time parameter . We are interested in the geometry of the solution (output data) trajectory, defined as the set of solutions depending on the parameter . We propose an exhaustive description of the geometry of the solution trajectory. As our main result, we show that only six distinct behaviors can be observed at a neighborhood of a given point along the solution trajectory. Each possible behavior is then illustrated by an example. Funding: This work was supported by OP RDE [Grant CZ.02.1.01/0.0/0.0/16_019/0000765].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0097},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {410-430},
  shortjournal = {Math. Oper. Res.},
  title        = {Parametric semidefinite programming: Geometry of the trajectory of solutions},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the (im-)possibility of representing probability
distributions as a difference of i.i.d. Noise terms. <em>MOOR</em>,
<em>50</em>(1), 390–409. (<a
href="https://doi.org/10.1287/moor.2023.0081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A random variable is difference-form decomposable ( DFD ) if it may be written as the difference of two i.i.d. random terms. We show that densities of such variables exhibit a remarkable degree of structure. Specifically, a DFD density can be neither approximately uniform, nor quasiconvex, nor strictly concave. On the other hand, a DFD density need, in general, be neither unimodal nor logconcave. Regarding smoothness, we show that a compactly supported DFD density cannot be analytic and will often exhibit a kink even if its components are smooth. The analysis highlights the risks for model consistency resulting from the strategy widely adopted in the economics literature of imposing assumptions directly on a difference of noise terms rather than on its components.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0081},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {390-409},
  shortjournal = {Math. Oper. Res.},
  title        = {On the (Im-)Possibility of representing probability distributions as a difference of I.I.D. noise terms},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal consumption and investment with independent
stochastic labor income. <em>MOOR</em>, <em>50</em>(1), 356–389. (<a
href="https://doi.org/10.1287/moor.2023.0119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new dynamic continuous-time model of optimal consumption and investment to include independent stochastic labor income. We reduce the problem of solving the Bellman equation to a problem of solving an integral equation. We then explicitly characterize the optimal consumption and investment strategy as a function of income-to-wealth ratio. We provide some analytical comparative statics associated with the value function and optimal strategies. We also develop a quite general numerical algorithm for control iteration and solve the Bellman equation as a sequence of solutions to ordinary differential equations. This numerical algorithm can be readily applied to many other optimal consumption and investment problems especially with extra nondiversifiable Brownian risks, resulting in nonlinear Bellman equations. Finally, our numerical analysis illustrates how the presence of stochastic labor income affects the optimal consumption and investment strategy. Funding: A. Bensoussan was supported by the National Science Foundation under grant [DMS-2204795]. S. Park was supported by the Ministry of Education of the Republic of Korea and the National Research Foundation of Korea, South Korea [NRF-2022S1A3A2A02089950].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0119},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {356-389},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal consumption and investment with independent stochastic labor income},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strongly convergent homogeneous approximations to
inhomogeneous markov jump processes and applications. <em>MOOR</em>,
<em>50</em>(1), 334–355. (<a
href="https://doi.org/10.1287/moor.2022.0153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of time-inhomogeneous Markov jump processes is a traditional topic within probability theory that has recently attracted substantial attention in various applications. However, their flexibility also incurs a substantial mathematical burden which is usually circumvented by using well-known generic distributional approximations or simulations. This article provides a novel approximation method that tailors the dynamics of a time-homogeneous Markov jump process to meet those of its time-inhomogeneous counterpart on an increasingly fine Poisson grid. Strong convergence of the processes in terms of the Skorokhod J 1 metric is established, and convergence rates are provided. Under traditional regularity assumptions, distributional convergence is established for unconditional proxies, to the same limit. Special attention is devoted to the case where the target process has one absorbing state and the remaining ones transient, for which the absorption times also converge. Some applications are outlined, such as univariate hazard-rate density estimation, ruin probabilities, and multivariate phase-type density evaluation. Funding: M. Bladt and O. Peralta would like to acknowledge financial support from the Swiss National Science Foundation Project 200021_191984. O. Peralta acknowledges financial support from NSF Award #1653354 and AXA Research Fund Award on “Mitigating risk in the wake of the pandemic”.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0153},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {334-355},
  shortjournal = {Math. Oper. Res.},
  title        = {Strongly convergent homogeneous approximations to inhomogeneous markov jump processes and applications},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk sharing with lambda value at risk. <em>MOOR</em>,
<em>50</em>(1), 313–333. (<a
href="https://doi.org/10.1287/moor.2023.0246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the risk-sharing problem among multiple agents using lambda value at risk ( Λ VaR ) as their preferences via the tool of inf-convolution, where Λ VaR is an extension of value at risk ( VaR ). We obtain explicit formulas of the inf-convolution of multiple Λ VaR with monotone Λ and explicit forms of the corresponding optimal allocations, extending the results of the inf-convolution of VaR . It turns out that the inf-convolution of several Λ VaR is still a Λ VaR under some mild condition. Moreover, we investigate the inf-convolution of one Λ VaR and a general monotone risk measure without cash additivity, including Λ VaR , expected utility, and rank-dependent expected utility as special cases. The expression of the inf-convolution and the explicit forms of the optimal allocation are derived, leading to some partial solution of the risk-sharing problem with multiple Λ VaR for general Λ functions. Finally, we discuss the risk-sharing problem with Λ VaR + , another definition of lambda value at risk. We focus on the inf-convolution of Λ VaR + and a risk measure that is consistent with the second-order stochastic dominance, deriving very different expression of the inf-convolution and the forms of the optimal allocations.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0246},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {313-333},
  shortjournal = {Math. Oper. Res.},
  title        = {Risk sharing with lambda value at risk},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational inequalities on unbounded domains for zero-sum
singular controller vs. Stopper games. <em>MOOR</em>, <em>50</em>(1),
277–312. (<a href="https://doi.org/10.1287/moor.2023.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of zero-sum games between a singular controller and a stopper over a finite-time horizon. The underlying process is a multidimensional (locally nondegenerate) controlled stochastic differential equation (SDE) evolving in an unbounded domain. We prove that such games admit a value and provide an optimal strategy for the stopper. The value of the game is shown to be the maximal solution in a suitable Sobolev class of a variational inequality of min-max type with an obstacle constraint and a gradient constraint. Although the variational inequality and the game are solved on an unbounded domain, we do not require boundedness of either the coefficients of the controlled SDE or of the cost functions in the game. Funding: A. Bovo was partially supported by the Doctoral Studentship from the University of Leeds.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0029},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {277-312},
  shortjournal = {Math. Oper. Res.},
  title        = {Variational inequalities on unbounded domains for zero-sum singular controller vs. stopper games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A characterization of simultaneous optimization,
majorization, and (bi-)submodular polyhedra. <em>MOOR</em>,
<em>50</em>(1), 252–276. (<a
href="https://doi.org/10.1287/moor.2023.0054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by resource allocation problems (RAPs) in power management applications, we investigate the existence of solutions to optimization problems that simultaneously minimize the class of Schur-convex functions, also called least-majorized elements. For this, we introduce a generalization of majorization and least-majorized elements, called ( a , b )-majorization and least ( a , b )-majorized elements, and characterize the feasible sets of problems that have such elements in terms of base and (bi-)submodular polyhedra. Hereby, we also obtain new characterizations of these polyhedra that extend classical characterizations in terms of optimal greedy algorithms from the 1970s. We discuss the implications of our results for RAPs in power management applications and derive a new characterization of convex cooperative games and new properties of optimal estimators of specific regularized regression problems. In general, our results highlight the combinatorial nature of simultaneously optimizing solutions and provide a theoretical explanation for why such solutions generally do not exist.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0054},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {252-276},
  shortjournal = {Math. Oper. Res.},
  title        = {A characterization of simultaneous optimization, majorization, and (Bi-)Submodular polyhedra},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stationary points of a shallow neural network with quadratic
activations and the global optimality of the gradient descent algorithm.
<em>MOOR</em>, <em>50</em>(1), 209–251. (<a
href="https://doi.org/10.1287/moor.2021.0082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of training a shallow neural network with quadratic activation functions and the generalization power of such trained networks. Assuming that the samples are generated by a full rank matrix W * of the hidden network node weights, we obtain the following results. We establish that all full-rank approximately stationary solutions of the risk minimization problem are also approximate global optimums of the risk (in-sample and population). As a consequence, we establish that, when trained on polynomially many samples, the gradient descent algorithm converges to the global optimum of the risk minimization problem regardless of the width of the network when it is initialized at some value ν * , which we compute. Furthermore, the network produced by the gradient descent has a near zero generalization error. Next, we establish that initializing the gradient descent algorithm below ν * is easily achieved when the weights of the ground truth matrix W * are randomly generated and the matrix is sufficiently overparameterized. Finally, we identify a simple necessary and sufficient geometric condition on the size of the training set under which any global minimizer of the empirical risk has necessarily zero generalization error. Funding: The research of E. C. Kizildag is supported by Columbia University, with the Distinguished Postdoctoral Fellowship in Statistics. Support from the National Science Foundation [Grant DMS-2015517] is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0082},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {209-251},
  shortjournal = {Math. Oper. Res.},
  title        = {Stationary points of a shallow neural network with quadratic activations and the global optimality of the gradient descent algorithm},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Submodular functions and perfect graphs. <em>MOOR</em>,
<em>50</em>(1), 189–208. (<a
href="https://doi.org/10.1287/moor.2021.0302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a combinatorial polynomial-time algorithm to find a maximum weight independent set in perfect graphs of bounded degree that do not contain a prism or a hole of length four as an induced subgraph. An even pair in a graph is a pair of vertices all induced paths between which are even. An even set is a set of vertices every two of which are an even pair. We show that every perfect graph that does not contain a prism or a hole of length four as an induced subgraph has a balanced separator which is the union of a bounded number of even sets, where the bound depends only on the maximum degree of the graph. This allows us to solve the maximum weight independent set problem using the well-known submodular function minimization algorithm. Funding: This work was supported by the Engineering and Physical Sciences Research Council [Grant EP/V002813/1]; the National Science Foundation [Grants DMS-1763817, DMS-2120644, and DMS-2303251]; and Alexander von Humboldt-Stiftung.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0302},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {189-208},
  shortjournal = {Math. Oper. Res.},
  title        = {Submodular functions and perfect graphs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluctuation theory of continuous-time, skip-free downward
markov chains with applications to branching processes with immigration.
<em>MOOR</em>, <em>50</em>(1), 169–188. (<a
href="https://doi.org/10.1287/moor.2022.0246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a comprehensive methodology for the fluctuation theory of continuous-time, skip-free Markov chains, extending and improving the recent work of Choi and Patie for discrete-time, skip-free Markov chains. As a significant application, we use it to derive a full set of fluctuation identities regarding exiting a finite or infinite interval for Markov branching processes with immigration, thereby uncovering many new results for this classic family of continuous-time Markov chains. The theory also allows us to recover in a simple manner fluctuation identities for skip-free downward compound Poisson processes.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0246},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {169-188},
  shortjournal = {Math. Oper. Res.},
  title        = {Fluctuation theory of continuous-time, skip-free downward markov chains with applications to branching processes with immigration},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternating and parallel proximal gradient methods for
nonsmooth, nonconvex minimax: A unified convergence analysis.
<em>MOOR</em>, <em>50</em>(1), 141–168. (<a
href="https://doi.org/10.1287/moor.2022.0294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in nonconvex minimax problems that is driven by an abundance of applications. Our focus is on nonsmooth, nonconvex-strongly concave minimax, thus departing from the more common weakly convex and smooth models assumed in the recent literature. We present proximal gradient schemes with either parallel or alternating steps. We show that both methods can be analyzed through a single scheme within a unified analysis that relies on expanding a general convergence mechanism used for analyzing nonconvex, nonsmooth optimization problems. In contrast to the current literature, which focuses on the complexity of obtaining nearly approximate stationary solutions, we prove subsequence convergence to a critical point of the primal objective and global convergence when the latter is semialgebraic. Furthermore, the complexity results we provide are with respect to approximate stationary solutions. Lastly, we expand the scope of problems that can be addressed by generalizing one of the steps with a Bregman proximal gradient update, and together with a few adjustments to the analysis, this allows us to extend the convergence and complexity results to this broader setting. Funding: The research of E. Cohen was partially supported by a doctoral fellowship from the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240]. The research of M. Teboulle was partially supported by the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0294},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {141-168},
  shortjournal = {Math. Oper. Res.},
  title        = {Alternating and parallel proximal gradient methods for nonsmooth, nonconvex minimax: A unified convergence analysis},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling in the high-uncertainty heavy traffic regime.
<em>MOOR</em>, <em>50</em>(1), 107–140. (<a
href="https://doi.org/10.1287/moor.2022.0100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model uncertainty approach to heavy traffic asymptotics that allows for a high level of uncertainty. That is, the uncertainty classes of underlying distributions accommodate disturbances that are of order 1 at the usual diffusion scale as opposed to asymptotically vanishing disturbances studied previously in relation to heavy traffic. A main advantage of the approach is that the invariance principle underlying diffusion limits makes it possible to define uncertainty classes in terms of the first two moments only. The model we consider is a single-server queue with multiple job types. The problem is formulated as a zero sum stochastic game played between the system controller, who determines scheduling and attempts to minimize an expected linear holding cost, and an adversary, who dynamically controls the service time distributions of arriving jobs and attempts to maximize the cost. The heavy traffic asymptotics of the game are fully solved. It is shown that an asymptotically optimal policy for the system controller is to prioritize according to an index rule, and for the adversary, it is to select distributions based on the system’s current workload. The workload-to-distribution feedback mapping is determined by a Hamilton–Jacobi–Bellman equation, which also characterizes the game’s limit value. Unlike in the vast majority of results in the heavy traffic theory and as a direct consequence of the diffusive size disturbances, the limiting dynamics under asymptotically optimal play are captured by a stochastic differential equation where both the drift and the diffusion coefficients may be discontinuous. Funding: R. Atar is supported by the Israeli Science Foundation [Grant 1035/20].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0100},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {107-140},
  shortjournal = {Math. Oper. Res.},
  title        = {Scheduling in the high-uncertainty heavy traffic regime},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial voting rules. <em>MOOR</em>, <em>50</em>(1),
90–106. (<a href="https://doi.org/10.1287/moor.2023.0080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a new class of polynomial voting rules for a general decentralized decision/consensus system, and more specifically for the proof-of-stake protocol. The main idea, inspired by the Penrose square-root law and the more recent quadratic voting rule, is to differentiate a voter’s voting power and the voter’s share (fraction of the total in the system). We show that, whereas voter shares form a martingale process that converges to a Dirichlet distribution, their voting powers follow a supermartingale process that decays to zero over time. This prevents any voter from controlling the voting process and, thus, enhances security. For both limiting results, we also provide explicit rates of convergence. When the initial total volume of votes (or stakes) is large, we show a phase transition in share stability (or the lack thereof), corresponding to the voter’s initial share relative to the total. We also study the scenario in which trading (of votes/stakes) among the voters is allowed and quantify the level of risk sensitivity (or risk aversion) in three categories, corresponding to the voter’s utility being a supermartingale, a submartingale, and a martingale. For each category, we identify the voter’s best strategy in terms of participation and trading. Funding: W. Tang gratefully acknowledges financial support through the National Science Foundation [Grants DMS-2113779 and DMS-2206038] and through a start-up grant at Columbia University. D. D. Yao’s work is part of a Columbia–City University/Hong Kong collaborative project that is supported by InnoHK Initiative, the Government of Hong Kong Special Administrative Region, and the Laboratory for AI-Powered Financial Technologies.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0080},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {90-106},
  shortjournal = {Math. Oper. Res.},
  title        = {Polynomial voting rules},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow allocation games. <em>MOOR</em>, <em>50</em>(1), 68–89.
(<a href="https://doi.org/10.1287/moor.2022.0355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a game-theoretic variant of the maximum circulation problem. In a flow allocation game , we are given a directed flow network. Each node is a rational agent and can strategically allocate any incoming flow to the outgoing edges. Given the strategy choices of all agents, a maximal circulation that adheres to the chosen allocation strategies evolves in the network. Each agent wants to maximize the amount of flow through his or her node. Flow allocation games can be used to express strategic incentives of clearing in financial networks. We provide a cumulative set of results on the existence and computational complexity of pure Nash and strong equilibria as well as tight bounds on the (strong) prices of anarchy and stability. Our results show an interesting dichotomy. Ranking strategies over individual flow units allows us to obtain optimal strong equilibria for many objective functions. In contrast, more intuitive ranking strategies over edges can give rise to unfavorable incentive properties. Funding: This work was supported by Deutsche Forschungsgemeinschaft Research Group ADYN [411362735].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0355},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {68-89},
  shortjournal = {Math. Oper. Res.},
  title        = {Flow allocation games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimal problem dependent generalization error
bounds in statistical learning theory. <em>MOOR</em>, <em>50</em>(1),
40–67. (<a href="https://doi.org/10.1287/moor.2021.0076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study problem-dependent rates, that is, generalization errors that scale near-optimally with the variance, effective loss, or gradient norms evaluated at the “best hypothesis.” We introduce a principled framework dubbed “uniform localized convergence” and characterize sharp problem-dependent rates for central statistical learning problems. From a methodological viewpoint, our framework resolves several fundamental limitations of existing uniform convergence and localization analysis approaches. It also provides improvements and some level of unification in the study of localized complexities, one-sided uniform inequalities, and sample-based iterative algorithms. In the so-called “slow rate” regime, we provide the first (moment-penalized) estimator that achieves the optimal variance-dependent rate for general “rich” classes; we also establish an improved loss-dependent rate for standard empirical risk minimization. In the “fast rate” regime, we establish finite-sample, problem-dependent bounds that are comparable to precise asymptotics. In addition, we show that iterative algorithms such as gradient descent and first order expectation maximization can achieve optimal generalization error in several representative problems across the areas of nonconvex learning, stochastic optimization, and learning with missing data. Supplemental Material: The online appendix is available at https://doi.org/10.1287/moor.2021.0076 .},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0076},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {40-67},
  shortjournal = {Math. Oper. Res.},
  title        = {Towards optimal problem dependent generalization error bounds in statistical learning theory},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The online saddle point problem and online convex
optimization with knapsacks. <em>MOOR</em>, <em>50</em>(1), 1–39. (<a
href="https://doi.org/10.1287/moor.2018.0330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online saddle point problem, an online learning problem where at each iteration, a pair of actions needs to be chosen without knowledge of the current and future (convex-concave) payoff functions. The objective is to minimize the gap between the cumulative payoffs and the saddle point value of the aggregate payoff function, which we measure using a metric called saddle point regret (SP-Regret). The problem generalizes the online convex optimization framework, but here, we must ensure that both players incur cumulative payoffs close to that of the Nash equilibrium of the sum of the games. We propose an algorithm that achieves SP-Regret proportional to ln ( T ) T in the general case, and log ( T ) SP-Regret for the strongly convex-concave case. We also consider the special case where the payoff functions are bilinear and the decision sets are the probability simplex. In this setting, we are able to design algorithms that reduce the bounds on SP-Regret from a linear dependence in the dimension of the problem to a logarithmic one. We also study the problem under bandit feedback and provide an algorithm that achieves sublinear SP-Regret. We then consider an online convex optimization with knapsacks problem motivated by a wide variety of applications, such as dynamic pricing, auctions, and crowdsourcing. We relate this problem to the online saddle point problem and establish O ( T ) regret using a primal-dual algorithm.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2018.0330},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Math. Oper. Res.},
  title        = {The online saddle point problem and online convex optimization with knapsacks},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="ms---41">MS - 41</h2>
<ul>
<li><details>
<summary>
(2025). Publicly traded debt restructuring methods, corporate
investment, and debt contracting. <em>MS</em>, <em>71</em>(2),
1846–1863. (<a href="https://doi.org/10.1287/mnsc.2022.01831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use an important ruling, Marblegate Asset Management v. Education Management Corporation (EMC), to study the economic role of two publicly traded debt restructuring methods: coercive bond exchange offers and Chapter 11. This ruling restricted firms from employing coercive bond exchange offers to facilitate out-of-court restructurings, thereby increasing the likelihood of restructuring publicly traded debt under Chapter 11. Following the ruling, investment in affected distressed firms decreased substantially, but investment efficiency improved. The changes in covenant, maturity, and offering yield of newly issued bonds suggested that existing bondholders with covenants gained more bargaining power than shareholders and new bondholders. The paper provides causal evidence from a large sample analysis, demonstrating the divergent effects of these two publicly traded debt restructuring methods on investment policies. This paper was accepted by Victoria Ivashina, finance. Funding: Funding for this research was provided by University of Pittsburgh (Doctoral Fellowship) and Vanderbilt University (standard research funds). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01831 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01831},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1846-1863},
  shortjournal = {Manag. Sci.},
  title        = {Publicly traded debt restructuring methods, corporate investment, and debt contracting},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The early exercise risk premium. <em>MS</em>,
<em>71</em>(2), 1824–1845. (<a
href="https://doi.org/10.1287/mnsc.2023.00440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the asset pricing implications of being able to optimally early exercise plain vanilla puts, contrasting expected raw and delta-hedged returns across equivalent American and European puts. Our theory suggests that American puts yield less negative raw but more negative delta-hedged expected returns than equivalent European puts. The raw (delta-hedged) spread widens with a higher early exercise probability as induced through, for example, moneyness, time to maturity, and underlying asset volatility (variance and jump risk premiums). An empirical comparison of single-stock American puts with equivalent synthetic European puts formed from put–call parity supports our theory if and only if we allow for optimal early exercises in our return calculations. More strikingly, allowing for optimal early exercises significantly alters the profitability of 14 out of 15 well-known option anomalies with the average absolute change equal to 33% and five anomalies becoming insignificant. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The internet appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00440 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00440},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1824-1845},
  shortjournal = {Manag. Sci.},
  title        = {The early exercise risk premium},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk management with variable capital utilization and
time-varying collateral capacity. <em>MS</em>, <em>71</em>(2),
1803–1823. (<a href="https://doi.org/10.1287/mnsc.2022.00415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a risk management model that incorporates variable capital utilization and time-varying collateral capacity. The former lets firms optimally choose capital utilization, and hence production, which affects capital depreciation and risk exposure. The latter means firms’ ability to borrow and hedge increases with expected earnings and thus utilization. Calibrated solutions show both ingredients matter for firm value. We test the novel model predictions using a new data set of oil and gas producers. Consistent with model predictions, we find utilization is negatively correlated with firm liquidity, while hedging is positively correlated with liquidity and expected profitability. This paper was accepted by Lukas Schmid, finance. Funding: Lu and Vij acknowledge financial support from the Terry-Sanford Research Award, University of Georgia. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.00415 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00415},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1803-1823},
  shortjournal = {Manag. Sci.},
  title        = {Risk management with variable capital utilization and time-varying collateral capacity},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-adapting network relaxations for weakly coupled markov
decision processes. <em>MS</em>, <em>71</em>(2), 1779–1802. (<a
href="https://doi.org/10.1287/mnsc.2022.01108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional weakly coupled Markov decision processes (WDPs) arise in dynamic decision making and reinforcement learning, decomposing into smaller Markov decision processes (MDPs) when linking constraints are relaxed. The Lagrangian relaxation of WDPs (LAG) exploits this property to compute policies and (optimistic) bounds efficiently; however, dualizing linking constraints averages away combinatorial information. We introduce feasibility network relaxations (FNRs), a new class of linear programming relaxations that exactly represents the linking constraints. We develop a procedure to obtain the unique minimally sized relaxation, which we refer to as self-adapting FNR, as its size automatically adjusts to the structure of the linking constraints. Our analysis informs model selection: (i) the self-adapting FNR provides (weakly) stronger bounds than LAG, is polynomially sized when linking constraints admit a tractable network representation, and can even be smaller than LAG, and (ii) self-adapting FNR provides bounds and policies that match the approximate linear programming (ALP) approach but is substantially smaller in size than the ALP formulation and a recent alternative Lagrangian that is equivalent to ALP. We perform numerical experiments on constrained dynamic assortment and preemptive maintenance applications. Our results show that self-adapting FNR significantly improves upon LAG in terms of policy performance and/or bounds, while being an order of magnitude faster than an alternative Lagrangian and ALP, which are unsolvable in several instances. This paper was accepted by Baris Ata, stochastic models and simulation. Supplemental Material: The electronic companion and data files are available at https://doi.org/10.1287/mnsc.2022.01108 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01108},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1779-1802},
  shortjournal = {Manag. Sci.},
  title        = {Self-adapting network relaxations for weakly coupled markov decision processes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intangible capital in factor models. <em>MS</em>,
<em>71</em>(2), 1756–1778. (<a
href="https://doi.org/10.1287/mnsc.2022.01261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition from a traditional manufacturing-based economy to a knowledge- and service-based economy over recent decades resulted in a considerable rise in intangible capital, most of which is not reported on companies’ balance sheets. As a result, balance sheet-based valuation ratios, investment measures, and other firm characteristics that do not incorporate off-balance sheet (OBS) intangible capital suffer from significant measurement error problems. We incorporate a new measure of OBS intangible capital into firm characteristics, such as book to market, investment, and profitability, to address these measurement errors. These OBS intangible adjustments improve the performance of the Fama–French three- and five-factor models and the q -factor model, especially during recent decades. We further find that the value factor is no longer redundant in these empirical factor models. This paper was accepted by Lukas Schmid, finance. Funding: The authors are grateful for financial support from the 2019 EDHEC Scientific Beta “Advanced ESG &amp; Factor Investing” Research Chair. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01261 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01261},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1756-1778},
  shortjournal = {Manag. Sci.},
  title        = {Intangible capital in factor models},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A procedure for revising data-based priors in a group.
<em>MS</em>, <em>71</em>(2), 1737–1755. (<a
href="https://doi.org/10.1287/mnsc.2022.02912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A group of individuals is interested in predicting the outcome of a current problem. Each individual has access to private data that are used to form a prior probability over possible outcomes. While individuals may be reluctant or unable to disclose their private data, they are willing to publish their priors. We characterize a procedure for revising the individuals’ prior probabilities based on the published priors of others and discuss its implications regarding the formation of a common prior in the group, both in the short and long run. This paper was accepted by Manel Baucells, behavioral economics and decision analysis. Funding: G. Gayer gratefully acknowledges support from ISF [Grant 1443/20].},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02912},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1737-1755},
  shortjournal = {Manag. Sci.},
  title        = {A procedure for revising data-based priors in a group},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detours in shared rides. <em>MS</em>, <em>71</em>(2),
1716–1736. (<a href="https://doi.org/10.1287/mnsc.2020.03125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detours are considered key for the efficient operation of a shared rides service, but they are also a major pain point for consumers of such services. This paper studies the relationship between the value generated by shared rides and the detours they create for riders. We establish a limit on the sum of value and detour, and we prove that this leads to a tight bound on the Pareto frontier of values and detours in a general setting with an arbitrary number of requests. We explicitly compute the Pareto frontier for one family of city topologies and construct it via simulation for several more networks, including one based on ride-sharing data from commute hours in Manhattan. We find that average detours are usually small, even in low-demand-density settings. We also find that by carefully choosing the match objective, detours can be reduced with a relatively small impact on values and that the density of ride requests is far more important than detours for the effective operations of a shared rides service. In response, we propose that platforms implement a two-product version of shared rides and limit the worst-case detours of its users. This paper was accepted by Hamid Nazerzadeh, data science. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2020.03125 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.03125},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1716-1736},
  shortjournal = {Manag. Sci.},
  title        = {Detours in shared rides},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industry peer information and the equity valuation accuracy
of firms emerging from chapter 11. <em>MS</em>, <em>71</em>(2),
1692–1715. (<a href="https://doi.org/10.1287/mnsc.2022.01233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Valuation plays a central role in determining Chapter 11 reorganization outcomes. However, obtaining accurate valuation estimates of reorganized firms is challenging because of limited firm-specific market-based information and the oft-conflicting incentives of claimholders. We examine the role of industry peer information in reducing misvaluations and its implications for unintended interclaimant wealth transfers and postreorganization performance. First, we find that the availability of relevant industry peer information is negatively associated with equity valuation errors for firms emerging from Chapter 11. Cross-sectional results suggest that the relation between industry peer information and valuation errors varies substantially with debtors’ information environment and case characteristics. Second, we find that industry peer information quality is associated with better ex post financial performance of emerged firms because of lower overvaluation. Finally, we document the role of industry peer information in substantially reducing the frequency and magnitude of unintended wealth transfers between claimants arising from equity valuation errors. This paper was accepted by Suraj Srinivasan, accounting. Funding: The authors appreciate financial support from the Social Sciences and Humanities Research Council of Canada [Grant 435-2020-0583] and the Canadian Academic Accounting Association. B. Fang acknowledges financial support from the Della Suantio Fellowship. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01233 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01233},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1692-1715},
  shortjournal = {Manag. Sci.},
  title        = {Industry peer information and the equity valuation accuracy of firms emerging from chapter 11},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pitfalls of review solicitation: Evidence from a natural
experiment on TripAdvisor. <em>MS</em>, <em>71</em>(2), 1671–1691. (<a
href="https://doi.org/10.1287/mnsc.2023.01006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the effect of firms’ participation in platform-endorsed review solicitation programs on consumers’ online review generation. We leverage a natural experiment on TripAdvisor, which launched a review solicitation program that allows hotels to collect reviews directly from guests after their stays with the aid of certified connectivity partners. Applying a two-stage difference-in-differences approach to a panel data set of online reviews for a matched set of hotels across TripAdvisor and Expedia, we find that hotels’ participation in the review solicitation program results in a 34.3% increase in review volume, a 0.151 increase in review rating, but a 16.9% decrease in review length. Review solicitation, however, generates a notable negative spillover effect on the volume of organic reviews. Specifically, the volume of organic reviews is reduced by 15.5% after hotels start soliciting reviews. We provide evidence that the motivational crowding-out effect plays an important role in driving this negative spillover. Further analyses reveal that the effects of review solicitation are heterogeneous with respect to hotels of different types and consumers with different demographic and behavioral characteristics. Finally, using a novel structural topic model, we detect a significant shift in review content from specific and concrete topics to general and abstract topics. Our findings suggest that review platforms and firms should be cautious about the unintended negative consequences of review solicitation on consumers’ review generation. This paper was accepted by Hemant Bhargava, information systems. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72371192, 72132008, 71872061, and 72061127002] and the Humanities and Social Science Fund of Ministry of Education of China [Grant 22YJA630021]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.01006 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01006},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1671-1691},
  shortjournal = {Manag. Sci.},
  title        = {The pitfalls of review solicitation: Evidence from a natural experiment on TripAdvisor},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tech-enabled financial data access, retail investors, and
gambling-like behavior in the stock market. <em>MS</em>, <em>71</em>(2),
1646–1670. (<a href="https://doi.org/10.1287/mnsc.2021.01379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in technology have reduced information acquisition costs, creating an improved information environment for retail investors. Specifically, new technologies, such as application programming interface (API), deliver high-volume, institutional-like raw data directly to Main Street investors. Although greater availability of information can be beneficial, it may also exacerbate retail investors’ existing trading deficiencies. Exploiting the sudden shutdown of Yahoo! Finance API, the largest free API for retail investors, this study examines how access to tech-enabled raw financial data affects retail investment. We find that retail trading volumes in stocks favored by active retail investors dropped by 8.6%–10.5% within one month of the API shutdown. The remaining retail trades collectively became more predictive of future returns, suggesting less gambling-like behavior after the API shutdown. Moreover, our randomized controlled experiment affirms the underlying mechanism: tech-enabled access to high-volume historical price data increases individuals’ overconfidence, which further leads them to engage in excessive trading. The study reveals an unintended consequence of technology-led, wider data access for retail investors. This paper was accepted by D. J. Wu, information systems. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.01379 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01379},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1646-1670},
  shortjournal = {Manag. Sci.},
  title        = {Tech-enabled financial data access, retail investors, and gambling-like behavior in the stock market},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investor sentiment and the pricing of macro risks for hedge
funds. <em>MS</em>, <em>71</em>(2), 1623–1645. (<a
href="https://doi.org/10.1287/mnsc.2022.02792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hedge funds with larger macroeconomic-risk betas do not earn higher returns, in contrast to the theoretically predicted risk-return trade-off. Meanwhile, high macro-beta funds deliver higher returns than low macro-beta funds following a low-sentiment period, whereas the risk-return relation is flat following a high-sentiment period. We show that the sophisticated management of hedge funds explains this pattern. The relation between funds’ macro-risk betas and the timing abilities/investor flows is sentiment dependent, and such variation likely drives the contrasting beta-return trade-offs after high- and low-sentiment periods. A similar pattern is also observed in mutual funds. This paper was accepted by Lin William Cong, finance. Funding: X. Zhu acknowledges financial support from the National Natural Science Foundation of China [Grant 72203035] and the Ministry of Education Project of Humanities and Social Sciences [Grant 22YJC790194]. Z. Chen acknowledges financial support from the National Natural Science Foundation of China [Grant 72222004] and Tsinghua University [Grant 20225080020]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02792 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02792},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1623-1645},
  shortjournal = {Manag. Sci.},
  title        = {Investor sentiment and the pricing of macro risks for hedge funds},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit and implicit belief-based gender discrimination: A
hiring experiment. <em>MS</em>, <em>71</em>(2), 1600–1622. (<a
href="https://doi.org/10.1287/mnsc.2022.01229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a key element of discrimination, namely, when stereotypes translate into discriminatory actions. Using a hiring experiment, we rule out taste-based discrimination by design and test for the presence of two types of belief-based gender discrimination. We document evidence of explicit discriminators —individuals who are willing to discriminate even when their hiring choices are highly revealing of their gender-biased beliefs. Crucially, we also identify implicit discriminators —individuals who do not discriminate against women when taking a discriminatory action is highly revealing of their biased beliefs, but do discriminate against women when their biased motive is obscured. Our analysis highlights the central role played by features of the choice environment in determining whether and how discrimination will manifest. We conclude by discussing the implications for policy design. This paper was accepted by Marie Claire Villeval, behavioral economics and decision analysis. Funding: K. Barron and S. Schweighofer-Kodritsch gratefully acknowledge financial support from the Deutsche Forschungsgemeinschaft through CRC TRR 190 [Grant 280092119]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01229 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01229},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1600-1622},
  shortjournal = {Manag. Sci.},
  title        = {Explicit and implicit belief-based gender discrimination: A hiring experiment},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinated inattention and disclosure complexity.
<em>MS</em>, <em>71</em>(2), 1581–1599. (<a
href="https://doi.org/10.1287/mnsc.2021.01029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine a beauty contest game with an option to analyze an additional disclosure. We analytically prove that in some scenarios, coordination incentives cause sophisticated players who can comprehend disclosures to choose not to analyze them to match unsophisticated players’ actions, a phenomenon we call “coordinated inattention.” Laboratory experiments provide support for the coordinated inattention mechanism: Coordination incentives reduce sophisticated subjects’ propensity to analyze disclosures, especially when they believe others are unlikely to comprehend them. We further find that psychological biases help reduce coordinated inattention. Subjects are overconfident, sophisticated subjects overestimate others’ ability to comprehend disclosures, and both biases are associated with a higher tendency to analyze disclosures. Our analysis suggests that unsophisticated decision makers’ inability to comprehend complex disclosures has a negative spillover effect by reducing sophisticated decision makers’ attention to disclosures. Our results highlight the importance of the recent efforts of the Securities and Exchange Commission (SEC) and the Financial Accounting Standards Board (FASB) to make disclosures easier to comprehend. This paper was accepted by Brian Bushee, accounting. Funding: This study involved no funding except the payments made to the experimental subjects; these funds were provided by Penn State University. Supplemental Material: The online appendix and electronic companion are available at https://doi.org/10.1287/mnsc.2021.01029 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01029},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1581-1599},
  shortjournal = {Manag. Sci.},
  title        = {Coordinated inattention and disclosure complexity},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free nonstationary reinforcement learning:
Near-optimal regret and applications in multiagent reinforcement
learning and inventory control. <em>MS</em>, <em>71</em>(2), 1564–1580.
(<a href="https://doi.org/10.1287/mnsc.2022.02533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider model-free reinforcement learning (RL) in nonstationary Markov decision processes. Both the reward functions and the state transition functions are allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain variation budgets. We propose Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free algorithm for nonstationary RL, and show that it outperforms existing solutions in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret bound of O ˜ ( S 1 3 A 1 3 Δ 1 3 H T 2 3 ) , where S and A are the numbers of states and actions, respectively, Δ &gt; 0 is the variation budget, H is the number of time steps per episode, and T is the total number of time steps. We further present a parameter-free algorithm named Double-Restart Q-UCB that does not require prior knowledge of the variation budget. We show that our algorithms are nearly optimal by establishing an information-theoretical lower bound of Ω ( S 1 3 A 1 3 Δ 1 3 H 2 3 T 2 3 ) , the first lower bound in nonstationary RL. Numerical experiments validate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency. We demonstrate the power of our results in examples of multiagent RL and inventory control across related products. This paper was accepted by Omar Besbes, revenue management and market analytics. Funding: The research of D. Simchi-Levi and R. Zhu was supported by the MIT Data Science Laboratory. The research of W. Mao, K. Zhang, and T. Başar was supported in part by the U.S. Army Research Laboratory (ARL) Cooperative Agreement W911NF-17-2-0196, in part by the Office of Naval Research (ONR) [MURI Grant N00014-16-1-2710], and in part by the Air Force Office of Scientific Research (AFOSR) [Grant FA9550-19-1-0353]. K. Zhang also acknowledges support from U.S. Army Research Laboratory (ARL) [Grant W911NF-24-1-0085]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02533 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02533},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1564-1580},
  shortjournal = {Manag. Sci.},
  title        = {Model-free nonstationary reinforcement learning: Near-optimal regret and applications in multiagent reinforcement learning and inventory control},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The algorithmic assignment of incentive schemes.
<em>MS</em>, <em>71</em>(2), 1546–1563. (<a
href="https://doi.org/10.1287/mnsc.2022.03362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assignment of individuals with different observable characteristics to different treatments is a central question in designing optimal policies. We study this question in the context of increasing workers’ performance via targeted incentives using machine learning algorithms with worker demographics, personality traits, and preferences as input. Running two large-scale experiments, we show that (i) performance can be predicted by accurately measured worker characteristics, (ii) a machine learning algorithm can detect heterogeneity in responses to different schemes, (iii) a targeted assignment of schemes to individuals increases performance significantly above the level of the single best scheme, and (iv) algorithmic assignment is more effective for workers who have a high likelihood to repeatedly interact with the employer or who provide more consistent survey answers. This paper was accepted by Yan Chen, behavioral economics and decision analysis. Funding: Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy [Grant EXC 2126/1-390838866]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03362 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.03362},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1546-1563},
  shortjournal = {Manag. Sci.},
  title        = {The algorithmic assignment of incentive schemes},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Employment protection and venture capital investment: The
impact of wrongful discharge laws. <em>MS</em>, <em>71</em>(2),
1523–1545. (<a href="https://doi.org/10.1287/mnsc.2023.01936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wrongful discharge laws (WDLs) provide limits to the employment-at-will doctrine, and thus impair operating flexibility, increasing expected financial distress costs by making it costly to fire employees. This impairment is detrimental to start-ups, leading to a decline in venture capital (VC) investment. Using a difference-in-differences framework enabled by the staggered adoption of WDLs across the U.S. states, we show VC investment declines after a state adopts the good faith exception (the strongest form of WDL). This decline is most pronounced in sectors with high labor dependency. This paper was accepted by Victoria Ivashina, finance. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2023.01936 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01936},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1523-1545},
  shortjournal = {Manag. Sci.},
  title        = {Employment protection and venture capital investment: The impact of wrongful discharge laws},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using machine learning to measure conservatism. <em>MS</em>,
<em>71</em>(2), 1504–1522. (<a
href="https://doi.org/10.1287/mnsc.2024.4983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an approach to measure conservatism using machine learning techniques that are not constrained by functional form restrictions. We extend the differential timeliness model to allow for observable characteristics related to conservatism to follow nonlinear relationships. By developing machine learning measures of conservatism, we draw attention to potential benefits and drawbacks and show how its insights complement conventional measures. Our broader goal is to investigate the effectiveness of machine learning algorithms for filtering noise in traditional archival studies and uncovering more complex empirical patterns. This paper was accepted by Suraj Srinivasan, accounting. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.4983 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.4983},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1504-1522},
  shortjournal = {Manag. Sci.},
  title        = {Using machine learning to measure conservatism},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dueling contests and platform’s coordinating role.
<em>MS</em>, <em>71</em>(2), 1488–1503. (<a
href="https://doi.org/10.1287/mnsc.2021.03973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing platforms typically take a passive approach, and they let the competing firms freely design their own contests and allow every solver to self-select and join any of the concurrently running contests. In a model of competing noise-driven contests, we show that the duopoly prize allocation has fewer (but larger) prizes compared with a monopolist contest designer. We also find that contests with firm-chosen budgets and solvers’ endogenous participation create coordination inefficiencies. Thus, platform policies that constrain the competing firms from freely choosing their budgets and offer solvers non-enforceable recommendations toward specific noise-driven contests strictly enhance total welfare. Extending our framework to include arbitrarily correlated ability-driven contests, we highlight the critical role of inter-contest dependence on the efficacy of a platform’s interventions. Specifically, platform nudges to improve solver-contest (mis)matches are welfare enhancing only when the contests are sufficiently related, and allowing solvers to self-sort is appropriate otherwise. This paper was accepted by Gabriel Weintraub, revenue management and market analytics. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2021.03973 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03973},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1488-1503},
  shortjournal = {Manag. Sci.},
  title        = {Dueling contests and platform’s coordinating role},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Congressional apportionment: A multiobjective optimization
approach. <em>MS</em>, <em>71</em>(2), 1464–1487. (<a
href="https://doi.org/10.1287/mnsc.2023.02472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two events, with major implications for U.S. voters, occur after each decennial census. First, congressional “apportionment” takes place, followed by congressional “districting.” Apportionment determines how to allocate the 435 seats in the House of Representatives across the 50 states, whereas districting determines the geographic boundaries assigned to representatives within each state. Although districting and the practice of gerrymandering often receive great attention in the media and courts, the best way to apportion representatives across states has been debated for nearly 250 years. Historical methods (including the current method) each satisfy some desirable optimality criteria that the others are not guaranteed to satisfy. Moreover, none are guaranteed to optimize certain reasonable fairness measures (e.g., minimum range, minimum bias). To our knowledge, we are the first to formulate and analyze a multiobjective optimization approach to apportionment, allowing policymakers to identify Pareto-optimal allocations and quantify their trade-offs between several competing criteria. Some of these models can be formulated and solved as mixed-integer linear programs, whereas others require the solution of mixed-integer, nonconvex, quadratically constrained quadratic programs. We take advantage of recent software advances that allow one to solve these problems with optimality guarantees. Policy implications of our work include Pareto curves from historical censuses and simulations, which suggest opportunities for improvement in some objectives at little sacrifice to others. This paper was accepted by David Simchi-Levi, operations management. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.02472 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.02472},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1464-1487},
  shortjournal = {Manag. Sci.},
  title        = {Congressional apportionment: A multiobjective optimization approach},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target firm advertising and firm value. <em>MS</em>,
<em>71</em>(2), 1438–1463. (<a
href="https://doi.org/10.1287/mnsc.2022.01534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistent with hypotheses underlying firm advertising, we find that targets with pretakeover advertising obtain higher premiums, whereas their acquirers earn lower announcement returns. These economically significant effects suggest that through advertising, targets increase their profile and negotiating power. Further, targets that advertise are more likely to initiate their takeovers, attract multiple bidders, receive enhanced bids, capture more merger rents, and even in failed acquisitions, experience a 1% permanent revaluation. The latter result differentiates between information asymmetry and behavioral explanations for the target advertising. Overall, the results support the hypothesis that management advertises to transmit information to investors and potential acquirers. This paper was accepted by Victoria Ivashina, finance. Funding: A. L. Tran acknowledges financial support from the Mergers and Acquisitions Research Centre at Bayes Business School. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01534 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01534},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1438-1463},
  shortjournal = {Manag. Sci.},
  title        = {Target firm advertising and firm value},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentivizing organ donation under different priority rules:
The role of information. <em>MS</em>, <em>71</em>(2), 1418–1437. (<a
href="https://doi.org/10.1287/mnsc.2022.01530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the incentive to register for deceased organ donation under alternative organ allocation priority rules, which may prioritize registered donors and/or patients with higher valuations for organ transplantation. Specifically, the donor priority rule grants higher priority on the organ waiting list to those who have previously registered as donors. The dual-incentive priority rules allocate organs based on donor status, followed by individual valuations within the same donor status, or vice versa. Both theoretical and experimental results suggest that the efficacy of the donor priority rule and the dual-incentive priority rules critically depends on the information environment. When organ transplantation valuations are unobservable prior to making donation decisions, the hybrid dual-incentive rules generate higher donation rates. In contrast, if valuations are observable, the dual-incentive priority rules create unbalanced incentives between high- and low-value agents, potentially undermining the efficacy of the hybrid dual-incentive rules in increasing overall donation rates. This paper was accepted by Marie Claire Villeval, behavioral economics and decision analysis. Funding: This research is supported by the National Natural Science Foundation of China [Grants 72173103, 72373127, and 71988101], the Singapore Ministry of Education (MOE) Academic Research Fund Tier 1 [RG57/20], and the Open Foundation of Key Laboratory of Interdisciplinary Research of Computation and Economics (Shanghai University of Finance and Economics), Ministry of Education of China. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2022.01530 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01530},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1418-1437},
  shortjournal = {Manag. Sci.},
  title        = {Incentivizing organ donation under different priority rules: The role of information},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidder-specific synergies and the evolution of acquirer
returns. <em>MS</em>, <em>71</em>(2), 1391–1417. (<a
href="https://doi.org/10.1287/mnsc.2022.02208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Largely constant average acquirer returns over the past four decades mask fundamental changes in the takeover market. Controlling for bidder composition, the common component of acquirer returns has increased by five percentage points relative to the 1980s. Offsetting this increase, the average bidder-specific component has declined. We propose a theory of bidder-specific synergies to help interpret these opposing trends. In our theory and in the data, acquirer returns increase with the extent to which synergies are unique to that bidder. The composition effect reflects bidder uniqueness. Overall, the evidence is consistent with rising merger synergies that have become less bidder specific. This paper was accepted by Victoria Ivashina, finance. Funding: A. Golubov acknowledges financial support from the Bank of Canada [Governor’s Award]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02208 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02208},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1391-1417},
  shortjournal = {Manag. Sci.},
  title        = {Bidder-specific synergies and the evolution of acquirer returns},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is blockholder diversity detrimental? <em>MS</em>,
<em>71</em>(2), 1356–1390. (<a
href="https://doi.org/10.1287/mnsc.2023.00528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We find that, overall, blockholder diversity, i.e., the firm shareholder base including several different types of blocks, is detrimental to firm performance. We show that lagged disclosure, on exogenous predetermined dates, that reveals an increase in block diversity is followed by a negative market reaction. Firms held by heterogeneous blockholders consistently perform worse than firms held by homogeneous blockholders. Block diversity is particularly detrimental when uncertainty is high. Disagreement among shareholders (e.g., as reflected in the frequency of lawsuits being filed) increases when the blockholder base is diverse. We make our blockholder data set public for the benefit of other researchers. This paper was accepted by Victoria Ivashina, finance. Funding: This work was supported by the Israel Science Foundation [Grant 264/20], the Faculty of Business and Economics, University of Melbourne [Grant ECR00009FNL], and the Accounting and Finance Association of Australia and New Zealand [Grant 2020-033]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2023.00528 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00528},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1356-1390},
  shortjournal = {Manag. Sci.},
  title        = {Is blockholder diversity detrimental?},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving customer compatibility with tradeoff transparency.
<em>MS</em>, <em>71</em>(2), 1335–1355. (<a
href="https://doi.org/10.1287/mnsc.2024.4985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through a large-scale field experiment with 393,036 customers considering opening a credit card account with a nationwide retail bank, we investigate how providing transparency into an offering’s tradeoffs affects subsequent rates of customer acquisition and long-run engagement. Although we find tradeoff transparency to have an insignificant effect on acquisition rates, customers who were shown each offering’s tradeoffs selected different products than those who were not. Moreover, prospective customers who experienced transparency and subsequently chose to open an account went on to exhibit higher-quality service relationships over time. Monthly spending was 9.9% higher and cancellation rates were 20.5% lower among those who experienced transparency into each offering’s tradeoffs. Increased product use and retention accrued disproportionately to customers with prior category experience: more-experienced customers who were provided transparency spent 19.2% more on a monthly basis and were 33.7% less likely to defect after nine months. Importantly, we find that these gains in engagement and retention do not come at the expense of customers’ financial well-being: the probability of making late payments was reduced among customers who experienced transparency. We further find that the positive effects of tradeoff transparency on engagement and retention were attenuated in the presence of a promotion that provided financial incentives to choose particular offerings. Taken together, these results suggest that providing transparency into an offering’s tradeoffs may be an effective strategy for informing customer choices, leading to better outcomes for customers and firms alike. This paper was accepted by Vishal Gaur, operations management. Funding: Funding for this research, apart from the costs Commonwealth Bank of Australia internally incurred to support the experiment, was provided by Harvard Business School. R. W. Buell has received compensation from Commonwealth Bank of Australia in the past for executive education teaching. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.4985 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.4985},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1335-1355},
  shortjournal = {Manag. Sci.},
  title        = {Improving customer compatibility with tradeoff transparency},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Patents, freedom to operate, and follow-on innovation:
Evidence from post-grant opposition. <em>MS</em>, <em>71</em>(2),
1315–1334. (<a href="https://doi.org/10.1287/mnsc.2019.02294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the blocking effect of patents on follow-on innovation by others. We posit that follow-on innovation requires freedom to operate (FTO), which firms typically obtain through a license from the patentee holding the original innovation. Where licensing fails, follow-on innovation is blocked unless firms gain FTO through patent invalidation. Using large-scale data from post-grant oppositions at the European Patent Office, we find that patent invalidation increases follow-on innovation, measured in citations, by 16% on average. This effect exhibits a U-shape in the value of the original innovation. For patents on low-value original innovations, invalidation predominantly increases low-value follow-on innovation outside the patentee’s product market. Here, transaction costs likely exceed the joint surplus of licensing, causing licensing failure. In contrast, for patents on high-value original innovations, invalidation mainly increases high-value follow-on innovation in the patentee’s product market. We attribute this latter result to rent dissipation, which renders patentees unwilling to license out valuable technologies to (potential) competitors. This paper was accepted by Ashish Arora, entrepreneurship and innovation. Funding: This work was supported by the Deutsche Forschungsgemeinschaft [Collaborative Research Center TRR 190]. F. Gaessler acknowledges financial support from the Spanish Agencia Estatal de Investigación through the Severo Ochoa Programme for Centres of Excellence in R&amp;D [Barcelona School of Economics CEX2019-000915-S]. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2019.02294 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2019.02294},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1315-1334},
  shortjournal = {Manag. Sci.},
  title        = {Patents, freedom to operate, and follow-on innovation: Evidence from post-grant opposition},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Workplace automation and corporate liquidity policy.
<em>MS</em>, <em>71</em>(2), 1287–1314. (<a
href="https://doi.org/10.1287/mnsc.2021.03902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using an occupational probability of computerization, we measure a firm’s ability to replace labor with automated capital. Our evidence suggests that the potential to automate a workforce enhances operating flexibility, allowing firms to hold less precautionary cash. To provide evidence for this mechanism, we exploit the 2011–2012 Thailand hard drive crisis as an exogenous shock to the cost of automation. In addition, the negative relation between prospective automation and cash holdings is greater for firms with a lower expected cost of worker displacement and greater labor-induced operating leverage. This paper was accepted by Lukas Schmid, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03902 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03902},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1287-1314},
  shortjournal = {Manag. Sci.},
  title        = {Workplace automation and corporate liquidity policy},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bank specialization and zombie lending. <em>MS</em>,
<em>71</em>(2), 1260–1286. (<a
href="https://doi.org/10.1287/mnsc.2023.01437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study whether banks internalize congestion externalities when lending to zombie firms. We conjecture that banks should be better informed about the presence of zombie firms and the congestion externalities that such firms exert on healthy borrowers in industries where banks are specialized and show that banks’ credit supply to zombie firms relates negatively to their industry specialization. This relation is stronger when congestion externalities are likely to have stronger adverse effects, namely when zombie firms take a higher fraction of resources in the industry or when the industry is geographically more concentrated. Additionally, this relation is weaker in industries with higher asset specificity as zombie firms’ default (and potential asset fire sales) could reduce healthy borrowers’ collateral value. This paper was accepted by Victoria Ivashina, finance. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/mnsc.2023.01437 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.01437},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1260-1286},
  shortjournal = {Manag. Sci.},
  title        = {Bank specialization and zombie lending},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The (surprising) sample optimality of greedy procedures for
large-scale ranking and selection. <em>MS</em>, <em>71</em>(2),
1238–1259. (<a href="https://doi.org/10.1287/mnsc.2023.00694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking and selection (R&amp;S) aims to select the best alternative with the largest mean performance from a finite set of alternatives. Recently, considerable attention has turned toward the large-scale R&amp;S problem which involves a large number of alternatives. Ideal large-scale R&amp;S procedures should be sample optimal; that is, the total sample size required to deliver an asymptotically nonzero probability of correct selection (PCS) grows at the minimal order (linear order) in the number of alternatives, k . Surprisingly, we discover that the naïve greedy procedure, which keeps sampling the alternative with the largest running average, performs strikingly well and appears sample optimal. To understand this discovery, we develop a new boundary-crossing perspective and prove that the greedy procedure is sample optimal for the scenarios where the best mean maintains at least a positive constant away from all other means as k increases. We further show that the derived PCS lower bound is asymptotically tight for the slippage configuration of means with a common variance. For other scenarios, we consider the probability of good selection and find that the result depends on the growth behavior of the number of good alternatives: if it remains bounded as k increases, the sample optimality still holds; otherwise, the result may change. Moreover, we propose the explore-first greedy procedures by adding an exploration phase to the greedy procedure. The procedures are proven to be sample optimal and consistent under the same assumptions. Last, we numerically investigate the performance of our greedy procedures in solving large-scale R&amp;S problems. This paper was accepted by Baris Ata, stochastic models and simulation. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72091211, 72071146, 72161160340]. Supplemental Material: The e-companion and data files are available at https://doi.org/10.1287/mnsc.2023.00694 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2023.00694},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1238-1259},
  shortjournal = {Manag. Sci.},
  title        = {The (Surprising) sample optimality of greedy procedures for large-scale ranking and selection},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trade secret protection and the integration of information
within firms. <em>MS</em>, <em>71</em>(2), 1213–1237. (<a
href="https://doi.org/10.1287/mnsc.2021.03484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the effect of trade secret protection laws on internal information integration (i.e., the extent to which economic agents are provided with access to decision-relevant information from other economic agents within a firm). We argue that stronger trade secret protection laws increase firms’ internal information integration because they reduce the proprietary costs of information leakage. To test our prediction, we measure a firm’s internal information integration by the share of its sites integrated into its enterprise management system. Exploiting the staggered adoption of trade secret protection laws via the Uniform Trade Secrets Act (UTSA), we find that these laws increase firms’ internal information integration. This effect is stronger (weaker) for firms with higher proprietary costs (coordination benefits). Further, we provide evidence that the UTSA-induced increase in internal information integration translates into improvements of firms’ internal information quality and decision-making quality. Taken together, our results enhance the understanding of the economic trade-offs shaping firms’ internal information environment. This paper was accepted by Ranjani Krishnan, accounting. Funding: S. Bormann and K. Hombach gratefully acknowledge funding from Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) [Grant Project-ID 403041268–TRR 266 Accounting for Transparency]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03484 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03484},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1213-1237},
  shortjournal = {Manag. Sci.},
  title        = {Trade secret protection and the integration of information within firms},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unconventional monetary policy transmission and bank lending
relationships. <em>MS</em>, <em>71</em>(2), 1187–1212. (<a
href="https://doi.org/10.1287/mnsc.2022.01871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms with only one bank relationship make up the majority of firms in many economies. This paper explores whether policy-driven lending is differentially transmitted to single-bank firms in comparison with the multibank firms that are the focus of the literature. Using unique variation in the ECB’s very long-term refinancing operations (VLTROs), which affected lending to firms discontinuously across credit ratings but within banks, we find selective transmission of VLTRO liquidity to single-bank firms. Banks apply higher lending standards to single-bank firms, with banking relationships determining both new lending and lending maturity. By contrast, banks appear to transmit policy lending near-uniformly across multibank firms. This paper was accepted by David Sraer, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.01871 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.01871},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1187-1212},
  shortjournal = {Manag. Sci.},
  title        = {Unconventional monetary policy transmission and bank lending relationships},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to optimize contextually constrained problems for
real-time decision generation. <em>MS</em>, <em>71</em>(2), 1165–1186.
(<a href="https://doi.org/10.1287/mnsc.2020.03565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of learning to solve optimization problems has received interest from both the operations research and machine learning communities. In this paper, we combine ideas from both fields to address the problem of learning to generate decisions to instances of optimization problems with potentially nonlinear or nonconvex constraints where the feasible set varies with contextual features. We propose a novel framework for training a generative model to produce provably optimal decisions by combining interior point methods and adversarial learning, which we further embed within an iterative data generation algorithm. To this end, we first train a classifier to learn feasibility and then train the generative model to produce optimal decisions to an optimization problem using the classifier as a regularizer. We prove that decisions generated by our model satisfy in-sample and out-of-sample optimality guarantees. Furthermore, the learning models are embedded in an active learning loop in which synthetic instances are iteratively added to the training data; this allows us to progressively generate provably tighter optimal decisions. We investigate case studies in portfolio optimization and personalized treatment design, demonstrating that our approach yields advantages over predict-then-optimize and supervised deep learning techniques, respectively. In particular, our framework is more robust to parameter estimation error compared with the predict-then-optimize paradigm and can better adapt to domain shift as compared with supervised learning models. This paper was accepted by Chung Piaw Teo, optimization. Funding: This work was supported in part by the Natural Sciences and Engineering Research Council of Canada. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2020.03565 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2020.03565},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1165-1186},
  shortjournal = {Manag. Sci.},
  title        = {Learning to optimize contextually constrained problems for real-time decision generation},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost structure and the usefulness of earnings. <em>MS</em>,
<em>71</em>(2), 1138–1164. (<a
href="https://doi.org/10.1287/mnsc.2021.03023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the relation between the usefulness of earnings and firm cost structure. We document that the usefulness of earnings to external users decreases in the degree of operating leverage, which measures the proportion of fixed to variable costs, and that this decline extends at least partially from the relation between the degree of operating leverage and two earnings properties: aggressive revenue recognition to meet an earnings target and earnings volatility. Our results illustrate how firm fundamentals influence a firm’s information environment and thereby the usefulness of earnings to external users. This paper was accepted by Ranjani Krishnan, accounting. Supplemental Material: The data files are available at https://doi.org/10.1287/mnsc.2021.03023 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03023},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1138-1164},
  shortjournal = {Manag. Sci.},
  title        = {Cost structure and the usefulness of earnings},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis of the cost coefficients in
multiobjective integer linear optimization. <em>MS</em>, <em>71</em>(2),
1120–1137. (<a href="https://doi.org/10.1287/mnsc.2021.01406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers sensitivity analysis of the cost coefficients in multiobjective integer linear programming problems. We define the sensitivity region as the set of simultaneous changes to the coefficients for which the efficient set and its structure remain the same. In particular, we require that the component-wise relation between efficient solutions is preserved and that inefficient solutions remain inefficient, and we show that this is sufficient for the efficient set to be the same upon changes. For a single coefficient, we show that a subset of the inefficient solutions can be excluded from consideration. More specifically, we prove that it suffices to inspect the inefficient solutions of a q -objective problem that are efficient in one of two related q + 1-objective problems. Finally, we show that the sensitivity region is a convex set (an interval). Our approach generalizes to simultaneous changes in multiple coefficients. For illustration, we consider mean-variance capital budgeting and determine the region for which the set of efficient portfolios remains the same, despite a misspecification or a change in the net present values of the projects. Further computational experience with multiobjective binary and integer knapsack problems demonstrates the general applicability of our technique. For instance, we obtain all sensitivity intervals for changes to single coefficients of biobjective problems with 500 binary variables in less than half an hour of CPU time by excluding a large number of inefficient solutions. In fact, the number of excluded solutions is above 100 orders of magnitude larger than the number of remaining solutions. This paper was accepted by Chung Piaw Teo, optimization. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.01406 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.01406},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1120-1137},
  shortjournal = {Manag. Sci.},
  title        = {Sensitivity analysis of the cost coefficients in multiobjective integer linear optimization},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bargaining with voluntary disclosure and endogenous
matching. <em>MS</em>, <em>71</em>(2), 1102–1119. (<a
href="https://doi.org/10.1287/mnsc.2022.03566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a bargaining setting between an “informed” player, who has private information, and an “uninformed” player. The informed player has the option to truthfully disclose private information in two unique environments. In the first environment, the informed player is randomly matched with an uninformed player and, after matching, can voluntarily disclose private information prior to a negotiation taking place. In the second environment, the informed player can voluntarily disclose private information before any endogenous matching between players (and subsequent negotiation) takes place. We begin by examining these environments theoretically. When disclosure occurs after matching, we show that low informed types should disclose more often than high informed types to avoid disagreement. However, when disclosure occurs before matching, for a range of parameter values, we show that high informed types should disclose more than low informed types to secure a better match. We test these predictions in a controlled human-subjects experiment and verify many of the theoretical predictions. Among other results, we find that low informed types do indeed disclose their private information to avoid disagreement and that, when it is in their interest to do so, high informed types disclose their private information to secure a better match. Another key insight is that high uninformed types benefit from disclosure regardless of whether it occurs before or after matching. This paper was accepted by Axel Ockenfels, behavioral economics and decision analysis. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03566 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.03566},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1102-1119},
  shortjournal = {Manag. Sci.},
  title        = {Bargaining with voluntary disclosure and endogenous matching},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Politically affiliated analysts. <em>MS</em>,
<em>71</em>(2), 1074–1101. (<a
href="https://doi.org/10.1287/mnsc.2022.00579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Government ownership of financial intermediaries is pervasive around the world. In this study, we examine the impact of common government ownership between the brokerage and listed firms on the information production role of brokerage firms. We show that affiliated analysts tend to issue more optimistic recommendations for stocks of firms controlled by the same government entity that controls their brokerage firms. This optimistic bias is particularly pronounced during periods of economic shocks. Our study demonstrates this by utilizing additional tariff impositions and tariff exemptions during the U.S.–China trade war as exogenous negative and positive shocks, respectively. Additionally, our study indicates that stocks recommended by politically affiliated analysts tend to underperform those recommended by independent analysts, implying that the optimism stems from conflicts of interest rather than superior information. Furthermore, our research highlights that sophisticated investors perceive the potential bias and incorporate it into their trading. Consistent with an exchange of favors story, politically affiliated brokerage firms receive a larger allocation during the issuance of local government debt, whereas governments subscribe for more shares during seasoned equity offerings by these affiliated brokerage firms. This paper was accepted by Kay Giesecke, finance. Funding: The authors gratefully acknowledge the financial support from the National Natural Science Foundation of China [Grants 71972088, 72132002, and 71991473], the National Social Science Foundation of China [Grants 21ZDA010 and 22VRC145], and the Innovation and Talent Base for Digital Technology and Finance [Grant B21038]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00579 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.00579},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1074-1101},
  shortjournal = {Manag. Sci.},
  title        = {Politically affiliated analysts},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online advertising as passive search. <em>MS</em>,
<em>71</em>(2), 1050–1073. (<a
href="https://doi.org/10.1287/mnsc.2022.02154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard search models assume that consumers actively decide on the order, identity, and number of products they search. We document that online, a large fraction of searches happen in a more passive manner, with consumers merely reacting to online advertisements that do not allow them to choose the timing or the identity of products to which they will be exposed. Using a clickstream panel data set capturing full URL addresses of websites that consumers visit, we show how to detect whether a click is ad initiated. We then report that in the apparel category, ad-initiated clicks account for more than half of all website arrivals, are more concentrated early on in the consumer search process, and lead to less in-depth searches and fewer transactions, consistent with the passive nature of these searches. To account for these systematic differences between active and passive searches, we propose and estimate a simple model that accommodates both types of searches. Our results show that incorrectly treating all searches as active inflates the estimated value of brands that advertise frequently. Our model can more accurately recover data patterns, especially for advertising brands. We finish with model extensions and a discussion of the managerial implications. This paper was accepted by Dmitry Kuksov, marketing. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02154 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.02154},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1050-1073},
  shortjournal = {Manag. Sci.},
  title        = {Online advertising as passive search},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Production chain organization in the digital age:
Information technology use and vertical integration in u.s.
manufacturing. <em>MS</em>, <em>71</em>(2), 1027–1049. (<a
href="https://doi.org/10.1287/mnsc.2019.01586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in information technology (IT) may affect the organizational design of production. Exploiting the rapid diffusion of the internet in the United States, we assess the sensitivity of production chain organization to this innovation in IT access and use. Extending theories of the firm that recognize the importance of downstream transfers (selling as opposed to sourcing) and plural governance in organizational design, we predict IT-driven shifts in downstream vertical integration. In a detailed panel of Census Bureau data for over 5,600 manufacturing plants, we observe the extent of a production unit’s downstream transactions within the firm alongside concurrent sales to external customers—a mix we refer to as plural selling . Our main finding is that the use of the internet for external coordination precipitated a significant decline in downstream vertical integration across the manufacturing sector. Instrumental variables estimation points to a causal relationship but also heterogeneous treatment effects. Key drivers of plural organization, such as complementarities and constraints across differently governed transactions, help explain such heterogeneity, as does concurrent use of internal production management IT. Our study is the first study to leverage a plural governance framework and large-scale microdata to understand how U.S. production chain organization shifted in response to this rapid and far-reaching technological change. This paper was accepted by David Simchi-Levi, information systems. Funding: This research was performed at the Atlanta, Boston, and Cornell (supported by the Cornell Center for the Social Sciences) Federal Statistical Research Data Centers [Project 1069 (CBDRB-FY22-279)]. Support for the Research Data Centers network from the National Science Foundation [Grant ITR-0427889] is gratefully acknowledged, as is support from the Social Sciences and Humanities Research Council of Canada. Supplemental Material: The online appendix is available at https://doi.org/10.1287/mnsc.2019.01586 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2019.01586},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1027-1049},
  shortjournal = {Manag. Sci.},
  title        = {Production chain organization in the digital age: Information technology use and vertical integration in U.S. manufacturing},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adwords with unknown budgets and beyond. <em>MS</em>,
<em>71</em>(2), 1009–1026. (<a
href="https://doi.org/10.1287/mnsc.2021.03243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the classic Adwords problem introduced by [Mehta A, Saberi A, Vazirani U, Vazirani V (2007) Adwords and generalized online matching. J. ACM 54(5):22-es.], we have a bipartite graph between advertisers and queries. Each advertiser has a maximum budget that is known a priori. Queries are unknown a priori and arrive sequentially. When a query arrives, advertisers make bids, and we (immediately and irrevocably) decide which (if any) Ad to display based on the bids and advertiser budgets. The winning advertiser for each query pays their bid up to their remaining budget. Our goal is to maximize total budget used without any foreknowledge of the arrival sequence (which could be adversarial). We consider the setting where the online algorithm does not know the advertisers’ budgets a priori and the budget of an advertiser is revealed to the algorithm only when it is exceeded. A naïve greedy algorithm is 0.5 competitive for this setting, and finding an algorithm with better performance remained an open problem. We show that no deterministic algorithm has competitive ratio better than 0.5 and give the first (randomized) algorithm with strictly better performance guarantee. We show that the competitive ratio of our algorithm is at least 0.522 but also strictly less than ( 1 − 1 / e ) . We present novel applications of budget oblivious algorithms in search ads and beyond. In particular, we show that our algorithm achieves the best possible performance guarantee for deterministic online matching in the presence of multichannel traffic [Manshadi V, Rodilitz S, Saban D, Suresh A (2022) Online algorithms for matching platforms with multi-channel traffic. Proc. 23rd ACMConf. Econom. Comput. (ACM, NewYork), 986–987.]. This paper was accepted by Omar Besbes, revenue management and market analytics. Funding: NSF Division of Civil, Mechanical, and Manufacturing Innovation [Grant 2340306], and Google Research Scholar Program. Supplemental Material: The online appendices are available at https://doi.org/10.1287/mnsc.2021.03243 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03243},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {1009-1026},
  shortjournal = {Manag. Sci.},
  title        = {Adwords with unknown budgets and beyond},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the impact of community first responders.
<em>MS</em>, <em>71</em>(2), 992–1008. (<a
href="https://doi.org/10.1287/mnsc.2022.04024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In community first responder (CFR) systems, traditional emergency service response is augmented by a network of trained volunteers who are dispatched via an app. A central application of such systems is out-of-hospital cardiac arrest (OHCA), where a very fast response is crucial. For a target performance level, how many volunteers are needed, and from which locations should they be recruited? We model the presence of volunteers throughout a region as a Poisson point process, which permits the computation of the response-time distribution of the first-arriving volunteer. Combining this with known survival-rate functions, we deduce survival probabilities in the cardiac arrest setting. We then use convex optimization to compute a location distribution of volunteers across the region that optimizes either the fraction of incidents with a fast response (a common measure in the industry) or patient survival in the case of OHCA. The optimal location distribution provides a bound on the best possible performance with a given number of volunteers. This can be used to determine whether introducing a CFR system in a new region is worthwhile or can serve as a guide for additional recruitment in existing systems. Effective target areas for recruitment are not always obvious because volunteers recruited from one area may be found in various areas across the city depending on the time of day; we explicitly capture this issue. We demonstrate these methods through an extended case study of Auckland, New Zealand. This paper was accepted by Carri Chan, healthcare management. Funding: This research was financed in part by the Netherlands Organization for Scientific Research (NWO) in the form of a Rubicon grant (019.172EN.016) to C. J. Jagtenberg and a Veni grant (VI.Veni.191E.005) to P. L. van den Berg. S. G. Henderson and H. Li were supported in part by National Science Foundation [Grant CMMI-2035086]. Vrije Universiteit Amsterdam and Erasmus University received funding from TKI Dinalog [Grant 2023-1-307TKI]. Some of this work has been executed under the TKI Dinalog [Grant 2023-1-307TKI]. Part of this work was completed during a research visit made possible by a [Distinguished Visitor Award] from the University of Auckland. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.04024 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2022.04024},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {992-1008},
  shortjournal = {Manag. Sci.},
  title        = {Modeling the impact of community first responders},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk taking under limited liability and moral hazard:
Quantifying the role of motivated beliefs. <em>MS</em>, <em>71</em>(2),
976–991. (<a href="https://doi.org/10.1287/mnsc.2021.03947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates whether limited liability and moral hazard affect risk taking through motivated beliefs. On the one hand, limited liability encourages investors to take excessive risks. On the other, excessive risk taking makes it hard for investors to maintain a positive self-image when moral hazard is present. Using a novel experimental design, we show that subjects form motivated beliefs to self-justify their excessive risk taking. For the same investment opportunity, subjects invest more and are significantly more optimistic about the success of the investment if its failure can harm others. We show that more than one third of the investment increases under limited liability, and moral hazard can be explained through motivated beliefs. Moreover, through a treatment with limited liability but no moral hazard, we show that motivated beliefs are formed subconsciously and can lead to the paradoxical result of investors taking larger risks when their investment can harm a third party compared with when it cannot. These results underscore the importance of motivated beliefs in regulatory policy, emphasizing that policymakers must not only address bad incentives, but also address the role of bad beliefs. This paper was accepted by Bruno Biais, finance. Funding: Financial support by Deutsche Forschungsgemeinschaft [Grant CRC TRR 190, project number 280092119] is gratefully acknowledged. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2021.03947 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2021.03947},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {976-991},
  shortjournal = {Manag. Sci.},
  title        = {Risk taking under limited liability and moral hazard: Quantifying the role of motivated beliefs},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rise of the machines: The impact of automated underwriting.
<em>MS</em>, <em>71</em>(2), 955–975. (<a
href="https://doi.org/10.1287/mnsc.2024.4986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a randomized experiment in auto lending, we find that algorithmic underwriting outperforms the human underwriting process, resulting in 10.2% higher loan profits and 6.8% lower default rates. The human and machine underwriters show similar performance for low-risk, less complex loans. However, the performance of human underwritten loans largely declines for riskier and more complex loans, whereas the machine performance stays relatively stable across various risk dimensions and loan characteristics. The performance difference is more pronounced at underwriting thresholds with a high potential for agency conflict. These results are consistent with algorithmic underwriting mitigating agency conflicts and humans’ limited capacity for analyzing complex problems. This paper was accepted by Will Cong, finance. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.4986 .},
  archive      = {J_MS},
  doi          = {10.1287/mnsc.2024.4986},
  journal      = {Management Science},
  month        = {2},
  number       = {2},
  pages        = {955-975},
  shortjournal = {Manag. Sci.},
  title        = {Rise of the machines: The impact of automated underwriting},
  volume       = {71},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="msom---19">MSOM - 19</h2>
<ul>
<li><details>
<summary>
(2025). Optimal prototyping with noisy measurements. <em>MSOM</em>,
<em>27</em>(1), 322–338. (<a
href="https://doi.org/10.1287/msom.2024.1133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Prototyping and testing are an integral part of almost any new product development process, helping firms navigate the inherent uncertainties of creating new products. Recent developments in rapid prototyping, including technologies that enable cheaper low-fidelity tests, have opened up the possibilities for firms in reconfiguring their product development processes. Firms can, by choosing the level of evaluation fidelity, alter the traditional cost-quality trade-offs inherent in sequential prototyping. Methodology/results : The current article formulates a general model of sequential search where firms can proceed by obtaining noisy low-fidelity evaluations of their prototypes. Our results demonstrate that the imperfect fidelity of evaluations alters the firm’s optimal experimentation, with the starkest difference being that it may make it optimal for the firm to select and launch a prototype that did not yield the best evaluation. In addition, our analysis of optimal measurement technology reveals that the focal firm should demand the most precise measurements when their ex-ante uncertainty is moderate (not too high or low). We also consider extensions analyzing how the optimal choice of evaluation fidelity is affected by the number of available prototypes, by operational flexibility (to dynamically change measurement technology), and by the ability to outsource evaluations to an experimentation platform. Managerial implications : We develop managerial insights for how the optimal choice of fidelity and the optimal length of the evaluation cycle should be planned depending on the evaluation costs and the firm’s ex-ante uncertainty. The resulting framework offers guidance to product and software development firms to successfully leverage imperfect fidelity experiments. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2024.1133 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.1133},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {322-338},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Optimal prototyping with noisy measurements},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service operations for justice-on-time: A data-driven
queueing approach. <em>MSOM</em>, <em>27</em>(1), 305–321. (<a
href="https://doi.org/10.1287/msom.2023.0530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Limited resources in the judicial system can lead to costly delays, stunted economic development, and even failure to deliver justice. Using the Supreme Court of India as an exemplar for such resource-constrained settings, we apply ideas from service operations to study delay. Specifically, court dynamics constitute a case-management queue , whereby each case may experience multiple service encounters spread across time, but all are necessarily with the same server. Our goal is to elucidate the drivers of congestion, focusing on metrics such as the expected case-disposition time (delay) and expected number of cases awaiting adjudication (pendency), and leverage this understanding to recommend operational interventions. Methodology/results : We employ data-driven calibrated simulations to model the analytically intractable case-management queue. The life cycle of a case comprises two stages: preadmission (before determining its merit for detailed hearings) and postadmission. Our methodology allows us to capture the queueing dynamics in which the judges are shared resources across the two stages. It also permits modeling of holiday capacity, which is flexibly tailored to address any surplus work that spills over from the regular year. We find that the second stage of this judicial queue is overloaded, but holiday capacity creates a perception of stability by steadying performance metrics. Managerial implications : The sources of inefficiency that drive congestion include a misalignment between scheduling guidelines and judicial capacity, coupled with the requirement to schedule hearings in advance. Together, these factors inhibit utilization of shared capacity across the two-stage judicial queue. We demonstrate how interventions that account for these inefficiencies can successfully tackle judicial delay. In particular, scheduling to improve the allocation of time across preadmission and postadmission cases can cut down the expected delay by as much as 65%. Funding: This study is (partially) supported by a Korea University Business School Research Grant. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0530 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0530},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {305-321},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Service operations for justice-on-time: A data-driven queueing approach},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Not all lines are skipped equally: An experimental
investigation of line-sitting and express lines. <em>MSOM</em>,
<em>27</em>(1), 287–304. (<a
href="https://doi.org/10.1287/msom.2022.0338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : In this paper, we investigate how line-sitting and express lines affect customers’ satisfaction and fairness perception about queues. In line-sitting, customers skip the wait by hiring others to stand in line on their behalf. In express lines, customers skip the wait by purchasing priority. Because both schemes disrupt the first-in-first-out (FIFO) rule, they may be perceived to be unfair and lead to customer dissatisfaction. Methodology/results : In three experiments, we find that customers are more satisfied with the overall queueing experience when they encounter a line-sitter than when they encounter an express-line customer, even when the wait time is held constant. We also find that customers perceive express lines as less fair than line-sitting, which has a significant influence on their satisfaction. A key operational difference between line-sitting and express lines is that line-sitting involves a one-to-one swap between a line-sitter doing a fair share of waiting and their client, whereas express lines insert a new priority customer into the queue without an existing surrogate in the queue. We show that if line-sitting includes an insertion by letting a line-sitter hold a spot for more than one customer, then line-sitting is perceived to be just as unfair and unsatisfying as express lines. Managerial implications : Our results imply that express lines can engender a lower fairness perception, thus harming future business through both more negative word of mouth and a higher customer churn. Therefore, service providers should beware of customer backlash against express lines and yet be more open-minded about line-sitting, provided that one-to-one swaps can be enforced. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0338 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0338},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {287-304},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Not all lines are skipped equally: An experimental investigation of line-sitting and express lines},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information provision from a platform to competing sellers:
The role of strategic ambiguity. <em>MSOM</em>, <em>27</em>(1), 269–286.
(<a href="https://doi.org/10.1287/msom.2023.0262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : With the rapid growth of e-commerce, platforms are able to gather large quantities of data, which can result in high-precision predictions regarding consumer purchasing patterns and future demand. A fundamental question is whether a platform has the incentives and ability to share such nonverifiable information with its sellers. Methodology/results : We show that when information is nonverifiable, sharing the precise value of the platform’s private information by means of cheap-talk cannot result in an equilibrium. This outcome is due to the incentive of the platform to portray a more favorable market condition than that predicted, in order to encourage the sellers to raise their prices and improve market efficiency. In spite of this negative result, we demonstrate that the level of incentives misalignment between the platform and its sellers is bounded; consequently, a region-forecast information-sharing equilibrium can emerge. In this equilibrium, the support of the platform’s private information is divided into several intervals, and the platform strategically chooses to report truthfully the interval containing its private information. The structure of the partition is influenced by two main factors: the incentive of the platform to reduce market uncertainty for the sellers, and the motivation of the platform to soften competition among the sellers. Although both the sellers and the platform benefit from the ability to share some level of information, such an outcome hurts the consumers. Managerial implications : This work explains the observed practice of a platform providing nonverifiable information to its sellers via cheap-talk. The main advantage of this equilibrium is the ability to share information in a costless manner; however, the amount of information that can be shared is limited and is influenced by the level of market competition between the sellers. Funding: N. Shamir gratefully acknowledges financial support from the Israel Science Foundation [Grant 2358/22] and The Henry Crown Institute of Business Research in Israel. T. Avinadav and T. Chernonog gratefully acknowledge financial support from the Israel Science Foundation [Grant 1571/20]. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/msom.2023.0262 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0262},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {269-286},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Information provision from a platform to competing sellers: The role of strategic ambiguity},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The choice overload effect in online recommender systems.
<em>MSOM</em>, <em>27</em>(1), 249–268. (<a
href="https://doi.org/10.1287/msom.2022.0659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Online retailing platforms are increasingly relying on personalized recommender systems to help guide consumer choice. An important but understudied question in such settings is how many products to include in a recommendation set. In this work, we study how the number of recommended products influences consumers’ search and purchase behavior in an online personalized recommender system within a retargeting setting. Methodology/results : Via a field experiment involving 1.6 million consumers on an online retailing platform, we causally demonstrate that consumers’ likelihood of purchasing any product from the recommendation set first increases then decreases as the number of recommended products increases. Importantly, as much as 64% of the decrease in purchase probability (i.e., the choice overload effect) can be attributed to a decrease in consumers’ likelihood of starting a search (i.e., clicking on any recommended product). We discuss the possible behavioral mechanisms driving these results and analyze how these effects could be heterogeneous across different product categories, price ranges, and timing. Managerial implications : This work presents real-world experimental evidence for the choice overload effect in online retailing platforms, highlights the important role of consumer search behavior in driving this effect, and sheds light on when and how limiting the number of options in a recommender system may be beneficial to online retailers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0659 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0659},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {249-268},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {The choice overload effect in online recommender systems},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian ensembles of exponentially smoothed life-cycle
forecasts. <em>MSOM</em>, <em>27</em>(1), 230–248. (<a
href="https://doi.org/10.1287/msom.2022.0359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : We study the problem of forecasting an entire demand distribution for a new product before and after its launch. Firms need accurate distributional forecasts of demand to make operational decisions about capacity, inventory, and marketing expenditures. We introduce a unified, robust, and interpretable approach to producing these pre- and postlaunch distributional forecasts. Methodology/results : Our approach is inspired by Bayesian model averaging. Each candidate model in our ensemble is a life-cycle model fitted to the completed life cycle of a comparable product. A prelaunch forecast is an ensemble with equal weights on the candidate models’ forecasts, whereas a postlaunch forecast is an ensemble with weights that evolve according to Bayesian updating. Our approach is part frequentist and part Bayesian, resulting in a novel approach tailored to the demand forecasting challenge. We also introduce a new type of life-cycle or product diffusion model with states that can be updated using exponential smoothing. The trend in this model follows the density of an exponentially tilted Gompertz random variable. For postlaunch forecasting, this model is attractive because it can adapt itself to the most recent changes in a product’s life cycle. We provide closed-form distributional forecasts from our model. In two empirical studies, we show that when the ensemble’s candidate models are all in our new type of exponential smoothing model, this version of the ensemble outperforms several leading approaches in both point and quantile forecasting. Managerial implications : In a data-driven operations environment, our model can produce accurate forecasts frequently and at scale. When quantile forecasts are needed, our model has the potential to provide meaningful economic benefits. In addition, our model’s interpretability should be attractive to managers who already use exponential smoothing and ensemble methods for other forecasting purposes. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0359 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0359},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {230-248},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Bayesian ensembles of exponentially smoothed life-cycle forecasts},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in repeated multiunit pay-as-bid auctions.
<em>MSOM</em>, <em>27</em>(1), 200–229. (<a
href="https://doi.org/10.1287/msom.2023.0403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Motivated by carbon emissions trading schemes (ETSs), Treasury auctions, procurement auctions, and wholesale electricity markets, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multiunit pay-as-bid (PAB) auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. In this work, we study the problem of optimizing bidding strategies from the perspective of a single bidder. Methodology/results : Effective bidding in PAB auctions is complex due to the combinatorial nature of the action space. We show that a utility decoupling trick enables a polynomial time algorithm to solve the offline problem where competing bids are known in advance. Leveraging this structure, we design efficient algorithms for the online problem under both full information and bandit feedback settings that achieve an upper bound on regret of O ( M T log T ) and O ( M T 2 3 log T ) , respectively, where M is the number of units demanded by the bidder, and T is the total number of auctions. We accompany these results with a regret lower bound of Ω ( M T ) for the full information setting and Ω ( M 2 / 3 T 2 / 3 ) for the bandit setting. We also present additional findings on the characterization of PAB equilibria. Managerial implications : Although the Nash equilibria of PAB auctions possess nice properties such as winning bid uniformity and high welfare and revenue, they are not guaranteed under no-regret learning dynamics. Nevertheless, our simulations suggest that these properties hold anyways, regardless of Nash equilibrium existence. Compared with its uniform price counterpart, the PAB dynamics converge faster and achieve higher revenue, making PAB appealing whenever revenue holds significant social value—for example, ETSs and Treasury auctions. Funding: R. Galgana and N. Golrezaei were supported in part by the Young Investigator Program Award from the Office of Naval Research [Grant N00014-21-1-2776] and the Massachusetts Institute of Technology Research Support Award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0403 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0403},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {200-229},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Learning in repeated multiunit pay-as-bid auctions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling regulatory operations: A data set of the
determinants, process, and outcomes of product defect investigations by
the u.s. Automotive safety regulator. <em>MSOM</em>, <em>27</em>(1),
181–199. (<a href="https://doi.org/10.1287/msom.2023.0705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The paucity of data on governmental regulatory agencies’ product safety defect investigations has restricted our knowledge about (1) the determinants of a regulator’s decisions to open or close an investigation, (2) the process it follows between opening and closing of an investigation, and (3) the outcomes of the investigation when it is closed. Methodology/results : The authors view a safety regulator’s opening and closing of a product defect investigation as a decision of interest to the operations management discipline. This data paper describes a rich, novel, and hand-collected data set of all investigations that the National Highway Traffic Safety Administration—the U.S. regulator for automobile safety—opened and closed against 187 manufacturers between 2009 and 2021. The authors provide two Microsoft Excel data files, one capturing data for the investigations opened and the other for the investigations closed. The data files enable researchers to address three sets of research questions. First, researchers can use the “Data on Investigations Opened” file to model the determinants of a regulator’s opening of a product defect investigation. Second, researchers can mine the textual variables from both files to identify the steps involved in the investigation process. They can also use the process variables included in the data to investigate the regulator’s efficiency in opening and closing investigations. Third, researchers can use the “Data on Investigations Closed” file to better understand when and why a regulator closes an investigation and the outcomes of the closed investigations. Managerial implications : The data files can also be valuable to nonacademic stakeholders (e.g., governmental organizations and regulators, journalists, liability lawyers, politicians, and safety advocates). The authors provide an open-access website that simplifies the use of the data for a nonacademic audience and allows them to draw insights from the data via graphs and tables. Supplemental Material: The online supplement is available at https://doi.org/10.1287/msom.2023.0705 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0705},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {181-199},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Unveiling regulatory operations: A data set of the determinants, process, and outcomes of product defect investigations by the U.S. automotive safety regulator},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating demand with unobserved no-purchases on
revenue-managed data. <em>MSOM</em>, <em>27</em>(1), 161–180. (<a
href="https://doi.org/10.1287/msom.2021.0291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper studies the joint estimation of the consumer arrival rate and choice model parameters when “no-purchasers” (customers who considered the product but did not purchase) are not observable. Estimating this unconstrained demand even with the simplest discrete-choice model such as the multinomial logit (MNL) becomes challenging as we do not know the fraction that have chosen the outside option (i.e., not purchased). Methods have been proposed to use market share to pin down the parameter associated with the outside option. However, market share data are difficult to obtain in many situations, and in some industries, such as fashion retail, have little meaning as the items are difficult to compare. In this paper, we point out an additional difficulty that can arise in practice: Many firms monitor sales and optimize their prices and assortments within the sale period as part of their revenue management (RM) process, based on partially observed demand. This can potentially cause a revenue management induced endogeneity as the data used for estimation is the result of optimization (in turn based on prior data) to set controls. As we demonstrate, methods that work well on randomly generated assortments may do badly on optimized assortment data. Methodology/results : In this paper, we propose a robust method when the firm cannot observe no-purchases and has no market share information, and the data have been revenue-managed. We develop a two-step generalized method-of-moments (GMM) procedure that is based on a modified moment condition, and importantly, does not require instrumental variables (IVs), a significant advantage in practice. Managerial implications : In Monte Carlo simulations, the performance of our method matches existing methods when the controls are generated randomly, and is robust under all conditions, whether RM-induced endogeneity is present or not. On a large real-world data set from the fashion industry, subject to stock-outs and markdown pricing along with unknown management controls, our method provides robust estimates compared with existing methods without requiring any input on market shares, which is especially difficult to pin down at a category and season/collection level. Funding: This work was supported by the Hong Kong Research Grants Council’s General Research Fund [Grant 14506423]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2021.0291 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0291},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {161-180},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Estimating demand with unobserved no-purchases on revenue-managed data},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of temporary store closures on online sales: Evidence
from a natural experiment. <em>MSOM</em>, <em>27</em>(1), 147–160. (<a
href="https://doi.org/10.1287/msom.2022.0527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper examines the impact of retail store closures on omnichannel sales and consumer shopping behavior in the context of the coronavirus disease 2019 pandemic. To explain the likelihood of store closure, we develop a novel instrumental variable motivated by varying geopolitical responses across the United States to the pandemic. Methodology/results : Using data from a luxury fashion retailer, we find that when a store is closed, the volume of online orders originating from its location increases by 24%. Furthermore, when the retailer closes 10% of its stores, the omnichannel total sales (offline + online) decrease by 5.5%. Notably, our findings indicate that the online channel enables the retailer to recover 11% of offline sales that would have otherwise been lost because of store closures. We also show that compared with existing e-shoppers, new e-shoppers are more likely to order popular product models in an effort to mitigate the heightened mismatch risk associated with online transactions. For new e-shoppers, the likelihood of ordering a popular model stands at 70%, whereas it is 45% for existing online consumers. Additionally, the conservative behavior of favoring popular models reduces the likelihood of returns by new e-shoppers. Managerial implications : Even for luxury apparel, which is often associated with in-store purchases requiring “touch and feel” and customer tryout, the option to purchase online proves immensely valuable. The tendency of new e-shoppers to limit product mismatch risk by choosing popular products may create an opportunity for retailers to strategically target these inexperienced online customers with advertisements, product promotions, or virtual fitting rooms, all geared toward reducing online shopping risk of product mismatch. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0527 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0527},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {147-160},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Impact of temporary store closures on online sales: Evidence from a natural experiment},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recycling standards, green inventions, and spillover:
Evidence from california’s electronic waste recycling act (EWRA).
<em>MSOM</em>, <em>27</em>(1), 127–146. (<a
href="https://doi.org/10.1287/msom.2023.0444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The improper disposal of electronic and electrical goods at the end of their useful lives (i.e., e-waste) can have adverse effects on the environment and human health. The increased awareness of the negative consequences of e-waste has prompted many regulators to enact recycling standards that promote the proper disposal and recycling of e-waste. A large body of research has used analytical models to explore how recycling standards affect firms that make electronic and electrical goods. A key insight from this research is that e-waste recycling standards would induce firms to design products that have reduced environmental impact and are easier to recycle. In other words, e-waste recycling standards would enhance inventive activity at firms to better comply with regulatory requirements. But hardly any empirical work has validated the insights developed with analytical models. Methodology/results : We empirically examine whether California’s Electronic Waste Recycling Act (EWRA) affects the inventive output (i.e., measured as patents) of firms that manufacture electronic and electrical goods. We leverage a quasi-experimental setup that arises when California enacted the EWRA and use multiple identification strategies to isolate the law’s effect on the inventive output of firms. We disentangle two causal pathways, industry and headquarter location , by which EWRA affects manufacturers. We find that EWRA increased the environmentally focused inventive output (i.e., “green” patents) of affected firms in California by nearly 14% and by nearly 8% for firms in other states. Interestingly, we also observe spillover effects—EWRA increased other inventive output (i.e., patents other than green patents) of affected firms in California by nearly 41% and by nearly 24% for firms in other states. Managerial implications : Our study provides important insights for managers and policy makers by empirically quantifying the impact of recycling standards on environmentally focused inventions and by identifying spillover effects for other inventions. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2023.0444 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0444},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {127-146},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Recycling standards, green inventions, and spillover: Evidence from california’s electronic waste recycling act (EWRA)},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interplay between servicizing and remanufacturing: Economic
and environmental implications. <em>MSOM</em>, <em>27</em>(1), 114–126.
(<a href="https://doi.org/10.1287/msom.2022.0379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : To investigate the interplay between servicizing and remanufacturing, two widely used strategies in the circular economy, and its impact on firms’ economic and environmental performance, we consider a profit-maximizing manufacturer who explores the possibility of jointly adopting the two strategies. Methodology and results : We develop optimization models that capture the main attributes for servicizing (pay-per-use, demand pooling) and remanufacturing (lower production costs, higher operating costs). We show that the presence of remanufacturing always makes servicizing less attractive to adopt, and having servicizing may discourage the adoption of remanufacturing if the remanufacturing cost reduction effect is not sufficiently large. The joint adoption of servicizing and remanufacturing is preferred when the remanufacturing cost is neither too high nor too low and the range of the remanufacturing cost that favors the joint adoption is further moderated by the firm’s pooling level and the operating cost of remanufactured products. Lastly, we find that when the firm adopts servicizing in the presence of remanufacturing with relatively high remanufacturing costs, it can reduce the number of remanufactured products with low environmental impact in the market and, thus, harm the environment, resulting in a misalignment between economic and environmental goals. Managerial implications : Our findings suggest that managers considering jointly adopting servicizing and remanufacturing must balance their competition on the low-usage end of the market with the potential complementarity due to remanufacturing cost reduction. Further, by showing the win-win conditions under which the joint adoption of servicizing and remanufacturing improves firms’ economic and environmental performance, we demonstrate that there are no one-size-fits-all circular economy strategies and call for caution when promoting these strategies across industries. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/msom.2022.0379 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0379},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {114-126},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Interplay between servicizing and remanufacturing: Economic and environmental implications},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Green disposable packaging and communication: The
implications of bring-your-own-container. <em>MSOM</em>, <em>27</em>(1),
94–113. (<a href="https://doi.org/10.1287/msom.2021.0605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : A growing number of firms are encouraging consumers to participate in “bring-your-own-container” (BYOC) behavior in which consumers bring their own reusable packaging to purchase and consume products, thus reducing single-use packaging waste. In this paper, we study the environmental implications of a firm’s BYOC implementation when considering its disposable packaging choice and communication strategy. Methodology/results : We build a stylized model to study a firm’s joint decisions on BYOC, disposable packaging choice, and communication and their implications on the environment. Our main results follow. First, allowing BYOC reduces the firm’s incentive to make fraudulent green claims about its disposable product packaging; however, BYOC implementation may harm the overall environment while improving the firm’s profit, thereby creating a new form of greenwashing. Second, the adoption of third-party certification for green disposable packaging is an effective remedy to mitigate the negative environmental impact of BYOC. In addition, the environmental implications of adopting third-party certification (either voluntarily or because of government mandates) depend on the relationship between the environmental qualities of green disposable packaging and reusable packaging. Whereas it always benefits the environment when the firm’s green disposable packaging has better environmental performance, adopting certification may negatively impact the environment if consumers’ reusable packaging is greener. Furthermore, we find numerically that offering a price discount for BYOC may encourage the firm to adopt certification because of increased profitability, thereby leading to the aforementioned environmental implications. Managerial implications : We offer operational insights on how firms should make joint decisions on BYOC, disposable packaging choice, and communication. We also generate insights on how governments should regulate firms’ green claims when firms start to allow BYOC. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2021.0605 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2021.0605},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {94-113},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Green disposable packaging and communication: The implications of bring-your-own-container},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emergency care efficiency vs. Quality: Uncovering hidden
consequences of fast-track routing decisions. <em>MSOM</em>,
<em>27</em>(1), 75–93. (<a
href="https://doi.org/10.1287/msom.2022.0440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This work aims to examine the role of emergency department (ED) operational status related to congestion in fast-track (FT) routing decisions and the subsequent effects on patient outcomes. Methodology/results : In this paper, we utilize a two-year data set from two hospital EDs in Alberta, Canada, and adopt an instrumental variable approach to examine the effects of FT routing decisions on patient outcomes. Based on the empirical findings, we utilize a data-calibrated simulation to compare the performance of different routing policies. First, our study reveals that FT routing decisions are not purely clinically driven, and ED operational status is also associated with FT routing decisions. Second, being routed to FT can improve ED efficiency by reducing the average length of stay and left without being seen rates. However, this efficiency improvement comes at the cost of potential quality decline. In particular, being routed to the FT leads to an 8.2% increase in the 48-hour revisit rate for the high-complexity group and a 2.3% increase for the medium-complexity group. Third, we delve into the mechanisms behind observed patient outcomes and find that physicians in the FT area may prioritize expediting patient flow by simplifying patient diagnosis and treatment procedures. Consequently, the quality of care may be compromised for high- and medium-complexity patients. Finally, our simulation findings highlight the importance of selecting the “right” patients to be routed to the FT unit. To this end, the complexity-based classification method and dynamic routing policies emerge as promising avenues. Managerial implications : Our findings call for immediate attention from healthcare practitioners to carefully balance the trade-off between emergency care efficiency and quality, emphasizing the necessity of selecting the right patients for routing. Funding: This study is partially supported by the Hong Kong Research Grants Council [Grants GRF 11508921 and CRF C7162-20G]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0440 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0440},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {75-93},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Emergency care efficiency vs. quality: Uncovering hidden consequences of fast-track routing decisions},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multichannel healthcare: Impact of asynchronous telemedicine
adoption on patient flow. <em>MSOM</em>, <em>27</em>(1), 59–74. (<a
href="https://doi.org/10.1287/msom.2022.0235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : This paper studies a multichannel healthcare system where physicians diagnose patients and prescribe treatment in-person or through asynchronous telemedicine (AT), a widely adopted yet relatively under-explored form of telemedicine. In collaboration with physicians at the Veterans Healthcare Administration (VHA), we examine the impact of introducing an AT channel on the existing in-person channel and on overall system performance. Methodology/results : VHA implemented AT at select clinics in the state of Georgia in 2012. Using a difference-in-differences design, we find that the introduction of the AT channel led to a sorting process whereby more complex patients were seen in the in-person channel. AT implementation led to a 20% increase in recommended visit time and an 8.5% increase in required clinical resources for in-person consultations. In addition, the adoption of AT resulted in higher throughput—more patients seen by the specialists per month across both channels. Using a fixed-effects model we find a reduction in average wait time for in-person referrals (37.5%), and for the most common medically necessary procedure (43%) despite an increase in the total number of consultations at the specialist clinic. We attribute the improved efficiency to early patient triage, better match between patient needs and treatment modality, and reduction of setup and switching costs in physicians’ workflow. Managerial implications : This paper contributes to our understanding of a rapidly expanding form of healthcare delivery: multichannel healthcare with in-person and AT channels. Our results suggest that healthcare managers and physicians can adopt AT to improve overall system efficiency. At the same time, they should take into account the additional impact of AT on the in-person channel when making capacity decisions and developing guidance on patient referrals. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0235 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0235},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {59-74},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Multichannel healthcare: Impact of asynchronous telemedicine adoption on patient flow},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Telehealth in acute care: Pay parity and patient access.
<em>MSOM</em>, <em>27</em>(1), 40–58. (<a
href="https://doi.org/10.1287/msom.2022.0345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : In response to the increased use of telehealth to replace traditional office visits with a physician, several U.S. states have recently adopted telehealth pay-parity policies. Such policies state that payers must reimburse healthcare providers for telehealth services at the same rate that would apply if those services had been provided in a traditional office setting. But health policy researchers have pointed out that telehealth may not be as effective as a traditional office visit for acute care. Specifically, telehealth is associated with increased probability of a subsequent office visit (a “duplicate visit”). We examine whether telehealth pay-parity policies are effective at improving access to acute care, and under what conditions. Methodology/results : We use a three-stage game-theoretic model to study the impact of telehealth pay parity. In the first stage, the payer sets a reimbursement policy for telehealth visits. In the second stage, a healthcare provider commits a portion of its capacity to telehealth, and in the third stage, patients arrive and choose between telehealth and office visits according to an equilibrium queueing network. We find structural results for the equilibria and characterize the equilibria in closed-form. When the chance of a duplicate visit is moderate (neither too high nor too low), pay parity leads providers to allocate too much capacity to telehealth, resulting in lower overall patient access than could be otherwise achieved. We characterize a reimbursement level that avoids this misalignment and maximizes patient access, which we show is less than parity. Managerial implications : The literature shows that patients receiving acute care via telehealth may be more likely to require a duplicate, in-person visit to resolve their health concern. In the fee-for-service environment that is common in the United States for acute care, duplicate visits resulting from telehealth lead to an incentive alignment problem because they generate extra work and provider revenue, without any corresponding increase in patient access. Legislating pay parity for telehealth can lead to providers committing more capacity to telehealth, which may not always be good. However, there is good news in that all parties (payers, providers, and patients) would be better off if duplicate visits could be decreased. Policy makers should understand these implications before enacting policies that affect reimbursements for telehealth. Funding: Support for this project was provided by a PSC-CUNY Award, jointly funded by The Professional Staff Congress and The City University of New York. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0345 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0345},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {40-58},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {Telehealth in acute care: Pay parity and patient access},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated approach to improving itinerary completion in
coordinated care networks. <em>MSOM</em>, <em>27</em>(1), 21–39. (<a
href="https://doi.org/10.1287/msom.2022.0649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : Coordinated care network (CCN) is a burgeoning paradigm where patients’ diagnosis and treatment plans are developed based on collaboration between multiple, colocated medical specialties to holistically address patients’ health needs. A primary performance metric for CCNs is how quickly patients can complete their itinerary of appointments at multiple medical services in the network. Rapid completion is critical to care delivery but also presents a major operational challenge. Because information about a patient’s condition and treatment options evolves over the course of the itinerary, care paths are not known a priori. Thus, appointments (except for the first one) cannot be reserved in advance, which may result in significant delays if capacity is not allocated properly. Methodology/results : We study capacity allocation for the patient’s first (root) appointment as the primary operational lever to achieve rapid itinerary completion in CCNs. We develop a novel queueing-based analytical framework to optimize this root appointment allocation, maximizing the proportion of patients completing care by prespecified deadlines. Our framework accounts for the complex interactions among all patients in the network through the blocking process, which contrasts with conventional siloed planning. We provide an exact characterization of the itinerary time and develop a mean-field approximation with convergence guarantees that permits tractable solutions for large-scale network problems. In a simulation case study of Mayo Clinic, our solution improves on-time completion from 60% under the current plan to more than 93%. Managerial implications : We demonstrate that root appointment allocation is a multifaceted problem and that ignoring any of those facets can lead to poor performance. Simultaneously accounting for all of these complexities makes manual template design or traditional optimization methods inadequate, highlighting the significance of our integrated approach. Supplemental Material: The online appendix is available at https://doi.org/10.1287/msom.2022.0649 .},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2022.0649},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {21-39},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {An integrated approach to improving itinerary completion in coordinated care networks},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OM forum—asset-level perspectives on the clean energy
transition. <em>MSOM</em>, <em>27</em>(1), 8–20. (<a
href="https://doi.org/10.1287/msom.2024.1421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : The clean energy transition is an important component of the net zero transformation, which aims at averting potentially disastrous climate change related to excessive global warming by practically eliminating new global greenhouse gas emissions within 2050. The resulting move toward decarbonized energy systems is projected to encompass massive investments in assets that employ technologies with low environmental impact, some of which are innovative or yet unproven or even undeveloped, an area known as CleanTech. The typical analyses that are informing the dialogue on this process in practice provide insights into decarbonization paths at the technology level. In contrast, managers need to make decisions about assets. Methodology/results : Taking an asset-level view of the clean energy transition that complements its technology outlook, this essay provides a concise entry point into this topic based on both the practitioner literature and personal reflections, highlighting CleanTech innovations and related operational considerations in carbon capture, use, and storage; sustainable fuels; hydrogen; electricity; and concomitant government support activities. Managerial implications : Further, this work advocates for potential CleanTech research or teaching pursuits, especially ones aimed at directly supporting managerial decision making, for which it provides an example from the literature.},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2024.1421},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {8-20},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {OM Forum—Asset-level perspectives on the clean energy transition},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OM forum—operations management research: Relevance and
impact. <em>MSOM</em>, <em>27</em>(1), 1–7. (<a
href="https://doi.org/10.1287/msom.2023.0691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem definition : A fundamental issue faced by operations management researcher relates to striking the right balance between rigor and relevance in their work. Another important aspect of operations management research relates to influencing and positively impacting businesses and society at large. We constantly struggle to achieve these objectives. Methodology/results : This MSOM Fellow forum article discusses key opportunities for increasing the relevance and impact of Operations Management research. In particular, it highlights two major areas: technology enabled operations and society and operations where unique opportunities exist for the field to make lasting contributions to business and society. Managerial implications : It concludes with a menu of approaches to enhance practical impact of our research.},
  archive      = {J_MSOM},
  doi          = {10.1287/msom.2023.0691},
  journal      = {Manufacturing &amp; Service Operations Management},
  month        = {1-2},
  number       = {1},
  pages        = {1-7},
  shortjournal = {Manuf. Serv. Oper. Manag.},
  title        = {OM Forum—Operations management research: Relevance and impact},
  volume       = {27},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="or---29">OR - 29</h2>
<ul>
<li><details>
<summary>
(2025). On the optimality of greedy policies in dynamic matching.
<em>OR</em>, <em>73</em>(1), 560–582. (<a
href="https://doi.org/10.1287/opre.2021.0596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study centralized dynamic matching markets with finitely many agent types and heterogeneous match values. A network topology describes the pairs of agent types that can form a match and the value generated from each match. A matching policy is hindsight optimal if the policy can (nearly) maximize the total value simultaneously at all times. We find that suitably designed greedy policies are hindsight optimal in two-way matching networks. This implies that there is essentially no positive externality from having agents waiting to form future matches. We first show that the greedy longest-queue policy with a minor variation is hindsight optimal. Importantly, the policy is greedy relative to a residual network, which includes only nonredundant matches with respect to the static optimal matching rates. Moreover, when the residual network is acyclic (e.g., as in two-sided networks), we prescribe a greedy static priority policy that is also hindsight optimal. The priority order of this policy is robust to arrival rate perturbations that do not alter the residual network. Hindsight optimality is closely related to the lengths of type-specific queues. Queue lengths cannot be smaller (in expectation) than of the order of ϵ − 1 , where ϵ is the general position gap that quantifies the stability in the network. The greedy longest-queue policy achieves this lower bound. Funding: This work was supported by National Science Foundation (CMMI-2010940) and U.S. Department of Defense (STTR A18B-T007).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0596},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {560-582},
  shortjournal = {Oper. Res.},
  title        = {On the optimality of greedy policies in dynamic matching},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape-constrained regression using sum of squares
polynomials. <em>OR</em>, <em>73</em>(1), 543–559. (<a
href="https://doi.org/10.1287/opre.2021.0383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hierarchy of semidefinite programs (SDPs) for the problem of fitting a shape-constrained (multivariate) polynomial to noisy evaluations of an unknown shape-constrained function. These shape constraints include convexity or monotonicity over a box. We show that polynomial functions that are optimal to any fixed level of our hierarchy form a consistent estimator of the underlying shape-constrained function. As a by-product of the proof, we establish that sum of squares-convex polynomials are dense in the set of polynomials that are convex over an arbitrary box. A similar sum-of-squares-type density result is established for monotone polynomials. In addition, we classify the complexity of convex and monotone polynomial regression as a function of the degree of the polynomial regressor. Whereas our results show NP-hardness of these problems for degree three or larger, we can check numerically that our SDP-based regressors often achieve a similar training error at low levels of the hierarchy. Finally, on the computational side, we present an empirical comparison of our SDP-based convex regressors with the convex least squares estimator introduced in Hildreth [ Hildreth C (1954) Point estimates of ordinates of concave functions. J. Amer. Statist. Assoc. 49(267):598–619] and Holloway [ Holloway CA (1979) On the estimation of convex functions. Oper. Res. 27(2):401–407] and show that our regressor is valuable in settings in which the number of data points is large and the dimension is relatively small. We demonstrate the performance of our regressor for the problem of computing optimal transport maps in a color transfer task and that of estimating the optimal value function of a conic program. A real-time application of the latter problem to inventory management contract negotiation is presented. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0383 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0383},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {543-559},
  shortjournal = {Oper. Res.},
  title        = {Shape-constrained regression using sum of squares polynomials},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal routing under demand surges: The value of future
arrival rates. <em>OR</em>, <em>73</em>(1), 510–542. (<a
href="https://doi.org/10.1287/opre.2022.0282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the growing availability of advanced demand forecast tools, we study how to use future demand information in designing routing strategies in queueing systems under demand surges. We consider a parallel server system operating in a nonstationary environment with general time-varying arrival rates. Servers are cross-trained to help nonprimary customer classes during demand surges. However, such flexibility comes with various operational costs, such as a loss of efficiency and inconvenience in coordination. We characterize how to incorporate the future arrival information into the routing policy to balance the tradeoff between various costs and quantify the benefit of doing so. Based on transient fluid control analysis, we develop a two-stage index-based look-ahead policy that explicitly takes the overflow costs and future arrival rates into account. The policy has an interpretable structure, is easy to implement and is adaptive when the future arrival information is inaccurate. In the special case of the N-model, we prove that this policy is asymptotically optimal even in the presence of certain prediction errors in the demand forecast. We substantiate our theoretical analysis with extensive numerical experiments, showing that our policy achieves superior performance compared with other benchmark policies (i) in complicated parallel server systems and (ii) when the demand forecast is imperfect with various forms of prediction errors. Funding: This work was supported by the National Science Foundation Civil, Mechanical, and Manufacturing Innovation [Grant 1944209]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0282 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0282},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {510-542},
  shortjournal = {Oper. Res.},
  title        = {Optimal routing under demand surges: The value of future arrival rates},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal routing to parallel servers in heavy traffic.
<em>OR</em>, <em>73</em>(1), 483–509. (<a
href="https://doi.org/10.1287/opre.2022.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a system with heterogeneous parallel servers, each with an infinite waiting room. Upon arrival, a job is routed to the queue of one of the servers, possibly depending on the dynamic state information such as the real-time queue lengths, the arrival, and service history of jobs. The objective is to find the routing policy that best uses the available state information to minimize the expected stationary queue length. In this paper, we establish the diffusion limit for the round-robin policy (respectively, arrival-chasing policy, service-chasing policy), and show that with properly chosen parameters, it achieves the optimal performance asymptotically within the class of admissible policies that require no state information (respectively, require arrival history, service history). Like the jointhe-shortest-queue and the balanced routing policies that use real-time queue length information, the optimal service-chasing policy is also asymptotically optimal over all admissible policies. Further analysis of the diffusion limits yields a number of insights into the performance of these routing policies and reveals the value of various state information. We numerically demonstrate the effectiveness of the estimators derived from the diffusion limits for the policies being studied and obtain interesting observations. We also address the problem of interchange of limits under the aforementioned policies, which justifies the stationary performance of the diffusion limit as a valid approximation to that of the original system under respective policies. Methodologically, this study contributes to the application of the BIGSTEP method for constructing control policy to optimize stationary performance and the recipe for justifying the interchange of limits in the heavy traffic analysis of stochastic processing networks. Funding: This work was supported by the Research Grants Council, University Grants Committee [GRF Grant 15501421 and NSFC/RGC Grant N_PolyU590/22]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0055 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0055},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {483-509},
  shortjournal = {Oper. Res.},
  title        = {Optimal routing to parallel servers in heavy traffic},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The when and how of delegated search. <em>OR</em>,
<em>73</em>(1), 461–482. (<a
href="https://doi.org/10.1287/opre.2019.0498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms often outsource search processes, such as the acquisition of real estate, new technologies, or talent. To ensure the efficacy of such delegated search, firms need to carefully design incentive contracts to attenuate the ill effects of agency issues. We model this problem using a dynamic principal-agent framework, embedding the standard sequential search model. The optimal contract pays the agent a fixed per-period fee plus a bonus for finding a suitable alternative. The bonus size is defined a priori and decreases over time, whereas the range of values deemed suitable expands over time. If the principal is unable to contract on the value of the delivered alternatives, the optimal contract consists of two parts. Early in the search process, the agent is granted a small bonus for every alternative brought to the principal, irrespective of whether the principal accepts it; late in the search process, the agent is awarded a comparatively larger bonus, which is decreasing in time, but only if the principal accepts the alternative. We also consider situations where the principal chooses between searching in house and outsourcing. This decision is shown to hinge on the principal’s trade-off between speed and quality. The age-old aphorism “if you want it done right, do it yourself” holds, as in-house search is optimal for a principal who prioritizes quality. Yet, in the context of our model, we also establish an addendum: “If you want it done fast, hire someone else to do it.” Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2019.0498 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.0498},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {461-482},
  shortjournal = {Oper. Res.},
  title        = {The when and how of delegated search},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic pricing in volatile markets. <em>OR</em>,
<em>73</em>(1), 444–460. (<a
href="https://doi.org/10.1287/opre.2021.0550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic entry deterrence through limit pricing in markets subject to persistent demand shocks. An incumbent is privately informed about its costs, high or low, and can deter a Bayesian potential entrant by setting its prices strategically. The entrant can irreversibly enter the market at any time for a fixed cost, earning a payoff that depends on the market conditions and the incumbent’s unobserved type. Market demand evolves as a geometric Brownian motion. When market demand is low, entry becomes a distant threat, so there is little benefit to further deterrence, and, in equilibrium, a weak incumbent becomes tempted to reveal itself by raising its prices. We characterize a unique equilibrium in which the entrant enters when market demand is sufficiently high (relative to the incumbent’s current reputation), and the weak incumbent mixes over revealing itself when market demand is sufficiently low. In this equilibrium, pricing and entry decisions exhibit path dependence, depending not only on the market’s current size, but also its historical minimum. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2021.0550 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0550},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {444-460},
  shortjournal = {Oper. Res.},
  title        = {Strategic pricing in volatile markets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual inverse optimization: Offline and online
learning. <em>OR</em>, <em>73</em>(1), 424–443. (<a
href="https://doi.org/10.1287/opre.2021.0369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problems of offline and online contextual optimization with feedback information, where instead of observing the loss, we observe, after the fact, the optimal action an oracle with full knowledge of the objective function would have taken. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. In the offline setting, the decision maker has information available from past periods and needs to make one decision, whereas in the online setting, the decision maker optimizes decisions dynamically over time based a new set of feasible actions and contextual functions in each period. For the offline setting, we characterize the optimal minimax policy, establishing the performance that can be achieved as a function of the underlying geometry of the information induced by the data. In the online setting, we leverage this geometric characterization to optimize the cumulative regret. We develop an algorithm that yields the first regret bound for this problem that is logarithmic in the time horizon. Finally, we show via simulation that our proposed algorithms outperform previous methods from the literature. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.0369 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0369},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {424-443},
  shortjournal = {Oper. Res.},
  title        = {Contextual inverse optimization: Offline and online learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Competitive algorithms for the online minimum peak job
scheduling. <em>OR</em>, <em>73</em>(1), 408–423. (<a
href="https://doi.org/10.1287/opre.2021.0080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a fundamental online scheduling problem called the minimum peak job scheduling (MPJS) problem. In this problem, there is a sequence of arriving jobs, each with a specified required scheduled time for one unit of a scarce and reusable resource. The goal is to schedule each job upon arrival within a scheduling interval to minimize the resulting peak utilization (i.e., the maximum number of units used simultaneously throughout the entire scheduling interval). The MPJS problem captures many practical settings of real-time appointment scheduling. Its offline version where all jobs are known in advance is equivalent to the well-known bin-packing problem, where jobs correspond to items and the unit resource is a bin. However, the online variant of MPJS allows additional flexibility in that initially, one only commits to the scheduling time, but the allocation to the resources can be done later. In the bin-packing problem, this corresponds to the ability to move items across bins. Some relaxed versions of online bin-packing problems have already been studied, but none fundamentally capture the MPJS model studied in this paper. The paper describes the first competitive online algorithm to the MPJS problem called the harmonic rematching (HR) algorithm. The analysis shows that the HR algorithm has an asymptotic competitive ratio below 1.5. The fact that the current best lower bound on randomized online algorithms for the bin-packing problem is 1.536 highlights the fundamental difference between these two related models. Funding: The work of C. Escribe was partially supported by the Centre for Humane Innovations in Clinical Care [Grant 4000093665]. The work of M. Hu was partially funded by an MGH-MIT Fellowship. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0080 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0080},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {408-423},
  shortjournal = {Oper. Res.},
  title        = {Competitive algorithms for the online minimum peak job scheduling},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process-based random search for continuous
optimization via simulation. <em>OR</em>, <em>73</em>(1), 385–407. (<a
href="https://doi.org/10.1287/opre.2021.0303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random search is an important category of algorithms to solve continuous optimization via simulation problems. To design an efficient random search algorithm, the handling of the triple “E” (i.e., exploration, exploitation and estimation) is critical. The first two E’s refer to the design of sampling distribution to balance explorative and exploitative searches, whereas the third E refers to the estimation of objective function values based on noisy simulation observations. In this paper, we propose a class of Gaussian process-based random search (GPRS) algorithms, which provide a new framework to handle the triple “E.” In each iteration, algorithms under the framework build a Gaussian process surrogate model to estimate the objective function based on single observation of each sampled solution and randomly sample solutions from a lower-bounded sampling distribution. Under the assumption of heteroscedastic and known simulation noise, we prove the global convergence of GPRS algorithms. Moreover, for Gaussian processes having continuously differentiable sample paths, we show that the rate of convergence of GPRS algorithms can be no slower than O p ( n − 1 / ( d + 2 ) ) . Then, we introduce a specific GPRS algorithm to show how to design an integrated GPRS algorithm with adaptive sampling distributions and how to implement the algorithm efficiently. Numerical experiments show that the algorithm has good performances, even for problems where the variances of simulation noises are unknown. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72031007, 72091211, 71931007]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0303 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0303},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {385-407},
  shortjournal = {Oper. Res.},
  title        = {Gaussian process-based random search for continuous optimization via simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expanding service capabilities through an on-demand
workforce. <em>OR</em>, <em>73</em>(1), 363–384. (<a
href="https://doi.org/10.1287/opre.2021.0651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An on-demand workforce can greatly benefit a traditional call center by allowing it to adjust its service capacity on demand quickly. Despite its conceptual elegance, the operationalization of this process is challenging due to the various sources of randomness involved. The purpose of this paper is to help call centers enhance service levels while keeping operating expenses low by taking advantage of an on-call pool of temporary agents in day-to-day operations. For that purpose, we develop a two-stage decision model in which the first stage seeks the optimal mix of permanent and on-call staff, and the second stage seeks a joint on-demand staffing and call scheduling policy to minimize the associated cost given the base staffing level and the size of the on-call pool. Because the exact analysis of the two-stage decision model seems analytically intractable, we resort to an approximation in a suitable asymptotic regime. In that regime, we characterize the system dynamics of the service operation and derive an optimal joint on-demand staffing and call scheduling policy for the second-stage problem, which in turn is used to find an approximate solution to the first-stage problem. In particular, the derived policy for the second-stage problem involves tapping into the on-call pool to procure a team of on-demand agents when the number of calls to be processed exceeds a certain threshold and dismissing them when it falls below another threshold; additionally, the call scheduling rule shows an unusual pattern due to the interplay between staffing and scheduling decisions. Extensive numerical studies under realistic parameter settings show that the solution approach we propose can achieve significant cost savings. Funding: W. Liu has been supported by the President’s Graduate Fellowship at the National University of Singapore. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0651 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0651},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {363-384},
  shortjournal = {Oper. Res.},
  title        = {Expanding service capabilities through an on-demand workforce},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scores for multivariate distributions and level sets.
<em>OR</em>, <em>73</em>(1), 344–362. (<a
href="https://doi.org/10.1287/opre.2020.0365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasts of multivariate probability distributions are required for a variety of applications. Scoring rules enable the evaluation of forecast accuracy and comparison between forecasting methods. We propose a theoretical framework for scoring rules for multivariate distributions that encompasses the existing quadratic score and multivariate continuous ranked probability score. We demonstrate how this framework can be used to generate new scoring rules. In some multivariate contexts, it is a forecast of a level set that is needed, such as a density level set for anomaly detection or the level set of the cumulative distribution as a measure of risk. This motivates consideration of scoring functions for such level sets. For univariate distributions, it is well established that the continuous ranked probability score can be expressed as the integral over a quantile score. We show that, in a similar way, scoring rules for multivariate distributions can be decomposed to obtain scoring functions for level sets. Using this, we present scoring functions for different types of level sets, including density level sets and level sets for cumulative distributions. To compute the scores, we propose a simple numerical algorithm. We perform a simulation study to support our proposals, and we use real data to illustrate usefulness for forecast combining and conditional value at risk estimation. Funding: The work of S. Li was supported by the National Natural Science Foundation of China [Grant 12201399] and the Shanghai Frontier Research Institute for Modern Analysis.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0365},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {344-362},
  shortjournal = {Oper. Res.},
  title        = {Scores for multivariate distributions and level sets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving efficiency in black-box simulation of distribution
tails with self-structuring importance samplers. <em>OR</em>,
<em>73</em>(1), 325–343. (<a
href="https://doi.org/10.1287/opre.2021.0331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel importance sampling (IS) scheme for estimating distribution tails of performance measures modeled with a rich set of tools, such as linear programs, integer linear programs, piecewise linear/quadratic objectives, feature maps specified with deep neural networks, etc. The conventional approach of explicitly identifying efficient changes of measure suffers from feasibility and scalability concerns beyond highly stylized models because of their need to be tailored intricately to the objective and the underlying probability distribution. This bottleneck is overcome in the proposed scheme with an elementary transformation that is capable of implicitly inducing an effective IS distribution in a variety of models by replicating the concentration properties observed in less rare samples. This novel approach is guided by developing a large deviations principle that brings out the phenomenon of self-similarity of optimal IS distributions. The proposed sampler is the first to attain asymptotically optimal variance reduction across a spectrum of multivariate distributions despite being oblivious to the specifics of the underlying model. Its applicability is illustrated with contextual shortest-path and portfolio credit risk models informed by neural networks. Funding: This work was supported by the Singapore Ministry of Education Academic Research Fund [Grant MOE2019-T2-2-163]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0331 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0331},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {325-343},
  shortjournal = {Oper. Res.},
  title        = {Achieving efficiency in black-box simulation of distribution tails with self-structuring importance samplers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projective hedging algorithms for multistage stochastic
programming, supporting distributed and asynchronous implementation.
<em>OR</em>, <em>73</em>(1), 311–324. (<a
href="https://doi.org/10.1287/opre.2022.0228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a decomposition algorithm for multistage stochastic programming that resembles the progressive hedging method of Rockafellar and Wets but is provably capable of several forms of asynchronous operation. We derive the method from a class of projective operator splitting methods fairly recently proposed by Combettes and Eckstein, significantly expanding the known applications of those methods. Our derivation assures convergence for convex problems whose feasible set is compact, subject to some standard regularity conditions and a mild “fairness” condition on subproblem selection. The method’s convergence guarantees are deterministic and do not require randomization, in contrast to other proposed asynchronous variations of progressive hedging. Computational experiments described in an online appendix show the method to outperform progressive hedging on large-scale problems in a highly parallel computing environment. Funding: This work was supported by the National Science Foundation Directorate of Computer and Information Science and Engineering [Grant CCF-1617617] and U.S. Department of Energy [Office of Electricity’s Advanced Grid Modeling program]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0228 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0228},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {311-324},
  shortjournal = {Oper. Res.},
  title        = {Projective hedging algorithms for multistage stochastic programming, supporting distributed and asynchronous implementation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior-aware queueing: The finite-buffer setting with many
strategic servers. <em>OR</em>, <em>73</em>(1), 290–310. (<a
href="https://doi.org/10.1287/opre.2023.2487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service system design is often informed by queueing theory. Traditional queueing theory assumes that servers work at constant speeds. That is reasonable in computer science and manufacturing contexts. However, servers in service systems are people, and in contrast to machines, the incentives created by design decisions influence their work speeds. We study how server work speed is affected by managerial decisions concerning (i) how many servers to staff and how much to pay them and (ii) whether and when to turn away customers in the context of many-server queues with finite or infinite buffers ( M / M / N / k with k ∈ Z + ∪ { ∞ } ) in which the work speeds emerge as the solution to a noncooperative game. We show that a symmetric equilibrium always exists in a loss system ( N = k ) and provide conditions for equilibrium existence in a single-server system ( N = 1). For the general M / M / N / k system, we provide a sufficient condition for the existence of a solution to the first-order condition and bounds on such a solution; however, showing that it is an equilibrium is challenging because of the existence of multiple local maxima in the utility function. Nevertheless, in an asymptotic regime in which demand becomes large, the utility function becomes concave, allowing us to characterize underloaded, critically loaded, and overloaded equilibria. Funding: This work was supported in part by funding from the Social Sciences and Humanities Research Council of Canada [Grant 430-2020-00334] and the Charles M. Harper Faculty Fellowship at the University of Chicago Booth School of Business. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2487 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2487},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {290-310},
  shortjournal = {Oper. Res.},
  title        = {Behavior-aware queueing: The finite-buffer setting with many strategic servers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—online matching with bayesian rewards.
<em>OR</em>, <em>73</em>(1), 278–289. (<a
href="https://doi.org/10.1287/opre.2021.0499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study in this paper an online matching problem where a central platform needs to match a number of limited resources to different groups of users that arrive sequentially over time. The reward of each matching option depends on both the type of resource and the time period the user arrives. The matching rewards are assumed to be unknown but drawn from probability distributions that are known a priori. The platform then needs to learn the true rewards online based on real-time observations of the matching results. The goal of the central platform is to maximize the total reward from all of the matchings without violating the resource capacity constraints. We formulate this matching problem with Bayesian rewards as a Markovian multiarmed bandit problem with budget constraints, where each arm corresponds to a pair of a resources and a time period. We devise our algorithm by first finding policies for each single arm separately via a relaxed linear program and then “assembling” these policies together through judicious selection criteria and well-designed pulling orders. We prove that the expected reward of our algorithm is at least 1 2 ( 2 − 1 ) of the expected reward of an optimal algorithm. Funding: The authors thank the Massachusetts Institute of Technology (MIT)-IBM partnership in Artificial Intelligence and the MIT Data Science Laboratory for support. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0499 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0499},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {278-289},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Online matching with bayesian rewards},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—a stationary infinite-horizon supply contract
under asymmetric inventory information. <em>OR</em>, <em>73</em>(1),
270–277. (<a href="https://doi.org/10.1287/opre.2020.0495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a decentralized supply chain in which a supplier sells goods to a retailer facing general random demand over an infinite horizon. The retailer satisfies the demand to the extent of the inventory on hand. The retailer has private information about the retailer’s stock in each period, and the supplier offers the retailer a supply contract menu to account for the information asymmetry. We obtain a necessary condition for optimizing a long-term stationary truth-telling contract under general demand and belief distributions. We apply it to a batch-order contract, which replenishes a prespecified inventory quantity for a fixed payment in each period only when the retailer’s beginning inventory becomes zero. Methodologically, we formulate the supplier’s contract design as a calculus of variations problem and apply the concept of Gâteaux derivative to obtain these results. This methodology can potentially be applied to other dynamic contracting problems. Funding: A. Bensoussan acknowledges support from the National Science Foundation [Grant NSF-DMS 220 47 95]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2020.0495 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0495},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {270-277},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A stationary infinite-horizon supply contract under asymmetric inventory information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—conic mixed-binary sets: Convex hull
characterizations and applications. <em>OR</em>, <em>73</em>(1),
251–269. (<a href="https://doi.org/10.1287/opre.2020.0827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a general conic mixed-binary set where each homogeneous conic constraint j involves an affine function of independent continuous variables and an epigraph variable associated with a nonnegative function, f j , of common binary variables. Sets of this form naturally arise as substructures in a number of applications, including mean-risk optimization, chance-constrained problems, portfolio optimization, lot sizing and scheduling, fractional programming, variants of the best subset selection problem, a class of sparse semidefinite programs, and distributionally robust chance-constrained programs. We give a convex hull description of this set that relies on simultaneous characterization of the epigraphs of f j ’s, which is easy to do when all functions f j ’s are submodular. Our result unifies and generalizes an existing result in two important directions. First, it considers multiple general convex cone constraints instead of a single second-order cone type constraint. Second, it takes arbitrary nonnegative functions instead of a specific submodular function obtained from the square root of an affine function. We close by demonstrating the applicability of our results in the context of a number of problem classes. Funding: The research is supported, in part, by ONR [Grants N00014-19-1-2321 and N00014-22-1-2602], AFOSR [Grant FA9550-22-1-0365], the Institute for Basic Science [IBS-R029-C1, Y2], the FOUR Brain Korea 21 Program [NRF-5199990113928], the National Research Foundation of Korea [NRF-2022M3J6A1063021], the KAIST Starting Fund [KAIST-G04220016], NSF [Grant CMMI 1454548], and Early Postdoc Mobility Fellowship SNSF [Grant P2ELP2_195149].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0827},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {251-269},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Conic mixed-binary sets: Convex hull characterizations and applications},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical note—a new rate-optimal sampling allocation for
linear belief models. <em>OR</em>, <em>73</em>(1), 239–250. (<a
href="https://doi.org/10.1287/opre.2022.2337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a new optimal sampling budget allocation for belief models based on linear regression with continuous covariates, where the expected response is interpreted as the value of the covariate vector, and an “error” occurs if a lower-valued vector is falsely identified as being better than a higher-valued one. Our allocation optimizes the rate at which the probability of error converges to zero using a large deviations theoretic characterization. This is the first large deviations-based optimal allocation for continuous decision spaces, and it turns out to be considerably simpler and easier to implement than allocations that use discretization. We give a practicable sequential implementation and illustrate its empirical potential. Funding: This work was supported by the National Science Foundation [Grant CMMI-2112828].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2337},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {239-250},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A new rate-optimal sampling allocation for linear belief models},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal no-regret learning in repeated first-price auctions.
<em>OR</em>, <em>73</em>(1), 209–238. (<a
href="https://doi.org/10.1287/opre.2020.0282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study online learning in repeated first-price auctions where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid to maximize the cumulative payoff. To achieve this goal, the bidder faces censored feedback: If the bidder wins the bid, then the bidder is not able to observe the highest bid of the other bidders, which we assume is i.i.d. drawn from an unknown distribution. In this paper, we develop the first learning algorithm that achieves a near-optimal O ˜ ( T ) regret bound, by exploiting two structural properties of first-price auctions, that is, the specific feedback structure and payoff function. We first formulate the feedback structure in first-price auctions as partially ordered contextual bandits, a combination of the graph feedback across actions (bids), the cross-learning across contexts (private values), and a partial order over the contexts. We establish both strengths and weaknesses of this framework by showing a curious separation that a regret nearly independent of the action/context sizes is possible under stochastic contexts but is impossible under adversarial contexts. In particular, this framework leads to an O ( T log 2.5 T ) regret for first-price auctions when the bidder’s private values are independent and identically distributed. Despite the limitation of this framework, we further exploit the special payoff function of first-price auctions to develop a sample-efficient algorithm even in the presence of adversarially generated private values. We establish an O ( T log 3 T ) regret bound for this algorithm, hence providing a complete characterization of optimal learning guarantees for first-price auctions. Funding: This project was supported in part by the National Science Foundation [Awards CCF-2106467 and CCF-2106508]. Y. Han and T. Weissman were partially supported by the Yahoo Faculty Research and Engagement Program.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0282},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {209-238},
  shortjournal = {Oper. Res.},
  title        = {Optimal no-regret learning in repeated first-price auctions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to persuade on the fly: Robustness against
ignorance. <em>OR</em>, <em>73</em>(1), 194–208. (<a
href="https://doi.org/10.1287/opre.2021.0529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by information sharing in online platforms, we study repeated persuasion between a sender and a stream of receivers, where, at each time, the sender observes a payoff-relevant state drawn independently and identically from an unknown distribution and shares state information with the receivers, who each choose an action. The sender seeks to persuade the receivers into taking actions aligned with the sender’s preference by selectively sharing state information. However, in contrast to the standard models, neither the sender nor the receivers know the distribution, and the sender has to persuade while learning the distribution on the fly. We study the sender’s learning problem of making persuasive action recommendations to achieve low regret against the optimal persuasion mechanism with the knowledge of the distribution. To do this, we first propose and motivate a persuasiveness criterion for the unknown distribution setting that centers robustness as a requirement in the face of uncertainty. Our main result is an algorithm that, with high probability, is robustly persuasive and achieves O ( T log T ) regret, where T is the horizon length. Intuitively, at each time, our algorithm maintains a set of candidate distribution and chooses a signaling mechanism that is simultaneously persuasive for all of them. Core to our proof is a tight analysis about the cost of robust persuasion, which may be of independent interest. We further prove that this regret order is optimal (up to logarithmic terms) by showing that no algorithm can achieve regret better than Ω ( T ) . Funding: Y. Zu and K. Iyer gratefully acknowledge partial support from the National Science Foundation (NSF) Division of Civil, Mechanical, and Manufacturing Innovation [Grant CMMI-2002156]. H. Xu is supported by the NSF Division of Computing and Communication Foundations [Award CCF-2303372], the Army Research Office [Award W911NF-23-1-0030], and a Google Faculty Research Award. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0529 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0529},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {194-208},
  shortjournal = {Oper. Res.},
  title        = {Learning to persuade on the fly: Robustness against ignorance},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pricing optimal outcomes in coupled and non-convex markets:
Theory and applications to electricity markets. <em>OR</em>,
<em>73</em>(1), 178–193. (<a
href="https://doi.org/10.1287/opre.2023.0401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world markets, participants have non-convex preferences, and the allocation problem needs to consider complex constraints. Electricity markets are a prime example, but similar problems appear in many markets, which has led to a growing literature on market design. Competitive equilibrium does not generally exist in such markets. Today, power markets use heuristic pricing rules based on the dual of a relaxed allocation problem. With increasing levels of renewables, these rules have come under scrutiny as they lead to high out-of-market side payments to some participants and inadequate congestion signals. We show that existing pricing heuristics optimize specific design goals that can be conflicting. The tradeoffs can be substantial, and we establish that the design of pricing rules is fundamentally a multiobjective optimization problem addressing different incentives. In addition to traditional multiobjective optimization techniques that involve weighting individual objectives, we introduce a novel parameter-free pricing rule that minimizes incentives for market participants to deviate locally. Our theoretical and experimental findings show how the new pricing rule capitalizes on the upsides of existing pricing rules under scrutiny today. It leads to prices that incur low make-whole payments while providing adequate congestion signals and low lost opportunity costs. Our suggested pricing rule does not require weighing objectives, it is computationally scalable, and balances tradeoffs in a principled manner, addressing a critical policy issue in electricity markets. Funding: The financial support from the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) [Grant BI 1057/9-1] is gratefully acknowledged. Supplemental Material: The computer code and data that supports the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0401 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0401},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {178-193},
  shortjournal = {Oper. Res.},
  title        = {Pricing optimal outcomes in coupled and non-convex markets: Theory and applications to electricity markets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Price interpretability of prediction markets: A convergence
analysis. <em>OR</em>, <em>73</em>(1), 157–177. (<a
href="https://doi.org/10.1287/opre.2022.0417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction markets are long known for prediction accuracy. This study systematically explores the fundamental properties of prediction markets, addressing questions about their information aggregation process and the factors contributing to their remarkable efficacy. We propose a novel multivariate utility–based mechanism that unifies several existing automated market-making schemes. Using this mechanism, we establish the convergence results for markets comprised of risk-averse traders who have heterogeneous beliefs and repeatedly interact with the market maker. We demonstrate that the resulting limiting wealth distribution aligns with the Pareto efficient frontier defined by the utilities of all market participants. With the help of this result, we establish analytical and numerical results for the limiting price in different market models. Specifically, we show that the limiting price converges to the geometric mean of agent beliefs in exponential utility-based markets. In risk measure-based markets, we construct a family of risk measures that satisfy the convergence criteria and prove that the price converges to a unique level represented by the weighted power mean of agent beliefs. In broader markets with constant relative risk aversion utilities, we reveal that the limiting price can be characterized by systems of equations that encapsulate agent beliefs, risk parameters, and wealth. Despite the impact of traders’ trading sequences on the limiting price, we establish a price invariance result for markets with a large trader population. Using this result, we propose an efficient approximation scheme for the limiting price. Numerical experiments demonstrate that the accuracy of this approximation scheme outperforms existing approximation methods across various scenarios. Our findings serve to aid market designers in better tailoring and adjusting the market-making mechanism for more effective opinion elicitation. Funding: This work was supported by the National Natural Science Foundation of China [Grants 71671045, 71971132, 72150002, 72201067, and 72394361], the InnoHK initiative of the Government of the HKSAR, Laboratory for AI-Powered Financial Technologies, the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [Grant 2023B1212010001], the Shanghai Research Center for Data Science and Decision Technology, and the Key Laboratory of Interdisciplinary Research of Computation and Economics, Ministry of Education, Shanghai University of Finance and Economics. Supplemental Material: The computer code and data that supports the findings of this study is available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0417 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0417},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {157-177},
  shortjournal = {Oper. Res.},
  title        = {Price interpretability of prediction markets: A convergence analysis},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of lookahead and approximate policy evaluation in
reinforcement learning with linear value function approximation.
<em>OR</em>, <em>73</em>(1), 139–156. (<a
href="https://doi.org/10.1287/opre.2022.0357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function approximation is widely used in reinforcement learning to handle the computational difficulties associated with very large state spaces. However, function approximation introduces errors that may lead to instabilities when using approximate dynamic programming techniques to obtain the optimal policy. Therefore, techniques such as lookahead for policy improvement and m -step rollout for policy evaluation are used in practice to improve the performance of approximate dynamic programming with function approximation. We quantitatively characterize the impact of lookahead and m -step rollout on the performance of approximate dynamic programming (DP) with function approximation. (i) Without a sufficient combination of lookahead and m -step rollout, approximate DP may not converge. (ii) Both lookahead and m -step rollout improve the convergence rate of approximate DP. (iii) Lookahead helps mitigate the effect of function approximation and the discount factor on the asymptotic performance of the algorithm. Our results are presented for two approximate DP methods: one that uses least-squares regression to perform function approximation and another that performs several steps of gradient descent of the least-squares objective in each iteration. Funding: The research presented here was supported in part by a grant from Sandia National Labs and the NSF [Grants CCF 1934986, CCF 2207547, CNS 2106801], ONR [Grant N00014-19-1-2566], and ARO [Grant W911NF-19-1-0379].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0357},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {139-156},
  shortjournal = {Oper. Res.},
  title        = {The role of lookahead and approximate policy evaluation in reinforcement learning with linear value function approximation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning for constrained assortment optimization
under markov chain choice model. <em>OR</em>, <em>73</em>(1), 109–138.
(<a href="https://doi.org/10.1287/opre.2022.0693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a dynamic assortment selection problem where arriving customers make purchase decisions among offered products from a universe of products under a Markov chain choice (MCC) model. The retailer only observes the assortment and the customer’s single choice per period. Given limited display capacity, resource constraints, and no a priori knowledge of problem parameters, the retailer’s objective is to sequentially learn the choice model and optimize cumulative revenues over a finite selling horizon. We develop a fast linear system based explore-then-commit (FastLinETC for short) learning algorithm that balances the tradeoff between exploration and exploitation. The algorithm can simultaneously estimate the arrival and transition probabilities in the MCC model by solving a linear system of equations and determining the near-optimal assortment based on these estimates. Furthermore, our consistent estimators offer superior computational times compared with existing heuristic estimation methods, which often suffer from inconsistency or a significant computational burden. Funding: The research of Q. Luo is partially supported by the National Science Foundation [Grant CMMI-2308750]. The research of Z. Huang is partially supported by the Shanghai Sailing Program [Grant 22YF1451100 and the Fundamental Research Funds for the Central Universities]. The research of C. Shi is partially supported by Amazon [Research Award].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0693},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {109-138},
  shortjournal = {Oper. Res.},
  title        = {Online learning for constrained assortment optimization under markov chain choice model},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone-delivery network for opioid overdose: Nonlinear
integer queueing-optimization models and methods. <em>OR</em>,
<em>73</em>(1), 86–108. (<a
href="https://doi.org/10.1287/opre.2022.0489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new stochastic emergency network design model that uses a fleet of drones to quickly deliver naloxone in response to opioid overdoses. The network is represented as a collection of M / G / K queueing systems in which the capacity K of each system is a decision variable, and the service time is modeled as a decision-dependent random variable. The model is a queuing-based optimization problem which locates fixed (drone bases) and mobile (drones) servers and determines the drone dispatching decisions and takes the form of a nonlinear integer problem intractable in its original form. We develop an efficient reformulation and algorithmic framework. Our approach reformulates the multiple nonlinearities (fractional, polynomial, exponential, factorial terms) to give a mixed-integer linear programming (MILP) formulation. We demonstrate its generalizability and show that the problem of minimizing the average response time of a collection of M / G / K queueing systems with unknown capacity K is always MILP-representable. We design an outer approximation branch-and-cut algorithmic framework that is computationally efficient and scales well. The analysis based on real-life data reveals that drones can in Virginia Beach: (1) decrease the response time by 82%, (2) increase the survival chance by more than 273%, (3) save up to 33 additional lives per year, and (4) provide annually up to 279 additional quality-adjusted life years. Funding: M. A. Lejeune acknowledges the support of the National Science Foundation [Grant ECCS-2114100] and the Office of Naval Research [Grant N00014-22-1-2649]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0489 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0489},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {86-108},
  shortjournal = {Oper. Res.},
  title        = {Drone-delivery network for opioid overdose: Nonlinear integer queueing-optimization models and methods},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the security of united states elections with
robust optimization. <em>OR</em>, <em>73</em>(1), 61–85. (<a
href="https://doi.org/10.1287/opre.2023.0422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For more than a century, election officials across the United States have inspected voting machines before elections using a procedure called logic and accuracy testing (LAT). This procedure consists of election officials casting a test deck of ballots into each voting machine and confirming the machine produces the expected vote total for each candidate. We bring a scientific perspective to LAT by introducing the first formal approach to designing test decks with rigorous security guarantees. Specifically, our approach employs robust optimization to find test decks that are guaranteed to detect any voting machine misconfiguration that would cause votes to be swapped across candidates. Of all the test decks with this security guarantee, our robust optimization problem yields the test deck with the minimum number of ballots, thereby minimizing implementation costs for election officials. To facilitate deployment at scale, we develop a practically efficient exact algorithm for solving our robust optimization problems based on the cutting plane method. In partnership with the Michigan Bureau of Elections, we retrospectively applied our approach to all 6,928 ballot styles from Michigan’s November 2022 general election; this retrospective study reveals that the test decks with rigorous security guarantees obtained by our approach require, on average, only 1.2% more ballots than current practice. Our approach has since been piloted in real-world elections by the Michigan Bureau of Elections as a low-cost way to improve election security and increase public trust in democratic institutions. Funding: This research was supported by the U.S. National Science Foundation [Grant CNS-1518888]. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study are available at https://doi.org/10.1287/opre.2023.0422 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0422},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {61-85},
  shortjournal = {Oper. Res.},
  title        = {Improving the security of united states elections with robust optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preventing price-mediated contagion due to fire sales
externalities: Strategic foundations of macroprudential regulation.
<em>OR</em>, <em>73</em>(1), 40–60. (<a
href="https://doi.org/10.1287/opre.2023.0237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We offer a stress test framework in which interaction between regulated banks occurs through the impact they may have on asset prices when they deleverage. Because banks are constrained to maintain their risk-based capital ratio higher than a threshold, the deleveraging problem yields a generalized game in which the solvency constraint of each bank depends on the decisions of the others. We analyze the game under microprudential but also under macroprudential regulation. Microprudential regulation corresponds to the standard situation in which each bank considers its own solvency constraint, whereas macroprudential regulation is defined as the situation in which each bank faces a systemic constraint in that it must consider the solvency constraints of all the banks. When bankruptcies can be avoided, we show that a Nash equilibrium generically exists under macroprudential regulation, contagion of failures due to fire sales externalities is prevented, whereas it may not exist under microprudential regulation. We eventually analyze the deleveraging problem when bankruptcies cannot be avoided and present additional results. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0237 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0237},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {40-60},
  shortjournal = {Oper. Res.},
  title        = {Preventing price-mediated contagion due to fire sales externalities: Strategic foundations of macroprudential regulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application-driven learning: A closed-loop prediction and
optimization approach applied to dynamic reserves and demand
forecasting. <em>OR</em>, <em>73</em>(1), 22–39. (<a
href="https://doi.org/10.1287/opre.2023.0565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision making is generally modeled as sequential forecast-decision steps with no feedback, following an open-loop approach. For instance, in the electricity sector, system operators use the forecast-decision approach followed by ad hoc rules to determine reserve requirements and biased net load forecasts to guard the system against renewable generation and demand uncertainty. Such procedures lack technical formalism to minimize operating and reliability costs. We present a new closed-loop framework, named application-driven learning, in which the best forecasting model is defined according to a given application cost function. We consider applications in which the decision-making process is driven by two-stage optimization schemes fed by multivariate point forecasts. We present our estimation method as a bilevel optimization problem and prove convergence to the best estimator regarding the expected application cost. We propose two solution methods: an exact method based on the KKT conditions of the second-level problems and a scalable heuristic suitable for decomposition. Thus, we offer an alternative scientifically grounded approach to current ad hoc procedures implemented in industry practices. We test the proposed methodology with real data and large-scale systems with thousands of buses. Results show that the proposed methodology is scalable and consistently performs better than the standard open-loop approach. Funding: J. Dias Garcia was partially supported by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) [Finance Code 001]. A. Street was partially supported by Fundação de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq). T. Homem-de-Mello acknowledges the support of Grant FONDECYT 1221770 from ANID, Chile. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0565 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0565},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {22-39},
  shortjournal = {Oper. Res.},
  title        = {Application-driven learning: A closed-loop prediction and optimization approach applied to dynamic reserves and demand forecasting},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error propagation in asymptotic analysis of the data-driven
(s, s) inventory policy. <em>OR</em>, <em>73</em>(1), 1–21. (<a
href="https://doi.org/10.1287/opre.2020.0568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study periodic review stochastic inventory control in the data-driven setting where the retailer makes ordering decisions based only on historical demand observations without any knowledge of the probability distribution of the demand. Because an ( s , S )-policy is optimal when the demand distribution is known, we investigate the statistical properties of the data-driven ( s , S )-policy obtained by recursively computing the empirical cost-to-go functions. This policy is inherently challenging to analyze because the recursion induces propagation of the estimation error backward in time. In this work, we establish the asymptotic properties of this data-driven policy by fully accounting for the error propagation. In this setting, the empirical cost-to-go functions for the estimated parameters are not i.i.d. sums because of the error propagation. Our main methodological innovation comes from an asymptotic representation for multi-sample U -processes in terms of i.i.d. sums. This representation enables us to apply empirical process theory to derive the influence functions of the estimated parameters and to establish joint asymptotic normality. Based on these results, we also propose an entirely data-driven estimator of the optimal expected cost, and we derive its asymptotic distribution. We demonstrate some useful applications of our asymptotic results, including sample size determination and interval estimation. Funding: This work was supported by Singapore MOE AcRF Tier 2 [A-8001052-00-00] and the National Natural Science Foundation of China [72071138]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2020.0568 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0568},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Oper. Res.},
  title        = {Error propagation in asymptotic analysis of the data-driven (s, s) inventory policy},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="orsc---22">ORSC - 22</h2>
<ul>
<li><details>
<summary>
(2025). Timing is everything: An imprinting framework for the
implications of leader emotional expressions for team member social
worth and performance. <em>ORSC</em>, <em>36</em>(1), 514–546. (<a
href="https://doi.org/10.1287/orsc.2023.17390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leader emotional expressions have profound implications for team members. Research has established that how frequently leaders express positive and negative emotional expressions shapes team member performance through conveying critical social-functional information about team member social worth. Yet, this social-functional approach to emotions has not fully considered how the timing of leader emotional expressions during a team’s lifecycle can also shape the information conveyed to individual team members about their social worth. In this paper, we integrate the social-functional approach to emotions with imprinting theory to propose that the temporal context of leader emotional expressions has performance implications for individual team members through two distinct facets of social worth: respect and status. Specifically, our imprinting framework explains how positive leader emotional expressions during the early team phase have the most beneficial performance implications through imprinting respect in individual team members. We then propose that these positive implications are amplified by more-frequent-than-average negative leader emotional expressions during the midpoint phase. When filtered through earlier positive expressions, negative emotional expressions during the midpoint phase may signal opportunities for respect and status gains rather than respect and status losses. We find general support for our model in a preregistered four-wave longitudinal archival study of 9,968 team members on 234 consulting teams at a leading professional services company and a four-wave longitudinal field study at a NCAA Division 1 sports program including 245 student-athletes and 86 coaches on 20 varsity teams. Our work highlights that the temporal context of leader emotional expressions is an important performance predictor through social worth. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17390 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17390},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {514-546},
  shortjournal = {Organ. Sci.},
  title        = {Timing is everything: An imprinting framework for the implications of leader emotional expressions for team member social worth and performance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trailblazing motivation and marginalized group members:
Changing expectations to pave the way for others. <em>ORSC</em>,
<em>36</em>(1), 477–513. (<a
href="https://doi.org/10.1287/orsc.2021.15624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employees from marginalized groups frequently face low performance expectations based on group membership. Although past research shows several different reactions to these types of expectations, such as stereotype threat and stereotype reactance, scholars still know little about when and why low expectations spur individuals to not only try to prove themselves, but to seek to change expectations and opportunities for others like them. Addressing this discrepancy, I introduce trailblazing motivation, which captures the desire to set new precedents that open doors for others. I integrate self-determination theory and regulatory focus theory to identify group-based low expectations, moderated by a sense of belonging with one’s broad marginalized group, and core self-evaluations as key antecedents of trailblazing motivation. I hypothesize that trailblazing motivation will lead to not only greater persistence in one’s work—as with stereotype reactance—but also potentially riskier behaviors aimed at changing expectations for one’s broad group on a larger scale, including advocacy for other marginalized group members and diversity, equity, and inclusion–related issue selling. I test and find support for these hypotheses across a time-lagged survey study and a preregistered experiment. I also establish discriminant validity for trailblazing motivation from other responses to group-based low performance expectations. This research advances our understanding of the behavior of marginalized individuals at work by helping to explain (1) when and why people facing group-based low expectations go beyond seeking to prove their own abilities and also strive to effect change for their marginalized group as a whole and (2) how a closer connection to one’s marginalized group can drive people to increase opportunities for that broad group. Funding: This research was funded by a grant from The Leadership Center at The Wharton School, University of Pennsylvania [Grant 2000-0267] as well as through Post-doctoral Fellowship research funds allocated by The Tuck School, Dartmouth College.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15624},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {477-513},
  shortjournal = {Organ. Sci.},
  title        = {Trailblazing motivation and marginalized group members: Changing expectations to pave the way for others},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The indirect effect of entrepreneurship on pay dispersion:
Entry cost reduction, mobility threat, and wage redistribution within
incumbent firms. <em>ORSC</em>, <em>36</em>(1), 452–476. (<a
href="https://doi.org/10.1287/orsc.2022.16201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Past research has examined the link between initiatives promoting entrepreneurship and compensation, but scholars have predominantly focused on earnings of individuals directly engaged in the founding process, such as founders, cofounders, and start-up employees. Shifting our focus to incumbent workers, we instead propose that a decline in the cost of entrepreneurship increases the variance in pay among incumbent workers who are not involved in entrepreneurial activities. We posit that, as entrepreneurship becomes a more attractive career option, due to institutional changes, the outside option value of entrepreneurship increases. The resulting increase in mobility threat will disproportionately benefit high earners or those employees who are more difficult to replace: As their bargaining power increases, incumbents will disproportionately reward these workers, especially when they are systematically more inclined to leave for entrepreneurship. We explore these arguments using a difference-in-differences methodology, based on the enactment of an entry reform that reduced the cost of entry in Portugal between 1995 and 2009. We find that an exogenous decrease in the administrative costs of establishing a new venture led to high earners capturing disproportionate rewards relative to low earners. We further show that this relationship was especially pronounced among high earners who (a) exhibited a higher ex ante propensity to transition into entrepreneurship; (b) had fewer credible outside options in paid employment; and (c) operated in industries with decentralized wage bargaining arrangements. By documenting the impact of institutional changes that promote entrepreneurship on incumbent workers’ pay, our study contributes to recent debates about the impact of entrepreneurship on individual earnings. Funding: This work was supported by the CY Initiative, the FCT—Portuguese Foundation of Science and Technology, and the Knowledge, Technology and Organization Research Center at Skema. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16201 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16201},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {452-476},
  shortjournal = {Organ. Sci.},
  title        = {The indirect effect of entrepreneurship on pay dispersion: Entry cost reduction, mobility threat, and wage redistribution within incumbent firms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving virtual team collaboration paradox management: A
field experiment. <em>ORSC</em>, <em>36</em>(1), 429–451. (<a
href="https://doi.org/10.1287/orsc.2023.17952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual teams are ubiquitous in the workplace, yet they experience frequent collaboration challenges. Successfully managing the team collaboration paradox, in terms of maintaining a unified team perspective and diverse individual perspectives, presents a potentially important lever to improve virtual team performance. However, scholars have conflicting opinions regarding whether such improvement is possible. We argue that team collaboration paradox management will positively relate to team performance over time and can be improved via a theory-based intervention. This intervention draws from theory on paradoxes for its content (paradoxical thinking) and team development interventions for its structure (general content knowledge, team-specific feedback, action-focused planning). Given the complexity of paradoxes, it is unclear whether a single training session could substantively improve their management; therefore, one intervention condition was comprised of a single training session and the other condition included a follow-up session. Analyzing two waves of multisource quantitative data from a sample of 76 virtual teams from 37 organizations, we find a positive relationship between team collaboration paradox management and team performance at both time periods. We also find that only the intervention condition with the follow-up session, as compared with untreated control teams, significantly improved how well teams managed the collaboration paradox and thereby facilitated subsequent changes in team performance. Supplementary qualitative insights from the intervention sessions illuminate the actions virtual teams took to improve their collaboration paradox management. These results have important implications for the paradox and teams literatures, as well as the managers and members of virtual teams. Funding: This work was supported by the SHRM Foundation [Project 166].},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17952},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {429-451},
  shortjournal = {Organ. Sci.},
  title        = {Improving virtual team collaboration paradox management: A field experiment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cultural spawning: Founders bringing organizational cultures
to their startup. <em>ORSC</em>, <em>36</em>(1), 411–428. (<a
href="https://doi.org/10.1287/orsc.2023.17771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In searching for the sources of heterogeneity in organizational cultures, one possibility is that new ventures show a cultural spawning of incorporating elements from the culture of the organization that the founder left (the parent) when starting the new venture. Such a genealogical effect would explain why some organizational cultures remain different, despite the opportunities to learn cultural elements from other organizations. This study investigates whether and under what circumstances such cultural spawning occurs. We argue that cultural spawning occurs, but with varying strength, depending on the founder and the culture of the parent organization. Applying the cultural toolkit perspective, we predict that it is stronger when founders have a longer tenure in the parent organization, the parent culture is more internally coherent, and the parent culture is more atypical, compared with other organizations. These ideas were tested with a sample of U.S. technology startups in Crunchbase. Natural language processing of Glassdoor employee reviews was used to identify the cultural elements of the technology companies. The analysis demonstrates that cultural spawning occurs and reveals previously unexplored contingencies of cultural transmission through congruency and atypicality. It contributes to research on new venture formation and organizational culture. Funding: Financial support from the Rudolf and Valeria Maag Endowment is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17771 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17771},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {411-428},
  shortjournal = {Organ. Sci.},
  title        = {Cultural spawning: Founders bringing organizational cultures to their startup},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Organizational selection of innovation. <em>ORSC</em>,
<em>36</em>(1), 387–410. (<a
href="https://doi.org/10.1287/orsc.2023.17357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules—including averaging agents’ project scores and counting their approval votes—especially when organizations have tight budgets and can select only a few project alternatives out of many. Funding: L. Böttcher acknowledges financial support from hessian.AI and the Army Research Office [Grant W911NF-23-1-0129]. Supplemental Material: The data files are available at https://doi.org/10.1287/orsc.2023.17357 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17357},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {387-410},
  shortjournal = {Organ. Sci.},
  title        = {Organizational selection of innovation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A buddhist mindfulness view of paradox: Silence and
skepticism of language to dismantle paradoxes. <em>ORSC</em>,
<em>36</em>(1), 361–386. (<a
href="https://doi.org/10.1287/orsc.2023.17606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how Buddhist mindfulness as a self-reflective practice helps individuals respond to a paradox and ultimately dismantle it. To deeply immerse myself into this context, I conducted a nine-month ethnographic fieldwork in three Korean Buddhist temples that confront the paradox between the need for financial resources and spiritual values that disavow money. The findings show a series of cognitive mechanisms that reveal multiple roles of mindfulness, manifested as silence and skepticism of language. First, the monastic environment enables monks to become familiar with a life of silence that turns their attention to the inner mind from the external-empirical world. The silence serves as a mental buffer when monks switch between their sacred role and their business role. Over time, deep silence directs them to skepticism of language that triggers doubt on preexisting linguistic categories, boundaries, and separations. When the preexisting linguistic categories finally disappear in their mind, monks no longer rely on any differentiating or integrating tactic to navigate their paradox. In other words, they no longer perceive a paradox, which means the paradox has disappeared from their life. These cognitive mechanisms construct the monks’ worldview on contradictions, conflicts, and dualities, leading them from the experience of paradox to a unique mental state, the nonexperience of paradox. Integrating this mental state and the worldview of Buddhist monks with paradox research, this study theorizes a Buddhist mindfulness view of paradox. Funding: This work was supported by Chulalongkorn University.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17606},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {361-386},
  shortjournal = {Organ. Sci.},
  title        = {A buddhist mindfulness view of paradox: Silence and skepticism of language to dismantle paradoxes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Through the narrows: The meaning and enactment of
interpersonal holding. <em>ORSC</em>, <em>36</em>(1), 340–360. (<a
href="https://doi.org/10.1287/orsc.2023.17661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Helping practices that focus on returning members to their work tasks amid significant distress do little to attend to how they may be severely hindered by their own self-limiting responses to that distress. The focus of this study is on helping that attends to individuals limited in their responses to distress—individuals who inhabit, metaphorically, the narrows, a place of diminishment in which individuals are made smaller by accessing only specific parts of their selves. Through in-depth interviews with professionals working with individuals suffering from various distressing conditions, events, and situations, I develop generalizable theoretical insights about interpersonal holding as a specific form of helping dedicated to surfacing, expanding, and integrating aspects of individuals’ selves that have receded amid distress. The findings indicate a sequence of holding behaviors marked by three overlapping phases: holders coming alongside, linking up with, and guiding individuals in distress through narrowed intrapsychic spaces. This sequence is enabled, first, by the availability of individuals to interpersonal holding and second, by aspects of the holders that stabilize them during the complicated work of attending to the agentic selves of others. The study contributes to both the evolution of scholarship on distress helping in the context of resilience, loss, and role distress and to theory about interpersonal holding.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17661},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {340-360},
  shortjournal = {Organ. Sci.},
  title        = {Through the narrows: The meaning and enactment of interpersonal holding},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do investors value workforce gender diversity?
<em>ORSC</em>, <em>36</em>(1), 313–339. (<a
href="https://doi.org/10.1287/orsc.2022.17098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine whether investors value workforce gender diversity. Consistent with the view that investors believe workforce gender diversity can be valuable in major firms, we use event studies to demonstrate that U.S. technology firms and financial firms experience more positive stock price reactions when it is revealed that they have relatively higher (versus lower) workforce gender diversity numbers. For instance, we find that Google’s revelation of relatively low workforce gender diversity numbers triggered a negative stock price reaction, whereas eBay’s revelation of relatively high workforce gender diversity numbers triggered a positive stock price reaction. These stock price reactions are both economically and statistically significant; for example, we estimate that if a technology firm had revealed gender diversity numbers that were one standard deviation higher, its market valuation would have increased by $1.11 billion. Corroborating this plausibly causal field evidence, we also find positive investor reactions to workforce gender diversity in randomized experiments using Prolific participants with investing experience; these reactions seem to be underpinned by investors’ beliefs about potential upsides of diversity for the firm (e.g., reduced legal risks; increased creativity) but not by investors’ beliefs about potential downsides of diversity for the firm (e.g., increased conflict). Our findings highlight the importance of understanding investors’ intuitions or beliefs about major organizational phenomena such as workforce gender diversity. Our results also point toward a new type of business case for diversity, driven by investors: if major firms had more workforce gender diversity, investors may “reward” them with substantially higher valuations. Funding: This research is supported by NUS (National University of Singapore) Startup Grant WBS A-0003900-00-00 to D. P. Daniels.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.17098},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {313-339},
  shortjournal = {Organ. Sci.},
  title        = {Do investors value workforce gender diversity?},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal miscoupling: The challenges and consequences of
enacting a practice in decline. <em>ORSC</em>, <em>36</em>(1), 288–312.
(<a href="https://doi.org/10.1287/orsc.2021.16026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A practice firmly entrenched in one period can experience decline or even deinstitutionalization in another. However, at the organization level, practice abandonment can be a slow process. Organizations may continue to use a practice that is in decline or out of date, coupling asynchronously with a prior institutional environment. In this paper we develop the concept of temporal miscoupling and examine the challenges of performing a practice under this condition. Drawing on a field study of a labor strike in Canada, we examine the difficulties of inhabiting the practice of striking—picketing a workplace—with eroding political, legal, and cultural support; declining familiarity with the practice; and fading narratives to motivate and justify the practice. We show how strikers developed extemporaneous resources that gave local meaning and form to the practice. These improvised resources supported the practice but distorted historical meanings and performances. Our study expands the analytical repertoire of inhabited institutionalism by problematizing the temporal lag between institutional conditions and organizational practices for the on-the-ground enactment of practices. The extemporaneous resources generated in the context of temporal miscoupling threaten future enactments, indicating an important role for practice enactment in processes of decline and revitalization. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.16026 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.16026},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {288-312},
  shortjournal = {Organ. Sci.},
  title        = {Temporal miscoupling: The challenges and consequences of enacting a practice in decline},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the chasm between intentions and behaviors:
Developing and testing a construal level theory of internal
whistle-blowing. <em>ORSC</em>, <em>36</em>(1), 261–287. (<a
href="https://doi.org/10.1287/orsc.2021.15292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent wave of corporate scandals has necessitated a more systematic investigation of internal whistle-blowing as a potential way to prevent wrongdoing. Our understanding of whistle-blowing, however, has been hampered by a deep chasm that exists between employees’ intent to blow the whistle and their whistle-blowing behaviors. We argue that to fully bridge this gap, we need to consider employees’ cognitive states at the time of whistle-blowing intentions versus behaviors and to link these cognitive states to the ethical systems within the organization’s ethical infrastructure to understand which systems are more effective in cultivating whistle-blowing intentions and which systems help translate those intentions into behaviors. Across one multisource field study and one multiwave experiment, we found support for our arguments that top management values-based communication systems, which are more high construal (abstract), affect whistle-blowing intentions whereas ethical accountability systems and ethical retaliatory systems, which are more low construal (concrete), moderate the relationship between whistle-blowing intentions and behaviors. By linking ethical systems within the organization’s ethical infrastructure to the two stages (intentions and behaviors) of the whistle-blowing process and the accompanying cognitive states, we develop and empirically test a construal level theory of internal whistle-blowing. Funding: This work was supported by the Ministry of Education, Singapore, under its Academic Research Fund Grant Calls [MOE 2019-T2-1-192 and MSS16B012].},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15292},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {261-287},
  shortjournal = {Organ. Sci.},
  title        = {Bridging the chasm between intentions and behaviors: Developing and testing a construal level theory of internal whistle-blowing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulation and innovation revisited: How restrictive
environments can promote destabilizing new technologies. <em>ORSC</em>,
<em>36</em>(1), 240–260. (<a
href="https://doi.org/10.1287/orsc.2022.16770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous literature offers conflicting findings on how the restrictiveness of the regulatory environment—the amount of rules that prohibit specific activities—affects innovation of firms. One camp suggests that restrictiveness circumscribes the range of available technological components and therefore decreases innovation. The other camp believes that restrictiveness can lead firms to seek new alternative technological components, which could increase innovation. In this article, we develop a new theory on regulation and innovation to reconcile these views, which we test using novel data on federal regulations and the patents of 1,242 firms, from 1994 to 2013. We find that restrictiveness can have both a negative and positive relationship with innovation output depending on the level of regulatory uncertainty and the innovation type in question. Funding: This work was supported by the Mercatus Center, George Mason University, and the Strategy Research Foundation [Grant SRF-2021-DRG-8365]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16770 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16770},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {240-260},
  shortjournal = {Organ. Sci.},
  title        = {Regulation and innovation revisited: How restrictive environments can promote destabilizing new technologies},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The relative effects of a scandal on member engagement in
rites of integration and rites of passage: Evidence from a child abuse
scandal in the catholic archdiocese of philadelphia. <em>ORSC</em>,
<em>36</em>(1), 213–239. (<a
href="https://doi.org/10.1287/orsc.2022.16682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizational research has documented that scandals lead to negative aggregate stakeholder reactions. There is little reason to believe, however, that the effects of a scandal are homogenous across different types of engagement. We therefore compare the effects of a scandal on member engagement in two types of rites at normative organizations: rites of integration and rites of passage. Rites of integration focus on the community, celebrate organizational values, and help strengthen organizational identification; they are thus enacted more by core members. Rites of passage focus on the individual, celebrate transition between social roles, and require only occasional engagement; they are thus enacted by core and peripheral members. Because of these differences, we hypothesize that a normative organization’s implication in a scandal affects rites of passage more negatively than rites of integration, but that this effect depends on scandal prevalence among neighboring organizations, organizational age, and organizational size. We test our hypotheses in the context of a child abuse scandal in the Catholic Archdiocese of Philadelphia. Using yearly parish-level data from 1990 to 2010, we find that a parish’s implication in the scandal was associated with a larger decline in rites of passage (marriages, baptisms, and funerals) than in rites of integration (mass attendance). This difference was reversed with the increase in scandal prevalence. Furthermore, rites of integration were more resilient than rites of passage at older and larger parishes. To help rule in the plausibility of our organization-level theory, we present a simulation grounded in individual-level polling data from the context. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16682 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16682},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {213-239},
  shortjournal = {Organ. Sci.},
  title        = {The relative effects of a scandal on member engagement in rites of integration and rites of passage: Evidence from a child abuse scandal in the catholic archdiocese of philadelphia},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Location-specificity and relocation incentive programs for
remote workers. <em>ORSC</em>, <em>36</em>(1), 186–212. (<a
href="https://doi.org/10.1287/orsc.2023.17712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precipitous growth of remote work has given rise to a new phenomenon: the emergence of relocation incentive programs that localities use to compete for the physical presence of remote workers. Remote workers with high general human capital may create value for their new destinations and reverse net talent outflow from smaller cities in middle America and globally. However, localities seeking to attract, retain, and create value from remote workers face significant challenges because such workers may have a low attachment to their new destination. Analogizing these challenges to the problem of creating and capturing value from workers with general human capital, we argue that localities can benefit from using relocation incentive program by leveraging location-specific attributes that create value for the individual and the locality. We examined these ideas in the context of Tulsa Remote, a program that provides relocation incentives and a bundle of services to increase engagement and embeddedness in Tulsa, Oklahoma. We found that Tulsa Remote increased community engagement, real income, and entrepreneurship of remote workers, benefiting both the community and the individual. Tulsa Remote increased the worker’s willingness to stay, and local community engagement is a key driver of this relationship. This work thus suggests that location specificity enables localities to both create and capture value from remote workers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17712 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17712},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {186-212},
  shortjournal = {Organ. Sci.},
  title        = {Location-specificity and relocation incentive programs for remote workers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How mixed performance feedback shapes exploration: The
moderating role of self-enhancement. <em>ORSC</em>, <em>36</em>(1),
166–185. (<a href="https://doi.org/10.1287/orsc.2021.15676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct an experiment to examine how providing decision makers with high versus low peer performance information influences choices between exploration and exploitation. Previous work on organization-level learning suggests that a high-performing peer would fuel exploration, whereas a low-performing peer would dampen it. In line with this, we find that individuals who receive information about a high-performing peer explore more than those who receive information about a low-performing peer. However, we also find that compared to individuals with a low tendency to self-enhance, individuals with a high tendency to self-enhance are less likely to explore when receiving information about a high-performing peer. In fact, these individuals explore at levels comparable to those who receive information about a low-performing peer. We explain this behavioral pattern by demonstrating that as individuals learn and improve, information about a high-performing peer increasingly results in mixed performance feedback; under these conditions of relative interpretive flexibility, exploration is moderated by decision makers’ tendency to self-enhance. When these individual dynamics are aggregated, our data suggest that an organization that provides peer performance information may experience either the same or less exploration than an organization that does not, with the exact difference depending on its proportion of high self-enhancers. These insights into the contingencies and aggregate effects of how individuals interpret and respond to peer performance information are particularly relevant given recent interest in designing organizations that shape employee behavior through the provision of feedback rather than through traditional instruments of coordination and control, such as incentives or hierarchy. Funding: This work was supported by Danmarks Frie Forskningsfond [Grant 25194]. Additionally, this research was supported by a grant from the HEC Paris Foundation.},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15676},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {166-185},
  shortjournal = {Organ. Sci.},
  title        = {How mixed performance feedback shapes exploration: The moderating role of self-enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of financial resources on misconduct: Evidence
from lottery ticket sales. <em>ORSC</em>, <em>36</em>(1), 145–165. (<a
href="https://doi.org/10.1287/orsc.2023.17541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the influence of financial resources on a firm’s propensity for misconduct. Previous studies offer conflicting predictions regarding the relationship, and much of the empirical evidence suffers from issues like selection, measurement error, reverse causality, and omitted variable bias. Leveraging a difference-in-differences design, we first examine quasirandom fluctuations in retailers’ financial resources resulting from large windfalls from selling winning lottery tickets. Our results suggest that an increase in financial resources from selling a large winning lottery ticket reduces retailers’ tobacco sales to minors. Next, to rule in strain theory and to rule out alternative explanations, we leverage a second natural experiment, heterogeneity analysis, an alternate measure of misconduct, and an online randomized experiment. In doing so, we provide plausibly causal evidence on the relationship between a firm’s financial resources and its propensity for misconduct and provide potentially useful findings for policymakers and regulators. Funding: This work was supported by the Blake Family Fund for Ethics, Leadership, and Governance at Purdue University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2023.17541 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17541},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {145-165},
  shortjournal = {Organ. Sci.},
  title        = {The effect of financial resources on misconduct: Evidence from lottery ticket sales},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The double-edged sword of exemplar similarity.
<em>ORSC</em>, <em>36</em>(1), 121–144. (<a
href="https://doi.org/10.1287/orsc.2022.16855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how a firm’s positioning relative to category exemplars shapes security analysts’ evaluations. Using a two-stage model of evaluation (initial screening and subsequent assessment), we propose that exemplar similarity enhances a firm’s recognizability and legitimacy, increasing the likelihood that it passes the initial screening stage and attracts analyst coverage. However, exemplar similarity may also prompt unfavorable comparisons with exemplar firms, leading to lower analyst recommendations in the assessment stage. We further argue that category coherence, distinctiveness, and exemplar typicality influence the impact of exemplar similarity on firm evaluation. Leveraging natural language processing (NLP) techniques to analyze a sample of 7,603 U.S. public firms from 1997 to 2022, we find robust support for our predictions. By highlighting the intricate role of strategic positioning vis-à-vis category exemplars in shaping audience evaluations, our findings have important implications for research on positioning relative to category exemplars, category viability, optimal distinctiveness, and security analysts. Supplemental Material: The online appendices are available at https://doi.org/10.1287/orsc.2022.16855 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16855},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {121-144},
  shortjournal = {Organ. Sci.},
  title        = {The double-edged sword of exemplar similarity},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measure twice, cut once: Unit profitability, scalability,
and the exceptional growth of new firms. <em>ORSC</em>, <em>36</em>(1),
88–120. (<a href="https://doi.org/10.1287/orsc.2021.15970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By studying three promising venture pairs with different business models in different nascent markets, we explore how some new firms grow into mature firms and some promising peers do not. The successful firms first use a learning process that begins with (1) focused attention on unit profitability (not growth), then broad (not narrow) learning, and finally active delay (not acceleration) of growth. They then use a capability-building process that (2) shifts to focused attention on unit-profitable growth, then makes long-term investments in profit-oriented capabilities, and finally leverages these capabilities into new products. In contrast, peers with focused attention on growth ironically fail to achieve it. Broadly, we contribute a process theory of exceptional growth that fills the gap between the growth of new firms in the entrepreneurship literature and the growth of mature firms in the strategy and organization theory literatures. Further, we add the importance of learning content —not just learning processes—to the entrepreneurship literature on new firm growth and a rare, novel, and grounded account of the origin of capabilities to the strategy literature on the resource-based view and mature firm growth. Finally, we add to practice the significance of product–market–profit fit and a precise definition of premature scaling . Funding: This work was supported by the Strategy Research Foundation [Grant SRF-2018-DP-9167].},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15970},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {88-120},
  shortjournal = {Organ. Sci.},
  title        = {Measure twice, cut once: Unit profitability, scalability, and the exceptional growth of new firms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CEO initial contract duration and corporate acquisitions.
<em>ORSC</em>, <em>36</em>(1), 65–87. (<a
href="https://doi.org/10.1287/orsc.2022.16493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the organizational impact of CEO initial contract duration on corporate acquisitions. We argue that CEOs with shorter initial contract durations are more likely to experience time pressure. Consequently, they are more likely to manage time by engaging in corporate mergers and acquisitions (M&amp;As) to achieve quick growth. In addition, these CEOs are more likely to engage in straightforward deals, acquiring targets that are private, divested, related, small, and using cash payment, because these types of transactions are quicker to complete, carry less risk, and generally come with good performance prospects. Using a sample of firms that underwent new CEO appointments between 1990 and 2017 and detailed employment contract data collected from SEC filings, we find strong support for our hypotheses. In addition, we apply UK corporate governance reform to CEO contract duration as an exogenous shock to show causal evidence of such relations. This study contributes to the literature on CEO contracts, corporate acquisitions, time management and strategic leadership. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2022.16493 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2022.16493},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {65-87},
  shortjournal = {Organ. Sci.},
  title        = {CEO initial contract duration and corporate acquisitions},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opportunistic change during a punctuation: How and when the
front lines can drive bursts of incremental change. <em>ORSC</em>,
<em>36</em>(1), 40–64. (<a
href="https://doi.org/10.1287/orsc.2021.15120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental jolts can trigger more conducive conditions for driving change in organizations. However, punctuated equilibrium theories of organizational change concentrate on top managers’ implementation of de novo radical changes after jolts. Existing research has not examined frontline-driven, incremental change efforts during these periods of disrupted stasis, despite the value of frontline change ideas. We develop a process model to explain how and when those on an organization’s front lines can leverage a jolt to opportunistically implement long-desired change ideas in ways that promote their retention. We conducted a two-year qualitative field study at a hospital during the Covid-19 pandemic, examining the trajectories of 33 premeditated change ideas raised by frontline staff. By comparing ideas that persisted to become part of normal operations with those that failed to be selected or retained, we identified practices and conditions that promoted the selection and retention of frontline change ideas. Our study suggests that frontline change advocates can seed the long-term retention of “shovel-ready” ideas—as opposed to de novo ideas—after a jolt by rapidly and opportunistically deploying a novel set of practices before the brief window of opportunity created by lessened constraints and increased managerial receptivity closes. Prior theories of change largely assume frontline-driven change to be slow and continuous, proceeding in a one-off fashion; we explain how and when frontline change can instead occur in rapid, opportunistic bursts. This study advances theories of punctuated equilibrium and bottom-up change in organizations by unearthing an alternative way that change can be intentionally accomplished in organizations. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.15120 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15120},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {40-64},
  shortjournal = {Organ. Sci.},
  title        = {Opportunistic change during a punctuation: How and when the front lines can drive bursts of incremental change},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention focus and new opportunities: The moderating role
of managerial attention to alternative issues. <em>ORSC</em>,
<em>36</em>(1), 21–39. (<a
href="https://doi.org/10.1287/orsc.2021.15861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The strategy literature highlights the critical role that managerial attention plays in shaping firms’ responses to industry change. However, attention to change alone is not sufficient to precipitate action. The context in which this attention is paid is also critical in determining the translation of attention to action as managers generally pay attention to multiple issues within their strategic agendas. We argue that attention to a competing issue and the breadth of managerial strategic attention impair the relationship between attention to a focal issue and related organizational action as these attentional constraints may lead to hesitancy or confusion at an individual level and reduced coordination at an organizational level. Realizing that at an organizational level, firms vary tremendously with respect to their sizes, resources, and by implication their overall attentional capacities, we argue that the negative moderation impacts of attention to a competing issue and the breadth of strategic attention on the attention to a focal issue–subsequent action relationship are larger for smaller firms. Studying the U.S. electric utility industry and the emerging new distributed electricity generation business model, we find broad support for our arguments and also important nuance. By melding theory, important details of our empirical setting, and intriguing empirical results, we develop greater insights into when and how the other strategic issues to which managers pay attention can shape the translation of managerial attention to a new business model into subsequent organizational action. Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.15861 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2021.15861},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {21-39},
  shortjournal = {Organ. Sci.},
  title        = {Attention focus and new opportunities: The moderating role of managerial attention to alternative issues},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). More to lose: The adverse effect of high performance ranking
on employees’ preimplementation attitudes toward the integration of
powerful AI aids. <em>ORSC</em>, <em>36</em>(1), 1–20. (<a
href="https://doi.org/10.1287/orsc.2023.17515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing availability of algorithm-augmented work, algorithm aversion is prevalent among employees, hindering successful implementations of powerful artificial intelligence (AI) aids. Applying a social comparison perspective, this article examines the adverse effect of employees’ high performance ranking on their preimplementation attitudes toward the integration of powerful AI aids within their area of advantage. Five studies, using a weight estimation simulation (Studies 1–3), recall of actual job tasks (Study 4), and a workplace scenario (Study 5), provided consistent causal evidence for this effect by manipulating performance ranking (performance advantage compared with peers versus no advantage). Studies 3–4 revealed that this effect was driven in part by employees’ perceived potential loss of standing compared with peers, a novel social-based mechanism complementing the extant explanation operating via one’s confidence in own (versus AI) ability. Stronger causal evidence for this mechanism was provided in Study 5 using a “moderation-of-process” design. It showed that the adverse effect of high performance ranking on preimplementation AI attitudes was reversed when bolstering the stability of future performance rankings (presumably counteracting one’s concern with potential loss of standing). Finally, pointing to the power of symbolic threats, this adverse effect was evident both in the absence of financial incentives for high performance (Study 1) and in various incentive-based settings (Studies 2–3). Implications for understanding and managing high performers’ aversion toward the integration of powerful algorithmic aids are discussed. Funding: This work was supported by the Coller Foundation. Supplemental Material: The supplemental material is available at https://doi.org/10.1287/orsc.2023.17515 .},
  archive      = {J_ORSC},
  doi          = {10.1287/orsc.2023.17515},
  journal      = {Organization Science},
  month        = {1-2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Organ. Sci.},
  title        = {More to lose: The adverse effect of high performance ranking on employees’ preimplementation attitudes toward the integration of powerful AI aids},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="serv---5">SERV - 5</h2>
<ul>
<li><details>
<summary>
(2025). Call for papers—service science special issue on
sustainability-driven service innovations. <em>SERV</em>,
<em>17</em>(1), 71–72. (<a
href="https://doi.org/10.1287/serv.2025.cfp.v17.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SERV},
  doi          = {10.1287/serv.2025.cfp.v17.n1},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {71-72},
  shortjournal = {Serv. Sci.},
  title        = {Call for Papers—Service science special issue on sustainability-driven service innovations},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does negative publicity change tourists’ advocacy intention
on online hotel websites? Searching for answers from an online travel
agency study. <em>SERV</em>, <em>17</em>(1), 58–70. (<a
href="https://doi.org/10.1287/serv.2023.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Travel virtual communities have reshaped trip planning and experience-sharing amid COVID-19. Postpandemic, information technology in tourism must adjust to changes in traveler behavior, including online booking trends and risk perceptions. Based on the online platform Trivago, the research objectives of this study seek to explore the relationships among electronic service quality, perceived severity of negative publicity, perceived risk, and consumers’ advocacy intention to embrace after the impact of Trivago’s negative publicity. Further, this study aims to explore the moderating effects of electronic service quality and mediating effects of perceived risk. This research collected a total of 468 valid responses and verified the hypothesis by regression analysis and Sobel test. The research results are summarized as follows: the perceived severity of negative publicity negatively affects advocacy intention and positively affects perceived risk, which also negatively affects advocacy intention. These effects are all moderated by e-service quality, which negatively affects perceived risk; perceived risk is the mediator between perceived severity of negative publicity and advocacy intention. The results enrich literature on online hotel websites, offering valuable insights into how managers can effectively use online travel agencies to enhance their online presence and increase revenue. Funding: This study was supported by a grant from the National Science and Technology Council, Taiwan [Grant 111-2410-H-227-009].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0055},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {58-70},
  shortjournal = {Serv. Sci.},
  title        = {Does negative publicity change tourists’ advocacy intention on online hotel websites? searching for answers from an online travel agency study},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Could auto dealers benefit from vertical media platforms’
encroachment? <em>SERV</em>, <em>17</em>(1), 35–57. (<a
href="https://doi.org/10.1287/serv.2023.0070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating sales leads for auto dealers is the core business to auto vertical media platforms (VMPs). However, in recent years, many auto VMPs have started directly selling vehicles to consumers. Anecdotal evidence suggests that the primary purpose of auto VMPs’ business-to-consumer (B2C) endeavors is not solely B2C profitability, but rather the acquisition of sales data to calculate the leads-to-sales conversion ratio. This ratio is crucial in determining the pricing strategy for sales leads. To understand this, we consider an asymmetric information game in which the auto dealer possesses information about the leads-to-sales conversion ratio, whereas the VMP does not. We find that, when the VMP solely focuses on profiting from the sales lead business, if the market potential is large, the auto dealer prefers the cost-per-lead scheme, whereas the VMP always tends to the cost-per-sale scheme. However, when the VMP encroaches on the consumer market as a reseller, this divergence in contract-type preference can potentially be resolved. We recognize that VMPs need to conduct a comprehensive assessment of factors such as market potential, fixed cost related to market encroachment, and the correlation between market uncertainty and leads-to-sales conversion ratio when encroachment. Moreover, the encroachment of the VMP has the potential to create a win–win outcome. It is essential to highlight that engaging in B2C business enables the VMP to expand its market sample, thereby enhancing the value of leveraging information technology to optimize the leads-to-sales ratio and achieve superior performance. Funding: This work is partly supported by the Major program of the National Social Science Foundation of China [Grant 22&amp;ZD082], the National Natural Science Foundation of China [Grants 72321001, 72371104, 72071080], and the Guangdong Basic and Applied Basic Research Foundation, China [Grants 2023A1515012529, 2023A1515012435, 2024A1515010940].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0070},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {35-57},
  shortjournal = {Serv. Sci.},
  title        = {Could auto dealers benefit from vertical media platforms’ encroachment?},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Earnings pressure and corporate social responsibility
impression management. <em>SERV</em>, <em>17</em>(1), 18–34. (<a
href="https://doi.org/10.1287/serv.2023.0052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive research has investigated the buffering effect of corporate social responsibility (CSR) when firms have poor financial performance. However, few studies have examined the possible consequences of this effect. By analyzing data on listed firms in China from 2010 to 2018, this study investigates the buffering effect of CSR under conditions of earnings pressure, and then discusses its impact on CSR disclosure impression management behavior. The results show that CSR can buffer the negative impact of earnings pressure on stock prices, chief executive officer stability, and top management team stability. This buffering effect is mainly exerted by technical CSR. Moreover, earnings pressure leads to CSR disclosure impression management without resulting in improved CSR performance. These findings have implications for cultivating CSR buffering awareness among firms and managers. The results also have practical implications for future research on the antecedents and motivations of CSR disclosure impression management. Funding: This work was supported by the National Natural Science Foundation of China [Grant 72372104].},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0052},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {18-34},
  shortjournal = {Serv. Sci.},
  title        = {Earnings pressure and corporate social responsibility impression management},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic exception points for fair liver allocation.
<em>SERV</em>, <em>17</em>(1), 1–17. (<a
href="https://doi.org/10.1287/serv.2023.0092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are disparities in access to livers based on transplant patients’ height, which disproportionately affects Hispanics, Asians, and women (across all ethnicities), because short patients can receive transplants from a smaller pool of available deceased donors for medical reasons. Reduced likelihood of transplantation leads to higher mortality rates and longer waiting times. We analyze fairness within the current U.S. liver allocation system where patients receive priority dynamically, based on their model for end-stage liver disease (MELD) scores, which reflect the severity of liver disease. We propose a simple adjustment, providing additional (exception) points based on height and MELD score, that can be easily implemented in practice, which materially reduces the disparity without sacrificing overall efficiency. We model the liver allocation system as a multiclass fluid model of overloaded queues with heterogeneous servers. We impose explicit equity constraints for all static patient classes, that is, height. We characterize the optimal solution under the objective of minimizing pretransplant mortality. The discretized version of the optimal policy is numerically solved using estimates from clinical data and a detailed simulation study demonstrates its effectiveness. The optimal policy, called the equity adjusted mortality risk policy, advocates ranking patients based on their short-term mortality risk adjusted for equity among height classes. Interpretation of the shadow prices of equity constraints in the optimal control problem as MELD exception points is novel in the transplant context since they can be seamlessly mapped into the existing system. Our simulations show that for women, the disparity can be almost completely eliminated. Hispanics and Asians greatly benefit from receiving these MELD exception points also. Our work provides a remedy to reduce the disparities in access to liver transplantation within the MELD-based allocation. Our approach can help the on-going analysis of the continuous distribution model for livers because it also considers aspects of candidate biology, notably height and body surface area. Funding: M. Akan was supported by the National Science Foundation [Grant CMMI-1334194] and the Carnegie Mellon University (CMU) [Onetto Fellowship in Operations Management]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/serv.2023.0092 .},
  archive      = {J_SERV},
  doi          = {10.1287/serv.2023.0092},
  journal      = {Service Science},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Serv. Sci.},
  title        = {Dynamic exception points for fair liver allocation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="stsc---3">STSC - 3</h2>
<ul>
<li><details>
<summary>
(2025). Innovation disclosures and the design of technology
acquisition contracts: Evidence from the american inventors protection
act. <em>STSC</em>, <em>10</em>(1), 68–92. (<a
href="https://doi.org/10.1287/stsc.2022.0069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material adverse change (MAC) clauses and contingent earnouts are important contractual mechanisms used to protect acquirers from the risk of adverse selection. Yet, the extant literature has not sufficiently explored the antecedents of their use, in particular within the context of technology acquisitions. In this study, we take advantage of the passage of the American Inventors Protection Act (AIPA), which disseminated information through the publication of patent applications, to explore the impact of innovation disclosures on the design of technology acquisition contracts. Consistent with the view that an increase in the availability of information related to the broader technological landscape reduces the need for contractual protections in acquisition contracts, our analysis demonstrates that deals disproportionately affected by AIPA have less expansive MAC clauses and are less likely to feature contingent earnouts. These results provide new evidence linking the use of MAC clauses and earnouts with acquisitions subject to information frictions.},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2022.0069},
  journal      = {Strategy Science},
  month        = {3},
  number       = {1},
  pages        = {68-92},
  shortjournal = {Strat. Sci.},
  title        = {Innovation disclosures and the design of technology acquisition contracts: Evidence from the american inventors protection act},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource redeployment and the pursuit of the new best use:
Economic logic and organizational challenges. <em>STSC</em>,
<em>10</em>(1), 32–47. (<a
href="https://doi.org/10.1287/stsc.2022.0105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shifting economic circumstances may suggest some degree of resource redeployment of capacity-constrained capabilities. However, a separate question remains as to how corporate actors operate within the space of this economic logic. While prior literature on diversification has tended to highlight the fungibility of resources as a basis for their redeployment, we point to an additional consideration of the complementarity of resources that makes allocation within the firm attractive, what we term a “new best use.” Further, we point to a potential tension as the economic logic suggested by this property of complementarity, and more generally interdependence with the firm’s other resources and activities, may make assessing what constitutes what we term the “new best use” of a resource challenging. The latent fungibility of a resource may differ from the range of reliable assessment of relative value creation. The allocation of resources and capabilities across a firm of any scale or scope is generally guided by the organizational and task structure of the enterprise. Building on these arguments, we consider two classes of factors that might impact the realization of the latent opportunities of resource redeployment. One is the “pipes” of organizational structures and grouping of business units and allocation of decision rights of specific managers to those units, and the other is the “prisms” of the various metrics of performance and value creation that may make resource use across different domains of the business more or less comparable.},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2022.0105},
  journal      = {Strategy Science},
  month        = {3},
  number       = {1},
  pages        = {32-47},
  shortjournal = {Strat. Sci.},
  title        = {Resource redeployment and the pursuit of the new best use: Economic logic and organizational challenges},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How demand shocks “jumpstart” technological ecosystems and
commercialization: Evidence from the global electric vehicle industry.
<em>STSC</em>, <em>10</em>(1), 1–31. (<a
href="https://doi.org/10.1287/stsc.2022.0075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine how exogenous demand shocks overcome ecosystem bottlenecks in the commercialization of an emergent technology. We argue that demand shocks that spur new technology adoption by niche users pull “hub” firms into country technology markets, despite ecosystem bottlenecks, thereby serving to “jumpstart” the process of ecosystem development and technology commercialization. By analyzing global electric vehicle markets over the period 2008–2017, we find that extreme weather events such as abnormal heat-related events spur adoption of electric vehicles by end users, thereby propelling automotive or hub firms’ entry into country technology markets, and the subsequent shift of their electric vehicle product portfolios toward the more radical version of the technology. Notably, such demand shocks propel firms’ commercialization strategy, despite ecosystem bottlenecks such as the lack of regulatory and economic inducements for adoption, relative absence of complements, and product market differences. After entry, entrant firms’ electric vehicle product portfolios transition from hybrids toward radical technology products and investments in complements, albeit contingent on their competitive market position in the legacy technology. We discuss the implications of these findings concerning the uptake of demand shocks, and their robustness to modeling choices, technological generations across extended timeframes, potential mediating forces, and boundary conditions owing to firm and country-market heterogeneity. Supplemental Material: The online appendices are available at https://doi.org/10.1287/stsc.2022.0075 .},
  archive      = {J_STSC},
  doi          = {10.1287/stsc.2022.0075},
  journal      = {Strategy Science},
  month        = {3},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Strat. Sci.},
  title        = {How demand shocks “Jumpstart” technological ecosystems and commercialization: Evidence from the global electric vehicle industry},
  volume       = {10},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="stsy---3">STSY - 3</h2>
<ul>
<li><details>
<summary>
(2025). Normal approximation of random gaussian neural networks.
<em>STSY</em>, <em>15</em>(1), 88–110. (<a
href="https://doi.org/10.1287/stsy.2023.0033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide explicit upper bounds on some distances between the (law of the) output of a random Gaussian neural network and (the law of) a random Gaussian vector. Our main results concern deep random Gaussian neural networks with a rather general activation function. The upper bounds show how the widths of the layers, the activation function, and other architecture parameters affect the Gaussian approximation of the output. Our techniques, relying on Stein’s method and integration by parts formulas for the Gaussian law, yield estimates on distances that are indeed integral probability metrics and include the convex distance. This latter metric is defined by testing against indicator functions of measurable convex sets and so allows for accurate estimates of the probability that the output is localized in some region of the space, which is an aspect of a significant interest both from a practitioner’s and a theorist’s perspective. We illustrated our results by some numerical examples. Funding: This research was supported by the European Union’s Horizon 2020 research project WARIFA under grant agreement no. 101017385, by the PRIN project 2022 “Variational Analysis of Complex Systems in Materials Science, Physics and Biology” (CUP B53D23009290006), and by the INdAM project “Modelli ed Algoritmi per dati ad elevata dimensionalità” (CUP E53C23001670001).},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0033},
  journal      = {Stochastic Systems},
  month        = {3},
  number       = {1},
  pages        = {88-110},
  shortjournal = {Stoch. Syst.},
  title        = {Normal approximation of random gaussian neural networks},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast exact simulation of the first passage of a tempered
stable subordinator across a non-increasing function. <em>STSY</em>,
<em>15</em>(1), 50–87. (<a
href="https://doi.org/10.1287/stsy.2023.0014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a fast exact algorithm for the simulation of the first-passage time, jointly with the undershoot and overshoot, of a tempered stable subordinator over an arbitrary, nonincreasing, absolutely continuous function. We prove that the running time of our algorithm has finite exponential moments and provide bounds on its expected running time, with explicit dependence on the characteristics of the process and the initial value of the function. The expected running time grows most cubically in the stability parameter (as it approaches either 0 or 1) and is linear in the tempering parameter and the initial value of the function. Numerical performance, based on the implementation in the dedicated GitHub repository, exhibits a good agreement with our theoretical bounds. We provide numerical examples to illustrate the performance of our algorithm in Monte Carlo estimation. Funding: J. I. González Cázares and A. Mijatović are supported by the EPSRC Grant EP/V009478/1 and by The Alan Turing Institute under the EPSRC grant EP/X03870X/1. A. Mijatović is also supported by the EPSRC grant EP/W006227/1. F. Lin is funded by The China Scholarship Council and The University of Warwick PhD scholarship.},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0014},
  journal      = {Stochastic Systems},
  month        = {3},
  number       = {1},
  pages        = {50-87},
  shortjournal = {Stoch. Syst.},
  title        = {Fast exact simulation of the first passage of a tempered stable subordinator across a non-increasing function},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The BAR approach for multiclass queueing networks with SBP
service policies. <em>STSY</em>, <em>15</em>(1), 1–49. (<a
href="https://doi.org/10.1287/stsy.2023.0011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basic adjoint relationship (BAR) approach is an analysis technique based on the stationary equation of a Markov process. This approach was introduced to study heavy-traffic, steady-state convergence of generalized Jackson networks in which each service station has a single job class. We extend it to multiclass queueing networks operating under static-buffer-priority (SBP) service disciplines. Our extension makes a connection with Palm distributions that allows one to attack a difficulty arising from queue-length truncation, which appears to be unavoidable in the multiclass setting. For multiclass queueing networks operating under SBP service disciplines, our BAR approach provides an alternative to the “interchange of limits” approach that has dominated the literature in the last twenty years. The BAR approach can produce sharp results and allows one to establish steady-state convergence under three additional conditions: stability, state space collapse (SSC) and a certain matrix being “tight.” These three conditions do not appear to depend on the interarrival and service-time distributions beyond their means, and their verification can be studied as three separate modules. In particular, they can be studied in a simpler, continuous-time Markov chain setting when all distributions are exponential. As an example, these three conditions are shown to hold in reentrant lines operating under last-buffer-first-serve discipline. In a two-station, five-class reentrant line, under the heavy-traffic condition, the tight-matrix condition implies both the stability condition and the SSC condition. Whether such a relationship holds generally is an open problem.},
  archive      = {J_STSY},
  doi          = {10.1287/stsy.2023.0011},
  journal      = {Stochastic Systems},
  month        = {3},
  number       = {1},
  pages        = {1-49},
  shortjournal = {Stoch. Syst.},
  title        = {The BAR approach for multiclass queueing networks with SBP service policies},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>
<h2 id="trsc---12">TRSC - 12</h2>
<ul>
<li><details>
<summary>
(2025). A 0,1 linear programming approach to deadlock detection and
management in railways. <em>TRSC</em>, <em>59</em>(1), 187–205. (<a
href="https://doi.org/10.1287/trsc.2024.0521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In railway systems, a deadlock occurs when trains accidentally occupy positions that prevent each other from moving forward. Although deadlocks are rare events, they do occur from time to time, requiring costly recourse actions and generating significant knock-on delays. In this paper, we present a noncompact 0,1 linear programming formulation and a methodology for discovering (possibly future) deadlocks and the subsequent implementation of optimal recovery measures. The approach is implemented in a tool to dispatch trains in real time developed in cooperation with Union Pacific (UP) and currently in operations on the entire UP network. Funding: This work was partially funded by Europe’s Rail, Flagship Project MOTIONAL [Action Horizon JU Innovation, Project 101101973]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0521 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0521},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {187-205},
  shortjournal = {Trans. Sci.},
  title        = {A 0,1 linear programming approach to deadlock detection and management in railways},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-dock trailer scheduling with workforce constraints: A
dynamic discretization discovery approach. <em>TRSC</em>,
<em>59</em>(1), 165–186. (<a
href="https://doi.org/10.1287/trsc.2023.0406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Less-than-truckload (LTL) freight carriers operate consolidation networks that utilize cross-docking terminals to facilitate the transfer of freight between trailers and enhance trailer utilization. This research addresses the problem of determining an optimal schedule for unloading inbound trailers at specific unloading doors using teams of dock workers. The optimization objective is chosen to ensure that outbound trailers are loaded with minimal delay with respect to their target loading due dates. Formulating this problem, which is known to be NP-hard, using a typical time-expanded network often results in an excessively large mixed-integer programming (MIP) model. To overcome this challenge, we propose an exact dynamic discretization discovery (DDD) algorithm that iteratively solves MIPs formulated over partial networks. The algorithm employs a combination of a simple time discretization refinement strategy to progressively refine the partial network until a provably optimal solution is obtained. We demonstrate the effectiveness of the algorithm in solving problem instances representative of a large L-shaped cross-dock in Atlanta. The DDD algorithm outperforms solving the model formulated over a complete time-expanded network with a commercial solver in terms of both computational time and solution quality for practical instances with 180 trailers, 44 unloading doors, and 57 loading doors. Additionally, we compare the DDD algorithm with a state-of-the-art interval scheduling approach using instances from a previous study with a different objective function and additional constraints. The DDD algorithm is computationally faster for most of the small and medium instances and achieves competitive bounds for the larger instances. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0406 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0406},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {165-186},
  shortjournal = {Trans. Sci.},
  title        = {Cross-dock trailer scheduling with workforce constraints: A dynamic discretization discovery approach},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiperiod, multicommodity, capacitated international
agricultural trade network equilibrium model with applications to
ukraine in wartime. <em>TRSC</em>, <em>59</em>(1), 143–164. (<a
href="https://doi.org/10.1287/trsc.2023.0294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is facing immense challenges because of increasing strife and the impacts of climate change with accompanying disasters, both sudden-onset as well as slow-onset ones, which have affected the trade of agricultural commodities needed for food security. In this paper, a multiperiod, multicommodity, international, agricultural trade network equilibrium model is constructed with capacity constraints on the production, transportation, and storage of agricultural commodities. The model allows for multiple routes between supply and demand country markets, different modes of transport, and storage in the producing and consuming countries as well as in the intermediate countries. The generality of the underlying functions, coupled with the capacity constraints, allow for the modeling of competition among agricultural commodities for production, transportation, and storage. The capacity constraints also enable the quantification of various disaster-related disruptions to production, transportation, and storage on the volumes of commodity flows as well as on the prices. A series of numerical examples inspired by the effects of Russia’s full-scale invasion of Ukraine on agricultural trade is presented, and the results are analyzed to provide insights into food insecurity issues caused by the war.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0294},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {143-164},
  shortjournal = {Trans. Sci.},
  title        = {A multiperiod, multicommodity, capacitated international agricultural trade network equilibrium model with applications to ukraine in wartime},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the concept of opportunity cost in integrated demand
management and vehicle routing. <em>TRSC</em>, <em>59</em>(1), 125–142.
(<a href="https://doi.org/10.1287/trsc.2024.0644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated demand management and vehicle routing problems are characterized by a stream of customers arriving dynamically over a booking horizon and requesting logistical services, fulfilled by a given fleet of vehicles during a service horizon. Prominent examples are attended home delivery and same-day delivery problems, where customers commonly have heterogeneous preferences regarding service fulfillment and requests differ in profitability. Thus, demand management methods are applied to steer the booking process to maximize total profit considering the cost of the routing decisions for the resulting orders. To measure the requests’ profitability for any demand management method, it is common to estimate their opportunity cost. In the context of integrated demand management and vehicle routing problems, this estimation differs substantially from the estimation in the well-examined demand management problems of traditional revenue management applications as, for example, found in the airline or car rental industry. This is because of the unique interrelation of demand control decisions and vehicle routing decisions as it inhibits a clear quantification and attribution of cost, and of displaced revenue, to certain customer requests. In this paper, we extend the theoretical foundation of opportunity cost in integrated demand management and vehicle routing problems. By defining and analyzing a generic Markov decision process model, we formally derive a definition of opportunity cost and prove opportunity cost properties on a general level. Hence, our findings are valid for a wide range of specific problems. Further, based on these theoretical findings, we propose approximation approaches that have not yet been applied in the existing literature, and evaluate their potential in a computational study. Thereby, we provide evidence that the theoretical results can be practically exploited in the development of solution algorithms. Funding: This work was supported by the University of the Bundeswehr Munich. Supplemental Material: The online appendices are available at https://doi.org/10.1287/trsc.2024.0644 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0644},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {125-142},
  shortjournal = {Trans. Sci.},
  title        = {On the concept of opportunity cost in integrated demand management and vehicle routing},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal multi-agent pickup and delivery using
branch-and-cut-and-price algorithms. <em>TRSC</em>, <em>59</em>(1),
104–124. (<a href="https://doi.org/10.1287/trsc.2023.0268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of agents and a set of pickup-delivery requests located on a two-dimensional grid map, the multi-agent pickup and delivery problem assigns the requests to the agents such that every agent moves from its start location to the locations of its assigned requests and finally, to its end location without colliding into other agents and that the sum of arrival times is minimized. This paper proposes two exact branch-and-cut-and-price algorithms for the problem. The first algorithm performs a three-level search. A high-level master problem selects an optimal sequence of requests and a path for every agent from a large collection. A mid-level sequencing problem and a low-level navigation problem are solved simultaneously to incrementally enlarge the collection of request sequences and paths. The second algorithm first solves the sequencing problem to find a set of request sequences and then solves the navigation problem to determine if paths compatible with the request sequences exist. Experimental results indicate that the integrated algorithm solves more instances with higher congestion, and the deferred algorithm solves more instances with lower congestion and could scale to 100 agents and 100 requests, significantly higher than a state-of-the-art suboptimal approach. Funding: This research was supported by the Australian Research Council [Discovery Early Career Researcher Award DE240100042 and Discovery Projects DP190100013 and DP200100025] and by Amazon. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0268 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0268},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {104-124},
  shortjournal = {Trans. Sci.},
  title        = {Optimal multi-agent pickup and delivery using branch-and-cut-and-price algorithms},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heatmap design for probabilistic driver repositioning in
crowdsourced delivery. <em>TRSC</em>, <em>59</em>(1), 81–103. (<a
href="https://doi.org/10.1287/trsc.2022.0418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the use of heatmaps as a control lever to manage the probabilistic repositioning of independent drivers in crowdsourced delivery platforms. The platform aims to maximize order fulfillment by dynamically matching drivers and orders and selecting heatmaps that trigger the probabilistic flow of unmatched drivers to balance driver supply and delivery requests across the service region. We develop a Markov decision process (MDP) model to sequentially select matching and heatmap decisions in which the repositioning behavior of drivers is captured by a multinomial logit discrete choice model. Because of the curse of dimensionality and the endogenous uncertainty of driver repositioning, the MDP model is solved using a rolling-horizon stochastic lookahead policy. This policy decomposes matching and heatmap decisions into two optimization problems: a two-stage stochastic programming upper bounding problem for matching decisions and a mixed-integer programming problem for heatmap decisions. We also propose a simple policy for efficiently solving large-scale problems. An extensive computational study on instances derived from the Chicago ride-hailing data set is conducted. Computational experiments demonstrate the value of heatmaps in improving order fulfillment beyond the level achieved by matching alone (up to 25%) and identify conditions that affect the benefit of using heatmaps to guide driver repositioning. Funding: The authors gratefully acknowledge the support of the Natural Sciences and Engineering Research Council of Canada through Discovery Grants [Grants RGPIN-2024-04881, RGPIN-2020-04498, and RGPIN-2019-06207] awarded to the first, second, and third authors, respectively. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2022.0418 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2022.0418},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {81-103},
  shortjournal = {Trans. Sci.},
  title        = {Heatmap design for probabilistic driver repositioning in crowdsourced delivery},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact solution of the vehicle routing problem with drones.
<em>TRSC</em>, <em>59</em>(1), 60–80. (<a
href="https://doi.org/10.1287/trsc.2024.0544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with drones (VRP-D) that we consider is an extension of the capacitated vehicle routing problem in which the fleet consists of trucks equipped with one drone each. A truck and its drone can either move together or separately. A truck can release its drone at the depot or at a customer location and must pick it up later at another customer or the depot location. In this way, both trucks and drones deliver goods to customers working together as synchronized working units. A feasible route has to satisfy the capacity constraints of both the truck and the drone. A feasible solution to the VRP-D is a set of feasible routes such that each customer is served exactly once by either a truck or a drone. We investigate two standard objectives considered in the literature, that is, the minimization of the total routing cost and the sum of the routes’ durations. To solve the VRP-D exactly, we develop a branch-price-and-cut (BPC) algorithm. In particular, we present a new forward and implicit bidirectional labeling algorithm defined over an artificial network to solve the column-generation subproblems. The new bidirectional labeling algorithm substantially accelerates the solution process compared with its monodirectional counterpart. The time needed to solve the pricing problems is reduced by 55% on average when minimizing routing costs and by 30% when minimizing the sum of the routes’ durations. In further computational experiments, we analyze algorithmic components of the BPC algorithm, compare the cost and duration objectives, and highlight the impact of the drones’ speed on the structure of VRP-D solutions. For the routing-cost minimization objective, our BPC algorithm is able to solve several VRP-D instances with 50 vertices to proven optimality within one hour of computation time. The same instances with duration minimization are more difficult, and the BPC algorithm provides only heuristic solutions with an average gap not exceeding 3%. Funding: This work was supported by Deutsche Forschungsgemeinschaft [Project 418727865, Grant IR 122/10-1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2024.0544 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024.0544},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {60-80},
  shortjournal = {Trans. Sci.},
  title        = {Exact solution of the vehicle routing problem with drones},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The online shortest path problem: Learning travel times
using a multiarmed bandit framework. <em>TRSC</em>, <em>59</em>(1),
28–59. (<a href="https://doi.org/10.1287/trsc.2023.0196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of e-commerce, logistics companies often operate within extensive road networks without accurate knowledge of travel times for their specific fleet of vehicles. Moreover, millions of dollars are spent on routing services that fail to accurately capture the unique characteristics of the drivers and vehicles of the companies. In this work, we address the challenge faced by a logistic operator with limited travel time information, aiming to find the optimal expected shortest path between origin-destination pairs. We model this problem as an online shortest path problem, common to many last-mile routing settings; given a graph whose arcs’ travel times are stochastic and follow an unknown distribution, the objective is to find a vehicle route of minimum travel time from an origin to a destination. The planner progressively collects travel condition data as drivers complete their routes. Inspired by the combinatorial multiarmed bandit and kriging literature, we propose three methods with distinct features to effectively learn the optimal shortest path, highlighting the practical advantages of incorporating spatial correlation in the learning process. Our approach balances exploration (improving estimates for unexplored arcs) and exploitation (executing the minimum expected time path) using the Thompson sampling algorithm. In each iteration, our algorithm executes the path that minimizes the expected travel time based on data from a posterior distribution of the speeds of the arcs. We conduct a computational study comprising two settings: a set of four artificial instances and a real-life case study. The case study uses empirical data of taxis in the 17-km-radius area of the center of Beijing, encompassing Beijing’s “5th Ring Road.” In both settings, our algorithms demonstrate efficient and effective balancing of the exploration-exploitation trade-off.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.0196},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {28-59},
  shortjournal = {Trans. Sci.},
  title        = {The online shortest path problem: Learning travel times using a multiarmed bandit framework},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plan your system and price for free: Fast algorithms for
multimodal transit operations. <em>TRSC</em>, <em>59</em>(1), 13–27. (<a
href="https://doi.org/10.1287/trsc.2022.0452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of jointly pricing and designing a smart transit system, where a transit agency (the platform ) controls a fleet of demand-responsive vehicles (cars) and a fixed line service (buses). The platform offers commuters a menu of options ( modes ) to travel between origin and destination (e.g., direct car trip, a bus ride, or a combination of the two), and commuters make a utility-maximizing choice within this menu, given the price of each mode. The goal of the platform is to determine an optimal set of modes to display to commuters, prices for these modes, and the design of the transit network in order to maximize the social welfare of the system. In this work, we tackle the commuter choice aspect of this problem, traditionally approached via computationally intensive bilevel programming techniques. In particular, we develop a framework that efficiently decouples the pricing and network design problem: Given an efficient (approximation) algorithm for centralized network design without prices , there exists an efficient (approximation) algorithm for decentralized network design with prices and commuter choice . We demonstrate the practicality of our framework via extensive numerical experiments on a real-world data set. We moreover explore the dependence of metrics such as welfare, revenue, and mode usage on (i) transfer costs and (ii) cost of contracting with on-demand service providers and exhibit the welfare gains of a fully integrated mobility system. Funding: This work was supported by the National Science Foundation [Awards CMMI-2308750, CNS-1952011, and CMMI-2144127]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2022.0452 .},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2022.0452},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {13-27},
  shortjournal = {Trans. Sci.},
  title        = {Plan your system and price for free: Fast algorithms for multimodal transit operations},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transportation science and logistics society best
dissertation award competition: Abstracts of 2024 winners.
<em>TRSC</em>, <em>59</em>(1), 5–12. (<a
href="https://doi.org/10.1287/trsc.2024_dissertation_award">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The journal is pleased to bring back the tradition of publishing the abstracts of the winners of the TSL Best Dissertation Award. The 2024 dissertation prize committee was chaired by Margaretha Gansterer. The other committee members were Shadi Sharif Azadeh, Justin Goodson, Tal Raviv, and Hai Wang.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2024_dissertation_award},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {5-12},
  shortjournal = {Trans. Sci.},
  title        = {Transportation science and logistics society best dissertation award competition: Abstracts of 2024 winners},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2024 transportation science meritorious service awards.
<em>TRSC</em>, <em>59</em>(1), 3–4. (<a
href="https://doi.org/10.1287/trsc.2023.servawards.v59.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are pleased to announce the recipients of the Transportation Science Meritorious Service Awards. These awards recognize associate editors, special issue guest editors, and reviewers who have offered exceptional service in the review process. We truly appreciate all the efforts of the many volunteers who provide invaluable service to the journal. The 2024 recipients have distinguished themselves by the number of papers handled, their efficiency in handling papers, and the quality of their reviews.},
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2023.servawards.v59.n1},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {3-4},
  shortjournal = {Trans. Sci.},
  title        = {2024 transportation science meritorious service awards},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial statement. <em>TRSC</em>, <em>59</em>(1), 1–2. (<a
href="https://doi.org/10.1287/trsc.2025.ed.v59.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TRSC},
  doi          = {10.1287/trsc.2025.ed.v59.n1},
  journal      = {Transportation Science},
  month        = {1-2},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Trans. Sci.},
  title        = {Editorial statement},
  volume       = {59},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
