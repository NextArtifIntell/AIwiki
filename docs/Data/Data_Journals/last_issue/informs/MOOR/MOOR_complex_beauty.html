<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MOOR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="moor---28">MOOR - 28</h2>
<ul>
<li><details>
<summary>
(2025). Steiner cut dominants. <em>MOOR</em>, <em>50</em>(1),
764–781. (<a href="https://doi.org/10.1287/moor.2022.0280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a subset T of nodes of an undirected graph G , a T-Steiner cut is a cut δ ( S ) with T ∩ S ≠ ø and T \ S ≠ ø . The T-Steiner cut dominant of G is the dominant CUT + ( G , T ) of the convex hull of the incidence vectors of the T -Steiner cuts of G . For T = { s , t } , this is the well-understood s - t -cut dominant. Choosing T as the set of all nodes of G , we obtain the cut dominant for which an outer description in the space of the original variables is still not known. We prove that for each integer τ , there is a finite set of inequalities such that for every pair ( G , T ) with | T | ≤ τ , the nontrivial facet-defining inequalities of CUT + ( G , T ) are the inequalities that can be obtained via iterated applications of two simple operations, starting from that set. In particular, the absolute values of the coefficients and of the right-hand sides in a description of CUT + ( G , T ) by integral inequalities can be bounded from above by a function of | T | . For all | T | ≤ 5 , we provide descriptions of CUT + ( G , T ) by facet-defining inequalities, extending the known descriptions of s - t -cut dominants.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0280},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {764-781},
  shortjournal = {Math. Oper. Res.},
  title        = {Steiner cut dominants},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uniqueness of convex-ranged probabilities and applications
to risk measures and games. <em>MOOR</em>, <em>50</em>(1), 743–763. (<a
href="https://doi.org/10.1287/moor.2023.0015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit Marinacci’s uniqueness theorem for convex-ranged probabilities and its applications. Our approach does away with both the countable additivity and the positivity of the charges involved. In the process, we uncover several new equivalent conditions, which lead to a novel set of applications. These include extensions of the classic Fréchet–Hoeffding bounds as well as of the automatic Fatou property of law-invariant functionals. We also generalize existing results of the “collapse to the mean”-type concerning capacities and α -MEU preferences.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0015},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {743-763},
  shortjournal = {Math. Oper. Res.},
  title        = {Uniqueness of convex-ranged probabilities and applications to risk measures and games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-path large deviations for unbounded additive
functionals of the reflected random walk. <em>MOOR</em>, <em>50</em>(1),
711–742. (<a href="https://doi.org/10.1287/moor.2020.0094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove a sample-path large deviation principle (LDP) with sublinear speed for unbounded functionals of certain Markov chains induced by the Lindley recursion. The LDP holds in the Skorokhod space D [ 0 , 1 ] equipped with the M 1 ′ topology. Our technique hinges on a suitable decomposition of the Markov chain in terms of regeneration cycles. Each regeneration cycle denotes the area accumulated during the busy period of the reflected random walk. We prove a large deviation principle for the area under the busy period of the Markov random walk, and we show that it exhibits a heavy-tailed behavior. Funding: The research of B. Zwart and M. Bazhba is supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek [Grant 639.033.413]. The research of J. Blanchet is supported by the National Science Foundation (NSF) [Grants 1915967, 1820942, and 1838576] as well as the Defense Advanced Research Projects Agency [Grant N660011824028]. The research of C.-H. Rhee is supported by the NSF [Grant CMMI-2146530].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2020.0094},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {711-742},
  shortjournal = {Math. Oper. Res.},
  title        = {Sample-path large deviations for unbounded additive functionals of the reflected random walk},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact characterization of the jointly optimal restocking and
auditing policy in inventory systems with record inaccuracy.
<em>MOOR</em>, <em>50</em>(1), 656–710. (<a
href="https://doi.org/10.1287/moor.2022.0145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a continuous-time stochastic model of an inventory system with record inaccuracy. In this formulation, demand is modeled by a point process and is observable only when it leads to sales. In addition to demand that can reduce the stock, an unobservable stochastic loss process can also reduce the stock. The retailer’s goal is to identify the restocking and auditing policy that minimizes the expected discounted cost of carrying a product over an infinite horizon. We analytically characterize the optimal restocking and jointly optimal auditing policy. We prove that the optimal restocking policy is a threshold policy. Our proof of this result is based on a coupling argument that is valid for any demand and loss model. Unlike the optimal restocking policy, the jointly optimal auditing policy is not of threshold type. We show that a complete proof of this statement cannot be obtained by solely resorting to the first-order stochastic dominance property of the Bayesian shelf stock distribution induced by the demand and loss process. Instead, our characterization of the jointly optimal auditing policy is based on proving that the dynamics of the shelf stock distribution constitute a (strictly) sign-regular kernel. To our knowledge, this is the first paper that characterizes the optimal policy of a complex control problem by establishing sign regularity of its underlying Markovian dynamics. Our theoretical results lead to a fast algorithm for computing the exact jointly optimal auditing/restocking policy over the problem’s entire state space. This enables comparative statics analysis, which allows us to determine how inventory record inaccuracy affects the economic significance of various cost drivers. This, in turn, allows us to determine when or, better, under what conditions auditing can be an effective tool for reducing the total cost.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0145},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {656-710},
  shortjournal = {Math. Oper. Res.},
  title        = {Exact characterization of the jointly optimal restocking and auditing policy in inventory systems with record inaccuracy},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast rates for the regret of offline reinforcement learning.
<em>MOOR</em>, <em>50</em>(1), 633–655. (<a
href="https://doi.org/10.1287/moor.2021.0167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the regret of offline reinforcement learning in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted Q -iteration (FQI), suggest root- n convergence for regret, empirical behavior exhibits much faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function, the regret of the policy it defines converges at a rate given by the exponentiation of the estimate’s pointwise convergence rate, thus speeding up the rate. The level of exponentiation depends on the level of noise in the decision-making problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman residual minimization to establish the correct pointwise convergence guarantees. As specific cases, our results imply one-over- n rates in linear cases and exponential-in- n rates in tabular cases. We extend our findings to general function approximation by extending our results to regret guarantees based on L p -convergence rates for estimating the optimal quality function rather than pointwise rates, where L 2 guarantees for nonparametric estimation can be ensured under mild conditions. Funding: This work was supported by the Division of Information and Intelligent Systems, National Science Foundation [Grant 1846210].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0167},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {633-655},
  shortjournal = {Math. Oper. Res.},
  title        = {Fast rates for the regret of offline reinforcement learning},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal investment strategy for α-robust utility
maximization problem. <em>MOOR</em>, <em>50</em>(1), 606–632. (<a
href="https://doi.org/10.1287/moor.2023.0076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reality, investors are uncertain about the dynamics of risky asset returns. Therefore, investors prefer to make robust investment decisions. In this paper, we propose an α-robust utility maximization problem under uncertain parameters. The investor is allowed to invest in a financial market consisting of a risk-free asset and a risky asset. The uncertainty about the expected return rate is parameterized by a nonempty set. Different from most existing literature on robust utility maximization problems where investors are generally assumed to be extremely ambiguity averse because they tend to consider only expected utility in the worst-case scenario, we pay attention to the investors who are not only ambiguity averse but also ambiguity seeking. Under power utility, we provide the implicit function representations for the precommitted strategy, equilibrium strategy of the open-loop type, and equilibrium strategy of the closed-loop type. Some properties about the optimal trading strategies, the best-case and worst-case parameters under three different kinds of strategies, are provided. Funding: This work was supported by National Natural Science Foundation of China [Grants 12071147, 12171169, 12271171, 12371470, 71721001, 71931004, 72371256], the Shanghai Philosophy Social Science Planning Office Project [Grant 2022ZJB005], Fundamental Research Funds for the Central Universities [Grant 2022QKT001], the Excellent Young Team Project Natural Science Foundation of Guangdong Province of China [Grant 2023B1515040001], the Philosophy and Social Science Programming Foundation of Guangdong Province [Grant GD22CYJ17], the Nature Science Foundation of Guangdong Province of China [Grant 2022A1515011472], and the 111 Project [Grant B14019].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0076},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {606-632},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal investment strategy for α-robust utility maximization problem},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel langevin pathwise average for gibbs
approximation. <em>MOOR</em>, <em>50</em>(1), 573–605. (<a
href="https://doi.org/10.1287/moor.2021.0243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a new multilevel method for the numerical approximation of a Gibbs distribution π on R d , based on (overdamped) Langevin diffusions. This method relies on a multilevel occupation measure, that is, on an appropriate combination of R occupation measures of (constant-step) Euler schemes with respective steps γ r = γ 0 2 − r , r = 0 , … , R . We first state a quantitative result under general assumptions that guarantees an ε-approximation (in an L 2 -sense) with a cost of the order ε − 2 or ε − 2 | log ε | 3 under less contractive assumptions. We then apply it to overdamped Langevin diffusions with strongly convex potential U : R d → R and obtain an ε-complexity of the order O ( d ε − 2 log 3 ( d ε − 2 ) ) or O ( d ε − 2 ) under additional assumptions on U . More precisely, up to universal constants, an appropriate choice of the parameters leads to a cost controlled by ( λ ¯ U ∨ 1 ) 2 λ ¯ U − 3 d ε − 2 (where λ ¯ U and λ ¯ U respectively denote the supremum and the infimum of the largest and lowest eigenvalue of D 2 U ). We finally complete these theoretical results with some numerical illustrations, including comparisons to other algorithms in Bayesian learning and opening to the non–strongly convex setting. Funding: The authors are grateful to the SIRIC ILIAD Nantes-Angers program, supported by the French National Cancer Institute [INCA-DGOS-Inserm Grant 12558].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0243},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {573-605},
  shortjournal = {Math. Oper. Res.},
  title        = {Multilevel langevin pathwise average for gibbs approximation},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semidefinite approximations for bicliques and bi-independent
pairs. <em>MOOR</em>, <em>50</em>(1), 537–572. (<a
href="https://doi.org/10.1287/moor.2023.0046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate some graph parameters dealing with bi-independent pairs ( A , B ) in a bipartite graph G = ( V 1 ∪ V 2 , E ) , that is, pairs ( A , B ) where A ⊆ V 1 , B ⊆ V 2 , and A ∪ B are independent. These parameters also allow us to study bicliques in general graphs. When maximizing the cardinality | A ∪ B | , one finds the stability number α ( G ) , well-known to be polynomial-time computable. When maximizing the product | A | · | B | , one finds the parameter g ( G ), shown to be NP-hard by Peeters in 2003, and when maximizing the ratio | A | · | B | / | A ∪ B | , one finds h ( G ), introduced by Vallentin in 2020 for bounding product-free sets in finite groups. We show that h ( G ) is an NP-hard parameter and, as a crucial ingredient, that it is NP-complete to decide whether a bipartite graph G has a balanced maximum independent set. These hardness results motivate introducing semidefinite programming (SDP) bounds for g ( G ), h ( G ), and α bal ( G ) (the maximum cardinality of a balanced independent set). We show that these bounds can be seen as natural variations of the Lovász ϑ-number, a well-known semidefinite bound on α ( G ) . In addition, we formulate closed-form eigenvalue bounds, and we show relationships among them as well as with earlier spectral parameters by Hoffman and Haemers in 2001 and Vallentin in 2020. Funding: This work was supported by H2020 Marie Skłodowska-Curie Actions [Grant 813211 (POEMA)].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0046},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {537-572},
  shortjournal = {Math. Oper. Res.},
  title        = {Semidefinite approximations for bicliques and bi-independent pairs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-field multiagent reinforcement learning: A
decentralized network approach. <em>MOOR</em>, <em>50</em>(1), 506–536.
(<a href="https://doi.org/10.1287/moor.2022.0055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges for multiagent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial information of the entire system. Whereas exciting progress has been made to analyze decentralized MARL with the network of agents for social networks and team video games, little is known theoretically for decentralized MARL with the network of states for modeling self-driving vehicles, ride-sharing, and data and traffic routing. This paper proposes a framework of localized training and decentralized execution to study MARL with the network of states. Localized training means that agents only need to collect local information in their neighboring states during the training phase; decentralized execution implies that agents can execute afterward the learned decentralized policies, which depend only on agents’ current states. The theoretical analysis consists of three key components: the first is the reformulation of the MARL system as a networked Markov decision process with teams of agents, enabling updating the associated team Q-function in a localized fashion; the second is the Bellman equation for the value function and the appropriate Q-function on the probability measure space; and the third is the exponential decay property of the team Q-function, facilitating its approximation with efficient sample efficiency and controllable error. The theoretical analysis paves the way for a new algorithm LTDE-N eural -AC, in which the actor–critic approach with overparameterized neural networks is proposed. The convergence and sample complexity are established and shown to be scalable with respect to the sizes of both agents and states. To the best of our knowledge, this is the first neural network–based MARL algorithm with network structure and provable convergence guarantee. Funding: X. Wei is partially supported by NSFC no. 12201343. R. Xu is partially supported by the NSF CAREER award DMS-2339240.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0055},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {506-536},
  shortjournal = {Math. Oper. Res.},
  title        = {Mean-field multiagent reinforcement learning: A decentralized network approach},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Marginal values of a stochastic game. <em>MOOR</em>,
<em>50</em>(1), 482–505. (<a
href="https://doi.org/10.1287/moor.2023.0297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-sum stochastic games are parameterized by payoffs, transitions, and possibly a discount rate. In this article, we study how the main solution concepts, the discounted and undiscounted values, vary when these parameters are perturbed. We focus on the marginal values, introduced by Mills in 1956 in the context of matrix games—that is, the directional derivatives of the value along any fixed perturbation. We provide a formula for the marginal values of a discounted stochastic game. Further, under mild assumptions on the perturbation, we provide a formula for their limit as the discount rate vanishes and for the marginal values of an undiscounted stochastic game. We also show, via an example, that the two latter differ in general. Funding: This work was supported by Fondation CFM pour la Recherche; the European Research Council [Grant ERC-CoG-863818 (ForM-SMArt)]; and Agence Nationale de la Recherche [Grant ANR-21-CE40-0020].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0297},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {482-505},
  shortjournal = {Math. Oper. Res.},
  title        = {Marginal values of a stochastic game},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence and stability of coupled belief-strategy
learning dynamics in continuous games. <em>MOOR</em>, <em>50</em>(1),
459–481. (<a href="https://doi.org/10.1287/moor.2022.0161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning dynamics to model how strategic agents repeatedly play a continuous game while relying on an information platform to learn an unknown payoff-relevant parameter. In each time step, the platform updates a belief estimate of the parameter based on players’ strategies and realized payoffs using Bayes’ rule. Then, players adopt a generic learning rule to adjust their strategies based on the updated belief. We present results on the convergence of beliefs and strategies and the properties of convergent fixed points of the dynamics. We obtain sufficient and necessary conditions for the existence of globally stable fixed points. We also provide sufficient conditions for the local stability of fixed points. These results provide an approach to analyzing the long-term outcomes that arise from the interplay between Bayesian belief learning and strategy learning in games and enable us to characterize conditions under which learning leads to a complete information equilibrium. Funding: Financial support from the Air Force Office of Scientific Research [Project Building Attack Resilience into Complex Networks], the Simons Institute [research fellowship], and a Michael Hammer Fellowship is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0161},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {459-481},
  shortjournal = {Math. Oper. Res.},
  title        = {Convergence and stability of coupled belief-strategy learning dynamics in continuous games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A policy gradient algorithm for the risk-sensitive
exponential cost MDP. <em>MOOR</em>, <em>50</em>(1), 431–458. (<a
href="https://doi.org/10.1287/moor.2022.0139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the risk-sensitive exponential cost Markov decision process (MDP) formulation and develop a trajectory-based gradient algorithm to find the stationary point of the cost associated with a set of parameterized policies. We derive a formula that can be used to compute the policy gradient from (state, action, cost) information collected from sample paths of the MDP for each fixed parameterized policy. Unlike the traditional average cost problem, standard stochastic approximation theory cannot be used to exploit this formula. To address the issue, we introduce a truncated and smooth version of the risk-sensitive cost and show that this new cost criterion can be used to approximate the risk-sensitive cost and its gradient uniformly under some mild assumptions. We then develop a trajectory-based gradient algorithm to minimize the smooth truncated estimation of the risk-sensitive cost and derive conditions under which a sequence of truncations can be used to solve the original, untruncated cost problem. Funding: This work was supported by the Office of Naval Research Global [Grant N0001419-1-2566], the Division of Computer and Network Systems [Grant 21-06801], the Army Research Office [Grant W911NF-19-1-0379], and the Division of Computing and Communication Foundations [Grants 17-04970 and 19-34986].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0139},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {431-458},
  shortjournal = {Math. Oper. Res.},
  title        = {A policy gradient algorithm for the risk-sensitive exponential cost MDP},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric semidefinite programming: Geometry of the
trajectory of solutions. <em>MOOR</em>, <em>50</em>(1), 410–430. (<a
href="https://doi.org/10.1287/moor.2021.0097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, solutions of convex optimization problems are updated on-line, as functions of time. In this paper, we consider parametric semidefinite programs, which are linear optimization problems in the semidefinite cone whose coefficients (input data) depend on a time parameter . We are interested in the geometry of the solution (output data) trajectory, defined as the set of solutions depending on the parameter . We propose an exhaustive description of the geometry of the solution trajectory. As our main result, we show that only six distinct behaviors can be observed at a neighborhood of a given point along the solution trajectory. Each possible behavior is then illustrated by an example. Funding: This work was supported by OP RDE [Grant CZ.02.1.01/0.0/0.0/16_019/0000765].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0097},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {410-430},
  shortjournal = {Math. Oper. Res.},
  title        = {Parametric semidefinite programming: Geometry of the trajectory of solutions},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the (im-)possibility of representing probability
distributions as a difference of i.i.d. Noise terms. <em>MOOR</em>,
<em>50</em>(1), 390–409. (<a
href="https://doi.org/10.1287/moor.2023.0081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A random variable is difference-form decomposable ( DFD ) if it may be written as the difference of two i.i.d. random terms. We show that densities of such variables exhibit a remarkable degree of structure. Specifically, a DFD density can be neither approximately uniform, nor quasiconvex, nor strictly concave. On the other hand, a DFD density need, in general, be neither unimodal nor logconcave. Regarding smoothness, we show that a compactly supported DFD density cannot be analytic and will often exhibit a kink even if its components are smooth. The analysis highlights the risks for model consistency resulting from the strategy widely adopted in the economics literature of imposing assumptions directly on a difference of noise terms rather than on its components.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0081},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {390-409},
  shortjournal = {Math. Oper. Res.},
  title        = {On the (Im-)Possibility of representing probability distributions as a difference of I.I.D. noise terms},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal consumption and investment with independent
stochastic labor income. <em>MOOR</em>, <em>50</em>(1), 356–389. (<a
href="https://doi.org/10.1287/moor.2023.0119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new dynamic continuous-time model of optimal consumption and investment to include independent stochastic labor income. We reduce the problem of solving the Bellman equation to a problem of solving an integral equation. We then explicitly characterize the optimal consumption and investment strategy as a function of income-to-wealth ratio. We provide some analytical comparative statics associated with the value function and optimal strategies. We also develop a quite general numerical algorithm for control iteration and solve the Bellman equation as a sequence of solutions to ordinary differential equations. This numerical algorithm can be readily applied to many other optimal consumption and investment problems especially with extra nondiversifiable Brownian risks, resulting in nonlinear Bellman equations. Finally, our numerical analysis illustrates how the presence of stochastic labor income affects the optimal consumption and investment strategy. Funding: A. Bensoussan was supported by the National Science Foundation under grant [DMS-2204795]. S. Park was supported by the Ministry of Education of the Republic of Korea and the National Research Foundation of Korea, South Korea [NRF-2022S1A3A2A02089950].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0119},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {356-389},
  shortjournal = {Math. Oper. Res.},
  title        = {Optimal consumption and investment with independent stochastic labor income},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strongly convergent homogeneous approximations to
inhomogeneous markov jump processes and applications. <em>MOOR</em>,
<em>50</em>(1), 334–355. (<a
href="https://doi.org/10.1287/moor.2022.0153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of time-inhomogeneous Markov jump processes is a traditional topic within probability theory that has recently attracted substantial attention in various applications. However, their flexibility also incurs a substantial mathematical burden which is usually circumvented by using well-known generic distributional approximations or simulations. This article provides a novel approximation method that tailors the dynamics of a time-homogeneous Markov jump process to meet those of its time-inhomogeneous counterpart on an increasingly fine Poisson grid. Strong convergence of the processes in terms of the Skorokhod J 1 metric is established, and convergence rates are provided. Under traditional regularity assumptions, distributional convergence is established for unconditional proxies, to the same limit. Special attention is devoted to the case where the target process has one absorbing state and the remaining ones transient, for which the absorption times also converge. Some applications are outlined, such as univariate hazard-rate density estimation, ruin probabilities, and multivariate phase-type density evaluation. Funding: M. Bladt and O. Peralta would like to acknowledge financial support from the Swiss National Science Foundation Project 200021_191984. O. Peralta acknowledges financial support from NSF Award #1653354 and AXA Research Fund Award on “Mitigating risk in the wake of the pandemic”.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0153},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {334-355},
  shortjournal = {Math. Oper. Res.},
  title        = {Strongly convergent homogeneous approximations to inhomogeneous markov jump processes and applications},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk sharing with lambda value at risk. <em>MOOR</em>,
<em>50</em>(1), 313–333. (<a
href="https://doi.org/10.1287/moor.2023.0246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the risk-sharing problem among multiple agents using lambda value at risk ( Λ VaR ) as their preferences via the tool of inf-convolution, where Λ VaR is an extension of value at risk ( VaR ). We obtain explicit formulas of the inf-convolution of multiple Λ VaR with monotone Λ and explicit forms of the corresponding optimal allocations, extending the results of the inf-convolution of VaR . It turns out that the inf-convolution of several Λ VaR is still a Λ VaR under some mild condition. Moreover, we investigate the inf-convolution of one Λ VaR and a general monotone risk measure without cash additivity, including Λ VaR , expected utility, and rank-dependent expected utility as special cases. The expression of the inf-convolution and the explicit forms of the optimal allocation are derived, leading to some partial solution of the risk-sharing problem with multiple Λ VaR for general Λ functions. Finally, we discuss the risk-sharing problem with Λ VaR + , another definition of lambda value at risk. We focus on the inf-convolution of Λ VaR + and a risk measure that is consistent with the second-order stochastic dominance, deriving very different expression of the inf-convolution and the forms of the optimal allocations.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0246},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {313-333},
  shortjournal = {Math. Oper. Res.},
  title        = {Risk sharing with lambda value at risk},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational inequalities on unbounded domains for zero-sum
singular controller vs. Stopper games. <em>MOOR</em>, <em>50</em>(1),
277–312. (<a href="https://doi.org/10.1287/moor.2023.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of zero-sum games between a singular controller and a stopper over a finite-time horizon. The underlying process is a multidimensional (locally nondegenerate) controlled stochastic differential equation (SDE) evolving in an unbounded domain. We prove that such games admit a value and provide an optimal strategy for the stopper. The value of the game is shown to be the maximal solution in a suitable Sobolev class of a variational inequality of min-max type with an obstacle constraint and a gradient constraint. Although the variational inequality and the game are solved on an unbounded domain, we do not require boundedness of either the coefficients of the controlled SDE or of the cost functions in the game. Funding: A. Bovo was partially supported by the Doctoral Studentship from the University of Leeds.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0029},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {277-312},
  shortjournal = {Math. Oper. Res.},
  title        = {Variational inequalities on unbounded domains for zero-sum singular controller vs. stopper games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A characterization of simultaneous optimization,
majorization, and (bi-)submodular polyhedra. <em>MOOR</em>,
<em>50</em>(1), 252–276. (<a
href="https://doi.org/10.1287/moor.2023.0054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by resource allocation problems (RAPs) in power management applications, we investigate the existence of solutions to optimization problems that simultaneously minimize the class of Schur-convex functions, also called least-majorized elements. For this, we introduce a generalization of majorization and least-majorized elements, called ( a , b )-majorization and least ( a , b )-majorized elements, and characterize the feasible sets of problems that have such elements in terms of base and (bi-)submodular polyhedra. Hereby, we also obtain new characterizations of these polyhedra that extend classical characterizations in terms of optimal greedy algorithms from the 1970s. We discuss the implications of our results for RAPs in power management applications and derive a new characterization of convex cooperative games and new properties of optimal estimators of specific regularized regression problems. In general, our results highlight the combinatorial nature of simultaneously optimizing solutions and provide a theoretical explanation for why such solutions generally do not exist.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0054},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {252-276},
  shortjournal = {Math. Oper. Res.},
  title        = {A characterization of simultaneous optimization, majorization, and (Bi-)Submodular polyhedra},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stationary points of a shallow neural network with quadratic
activations and the global optimality of the gradient descent algorithm.
<em>MOOR</em>, <em>50</em>(1), 209–251. (<a
href="https://doi.org/10.1287/moor.2021.0082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of training a shallow neural network with quadratic activation functions and the generalization power of such trained networks. Assuming that the samples are generated by a full rank matrix W * of the hidden network node weights, we obtain the following results. We establish that all full-rank approximately stationary solutions of the risk minimization problem are also approximate global optimums of the risk (in-sample and population). As a consequence, we establish that, when trained on polynomially many samples, the gradient descent algorithm converges to the global optimum of the risk minimization problem regardless of the width of the network when it is initialized at some value ν * , which we compute. Furthermore, the network produced by the gradient descent has a near zero generalization error. Next, we establish that initializing the gradient descent algorithm below ν * is easily achieved when the weights of the ground truth matrix W * are randomly generated and the matrix is sufficiently overparameterized. Finally, we identify a simple necessary and sufficient geometric condition on the size of the training set under which any global minimizer of the empirical risk has necessarily zero generalization error. Funding: The research of E. C. Kizildag is supported by Columbia University, with the Distinguished Postdoctoral Fellowship in Statistics. Support from the National Science Foundation [Grant DMS-2015517] is gratefully acknowledged.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0082},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {209-251},
  shortjournal = {Math. Oper. Res.},
  title        = {Stationary points of a shallow neural network with quadratic activations and the global optimality of the gradient descent algorithm},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Submodular functions and perfect graphs. <em>MOOR</em>,
<em>50</em>(1), 189–208. (<a
href="https://doi.org/10.1287/moor.2021.0302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a combinatorial polynomial-time algorithm to find a maximum weight independent set in perfect graphs of bounded degree that do not contain a prism or a hole of length four as an induced subgraph. An even pair in a graph is a pair of vertices all induced paths between which are even. An even set is a set of vertices every two of which are an even pair. We show that every perfect graph that does not contain a prism or a hole of length four as an induced subgraph has a balanced separator which is the union of a bounded number of even sets, where the bound depends only on the maximum degree of the graph. This allows us to solve the maximum weight independent set problem using the well-known submodular function minimization algorithm. Funding: This work was supported by the Engineering and Physical Sciences Research Council [Grant EP/V002813/1]; the National Science Foundation [Grants DMS-1763817, DMS-2120644, and DMS-2303251]; and Alexander von Humboldt-Stiftung.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0302},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {189-208},
  shortjournal = {Math. Oper. Res.},
  title        = {Submodular functions and perfect graphs},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluctuation theory of continuous-time, skip-free downward
markov chains with applications to branching processes with immigration.
<em>MOOR</em>, <em>50</em>(1), 169–188. (<a
href="https://doi.org/10.1287/moor.2022.0246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a comprehensive methodology for the fluctuation theory of continuous-time, skip-free Markov chains, extending and improving the recent work of Choi and Patie for discrete-time, skip-free Markov chains. As a significant application, we use it to derive a full set of fluctuation identities regarding exiting a finite or infinite interval for Markov branching processes with immigration, thereby uncovering many new results for this classic family of continuous-time Markov chains. The theory also allows us to recover in a simple manner fluctuation identities for skip-free downward compound Poisson processes.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0246},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {169-188},
  shortjournal = {Math. Oper. Res.},
  title        = {Fluctuation theory of continuous-time, skip-free downward markov chains with applications to branching processes with immigration},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternating and parallel proximal gradient methods for
nonsmooth, nonconvex minimax: A unified convergence analysis.
<em>MOOR</em>, <em>50</em>(1), 141–168. (<a
href="https://doi.org/10.1287/moor.2022.0294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in nonconvex minimax problems that is driven by an abundance of applications. Our focus is on nonsmooth, nonconvex-strongly concave minimax, thus departing from the more common weakly convex and smooth models assumed in the recent literature. We present proximal gradient schemes with either parallel or alternating steps. We show that both methods can be analyzed through a single scheme within a unified analysis that relies on expanding a general convergence mechanism used for analyzing nonconvex, nonsmooth optimization problems. In contrast to the current literature, which focuses on the complexity of obtaining nearly approximate stationary solutions, we prove subsequence convergence to a critical point of the primal objective and global convergence when the latter is semialgebraic. Furthermore, the complexity results we provide are with respect to approximate stationary solutions. Lastly, we expand the scope of problems that can be addressed by generalizing one of the steps with a Bregman proximal gradient update, and together with a few adjustments to the analysis, this allows us to extend the convergence and complexity results to this broader setting. Funding: The research of E. Cohen was partially supported by a doctoral fellowship from the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240]. The research of M. Teboulle was partially supported by the Israel Science Foundation [Grant 2619-20] and Deutsche Forschungsgemeinschaft [Grant 800240].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0294},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {141-168},
  shortjournal = {Math. Oper. Res.},
  title        = {Alternating and parallel proximal gradient methods for nonsmooth, nonconvex minimax: A unified convergence analysis},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling in the high-uncertainty heavy traffic regime.
<em>MOOR</em>, <em>50</em>(1), 107–140. (<a
href="https://doi.org/10.1287/moor.2022.0100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model uncertainty approach to heavy traffic asymptotics that allows for a high level of uncertainty. That is, the uncertainty classes of underlying distributions accommodate disturbances that are of order 1 at the usual diffusion scale as opposed to asymptotically vanishing disturbances studied previously in relation to heavy traffic. A main advantage of the approach is that the invariance principle underlying diffusion limits makes it possible to define uncertainty classes in terms of the first two moments only. The model we consider is a single-server queue with multiple job types. The problem is formulated as a zero sum stochastic game played between the system controller, who determines scheduling and attempts to minimize an expected linear holding cost, and an adversary, who dynamically controls the service time distributions of arriving jobs and attempts to maximize the cost. The heavy traffic asymptotics of the game are fully solved. It is shown that an asymptotically optimal policy for the system controller is to prioritize according to an index rule, and for the adversary, it is to select distributions based on the system’s current workload. The workload-to-distribution feedback mapping is determined by a Hamilton–Jacobi–Bellman equation, which also characterizes the game’s limit value. Unlike in the vast majority of results in the heavy traffic theory and as a direct consequence of the diffusive size disturbances, the limiting dynamics under asymptotically optimal play are captured by a stochastic differential equation where both the drift and the diffusion coefficients may be discontinuous. Funding: R. Atar is supported by the Israeli Science Foundation [Grant 1035/20].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0100},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {107-140},
  shortjournal = {Math. Oper. Res.},
  title        = {Scheduling in the high-uncertainty heavy traffic regime},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial voting rules. <em>MOOR</em>, <em>50</em>(1),
90–106. (<a href="https://doi.org/10.1287/moor.2023.0080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study a new class of polynomial voting rules for a general decentralized decision/consensus system, and more specifically for the proof-of-stake protocol. The main idea, inspired by the Penrose square-root law and the more recent quadratic voting rule, is to differentiate a voter’s voting power and the voter’s share (fraction of the total in the system). We show that, whereas voter shares form a martingale process that converges to a Dirichlet distribution, their voting powers follow a supermartingale process that decays to zero over time. This prevents any voter from controlling the voting process and, thus, enhances security. For both limiting results, we also provide explicit rates of convergence. When the initial total volume of votes (or stakes) is large, we show a phase transition in share stability (or the lack thereof), corresponding to the voter’s initial share relative to the total. We also study the scenario in which trading (of votes/stakes) among the voters is allowed and quantify the level of risk sensitivity (or risk aversion) in three categories, corresponding to the voter’s utility being a supermartingale, a submartingale, and a martingale. For each category, we identify the voter’s best strategy in terms of participation and trading. Funding: W. Tang gratefully acknowledges financial support through the National Science Foundation [Grants DMS-2113779 and DMS-2206038] and through a start-up grant at Columbia University. D. D. Yao’s work is part of a Columbia–City University/Hong Kong collaborative project that is supported by InnoHK Initiative, the Government of Hong Kong Special Administrative Region, and the Laboratory for AI-Powered Financial Technologies.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2023.0080},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {90-106},
  shortjournal = {Math. Oper. Res.},
  title        = {Polynomial voting rules},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow allocation games. <em>MOOR</em>, <em>50</em>(1), 68–89.
(<a href="https://doi.org/10.1287/moor.2022.0355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a game-theoretic variant of the maximum circulation problem. In a flow allocation game , we are given a directed flow network. Each node is a rational agent and can strategically allocate any incoming flow to the outgoing edges. Given the strategy choices of all agents, a maximal circulation that adheres to the chosen allocation strategies evolves in the network. Each agent wants to maximize the amount of flow through his or her node. Flow allocation games can be used to express strategic incentives of clearing in financial networks. We provide a cumulative set of results on the existence and computational complexity of pure Nash and strong equilibria as well as tight bounds on the (strong) prices of anarchy and stability. Our results show an interesting dichotomy. Ranking strategies over individual flow units allows us to obtain optimal strong equilibria for many objective functions. In contrast, more intuitive ranking strategies over edges can give rise to unfavorable incentive properties. Funding: This work was supported by Deutsche Forschungsgemeinschaft Research Group ADYN [411362735].},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2022.0355},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {68-89},
  shortjournal = {Math. Oper. Res.},
  title        = {Flow allocation games},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimal problem dependent generalization error
bounds in statistical learning theory. <em>MOOR</em>, <em>50</em>(1),
40–67. (<a href="https://doi.org/10.1287/moor.2021.0076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study problem-dependent rates, that is, generalization errors that scale near-optimally with the variance, effective loss, or gradient norms evaluated at the “best hypothesis.” We introduce a principled framework dubbed “uniform localized convergence” and characterize sharp problem-dependent rates for central statistical learning problems. From a methodological viewpoint, our framework resolves several fundamental limitations of existing uniform convergence and localization analysis approaches. It also provides improvements and some level of unification in the study of localized complexities, one-sided uniform inequalities, and sample-based iterative algorithms. In the so-called “slow rate” regime, we provide the first (moment-penalized) estimator that achieves the optimal variance-dependent rate for general “rich” classes; we also establish an improved loss-dependent rate for standard empirical risk minimization. In the “fast rate” regime, we establish finite-sample, problem-dependent bounds that are comparable to precise asymptotics. In addition, we show that iterative algorithms such as gradient descent and first order expectation maximization can achieve optimal generalization error in several representative problems across the areas of nonconvex learning, stochastic optimization, and learning with missing data. Supplemental Material: The online appendix is available at https://doi.org/10.1287/moor.2021.0076 .},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2021.0076},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {40-67},
  shortjournal = {Math. Oper. Res.},
  title        = {Towards optimal problem dependent generalization error bounds in statistical learning theory},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The online saddle point problem and online convex
optimization with knapsacks. <em>MOOR</em>, <em>50</em>(1), 1–39. (<a
href="https://doi.org/10.1287/moor.2018.0330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online saddle point problem, an online learning problem where at each iteration, a pair of actions needs to be chosen without knowledge of the current and future (convex-concave) payoff functions. The objective is to minimize the gap between the cumulative payoffs and the saddle point value of the aggregate payoff function, which we measure using a metric called saddle point regret (SP-Regret). The problem generalizes the online convex optimization framework, but here, we must ensure that both players incur cumulative payoffs close to that of the Nash equilibrium of the sum of the games. We propose an algorithm that achieves SP-Regret proportional to ln ( T ) T in the general case, and log ( T ) SP-Regret for the strongly convex-concave case. We also consider the special case where the payoff functions are bilinear and the decision sets are the probability simplex. In this setting, we are able to design algorithms that reduce the bounds on SP-Regret from a linear dependence in the dimension of the problem to a logarithmic one. We also study the problem under bandit feedback and provide an algorithm that achieves sublinear SP-Regret. We then consider an online convex optimization with knapsacks problem motivated by a wide variety of applications, such as dynamic pricing, auctions, and crowdsourcing. We relate this problem to the online saddle point problem and establish O ( T ) regret using a primal-dual algorithm.},
  archive      = {J_MOOR},
  doi          = {10.1287/moor.2018.0330},
  journal      = {Mathematics of Operations Research},
  month        = {2},
  number       = {1},
  pages        = {1-39},
  shortjournal = {Math. Oper. Res.},
  title        = {The online saddle point problem and online convex optimization with knapsacks},
  volume       = {50},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
