<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc---79">TMC - 79</h2>
<ul>
<li><details>
<summary>
(2025). MagSpy: Revealing user privacy leakage via magnetometer on
mobile devices. <em>TMC</em>, <em>24</em>(3), 2455–2469. (<a
href="https://doi.org/10.1109/TMC.2024.3495506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various characteristics of mobile applications (apps) and associated in-app services can reveal potentially-sensitive user information; however, privacy concerns have prompted third-party apps to restrict access to data related to mobile app usage. This paper outlines a novel approach to extracting detailed app usage information by analyzing electromagnetic (EM) signals emitted from mobile devices during app-related tasks. The proposed system, MagSpy, recovers user privacy information from magnetometer readings that do not require access permissions. This EM leakage becomes complex when multiple apps are used simultaneously and is subject to interference from geomagnetic signals generated by device movement. To address these challenges, MagSpy employs multiple techniques to extract and identify signals related to app usage. Specifically, the geomagnetic offset signal is canceled using accelerometer and gyroscope sensor data, and a Cascade-LSTM algorithm is used to classify apps and in-app services. MagSpy also uses CWT-based peak detection and a Random Forest classifier to detect PIN inputs. A prototype system was evaluated on over 50 popular mobile apps with 30 devices. Extensive evaluation results demonstrate the efficacy of MagSpy in identifying in-app services (96% accuracy), apps (93.5% accuracy), and extracting PIN input information (96% top-3 accuracy).},
  archive      = {J_TMC},
  author       = {Yongjian Fu and Lanqing Yang and Hao Pan and Yi-Chao Chen and Guangtao Xue and Ju Ren},
  doi          = {10.1109/TMC.2024.3495506},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2455-2469},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MagSpy: Revealing user privacy leakage via magnetometer on mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling feedback-free MIMO transmission for FD-RAN: A
data-driven approach. <em>TMC</em>, <em>24</em>(3), 2437–2454. (<a
href="https://doi.org/10.1109/TMC.2024.3495719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance flexibility and facilitate resource cooperation, a novel fully-decoupled radio access network (FD-RAN) architecture is proposed for 6G. However, the decoupling of uplink (UL) and downlink (DL) in FD-RAN makes the existing feedback mechanism ineffective. To this end, we propose an end-to-end data-driven MIMO solution without the conventional channel feedback procedure. Data-driven MIMO can alleviate the drawbacks of feedback including overheads and delay, and can provide customized precoding design for different BSs based on their historical channel data. It essentially learns a mapping from geolocation to MIMO transmission parameters. We first present a codebook-based approach, which selects transmission parameters from the statistics of discrete channel state information (CSI) values and utilizes nearest neighbor interpolation for spatial inference. We further present a non-codebook-based approach, which 1) derives the optimal precoder from the singular value decomposition (SVD) of the channel; 2) utilizes variational autoencoder (VAE) to select the representative precoder from the latent Gaussian representations; and 3) exploits Gaussian process regression (GPR) to predict unknown precoders in the space domain. Extensive simulations are performed on a link-level 5G simulator using realistic ray-tracing channel data. The results demonstrate the effectiveness of data-driven MIMO, showcasing its potential for application in FD-RAN and 6G.},
  archive      = {J_TMC},
  author       = {Jingbo Liu and Jiacheng Chen and Zongxi Liu and Haibo Zhou},
  doi          = {10.1109/TMC.2024.3495719},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2437-2454},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling feedback-free MIMO transmission for FD-RAN: A data-driven approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPUF: An entropy-derived latency-based DRAM physical
unclonable function for lightweight authentication in internet of
things. <em>TMC</em>, <em>24</em>(3), 2422–2436. (<a
href="https://doi.org/10.1109/TMC.2024.3494612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical Unclonable Functions (PUFs) are hardware-based mechanisms that exploit inherent manufacturing variations to generate unique identifiers for devices. Dynamic Random Access Memory (DRAM) has emerged as a promising medium for implementing PUFs, providing a cost-effective solution without the need for additional circuitry. This makes DRAM PUFs ideal for use in resource-constrained environments such as Internet of Things (IoT) networks. However, current DRAM PUF implementations often either disrupt host system functions or produce unreliable responses due to environmental sensitivity. In this paper, we present EPUF, a novel approach to extracting random and unique features from DRAM cells to generate reliable PUF responses. We leverage bitmap images of binary DRAM values and their entropy features to enhance the robustness of our PUF. Through extensive real-world experiments, we demonstrate that EPUF is approximately 1.7 times faster than existing solutions, achieves 100% reliability, produces features with 47.79% uniqueness, and supports a substantial set of Challenge-Response Pairs (CRPs). These capabilities make EPUF a powerful tool for DRAM PUF-based authentication. Based on EPUF, we then propose a lightweight authentication protocol that not only offers superior security features but also surpasses state-of-the-art authentication schemes in terms of communication overhead and computational efficiency.},
  archive      = {J_TMC},
  author       = {Fatemeh Najafi and Masoud Kaveh and Mohammad Reza Mosavi and Alessandro Brighente and Mauro Conti},
  doi          = {10.1109/TMC.2024.3494612},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2422-2436},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EPUF: An entropy-derived latency-based DRAM physical unclonable function for lightweight authentication in internet of things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biasing federated learning with a new adversarial graph
attention network. <em>TMC</em>, <em>24</em>(3), 2407–2421. (<a
href="https://doi.org/10.1109/TMC.2024.3499371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness in Federated Learning (FL) is imperative not only for the ethical utilization of technology but also for ensuring that models provide accurate, equitable, and beneficial outcomes across varied user demographics and equipment. This paper proposes a new adversarial architecture, referred to as Adversarial Graph Attention Network (AGAT), which deliberately instigates fairness attacks with an aim to bias the learning process across the FL. The proposed AGAT is developed to synthesize malicious, biasing model updates, where the minimum of Kullback-Leibler (KL) divergence between the user&#39;s model update and the global model is maximized. Due to a limited set of labeled input-output biasing data samples, a surrogate model is created, which presents the behavior of a complex malicious model update. Moreover, a graph autoencoder (GAE) is designed within the AGAT architecture, which is trained together with sub-gradient descent to reconstruct manipulatively the correlations of the model updates, and maximize the reconstruction loss while keeping the malicious, biasing model updates undetectable. The proposed AGAT attack is implemented in PyTorch, showing experimentally that AGAT successfully increases the minimum value of KL divergence of benign model updates by 60.9% and bypasses the detection of existing defense models. The source code of the AGAT attack is released on GitHub.},
  archive      = {J_TMC},
  author       = {Kai Li and Jingjing Zheng and Wei Ni and Hailong Huang and Pietro Liò and Falko Dressler and Ozgur B. Akan},
  doi          = {10.1109/TMC.2024.3499371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2407-2421},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Biasing federated learning with a new adversarial graph attention network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligence-based reinforcement learning for dynamic
resource optimization in edge computing-enabled vehicular networks.
<em>TMC</em>, <em>24</em>(3), 2394–2406. (<a
href="https://doi.org/10.1109/TMC.2024.3506161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent transportation systems demand efficient resource allocation and task offloading to ensure low-latency, high-bandwidth vehicular services. The dynamic nature of vehicular environments, characterized by high mobility and extensive interactions among vehicles, necessitates considering time-varying statistical regularities, especially in scenarios with sharp variations. Despite the widespread use of traditional reinforcement learning for resource allocation, its limitations in generalization and interpretability are evident. To overcome these challenges, we propose an Intelligence-based Reinforcement Learning (IRL) algorithm. This algorithm utilizes active inference to infer the real world and maintain an internal model by minimizing free energy. Enhancing the efficiency of active inference, we incorporate prior knowledge as macro guidance, ensuring more accurate and efficient training. By constructing an intelligence-based model, we eliminate the need for designing reward functions, aligning better with human thinking, and providing a method to reflect the learning, information transmission and intelligence accumulation processes. This approach also allows for quantifying intelligence to a certain extent. Considering the dynamic and uncertain nature of vehicular scenarios, we apply the IRL algorithm to environments with constantly changing parameters. Extensive simulations confirm the effectiveness of IRL, significantly improving the generalization and interpretability of intelligent models in vehicular networks.},
  archive      = {J_TMC},
  author       = {Yuhang Wang and Ying He and F. Richard Yu and Kaishun Wu and Shanzhi Chen},
  doi          = {10.1109/TMC.2024.3506161},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2394-2406},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intelligence-based reinforcement learning for dynamic resource optimization in edge computing-enabled vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring long-term commensalism: Throughput maximization
for symbiotic radio networks. <em>TMC</em>, <em>24</em>(3), 2376–2393.
(<a href="https://doi.org/10.1109/TMC.2024.3495015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbiotic radio (SR), combining the advantages of cognitive radio and ambient backscatter communication (AmBC), stands as a promising solution for spectrum-and-energy-efficient wireless communications. In an SR network, backscatter devices (BDs) share the spectrum resources with the primary transmitter (PT) by utilizing the incident radio frequency (RF) signal from PT for uplink non-orthogonal multiple access (NOMA) transmission. The primary receiver (PR) decodes the signals of PT and BDs via the successive interference cancellation (SIC) technique. Our goal is to establish a long-term commensalistic relationship between PT and BDs. We address the problem of maximizing the long-term average sum rate of BDs while ensuring a minimum average rate for the PT by optimizing the power reflection coefficients of the BDs. We explicitly consider practical constraints such as the required power difference among signals for SIC decoding and the unknown future channel state information (CSI). We prove the NP-hardness of the offline version of the problem and subsequently employ the Lyapunov optimization technique to convert the original problem into a series of sub-problems in each individual time slot that can be solved in an online manner without relying on future CSI. We then utilize the successive convex optimization (SCO) technique to solve the non-convex sub-problems. Extensive simulations validate that our proposed Lyapunov-SCO algorithm achieves superior performance in terms of the average sum rate of BDs while ensuring PT’s required average rate. In addition, we provide discussions on extending the proposed solution to SR networks with multiple PT-PR pairs, high-mobility BDs, and enhancing fairness among BDs.},
  archive      = {J_TMC},
  author       = {Yuzhe Chen and Yanjun Li and Chung Shue Chen and Kaikai Chi},
  doi          = {10.1109/TMC.2024.3495015},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2376-2393},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring long-term commensalism: Throughput maximization for symbiotic radio networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAiMo: Motif density enhances topology robustness for highly
dynamic scale-free IoT. <em>TMC</em>, <em>24</em>(3), 2360–2375. (<a
href="https://doi.org/10.1109/TMC.2024.3492002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust Topology is a key prerequisite to providing consistent connectivity for highly dynamic Internet-of-Things (IoT) applications that are suffering node failures. In this paper, we present a two-step approach to organizing the most robust IoT topology. First, we propose a novel robustness metric denoted as $I$, which is based on network motifs and is specifically designed to sensitively analyze the dynamic changes in topology resulting from node failures. Second, we introduce a Distributed duAl-layer collaborative competition optimization strategy based on Motif density (DAiMo). This strategy significantly expands the search space for optimal solutions and facilitates the identification of the optimal IoT topology. We utilize the motif density concept in the collaborative optimization process to efficiently search for the optimal topology. To support our approach, extensive mathematical proofs are provided to demonstrate the advantages of the metric $I$ in effectively perceiving changes in IoT topology and to establish the convergence of the DAiMo algorithm. Finally, we conduct comprehensive performance evaluations of DAiMo and investigate the influence of network motifs on the resilience and reliability of IoT topologies. Experimental results clearly indicate that the proposed method outperforms existing state-of-the-art topology optimization methods in terms of enhancing network robustness.},
  archive      = {J_TMC},
  author       = {Ning Chen and Tie Qiu and Weisheng Si and Dapeng Oliver Wu},
  doi          = {10.1109/TMC.2024.3492002},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2360-2375},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DAiMo: Motif density enhances topology robustness for highly dynamic scale-free IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative task offloading and resource allocation in
small-cell MEC: A multi-agent PPO-based scheme. <em>TMC</em>,
<em>24</em>(3), 2346–2359. (<a
href="https://doi.org/10.1109/TMC.2024.3496536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-cell mobile edge computing (SE-MEC) networks amalgamate the virtues of MEC and small-cell networks, enhancing data processing capabilities of user devices (UDs). Nevertheless, time-varying wireless channels, dynamic UD requirements, and severe interference among UDs make it difficult to fully exploit the limited network resources and stably provide computing services for UDs. Therefore, efficient task offloading and resource allocation (TORA) is essential. Moreover, since multiple small cells are deployed, decentralized TORA schemes are preferred in practice. Thus, this paper aims to design distributed adaptive TORA schemes for SE-MEC networks. In pursuit of an eco-friendly design, an optimization problem is formulated to minimize the total energy consumption (TEC) of UDs subject to delay constraints. To effectively deal with network&#39;s dynamic characteristics, the reinforce learning framework is applied, where the TEC minimization problem is first modeled as a partially observable Markov decision process (POMDP), and then an efficient multi-agent proximal policy optimization (MAPPO)-based scheme is presented to solve it. In the presented scheme, each small-cell base station (SBS) serves as an agent and is capable of making TORA decisions only with its own local information. To promote collaboration among multiple agents, a global reward function is designed. A state normalization mechanism is also introduced into the presented scheme for enhancing learning performance. Simulation results show that although the proposed MAPPO-based scheme works in a distributed manner, it achieves very similar performance to the centralized one. In addition, it is demonstrated that the state normalization mechanism has a significant effect on reducing TEC.},
  archive      = {J_TMC},
  author       = {Han Li and Ke Xiong and Yuping Lu and Wei Chen and Pingyi Fan and Khaled Ben Letaief},
  doi          = {10.1109/TMC.2024.3496536},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2346-2359},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative task offloading and resource allocation in small-cell MEC: A multi-agent PPO-based scheme},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAST: Efficient traffic scenario inpainting in cellular
vehicle-to-everything systems. <em>TMC</em>, <em>24</em>(3), 2331–2345.
(<a href="https://doi.org/10.1109/TMC.2024.3492148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising vehicular communication technology, Cellular Vehicle-to-Everything (C-V2X) is expected to ensure the safety and convenience of Intelligent Transportation Systems (ITS) by providing global road information. However, it is difficult to obtain global road information in practical scenarios since there will still be many vehicles on the road without onboard units (OBUs) in the near future. Specifically, although C-V2X vehicles have sensors that can perceive their surroundings and broadcast their perceived information to the C-V2X system, their line-of-sight (LoS) is limited and obscured by the environment, such as other vehicles and terrain. Besides, vehicles without OBUs cannot share their perceived information. These two problems cause extensive areas with unperceived information in the C-V2X system, and whether vehicles are in these areas is unknown. Thus, extending the perceivable range of the limited scenario for C-V2X applications that require global road information is necessary. To this end, this paper pioneers investigating the scenario inpainting task problem in C-V2X. To solve this challenging problem, we propose an effiCient trAffic Scenario inpainTing (CAST) solution consisting of a generative architecture and knowledge distillation, simultaneously considering the inpainting precision and computation efficiency. Extensive experiments have been conducted to demonstrate the effectiveness of CAST in terms of Precise Inpaint Rate (PIR), Rough Inpaint Rate (RIR), Lane-Level Inpaint Rate (LLIR), and Inpaint Confidence Error (ICE), paving the way for novel solutions for the inpainting problem in more complex road scenarios.},
  archive      = {J_TMC},
  author       = {Liang Zhao and Chaojin Mao and Shaohua Wan and Ammar Hawbani and Ahmed Y. Al-Dubai and Geyong Min and Albert Y. Zomaya},
  doi          = {10.1109/TMC.2024.3492148},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2331-2345},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAST: Efficient traffic scenario inpainting in cellular vehicle-to-everything systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semi-supervised federated learning with
progressive training in heterogeneous edge computing. <em>TMC</em>,
<em>24</em>(3), 2315–2330. (<a
href="https://doi.org/10.1109/TMC.2024.3492140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an efficient distributed learning method that facilitates collaborative model training among multiple edge devices (or clients). However, current research always assumes that clients have access to ground-truth data for training, which is unrealistic in practice because of a lack of expertise. Semi-supervised federated learning (SSFL) has been proposed in many existing works to address this problem, which always adopts a fixed model architecture for training, bringing two main problems with varying amounts of pseudo-labeled data. First, the shallow model cannot have the capability to fit the increasing pseudo-labeled data, leading to poor training performance. Second, the large model suffers from an overfitting problem when exploiting a few labeled data samples in SSFL, and also requires tremendous resource (e.g., computation and communication) costs. To tackle these problems, we propose a novel framework, called star, which adopts progressive training to enhance model training in SSFL. Specifically, star gradually increases the model depth through adding the sub-module (e.g., one or several layers) from a shallow model, and performs pseudo-labeling for unlabeled data with a specialized confidence threshold simultaneously. Then, we propose an efficient algorithm to determine the appropriate model depth for each client with varied resource budgets and the proper confidence threshold for pseudo-labeling in SSFL. The experimental results demonstrate the high effectiveness of STAR. For instance, star can reduce the bandwidth consumption by about 40%, and achieve an average accuracy improvement of around 9.8% compared with the baselines, on CIFAR10.},
  archive      = {J_TMC},
  author       = {Jianchun Liu and Jun Liu and Hongli Xu and Yunming Liao and Zhiwei Yao and Min Chen and Chen Qian},
  doi          = {10.1109/TMC.2024.3492140},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2315-2330},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing semi-supervised federated learning with progressive training in heterogeneous edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise distribution decomposition based multi-agent
distributional reinforcement learning. <em>TMC</em>, <em>24</em>(3),
2301–2314. (<a href="https://doi.org/10.1109/TMC.2024.3492272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, Reinforcement Learning (RL) agent updates its policy by repetitively interacting with the environment, contingent on the received rewards to observed states and undertaken actions. However, the environmental disturbance, commonly leading to noisy observations (e.g., rewards and states), could significantly shape the performance of agent. Furthermore, the learning performance of Multi-Agent Reinforcement Learning (MARL) is more susceptible to noise due to the interference among intelligent agents. Therefore, it becomes imperative to revolutionize the design of MARL, so as to capably ameliorate the annoying impact of noisy rewards. In this paper, we propose a novel decomposition-based multi-agent distributional RL method by approximating the globally shared noisy reward by a Gaussian Mixture Model (GMM) and decomposing it into the combination of individual distributional local rewards, with which each agent can be updated locally through distributional RL. Moreover, a Diffusion Model (DM) is leveraged for reward generation in order to mitigate the issue of costly interaction expenditure for learning distributions. Furthermore, the monotonicity of the reward distribution decomposition is theoretically validated under nonnegative weights and increasing distortion risk function, while the design of the loss function is carefully calibrated to avoid decomposition ambiguity. We also verify the effectiveness of the proposed method through extensive simulation experiments with noisy rewards. Besides, different risk-sensitive policies are evaluated in order to demonstrate the superiority of distributional RL in different MARL tasks.},
  archive      = {J_TMC},
  author       = {Wei Geng and Baidi Xiao and Rongpeng Li and Ning Wei and Dong Wang and Zhifeng Zhao},
  doi          = {10.1109/TMC.2024.3492272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2301-2314},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Noise distribution decomposition based multi-agent distributional reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harvesting physical-layer randomness in millimeter wave
bands. <em>TMC</em>, <em>24</em>(3), 2285–2300. (<a
href="https://doi.org/10.1109/TMC.2024.3499876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unpredictability of the wireless channel has been used as a natural source of randomness to build physical-layer security primitives for shared key generation, authentication, access control, proximity verification, and other security properties. Compared to pseudo-random generators, it has the potential to achieve information-theoretic security. In sub-6 GHz frequencies, the randomness is harvested from the small-scale fading effects of RF signal propagation in rich scattering environments. However, the RF propagation characteristics follow sparse models with clustered paths when devices operate in millimeter-wave (mmWave) bands (5G and Next-Generation networks, Wi-Fi in 60GHz). Millimeter-wave transmissions are typically directional to increase the gain and combat high signal attenuation, leading to stable and more predictable channels. In this paper, we first demonstrate that state-of-the-art methods relying on channel state information or received signal strength measurements fail to produce high randomness. Accounting for the unique features of mmWave propagation, we propose a novel randomness extraction mechanism that exploits the random timing of channel blockage to harvest random bits. Compared with the prior art in CSI-based and context-based randomness extraction, our protocol remains secure against passive and active Man-in-the-Middle adversaries co-located with the legitimate devices. We demonstrate the security properties of our method in a 28 GHz mmWave testbed in an indoor setting.},
  archive      = {J_TMC},
  author       = {Ziqi Xu and Jingcheng Li and Yanjun Pan and Ming Li and Loukas Lazos},
  doi          = {10.1109/TMC.2024.3499876},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2285-2300},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Harvesting physical-layer randomness in millimeter wave bands},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual network computation offloading based on DRL for
satellite-terrestrial integrated networks. <em>TMC</em>, <em>24</em>(3),
2270–2284. (<a href="https://doi.org/10.1109/TMC.2024.3493388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite-terrestrial integrated networks based on edge computing can provide computation offloading service to terminal devices in remote areas. However, it faces various limitations, including satellite energy consumption, computation delay, and environmental dynamics, etc. In this paper, we propose a satellite-terrestrial integrated cloud and edge computing network (STCECN) architecture, including satellite layer, terrestrial layer and cloud center, where computing resources exist in multi-layer heterogeneous edge computing clusters. Optimization of system delay and energy consumption is defined as a mixed-integer programming problem. Moreover, we present a deep reinforcement learning-based computation offloading decision algorithm that can adapt to the dynamics and variability of satellite networks. A dual network computation offloading decision method is proposed for delay and energy consumption based on deep reinforcement learning offloading (DRLO), including deep convolutional network update method, quantization strategy, and bandwidth resource allocation. Meanwhile, the proposed method is based on previous experience and integrates deviation adjustment strategies for decision making to solve the problem of pseudo-patch loss caused by satellite network switching. The simulation results indicate that the proposed method performs almost consistently with traditional heuristic algorithms, with only 20% of the time consumption of the latter, and the number of pseudo packet loss also decreases to the original 10–20%.},
  archive      = {J_TMC},
  author       = {Dongbo Li and Yuchen Sun and Jielun Peng and Siyao Cheng and Zhisheng Yin and Nan Cheng and Jie Liu and Zhijun Li and Chenren Xu},
  doi          = {10.1109/TMC.2024.3493388},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2270-2284},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual network computation offloading based on DRL for satellite-terrestrial integrated networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient missing key tag identification in large-scale RFID
systems: An iterative verification and selection method. <em>TMC</em>,
<em>24</em>(3), 2253–2269. (<a
href="https://doi.org/10.1109/TMC.2024.3493597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio frequency identification (RFID) system has been extensively employed to track missing items by affixing them with RFID tags. Many practical applications require to efficiently identify missing events for a specific subset of system tags (called key tags) due to their elevated importance. Existing methods primarily aim to identify all tags, which makes it challenging to specifically identify key tags because of interference from other non-key tags (called ordinary tags). In light of this, several key tag identification methods follow a two-step scheme that filters ordinary tags first and then identifies key tags. Nevertheless, this wastes too much time on tag filtering, resulting in low time efficiency. This paper presents a novel missing key tag identification protocol with two creative designs to gain high efficiency. First, we develop a novel verification technique that can rapidly determine the presence or absence of key tags amid the scenarios with both key tags and ordinary ones. By combining the ON-OFF Keying modulation, we could verify multiple key tags in a single slot, thereby reducing the total slots required. Second, we design a new selection technique that efficiently selects the unverified key tags for further verification, while filtering out the verified key tags and irrelevant ordinary tags to avoid redundant data transmission. Additionally, we present an enhancement protocol that leverages a preselection technique to avoid collecting useless tag responses, further boosting efficiency. We carry out rigorous theoretical analysis to optimize the performance of the proposed protocols. Both simulations and practical experiments demonstrate that our method is markedly superior to state-of-the-art solutions.},
  archive      = {J_TMC},
  author       = {Jiangjin Yin and Xin Xie and Hangyu Mao and Song Guo},
  doi          = {10.1109/TMC.2024.3493597},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2253-2269},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient missing key tag identification in large-scale RFID systems: An iterative verification and selection method},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving federated neural architecture search with
enhanced robustness for edge computing. <em>TMC</em>, <em>24</em>(3),
2234–2252. (<a href="https://doi.org/10.1109/TMC.2024.3490835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of large-scale artificial intelligence services, edge devices are becoming essential providers of data and computing power. However, these edge devices are not immune to malicious attacks. Federated learning (FL), while protecting privacy of decentralized data through secure aggregation, struggles to trace adversaries and lacks optimization for heterogeneity. We discover that FL augmented with Differentiable Architecture Search (DARTS) can improve resilience against backdoor attacks while compatible with secure aggregation. Based on this, we propose a federated neural architecture search (NAS) framwork named SLNAS. The architecture of SLNAS is built on three pivotal components: a server-side search space generation method that employs an evolutionary algorithm with dual encodings, a federated NAS process based on DARTS, and client-side architecture tuning that utilizes Gumbel softmax combined with knowledge distillation. To validate robustness, we adapt a framework that includes backdoor attacks based on trigger optimization, data poisoning, and model poisoning, targeting both model weights and architecture parameters. Extensive experiments demonstrate that SLNAS not only effectively counters advanced backdoor attacks but also handles heterogeneity, outperforming defense baselines across a wide range of backdoor attack scenarios.},
  archive      = {J_TMC},
  author       = {Feifei Zhang and Mao Li and Jidong Ge and Fenghui Tang and Sheng Zhang and Jie Wu and Bin Luo},
  doi          = {10.1109/TMC.2024.3490835},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2234-2252},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving federated neural architecture search with enhanced robustness for edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-sensitive task offloading in integrated
satellite-terrestrial networks. <em>TMC</em>, <em>24</em>(3), 2220–2233.
(<a href="https://doi.org/10.1109/TMC.2024.3489619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sixth-generation (6G) communication technology, global communication networks are moving towards the goal of comprehensive and seamless coverage. In particular, low earth orbit (LEO) satellites have become a critical component of satellite communication networks. The emergence of LEO satellites has brought about new computational resources known as the LEO satellite edge, enabling ground users (GU) to offload computing tasks to the resource-rich LEO satellite edge. However, existing LEO satellite computational offloading solutions primarily focus on optimizing system performance, neglecting the potential issue of malicious satellite attacks during task offloading. In this paper, we propose the deployment of LEO satellite edge in an integrated satellite-terrestrial networks (ISTN) structure to support security-sensitive computing task offloading. We model the task allocation and offloading order problem as a joint optimization problem to minimize task offloading delay, energy consumption, and the number of attacks while satisfying reliability constraints. To achieve this objective, we model the task offloading process as a Markov decision process (MDP) and propose a security-sensitive task offloading strategy optimization algorithm based on proximal policy optimization (PPO). Experimental results demonstrate that our algorithm significantly outperforms other benchmark methods in terms of performance.},
  archive      = {J_TMC},
  author       = {Wenjun Lan and Kongyang Chen and Jiannong Cao and Yikai Li and Ning Li and Qi Chen and Yuvraj Sahni},
  doi          = {10.1109/TMC.2024.3489619},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2220-2233},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-sensitive task offloading in integrated satellite-terrestrial networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETBP-TD: An efficient and trusted bilateral
privacy-preserving truth discovery scheme for mobile crowdsensing.
<em>TMC</em>, <em>24</em>(3), 2203–2219. (<a
href="https://doi.org/10.1109/TMC.2024.3489717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowdsensing (MCS) has emerged as a promising sensing paradigm for accomplishing large-scale tasks by leveraging ubiquitously distributed mobile workers. Due to the variability in sensory data provided by different workers, identifying truth values from them has garnered wide attention. However, existing truth discovery schemes either offer limited privacy protection or incur high participation costs and lower data aggregation quality due to malicious workers. In this paper, we propose an Efficient and Trusted Bilateral Privacy-preserving Truth Discovery scheme (ETBP-TD) to obtain high-quality truth values while preventing privacy leakage from both workers and the data requester. Specifically, a matrix encryption-based protocol is introduced to the whole truth discovery process, which keeps locations and data related to tasks and workers secret from other entries. Additionally, trust-based worker recruitment and trust update mechanisms are first integrated within a privacy-preserving truth discovery scheme to enhance truth value accuracy and reduce unnecessary participation costs. Our theoretical analyses on the security and regret of ETBP-TD, along with extensive simulations on real-world datasets, demonstrate that ETBP-TD effectively preserves workers’ and tasks’ privacy while reducing the estimated error by up to 84.40% and participation cost by 54.72%.},
  archive      = {J_TMC},
  author       = {Jing Bai and Jinsong Gui and Tian Wang and Houbing Song and Anfeng Liu and Neal N. Xiong},
  doi          = {10.1109/TMC.2024.3489717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2203-2219},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ETBP-TD: An efficient and trusted bilateral privacy-preserving truth discovery scheme for mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive scheme for protecting source location privacy in
underwater acoustic sensor networks. <em>TMC</em>, <em>24</em>(3),
2193–2202. (<a href="https://doi.org/10.1109/TMC.2024.3489722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the source location privacy (SLP) becomes a hot research interest in network security of Underwater Acoustic Sensor Networks (UASNs), and existing schemes are mostly proposed for a given scenario. Introducing source location privacy technologies inevitably increase the energy consumption of nodes, while they are widely deployed in available studies, resulting in massive energy wastage. Therefore, an adaptive scheme for protecting source location privacy (APSLP) in UASNs is proposed. The APSLP scheme first analyzes the possible locations of the adversary by trust method. Then, considering the lagging nature of the trust method, which means that the adversary may not stay in locations given by trust method, a hidden Markov-based backtracking method is proposed and location privacy methods are functioned according to the backtracking result. The simulation shows that even though the security level of the APSLP scheme is not the largest, the efficiency is the highest, approximately an increase of 69.1$\%$ and 10.3$\%$ compared with two comparison algorithms, respectively.},
  archive      = {J_TMC},
  author       = {Hao Wang and Huijuan Zheng and Guangjie Han and Dong Tang},
  doi          = {10.1109/TMC.2024.3489722},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2193-2202},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An adaptive scheme for protecting source location privacy in underwater acoustic sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel method to solve the maximum weight clique problem
for instantly decodable network coding. <em>TMC</em>, <em>24</em>(3),
2181–2192. (<a href="https://doi.org/10.1109/TMC.2024.3489724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing the decoding delay, the completion time, or the delivery time of instantly decodable network coding (IDNC) can all be approximated to a maximum weight clique (MWC) problem, which is well known to be NP hard. Due to its good tradeoff between performance and computational complexity, a heuristic approach named as maximum weight vertex (MWV) search is widely employed to select MWC for IDNC. However, in MWV, when there are few coding connection edges among the adjacent vertices of a vertex, its modified vertex weight cannot well reflect the weight of the MWC containing the vertex, which leads to incorrect selection of MWC. This paper proposes a new method to calculate the modified weight of a vertex by summing the weights of the vertices in the approximate maximum weight path (A-MWP) generated by this vertex. Since the vertices in an A-MWP can form a maximal clique, the proposed modified vertex weight may well indicate the weight of the MWC containing the vertex. The proposed algorithm has the same computational complexity as the MWV algorithm. Simulation results show that when employing any of the three performance metrics of IDNC, our proposed algorithm can achieve better system performance than the MWV algorithm.},
  archive      = {J_TMC},
  author       = {Zhonghui Mei},
  doi          = {10.1109/TMC.2024.3489724},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2181-2192},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel method to solve the maximum weight clique problem for instantly decodable network coding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IFNet: Deep imaging and focusing for handheld SAR with
millimeter-wave signals. <em>TMC</em>, <em>24</em>(3), 2166–2180. (<a
href="https://doi.org/10.1109/TMC.2024.3489641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements have showcased the potential of handheld millimeter-wave (mmWave) imaging, which applies synthetic aperture radar (SAR) principles in portable settings. However, existing studies addressing handheld motion errors either rely on costly tracking devices or employ simplified imaging models, leading to impractical deployment or limited performance. In this paper, we present IFNet, a novel deep unfolding network that combines the strengths of signal processing models and deep neural networks to achieve robust imaging and focusing for handheld mmWave systems. We first formulate the handheld imaging model by integrating multiple priors about mmWave images and handheld phase errors. Furthermore, we transform the optimization processes into an iterative network structure for improved and efficient imaging performance. Extensive experiments demonstrate that IFNet effectively compensates for handheld phase errors and recovers high-fidelity images from severely distorted signals. In comparison with existing methods, IFNet can achieve at least 11.89 dB improvement in average peak signal-to-noise ratio (PSNR) and 64.91% improvement in average structural similarity index measure (SSIM) on a real-world dataset.},
  archive      = {J_TMC},
  author       = {Yadong Li and Dongheng Zhang and Ruixu Geng and Jincheng Wu and Yang Hu and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2024.3489641},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2166-2180},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IFNet: Deep imaging and focusing for handheld SAR with millimeter-wave signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing IEEE 802.11ax network performance: An
investigation and modeling into multi-user transmission. <em>TMC</em>,
<em>24</em>(3), 2151–2165. (<a
href="https://doi.org/10.1109/TMC.2024.3493032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the performance optimization of uplink orthogonal frequency division multiple access (OFDMA)-based random access (UORA) in IEEE 802.11ax networks. UORA supports multi-user transmission via two methods, where users transmit either fixed-size or variable-size aggregated MAC protocol data units. However, three critical issues arise. 1 Existing studies only focus on the fixed-size method with low practicality, and overlook the impact of traffic load which leads to inaccurate evaluation of the network performance. 2 The variable-size method has never been studied due to a complex scenario, where user frames append padding bits to fulfill the transmission opportunity constraint. 3 In realistic networks, the variable-size method sacrifices throughput to achieve high practicality and low latency. To address the first two issues, we proposed two novel models based on queueing theory that accurately capture the impact of these transmission methods and various parameters (e.g., the traffic load and padding bits) on throughput, packet loss rate, and latency. To address Issue 3, we design a Dynamic Selection Algorithm of Transmission Methods (DSATM), which dynamically switches between the two transmission methods to enhance practicality, maximize throughput, and minimize latency. Finally, we conducted extensive simulations to verify the accuracy of our models and DSATM.},
  archive      = {J_TMC},
  author       = {Jin Meng and Qinglin Zhao and Weimin Wu and Minghao Jin and Penghui Song and Yingzhuang Liu},
  doi          = {10.1109/TMC.2024.3493032},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2151-2165},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing IEEE 802.11ax network performance: An investigation and modeling into multi-user transmission},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A wireless AI-generated content (AIGC) provisioning
framework empowered by semantic communication. <em>TMC</em>,
<em>24</em>(3), 2137–2150. (<a
href="https://doi.org/10.1109/TMC.2024.3493375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant advances in AI-generated content (AIGC) and the proliferation of mobile devices, providing high-quality AIGC services via wireless networks is becoming the future direction. However, the primary challenges of AIGC services provisioning in wireless networks lie in unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To this end, this paper proposes a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be generated and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion models within the semantic encoder and decoder to design a workload-adjustable transceiver thereby allowing adjustment of computational resource utilization in edge and local. In addition, a resource-aware workload trade-off (ROOT) scheme is devised to intelligently make workload adaptation decisions for the transceiver, thus efficiently generating, transmitting, and fine-tuning content as per dynamic wireless channel conditions and service requirements. Simulations verify the superiority of our proposed SemAIGC framework in terms of latency and content quality compared to conventional approaches.},
  archive      = {J_TMC},
  author       = {Runze Cheng and Yao Sun and Dusit Niyato and Lan Zhang and Lei Zhang and Muhammad Ali Imran},
  doi          = {10.1109/TMC.2024.3493375},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2137-2150},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A wireless AI-generated content (AIGC) provisioning framework empowered by semantic communication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater target tracking based on interrupted
software-defined multi-AUV reinforcement learning: A multi-AUV
time-saving MARL approach. <em>TMC</em>, <em>24</em>(3), 2124–2136. (<a
href="https://doi.org/10.1109/TMC.2024.3490545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of underwater materials technology and underwater robot technology, human exploitation of marine resources has been increasingly advanced, which has given rise to various application scenarios for Autonomous Underwater Vehicle (AUV) cluster networks, such as cooperative data collection and target tracking. In this paper, we aim to explore how to utilize networking and swarm intelligence to improve the AUV cluster network’s target tracking performance in a time-saving manner. Specifically, on account of our previous work, we introduce an underwater interrupted mechanism and propose an Interrupted Software-Defined Multi-AUV Reinforcement Learning (ISD-MARL) architecture. For MARL algorithm in ISD-MARL, we propose a time-saving MARL algorithm, S-MADDPG, integrating our proposed action optimization model and action network loss function, to accelerate the convergence of the MARL algorithm. Furthermore, to further improve the AUV cluster network’s path planning performance during the target tracking, we propose an Interrupted Tracking Path Planning Scheme (ITPPS) for the AUV cluster network based on the proposed ISD-MARL and S-MADDPG. The evaluation results showcase that our proposed scheme can effectively plan the underwater target tracking path for the AUV cluster network in a shorter time and outperform various mainstream strategies in terms of convergence speed and training time, etc.},
  archive      = {J_TMC},
  author       = {Shengchao Zhu and Guangjie Han and Chuan Lin and Yu Zhang},
  doi          = {10.1109/TMC.2024.3490545},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2124-2136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Underwater target tracking based on interrupted software-defined multi-AUV reinforcement learning: A multi-AUV time-saving MARL approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sampling for age of information in non-stationary
network traffic. <em>TMC</em>, <em>24</em>(3), 2110–2123. (<a
href="https://doi.org/10.1109/TMC.2024.3493592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time status updates play an important role in low-latency cyber-physical systems, in which the real network traffic statistics (i.e., transmission delay and/or error rate) are often unknown and non-stationary. In such cases, short-time age-of-information (ST-AoI) is more crucial than long-term average AoI, because instantaneous high ST-AoI could lead to system failures even if the long-term average AoI is low. In this paper, we propose an adaptive sampling control (ASC) scheme to ensure a low ST-AoI outage probability, defined as the probability of the average AoI in each control cycle, i.e., over a limited number of packets, exceeding a given threshold. This ASC scheme does not rely on an explicit statistical model for the non-stationary traffic behaviors. It establishes a dynamic linearization data model with a pseudo-partial derivative (PPD) parameter to capture the unknown and non-stationary traffic statistics. By estimating the PPD parameter in each control cycle, ASC can determine the sampling rates to ensure an extremely low ST-AoI outage probability. Both numerical simulation and real-world experiment show that the proposed ASC scheme significantly outperforms existing methods, reducing the ST-AoI outage probability almost by half.},
  archive      = {J_TMC},
  author       = {Yifan Gu and Zhi Quan},
  doi          = {10.1109/TMC.2024.3493592},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2110-2123},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive sampling for age of information in non-stationary network traffic},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient satellite-ground interconnection design for
low-orbit mega-constellation topology. <em>TMC</em>, <em>24</em>(3),
2098–2109. (<a href="https://doi.org/10.1109/TMC.2024.3490575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-orbit mega-constellation network (LMCN) is an important part of the space-air-ground integrated network system. An effective satellite-ground interconnection design can result in a stable constellation topology for LMCNs. A naïve solution is accessing the satellite with the longest remaining service time (LRST), which is widely used in previous designs. The Coordinated Satellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm, coordinates the establishment of ground-satellite links (GSLs). Compared with existing solutions, it reduces latency by 19% and jitter by 70% on average. However, CSGI only supports the scenario where terminals access only one satellite, and cannot fully utilize the multi-access capabilities of terminals. Additionally, CSGI&#39;s high computational complexity poses deployment challenges. To overcome these problems, we propose the Classification-based Longest Remaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario with multi-access capabilities. It adds optional paths during routing with low computational complexity, improving end-to-end communications quality. We conduct our 1000 s simulation from Brazil to Lithuania on the open-source platform Hypatia. Experiment results show that compared with CSGI, C-LRST reduces the latency and increases the throughput by approximately 60% and 40%, respectively. In addition, C-LRST&#39;s GSL switchings number is 14, whereas CSGI is 23. C-LRST has better link stability than CSGI.},
  archive      = {J_TMC},
  author       = {Wenhao Liu and Jiazhi Wu and Quanwei Lin and Handong Luo and Qi Zhang and Kun Qiu and Zhe Chen and Yue Gao},
  doi          = {10.1109/TMC.2024.3490575},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2098-2109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient satellite-ground interconnection design for low-orbit mega-constellation topology},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REM: Enabling real-time neural-enhanced video streaming on
mobile devices using macroblock-aware lookup table. <em>TMC</em>,
<em>24</em>(3), 2085–2097. (<a
href="https://doi.org/10.1109/TMC.2024.3496443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for mobile video streaming has seen a substantial surge in recent years. However, current platforms heavily depend on network capacity to ensure the delivery of high-quality video streams. The emergence of neural-enhanced video streaming presents a promising solution to address this challenge by leveraging client-side computation, thereby reducing bandwidth consumption. Nonetheless, deploying advanced super-resolution (SR) models on mobile devices is hindered by the computational demands of existing SR models. In this paper, we propose REM, a novel neural-enhanced mobile video streaming framework. REM utilizes a customized lookup table to facilitate real-time neural-enhanced video streaming on mobile devices. Initially, we conduct a series of measurements to identify abundant macroblock redundancies across frames in a video stream. Subsequently, we introduce a dynamic macroblock selection algorithm that prioritizes important macroblocks for neural enhancement. The SR-enhanced results are stored in the lookup table and efficiently reused to meet real-time requirements and minimize resource overhead. By considering macroblock-level characteristics of the video frames, the lookup table enables efficient and fast processing. Additionally, we design a lightweight macroblock-aware SR module to expedite inference. Finally, we perform extensive experiments on various mobile devices. The results demonstrate that REM enhances overall processing throughput by up to 10.2 times and reduces power consumption by up to 58.6% compared to state-of-the-art methods. Consequently, this leads to a 38.06% improvement in the quality of experience for mobile users.},
  archive      = {J_TMC},
  author       = {Baili Chai and Di Wu and Jinyu Chen and Mengyu Yang and Zelong Wang and Miao Hu},
  doi          = {10.1109/TMC.2024.3496443},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2085-2097},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {REM: Enabling real-time neural-enhanced video streaming on mobile devices using macroblock-aware lookup table},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneity-aware cooperative federated edge learning with
adaptive computation and communication compression. <em>TMC</em>,
<em>24</em>(3), 2073–2084. (<a
href="https://doi.org/10.1109/TMC.2024.3492916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the drawbacks of cloud-based federated learning (FL), cooperative federated edge learning (CFEL) has been proposed to improve efficiency for FL over mobile edge networks, where multiple edge servers collaboratively coordinate the distributed model training across a large number of edge devices. However, CFEL faces critical challenges arising from dynamic and heterogeneous device properties, which slow down the convergence and increase resource consumption. This paper proposes a heterogeneity-aware CFEL scheme called Heterogeneity-Aware Cooperative Edge-based Federated Averaging (HCEF) that aims to maximize the model accuracy while minimizing the training time and energy consumption via adaptive computation and communication compression in CFEL. By theoretically analyzing how local update frequency and gradient compression affect the convergence error bound in CFEL, we develop an efficient online control algorithm for HCEF to dynamically determine local update frequencies and compression ratios for heterogeneous devices. Experimental results show that compared with prior schemes, the proposed HCEF scheme can maintain higher model accuracy while reducing training latency and improving energy efficiency simultaneously.},
  archive      = {J_TMC},
  author       = {Zhenxiao Zhang and Zhidong Gao and Yuanxiong Guo and Yanmin Gong},
  doi          = {10.1109/TMC.2024.3492916},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2073-2084},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Heterogeneity-aware cooperative federated edge learning with adaptive computation and communication compression},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A “breathing” mobile communication network. <em>TMC</em>,
<em>24</em>(3), 2056–2072. (<a
href="https://doi.org/10.1109/TMC.2024.3487213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The frequent migration of large-scale users leads to the load imbalance of mobile communication networks, which causes resource waste and decreases user experience. To address the load balancing problem, this paper proposes a dynamic optimization framework for mobile communication networks inspired by the average consensus in multi-agent systems. In this framework, all antennas cooperatively optimize their CPICH (Common Pilot Channel) transmit power in real-time to balance their busy-degrees. Then, the coverage area of each antenna would change accordingly, and we call this framework a “breathing” mobile communication network. To solve this optimization problem, two algorithms named BDBA (Busy-degree Dynamic Balancing Algorithm) and BFDBA (Busy-degree Fast Dynamic Balancing Algorithm) are proposed. Moreover, a fast network coverage calculation method is introduced, by which each antenna&#39;s minimum CPICH transmit power is determined under the premise of meeting the network coverage requirements. Besides, we present the theoretical analysis of the two proposed algorithms’ performance, which prove that all antennas’ busy-degrees will reach consensus under certain assumptions. Furthermore, simulations carried out on three large datasets demonstrate that our cooperative optimization can significantly reduce the unbalance among antennas as well as the proportion of over-busy antennas.},
  archive      = {J_TMC},
  author       = {Chao Ge and Ge Chen and Zhipeng Jiang},
  doi          = {10.1109/TMC.2024.3487213},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2056-2072},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A “Breathing” mobile communication network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). V2I-coop: Accurate object detection for connected automated
vehicles at accident black spots with V2I cross-modality cooperation.
<em>TMC</em>, <em>24</em>(3), 2043–2055. (<a
href="https://doi.org/10.1109/TMC.2024.3486758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate object detection with on-board LiDAR sensors is crucial for ensuring driving safety of Connected Automated Vehicles (CAVs), especially at accident black spots with more occlusions. Fortunately, road-side infrastructure equipped with traffic cameras is usually available at these places, offers an extensive field of view and encounters fewer occlusions, and thus can provide sustained assistance to CAVs to improve their object detection performance. However, vehicle-to-infrastructure (V2I) cooperative object detection is quite challenging due to modality heterogeneity, agent heterogeneity, and bandwidth limitations. To address these challenges, in this paper, we propose V2I-Coop, an accurate object detection approach with V2I cross-modality cooperation for CAVs to improve perception performance at accident black spots. In V2I-Coop, first, we extract bird-eye-view (BEV) features from both multi-view 2D images and 3D point clouds, which facilitates the feature fusion of different modalities. Next, the most valuable features from the images are adaptively selected according to available bandwidth and then transmitted to CAVs. Then, a cross-modality feature fusion algorithm is adopted at CAVs to mitigate the modality difference and improve the feature fusion efficiency. Finally, extensive experiments demonstrate that V2I-Coop significantly improves the 3D object detection performance of CAVs at accident black spots.},
  archive      = {J_TMC},
  author       = {Xiaobo Zhou and Chuanan Wang and Qi Xie and Tie Qiu},
  doi          = {10.1109/TMC.2024.3486758},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2043-2055},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {V2I-coop: Accurate object detection for connected automated vehicles at accident black spots with V2I cross-modality cooperation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explanation-guided backdoor attacks against model-agnostic
RF fingerprinting systems. <em>TMC</em>, <em>24</em>(3), 2029–2042. (<a
href="https://doi.org/10.1109/TMC.2024.3487967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the proven capabilities of deep neural networks (DNNs) in identifying devices through radio frequency (RF) fingerprinting, the security vulnerabilities of these deep learning models have been largely overlooked. While the threat of backdoor attacks is well-studied in the image domain, few works have explored this threat in the context of RF signals. In this paper, we thoroughly analyze the susceptibility of DNN-based RF fingerprinting to backdoor attacks, focusing on a more practical scenario where attackers lack access to control model gradients and training processes. We propose leveraging explainable machine learning techniques and autoencoders to guide the selection of trigger positions and values, allowing for the creation of effective backdoor triggers in a model-agnostic manner. To comprehensively evaluate this backdoor attack, we employ four diverse datasets with two protocols (Wi-Fi and LoRa) across various DNN architectures. Given that RF signals are often transformed into the frequency or time-frequency domains, this study also assesses attack efficacy in the time-frequency domain. Furthermore, we experiment with potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack. Additionally, we consider the attack performance in the domain shift case.},
  archive      = {J_TMC},
  author       = {Tianya Zhao and Junqing Zhang and Shiwen Mao and Xuyu Wang},
  doi          = {10.1109/TMC.2024.3487967},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2029-2042},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Explanation-guided backdoor attacks against model-agnostic RF fingerprinting systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClassTer: Mobile shift-robust personalized federated
learning via class-wise clustering. <em>TMC</em>, <em>24</em>(3),
2014–2028. (<a href="https://doi.org/10.1109/TMC.2024.3487294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of mobile devices with abundant sensor data and computing power has driven the trend of federated learning (FL) on them. Personalized FL (PFL) aims to train tailored models for each device, addressing data heterogeneity from diverse user behaviors and preferences. However, due to dynamic mobile environments, PFL faces challenges in test-time data shifts, i.e., variations between training and testing. While this issue is well studied in generic deep learning through model generalization or adaptation, this issue remains less explored in PFL, where models often overfit local data. To address this, we introduce ${\sf ClassTer}$, a shift-robust PFL framework. We observe that class-wise clustering of clients in cluster-based PFL (CFL) can avoid class-specific biases by decoupling the training of classes. Thus, we propose a paradigm shift from traditional client-wise clustering to class-wise clustering, which allows effective aggregation of cluster models into a generalized one via knowledge distillation. Additionally, we extend ClassTer to asynchronous mobile clients to optimize wall clock time by leveraging critical learning periods and both intra- and inter-device scheduling. Experiments show that compared to status quo approaches, ${\sf ClassTer}$ achieves a reduction of up to 91% in convergence time, and an improvement of up to 50.45% in accuracy.},
  archive      = {J_TMC},
  author       = {Xiaochen Li and Sicong Liu and Zimu Zhou and Yuan Xu and Bin Guo and Zhiwen Yu},
  doi          = {10.1109/TMC.2024.3487294},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2014-2028},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ClassTer: Mobile shift-robust personalized federated learning via class-wise clustering},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing in-situ structural health monitoring through RF
energy-powered sensor nodes and mobile platform. <em>TMC</em>,
<em>24</em>(3), 1999–2013. (<a
href="https://doi.org/10.1109/TMC.2024.3491574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research contributes to long-term structural health monitoring (SHM) by exploring radio frequency energy-powered sensor nodes (RF-SNs) embedded in concrete. The RF-SN captures radio energy from a mobile radio transmitter for sensing and communication, which offers a cost-effective solution for consistent in-situ perception. To optimize the system performance across various situations, we’ve explored both active and passive communication methods. For the active RF-SN, we implement a specialized control circuit enabling the node to transmit data through ZigBee protocol at low incident power. For the passive RF-SN, radio energy is not only for power but also as a carrier signal, with data conveyed by modulating the amplitude of the backscattered radio wave. To address the challenge of significant attenuation of the backscattering signal in concrete, we utilize a square chirp-based modulation scheme for passive communication. This scheme allows the receiver to successfully decode the data even under a negative signal-to-noise ratio (SNR) condition. Performance modeling and optimization for both active and passive RF-SNs are provided in this study. The experimental results verify that an active RF-SN embedded in concrete at a depth of 13.5 cm can be effectively powered by a 915 MHz mobile radio transmitter with an effective isotropic radiated power (EIRP) of 32.5 dBm. This setup allows the RF-SN to send over 1 kB of data within 10 seconds, with an additional 1.7 kilobytes every 1.6 seconds of extra charging. For the passive RF-SN buried at the same depth, continuous data transmission at a rate of 224 bps with a 3% bit error rate (BER) is achieved when the EIRP of the transmitter is 23.6 dBm.},
  archive      = {J_TMC},
  author       = {Yu Luo and Lina Pu and Jun Wang and Isaac L. Howard},
  doi          = {10.1109/TMC.2024.3491574},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1999-2013},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing in-situ structural health monitoring through RF energy-powered sensor nodes and mobile platform},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of trajectory, offloading, caching, and
migration for UAV-assisted MEC. <em>TMC</em>, <em>24</em>(3), 1981–1998.
(<a href="https://doi.org/10.1109/TMC.2024.3486995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV-assisted MEC revolutionizes edge computing by deploying UAVs for real-time data processing in areas lacking infrastructure, supporting a wide range of applications from emergency responses to smart cities. Unlike edge servers, UAVs face substantial computational constraints, necessitating a comprehensive strategy that integrates UAV trajectory with task offloading, caching, and migration. Existing studies often overlook the synergy among these strategies, impacting their overall effectiveness. Furthermore, the focus on content pre-caching overlooks task caching’s critical role in addressing high computational demands with limited UAV resources. This research aims to jointly optimize UAV trajectories and task management strategies, including offloading, caching, and migration. Utilizing the Lyapunov optimization framework, we break down the complex optimization problem into manageable subproblems: UAV placement, user-UAV association, task offloading, scheduling, and bandwidth allocation, addressed iteratively using the Block Coordinate Descent method. Specifically, the scheduling subproblem is transformed into a non-convex quadratically constrained quadratic programming problem, managed effectively through semidefinite relaxation and a probabilistic mapping approach. Our simulations show that this integrated approach significantly boosts system throughput and reduces execution times compared to conventional methods. This study enhances the understanding of the interplay between UAV trajectory planning and task management, offering vital theoretical insights for advancing UAV-assisted MEC systems.},
  archive      = {J_TMC},
  author       = {Mingxiong Zhao and Rongqian Zhang and Zhenli He and Keqin Li},
  doi          = {10.1109/TMC.2024.3486995},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1981-1998},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of trajectory, offloading, caching, and migration for UAV-assisted MEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient resource management for massive MIMO in
high-density massive IoT networks. <em>TMC</em>, <em>24</em>(3),
1963–1980. (<a href="https://doi.org/10.1109/TMC.2024.3486712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive MIMO technology offers a promising solution for supporting the simultaneous connectivity of a large number of ultra high-density massive IoT devices. However, due to limited resources, effective channel estimation becomes a challenge. One approach is to reuse the orthogonal reference signal (ORS) in a repetitive manner, while another option is to employ random non-orthogonal reference signals (NORS) for distributed massive IoT devices. In order to enhance performance in the face of high RS congestion from massive IoT devices, the strategic utilization of several critical resources becomes imperative. In this paper, we delve into the methodology of resource management to effectively address the severe high-density scenario of massive IoT devices. Specifically, we examine the system bandwidth, which proves particularly advantageous in bandwidth-limited environments. Exploiting the spatial domain, we leverage the distinctive features of massive MIMO to enable simultaneous parallel transmission by increasing the number of service antennas at the base station (BS). Furthermore, we explore the potential benefits of sectorization, a technique that involves dividing a circular cell into multiple sectors, thereby reducing RS congestion. Nevertheless, it is crucial to acknowledge that increasing these resources may entail certain trade-offs and could potentially have adverse effects on overall system performance. To gain comprehensive insights, we conduct a thorough performance analysis under various scenarios, aiming to identify key characteristics that can facilitate the optimal operation of massive MIMO in high-density IoT environments. Building upon our findings, we devise an algorithm that efficiently manages resources, ultimately leading to improved system performance.},
  archive      = {J_TMC},
  author       = {Byung Moo Lee},
  doi          = {10.1109/TMC.2024.3486712},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1963-1980},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient resource management for massive MIMO in high-density massive IoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNN partitioning, task offloading, and resource allocation
in dynamic vehicular networks: A lyapunov-guided diffusion-based
reinforcement learning approach. <em>TMC</em>, <em>24</em>(3),
1945–1962. (<a href="https://doi.org/10.1109/TMC.2024.3486728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Artificial Intelligence (AI) has introduced Deep Neural Network (DNN)-based tasks to the ecosystem of vehicular networks. These tasks are often computation-intensive, requiring substantial computation resources, which are beyond the capability of a single vehicle. To address this challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering computing services for DNN-based tasks through resource pooling via Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. In this paper, we formulate the problem of joint DNN partitioning, task offloading, and resource allocation in VEC as a dynamic long-term optimization. Our objective is to minimize the DNN-based task completion time while guaranteeing the system stability over time. To this end, we first leverage a Lyapunov optimization technique to decouple the original long-term optimization with stability constraints into a per-slot deterministic problem. Afterwards, we propose a Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm, incorporating the innovative use of diffusion models to determine the optimal DNN partitioning and task offloading decisions. Furthermore, we integrate convex optimization techniques into MAD2RL as a subroutine to allocate computation resources, enhancing the learning efficiency. Through simulations under real-world movement traces of vehicles, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions.},
  archive      = {J_TMC},
  author       = {Zhang Liu and Hongyang Du and Junzhe Lin and Zhibin Gao and Lianfen Huang and Seyyedali Hosseinalipour and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3486728},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1945-1962},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DNN partitioning, task offloading, and resource allocation in dynamic vehicular networks: A lyapunov-guided diffusion-based reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiteChain: A lightweight blockchain for verifiable and
scalable federated learning in massive edge networks. <em>TMC</em>,
<em>24</em>(3), 1928–1944. (<a
href="https://doi.org/10.1109/TMC.2024.3488746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging blockchain in Federated Learning (FL) emerges as a new paradigm for secure collaborative learning on Massive Edge Networks (MENs). As the scale of MENs increases, it becomes more difficult to implement and manage a blockchain among edge devices due to complex communication topologies, heterogeneous computation capabilities, and limited storage capacities. Moreover, the lack of a standard metric for blockchain security becomes a significant issue. To address these challenges, we propose a lightweight blockchain for verifiable and scalable FL, namely LiteChain, to provide efficient and secure services in MENs. Specifically, we develop a distributed clustering algorithm to reorganize MENs into a two-level structure to improve communication and computing efficiency under security requirements. Moreover, we introduce a Comprehensive Byzantine Fault Tolerance (CBFT) consensus mechanism and a secure update mechanism to ensure the security of model transactions through LiteChain. Our experiments based on Hyperledger Fabric demonstrate that LiteChain presents the lowest end-to-end latency and on-chain storage overheads across various network scales, outperforming the other two benchmarks. In addition, LiteChain exhibits a high level of robustness against replay and data poisoning attacks.},
  archive      = {J_TMC},
  author       = {Handi Chen and Rui Zhou and Yun-Hin Chan and Zhihan Jiang and Xianhao Chen and Edith C. H. Ngai},
  doi          = {10.1109/TMC.2024.3488746},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1928-1944},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LiteChain: A lightweight blockchain for verifiable and scalable federated learning in massive edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing power and battery charging management for solar
energy powered edge computing. <em>TMC</em>, <em>24</em>(3), 1913–1927.
(<a href="https://doi.org/10.1109/TMC.2024.3489028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of energy harvesting capabilities into mobile edge computing (MEC) edge servers enables their deployment beyond the reach of electrical grids, expanding MEC services to isolated regions and geographically challenging terrains. However, the fluctuating nature of renewable energy sources, such as solar and wind, necessitates dynamic management of server computing power in response to variable energy harvesting rates. Unlike conventional models that assume predetermined amounts of harvested energy per time period, this study illustrates the complex interdependencies between server power consumption and variable energy harvesting rates due to battery charging characteristics. To address this, we introduce a novel energy harvesting model that comprehensively accounts for the interaction between computing power management and energy harvesting rates. We develop both offline and online offline optimal computing power management strategies aimed at maximizing the average computational capacity of edge servers. An analytical solution to the resulting nonlinear optimization problem is provided to determine the optimal computing power configurations. Simulation results indicate that the proposed strategy effectively balances energy harvesting rates and energy utilization, thereby enhancing computational performance in dynamic energy environments.},
  archive      = {J_TMC},
  author       = {Yu Luo and Lina Pu and Chun-Hung Liu},
  doi          = {10.1109/TMC.2024.3489028},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1913-1927},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computing power and battery charging management for solar energy powered edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigCan: Toward reliable ToF estimation leveraging multipath
signal cancellation on commodity WiFi devices. <em>TMC</em>,
<em>24</em>(3), 1895–1912. (<a
href="https://doi.org/10.1109/TMC.2024.3491337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread deployment of WiFi infrastructure has facilitated the development of Time-of-Flight (ToF) based sensing applications. ToF estimation, however, is a challenging task due to the complexity of multipath effect. In this paper, we propose a phase difference based method for ToF estimation and uncover the potential of signal cancellation to mitigate the impact of multipath and noise on phase differences among subcarriers. To separate the moving target path from the complex multipath for ToF estimation, we suggest employing specific elimination methods tailored to the characteristics of different signal components. For dynamic multipath, we observe that when a given subcarrier propagates along two paths to the receiver, with path lengths differing by half a wavelength, the phase difference introduced by these two paths cancels each other out. Therefore, we propose two metrics to identify signals that satisfy this condition, utilizing both frequency diversity and spatial diversity. Additionally, we propose leveraging time diversity to eliminate the static multipath component and reduce the impact of noise. We implemented the methods with off-the-shelf WiFi devices and achieved mean errors of 15.36 cm and 21.05 cm for distance estimation in outdoor and indoor scenarios, outperforming state-of-the-art ToF estimation method by 50% error reduction.},
  archive      = {J_TMC},
  author       = {Yang Li and Dan Wu and Jiahe Chen and Weiyan Shi and Leye Wang and Lu Su and Wenwei Li and Daqing Zhang},
  doi          = {10.1109/TMC.2024.3491337},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1895-1912},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SigCan: Toward reliable ToF estimation leveraging multipath signal cancellation on commodity WiFi devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGExplainer: Heterogeneous graph explainer for IoT device
identification. <em>TMC</em>, <em>24</em>(3), 1877–1894. (<a
href="https://doi.org/10.1109/TMC.2024.3486717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT device identification is vital for network asset and security management. However, existing methods use statistical features that can not identify IoT devices accurately in complex network environments. GraphIoT proposes using non-statistical features and building a heterogeneous graph neural network to identify IoT devices accurately. However, heterogeneous graph neural networks lack interpretability, which reduces trust in the model. Besides, it is difficult to deploy on resource-constrained devices, limiting the broad application of IoT device identification. To make IoT device identification interpretable, easy to deploy, and with high accuracy, we get the interpretation results of GraphIoT through interpretability and further build the rule set based on the interpretation results. Considering there is no suitable interpreter for GraphIoT with many nodes and edges, we propose HGExplainer, which reduces the time complexity by splitting the interpretation target into important relation solving and edge solving and uses a novel solution method, ExpandTree. Then, we also designed a rule extractor, which can build rule sets based on the interpretation results. Experimental results on Yourthings and UNSW datasets show that HGExplainer can build high fidelity, concise sample-level explanations in less than 3 seconds, and the established rule set can precisely identify IoT devices.},
  archive      = {J_TMC},
  author       = {Linna Fan and Bo Wu and Xuan Shen and Jun He and Guanglei Song and Gang Yang and Chaocan Xiang and Duohe Ma and Yongfeng Huang},
  doi          = {10.1109/TMC.2024.3486717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1877-1894},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HGExplainer: Heterogeneous graph explainer for IoT device identification},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending data poisoning attacks in DP-based crowdsensing: A
game-theoretic approach. <em>TMC</em>, <em>24</em>(3), 1859–1876. (<a
href="https://doi.org/10.1109/TMC.2024.3486689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy (DP) is widely used for protecting privacy in crowdsensing by adding noises. However, malicious attackers can exploit noise to launch covert data poisoning attacks. In this paper, we propose a game-based defense approach to resist such data poisoning attacks in DP-based crowdsensing systems. In this approach, attackers are believed to be powerful as they can refine their attack strategy based on the observations of deployed defenders’ defense strategy. Specifically, the defenders formulate the defense as a functional minimization problem (which cannot be directly solved by numerical optimization algorithms because its decision variable is a set of functions), resisting data poisoning attacks by deleting data shared by identified malicious workers through the log-likelihood ratio test. To obtain a current defense strategy, the decision variable of the problem is relaxed into the coefficients of basis-based linear combinations through the variable-basis approximation, and then solved using the simulated annealing genetic algorithm. Correspondingly, the attackers formulate their attack strategy as a bi-level maximization problem (which is an NP-hard problem), biasing crowdsensing results as much as possible while remaining undetected. Since the attackers can know the defense strategy, they may bypass the defenders by constraining the expected log-likelihood ratio test. Additionally, the attackers can evade truth discovery methods deployed in crowdsensing using DP noise. To determine a current attack strategy, the bi-level problem is decomposed into upper-level and lower-level sub-problems, wherein the upper-level sub-problem is solved by the variational methods, and then these sub-problems are alternately optimized. Finally, we propose a local minimax points calculating algorithm to obtain an equilibrium point in the defenders-attackers game, thereby finding an optimal defense strategy to resist the powerful data poisoning attack. Extensive experiments on real-world and synthetic datasets show that the proposed game-based defense approach can effectively defend powerful and covert attackers.},
  archive      = {J_TMC},
  author       = {Zhirun Zheng and Zhetao Li and Cheng Huang and Saiqin Long and Xuemin Shen},
  doi          = {10.1109/TMC.2024.3486689},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1859-1876},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Defending data poisoning attacks in DP-based crowdsensing: A game-theoretic approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking cost-efficient VM scheduling on public edge
platforms: A service provider’s perspective. <em>TMC</em>,
<em>24</em>(3), 1846–1858. (<a
href="https://doi.org/10.1109/TMC.2024.3488082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior studies on traditional centralized clouds independently optimizing static resource utilization and dynamic bandwidth cost are not applicable to edge scenarios, where edge sites are interconnected by wide area networks (WAN) rather than local area networks (LAN) as within clouds. Due to the lack of knowledge about the actual status of public edge platforms and real-world edge datasets, existing influential literature on edge scenarios demonstrates significant disparities in optimization objectives and perspectives. To bridge this gap, we collaborate with a public edge platform and perform a comprehensive measurement, which reveals limitations of the status quo VM scheduling schemes and potential opportunities for improvement. However, resolving VM scheduling considering static resource utilization, dynamic bandwidth cost, and end users’ QoE in a cost-efficient manner faces several challenges, including coupled objectives, exponentially increased complexity, and spatiotemporal dynamics. To address the above challenges, in this work, we propose a holistic online framework that integrates combinatorial bandit-based VM migration and seasonality-aware VM request allocation at two distinct time granularities. Large-scale experiments based on a real-world dataset confirm that our online framework achieves near-offline bandwidth cost and resource utilization while significantly lowering time consumption.},
  archive      = {J_TMC},
  author       = {Yanan Li and Xiao Ma and Zhe Fu and Ao Zhou and Mengwei Xu and Shangguang Wang},
  doi          = {10.1109/TMC.2024.3488082},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1846-1858},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Rethinking cost-efficient VM scheduling on public edge platforms: A service provider’s perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mighty: Towards long-range and high-throughput backscatter
for drones. <em>TMC</em>, <em>24</em>(3), 1833–1845. (<a
href="https://doi.org/10.1109/TMC.2024.3486993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While small drone video streaming systems create unprecedented video content, they also place a power burden exceeding 20% on the drone&#39;s battery, limiting flight endurance. We present ${\sf Mighty}$, a hardware-software solution to minimize the power consumption of a drone&#39;s video streaming system by offloading power overheads associated with both video compression and transmission to a ground controller. ${\sf Mighty}$ innovates a high performance co-design among: (1) a ring oscillator-based, ultra-low power backscatter radio; (2) a spectrally-efficient, non-linear, low-power physical layer modulation and multi-chain radio architecture; and (3) a lightweight video compression codec-bypassing software design. Our co-design exploits synergies among these components, resulting in joint throughput and range performance that pushes the known envelope. We prototype ${\sf Mighty}$ on PCB board and conduct extensive field studies both indoors and outdoors. The power efficiency of ${\sf Mighty}$ is about 16.6 nJ/bit. A head-to-head comparison with a DJI Mini2 drone&#39;s default video streaming system shows that ${\sf Mighty}$ achieves similar throughput at a drone-to-controller distance of up to 150 meters, with 34–55× improvement of power efficiency than WiFi-based video streaming solutions.},
  archive      = {J_TMC},
  author       = {Xiuzhen Guo and Yuan He and Longfei Shangguan and Yande Chen and Chaojie Gu and Yuanchao Shu and Kyle Jamieson and Jiming Chen},
  doi          = {10.1109/TMC.2024.3486993},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1833-1845},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mighty: Towards long-range and high-throughput backscatter for drones},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving contact query processing over trajectory
data in mobile cloud computing. <em>TMC</em>, <em>24</em>(3), 1818–1832.
(<a href="https://doi.org/10.1109/TMC.2024.3488728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of mobile devices and cloud computing, massive spatial trajectory data is generated and outsourced to the cloud for storage and analysis, enabling location-based mobile computing services. However, due to the sensitivity of the trajectory data, sharing it in plaintext could lead to privacy risks, especially in operations like contact queries. Thus, achieving secure and efficient contact queries based on the trajectory data in the cloud is a significant challenge. In this paper, we propose a privacy-preserving contact query processing over trajectory data in mobile cloud computing. The projection-based secure trajectory encoding is designed to convert trajectories into secure codes such that the comparison between the distance of two moving objects and the contact distance threshold is transformed into a problem of secure code matching. Adopting the secure code matching method, a baseline privacy-preserving contact query processing is proposed. To improve the query accuracy and efficiency, an amplification factor, an HTG-index and a filter table are designed for query processing optimization, based on which an enhanced privacy-preserving contact query processing is proposed. The game stimulation-based security analysis and experimental results show that the proposed query scheme is secure and performs well in query accuracy and efficiency.},
  archive      = {J_TMC},
  author       = {Qu Lu and Hua Dai and Pengyue Li and Shuyan Wan and Geng Yang and Yang Xiang and Fu Xiao},
  doi          = {10.1109/TMC.2024.3488728},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1818-1832},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving contact query processing over trajectory data in mobile cloud computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving single-sign-on with fine-grained access
control for IoT devices. <em>TMC</em>, <em>24</em>(3), 1805–1817. (<a
href="https://doi.org/10.1109/TMC.2024.3486719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT-based sharing economy is a win-win business model, where a transferor owns idle IoT devices and transfers the right to use a device to a user for a fee. Considering usage of multiple devices and privacy preservation, anonymous single-sign-on (ASSO) is a feasible solution for authentication. ASSO allows a user to access multiple devices with one token issued by the transferor and prevents the transferor from identifying the user. We also observe that in the scenario of IoT-based sharing economy, the token should (i) support attributes since a device should be available only to users with specific attributes (e.g., age) and (ii) avoid incurring significant communication/computation overhead as IoT devices are resource-constrained. In this paper, we proposed PILOT, a privacy-preserving single-sign-on with fine-grained access control for IoT devices. When a user attempts to access a device, he/she requests a token from the transferor. The token is actually a blind signature that cannot be tracked, and contains the user’s attributes which facilitate fine-grained access control on the device. Besides, the token consists of only four group elements and verification of the token involves only several exponentiation operations. This renders PILOT superior in terms of communication/computation overhead and suitable for IoT devices.},
  archive      = {J_TMC},
  author       = {Zhao Zhang and Chunxiang Xu and Man Ho Allen Au and Changsong Jiang},
  doi          = {10.1109/TMC.2024.3486719},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1805-1817},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving single-sign-on with fine-grained access control for IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A long-term-planning learning strategy to coordinate
viewport prediction and video transmission in 360° video streaming.
<em>TMC</em>, <em>24</em>(3), 1792–1804. (<a
href="https://doi.org/10.1109/TMC.2024.3487998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled by Metaverse, 360° video streaming has seen tremendous growth in the past years. However, our measurement reveals that current 360° streaming systems suffer from a dilemma that severely limits QoE. On the one hand, viewport prediction requires the shortest possible prediction distance for high predicting accuracy; On the other hand, video transmission requires more buffered data to compensate for bandwidth fluctuations otherwise substantial playback rebuffering would be incurred. There is so far no existing method that can break this dilemma so the QoE optimization for 360° video streaming was naturally bottlenecked. This work is the first attempt to tackle this challenge by developing QUTA – a novel learning-based streaming system. Specifically, according to our measurement, three kinds of internal streaming parameters have significant impacts on the prediction distance, namely, download pause, data rate threshold, and playback rate. On top of this, we design a new long-term-planning (LTP) continuous control deep reinforcement learning method that tunes the parameters dynamically based on the network condition and the streaming context. Extensive evaluations based on real system prototypes show that QUTA not only improves the prediction accuracy and QoE performance by up to 68.4% but also exhibits strong temporal and spatial robustness.},
  archive      = {J_TMC},
  author       = {Guanghui Zhang and Jing Guo and Mengbai Xiao and Dongxiao Yu and Vaneet Aggarwal and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2024.3487998},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1792-1804},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A long-term-planning learning strategy to coordinate viewport prediction and video transmission in 360° video streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeSFL: A fine-grained parallelization framework for split
federated learning on heterogeneous clients. <em>TMC</em>,
<em>24</em>(3), 1774–1791. (<a
href="https://doi.org/10.1109/TMC.2024.3489642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split Federated Learning (SFL) improves scalability of Split Learning (SL) by enabling parallel computing of the learning tasks on multiple clients. However, state-of-the-art SFL schemes neglect the effects of heterogeneity in the clients’ computation and communication performance as well as the computation time for the tasks offloaded to the cloud server. In this paper, we propose a fine-grained parallelization framework, called PipeSFL, to accelerate SFL on heterogeneous clients. PipeSFL is based on two key novel ideas. First, we design a server-side priority scheduling mechanism to minimize per-iteration time. Second, we propose a hybrid training mode to reduce per-round time, which employs asynchronous training within rounds and synchronous training between rounds. We theoretically prove the optimality of the proposed priority scheduling mechanism within one round and analyze the total time per round for PipeSFL, SFL and SL. We implement PipeSFL on PyTorch. Extensive experiments on seven 64-client clusters with different heterogeneity demonstrate that at training speed, PipeSFL achieves up to 1.65x and 1.93x speedup compared to EPSL and SFL, respectively. At energy consumption, PipeSFL saves up to 30.8% and 43.4% of the energy consumed within each training round compared to EPSL and SFL, respectively.},
  archive      = {J_TMC},
  author       = {Yunqi Gao and Bing Hu and Mahdi Boloursaz Mashhadi and Wei Wang and Mehdi Bennis},
  doi          = {10.1109/TMC.2024.3489642},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1774-1791},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PipeSFL: A fine-grained parallelization framework for split federated learning on heterogeneous clients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepSelector: A deep learning-based virtual network function
placement approach in SDN/NFV-enabled networks. <em>TMC</em>,
<em>24</em>(3), 1759–1773. (<a
href="https://doi.org/10.1109/TMC.2024.3483779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Software-Defined Networks (SDN) and Network Function Virtualization (NFV) has popularized the adoption of the Service Function Chain (SFC) paradigm for efficient network service delivery. This paradigm leverages the flexibility and cost-effectiveness of deploying Virtual Network Functions (VNFs) as software entities or virtual machines on off-the-shelf servers. Chaining VNFs together allows traffic to be directed through the network as required. However, existing algorithms for traffic steering and routing path computation in SFC suffer from many challenges, including complexity, lack of scalability, and low time efficiency. This paper focuses on addressing the challenges associated with VNF placement and SFC chaining in SDN/NFV-enabled networks. Our objective is to identify an optimal solution for VNF placement that maximizes the utilization of network resources. We formulate the problem as a Binary Integer Programming (BIP) model to accomplish this. Additionally, we propose a novel algorithm called DeepSelector, which incorporates deep learning techniques and an intelligent node selection network to determine the optimal placement of VNFs for SFC requests. Through performance evaluation, we demonstrate that DeepSelector achieves high network resource utilization and offers efficient VNF placement computation, significantly improving overall network performance.},
  archive      = {J_TMC},
  author       = {Yi Yue and Xiongyan Tang and Ying-Chang Liang and Chang Cao and Lexi Xu and Wencong Yang and Zhiyan Zhang},
  doi          = {10.1109/TMC.2024.3483779},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1759-1773},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeepSelector: A deep learning-based virtual network function placement approach in SDN/NFV-enabled networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepRP: Bottleneck theory guided relay placement for 6G mesh
backhaul augmentation. <em>TMC</em>, <em>24</em>(3), 1744–1758. (<a
href="https://doi.org/10.1109/TMC.2024.3487020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backhaul mesh networks are critical for ensuring coverage and connectivity of high-frequency 6G networks. To maintain high throughput, its architecture needs to be augmented by adding relays. However, how to place relays at appropriate sites poses two challenges: 1) there lacks a theory to capture the relationship between a certain change of network architecture and its throughput gain; 2) selecting the best sites for relays is a complicated combinatorial problem. To tackle the first challenge, this paper first establishes a clique-based bottleneck theory, through which a clique-based bottleneck structure of a given network architecture is constructed to determine the network throughput. Based on this bottleneck structure, clique gradients are then computed to quantify the impact of each clique on the overall network throughput. With the clique-based bottleneck theory, the second challenge is resolved by embedding clique gradients into a deep reinforcement learning (DRL) scheme. Specifically, the DRL actions are masked such that only the relay sites that match the highest clique gradients are selected. This DRL-based relay placement (DeepRP) scheme is evaluated via extensive simulations, and performance results show that it can boost network throughput by more than 50%, which is $\text{10.4} \!-\! \text{32.1}\% $ higher than those of baseline schemes.},
  archive      = {J_TMC},
  author       = {Tianxin Wang and Xudong Wang},
  doi          = {10.1109/TMC.2024.3487020},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1744-1758},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeepRP: Bottleneck theory guided relay placement for 6G mesh backhaul augmentation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FCER: A federated cloud-edge recommendation framework with
cluster-based edge selection. <em>TMC</em>, <em>24</em>(3), 1731–1743.
(<a href="https://doi.org/10.1109/TMC.2024.3484493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional recommendation system provides web services by modeling user behavior characteristics, which also faces the risk of leaking user privacy. To mitigate the rising concern on privacy leakage in recommender systems, federated learning (FL) based recommendation has received tremendous attention, which can preserve data privacy by conducting local model training on clients. However, devices (e.g., mobile phones) used by clients in a recommender system may have limited capacity for computation and communication, which can severely deteriorate FL training efficiency. Besides, offloading local training tasks to the cloud can lead to privacy leakage and excessive pressure to the cloud. To overcome this deficiency, we propose a novel federated cloud-edge recommendation framework, which is called FCER, by offloading local training tasks to powerful and trusted edge servers. The challenge of FCER lies in the heterogeneity of edge servers, which makes the parameter server (PS) deployed in the cloud face difficulty in judiciously selecting edge servers for model training. To address this challenge, we divide the FCER framework into two stages. In the first pre-training stage, edge servers expose their data statistical features protected by local differential privacy (LDP) to the PS so that edge servers can be grouped into clusters. In the second training stage, FCER activates a single cluster in each communication round, ensuring that edge servers with statistical homogenization are not repeatedly involved in FL. The PS only selects a certain number of edge servers with the highest data quality in each cluster for FL. Effective metrics are proposed to dynamically evaluate the data quality of each edge server. Convergence rate analysis is conducted to show the convergence of recommendation algorithms in FCER. We also perform extensive experiments to demonstrate that FCER remarkably outperforms competitive baselines by $3.85\%-9.14\%$ on HR@10 and $1.46\%-11.77\%$ on NDCG@10.},
  archive      = {J_TMC},
  author       = {Jiang Wu and Yunchao Yang and Miao Hu and Yipeng Zhou and Di Wu},
  doi          = {10.1109/TMC.2024.3484493},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1731-1743},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FCER: A federated cloud-edge recommendation framework with cluster-based edge selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model poisoning attack against neural network interpreters
in IoT devices. <em>TMC</em>, <em>24</em>(3), 1715–1730. (<a
href="https://doi.org/10.1109/TMC.2024.3486218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network models have become integral to Internet of Things (IoT) systems, with applications spanning from industrial automation to critical infrastructure management. Despite their prevalence, the deployment of these models within IoT systems introduces distinctive security vulnerabilities. In particular, adversaries may execute model poisoning attacks, which aim to alter the decision-making processes of embedded models, leading to erroneous outcomes. Existing model poisoning attacks necessitate access to extensive auxiliary datasets, such as the training dataset itself or one with same distribution. These requirements often render such attacks impractical in IoT contexts, given the constrained storage and computational resources of IoT devices. This paper proposes the first model poisoning attack against interpreters without auxiliary datasets to manipulate the model’s behavior. We evaluate the attack on three real-world datasets, and results indicate that this attack can successfully coerce the targeted interpreters to produce outcomes aligned with an adversary’s intentions, while maintaining nearly indistinguishable performance from the original model, thereby ensuring its stealthiness. Furthermore, beyond directly affected interpreters, our experiments reveal that four additional interpreters coupled to the poisoned model are indirectly influenced, underscoring the attack’s transferability.},
  archive      = {J_TMC},
  author       = {Xianglong Zhang and Feng Li and Huanle Zhang and Haoxin Zhang and Zhijian Huang and Lisheng Fan and Xiuzhen Cheng and Pengfei Hu},
  doi          = {10.1109/TMC.2024.3486218},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1715-1730},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Model poisoning attack against neural network interpreters in IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRadar: An efficient LoRa channel occupancy acquirer based
on cross-channel scanning. <em>TMC</em>, <em>24</em>(3), 1699–1714. (<a
href="https://doi.org/10.1109/TMC.2024.3487835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa is widely deployed for various applications. Though the knowledge of the channel occupancy is the prerequisite of many aspects of network management, acquiring the channel occupancy for LoRa is challenging due to the large number of possible channels. In this paper, we propose ${\sf LoRadar}$, a novel LoRa channel occupancy acquirer based on cross-channel scanning. Our in-depth study finds that Channel Activity Detection (CAD) in a narrow band can indicate the channel activities of wide bands because they have the same slope in the time-frequency domain. Based on this finding, we design a cross-channel scanning mechanism that infers the channel occupancy states of all the overlapping channels by the distribution of CAD results. We elaborately select and adjust the CAD settings to enhance the distribution features and design a pattern correction method to cope with distribution distortions. We also design a CAD scheduler to deal with the low duty-cycle LoRa operations. We implement ${\sf LoRadar}$ on commercial LoRa platforms and evaluate its performance in the indoor testbed and two outdoor deployed networks. The experimental results show that ${\sf LoRadar}$ can achieve a detection accuracy of 0.99 and reduce the acquisition overhead by up to 90%, compared to the traversal-based methods.},
  archive      = {J_TMC},
  author       = {Xiaolong Zheng and Fu Yu and Liang Liu and Huadong Ma},
  doi          = {10.1109/TMC.2024.3487835},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1699-1714},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LoRadar: An efficient LoRa channel occupancy acquirer based on cross-channel scanning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time continuous activity recognition with a commercial
mmWave radar. <em>TMC</em>, <em>24</em>(3), 1684–1698. (<a
href="https://doi.org/10.1109/TMC.2024.3483813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {mmWave-based activity recognition technology has attracted widespread attention as it provides the ability of device-free, ubiquitous and accurate sensing. Recognition of human activities intrinsically demands to be real-time and continuous, but the state of the arts is still far limited with the capacity in this regard. The main obstacle lies in activity sequence segmentation, i.e., locating the boundaries between consecutive activities in an activity sequence. This is a daunting task, due to the unclear activity boundaries and the variable activity duration. In this paper, we propose ZuMa, the first mmWave-based approach to real-time continuous activity recognition. When resorting to a machine learning model for activity recognition, our insight is that the recognition confidence of the recognition model is highly correlated to the accuracy of activity sequence segmentation, so that the former can be utilized as a feedback metric to finely adjust the segmentation boundaries. Based on this insight, ZuMa is a coarse-to-fine grained approach, which includes the fast coarse-grained activity chunk extraction and the find-grained explicit segmentation adjustment and recognition. We have implemented ZuMa with the commercial mmWave radar and evaluated its performance under various settings. The results demonstrate that ZuMa achieves an average recognition error of 12.67%, which is 65.08% and 71.87% lower than that of the two baseline methods. The average recognition delay of ZuMa is only 1.86 s.},
  archive      = {J_TMC},
  author       = {Yunhao Liu and Jia Zhang and Yande Chen and Weiguo Wang and Songzhou Yang and Xin Na and Yimiao Sun and Yuan He},
  doi          = {10.1109/TMC.2024.3483813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1684-1698},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time continuous activity recognition with a commercial mmWave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin backed closed-loops for energy-aware and open
RAN-based fixed wireless access serving rural areas. <em>TMC</em>,
<em>24</em>(3), 1669–1683. (<a
href="https://doi.org/10.1109/TMC.2024.3482985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet access in rural areas should be improved to support digital inclusion and 5G services. Due to the high deployment costs of fiber optics in these areas, Fixed Wireless Access (FWA) has become a preferable alternative. Additionally, the Open Radio Access Network (O-RAN) can facilitate the interoperability of FWA elements, allowing some FWA functions to be deployed at the edge cloud. However, deploying edge clouds in rural areas can increase network and energy costs. To address these challenges, we propose a closed-loop system assisted by a Digital Twin (DT) to automate energy-aware O-RAN based FWA resource management in rural areas. We consider the FWA and edge cloud as the Physical Twin (PT) and design a closed-loop that distributes radio resources to edge cloud instances for scheduling. We develop another closed-loop for intra-slice resource allocation to houses. We design an energy model that integrates radio resource allocation and formulate ultra-small and small-timescale optimizations for the PT to maximize slice requirement satisfaction while minimizing energy costs. We then design a reinforcement learning approach and successive convex approximation to address the formulated problems. We present a DT that replicates the PT by incorporating solution experiences into future states. The results show that our approach efficiently uses radio and energy resources.},
  archive      = {J_TMC},
  author       = {Anselme Ndikumana and Kim Khoa Nguyen and Mohamed Cheriet},
  doi          = {10.1109/TMC.2024.3482985},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1669-1683},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Digital twin backed closed-loops for energy-aware and open RAN-based fixed wireless access serving rural areas},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MANSY: Generalizing neural adaptive immersive video
streaming with ensemble and representation learning. <em>TMC</em>,
<em>24</em>(3), 1654–1668. (<a
href="https://doi.org/10.1109/TMC.2024.3487175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of immersive videos has prompted extensive research into neural adaptive tile-based streaming to optimize video transmission over networks with limited bandwidth. However, the diversity of users’ viewing patterns and Quality of Experience (QoE) preferences has not been fully addressed yet by existing neural adaptive approaches for viewport prediction and bitrate selection. Their performance can significantly deteriorate when users’ actual viewing patterns and QoE preferences differ considerably from those observed during the training phase, resulting in poor generalization. In this paper, we propose MANSY, a novel streaming system that embraces user diversity to improve generalization. Specifically, to accommodate users’ diverse viewing patterns, we design a Transformer-based viewport prediction model with an efficient multi-viewport trajectory input output architecture based on implicit ensemble learning. Besides, we for the first time combine the advanced representation learning and deep reinforcement learning to train the bitrate selection model to maximize diverse QoE objectives, enabling the model to generalize across users with diverse preferences. Extensive experiments demonstrate that MANSY outperforms state-of-the-art approaches in viewport prediction accuracy and QoE improvement on both trained and unseen viewing patterns and QoE preferences, achieving better generalization.},
  archive      = {J_TMC},
  author       = {Duo Wu and Panlong Wu and Miao Zhang and Fangxin Wang},
  doi          = {10.1109/TMC.2024.3487175},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1654-1668},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MANSY: Generalizing neural adaptive immersive video streaming with ensemble and representation learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SANSee: A physical-layer semantic-aware networking framework
for distributed wireless sensing. <em>TMC</em>, <em>24</em>(3),
1636–1653. (<a href="https://doi.org/10.1109/TMC.2024.3483272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contactless device-free wireless sensing has recently attracted significant interest due to its potential to support a wide range of immersive human-machine interactive applications using ubiquitously available radio frequency (RF) signals. Traditional approaches focus on developing a single global model based on a combined dataset collected from different locations. However, wireless signals are known to be location and environment specific. Thus, a global model results in inconsistent and unreliable sensing results. It is also unrealistic to construct individual models for all the possible locations and environmental scenarios. Motivated by the observation that signals recorded at different locations are closely related to a set of physical-layer semantic features, in this paper we propose SANSee, a semantic-aware networking-based framework for distributed wireless sensing. SANSee allows models constructed in one or a limited number of locations to be transferred to new locations without requiring any locally labeled data or model training. SANSee is built on the concept of physical-layer semantic-aware network (pSAN), which characterizes the semantic similarity and the correlations of sensed data across different locations. A pSAN-based zero-shot transfer learning solution is introduced to allow receivers in new locations to obtain location-specific models by directly aggregating the models trained by other receivers. We theoretically prove that models obtained by SANSee can approach the locally optimal models. Experimental results based on real-world datasets are used to verify that the accuracy of the transferred models obtained by SANSee matches that of the models trained by the locally labeled data based on supervised learning approaches.},
  archive      = {J_TMC},
  author       = {Huixiang Zhu and Yong Xiao and Yingyu Li and Guangming Shi and Marwan Krunz},
  doi          = {10.1109/TMC.2024.3483272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1636-1653},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SANSee: A physical-layer semantic-aware networking framework for distributed wireless sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and secure geometric range search over encrypted
spatial data in mobile cloud. <em>TMC</em>, <em>24</em>(3), 1621–1635.
(<a href="https://doi.org/10.1109/TMC.2024.3482321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile computing and the popularity of mobile devices equipped with GPS technology, massive spatial data have become available. Enterprises upload encrypted spatial data to the mobile cloud to save local storage and computation costs. However, the existing secure Geometric Range Search (GRS) solutions are inefficient in terms of building, updating index structure and querying processes. Moreover, the index structures of existing GRS schemes based on Order Preserving Encryption (OPE) leak location order, which may lead to reconstruction attacks. To solve these issues, we first propose an efficient and secure GRS scheme using Radix-Tree, namely GRSRT-I. Specifically, we construct an index structure based on Radix-tree to achieve efficient search and update, then use homomorphic encryption NTRU to resist chosen-plaintext attack, finally design a dual-server architecture to alleviate the burdens on mobile users caused by multiple rounds of interactions. Furthermore, we propose an enhanced scheme, GRSRT-II, by combining Order-Revealing Encryption and OPE, which greatly improves the search efficiency while slightly reducing the security. We formally prove the security of our proposed schemes, and conduct extensive experiments to demonstrate that GRSRT-I can improve the query efficiency by up to at least 1.5 times when compared with previous solutions and GRSRT-II can achieve a higher level of search efficiency.},
  archive      = {J_TMC},
  author       = {Yinbin Miao and Guijuan Wang and Xinghua Li and Hongwei Li and Kim-Kwang Raymond Choo and Rebert H. Deng},
  doi          = {10.1109/TMC.2024.3482321},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1621-1635},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and secure geometric range search over encrypted spatial data in mobile cloud},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint association, beamforming, and resource allocation for
multi-IRS enabled MU-MISO systems with RSMA. <em>TMC</em>,
<em>24</em>(3), 1602–1620. (<a
href="https://doi.org/10.1109/TMC.2024.3483193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems. This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes. The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively. Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems. The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms.},
  archive      = {J_TMC},
  author       = {Chunjie Wang and Xuhui Zhang and Huijun Xing and Liang Xue and Shuqiang Wang and Yanyan Shen and Bo Yang and Xinping Guan},
  doi          = {10.1109/TMC.2024.3483193},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1602-1620},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint association, beamforming, and resource allocation for multi-IRS enabled MU-MISO systems with RSMA},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time video forgery detection via vision-WiFi silhouette
correspondence. <em>TMC</em>, <em>24</em>(3), 1585–1601. (<a
href="https://doi.org/10.1109/TMC.2024.3483550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For safety guard and crime prevention, video surveillance systems have been pervasively deployed in many security-critical scenarios, such as the residence, retail stores, and banks. However, these systems could be infiltrated by the adversary and the video streams would be modified or replaced, i.e., under the video forgery attack. The prevalence of Internet of Things (IoT) devices and the emergence of Deepfake-like techniques severely emphasize the vulnerability of video surveillance systems under such attacks. To secure existing surveillance systems, in this paper we propose a vision-WiFi cross-modal video forgery detection system, namely WiSil. Leveraging a theoretical model based on the principle of signal propagation, WiSil constructs wave front information of the object in the monitoring area from WiFi signals. With a well-designed deep learning network, WiSil further recovers silhouettes from the wave front information. Based on a Siamese network-based semantic feature extractor, WiSil can eventually determine whether a frame is manipulated by comparing the semantic feature vectors extracted from the video’s silhouette with those extracted from the WiFi’s silhouette. We enhance the basic version of WiSil Fang et al. 2023 by developing a model compression method and a forgery trace localization method. Extensive experiments show that WiSil achieves 95%$+$ accuracy in detecting tampered frames.},
  archive      = {J_TMC},
  author       = {Jianwei Liu and Xinyue Fang and Yike Chen and Jiantao Yuan and Guanding Yu and Jinsong Han},
  doi          = {10.1109/TMC.2024.3483550},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1585-1601},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time video forgery detection via vision-WiFi silhouette correspondence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SITOff: Enabling size-insensitive task offloading in
D2D-assisted mobile edge computing. <em>TMC</em>, <em>24</em>(3),
1567–1584. (<a href="https://doi.org/10.1109/TMC.2024.3483951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC), along with device-to-device (D2D) assisted MEC (D-MEC), are promising technologies that could improve the quality-of-experience for mobile devices (MDs) by offloading their tasks to edge servers or nearby idle MDs. There is a popular trend to develop distributed task offloading algorithms using multi-agent reinforcement learning (MARL), whose adoption of central critics during training makes the offloading still size-sensitive. Therefore, this paper proposes a Size-Insensitive Task Offloading (SITOff) algorithm for D-MEC based on fully-distributed offloading without maintaining any central venue. Specifically, taking advantage of the inherent graph-like structure of D-MEC, SITOff adopts graphs to represent MDs’ states and relationships and form each MD&#39;s local knowledge about D-MEC through graph computation. Furthermore, considering the limitation of local knowledge in performing whole performance-oriented offloading, each MD utilizes D2D-transmitting to exchange knowledge with its neighbors and form a comprehensive knowledge about D-MEC to enhance the coordination of distributed offloading. Additionally, regarding the different impacts of neighbors’ knowledge, each MD leverages attention mechanisms to selectively learn its neighbors’ knowledge during knowledge-exchange. Extensive experimental results show the superiority of SITOff over state-of-the-art MARL-based offloading algorithms in D-MEC with various MDs, and the easy collaboration of SITOff with curriculum-learning for large-scale D-MEC offloading.},
  archive      = {J_TMC},
  author       = {Zheyuan Hu and Jianwei Niu and Tao Ren and Xuefeng Liu and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3483951},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1567-1584},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SITOff: Enabling size-insensitive task offloading in D2D-assisted mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical layer cross-technology communication via
explainable neural networks. <em>TMC</em>, <em>24</em>(3), 1550–1566.
(<a href="https://doi.org/10.1109/TMC.2024.3480109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-technology communication (CTC) facilitates seamless interaction between different wireless technologies. Most existing methods use reverse engineering to derive the required transmission payload, generating a waveform that the target device can successfully demodulate. However, traditional approaches have certain limitations, including reliance on specific reverse engineering algorithms or the need for manual parameter tuning to reduce emulation distortion. In this work, we present NNCTC, a framework for achieving physical layer cross-technology communication through explainable neural networks, incorporating relevant knowledge from the wireless communication physical layer into the neural network models. We first convert the various signal processing components within the CTC process into neural network models, then build a training framework for the CTC encoder-decoder structure to achieve CTC. NNCTC significantly reduces the complexity of CTC by automatically deriving CTC payloads through training. We demonstrate how NNCTC implements CTC in WiFi systems using OFDM and CCK modulation. On WiFi systems using OFDM modulation, NNCTC outperforms the WEBee and WIDE designs in terms of error performance, achieving an average packet reception ratio (PRR) of 92.3% and an average symbol error rate (SER) as low as 1.3%. In WiFi systems using OFDM modulation, the highest PRR can reach up to 99%.},
  archive      = {J_TMC},
  author       = {Haoyu Wang and Jiazhao Wang and Wenchao Jiang and Shuai Wang and Demin Gao},
  doi          = {10.1109/TMC.2024.3480109},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1550-1566},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Physical layer cross-technology communication via explainable neural networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility and cost aware inference accelerating algorithm for
edge intelligence. <em>TMC</em>, <em>24</em>(3), 1530–1549. (<a
href="https://doi.org/10.1109/TMC.2024.3484158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge intelligence (EI) has been widely applied recently. Splitting the model between device, edge server, and cloud can significantly improve the performance of EI. The model segmentation without user mobility has been investigated in detail in previous studies. However, in most EI use cases, the end devices are mobile. Few studies have been conducted on this topic. These works still have many issues, such as ignoring the energy consumption of mobile device, inappropriate network assumption, and low effectiveness on adapting user mobility, etc. Therefore, to address the disadvantages of model segmentation and resource allocation in previous studies, we propose mobility and cost aware model segmentation and resource allocation algorithm for accelerating the inference at edge (MCSA). Specifically, in the scenario without user mobility, the loop iteration gradient descent (Li-GD) algorithm is provided. When the mobile user has a large model inference task that needs to be calculated, it will take the energy consumption of mobile user, the communication and computing resource renting cost, and the inference delay into account to find the optimal model segmentation and resource allocation strategy. In the scenario with user mobility, the mobility aware Li-GD (MLi-GD) algorithm is proposed to calculate the optimal strategy. Then, the properties of the proposed algorithms are investigated, including convergence, complexity, and approximation ratio. The experimental results demonstrate the effectiveness of the proposed algorithms.},
  archive      = {J_TMC},
  author       = {Xin Yuan and Ning Li and Kang Wei and Wenchao Xu and Quan Chen and Hao Chen and Song Guo},
  doi          = {10.1109/TMC.2024.3484158},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1530-1549},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility and cost aware inference accelerating algorithm for edge intelligence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive obsolete packet management based analysis of age
of information for LCFS heterogeneous queueing system. <em>TMC</em>,
<em>24</em>(3), 1513–1529. (<a
href="https://doi.org/10.1109/TMC.2024.3481062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes the Age of Information (AoI), focusing on transmitting status updates from source to destination. We analyze the Age of Information in a system comprised of two heterogeneous servers with exponential distribution parameters $\mu _{1}$ and $\mu _{2}$, respectively. Our study adopts the stochastic hybrid systems (SHS) methodology to thoroughly assess the system’s performance. We explore various queueing disciplines, including Last-Come-First-Serve (LCFS) with work-conservative and LCFS with probabilistic routing, to accurately quantify AoI and Peak AoI (PAoI) metrics. We have used the Proactive Obsolete Packet Management (POPMAN) approach to identify and discard obsolete packets proactively, thus enhancing server processing efficiency and ensuring orderly packet reception. We also investigate the following parameters, such as the probability of preemption of packets, the probability of packets getting obsolete, the probability of informative packets, and optimal splitting probabilities. Results show an improvement in both AoI and PAoI within the LCFS with work-conservative queueing system with the integration of the POPMAN method. Furthermore, LCFS with probabilistic routing using the POPMAN approach performs similarly to conventional methods. In all the queueing systems studied, as the arrival rate $\lambda \to \infty$, the average AoI and PAoI approach $1/(\mu _{1}+\mu _{2})$. For c servers, they approach $1/(\mu _{1}+\mu _{2}+\cdots +\mu _{c})$.},
  archive      = {J_TMC},
  author       = {Y. Arun Kumar Reddy and T. G Venkatesh},
  doi          = {10.1109/TMC.2024.3481062},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1513-1529},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive obsolete packet management based analysis of age of information for LCFS heterogeneous queueing system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math inline">${\sf
Img2Acoustic}$</span>&lt;Mml:math
xmlns:mml=“http://www.w3.org/1998/math/MathML”&gt;&lt;mml:mrow&gt;&lt;mml:mi
mathvariant=“sans-serif”&gt;img&lt;/mml:mi&gt;&lt;mml:mn
mathvariant=“sans-serif”&gt;2&lt;/mml:mn&gt;&lt;mml:mi
mathvariant=“sans-serif”&gt;acoustic&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;:
A cross-modal gesture recognition method based on few-shot learning.
<em>TMC</em>, <em>24</em>(3), 1496–1512. (<a
href="https://doi.org/10.1109/TMC.2024.3481443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic-based human gesture recognition (HGR) offers diverse applications due to the ubiquity of sensors and touch-free interaction. However, existing machine learning approaches require substantial training data, making the process time-consuming, costly, and labor-intensive. Recent studies have explored cross-modal methods to reduce the need for large training datasets in behavior recognition, but they typically rely on open-source datasets that closely align with the target domain, limiting flexibility and complicating data collection. In this paper, we propose ${\sf Img2Acoustic}$, a novel cross-modal acoustic-based HGR approach that leverages models trained on open-source image datasets (i.e., EMNIST, Omniglot) to effectively recognize custom gestures detected via acoustic signals. Our model incorporates a task-aware attention layer (TAAL) and a task-aware local matching layer (TALML), enabling seamless transfer of knowledge from image datasets to acoustic gesture recognition. We implement ${\sf Img2Acoustic}$ on commercial devices and conduct comprehensive evaluations, demonstrating that our method not only delivers superior accuracy and robustness compared to existing approaches but also eliminates the need for extensive training data collection.},
  archive      = {J_TMC},
  author       = {Yongpan Zou and Jianhao Weng and Wenting Kuang and Yang Jiao and Victor C. M. Leung and Kaishun Wu},
  doi          = {10.1109/TMC.2024.3481443},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1496-1512},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {${\sf Img2Acoustic}$&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;sans-serif&quot;&gt;Img&lt;/mml:mi&gt;&lt;mml:mn mathvariant=&quot;sans-serif&quot;&gt;2&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;sans-serif&quot;&gt;Acoustic&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;: a cross-modal gesture recognition method based on few-shot learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ui-ear: On-face gesture recognition through on-ear vibration
sensing. <em>TMC</em>, <em>24</em>(3), 1482–1495. (<a
href="https://doi.org/10.1109/TMC.2024.3480216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the convenient design and prolific functionalities, wireless earbuds are fast penetrating in our daily life and taking over the place of traditional wired earphones. The sensing capabilities of wireless earbuds have attracted great interests of researchers on exploring them as a new interface for human-computer interactions. However, due to its extremely compact size, the interaction on the body of the earbuds is limited and not convenient. In this paper, we propose Ui-Ear, a new on-face gesture recognition system to enrich interaction maneuvers for wireless earbuds. Ui-Ear exploits the sensing capability of Inertial Measurement Units (IMUs) to extend the interaction to the skin of the face near ears. The accelerometer and gyroscope in IMUs perceive dynamic vibration signals induced by on-face touching and moving, which brings rich maneuverability. Since IMUs are provided on most of the budget and high-end wireless earbuds, we believe that Ui-Ear has great potential to be adopted pervasively. To demonstrate the feasibility of the system, we define seven different on-face gestures and design an end-to-end learning approach based on Convolutional Neural Networks (CNNs) for classifying different gestures. To further improve the generalization capability of the system, adversarial learning mechanism is incorporated in the offline training process to suppress the user-specific features while enhancing gesture-related features. We recruit 20 participants and collect a realworld datasets in a common office environment to evaluate the recognition accuracy. The extensive evaluations show that the average recognition accuracy of Ui-Ear is over 95% and 82.3% in the user-dependent and user-independent tasks, respectively. Moreover, we also show that the pre-trained model (learned from user-independent task) can be fine-tuned with only few training samples of the target user to achieve relatively high recognition accuracy (up to 95%). At last, we implement the personalization and recognition components of Ui-Ear on an off-the-shelf Android smartphone to evaluate its system overhead. The results demonstrate Ui-Ear can achieve real-time response while only brings trivial energy consumption on smartphones.},
  archive      = {J_TMC},
  author       = {Guangrong Zhao and Yiran Shen and Feng Li and Lei Liu and Lizhen Cui and Hongkai Wen},
  doi          = {10.1109/TMC.2024.3480216},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1482-1495},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Ui-ear: On-face gesture recognition through on-ear vibration sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate prediction of multi-dimensional required resources
in 5G via federated deep reinforcement learning. <em>TMC</em>,
<em>24</em>(3), 1469–1481. (<a
href="https://doi.org/10.1109/TMC.2024.3480136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of required resources in terms of storage, computing and bandwidth is essential for 5G to host diverse services. The existing efforts illustrate that it is more promising to efficiently predict the unknown required resources with a third-order tensor compared to the 2D-matrix-based solutions. However, most of them fail to leverage the inherent features hidden in network traffic like temporal stability and service correlation to build a third-order tensor for the multi-dimensional required resource prediction in an intelligent manner, incurring coarse-grained prediction accuracy. Furthermore, it is difficult to build a third-order tensor with rate-varied measurements in 5G due to different lengths of measurement time slots. To address these issues, we propose an Accurate Prediction of Multi-Dimensional Required Resources (APMR) approach in 5G via Federated Deep Reinforcement Learning (FDRL). We first confirm the resource requests originated from different Base Stations (BSs) at varied measurement rates have similar features in service and time domains, but cannot directly form a series of regular tensors. Built on these observations, we reshape these measurement data to form a series of standard third-order tensors with the same size, which include many elements obtained from measurements and some unknown elements needed to be inferred. In order to obtain accurately predicted results, the FDRL-based tensor factorization approach is introduced to intelligently utilize multiple specific iteration rules for local model learning, and the accuracy-aware and latency-based depreciation strategies are exploited to aggregate local models for resource prediction. Extensive simulation experiments demonstrate that APMR can accurately predict the multi-dimensional required resources compared to the state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Qifan Wang and Weimin Wu and Miao Wang and Geyong Min},
  doi          = {10.1109/TMC.2024.3480136},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1469-1481},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Accurate prediction of multi-dimensional required resources in 5G via federated deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-intrusive and efficient estimation of antenna 3-d
orientation for WiFi APs. <em>TMC</em>, <em>24</em>(3), 1453–1468. (<a
href="https://doi.org/10.1109/TMC.2024.3485228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of WiFi-based localization systems heavily relies on the spatial accuracy of WiFi AP. In real-world scenarios, factors such as AP rotation and irregular antenna tilt contribute significantly to inaccuracies, surpassing the impact of imprecise AP location and antenna separation. In this paper, we propose Anteumbler, a non-invasive, accurate, and efficient system for measuring the orientation of each antenna in physical space. By leveraging the fact that maximum received power occurs when a Tx-Rx antenna pair is perfectly aligned, we build a spatial angle model capable of determining antennas’ orientations without prior knowledge. However, achieving comprehensive coverage across the spatial angle necessitates extensive sampling points. To enhance efficiency, we exploit the orthogonality of antenna directivity and polarization, and adopt an iterative algorithm, thereby reducing the number of sampling points by several orders of magnitude. Additionally, to attain the required antenna orientation accuracy, we mitigate the influence of propagation distance using a dual plane intersection model while filtering out ambient noise. Our real-world experiments, covering six antenna types, two antenna layouts, two antenna separations ($\lambda /2$ and $\lambda$ ), and three AP heights, demonstrate that Anteumbler achieves median errors below $\text{6}^\circ$ for both elevation and azimuth angles, and exhibits robustness in NLoS and dynamic environments. Moreover, when integrated into the reverse localization system, Anteumbler deployed over LocAP reduces antenna separation error by $10 \,\mathrm{mm}$, while for user localization system, its integration over SpotFi reduces user localization error by more than $1 \,\mathrm{m}$.},
  archive      = {J_TMC},
  author       = {Dawei Yan and Panlong Yang and Fei Shang and Nikolaos M. Freris and Yubo Yan},
  doi          = {10.1109/TMC.2024.3485228},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1453-1468},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Non-intrusive and efficient estimation of antenna 3-D orientation for WiFi APs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enable practical long-range multi-target backscatter
sensing. <em>TMC</em>, <em>24</em>(3), 1437–1452. (<a
href="https://doi.org/10.1109/TMC.2024.3480137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backscatter sensing has emerged as a significant technology within the Internet of Things (IoT), prompting extensive research interest. This paper presents LoMu, the first long-range multi-target backscatter sensing system designed for low-cost tags operating under ambient LoRa. LoMuintroduces an orthogonal sensing model that processes backscatter signals from multiple tags to extract motion information. The design addresses several practical challenges, including near-far interference among multiple tags, phase offsets from unsynchronized transceivers, and phase errors due to frequency drift in low-cost tags. To overcome these issues, we propose a conjugate-based energy concentration method to extract high-quality signals and a Hamming-window-based method to mitigate the near-far problem. Additionally, we exploit the relationship between excitation and backscatter signals to synchronize the transmitter (TX) and receiver (RX) and combine double sidebands of backscatter signals to eliminate tag frequency drift. Furthermore, a novel joint estimation algorithm is introduced to exploit both amplitude and phase information in target signals, enhancing frequency sensing results and robustness. Our implementation and extensive experiments demonstrate that LoMucan accurately sense up to 35 tags simultaneously and achieve an average frequency sensing error of 0.5% at a range of 400 meters, which is $4\times$ the range of the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Yihao Liu and Jinyan Jiang and Jumin Zhao and Jiliang Wang},
  doi          = {10.1109/TMC.2024.3480137},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1437-1452},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enable practical long-range multi-target backscatter sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical sensing-based intelligent toothbrushing monitoring
system. <em>TMC</em>, <em>24</em>(3), 1417–1436. (<a
href="https://doi.org/10.1109/TMC.2024.3479455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorrect brushing methods normally lead to poor oral hygiene, and result in severe oral diseases and complications. While effective brushing can address this issue, individuals often struggle with incorrect brushing, like aggressive brushing, insufficient brushing, and missing brushing. To break this stalemate, in this paper, we proposed LiT, a toothbrushing monitoring system to assess the brushing status on 16 surfaces using the Bass technique. LiT utilizes commercial LED toothbrushes’ blue LEDs as transmitters, and incorporates only two low-cost photodetectors as receivers on the toothbrush head. It is challenging to determine optimal deployment positions and minimize photodetectors number to establish the light transmission channel in oral cavity. To address these challenges, we established mathematical models within the oral cavity based on the two photodetectors’ deployment to theoretically validate the feasibility and prove robustness. Furthermore, we designed a comprehensive framework to fight against the implementation challenges including brushing action separation, light interference on the outer surfaces of front teeth, toothpaste diversity, user variations, brushing hand variability, and incorrect brushings. Experimental results demonstrate that LiT achieves a highly accurate surface recognition rate of 95.3%, an estimated error for brushing duration of 6.1%, and incorrect brushing detection accuracy of 96.9%. Furthermore, LiT retains stable capability under a variety of circumstances, such as various lighting conditions, user movement, toothpaste diversity, and left and right-handed users.},
  archive      = {J_TMC},
  author       = {Kaixin Chen and Lei Wang and Yongzhi Huang and Kaishun Wu and Lu Wang},
  doi          = {10.1109/TMC.2024.3479455},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1417-1436},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optical sensing-based intelligent toothbrushing monitoring system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory optimization and pick-up and delivery sequence
design for cellular-connected cargo AAVs. <em>TMC</em>, <em>24</em>(3),
1402–1416. (<a href="https://doi.org/10.1109/TMC.2024.3480910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a cargo autonomous aerial vehicle (AAV)-aided multi-parcel pick-up and delivery network, where the communication ability of the AAV is provided by the ground base stations (GBSs). For such a system setup, our goal is to optimize the trajectory of the cargo AAV while minimizing the combined impact of total energy consumption and total outage time. Simultaneously, we aim to maximize overall user satisfaction throughout the entire flight duration. More specifically, we propose a pick-up and delivery of AAV (PDU) framework to address this problem and this framework consists of two parts. First, a simulated annealing (SA) algorithm is used to obtain the pick-up and delivery (P&amp;D) order of parcels. On the basis of obtaining the P&amp;D order through SA, we further use deep reinforcement learning (DRL) to optimize the flight trajectory of the AAV to ensure the expected communication quality between the AAV and GBSs. To verify the effectiveness of our proposed algorithms, we design three baseline strategies for comparison, and also investigate the effect of using the PDU framework with different weights. Finally, numerical results show that the performance of PDU strategy is improved by about 5%-30% compared with other strategies in solving the performance tradeoff of AAV energy consumption, communication quality, and user satisfaction.},
  archive      = {J_TMC},
  author       = {Jiangling Cao and Liang Yang and Dingcheng Yang and Tiankui Zhang and Lin Xiao and Hongbo Jiang and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3480910},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1402-1416},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trajectory optimization and pick-up and delivery sequence design for cellular-connected cargo AAVs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel cycle time: A new measure of short-term fairness.
<em>TMC</em>, <em>24</em>(3), 1386–1401. (<a
href="https://doi.org/10.1109/TMC.2024.3484177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper puts forth a new metric, dubbed channel cycle time (CCT), to measure the short-term fairness of communication networks. CCT characterizes the average duration between two consecutive successful transmissions of a user, during which all other users successfully accessed the channel at least once. In contrast to existing short-term fairness measures, CCT provides more comprehensive insight into the transient dynamics of communication networks, with a particular focus on users’ delays and jitter. To validate the efficacy of our approach, we analytically characterize the CCTs for two classical communication protocols: slotted Aloha and CSMA/CA. The analysis demonstrates that CSMA/CA exhibits superior short-term fairness over slotted Aloha. Beyond its role as a measurement metric, CCT has broader implications as a guiding principle for the design of future communication networks by emphasizing factors like fairness, delay, and jitter in short-term behaviors.},
  archive      = {J_TMC},
  author       = {Pengfei Shen and Yulin Shao and Haoyuan Pan and Lu Lu and Yonina C. Eldar},
  doi          = {10.1109/TMC.2024.3484177},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1386-1401},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Channel cycle time: A new measure of short-term fairness},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computation offloading and resource allocation in LEO
satellite-terrestrial integrated networks with system state delay.
<em>TMC</em>, <em>24</em>(3), 1372–1385. (<a
href="https://doi.org/10.1109/TMC.2024.3479243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing offloading optimization for energy saving is becoming increasingly important in low-Earth orbit (LEO) satellite-terrestrial integrated networks (STINs) since battery techniques have not kept up with the demand of ground terminal devices. In this paper, we design a delay-based deep reinforcement learning (DRL) framework specifically for computation offloading decisions, which can effectively reduce the energy consumption. Additionally, we develop a multi-level feedback queue for computing allocation (RAMLFQ), which can effectively enhance the CPU’s efficiency in task scheduling. We initially formulate the computation offloading problem with the system delay as Delay Markov Decision Processes (DMDPs), and then transform them into the equivalent standard Markov Decision Processes (MDPs). To solve the optimization problem effectively, we employ a double deep Q-network (DDQN) method, enhancing it with an augmented state space to better handle the unique challenges posed by system delays. Simulation results demonstrate that the proposed learning-based computing offloading algorithm achieves high levels of performance efficiency and attains a lower total cost compared to other existing offloading methods.},
  archive      = {J_TMC},
  author       = {Bo Xie and Haixia Cui and Ivan Wang-Hei Ho and Yejun He and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3479243},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1372-1385},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computation offloading and resource allocation in LEO satellite-terrestrial integrated networks with system state delay},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new data completion perspective on sparse CrowdSensing:
Spatiotemporal evolutionary inference approach. <em>TMC</em>,
<em>24</em>(3), 1357–1371. (<a
href="https://doi.org/10.1109/TMC.2024.3480983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile CrowdSensing (MCS) has emerged as a popular paradigm to engage mobile users in collaborative sensing tasks. However, its performance is hindered by its limited spatiotemporal range and the cost of data collection. An effective strategy is to integrate Sparse MCS with data completion, allowing for unsensed data inference. However, when confronted with situations where sensed data is excessively sparse, data inference results may be unsatisfactory due to several challenges including: 1) uneven data distribution, 2) complex spatiotemporal correlation, and 3) the presence of inference noise. To address these challenges, we propose a model named Spatiotemporal Evolutionary Inference (STEI) that achieves accurate inference of unsensed data in Sparse MCS. Specifically, we complete the unsensed data by uncovering strong local correlations in the data and gradually evolving those correlations to the global situation. In each evolution step, we thoroughly consider the impact of spatiotemporal consistency and difference. To minimize the interference of noise during the evolution process, we design an adaptive coefficient to enhance the dependence on sensed data. Finally, to validate the effectiveness of STEI, we conduct extensive qualitative and quantitative experiments using three popular datasets. The experimental results demonstrate that our approach excels in accurately inferring data, particularly in situations where the distribution of data is notably uneven.},
  archive      = {J_TMC},
  author       = {En Wang and Zixuan Song and Mengni Wu and Wenbin Liu and Bo Yang and Yongjian Yang and Jie Wu},
  doi          = {10.1109/TMC.2024.3480983},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1357-1371},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A new data completion perspective on sparse CrowdSensing: Spatiotemporal evolutionary inference approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed numerology-based intelligent resource management in a
sliced 6G space–terrestrial integrated radio access network.
<em>TMC</em>, <em>24</em>(3), 1338–1356. (<a
href="https://doi.org/10.1109/TMC.2024.3494842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although resource sharing and mixed numerology among slices are promising for improving wireless resource utilization, these techniques can compromise isolation performance and cause serious inter numerology interference (INI). Therefore, this paper studies wireless resource management in a mixed numerology-based sliced 6G space–terrestrial integrated radio access network (STI-RAN) with the aim of reducing INI and guaranteeing isolation performance while decreasing interference from Doppler frequency shifts caused by the high-speed movement of low-orbit satellites. First, an isolation performance indicator is defined to evaluate different isolation performances, and a universal spectral distance model is formulated to rewrite the INI power model. Next, the dynamic wireless resource management problem is formulated in a discrete form, yielding a scheme called Flex-$\mu$, which is designed to reduce the INI and Doppler frequency shifts, guarantee isolation performance, and enhance the SINR. Finally, an intelligent multi-characteristic matrix coding-based social group optimization (MultiMatrix-SGO) algorithm is designed to solve the proposed NP-hard discrete optimization problem. Compared with existing schemes, the system utility is efficiently increased by up to 58.32%, the SINR can converge to 38.44 dB, and the isolation performance is guaranteed while the INI and Doppler frequency shifts are reduced.},
  archive      = {J_TMC},
  author       = {Ning Hui and Qian Sun and Jie Zeng and Lin Tian and Yuanyuan Wang and Yiqing Zhou},
  doi          = {10.1109/TMC.2024.3494842},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1338-1356},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mixed numerology-based intelligent resource management in a sliced 6G Space–Terrestrial integrated radio access network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based joint caching and routing
in AI-driven networks. <em>TMC</em>, <em>24</em>(3), 1322–1337. (<a
href="https://doi.org/10.1109/TMC.2024.3481276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce redundant traffic transmission in both wired and wireless networks, optimal content placement problem naturally occurring in many applications is studied. In this paper, considering the limited cache capacity, unknown popularity distribution and non-stationary user demands, we address this problem by jointly optimizing content caching and routing with the objective of minimizing transmission cost. By optimizing the routing with the route-to-least cost-cache policy, the content caching process is modeled as a Markov decision process (MDP), aiming to maximize caching reward. However, the optimization problem consists of multiple nodes selecting caching contents, which leads to the combinatorial increase of the number of action dimensions with the number of possible actions. To handle this curse of dimensionality, we propose an intelligent caching algorithm by embedding action branching architecture into a dueling double deep Q-network (D3QN) to optimize caching decisions, and thus the agent at the controller can adaptively learn and track the underlying dynamics. Considering the independence of each branch, a marginal gain-based replacement rule is proposed to satisfy cache capacity constraint. Our simulation results show that compared with the prior art, the caching reward and hit rate of the proposed algorithm are increased by 35.3% and 33.6% respectively on average.},
  archive      = {J_TMC},
  author       = {Meiyi Yang and Deyun Gao and Weiting Zhang and Dong Yang and Dusit Niyato and Hongke Zhang and Victor C. M. Leung},
  doi          = {10.1109/TMC.2024.3481276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1322-1337},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep reinforcement learning-based joint caching and routing in AI-driven networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential privacy budget recycling for federated vector
mean estimation: A game-theoretic approach. <em>TMC</em>,
<em>24</em>(3), 1308–1321. (<a
href="https://doi.org/10.1109/TMC.2024.3484010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving vector mean estimation is a crucial primitive in federated analytics. Existing practices usually resort to Local Differentiated Privacy (LDP) mechanisms that inject random noise into users’ vectors when communicating with users and the central server. Due to the privacy-utility trade-off, the privacy budget has been widely recognized as the bottleneck resource that requires well-provisioning. In this paper, we explore the possibility of privacy budget recycling and propose a novel ChainDP framework enabling users to carry out data aggregation sequentially to recycle the privacy budget. We establish a sequential game to model the user interactions in our framework. We theoretically show the mathematical nature of the sequential game, solve its Nash Equilibrium, and design an incentive mechanism with provable economic properties. To alleviate potential privacy collusion attacks, we further derive a differentially privacy-guaranteed protocol to avoid holistic exposure. Our numerical simulation validates the effectiveness of ChainDP, showing that it can significantly save privacy budget as well as lower estimation error compared to the traditional LDP mechanism.},
  archive      = {J_TMC},
  author       = {Jingyi Li and Guangjing Huang and Liekang Zeng and Lin Chen and Xu Chen},
  doi          = {10.1109/TMC.2024.3484010},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1308-1321},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sequential privacy budget recycling for federated vector mean estimation: A game-theoretic approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedSN: A federated learning framework over heterogeneous LEO
satellite networks. <em>TMC</em>, <em>24</em>(3), 1293–1307. (<a
href="https://doi.org/10.1109/TMC.2024.3481275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communications but also for various machine learning applications. However, a ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, enabling FL on LEO satellites still face three critical challenges: i) heterogeneous computing and memory capabilities, ii) limited downlink/uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges. Specifically, we first present a novel sub-structure scheme to enable heterogeneous local model training considering different computing, memory, and communication constraints on LEO satellites. Additionally, we propose a pseudo-synchronous model aggregation strategy to dynamically schedule model aggregation for compensating model staleness. Extensive experiments with real-world satellite data demonstrate that FedSN framework achieves higher accuracy, lower computing, and communication overhead than the state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Zheng Lin and Zhe Chen and Zihan Fang and Xianhao Chen and Xiong Wang and Yue Gao},
  doi          = {10.1109/TMC.2024.3481275},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1293-1307},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedSN: A federated learning framework over heterogeneous LEO satellite networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale mechanism design for networks: Superimposability
and dynamic implementation. <em>TMC</em>, <em>24</em>(3), 1278–1292. (<a
href="https://doi.org/10.1109/TMC.2024.3499958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network utility maximization (NUM) is a fundamental framework for optimizing next-generation networks. However, self-interested agents with private information pose challenges due to potential system manipulation. To address these challenges, the literature on economic mechanism design has emerged. Existing mechanisms are not suited for large-scale networks due to their complexity, high implementation costs, and difficulty to adapt to dynamic settings. This paper proposes a large-scale mechanism design framework that mitigates these limitations. As the number of agents $I$ approaches infinity, their incentive to misreport decreases rapidly at a rate of $\mathcal {O}(1/I^{2})$. We introduce a superimposable framework applicable to any NUM algorithm without modifications, reducing implementation costs. In the dynamic setting, the large-scale mechanism design framework introduces the decomposability of the problem, enabling agents to align their own interests with the objectives of the dynamic NUM problem. This alignment helps overcome the additional, more stringent incentive constraints encountered in dynamic settings. Extending our results to dynamic settings, we present the design of a Dynamic Large-Scale mechanism with desirable properties and the corresponding Dynamic Superimposable Large-Scale mechanism. Our numerical experiments validate the fact that our proposed schemes are approximately $I$ times faster than the seminal VCG mechanism.},
  archive      = {J_TMC},
  author       = {Meng Zhang and Deepanshu Vasal},
  doi          = {10.1109/TMC.2024.3499958},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1278-1292},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Large-scale mechanism design for networks: Superimposability and dynamic implementation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Travel demand modeling and estimation for high-dimensional
mobility. <em>TMC</em>, <em>24</em>(3), 1264–1277. (<a
href="https://doi.org/10.1109/TMC.2024.3435436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive amount of data related to spatiotemporal mobility offers new opportunities to understand human mobility with applications in various sectors, including transportation, logistics, and safety. However, the increase in the volume and in the dimension of mobility data makes it challenging to retrieve important information and critical features of spatiotemporal mobility. This paper develops a method to estimate probabilistic occurrences of travel demands considering interactions between origin, destination, and departure time. First, we reveal the important features in the complex structure of mobility data and identify mobility patterns. Then, we derive a data-driven model, accounting for mobility patterns, to estimate and predict travel demands. We quantify the accuracy of the proposed method for a case study using both New York city yellow taxi trip data and for-hire vehicles trip data over the entire city. Results show the accuracy of the proposed method compared to existing approaches.},
  archive      = {J_TMC},
  author       = {Jeongyun Kim and Andrea Conti and Moe Z. Win},
  doi          = {10.1109/TMC.2024.3435436},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1264-1277},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Travel demand modeling and estimation for high-dimensional mobility},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient resource allocation scheme with uncertain
network status in edge computing-enabled networks. <em>TMC</em>,
<em>24</em>(3), 1249–1263. (<a
href="https://doi.org/10.1109/TMC.2024.3412810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative resource allocation is crucial for reducing overhead and enhancing resource utilization in edge computing-enabled networks. To ensure a satisfactory user experience, we recognize the importance of considering information uncertainty in resource allocation. Therefore, we explore information uncertainty in edge computing-enabled networks, especially within the complex environment of resource coupling. However, existing methods lack a comprehensive and robust solution for coordinating wireless, transport, and computing resource under this information uncertainty. This paper addresses this gap by proposing a joint optimization of access point (AP) selection, computing node association, and traffic engineering, aiming to maximize network utility under the uncertain conditions of wireless status and application QoS requirements. The constraints under these uncertainties are modeled as chance constraints, complicating the problem&#39;s solvability. We adopt the Bernstein approximation to establish convex conservative approximations of the chance constraints. Given the problem&#39;s substantial size and computational complexity, the alternating direction method of multipliers is employed to solve the approximated problem in a distributed manner. We further derive the closed solutions of the corresponding sub-problems. Extensive simulations validate the superiority of our proposed scheme, demonstrating its ability to achieve a good trade-off between meeting user requirements and optimizing resource utilization.},
  archive      = {J_TMC},
  author       = {Yuxia Cheng and Chengchao Liang and Qianbin Chen and F. Richard Yu},
  doi          = {10.1109/TMC.2024.3412810},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1249-1263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient resource allocation scheme with uncertain network status in edge computing-enabled networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
