<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai---18">TAI - 18</h2>
<ul>
<li><details>
<summary>
(2025). Artificial intelligence across europe: A study on awareness,
attitude and trust. <em>TAI</em>, <em>6</em>(2), 477–490. (<a
href="https://doi.org/10.1109/TAI.2024.3461633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the results of an extensive study investigating the opinions on artificial intelligence (AI) of a sample of 4006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people&#39;s views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe, we design and validate a new questionnaire (PAICE) structured around three dimensions: people&#39;s awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting on the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI policies. The introduction of rules that ensure legal and ethical standards, along with the activity of high-level educational entities, and the promotion of AI literacy are identified as key factors in supporting a trustworthy AI ecosystem. We make some recommendations for AI governance focused on the European context and conclude with suggestions for future work.},
  archive      = {J_TAI},
  author       = {Teresa Scantamburlo and Atia Cortés and Francesca Foffano and Cristian Barrué and Veronica Distefano and Long Pham and Alessandro Fabris},
  doi          = {10.1109/TAI.2024.3461633},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {477-490},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Artificial intelligence across europe: A study on awareness, attitude and trust},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Under the influence: A survey of large language models in
fake news detection. <em>TAI</em>, <em>6</em>(2), 458–476. (<a
href="https://doi.org/10.1109/TAI.2024.3471735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research into fake news detection has a long history, although it gained significant attention following the 2016 U.S. election. During this time, the widespread use of social media and the resulting increase in interpersonal communication led to the extensive spread of ambiguous and potentially misleading news. Traditional approaches, relying solely on pre-large language model (LLM) techniques and addressing the issue as a simple classification problem, have shown to be insufficient for improving detection accuracy. In this context, LLMs have become crucial, as their advanced architectures overcome the limitations of pre-LLM methods, which often fail to capture the subtleties of fake news. This literature review aims to shed light on the field of fake news detection by providing a brief historical overview, defining fake news, reviewing detection methods used before the advent of LLMs, and discussing the strengths and weaknesses of these models in an increasingly complex landscape. Furthermore, it will emphasize the importance of using multimodal datasets in the effort to detect fake news.},
  archive      = {J_TAI},
  author       = {Soveatin Kuntur and Anna Wróblewska and Marcin Paprzycki and Maria Ganzha},
  doi          = {10.1109/TAI.2024.3471735},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {458-476},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Under the influence: A survey of large language models in fake news detection},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal knowledge extrapolation based on fine-grained
tensor graph attention network for responsible AI. <em>TAI</em>,
<em>6</em>(2), 448–457. (<a
href="https://doi.org/10.1109/TAI.2024.3410932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge guidance is crucial for bridging the gap between high-level artificial intelligence (AI) ethics principles and the practical implementation of responsible AI systems. Diverging from static knowledge inference and temporal interpolation, the task of temporal knowledge extrapolation, which entails predicting future facts based on the evolution of historical facts, poses formidable challenges that remain largely unsolved. In existing temporal extrapolation methods, the structural learning of concurrent facts is primarily addressed by utilizing relation-aware graph neural networks. However, the temporal validity of facts leads to the sparsity of temporal subgraphs, where inherent coarse-level learning mechanisms hinder the capture of nuanced and scarce local semantics. Thus, this article proposes a fine-grained tensor graph attention network (F-GAT) that promotes representation learning of concurrent facts on sparse subgraphs by effectively distinguishing the significance of entities and relations within triplets and acquiring self-attention across diverse triplet contexts. Based on F-GAT, this article proposes a recurrent evolution network [recurrent evolution graph attention network (RE-GAT)] for temporal knowledge extrapolation. RE-GAT employed gated recurrent units to iteratively capture the sequential patterns between adjacent temporal facts, while simultaneously learning enriched embedding through the dual influence of historical factors and structural factors. The competitive results achieved by comparing the entity and relationship prediction performance of the proposed RE-GAT model with advanced methods on six public benchmarks demonstrate the effectiveness of RE-GAT for temporal extrapolation.},
  archive      = {J_TAI},
  author       = {Jing Yang and Chengxuan Huang and Xiangli Yang and Laurence T. Yang and Yuan Gao and Cong Liu},
  doi          = {10.1109/TAI.2024.3410932},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {448-457},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Temporal knowledge extrapolation based on fine-grained tensor graph attention network for responsible AI},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering machine learning with scalable feature
engineering and interpretable AutoML. <em>TAI</em>, <em>6</em>(2),
432–447. (<a href="https://doi.org/10.1109/TAI.2024.3400752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated feature engineering (FE) has gained considerable attention in academia and industry. Nevertheless, existing systems often lack practical scalability and efficiency. This article introduces BigFeat, a scalable and interpretable framework that streamlines critical phases of the machine learning pipeline: FE, model selection, and hyperparameter tuning. BigFeat presents two execution options: as a standalone FE framework, denoted as BigFeat-FE, and as an AutoML framework, referred to as BigFeat-AutoML. BigFeat-FE optimizes input feature quality with the ultimate aim of maximizing predictive performance, based on a user-defined metric. BigFeat-FE employs a dynamic feature generation and selection mechanism that systematically creates a set of expressive features. These features not only enhance prediction performance but also prioritize interpretability. BigFeat-FE employs a meta-learning technique to warm-start the optimization process, resulting in significant overall performance gains. BigFeat-AutoML, tailored for algorithm selection and hyperparameter tuning, harnesses a random search method over the space of interpretable models. We conducted extensive experiments, and the results demonstrate that BigFeat-FE consistently outperforms state-of-the-art automated FE frameworks, such as AutoFeat and scalable automatic feature engineering (SAFE), across a wide range of datasets, achieving an average performance improvement of 8.65% compared to AutoFeat and 4.71% compared to SAFE, respectively. Additionally, BigFeat-AutoML demonstrates substantial performance improvement when compared to a tree-based pipeline optimization tool (TPOT) for automating machine learning and Autosklearn, with average improvements of 0.74% over TPOT and 2.25% over Autosklearn, respectively. Furthermore, BigFeat&#39;s scalability is affirmed through its linear complexity, and execution times, averaging 20 times faster than AutoFeat and 14 times faster than SAFE.},
  archive      = {J_TAI},
  author       = {Hassan Eldeeb and Radwa Elshawi},
  doi          = {10.1109/TAI.2024.3400752},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {432-447},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Empowering machine learning with scalable feature engineering and interpretable AutoML},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fostering human rights in responsible AI: A systematic
review for best practices in industry. <em>TAI</em>, <em>6</em>(2),
416–431. (<a href="https://doi.org/10.1109/TAI.2024.3394389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent rapid development of generative artificial intelligence (AI), and the resulting market growth, has introduced new challenges for social responsibility, an area where companies may need more guidance. In this regard, the literature covers a broad spectrum, from the impact of bias to the potential use of this technology to implement undemocratic surveillance. Another focus area discusses the AI industry&#39;s commitment to human rights and social responsibility, examining the diverse actors involved in this commitment and the context-dependent nature of their impact on human rights. This work performs a systematic review and a comparative analysis of the strategies and actions taken by four leading companies—OpenAI, Meta AI Research, Google AI, and Microsoft AI—with respect to five critical dimensions: bias, privacy, cybersecurity, hate speech, and misinformation. Our study analyzes 192 publicly available documents and reveals that depending on the diversity of products and their nature, some companies excel in the research and development of technologies and methodologies for privacy preservation and bias reduction, offering user-friendly tools for managing personal data, establishing expert groups to research the social impact of their technologies, and possessing significant expertise in tackling hate speech and misinformation. Nonetheless, there is an urgent need for greater linguistic, cultural, and geographic diversity in research lines, tools, and collaborative efforts. From this analysis, we draw a set of actionable best practices aimed at supporting the responsible development of AI models, and foundation models, in particular, that are aligned with human rights principles.},
  archive      = {J_TAI},
  author       = {Maria Teresa Baldassarre and Danilo Caivano and Berenice Fernández Nieto and Domenico Gigante and Azzurra Ragone},
  doi          = {10.1109/TAI.2024.3394389},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {416-431},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fostering human rights in responsible AI: A systematic review for best practices in industry},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLMaaS: Serving large-language models on trusted serverless
computing platforms. <em>TAI</em>, <em>6</em>(2), 405–415. (<a
href="https://doi.org/10.1109/TAI.2024.3429480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the emergence of large-language models (LLMs) has profoundly transformed our production and lifestyle. These models have shown tremendous potential in fields, such as natural language processing, speech recognition, and recommendation systems, and are increasingly playing crucial roles in applications such as human–computer interaction and intelligent customer service. Efficient inference solutions for LLMs in data centers have been extensively researched, with a focus on meeting users’ quality of service requirements. In this article, we focus on two additional requirements that responsible LLM inference should meet under QoS conditions: security throughout the model execution process and low maintenance requirements for the inference system. Therefore, we propose LLMaaS, a trusted model inference platform based on a serverless computing platform aimed at providing inference as a service for LLMs. First, we design a trusted serverless computing platform based on software guard extension (SGX), which includes distributed identity verification and SGX device plugins to ensure the security and trustworthiness of the inference process. Additionally, to reduce the maintenance requirements of the system, we enhance the SGX-based deep learning computing framework, including replacing PyTorch and using a greedy algorithm for graph partitioning. We conduct tests on four typical large models, and the experimental results demonstrate that, with minimal overhead and user code modifications, we can ensure the security of model execution.},
  archive      = {J_TAI},
  author       = {Zinuo Cai and Rongbo Ma and Yicheng Fu and Weishan Zhang and Ruhui Ma and Haibing Guan},
  doi          = {10.1109/TAI.2024.3429480},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {405-415},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {LLMaaS: Serving large-language models on trusted serverless computing platforms},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A locality-sensitive-hashing-based collaborative
recommendation method for responsible AI-driven recommender systems.
<em>TAI</em>, <em>6</em>(2), 393–404. (<a
href="https://doi.org/10.1109/TAI.2024.3381571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most representative recommendation solutions, traditional collaborative filtering (CF) models typically have limitations in dealing with large-scale, sparse data to capture complex relationships between users and items. The rise of artificial intelligence (AI) provides powerful tools such as deep neural networks to overcome the data sparsity issue with typical CF models. Existing works on AI-driven recommender systems are focusing on improving recommendation accuracy by extracting rich user/item features from multisource data with deep learning tools to build more accurate user preference model. How to ensure the responsibility of AI-driven recommender systems is still a big challenge. On one hand, AI-driven recommender systems need to access raw data such as user profiles to train recommendation model, which face the risk of leaking user privacy information. On the other hand, the ever-increasing volume of user/item interaction records raises the efficiency challenge to provide recommendation results in a real-time manner. In view of those observations, we propose a collaborative recommendation method by adopting the locality-sensitive-hashing (LSH) technique (i.e., ${\text{DisRec}_{\text{LSH}}}$), which aims to achieve the goal of privacy protection and calculation efficiency for AI-driven recommender systems. More specifically, we split ${\text{DisRec}_{\text{LSH}}}$ into three phases: 1) offline feature extraction: deep neural networks are applied to extract user/item features from multisource user/item data on each local dataset; 2) offline user index building: LSH technique is adopted to map user features to hash codes to quickly recall correlated similar users for a given target user; and 3) online top-N items calculation: with similar users selected by phase 2, top-N items are calculated based on the ranking of predicted user–item rating score. Finally, extensive experiments are conducted on three public datasets to evaluate the efficiency of our proposal.},
  archive      = {J_TAI},
  author       = {Wenmin Lin and Xinyi Zhou and Lu Sun and Lianyong Qi and Sang-Bing Tsai and Yihong Yang and Hanwen Liu and Huaizhen Kou and Lingzhen Kong},
  doi          = {10.1109/TAI.2024.3381571},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {393-404},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A locality-sensitive-hashing-based collaborative recommendation method for responsible AI-driven recommender systems},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nudging toward responsible recommendations: A graph-based
approach to mitigate belief filter bubbles. <em>TAI</em>, <em>6</em>(2),
378–392. (<a href="https://doi.org/10.1109/TAI.2024.3373392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized recommendation systems homogenize user preferences, causing an extreme belief imbalance and aggravating user bias. This phenomenon is known as the filter bubble. This article presents the responsible graph-based recommendation (RGRec) system, designed to alleviate the filter bubble effect in personalized recommendation systems. Acting as an intermediate agency between users and existing preference-based recommendation systems, RGRec is composed of three collaborative modules: the multifaceted reasoning-based filter bubbles detection (FBDetect) module, the belief nudging module, and the generative artificial intelligence (GAI)-based recommendation strategy generation module (RecomGen). The FBDetect module identifies users with extreme belief imbalances based on their belief networks, which are represented as heterogeneous graphs. These graphs are then utilized in the Belief Nudging module, where a nudging strategy is employed to adapt prompts for the RecomGen module. Ultimately, the RecomGen module generates contextually rich items for recommendations. Experimental results demonstrate that RGRec can promote diverse content exploration based on user feedback and progressively stimulate interest in topics users initially showed less interest in, encouraging individual exploration.},
  archive      = {J_TAI},
  author       = {Mengyan Wang and Yuxuan Hu and Shiqing Wu and Weihua Li and Quan Bai and Zihan Yuan and Chenting Jiang},
  doi          = {10.1109/TAI.2024.3373392},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {378-392},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Nudging toward responsible recommendations: A graph-based approach to mitigate belief filter bubbles},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency- and privacy-aware convolutional neural network
distributed inference for reliable artificial intelligence systems.
<em>TAI</em>, <em>6</em>(2), 365–377. (<a
href="https://doi.org/10.1109/TAI.2024.3366880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable artificial intelligence (AI) systems not only propose a challenge on providing intelligent services with high quality for customers but also require customers’ privacy to be protected as much as possible during process of the services. Given the ultrahigh computing load brought by deep-learning-based intelligent services to edge devices and the ultralong distance between edge and cloud, low-latency requirement of intelligent services is hard to meet in single edge computing or cloud computing. Edge–cloud collaborative inference of deep neural networks (DNNs) is considered a feasible solution to the problem. However, former work has not reduced the inference latency to the greatest extent and has not considered privacy protection in distributed systems. To solve the problem, we first establish a novel queue mechanism. Then, the convolution layer split decisions are made based on deep reinforcement learning (DRL) to realize the parallel inference of convolutional neural networks (CNNs) for inference latency reduction. Next, for each CNN, the partition decision is made based on brute force algorithm to further reduce inference latency and protect customers’ privacy. Finally, simulation results show that our method performs better than the existing other methods.},
  archive      = {J_TAI},
  author       = {Yuhao Hu and Xiaolong Xu and Lianyong Qi and Xiaokang Zhou and Xiaoyu Xia},
  doi          = {10.1109/TAI.2024.3366880},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {365-377},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Latency- and privacy-aware convolutional neural network distributed inference for reliable artificial intelligence systems},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking transferable adversarial attacks with double
adversarial neuron attribution. <em>TAI</em>, <em>6</em>(2), 354–364.
(<a href="https://doi.org/10.1109/TAI.2024.3368361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferable adversarial attacks are a threat to deep neural networks, in particular, for black-box scenarios where access to model information is limited. One can, for example, exploit the intermediate layer neurons to generate transferable adversarial samples. However, current works show limitations in providing feature-level attack mechanisms across multiple victim models. In light of the attribution methods, in this article, we investigate the attribution similarity across different models. We leverage the similarity to incorporate different attribution properties to enhance the sample transferability, for the first time, formulating a novel neuron attribution-based transferable attack termed DANAA++. Specifically, we utilize a range of adversarial attack methods to generate different baseline points through adversarial training. The attribution results are thus obtained along both linear and nonlinear integration paths. In our experiments, the baseline points and integration paths significantly help improve the transferability of adversarial samples. Our approach provides novel insights for building effective attribution-based feature-level adversarial attacks. https://github.com/LMBTough/DANAAPP},
  archive      = {J_TAI},
  author       = {Zhiyu Zhu and Zhibo Jin and Xinyi Wang and Jiayu Zhang and Huaming Chen and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2024.3368361},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {354-364},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rethinking transferable adversarial attacks with double adversarial neuron attribution},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy inference attack and defense in centralized and
federated learning: A comprehensive survey. <em>TAI</em>, <em>6</em>(2),
333–353. (<a href="https://doi.org/10.1109/TAI.2024.3363670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of new machine learning methods has led to their widespread application across various domains, significantly advancing the field of artificial intelligence. However, the process of training and inferring machine learning models relies on vast amounts of data, which often include sensitive private information. Consequently, the privacy and security of machine learning have encountered significant challenges. Several studies have demonstrated the vulnerability of machine learning to privacy inference attacks, but they often focus on specific scenarios, leaving a gap in understanding the broader picture. We provide a comprehensive review of privacy attacks in machine learning, focusing on two scenarios: centralized learning and federated learning. This article begins by presenting the architectures of both centralized learning and federated learning, along with their respective application scenarios. It then conducts a comprehensive review and categorization of related inference attacks, providing a detailed analysis of the different stages involved in these attacks. Moreover, the article thoroughly describes and compares the existing defense methods. Finally, the article concludes by highlighting open questions and potential future research directions, aiming to contribute to the ongoing competition between privacy attackers and defenders.},
  archive      = {J_TAI},
  author       = {Bosen Rao and Jiale Zhang and Di Wu and Chengcheng Zhu and Xiaobing Sun and Bing Chen},
  doi          = {10.1109/TAI.2024.3363670},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {333-353},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Privacy inference attack and defense in centralized and federated learning: A comprehensive survey},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A membership inference and adversarial attack defense
framework for network traffic classifiers. <em>TAI</em>, <em>6</em>(2),
317–332. (<a href="https://doi.org/10.1109/TAI.2024.3357791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious traffic identification methods in intrusion detection systems have evolved from rule-based matching to machine learning. However, security risks such as membership inference and adversarial attacks hinder the practical deployment of machine learning-based network intrusion detection systems (ML-NIDSs). In this work, we design a defense framework called hierarchical differential privacy (HierarchicalDP) to safeguard ML-NIDS against membership inference and adversarial attacks. First, we analyze the principles of membership inference and adversarial attacks to find their correlation. Based on this, we propose the feature distribution security metric (FDSM) to measure the risk of membership inference and adversarial attacks on ML-NIDS. Then, we design the HierarchicalDP framework, which partitions network traffic sample features according to security levels and introduces distinct noise on each security level feature to satisfy FDSM, thus defensing against membership inference and adversarial attacks. Finally, we evaluate the defensive performance of the HierarchicalDP framework on two network traffic datasets and four machine-learning models. The HierarchicalDP defense framework, based on Laplace noise, reduces the success rate of membership inference from 64.9% to 54.4% (ineffective binary classification), the evasion rate of adversarial samples from 86.1% to 23.2%, and maintains model accuracy (ACC) fluctuations within 4.2%. Furthermore, the HierarchicalDP framework adjusts sample features without modifying the model, thereby not affecting the inference speed. HierarchicalDP offers efficient and convenient defenses for ML-NIDS deployed in a network.},
  archive      = {J_TAI},
  author       = {Guangrui Liu and Weizhe Zhang and Xurun Wang and Stephen King and Shui Yu},
  doi          = {10.1109/TAI.2024.3357791},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {317-332},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A membership inference and adversarial attack defense framework for network traffic classifiers},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A credible and fair federated learning framework based on
blockchain. <em>TAI</em>, <em>6</em>(2), 301–316. (<a
href="https://doi.org/10.1109/TAI.2024.3355362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables cooperative computation between multiple participants while protecting user privacy. Currently, FL algorithms assume that all participants are trustworthy and their systems are secure. However, the following problems arise in real-world scenarios: 1) Malicious clients disrupt FL through model poisoning and data poisoning attacks. Although some research has proposed secure aggregation methods to solve this problem, most methods have limitations. 2) The current method cannot fairly evaluate client contribution in some scenarios. Some clients exhibit free-rider behavior, seeking to cheat the reward system and manipulate global models. Evaluating client contribution and distributing rewards also present challenges. To address these challenges, we design a trustworthy federated framework to ensure secure computing throughout the federated task process. First, we propose a method of detecting malicious models to guarantee secure model aggregation. Then, we propose a fair method of assessing contribution to identify client-side free-riding behavior. Finally, we implement a computation process based on blockchain and smart contracts to guarantee the trustworthiness and fairness of federated tasks. To validate the performance of our framework, we simulate different types of client attacks and contribution evaluation scenarios on several open-source datasets. The experiments show that our framework ensures the credibility of federated tasks and achieves a fair evaluation of client contributions.},
  archive      = {J_TAI},
  author       = {Leiming Chen and Dehai Zhao and Liping Tao and Kai Wang and Sibo Qiao and Xingjie Zeng and Chee Wei Tan},
  doi          = {10.1109/TAI.2024.3355362},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {301-316},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A credible and fair federated learning framework based on blockchain},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency attacks based on invertible neural networks.
<em>TAI</em>, <em>6</em>(2), 292–300. (<a
href="https://doi.org/10.1109/TAI.2023.3349238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks reveal the vulnerability of classifiers based on deep neural networks to well-designed perturbations. Most existing attack methods focus on adding perturbations directly to the pixel space. However, the perturbations generated by these methods may be easily perceived by humans. To alleviate the aforementioned problem, we propose a novel high-frequency attack based on invertible neural networks (HA-INN) that relies on INNs to adds perturbations to the high-frequency space of the image instead of the pixel space. In this way, we can fool the classifier while the perturbations are not easily detected by humans. Specifically, we introduce INNs to separate the high-frequency and low-frequency components of the image. And the low-frequency components are guaranteed to be reconstructed as the original image, while the high-frequency components are replaced with resampled high-frequency latent variables with additional adversarial information. Then, the low-frequency components and the high-frequency components with adversarial information are inversely fed into the INN to generate effective adversarial examples. Extensive experiments on two datasets (CIFAR-10 and CIFAR-100) show that our method can generate misleading and transferable cross-architectural adversarial examples with greatly reduced computational resource requirements. Under the white-box setting, the attack success rate of CIFAR10 and CIFAR100 is 99.8% and 99.74%, respectively. Further, under the black-box setting, the adversarial examples generated by our method are more effective than the other methods.},
  archive      = {J_TAI},
  author       = {Ming-Wen Shao and Jian-Xin Yang and Ling-Zhuang Meng and Zhi-Yong Hu},
  doi          = {10.1109/TAI.2023.3349238},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {292-300},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Frequency attacks based on invertible neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FairCompass: Operationalizing fairness in machine learning.
<em>TAI</em>, <em>6</em>(2), 281–291. (<a
href="https://doi.org/10.1109/TAI.2023.3348429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) increasingly becomes an integral part of our societal and individual activities, there is a growing imperative to develop responsible AI solutions. Despite a diverse assortment of machine learning fairness solutions is proposed in the literature, there is reportedly a lack of practical implementation of these tools in real-world applications. Industry experts have participated in thorough discussions on the challenges associated with operationalizing fairness in the development of machine learning-empowered solutions, in which a shift toward human-centred approaches is promptly advocated to mitigate the limitations of existing techniques. In this work, we propose a human-in-the-loop approach for fairness auditing, presenting a mixed visual analytical system (hereafter referred to as “FairCompass”), which integrates both subgroup discovery technique and the decision tree-based schema for end users. Moreover, we innovatively integrate an exploration, guidance, and informed analysis loop, to facilitate the use of the knowledge generation model for visual analytics in FairCompass. We evaluate the effectiveness of FairCompass for fairness auditing in a real-world scenario, and the findings demonstrate the system&#39;s potential for real-world deployability. We anticipate this work will address the current gaps in research for fairness and facilitate the operationalization of fairness in machine learning systems.},
  archive      = {J_TAI},
  author       = {Jessica Liu and Huaming Chen and Jun Shen and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2023.3348429},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {281-291},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FairCompass: Operationalizing fairness in machine learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedDGA: Federated multitask learning based on dynamic guided
attention. <em>TAI</em>, <em>6</em>(2), 268–280. (<a
href="https://doi.org/10.1109/TAI.2024.3350538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of privacy-sensitive data has spurred the development of federated learning (FL), which is an important technology for state-of-the-art machine learning and responsible AI. However, most existing FL methods are constrained in their applicability and generalizability due to their narrow focus on specific tasks. This article presents a novel federated multitask learning (FMTL) framework that is capable of acquiring knowledge across multiple tasks. To address the challenges posed by non-IID data and task imbalance in FMTL, this study proposes a federated fusion strategy based on dynamic guided attention (FedDGA), which adaptively fine-tunes local models for multiple tasks with personalized attention. In addition, this article designed dynamic batch weight (DBW) to balance the task losses and improve the convergence speed. Extensive experiments were conducted on various datasets, tasks, and settings, and the proposed method was compared with state-of-the-art methods such as FedAvg, FedProx, and SCAFFOLD. The results show that our method achieves significant performance gains, with up to 11.1% increase in accuracy over the baselines.},
  archive      = {J_TAI},
  author       = {Haoyun Sun and Hongwei Zhao and Liang Xu and Weishan Zhang and Hongqing Guan and Su Yang},
  doi          = {10.1109/TAI.2024.3350538},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {268-280},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FedDGA: Federated multitask learning based on dynamic guided attention},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ethical aspects of ChatGPT in software engineering research.
<em>TAI</em>, <em>6</em>(2), 254–267. (<a
href="https://doi.org/10.1109/TAI.2023.3318183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT can improve software engineering (SE) research practices by offering efficient, accessible information analysis, and synthesis based on natural language interactions. However, ChatGPT could bring ethical challenges, encompassing plagiarism, privacy, data security, and the risk of generating biased or potentially detrimental data. This research aims to fill the given gap by elaborating on the key elements: motivators, demotivators, and ethical principles of using ChatGPT in SE research. To achieve this objective, we conducted a literature survey, identified the mentioned elements, and presented their relationships by developing a taxonomy. Furthermore, the identified literature-based elements (motivators, demotivators, and ethical principles) were empirically evaluated by conducting a comprehensive questionnaire-based survey involving SE researchers. In addition, we employed an interpretive structure modeling approach to analyze the relationships between the ethical principles of using ChatGPT in SE research and develop a level-based decision model. We further conducted a cross-impact matrix multiplication applied to classification analysis to create a cluster-based decision model. These models aim to help SE researchers devise effective strategies for ethically integrating ChatGPT into SE research by following the identified principles by adopting the motivators and addressing the demotivators. The findings of this study will establish a benchmark for incorporating ChatGPT services in SE research with an emphasis on ethical considerations.},
  archive      = {J_TAI},
  author       = {Muhammad Azeem Akbar and Arif Ali Khan and Peng Liang},
  doi          = {10.1109/TAI.2023.3318183},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {254-267},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ethical aspects of ChatGPT in software engineering research},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Operationalizing responsible AI.
<em>TAI</em>, <em>6</em>(2), 252–253. (<a
href="https://doi.org/10.1109/TAI.2025.3527806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAI},
  author       = {Qinghua Lu and Apostol Vassilev and Jun Zhu and Foutse Khomh},
  doi          = {10.1109/TAI.2025.3527806},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {252-253},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Operationalizing responsible AI},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
