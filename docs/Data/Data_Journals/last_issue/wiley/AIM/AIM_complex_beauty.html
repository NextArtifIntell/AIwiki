<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aim---8">AIM - 8</h2>
<ul>
<li><details>
<summary>
(2025). PADTHAI-MM: Principles-based approach for designing
trustworthy, human-centered AI using the MAST methodology. <em>AIM</em>,
<em>46</em>(1), e70000. (<a
href="https://doi.org/10.1002/aaai.70000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite an extensive body of literature on trust in technology, designing trustworthy AI systems for high-stakes decision domains remains a significant challenge. Widely used system design guidelines and tools are rarely attuned to domain-specific trustworthiness principles. In this study, we introduce a design framework to address this gap within intelligence analytic tasks, called the Principles-based Approach for Designing Trustworthy, Human-centered AI using the MAST Methodology (PADTHAI-MM). PADTHAI-MM builds on the Multisource AI Scorecard Table (MAST), an AI decision support system evaluation tool designed in accordance to the U.S. Intelligence Community&#39;s standards for system trustworthiness. We demonstrate PADTHAI-MM in our development of the Reporting Assistant for Defense and Intelligence Tasks (READIT), a research platform that leverages data visualizations and natural language processing-based text analysis to emulate AI-enabled intelligence reporting aids. To empirically assess the efficacy of PADTHAI-MM, we developed two versions of READIT for comparison: a “High-MAST” version, which incorporates AI contextual information and explanations, and a “Low-MAST” version, designed to be akin to inscrutable “black box” AI systems. Through an iterative design process guided by stakeholder feedback, our multidisciplinary design team developed prototypes that were evaluated by experienced intelligence analysts. Results substantially supported the viability of PADTHAI-MM in designing for system trustworthiness in this task domain. We also explored the relationship between analysts&#39; MAST ratings and three theoretical categories of information known to impact trust: process , purpose , and performance . Overall, our study supports the practical and theoretical viability of PADTHAI-MM as an approach to designing trustable AI systems.},
  archive      = {J_AIM},
  author       = {Myke C. Cohen and Nayoung Kim and Yang Ba and Anna Pan and Shawaiz Bhatti and Pouria Salehi and James Sung and Erik Blasch and Mickey V. Mancenido and Erin K. Chiou},
  doi          = {10.1002/aaai.70000},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e70000},
  shortjournal = {AI Mag.},
  title        = {PADTHAI-MM: Principles-based approach for designing trustworthy, human-centered AI using the MAST methodology},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What AIs are not learning (and why). <em>AIM</em>,
<em>46</em>(1), e12213. (<a
href="https://doi.org/10.1002/aaai.12213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s robots do not yet learn the general skills that are necessary to provide home care, to be nursing assistants, to interact with people, or do household chores nearly as well as people do. Addressing the aspirational goal of creating service robots requires improving how they are created. Today&#39;s mainstream AIs are not created by agents learning from experiences doing tasks in real-world contexts and interacting with people. Today&#39;s robots do not learn by sensing, acting, doing experiments, and collaborating. Future robots will need to learn from such experiences in order to be ready for robust deployment in human service applications. This paper investigates what aspirational future autonomous human-compatible service robots will need to know. It recommends developing experiential (robotic) foundation models (FMs) for bootstrapping them.},
  archive      = {J_AIM},
  author       = {Mark Stefik},
  doi          = {10.1002/aaai.12213},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12213},
  shortjournal = {AI Mag.},
  title        = {What AIs are not learning (and why)},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness amidst non-IID graph data: A literature review.
<em>AIM</em>, <em>46</em>(1), e12212. (<a
href="https://doi.org/10.1002/aaai.12212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing importance of understanding and addressing algorithmic bias in artificial intelligence (AI) has led to a surge in research on AI fairness, which often assumes that the underlying data are independent and identically distributed (IID). However, real-world data frequently exist in non-IID graph structures that capture connections among individual units. To effectively mitigate bias in AI systems, it is essential to bridge the gap between traditional fairness literature, designed for IID data, and the prevalence of non-IID graph data. This survey reviews recent advancements in fairness amidst non-IID graph data, including the newly introduced fair graph generation and the commonly studied fair graph classification. In addition, available datasets and evaluation metrics for future research are identified, the limitations of existing work are highlighted, and promising future directions are proposed.},
  archive      = {J_AIM},
  author       = {Wenbin Zhang and Shuigeng Zhou and Toby Walsh and Jeremy C. Weiss},
  doi          = {10.1002/aaai.12212},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12212},
  shortjournal = {AI Mag.},
  title        = {Fairness amidst non-IID graph data: A literature review},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond scaleup: Knowledge-aware parsimony learning from deep
networks. <em>AIM</em>, <em>46</em>(1), e12211. (<a
href="https://doi.org/10.1002/aaai.12211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brute-force scaleup of training datasets, learnable parameters and computation power, has become a prevalent strategy for developing more robust learning models. However, due to bottlenecks in data, computation, and trust, the sustainability of this strategy is a serious concern. In this paper, we attempt to address this issue in a parsimonious manner (i.e., achieving greater potential with simpler models). The key is to drive models using domain-specific knowledge, such as symbols, logic, and formulas, instead of purely relying on scaleup. This approach allows us to build a framework that uses this knowledge as “building blocks” to achieve parsimony in model design, training, and interpretation. Empirical results show that our methods surpass those that typically follow the scaling law. We also demonstrate our framework in AI for science, specifically in the problem of drug-drug interaction prediction. We hope our research can foster more diverse technical roadmaps in the era of foundation models.},
  archive      = {J_AIM},
  author       = {Quanming Yao and Yongqi Zhang and Yaqing Wang and Nan Yin and James Kwok and Qiang Yang},
  doi          = {10.1002/aaai.12211},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12211},
  shortjournal = {AI Mag.},
  title        = {Beyond scaleup: Knowledge-aware parsimony learning from deep networks},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric machine learning. <em>AIM</em>, <em>46</em>(1),
e12210. (<a href="https://doi.org/10.1002/aaai.12210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cornerstone of machine learning is the identification and exploitation of structure in high-dimensional data. While classical approaches assume that data lies in a high-dimensional Euclidean space, geometric machine learning methods are designed for non-Euclidean data, including graphs, strings, and matrices, or data characterized by symmetries inherent in the underlying system. In this article, we review geometric approaches for uncovering and leveraging structure in data and how an understanding of data geometry can lead to the development of more effective machine learning algorithms with provable guarantees.},
  archive      = {J_AIM},
  author       = {Melanie Weber},
  doi          = {10.1002/aaai.12210},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12210},
  shortjournal = {AI Mag.},
  title        = {Geometric machine learning},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of security and privacy issues of machine
unlearning. <em>AIM</em>, <em>46</em>(1), e12209. (<a
href="https://doi.org/10.1002/aaai.12209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning is a cutting-edge technology that embodies the privacy legal principle of the right to be forgotten within the realm of machine learning (ML). It aims to remove specific data or knowledge from trained models without retraining from scratch and has gained significant attention in the field of artificial intelligence in recent years. However, the development of machine unlearning research is associated with inherent vulnerabilities and threats, posing significant challenges for researchers and practitioners. In this article, we provide the first comprehensive survey of security and privacy issues associated with machine unlearning by providing a systematic classification across different levels and criteria. Specifically, we begin by investigating unlearning-based security attacks, where adversaries exploit vulnerabilities in the unlearning process to compromise the security of machine learning (ML) models. We then conduct a thorough examination of privacy risks associated with the adoption of machine unlearning. Additionally, we explore existing countermeasures and mitigation strategies designed to protect models from malicious unlearning-based attacks targeting both security and privacy. Further, we provide a detailed comparison between machine unlearning-based security and privacy attacks and traditional malicious attacks. Finally, we discuss promising future research directions for security and privacy issues posed by machine unlearning, offering insights into potential solutions and advancements in this evolving field.},
  archive      = {J_AIM},
  author       = {Aobo Chen and Yangyi Li and Chenxu Zhao and Mengdi Huai},
  doi          = {10.1002/aaai.12209},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12209},
  shortjournal = {AI Mag.},
  title        = {A survey of security and privacy issues of machine unlearning},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the reliability of large language models to misinformed
and demographically informed prompts. <em>AIM</em>, <em>46</em>(1),
e12208. (<a href="https://doi.org/10.1002/aaai.12208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate and observe the behavior and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots&#39; ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/Edge-of-Tomorrow .},
  archive      = {J_AIM},
  author       = {Toluwani Aremu and Oluwakemi Akinwehinmi and Chukwuemeka Nwagu and Syed Ishtiaque Ahmed and Rita Orji and Pedro Arnau Del Amo and Abdulmotaleb El Saddik},
  doi          = {10.1002/aaai.12208},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12208},
  shortjournal = {AI Mag.},
  title        = {On the reliability of large language models to misinformed and demographically informed prompts},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role and significance of state-building as ensuring
national security in the context of artificial intelligence development.
<em>AIM</em>, <em>46</em>(1), e12207. (<a
href="https://doi.org/10.1002/aaai.12207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a major technology and represents a fundamental and revolutionary innovation of our time that has the potential to significantly change the global scenario. In the context of further development of artificial intelligence, state establishment plays a central role in ensuring national security. Countries are tasked with developing legal frameworks for the development and application of AI. Additionally, governments should commit resources to AI research and development to ensure access to cutting-edge technology. As AI continues to evolve, nation-building remains crucial for the protection of national security. Countries must shoulder the responsibility of establishing legal structures to supervise the progression and implementation of artificial intelligence. Investing in AI research and development is essential to secure access to cutting-edge technology. Gracious society and open engagement apply critical impact on forming AI approaches. Civic organizations can contribute to expanding open mindfulness of the related dangers and openings of AI, guaranteeing straightforwardness and responsibility in legislative activities, and pushing for the creation of capable AI approaches. Open interest can help governments in comprehending the yearnings of citizens with respect to AI approaches. This study explores the role and importance of nation-building in ensuring national security in the context of the development of artificial intelligence. It also examines how civil society and public participation can effectively shape AI policy. The topic offers diverse research and analytical opportunities that enable a deeper understanding of the interactions and mutual influences between statehood and artificial intelligence in the context of ensuring national security. It examines the potential and threats that artificial intelligence poses to national security and considers strategies that countries can adopt to ensure security in this area. Based on the research findings, recommendations and suggestions are made for governments and civil society to improve the effectiveness of public participation in formulating AI policies.},
  archive      = {J_AIM},
  author       = {Vitaliy Gumenyuk and Anatolii Nikitin and Oleksandr Bondar and Iaroslav Zhydovtsev and Hanna Yermakova},
  doi          = {10.1002/aaai.12207},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12207},
  shortjournal = {AI Mag.},
  title        = {The role and significance of state-building as ensuring national security in the context of artificial intelligence development},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
