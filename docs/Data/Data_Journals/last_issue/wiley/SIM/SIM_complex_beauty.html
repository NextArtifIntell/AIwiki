<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---35">SIM - 35</h2>
<ul>
<li><details>
<summary>
(2025). Estimating mean viral load trajectory from intermittent
longitudinal data and unknown time origins. <em>SIM</em>,
<em>44</em>(5), e70033. (<a
href="https://doi.org/10.1002/sim.70033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral load (VL) in the respiratory tract is the leading proxy for assessing infectiousness potential. Understanding the dynamics of disease-related VL within the host is of great importance, as it helps to determine different policies and health recommendations. However, normally the VL is measured on individuals only once, in order to confirm infection, and furthermore, the infection date is unknown. It is therefore necessary to develop statistical approaches to estimate the typical VL trajectory. We show here that, under plausible parametric assumptions, two measures of VL on infected individuals can be used to accurately estimate the VL mean function. Specifically, we consider a discrete-time likelihood-based approach to modeling and estimating partial observed longitudinal samples. We study a multivariate normal model for a function of the VL that accounts for possible correlation between measurements within individuals. We derive an expectation-maximization (EM) algorithm which treats the unknown time origins and the missing measurements as latent variables. Our main motivation is the reconstruction of the daily mean VL, given measurements on patients whose VLs were measured multiple times on different days. Such data should and can be obtained at the beginning of a pandemic with the specific goal of estimating the VL dynamics. For demonstration purposes, the method is applied to SARS-Cov-2 cycle-threshold-value data collected in Israel.},
  archive      = {J_SIM},
  author       = {Yonatan Woodbridge and Micha Mandel and Yair Goldberg and Amit Huppert},
  doi          = {10.1002/sim.70033},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70033},
  shortjournal = {Stat. Med.},
  title        = {Estimating mean viral load trajectory from intermittent longitudinal data and unknown time origins},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the restricted mean survival time using
pseudo-value random forests. <em>SIM</em>, <em>44</em>(5), e70031. (<a
href="https://doi.org/10.1002/sim.70031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) has become a popular measure to summarize event times in longitudinal studies. Defined as the area under the survival function up to a time horizon τ &gt; 0 $$ \tau &gt;0 $$ , the RMST can be interpreted as the life expectancy within the time interval [ 0 , τ ] $$ \left[0,\tau \right] $$ . In addition to its straightforward interpretation, the RMST allows for the definition of valid estimands for the causal analysis of treatment contrasts in medical studies. In this work, we introduce a non-parametric approach to model the RMST conditional on a set of baseline variables (including, e.g., treatment variables and confounders). Our method is based on a direct modeling strategy for the RMST, using leave-one-out jackknife pseudo-values within a random forest regression framework. In this way, it can be employed to obtain precise estimates of both patient-specific RMST values and confounder-adjusted treatment contrasts. Since our method (termed “pseudo-value random forest”, PVRF) is model-free, RMST estimates are not affected by restrictive assumptions like the proportional hazards assumption. Particularly, PVRF offers a high flexibility in detecting relevant covariate effects from higher-dimensional data, thereby expanding the range of existing pseudo-value modeling techniques for RMST estimation. We investigate the properties of our method using simulations and illustrate its use by an application to data from the SUCCESS-A breast cancer trial. Our numerical experiments demonstrate that PVRF yields accurate estimates of both patient-specific RMST values and RMST-based treatment contrasts.},
  archive      = {J_SIM},
  author       = {Alina Schenk and Vanessa Basten and Matthias Schmid},
  doi          = {10.1002/sim.70031},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70031},
  shortjournal = {Stat. Med.},
  title        = {Modeling the restricted mean survival time using pseudo-value random forests},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network meta-analysis with individual participant-level data
of time-to-event outcomes using cox regression. <em>SIM</em>,
<em>44</em>(5), e70027. (<a
href="https://doi.org/10.1002/sim.70027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accessibility of individual participant-level data (IPD) enhances the evaluation of moderation effects of patient covariates. It facilitates the provision of accurate estimation of intervention effects and confidence intervals by incorporating covariate correlations across multiple clinical trials. With a time-to-event outcome, Cox regression can be applied for network meta-analysis (NMA) using IPD. However, there lacks comprehensive reviews and comparisons of the specifications and assumptions of these Cox models and their impact on the interpretation of hazard ratios, effect moderation, and trial heterogeneity in IPD-NMA. In this paper, we examine various Cox models for IPD-NMA and compare different approaches to modeling trial, treatment, and covariate effects. We employ multiple graphical tools and statistical tests to assess proportional hazard assumptions and discuss their implications. Additionally, we explore the application of extended Cox models when the proportional hazard assumption is violated. Practical guidance on interpreting and reporting NMA results is provided. A simulation study is conducted to compare the performance of different models. We illustrate the methods to conduct IPD-NMA through a real data example.},
  archive      = {J_SIM},
  author       = {Kaiyuan Hua and Daniel Wojdyla and Anthony Carnicelli and Christopher Granger and Xiaofei Wang and Hwanhee Hong},
  doi          = {10.1002/sim.70027},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70027},
  shortjournal = {Stat. Med.},
  title        = {Network meta-analysis with individual participant-level data of time-to-event outcomes using cox regression},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to assess the predictiveness of a
continuous biomarker in early phases of drug development. <em>SIM</em>,
<em>44</em>(5), e70026. (<a
href="https://doi.org/10.1002/sim.70026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and quantifying predictive biomarkers is a critical issue of personalized medicine approaches and patient-centric clinical development strategies. In early stages of the development process, significant challenges and numerous uncertainties arise. One of the challenges is the ability to assess the predictive value of a biomarker, i.e., the difference in primary outcomes between experimental and placebo arms above and below a certain threshold of the biomarker. Indeed, when the accumulated information is very limited and the sample size is small, preliminary conclusions about the predictive properties of the biomarker might be misleading. To date, the majority of investigations regarding the predictiveness of biomarkers were in the setting of moderate-to-large sample sizes. In this work, we propose a novel flexible approach inspired by the Kolmogorov-Smirnov Distance in order to assess the predictiveness of a continuous biomarker in a clinical setting where the sample size is small. Via simulations we show that the proposed method allows to achieve a higher power to declare predictiveness compared to the existing methods under a range of scenarios, whilst still maintaining a control of the type I error at a pre-specified level.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Julia Geronimi and Sandrine Guilleminot and Hugo Hadjur and Marie-Karelle Riviere and Gaëlle Saint-Hilary and Pavel Mozgunov},
  doi          = {10.1002/sim.70026},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70026},
  shortjournal = {Stat. Med.},
  title        = {A novel approach to assess the predictiveness of a continuous biomarker in early phases of drug development},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A double machine learning approach for the evaluation of
COVID-19 vaccine effectiveness under the test-negative design: Analysis
of québec administrative data. <em>SIM</em>, <em>44</em>(5), e70025. (<a
href="https://doi.org/10.1002/sim.70025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The test-negative design (TND), which is routinely used for monitoring seasonal flu vaccine effectiveness (VE), has recently become integral to COVID-19 vaccine surveillance, notably in Québec, Canada. Some studies have addressed the identifiability and estimation of causal parameters under the TND, but efficiency bounds for nonparametric estimators of the target parameter under the unconfoundedness assumption have not yet been investigated. Motivated by the goal of improving adjustment for measured confounders when estimating COVID-19 VE among community-dwelling people aged ≥ 60 $$ \ge 60 $$ years in Québec, we propose a one-step doubly robust and locally efficient estimator called TNDDR (TND doubly robust), which utilizes cross-fitting (sample splitting) and can incorporate machine learning techniques to estimate the nuisance functions and thus improve control for measured confounders. We derive the efficient influence function (EIF) for the marginal expectation of the outcome under a vaccination intervention, explore the von Mises expansion, and establish the conditions for n $$ \sqrt{n} $$ -consistency, asymptotic normality, and double robustness of TNDDR. The proposed estimator is supported by both theoretical and empirical justifications.},
  archive      = {J_SIM},
  author       = {Cong Jiang and Denis Talbot and Sara Carazo and Mireille E. Schnitzer},
  doi          = {10.1002/sim.70025},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70025},
  shortjournal = {Stat. Med.},
  title        = {A double machine learning approach for the evaluation of COVID-19 vaccine effectiveness under the test-negative design: Analysis of québec administrative data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using win odds to improve commit-to-phase-3 decision-making
in oncology. <em>SIM</em>, <em>44</em>(5), e70024. (<a
href="https://doi.org/10.1002/sim.70024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making good decisions about whether to commit-to-phase 3 clinical trials is challenging. This is especially true in oncology because the relationships between the registration endpoint, overall survival, and endpoints such as progression-free survival and confirmed objective response are often poorly understood. We present a framework for decision-making based on a three-endpoint win odds. We discuss properties of the win odds and suggest that it can be interpreted, for decision-making, as the reciprocal of an average hazard ratio for overall survival. We confirm the performance of the decision-making method using simulation studies and a clinical trial case study. As part of this work, we describe the simulation of correlated patient-level oncology endpoints using a multi-state model of disease. This model can provide clinically realistic data for testing the performance of analysis methods. We conclude that the win odds can improve commit-to-phase-3 decision-making compared with other methods.},
  archive      = {J_SIM},
  author       = {Benjamin F. Hartley and Thomas Drury and Brian Di Pace and Helen Zhou and Tai-Tsang Chen and Inna Perevozskaya},
  doi          = {10.1002/sim.70024},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70024},
  shortjournal = {Stat. Med.},
  title        = {Using win odds to improve commit-to-phase-3 decision-making in oncology},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly modeling time-to-event and longitudinal data with
individual-specific change points: A case study in modeling tumor
burden. <em>SIM</em>, <em>44</em>(5), e70021. (<a
href="https://doi.org/10.1002/sim.70021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oncology clinical trials, tumor burden (TB) stands as a crucial longitudinal biomarker, reflecting the toll a tumor takes on a patient&#39;s prognosis. With certain treatments, the disease&#39;s natural progression shows the tumor burden initially receding before rising once more. Biologically, the point of change may be different between individuals and must have occurred between the baseline measurement and progression time of the patient, implying a random effects model obeying a bound constraint. However, in practice, patients may drop out of the study due to progression or death, presenting a non-ignorable missing data problem. In this paper, we introduce a novel joint model that combines time-to-event data and longitudinal data, where the latter is parameterized by a random change point augmented by random pre-slope and post-slope dynamics. Importantly, the model is equipped to incorporate covariates across the longitudinal and survival models, adding significant flexibility. Adopting a Bayesian approach, we propose an efficient Hamiltonian Monte Carlo (HMC) algorithm for parameter inference. We demonstrate the superiority of our approach compared to a longitudinal-only model via simulations and apply our method to a data set in oncology. The code for implementation is publicly available on https://github.com/quyixiang/chgptModel .},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Yixiang Qu and Emily Meghan Damone and Jing-ou Liu and Chenguang Wang and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70021},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70021},
  shortjournal = {Stat. Med.},
  title        = {Jointly modeling time-to-event and longitudinal data with individual-specific change points: A case study in modeling tumor burden},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph-theoretic approach to detection of parkinsonian
freezing of gait from videos. <em>SIM</em>, <em>44</em>(5), e70020. (<a
href="https://doi.org/10.1002/sim.70020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of Gait (FOG) is a prevalent symptom in advanced Parkinson&#39;s Disease (PD), characterized by intermittent transitions between normal gait and freezing episodes. This study introduces a novel graph-theoretic approach to detect FOG from video data of PD patients. We construct a sequence of pose graphs that represent the spatial relations and temporal progression of a patient&#39;s posture over time. Each graph node corresponds to an estimated joint position, while the edges reflect the anatomical connections and their proximity. We propose a hypothesis testing procedure that deploys the Fréchet statistics to identify break points in time between regular gait and FOG episodes, where we model the central tendency and dispersion of the pose graphs in the presentation of graph Laplacian matrices by computing their Fréchet mean and variance. We implement binary segmentation and incremental computation in our algorithm for efficient calculation. The proposed framework is validated on two datasets, Kinect3D and AlphaPose, demonstrating its effectiveness in detecting FOG from video data. The proposed approach that extracts matrix features is distinct from the prevailing pixel-based deep learning methods. It provides a new perspective on feature extraction for FOG detection and potentially contributes to improved diagnosis and treatment of PD.},
  archive      = {J_SIM},
  author       = {Qi Liu and Jie Bao and Xu Zhang and Chuan Shi and Catherine Liu and Rui Luo},
  doi          = {10.1002/sim.70020},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70020},
  shortjournal = {Stat. Med.},
  title        = {A graph-theoretic approach to detection of parkinsonian freezing of gait from videos},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of statistical methods for time-to-event
analyses in randomized controlled trials under non-proportional hazards.
<em>SIM</em>, <em>44</em>(5), e70019. (<a
href="https://doi.org/10.1002/sim.70019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While well-established methods for time-to-event data are available when the proportional hazards assumption holds, there is no consensus on the best inferential approach under non-proportional hazards (NPH). However, a wide range of parametric and non-parametric methods for testing and estimation in this scenario have been proposed. To provide recommendations on the statistical analysis of clinical trials where non-proportional hazards are expected, we conducted a simulation study under different scenarios of non-proportional hazards, including delayed onset of treatment effect, crossing hazard curves, subgroups with different treatment effects, and changing hazards after disease progression. We assessed type I error rate control, power, and confidence interval coverage, where applicable, for a wide range of methods, including weighted log-rank tests, the MaxCombo test, summary measures such as the restricted mean survival time (RMST), average hazard ratios, and milestone survival probabilities, as well as accelerated failure time regression models. We found a trade-off between interpretability and power when choosing an analysis strategy under NPH scenarios. While analysis methods based on weighted logrank tests typically were favorable in terms of power, they do not provide an easily interpretable treatment effect estimate. Also, depending on the weight function, they test a narrow null hypothesis of equal hazard functions, and rejection of this null hypothesis may not allow for a direct conclusion of treatment benefit in terms of the survival function. In contrast, non-parametric procedures based on well-interpretable measures like the RMST difference had lower power in most scenarios. Model-based methods based on specific survival distributions had larger power; however, often gave biased estimates and lower than nominal confidence interval coverage. The application of the studied methods is illustrated in a case study with reconstructed data from a phase III oncologic trial.},
  archive      = {J_SIM},
  author       = {Florian Klinglmüller and Tobias Fellinger and Franz König and Tim Friede and Andrew C. Hooker and Harald Heinzl and Martina Mittlböck and Jonas Brugger and Maximilian Bardo and Cynthia Huber and Norbert Benda and Martin Posch and Robin Ristl},
  doi          = {10.1002/sim.70019},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70019},
  shortjournal = {Stat. Med.},
  title        = {A comparison of statistical methods for time-to-event analyses in randomized controlled trials under non-proportional hazards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power and sample size calculations for cluster randomized
hybrid type 2 effectiveness-implementation studies. <em>SIM</em>,
<em>44</em>(5), e70015. (<a
href="https://doi.org/10.1002/sim.70015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid studies allow investigators to simultaneously study an intervention effectiveness outcome and an implementation research outcome. In particular, type 2 hybrid studies support research that places equal importance on both outcomes rather than focusing on one and secondarily on the other (i.e., type 1 and type 3 studies). Hybrid 2 studies introduce the statistical issue of multiple testing, complicated by the fact that they are typically also cluster randomized trials. Standard statistical methods do not apply in this scenario. Here, we describe the design methodologies available for validly powering hybrid type 2 studies and producing reliable sample size calculations in a cluster-randomized design with a focus on binary outcomes. Through a literature search, 18 publications were identified that included methods relevant to the design of hybrid 2 studies. Five methods were identified, two of which did not account for clustering but are extended in this article to do so, namely the combined outcomes approach and the single 1-degree of freedom combined test. Procedures for powering hybrid 2 studies using these five methods are described and illustrated using input parameters inspired by a study from the Community Intervention to Reduce CardiovascuLar Disease in Chicago (CIRCL-Chicago) Implementation Research Center. In this illustrative example, the intervention effectiveness outcome was controlled blood pressure, and the implementation outcome was reach. The conjunctive test resulted in higher power than the popular p value adjustment methods, and the newly extended combined outcomes and single 1-DF test were found to be the most powerful among all of the tests.},
  archive      = {J_SIM},
  author       = {Melody A. Owen and Geoffrey M. Curran and Justin D. Smith and Yacob Tedla and Chao Cheng and Donna Spiegelman},
  doi          = {10.1002/sim.70015},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70015},
  shortjournal = {Stat. Med.},
  title        = {Power and sample size calculations for cluster randomized hybrid type 2 effectiveness-implementation studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DOD-SSR: An adaptive seamless phase II/III design with dose
optimization decision and sample size re-estimation. <em>SIM</em>,
<em>44</em>(5), e70014. (<a
href="https://doi.org/10.1002/sim.70014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of seamless Phase II/III designs has grown in popularity as a strategy to potentially accelerate the drug development. Making well-informed decisions regarding the drug&#39;s potential and addressing important clinical inquiries at the conclusion of the exploratory phase has become a critical step. In response to the increased emphasis on dose optimization, it becomes logical to integrate treatment arm/dose selections into Phase II and implement corresponding design adjustments. Within this framework, employing a fixed sample size presents challenges due to limited information availability before the trial planning and elevated development risks. Furthermore, practical and feasibility considerations have led to the increased utilization of surrogate endpoints for making interim decisions. In this study, we introduce a novel framework for a seamless Phase II/III design involving multiple treatment arms, leveraging Bayesian predictive probability of success (PPoS) for both treatment arm selection and interim sample size re-estimation (SSR) using surrogate endpoints. The proposed design demonstrates improved performance, including a higher likelihood of selecting favorable treatment arm, increased overall statistical power, and reduced average event sizes and trial durations compared to traditional separate Phase II and III designs, as well as other seamless Phase II/III designs without SSR or of which treatment arm selection is based on conditional power. We also showcase the implementation of the proposed design through a case study in non-small cell lung cancer (NSCLC).},
  archive      = {J_SIM},
  author       = {Meizi Liu and Jianchang Lin and Yefei Zhang and Rachael Liu},
  doi          = {10.1002/sim.70014},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70014},
  shortjournal = {Stat. Med.},
  title        = {DOD-SSR: An adaptive seamless phase II/III design with dose optimization decision and sample size re-estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Challenges for predictive modeling with neural network
techniques using error-prone dietary intake data. <em>SIM</em>,
<em>44</em>(5), e70013. (<a
href="https://doi.org/10.1002/sim.70013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dietary intake data are routinely drawn upon to explore diet-health relationships, and inform clinical practice and public health. However, these data are almost always subject to measurement error, distorting true diet-health relationships. Beyond measurement error, there are likely complex synergistic and sometimes antagonistic interactions between different dietary components, complicating the relationships between diet and health outcomes. Flexible models are required to capture the nuance that these complex interactions introduce. This complexity makes research on diet-health relationships an appealing candidate for the application of modern machine learning techniques, and in particular, neural networks. Neural networks are computational models that can capture highly complex, nonlinear relationships, so long as sufficient data are available. While these models have been applied in many domains, the impacts of measurement error on the performance of predictive modeling have not been widely investigated. In this work, we demonstrate the ways in which measurement error erodes the performance of neural networks and illustrate the care that is required for leveraging these models in the presence of error. We demonstrate the role that sample size and replicate measurements play in model performance, indicate a motivation for the investigation of transformations to additivity, and illustrate the caution required to prevent model overfitting. While the past performance of neural networks across various domains makes them an attractive candidate for examining diet-health relationships, our work demonstrates that substantial care and further methodological development are both required to observe increased predictive performance when applying these techniques compared to more traditional statistical procedures.},
  archive      = {J_SIM},
  author       = {Dylan Spicker and Amir Nazemi and Joy Hutchinson and Paul Fieguth and Sharon Kirkpatrick and Michael Wallace and Kevin W. Dodd},
  doi          = {10.1002/sim.70013},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70013},
  shortjournal = {Stat. Med.},
  title        = {Challenges for predictive modeling with neural network techniques using error-prone dietary intake data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMST for interval-censored data in oncology clinical trials.
<em>SIM</em>, <em>44</em>(5), e70012. (<a
href="https://doi.org/10.1002/sim.70012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oncology studies, the assumption of proportional hazards is often questionable due to factors such as the presence of cured patients, a delayed treatment benefit, and possible treatment switching. The restricted mean survival time (RMST) has emerged as a valuable alternative summary measure to the hazard ratio (HR) in this scenario as it provides a clinically meaningful interpretation of treatment benefit without additional assumptions. As a commonly used primary endpoint, progression-free survival (PFS) is defined as the time from randomization to the first occurrence of death or progression of disease (PD). However, PFS involves dual observation processes where, in practice, the exact death time is typically recorded, but PD is interval-censored. This feature is also present in other commonly used primary endpoints, including event-free survival, disease-free survival, and relapse-free survival. The conventional approach imputes the PD time with the right boundary of the time interval during which the PD occurs. This paper presents alternative estimation and inference approaches to estimate RMST with a mixture of right-censored and interval-censored data. Different approaches are explored by simulation under various plausible scenarios for oncology clinical trials with regard to the assessment frequency, randomness in the actual assessment times, and size of treatment effect. The choice of the restricted time point in RMST is also explored. The simulation results indicate that the RMST estimators that take account of the interval censoring inherent in the data are unbiased and more accurate than the conventional estimators, while the performance for two-group comparisons is comparable. Furthermore, the performance of the proposed estimators is contingent on the scheduled assessment plan and patients&#39; visit window.},
  archive      = {J_SIM},
  author       = {Xiyuan Gao and Tianmeng Lyu and Menghao Xu and Lisa V. Hampson and Yan Du and Renxin Lin and Nigel Yateman and Lu Tian and Jianguo Sun},
  doi          = {10.1002/sim.70012},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70012},
  shortjournal = {Stat. Med.},
  title        = {RMST for interval-censored data in oncology clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instability of the AUROC of clinical prediction models.
<em>SIM</em>, <em>44</em>(5), e70011. (<a
href="https://doi.org/10.1002/sim.70011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Florian D. van Leeuwen and Ewout W. Steyerberg and David van Klaveren and Ben Wessler and David M. Kent and Erik W. van Zwet},
  doi          = {10.1002/sim.70011},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70011},
  shortjournal = {Stat. Med.},
  title        = {Instability of the AUROC of clinical prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A calibrated sensitivity analysis for weighted causal
decompositions. <em>SIM</em>, <em>44</em>(5), e70010. (<a
href="https://doi.org/10.1002/sim.70010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disparities in health or well-being experienced by minority groups can be difficult to study using the traditional exposure-outcome paradigm in causal inference, since potential outcomes in variables such as race or sexual minority status are challenging to interpret. Causal decomposition analysis addresses this gap by positing causal effects on disparities under interventions to other intervenable exposures that may play a mediating role in the disparity. While invoking weaker assumptions than causal mediation approaches, decomposition analyses are often conducted in observational settings and require uncheckable assumptions that eliminate unmeasured confounders. Leveraging the marginal sensitivity model, we develop a sensitivity analysis for weighted causal decomposition estimators and use the percentile bootstrap to construct valid confidence intervals for causal effects on disparities. We also propose a two-parameter reformulation that enhances interpretability and facilitates an intuitive understanding of the plausibility of unmeasured confounders and their effects. We illustrate our framework on a study examining the effect of parental support on disparities in suicidal ideation among sexual minority youth. We find that the effect is small and sensitive to unmeasured confounding, suggesting that further screening studies are needed to identify mitigating interventions in this vulnerable population.},
  archive      = {J_SIM},
  author       = {Andy A. Shen and Elina Visoki and Ran Barzilay and Samuel D. Pimentel},
  doi          = {10.1002/sim.70010},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70010},
  shortjournal = {Stat. Med.},
  title        = {A calibrated sensitivity analysis for weighted causal decompositions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse probability of treatment weighting using the
propensity score with competing risks in survival analysis.
<em>SIM</em>, <em>44</em>(5), e70009. (<a
href="https://doi.org/10.1002/sim.70009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability of treatment weighting (IPTW) using the propensity score allows estimation of the effect of treatment in observational studies. We had three objectives: first, to describe methods for using IPTW to estimate the effects of treatments in settings with competing risks; second, to illustrate the application of these methods using empirical analyses; and third, to conduct Monte Carlo simulations to evaluate the relative performance of three methods for estimating time-specific risk differences and time-specific relative risks in settings with competing risks. In doing so, we provide guidance to applied biostatisticians and clinical investigators on the use of IPTW in settings with competing risks. We examined three estimators of time-specific risk differences and relative risks: the weighted Aalen–Johansen estimator, an estimator that combines IPTW with inverse probability of censoring weights (IPTW-IPCWs), and a double-robust augmented IPTW estimator combined with IPCW (AIPTW-IPCW). The design of our simulations reflected clinically realistic scenarios. Our simulations found that all three estimators tended to result in unbiased estimations of time-specific risk differences and time-specific relative risks. However, the weighted Aalen–Johansen estimator and the AIPTW-IPCW estimator tended to result in estimates with greater precision compared to the IPTW-IPCW estimator. In our empirical analyses, we illustrated the application of these methods by estimating the effect of statin prescribing on the risk of subsequent cardiovascular death in patients discharged from the hospital with a diagnosis of acute myocardial infarction.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Jason P. Fine},
  doi          = {10.1002/sim.70009},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70009},
  shortjournal = {Stat. Med.},
  title        = {Inverse probability of treatment weighting using the propensity score with competing risks in survival analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proportional mean residual life model with varying
coefficients for right censored data. <em>SIM</em>, <em>44</em>(5),
e70008. (<a href="https://doi.org/10.1002/sim.70008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life provides the remaining life expectancy of a subject who has survived to a specific time point. This paper considers a proportional mean residual life model with varying coefficients, which allows one to explore the nonlinear interactions between some covariates and an exposure variable. In a semiparametric setting, we construct local estimating equations to obtain the varying coefficients and establish the asymptotic normality of the proposed estimators. Moreover, the weak convergence property for the local estimator of the baseline mean residual life function is developed. We conduct simulation studies to empirically examine the finite-sample performance of the proposed methods and apply the methodology to a real-life dataset on type 2 diabetic complications.},
  archive      = {J_SIM},
  author       = {Bing Wang and Xinyuan Song and Qian Zhao},
  doi          = {10.1002/sim.70008},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70008},
  shortjournal = {Stat. Med.},
  title        = {Proportional mean residual life model with varying coefficients for right censored data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of the average causal effects
under dietary substitution strategies. <em>SIM</em>, <em>44</em>(5),
e70007. (<a href="https://doi.org/10.1002/sim.70007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2020–2025 Dietary Guidelines suggest that most people can improve their diet by making some changes to what they eat and drink. In many cases, these changes involve simple substitutions. For instance, the Dietary Guidelines recommend choosing chicken instead of processed red meat to reduce sodium intake and switching from refined grains to whole grains to increase dietary fiber intake. The question about such dietary substitution strategies seeks to estimate the average counterfactual outcome under a hypothetical intervention that replaces a food an individual would have consumed in the absence of intervention with a healthier substitute. In this work, we will show the conditions under which the average causal effects of substitution strategies can be non-parametrically identified, and provide efficient estimators for our proposed dietary substitution strategies. We evaluate the performance of our proposed methods via simulation studies and apply them to estimate the effect of substituting processed red meat with chicken on mortality, using data from the Nurses&#39; Health Study.},
  archive      = {J_SIM},
  author       = {Yu-Han Chiu and Lan Wen},
  doi          = {10.1002/sim.70007},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70007},
  shortjournal = {Stat. Med.},
  title        = {Identification and estimation of the average causal effects under dietary substitution strategies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating average treatment effects with support vector
machines. <em>SIM</em>, <em>44</em>(5), e70006. (<a
href="https://doi.org/10.1002/sim.70006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is one of the most popular classification algorithms in the machine learning literature. We demonstrate that SVM can be used to balance covariates and estimate average causal effects under the unconfoundedness assumption. Specifically, we adapt the SVM classifier as a kernel-based weighting procedure that minimizes the maximum mean discrepancy between the treatment and control groups while simultaneously maximizing effective sample size. We also show that SVM is a continuous relaxation of the quadratic integer program for computing the largest balanced subset, establishing its direct relation to the cardinality matching method. Another important feature of SVM is that the regularization parameter controls the trade-off between covariate balance and effective sample size. As a result, the existing SVM path algorithm can be used to compute the balance-sample size frontier. We characterize the bias of causal effect estimation arising from this trade-off, connecting the proposed SVM procedure to the existing kernel balancing methods. Finally, we conduct simulation and empirical studies to evaluate the performance of the proposed methodology and find that SVM is competitive with the state-of-the-art covariate balancing methods.},
  archive      = {J_SIM},
  author       = {Alexander Tarr and Kosuke Imai},
  doi          = {10.1002/sim.70006},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70006},
  shortjournal = {Stat. Med.},
  title        = {Estimating average treatment effects with support vector machines},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized time-varying nonparametric model with an
application in mobile health. <em>SIM</em>, <em>44</em>(5), e70005. (<a
href="https://doi.org/10.1002/sim.70005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized modeling has become increasingly popular in recent years with its growing application in fields such as personalized medicine and mobile health studies. With rich longitudinal measurements, it is of great interest to model certain subject-specific time-varying covariate effects. In this paper, we propose an individualized time-varying nonparametric model by leveraging the subgroup information from the population. The proposed method approximates the time-varying covariate effect using nonparametric B-splines and aggregates the estimated nonparametric coefficients that share common patterns. Moreover, the proposed method can effectively handle various missing data patterns that frequently arise in mobile health data. Specifically, our method achieves subgrouping by flexibly accommodating varying dimensions of B-spline coefficients due to missingness. This capability sets it apart from other fusion-type approaches for subgrouping. The subgroup information can also potentially provide meaningful insight into the characteristics of subjects and assist in recommending an effective treatment or intervention. An efficient ADMM algorithm is developed for implementation. Our numerical studies and application to mobile health data on monitoring pregnant women&#39;s deep sleep and physical activities demonstrate that the proposed method achieves better performance compared to other existing methods.},
  archive      = {J_SIM},
  author       = {Jenifer Rim and Qi Xu and Xiwei Tang and Yuqing Guo and Annie Qu},
  doi          = {10.1002/sim.70005},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70005},
  shortjournal = {Stat. Med.},
  title        = {Individualized time-varying nonparametric model with an application in mobile health},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive use of co-data through empirical bayes for bayesian
additive regression trees. <em>SIM</em>, <em>44</em>(5), e70004. (<a
href="https://doi.org/10.1002/sim.70004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For clinical prediction applications, we are often faced with small sample size data compared to the number of covariates. Such data pose problems for variable selection and prediction, especially when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate external information on the covariates into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate external information, an empirical Bayes (EB) framework is developed that estimates, assisted by a model, prior covariate weights in the BART model. The proposed EB framework enables the estimation of the other prior parameters of BART as well, rendering an appealing and computationally efficient alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is non-linear, the method benefits from the flexibility of BART to outperform regression-based learners. Finally, the benefit of incorporating external information is shown in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.},
  archive      = {J_SIM},
  author       = {Jeroen M. Goedhart and Thomas Klausch and Jurriaan Janssen and Mark A. van de Wiel},
  doi          = {10.1002/sim.70004},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70004},
  shortjournal = {Stat. Med.},
  title        = {Adaptive use of co-data through empirical bayes for bayesian additive regression trees},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework to assess complex heterogeneity in the
strength of a surrogate marker. <em>SIM</em>, <em>44</em>(5), e70001.
(<a href="https://doi.org/10.1002/sim.70001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A surrogate marker is a biological measurement in a clinical trial that aims to replace the primary outcome in evaluating the treatment effect, and can be measured earlier, with less cost, or with less patient burden. In theory, once a surrogate is validated, future studies can evaluate treatment efficacy using only the surrogate. While there are many methods to evaluate a surrogate, these methods rarely account for heterogeneity in surrogacy, that is, when a surrogate is valid for only certain people. We propose a general framework for the assessment of complex heterogeneity in the strength of a surrogate marker, as well as corresponding parametric and semiparametric estimation procedures. Our framework defines the proportion of the treatment effect on the primary outcome that is explained by the treatment effect on the surrogate, as a function of multiple baseline covariates, W $$ \mathbf{W} $$ . We additionally propose a formal test of heterogeneity and a method to identify a region of the covariate space where the surrogate is sufficiently strong. We examine the performance of our methods via a simulation study featuring varying levels of heterogeneity and use our methods to examine potential heterogeneity in the strength of a surrogate in an AIDS clinical trial.},
  archive      = {J_SIM},
  author       = {Rebecca Knowlton and Lu Tian and Layla Parast},
  doi          = {10.1002/sim.70001},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70001},
  shortjournal = {Stat. Med.},
  title        = {A general framework to assess complex heterogeneity in the strength of a surrogate marker},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “DL 101: Basic introduction to deep learning
with its application in biomedical related fields.” <em>SIM</em>,
<em>44</em>(5), e10349. (<a
href="https://doi.org/10.1002/sim.10349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.10349},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10349},
  shortjournal = {Stat. Med.},
  title        = {Correction to “DL 101: Basic introduction to deep learning with its application in biomedical related fields”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying the estimands framework to non-inferiority trials:
Guidance on choice of hypothetical estimands for non-adherence and
comparison of estimation methods. <em>SIM</em>, <em>44</em>(5), e10348.
(<a href="https://doi.org/10.1002/sim.10348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common concern in non-inferiority (NI) trials is that non-adherence due, for example, to poor study conduct can make treatment arms artificially similar. Because intention-to-treat analyses can be anti-conservative in this situation, per-protocol analyses are sometimes recommended. However, such advice does not consider the estimands framework, nor the risk of bias from per-protocol analyses. We therefore sought to update the above guidance using the estimands framework, and compare estimators to improve on the performance of per-protocol analyses. We argue the main threat to validity of NI trials is the occurrence of “trial-specific” intercurrent events (IEs), that is, IEs which occur in a trial setting, but would not occur in practice. To guard against erroneous conclusions of non-inferiority, we suggest an estimand using a hypothetical strategy for trial-specific IEs should be employed, with handling of other non-trial-specific IEs chosen based on clinical considerations. We provide an overview of estimators that could be used to estimate a hypothetical estimand, including inverse probability weighting (IPW), and two instrumental variable approaches (one using an informative Bayesian prior on the effect of standard treatment, and one using a treatment-by-covariate interaction as an instrument). We compare them, using simulation in the setting of all-or-nothing compliance in two active treatment arms, and conclude both IPW and the instrumental variable method using a Bayesian prior are potentially useful approaches, with the choice between them depending on which assumptions are most plausible for a given trial.},
  archive      = {J_SIM},
  author       = {Katy E. Morgan and Ian R. White and Clémence Leyrat and Simon Stanworth and Brennan C. Kahan},
  doi          = {10.1002/sim.10348},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10348},
  shortjournal = {Stat. Med.},
  title        = {Applying the estimands framework to non-inferiority trials: Guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of cohort stepped wedge cluster-randomized trials
with nonignorable dropout via joint modeling. <em>SIM</em>,
<em>44</em>(5), e10347. (<a
href="https://doi.org/10.1002/sim.10347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster-randomized trial (CRTs) designs randomize clusters of individuals to intervention sequences, ensuring that every cluster eventually transitions from a control period to receive the intervention under study by the end of the study period. The analysis of stepped wedge CRTs is usually more complex than parallel-arm CRTs due to more complex intra-cluster correlation structures. A further challenge in the analysis of closed-cohort stepped wedge CRTs, which follow groups of individuals enrolled in each period longitudinally, is the occurrence of dropout. This is particularly problematic in studies of individuals at high risk for mortality, which causes nonignorable missing outcomes. If not appropriately addressed, missing outcomes from death will erode statistical power, at best, and bias treatment effect estimates, at worst. Joint longitudinal-survival models can accommodate informative dropout and missingness patterns in longitudinal studies. Specifically, within the joint longitudinal-survival modeling framework, one directly models the dropout process via a time-to-event submodel together with the longitudinal outcome of interest. The two submodels are then linked using a variety of possible association structures. This work extends linear mixed-effects models by jointly modeling the dropout process to accommodate informative missing outcome data in closed-cohort stepped wedge CRTs. We focus on constant intervention and general time-on-treatment effect parametrizations for the longitudinal submodel and study the performance of the proposed methodology using Monte Carlo simulation under several data-generating scenarios. We illustrate the joint modeling methodology in practice by reanalyzing data from the “Frail Older Adults: Care in Transition” (ACT) trial, a stepped wedge CRT of a multifaceted geriatric care model versus usual care in 35 primary care practices in the Netherlands.},
  archive      = {J_SIM},
  author       = {Alessandro Gasparini and Michael J. Crowther and Emiel O. Hoogendijk and Fan Li and Michael O. Harhay},
  doi          = {10.1002/sim.10347},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10347},
  shortjournal = {Stat. Med.},
  title        = {Analysis of cohort stepped wedge cluster-randomized trials with nonignorable dropout via joint modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random survival forest for censored functional data.
<em>SIM</em>, <em>44</em>(5), e10344. (<a
href="https://doi.org/10.1002/sim.10344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a Random Survival Forest (RSF) method for functional data. The focus is specifically on defining a new functional data structure, the Censored Functional Data (CFD), for addressing the challenge of accurately modelling time-to-event data in the presence of censoring and irregular temporal structures. Traditional survival models struggle to incorporate complex functional patterns, making the proposed approach particularly valuable for improving prediction and interpretation. This approach allows for precise modelling of functional survival trajectories, leading to improved interpretation and prediction of survival dynamics across different groups. A medical survival study on the benchmark Sequential Organ Failure Assessment (SOFA) dataset and an extensive simulation study are presented. Results show good performance of the proposed approach, particularly in ranking the importance of predicting variables.},
  archive      = {J_SIM},
  author       = {Giuseppe Loffredo and Elvira Romano and Fabrizio Maturo},
  doi          = {10.1002/sim.10344},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10344},
  shortjournal = {Stat. Med.},
  title        = {Random survival forest for censored functional data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for the functional form of a continuous covariate in
the shared-parameter joint model. <em>SIM</em>, <em>44</em>(5), e10340.
(<a href="https://doi.org/10.1002/sim.10340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared-parameter joint modeling is a useful technique for properly associating longitudinal and time-to-event data. When the interest is in the survival outcome, the conditional logarithm of the hazard function for an event is conventionally presumed to be linearly related over time to a set of explanatory covariates, among other terms. However, this hypothesis is quite restrictive and may yield misleading results. Our objective here is to easily check such a modeling assumption for any continuous fixed covariate. For this purpose, we examine the appropriateness of a nonparametric test criterion based on a penalty-modified version of the Akaike information criterion. An extensive numerical study is conducted to check the validity of the test within the joint modeling framework, while determining the extent to which the function embedding the continuous covariate deviates from linearity. Furthermore, once a deviation from linearity is detected, the improvement in the model&#39;s predictive performance is examined. The usefulness of the testing procedure is illustrated using a clinical trial with HIV-infected subjects. Specifically, our example focuses on properly accounting for the effect of nadir CD4 cell count within a predictive joint model for the time to immune recovery.},
  archive      = {J_SIM},
  author       = {Xavier Piulachs and Anouar El Ghouch and Ingrid Van Keilegom},
  doi          = {10.1002/sim.10340},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10340},
  shortjournal = {Stat. Med.},
  title        = {Testing for the functional form of a continuous covariate in the shared-parameter joint model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). So many choices: A guide to selecting among methods to
adjust for observed confounders. <em>SIM</em>, <em>44</em>(5), e10336.
(<a href="https://doi.org/10.1002/sim.10336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-randomised studies (NRS) typically assume that there are no differences in unobserved baseline characteristics between the treatment groups under comparison. Traditionally regression models have been deployed to estimate treatment effects adjusting for observed confounders but can lead to biased estimates if the model is missspecified, by making incorrect functional form assumptions. A multitude of alternative methods have been developed which can reduce the risk of bias due to model misspecification. Investigators can now choose between many forms of matching, weighting, doubly robust, and machine learning methods. We review key concepts related to functional form assumptions and how those can contribute to bias from model misspecification. We then categorize the three frameworks for modeling treatment effects and the wide variety of estimation methods that can be applied to each framework. We consider why machine learning methods have been widely proposed for estimation and review the strengths and weaknesses of these approaches. We apply a range of these methods in re-analyzing a landmark case study. In the application, we examine how several widely used methods may be subject to bias from model misspecification. We conclude with a set of recommendations for practice.},
  archive      = {J_SIM},
  author       = {Luke Keele and Richard Grieve},
  doi          = {10.1002/sim.10336},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10336},
  shortjournal = {Stat. Med.},
  title        = {So many choices: A guide to selecting among methods to adjust for observed confounders},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScRecover: Discriminating true and false zeros in
single-cell RNA-seq data for imputation. <em>SIM</em>, <em>44</em>(5),
e10334. (<a href="https://doi.org/10.1002/sim.10334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput single-cell RNA-seq (scRNA-seq) data contains an excess of zero values, which can be contributed by unexpressed genes and detection signal dropouts. Existing imputation methods fail to distinguish between these two types of zeros. In this study, we introduce a statistical framework that effectively differentiates true zeros (lack of expression) from false zeros (dropouts). By focusing only on imputing the dropout zeros, we developed a new imputation tool, scRecover. Our approach utilizes a zero-inflated negative binomial framework to model the gene expression of each gene in each cell, enabling the estimation of zero-dropout probability. Additionally, we employ a modified version of the Good and Toulmin model to identify true zeros for each gene. To achieve imputation, scRecover is combined with other imputation methods such as scImpute, SAVER and MAGIC. Down-sampling experiments show that it recovers dropout zeros with higher accuracy and avoids over-imputing true zero values. Experiments conducted on real world data highlight the ability of scRecover to enhance downstream analysis and visualization.},
  archive      = {J_SIM},
  author       = {Zhun Miao and Xinyi Lin and Jiaqi Li and Joshua Ho and Qiuchen Meng and Xuegong Zhang},
  doi          = {10.1002/sim.10334},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10334},
  shortjournal = {Stat. Med.},
  title        = {ScRecover: Discriminating true and false zeros in single-cell RNA-seq data for imputation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal control of directional false discovery rates in
large-scale testing. <em>SIM</em>, <em>44</em>(5), e10329. (<a
href="https://doi.org/10.1002/sim.10329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-throughput biomedical technology enables measurement of thousands of gene expression levels contemporaneously. A major task in analyzing these gene expression data is to identify both over-expressed and under-expressed genes. The popular two-group models select the non-null genes without further classifying them as overexpression or underexpression. Consequently, two-group decision rules are unable to constrain the numbers of falsely discovered over-expressed or under-expressed genes respectively. We propose a general three-group model that allows dependence between the test statistics and develop a decision rule that separately controls the two types of false discoveries. We show that the optimal decision rule in our three-group model has a special monotonic structure. By making use of this monotonic structure, we can linearize the two-directional false discovery rate constraints. We prove that our decision rule optimizes the expected number of true discoveries while controlling the proportions of falsely discovered over-expressed and under-expressed genes at desired levels simultaneously. The data-driven versions of the proposed procedures are suggested, and their consistency is established. Comparisons with state-of-the-art approaches and applications to genomic studies show that our procedures work well.},
  archive      = {J_SIM},
  author       = {Guozhu Tang and Yicheng Kang and Dongdong Xiang},
  doi          = {10.1002/sim.10329},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10329},
  shortjournal = {Stat. Med.},
  title        = {Optimal control of directional false discovery rates in large-scale testing},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-dependent ROC curve for multiple longitudinal
biomarkers and its application in diagnosing cardiovascular events.
<em>SIM</em>, <em>44</em>(5), e10318. (<a
href="https://doi.org/10.1002/sim.10318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since they can help people detect the early signs of diseases, accurate diagnostic techniques based on biomarkers are crucial in biomedical research. This article proposes a novel bivariate time-varying coefficients logistic regression model for addressing the combined longitudinal biomarkers. Using the B-splines method to estimate the proposed model, we can effectively combine multiple longitudinal biomarkers and improve diagnostic accuracy. We show that the proposed method is theoretically consistent. And it exhibits superior performance compared to the existing method, as presented through numerical results. The proposed method is verified in a study on predicting the probability of onset of future cardiovascular events for type 2 diabetic patients. The longitudinal biomarkers, HbA1c and LDL-C, are considered in this study. We demonstrate that the combined longitudinal biomarkers significantly improved disease diagnostic accuracy over only a combination of the latest measured biomarkers in most cases.},
  archive      = {J_SIM},
  author       = {Lizhe Sun and Pingyuan Wei and Jie Zhou and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10318},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10318},
  shortjournal = {Stat. Med.},
  title        = {Time-dependent ROC curve for multiple longitudinal biomarkers and its application in diagnosing cardiovascular events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal mediation analysis: A summary-data mendelian
randomization approach. <em>SIM</em>, <em>44</em>(5), e10317. (<a
href="https://doi.org/10.1002/sim.10317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summary-data Mendelian randomization (MR), a widely used approach in causal inference, has recently attracted attention for improving causal mediation analysis. Two existing methods corresponding to the difference method and product method of linear mediation analysis have been developed to perform MR-based mediation analysis using the inverse-variance weighted method (MR-IVW). Despite these developments, there is still a need for more rigorous, efficient, and precise MR-based mediation methodologies. In this study, we develop summary-data MR-based frameworks for causal mediation analysis. We improve the accuracy, statistical efficiency and robustness of the existing MR-based mediation analysis by implementing novel variance estimators for the mediation effects, deriving rigorous procedures for statistical inference, and accounting for widespread pleiotropic effects. Specifically, we propose Diff-IVW and Prod-IVW to improve upon the existing methods and provide the pleiotropy-robust methods (Diff-Egger, Diff-Median, Prod-Egger, and Prod-Median), adapted from MR-Egger and MR-Median, to enhance the robustness of the MR-based mediation analysis. We conduct comprehensive simulation studies to compare the existing and proposed methods. The results show that the proposed methods, Diff-IVW and Prod-IVW, improve statistical efficiency and type I error control over the existing approaches. Although all IVW-based methods suffer from directional pleiotropy biases, the median-based methods (Diff-Median and Prod-Median) can mitigate such biases. The differences among the methods can lead to discrepant statistical conclusions as demonstrated in real data applications. Based on our simulation results, we recommend the three proposed methods in practice: Diff-IVW, Prod-IVW, and Prod-Median, which are complementary under various scenarios.},
  archive      = {J_SIM},
  author       = {Shu-Chin Lin and Sheng-Hsuan Lin and Tian Ge and Chia-Yen Chen and Yen-Feng Lin},
  doi          = {10.1002/sim.10317},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10317},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis: A summary-data mendelian randomization approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for association studies in the
presence of binary outcome misclassification. <em>SIM</em>,
<em>44</em>(5), e10316. (<a
href="https://doi.org/10.1002/sim.10316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical and public health association studies, binary outcome variables may be subject to misclassification, resulting in substantial bias in effect estimates. The feasibility of addressing binary outcome misclassification in regression models is often hindered by model identifiability issues. In this paper, we characterize the identifiability problems in this class of models as a specific case of “label-switching” and leverage a pattern in the resulting parameter estimates to solve the permutation invariance of the complete data log-likelihood. Our proposed algorithm in binary outcome misclassification models does not require gold standard labels and relies only on the assumption that the sum of the sensitivity and specificity exceeds 1. A label-switching correction is applied within estimation methods to recover unbiased effect estimates and to estimate misclassification rates. Open-source software is provided to implement the proposed methods. We give a detailed simulation study for our proposed methodology and apply these methods to data from the 2020 Medical Expenditure Panel Survey (MEPS).},
  archive      = {J_SIM},
  author       = {Kimberly A. Hochstedler Webb and Martin T. Wells},
  doi          = {10.1002/sim.10316},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10316},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for association studies in the presence of binary outcome misclassification},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dir-GLM: A bayesian GLM with data-driven reference
distribution. <em>SIM</em>, <em>44</em>(5), e10305. (<a
href="https://doi.org/10.1002/sim.10305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed semi-parametric generalized linear model (SPGLM) offers more flexibility as compared to the classical GLM by including the baseline or reference distribution of the response as an additional parameter in the model. However, some inference summaries are not easily generated under existing maximum-likelihood-based inference (GLDRM). This includes uncertainty in estimation for model-derived functionals such as exceedance probabilities. The latter are critical in a clinical diagnostic or decision-making setting. In this article, by placing a Dirichlet prior on the baseline distribution, we propose a Bayesian model-based approach for inference to address these important gaps. We establish consistency and asymptotic normality results for the implied canonical parameter. Simulation studies and an illustration with data from an aging research study confirm that the proposed method performs comparably or better in comparison with GLDRM. The proposed Bayesian framework is most attractive for inference with small sample training data or in sparse-data scenarios.},
  archive      = {J_SIM},
  author       = {Entejar Alam and Peter Müller and Paul J. Rathouz},
  doi          = {10.1002/sim.10305},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10305},
  shortjournal = {Stat. Med.},
  title        = {Dir-GLM: A bayesian GLM with data-driven reference distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian hierarchical penalized spline models for immediate
and time-varying intervention effects in stepped wedge cluster
randomized trials. <em>SIM</em>, <em>44</em>(5), e10304. (<a
href="https://doi.org/10.1002/sim.10304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SWCRTs) often face challenges related to potential confounding by time. Traditional frequentist methods may not provide adequate coverage of an intervention&#39;s true effect using confidence intervals, whereas Bayesian approaches show potential for better coverage of intervention effects. However, Bayesian methods remain underexplored in the context of SWCRTs. To bridge this gap, we propose two innovative Bayesian hierarchical penalized spline models. Our first model accommodates large numbers of clusters and time periods, focusing on immediate intervention effects. To evaluate this approach, we compared this model to traditional frequentist methods. We then extend our approach to account for time-varying intervention effects, conducting a comprehensive comparison with an existing Bayesian monotone effect curve model and alternative frequentist methods. The proposed models were applied in the Primary Palliative Care for Emergency Medicine stepped wedge trial to evaluate the effectiveness of the intervention. Through extensive simulations and real-world application, we demonstrate the robustness of our proposed Bayesian models. Notably, the Bayesian immediate effect model consistently achieves the nominal coverage probability, providing more reliable interval estimations while maintaining high estimation accuracy. Furthermore, our proposed Bayesian time-varying effect model represents a significant advancement over the existing Bayesian monotone effect curve model, offering improved accuracy and reliability in estimation while also achieving higher coverage probability than alternative frequentist methods. To the best of our knowledge, this marks the first development of Bayesian hierarchical spline modeling for SWCRTs. Our proposed models offer promising tools for researchers and practitioners, enabling more precise evaluation of intervention impacts.},
  archive      = {J_SIM},
  author       = {Danni Wu and Hyung G. Park and Corita R. Grudzen and Keith S. Goldfeld},
  doi          = {10.1002/sim.10304},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10304},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical penalized spline models for immediate and time-varying intervention effects in stepped wedge cluster randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
