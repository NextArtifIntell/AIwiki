<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietcv---31">IETCV - 31</h2>
<ul>
<li><details>
<summary>
(2025). Temporal optimisation of satellite image-based crop mapping:
A comparison of deep time series and semi-supervised time warping
strategies. <em>IETCV</em>, <em>19</em>(1), e70014. (<a
href="https://doi.org/10.1049/cvi2.70014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel approach to crop mapping using remotely sensed satellite images. It addresses the significant classification modelling challenges, including (1) the requirements for extensive labelled data and (2) the complex optimisation problem for selection of appropriate temporal windows in the absence of prior knowledge of cultivation calendars. We compare the lightweight Dynamic Time Warping (DTW) classification method with the heavily supervised Convolutional Neural Network - Long Short-Term Memory (CNN-LSTM) using high-resolution multispectral optical satellite imagery (3 m/pixel). Our approach integrates effective practical preprocessing steps, including data augmentation and a data-driven optimisation strategy for the temporal window, even in the presence of numerous crop classes. Our findings demonstrate that DTW, despite its lower data demands, can match the performance of CNN-LSTM through our effective preprocessing steps while significantly improving runtime. These results demonstrate that both CNN-LSTM and DTW can achieve deployment-level accuracy and underscore the potential of DTW as a viable alternative to more resource-intensive models. The results also prove the effectiveness of temporal windowing for improving runtime and accuracy of a crop classification study, even with no prior knowledge of planting timeframes.},
  archive      = {J_IETCV},
  author       = {Rosie Finnegan and Joseph Metcalfe and Sara Sharifzadeh and Fabio Caraffini and Xianghua Xie and Alberto Hornero and Nicholas W. Synes},
  doi          = {10.1049/cvi2.70014},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70014},
  shortjournal = {IET Comput. Vis.},
  title        = {Temporal optimisation of satellite image-based crop mapping: A comparison of deep time series and semi-supervised time warping strategies},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances of continual learning in computer vision: An
overview. <em>IETCV</em>, <em>19</em>(1), e70013. (<a
href="https://doi.org/10.1049/cvi2.70013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to batch learning where all training data is available at once, continual learning represents a family of methods that accumulate knowledge and learn continuously with data available in sequential order. Similar to the human learning process with the ability of learning, fusing and accumulating new knowledge acquired at different time steps, continual learning is considered to have high practical significance. Hence, continual learning has been studied in various artificial intelligence tasks. In this paper, we present a comprehensive review of the recent progress of continual learning in computer vision. In particular, the works are grouped by their representative techniques, including regularisation, knowledge distillation, memory, generative replay, parameter isolation and a combination of the above techniques. For each category of these techniques, both its characteristics and applications in computer vision are presented. At the end of this overview, several subareas, where continuous knowledge accumulation is potentially helpful while continual learning has not been well studied, are discussed.},
  archive      = {J_IETCV},
  author       = {Haoxuan Qu and Hossein Rahmani and Li Xu and Bryan Williams and Jun Liu},
  doi          = {10.1049/cvi2.70013},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70013},
  shortjournal = {IET Comput. Vis.},
  title        = {Recent advances of continual learning in computer vision: An overview},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAPCNet: Tactile-assisted point cloud completion network via
iterative fusion strategy. <em>IETCV</em>, <em>19</em>(1), e70012. (<a
href="https://doi.org/10.1049/cvi2.70012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the 3D point cloud field in recent years, point cloud completion of 3D objects has increasingly attracted researchers&#39; attention. Point cloud data can accurately express the shape information of 3D objects at different resolutions, but the original point clouds collected directly by various 3D scanning equipment are often incomplete and have uneven density. Tactile is one distinctive way to perceive the 3D shape of an object. Tactile point clouds can provide local shape information for unknown areas during completion, which is a valuable complement to the point cloud data acquired with visual devices. In order to effectively improve the effect of point cloud completion using tactile information, the authors propose an innovative tactile-assisted point cloud completion network, TAPCNet. This network is the first neural network customised for the input of tactile point clouds and incomplete point clouds, which can fuse two types of point cloud information in the feature domain. Besides, a new dataset named 3DVT was rebuilt, to fit the proposed network model. Based on the tactile fusion strategy and related modules, multiple comparative experiments were conducted by controlling the quantity of tactile point clouds on the 3DVT dataset. The experimental data illustrates that TAPCNet can outperform the state-of-the-art methods in the benchmark.},
  archive      = {J_IETCV},
  author       = {Yangrong Liu and Jian Li and Huaiyu Wang and Ming Lu and Haorao Shen and Qin Wang},
  doi          = {10.1049/cvi2.70012},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70012},
  shortjournal = {IET Comput. Vis.},
  title        = {TAPCNet: Tactile-assisted point cloud completion network via iterative fusion strategy},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crafting transferable adversarial examples against 3D object
detection. <em>IETCV</em>, <em>19</em>(1), e70011. (<a
href="https://doi.org/10.1049/cvi2.70011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection is one of the current popular hotspots by perceiving the surrounding environment through LiDAR and camera sensors to recognise the category and location of objects in the scene. Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples. Although some approaches have begun to investigate the robustness of 3D object detection models, they are currently generating adversarial examples in a white-box setting and there is a lack of research into generating transferable adversarial examples in a black-box setting. In this paper, a non-end-to-end attack algorithm was proposed for LiDAR pipelines that crafts transferable adversarial examples against 3D object detection. Specifically, the method generates adversarial examples by restraining features with high contribution to downstream tasks and amplifying features with low contribution to downstream tasks in the feature space. Extensive experiments validate that the method produces more transferable adversarial point clouds, for example, the method generates adversarial point clouds in the nuScenes dataset that are about 10 and 7 better than the state-of-the-art method on mAP and NDS, respectively.},
  archive      = {J_IETCV},
  author       = {Haiyan Long and Hai Chen and Mengyao Xu and Chonghao Zhang and Fulan Qian},
  doi          = {10.1049/cvi2.70011},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70011},
  shortjournal = {IET Comput. Vis.},
  title        = {Crafting transferable adversarial examples against 3D object detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of multi-object tracking in recent times.
<em>IETCV</em>, <em>19</em>(1), e70010. (<a
href="https://doi.org/10.1049/cvi2.70010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is a fundamental problem in computer vision that involves tracing the trajectories of foreground targets throughout a video sequence while establishing correspondences for identical objects across frames. With the advancement of deep learning techniques, methods based on deep learning have significantly improved accuracy and efficiency in MOT. This paper reviews several recent deep learning-based MOT methods and categorises them into three main groups: detection-based, single-object tracking (SOT)-based, and segmentation-based methods, according to their core technologies. Additionally, this paper discusses the metrics and datasets used for evaluating MOT performance, the challenges faced in the field, and future directions for research.},
  archive      = {J_IETCV},
  author       = {Suya Li and Hengyi Ren and Xin Xie and Ying Cao},
  doi          = {10.1049/cvi2.70010},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70010},
  shortjournal = {IET Comput. Vis.},
  title        = {A review of multi-object tracking in recent times},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating transferable adversarial point clouds via
autoencoders for 3D object classification. <em>IETCV</em>,
<em>19</em>(1), e70008. (<a
href="https://doi.org/10.1049/cvi2.70008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that deep neural networks are vulnerable to adversarial attacks. In the field of 3D point cloud classification, transfer-based black-box attack strategies have been explored to address the challenge of limited knowledge about the model in practical scenarios. However, existing approaches typically rely excessively on network structure, resulting in poor transferability of the generated adversarial examples. To address the above problem, the authors propose AEattack , an adversarial attack method capable of generating highly transferable adversarial examples. Specifically, AEattack employs an autoencoder (AE) to extract features from the point cloud data and reconstruct the adversarial point cloud based on these features. Notably, the AE does not require pre-training, and its parameters are jointly optimised using a loss function during the process of generating adversarial point clouds. The method makes the generated adversarial point cloud not overly dependent on the network structure, but more concerned with the data distribution. Moreover, this design endows AEattack with a broader potential for application. Extensive experiments on the ModelNet40 dataset show that AEattack is capable of generating highly transferable adversarial point clouds, with up to 61.8% improvement in transferability compared to state-of-the-art adversarial attacks.},
  archive      = {J_IETCV},
  author       = {Mengyao Xu and Hai Chen and Chonghao Zhang and Yuanjun Zou and Chenchu Xu and Yanping Zhang and Fulan Qian},
  doi          = {10.1049/cvi2.70008},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70008},
  shortjournal = {IET Comput. Vis.},
  title        = {Generating transferable adversarial point clouds via autoencoders for 3D object classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new large-scale dataset for marine vessel
re-identification based on swin transformer network in ocean
surveillance scenario. <em>IETCV</em>, <em>19</em>(1), e70007. (<a
href="https://doi.org/10.1049/cvi2.70007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an upward trend that marine vessels, an important object category in marine monitoring, have gradually become a research focal point in the field of computer vision, such as detection, tracking, and classification. Among them, marine vessel re-identification (Re-ID) emerges as a significant frontier research topics, which not only faces the dual challenge of huge intra-class and small inter-class differences, but also has complex environmental interference in the port monitoring scenarios. To propel advancements in marine vessel Re-ID technology, SwinTransReID, a framework grounded in the Swin Transformer for marine vessel Re-ID, is introduced. Specifically, the project initially encodes the triplet images separately as a sequence of blocks and construct a baseline model leveraging the Swin Transformer, achieving better performance on the Re-ID benchmark dataset in comparison to convolution neural network (CNN)-based approaches. And it introduces side information embedding (SIE) to further enhance the robust feature-learning capabilities of Swin Transformer, thus, integrating non-visual cues (orientation and type of vessel) and other auxiliary information (hull colour) through the insertion of learnable embedding modules. Additionally, the project presents VesselReID-1656, the first annotated large-scale benchmark dataset for vessel Re-ID in real-world ocean surveillance, comprising 135,866 images of 1656 vessels along with 5 orientations, 12 types, and 17 colours. The proposed method achieves 87.1 mAP and 96.1 Rank-1 accuracy on the newly-labelled challenging dataset, which surpasses the state-of-the-art (SOTA) method by 1.9 mAP regarding to performance. Moreover, extensive empirical results demonstrate the superiority of the proposed SwinTransReID on the person Market-1501 dataset, vehicle VeRi-776 dataset, and Boat Re-ID vessel dataset.},
  archive      = {J_IETCV},
  author       = {Zhi Lu and Liguo Sun and Pin Lv and Jiuwu Hao and Bo Tang and Xuanzhen Chen},
  doi          = {10.1049/cvi2.70007},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70007},
  shortjournal = {IET Comput. Vis.},
  title        = {A new large-scale dataset for marine vessel re-identification based on swin transformer network in ocean surveillance scenario},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-level compensation and alignment for
visible-infrared person re-identification. <em>IETCV</em>,
<em>19</em>(1), e70005. (<a
href="https://doi.org/10.1049/cvi2.70005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) aims to match pedestrian images captured by nonoverlapping visible and infrared cameras. Most existing compensation-based methods try to generate images of missing modality from the other ones. However, the generated images often fail to possess enough quality due to severe discrepancies between different modalities. Moreover, it is generally assumed that person images are roughly aligned during the extraction of part-based local features. However, this does not always hold true, typically when they are cropped via inaccurate pedestrian detectors. To alleviate such problems, the authors propose a novel feature-level compensation and alignment network (FCA-Net) for VI-ReID in this paper, which tries to compensate for the missing modality information on the channel-level and align part-based local features. Specifically, the visible and infrared features of low-level subnetworks are first processed by a channel feature compensation (CFC) module, which enforces the network to learn consistent distribution patterns of channel features, and thereby the cross-modality discrepancy is narrowed. To address spatial misalignment, a pairwise relation module (PRM) is introduced to incorporate human structural information into part-based local features, which can significantly enhance the feature discrimination power. Besides, a cross-modality part alignment loss (CPAL) is designed on the basis of a dynamic part matching algorithm, which can promote more accurate local matching. Extensive experiments on three standard VI-ReID datasets are conducted to validate the effectiveness of the proposed method, and the results show that state-of-the-art performance is achieved.},
  archive      = {J_IETCV},
  author       = {Husheng Dong and Ping Lu and Yuanfeng Yang and Xun Sun},
  doi          = {10.1049/cvi2.70005},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70005},
  shortjournal = {IET Comput. Vis.},
  title        = {Feature-level compensation and alignment for visible-infrared person re-identification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in smart agriculture: A systematic literature
review on state-of-the-art plant disease detection with computer vision.
<em>IETCV</em>, <em>19</em>(1), e70004. (<a
href="https://doi.org/10.1049/cvi2.70004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era of rapid digital transformation, ensuring sustainable and traceable food production is more crucial than ever. Plant diseases, a major threat to agriculture, lead to significant losses in crops and financial damage. Standard techniques for detecting diseases, though widespread, are lengthy and intensive work, especially in extensive agricultural settings. This systematic literature review examines the cutting-edge technologies in smart agriculture specifically computer vision, robotics, deep learning (DL), and Internet of Things (IoT) that are reshaping plant disease detection and management. By analysing 198 studies published between 2021 and 2023, from an initial pool of 19,838 papers, the authors reveal the dominance of DL, particularly with datasets such as PlantVillage, and highlight critical challenges, including dataset limitations, lack of geographical diversity, and the scarcity of real-world field data. Moreover, the authors explore the promising role of IoT, robotics, and drones in enhancing early disease detection, although the high costs and technological gaps present significant barriers for small-scale farmers, especially in developing countries. Through the preferred reporting items for systematic reviews and meta-analyses methodology, this review synthesises these findings, identifying key trends, uncovering research gaps, and offering actionable insights for the future of plant disease management in smart agriculture.},
  archive      = {J_IETCV},
  author       = {Esra Yilmaz and Sevim Ceylan Bocekci and Cengiz Safak and Kazim Yildiz},
  doi          = {10.1049/cvi2.70004},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70004},
  shortjournal = {IET Comput. Vis.},
  title        = {Advancements in smart agriculture: A systematic literature review on state-of-the-art plant disease detection with computer vision},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human activity recognition: A review of deep learning-based
methods. <em>IETCV</em>, <em>19</em>(1), e70003. (<a
href="https://doi.org/10.1049/cvi2.70003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) covers methods for automatically identifying human activities from a stream of data. End-users of HAR methods cover a range of sectors, including health, self-care, amusement, safety and monitoring. In this survey, the authors provide a thorough overview of deep learning based and detailed analysis of work that was performed between 2018 and 2023 in a variety of fields related to HAR with a focus on device-free solutions. It also presents the categorisation and taxonomy of the covered publication and an overview of publicly available datasets. To complete this review, the limitations of existing approaches and potential future research directions are discussed.},
  archive      = {J_IETCV},
  author       = {Sanjay Jyoti Dutta and Tossapon Boongoen and Reyer Zwiggelaar},
  doi          = {10.1049/cvi2.70003},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70003},
  shortjournal = {IET Comput. Vis.},
  title        = {Human activity recognition: A review of deep learning-based methods},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling semantics of diffusion-augmented data for
unsupervised domain adaptation. <em>IETCV</em>, <em>19</em>(1), e70002.
(<a href="https://doi.org/10.1049/cvi2.70002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) offers a compelling solution to bridge the gap between labelled synthetic data and unlabelled real-world data for training semantic segmentation models, given the high costs associated with manual annotation. However, the visual differences between the synthetic and real images pose significant challenges to their practical applications. This work addresses these challenges through synthetic-to-real style transfer leveraging diffusion models. The authors’ proposal incorporates semantic controllers to guide the diffusion process and low-rank adaptations (LoRAs) to ensure that style-transferred images align with real-world aesthetics while preserving semantic layout. Moreover, the authors introduce quality metrics to rank the utility of generated images, enabling the selective use of high-quality images for training. To further enhance reliability, the authors propose a novel loss function that mitigates artefacts from the style transfer process by incorporating only pixels aligned with the original semantic labels. Experimental results demonstrate that the authors’ proposal outperforms selected state-of-the-art methods for image generation and UDA training, achieving optimal performance even with a smaller set of high-quality generated images. The authors’ code and models are available at http://www-vpu.eps.uam.es/ControllingSem4UDA/ .},
  archive      = {J_IETCV},
  author       = {Henrietta Ridley and Roberto Alcover-Couso and Juan C. SanMiguel},
  doi          = {10.1049/cvi2.70002},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70002},
  shortjournal = {IET Comput. Vis.},
  title        = {Controlling semantics of diffusion-augmented data for unsupervised domain adaptation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TomoSAR 3D reconstruction: Cascading adversarial strategy
with sparse observation trajectory. <em>IETCV</em>, <em>19</em>(1),
e70001. (<a href="https://doi.org/10.1049/cvi2.70001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar tomography (TomoSAR) has shown significant potential for the 3D Reconstruction of buildings, especially in critical areas such as topographic mapping, urban planning, and disaster monitoring. In practical applications, the constraints of observation trajectories frequently lead to the acquisition of a limited dataset of sparse SAR images, presenting challenges for TomoSAR 3D Reconstruction and affecting its signal-to-noise ratio and elevation resolution performance. The study introduces a cascade adversarial strategy based on the Conditional Generative Adversarial Network (CGAN), optimised explicitly for sparse observation trajectories. In the preliminary phase of the CGAN, the U-Net architecture was employed to capture more global information and enhance image detail recovery capability, which is subsequently utilised in the cascade refinement network. The ResNet34 residual network in the advanced network stage was adopted to bolster feature extraction and image generation capabilities further. Based on experimental validation performed on the curated TomoSAR 3D super-resolution dataset tailored for buildings, the findings reveal that the methodology yields a notable enhancement in image quality and accuracy compared to other techniques.},
  archive      = {J_IETCV},
  author       = {Xian Zhu and Xiaoqin Zeng and Yuhua Cong and Yanhao Huang and Ziyan Zhu and Yantao Luo},
  doi          = {10.1049/cvi2.70001},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70001},
  shortjournal = {IET Comput. Vis.},
  title        = {TomoSAR 3D reconstruction: Cascading adversarial strategy with sparse observation trajectory},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A principal direction-guided local voxelisation structural
feature approach for point cloud registration. <em>IETCV</em>,
<em>19</em>(1), e70000. (<a
href="https://doi.org/10.1049/cvi2.70000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a crucial aspect of computer vision and 3D reconstruction. Traditional registration methods often depend on global features or iterative optimisation, leading to inefficiencies and imprecise outcomes when processing complex scene point cloud data. To address these challenges, the authors introduce a principal direction-guided local voxelisation structural feature (PDLVSF) approach for point cloud registration. This method reliably identifies feature points regardless of initial positioning. Approach begins with the 3D Harris algorithm to extract feature points, followed by determining the principal direction within the feature points&#39; radius neighbourhood to ensure rotational invariance. For scale invariance, voxel grid normalisation is utilised to maximise the point cloud&#39;s geometric resolution and make it scale-independent. Cosine similarity is then employed for effective feature matching, identifying corresponding feature point pairs and determining transformation parameters between point clouds. Experimental validations on various datasets, including the real terrain dataset, demonstrate the effectiveness of our method. Results indicate superior performance in root mean square error (RMSE) and registration accuracy compared to state-of-the-art methods, particularly in scenarios with high noise, limited overlap, and significant initial pose rotation. The real terrain dataset is publicly available at https://github.com/black-2000/Real-terrain-data .},
  archive      = {J_IETCV},
  author       = {Chenyang Li and Yansong Duan},
  doi          = {10.1049/cvi2.70000},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e70000},
  shortjournal = {IET Comput. Vis.},
  title        = {A principal direction-guided local voxelisation structural feature approach for point cloud registration},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Egocentric action anticipation from untrimmed videos.
<em>IETCV</em>, <em>19</em>(1), e12342. (<a
href="https://doi.org/10.1049/cvi2.12342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric action anticipation involves predicting future actions performed by the camera wearer from egocentric video. Although the task has recently gained attention in the research community, current approaches often assume that input videos are ‘trimmed’, meaning that a short video sequence is sampled a fixed time before the beginning of the action. However, trimmed action anticipation has limited applicability in real-world scenarios, where it is crucial to deal with ‘untrimmed’ video inputs and the exact moment of action initiation cannot be assumed at test time. To address these limitations, an untrimmed action anticipation task is proposed, which, akin to temporal action detection, assumes that the input video is untrimmed at test time, while still requiring predictions to be made before actions take place. The authors introduce a benchmark evaluation procedure for methods designed to address this novel task and compare several baselines on the EPIC-KITCHENS-100 dataset. Through our experimental evaluation, testing a variety of models, the authors aim to better understand their performance in untrimmed action anticipation. Our results reveal that the performance of current models designed for trimmed action anticipation is limited, emphasising the need for further research in this area.},
  archive      = {J_IETCV},
  author       = {Ivan Rodin and Antonino Furnari and Giovanni Maria Farinella},
  doi          = {10.1049/cvi2.12342},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12342},
  shortjournal = {IET Comput. Vis.},
  title        = {Egocentric action anticipation from untrimmed videos},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NBCDC-YOLOv8: A new framework to improve blood cell
detection and classification based on YOLOv8. <em>IETCV</em>,
<em>19</em>(1), e12341. (<a
href="https://doi.org/10.1049/cvi2.12341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, computer technology has successfully permeated all areas of medicine and its management, and it now offers doctors an accurate and rapid means of diagnosis. Existing blood cell detection methods suffer from low accuracy, which is caused by the uneven distribution, high density, and mutual occlusion of different blood cell types in blood microscope images, this article introduces NBCDC-YOLOv8: a new framework to improve blood cell detection and classification based on YOLOv8. Our framework innovates on several fronts: it uses Mosaic data augmentation to enrich the dataset and add small targets, incorporates a space to depth convolution (SPD-Conv) tailored for cells that are small and have low resolution, and introduces the Multi-Separated and Enhancement Attention Module (MultiSEAM) to enhance feature map resolution. Additionally, it integrates a bidirectional feature pyramid network (BiFPN) for effective multi-scale feature fusion and includes four detection heads to improve recognition accuracy of various cell sizes, especially small target platelets. Evaluated on the Blood Cell Classification Dataset (BCCD), NBCDC-YOLOv8 obtains a mean average precision (mAP) of 94.7%, and thus surpasses the original YOLOv8n by 2.3%.},
  archive      = {J_IETCV},
  author       = {Xuan Chen and Linxuan Li and Xiaoyu Liu and Fengjuan Yin and Xue Liu and Xiaoxiao Zhu and Yufeng Wang and Fanbin Meng},
  doi          = {10.1049/cvi2.12341},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12341},
  shortjournal = {IET Comput. Vis.},
  title        = {NBCDC-YOLOv8: A new framework to improve blood cell detection and classification based on YOLOv8},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust few-shot classifier with image as set of points.
<em>IETCV</em>, <em>19</em>(1), e12340. (<a
href="https://doi.org/10.1049/cvi2.12340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many few-shot classification methods have been proposed. However, only a few of them have explored robust classification, which is an important aspect of human visual intelligence. Humans can effortlessly recognise visual patterns, including lines, circles, and even characters, from image data that has been corrupted or degraded. In this paper, the authors investigate a robust classification method that extends the classical paradigm of robust geometric model fitting. The method views an image as a set of points in a low-dimensional space and analyses each image through low-dimensional geometric model fitting. In contrast, the majority of other methods, such as deep learning methods, treat an image as a single point in a high-dimensional space. The authors evaluate the performance of the method using a noisy Omniglot dataset. The experimental results demonstrate that the proposed method is significantly more robust than other methods. The source code and data for this paper are available at https://github.com/pengsuhua/PMF_OMNIGLOT .},
  archive      = {J_IETCV},
  author       = {Suhua Peng and Zongliang Zhang and Xingwang Huang and Zongyue Wang and Shubing Su and Guorong Cai},
  doi          = {10.1049/cvi2.12340},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12340},
  shortjournal = {IET Comput. Vis.},
  title        = {A robust few-shot classifier with image as set of points},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMGNFORMER: Fusion mamba-graph transformer network for human
pose estimation. <em>IETCV</em>, <em>19</em>(1), e12339. (<a
href="https://doi.org/10.1049/cvi2.12339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of 3D human pose estimation (HPE), many deep learning algorithms overlook the topological relationships between 2D keypoints, resulting in imprecise regression of 3D coordinates and a notable decline in estimation performance. To address this limitation, this paper proposes a novel approach to 3D HPE, termed the Spatial Mamba Graph Convolutional Neural Network (GCN) Former (SMGNFormer). The proposed method utilises the Mamba architecture to extract spatial information from 2D keypoints and integrates GCNs with multi-head attention mechanisms to build a relational graph of 2D keypoints across a global receptive field. The outputs are subsequently processed by a Time-Frequency Feature Fusion Transformer to estimate 3D human poses. SMGNFormer demonstrates superior estimation performance on the Human3.6M dataset and real-world video data compared to most Transformer-based algorithms. Moreover, the proposed method achieves a training speed comparable to PoseFormerv2, providing a clear advantage over other methods in its category.},
  archive      = {J_IETCV},
  author       = {Yi Li and Zan Wang and Weiran Niu},
  doi          = {10.1049/cvi2.12339},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12339},
  shortjournal = {IET Comput. Vis.},
  title        = {SMGNFORMER: Fusion mamba-graph transformer network for human pose estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLFormer4D: LiDAR-based lane detection method by temporal
feature fusion and sparse transformer. <em>IETCV</em>, <em>19</em>(1),
e12338. (<a href="https://doi.org/10.1049/cvi2.12338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lane detection is a fundamental problem in autonomous driving, which provides vehicles with essential road information. Despite the attention from scholars and engineers, lane detection based on LiDAR meets challenges such as unsatisfactory detection accuracy and significant computation overhead. In this paper, the authors propose LLFormer4D to overcome these technical challenges by leveraging the strengths of both Convolutional Neural Network and Transformer networks. Specifically, the Temporal Feature Fusion module is introduced to enhance accuracy and robustness by integrating features from multi-frame point clouds. In addition, a sparse Transformer decoder based on Lane Key-point Query is designed, which introduces key-point supervision for each lane line to streamline the post-processing. The authors conduct experiments and evaluate the proposed method on the K-Lane and nuScenes map datasets respectively. The results demonstrate the effectiveness of the presented method, achieving second place with an F1 score of 82.39 and a processing speed of 16.03 Frames Per Seconds on the K-Lane dataset. Furthermore, this algorithm attains the best mAP of 70.66 for lane detection on the nuScenes map dataset.},
  archive      = {J_IETCV},
  author       = {Jun Hu and Chaolu Feng and Haoxiang Jie and Zuotao Ning and Xinyi Zuo and Wei Liu and Xiangyu Wei},
  doi          = {10.1049/cvi2.12338},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12338},
  shortjournal = {IET Comput. Vis.},
  title        = {LLFormer4D: LiDAR-based lane detection method by temporal feature fusion and sparse transformer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-identification of patterned animals by multi-image
feature aggregation and geometric similarity. <em>IETCV</em>,
<em>19</em>(1), e12337. (<a
href="https://doi.org/10.1049/cvi2.12337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based re-identification of animal individuals allows gathering of information such as population size and migration patterns of the animals over time. This, together with large image volumes collected using camera traps and crowdsourcing, opens novel possibilities to study animal populations. For many species, the re-identification can be done by analysing the permanent fur, feather, or skin patterns that are unique to each individual. In this paper, the authors study pattern feature aggregation based re-identification and consider two ways of improving accuracy: (1) aggregating pattern image features over multiple images and (2) combining the pattern appearance similarity obtained by feature aggregation and geometric pattern similarity. Aggregation over multiple database images of the same individual allows to obtain more comprehensive and robust descriptors while reducing the computation time. On the other hand, combining the two similarity measures allows to efficiently utilise both the local and global pattern features, providing a general re-identification approach that can be applied to a wide variety of different pattern types. In the experimental part of the work, the authors demonstrate that the proposed method achieves promising re-identification accuracies for Saimaa ringed seals and whale sharks without species-specific training or fine-tuning.},
  archive      = {J_IETCV},
  author       = {Ekaterina Nepovinnykh and Veikka Immonen and Tuomas Eerola and Charles V. Stewart and Heikki Kälviäinen},
  doi          = {10.1049/cvi2.12337},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12337},
  shortjournal = {IET Comput. Vis.},
  title        = {Re-identification of patterned animals by multi-image feature aggregation and geometric similarity},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMF-net: A novel multi-feature and multi-level fusion
network for 3D human pose estimation. <em>IETCV</em>, <em>19</em>(1),
e12336. (<a href="https://doi.org/10.1049/cvi2.12336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation based on monocular video has always been the focus of research in the human computer interaction community, which suffers mainly from depth ambiguity and self-occlusion challenges. While the recently proposed learning-based approaches have demonstrated promising performance, they do not fully explore the complementarity of features. In this paper, the authors propose a novel multi-feature and multi-level fusion network (MMF-Net), which extracts and combines joint features, bone features and trajectory features at multiple levels to estimate 3D human pose. In MMF-Net, firstly, the bone length estimation module and the trajectory multi-level fusion module are used to extract the geometric size information of the human body and multi-level trajectory information of human motion, respectively. Then, the fusion attention-based combination (FABC) module is used to extract multi-level topological structure information of the human body, and effectively fuse topological structure information, geometric size information and trajectory information. Extensive experiments show that MMF-Net achieves competitive results on Human3.6M, HumanEva-I and MPI-INF-3DHP datasets.},
  archive      = {J_IETCV},
  author       = {Qianxing Li and Dehui Kong and Jinghua Li and Baocai Yin},
  doi          = {10.1049/cvi2.12336},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12336},
  shortjournal = {IET Comput. Vis.},
  title        = {MMF-net: A novel multi-feature and multi-level fusion network for 3D human pose estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking the power of multi-modal fusion in 3D object
tracking. <em>IETCV</em>, <em>19</em>(1), e12335. (<a
href="https://doi.org/10.1049/cvi2.12335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Single Object Tracking plays a vital role in autonomous driving and robotics, yet traditional approaches have predominantly focused on using pure LiDAR-based point cloud data, often neglecting the benefits of integrating image modalities. To address this gap, we propose a novel Multi-modal Image-LiDAR Tracker (MILT) designed to overcome the limitations of single-modality methods by effectively combining RGB and point cloud data. Our key contribution is a dual-branch architecture that separately extracts geometric features from LiDAR and texture features from images. These features are then fused in a BEV perspective to achieve a comprehensive representation of the tracked object. A significant innovation in our approach is the Image-to-LiDAR Adapter module, which transfers the rich feature representation capabilities of the image modality to the 3D tracking task, and the BEV-Fusion module, which facilitates the interactive fusion of geometry and texture features. By validating MILT on public datasets, we demonstrate substantial performance improvements over traditional methods, effectively showcasing the advantages of our multi-modal fusion strategy. This work advances the state-of-the-art in SOT by integrating complementary information from RGB and LiDAR modalities, resulting in enhanced tracking accuracy and robustness.},
  archive      = {J_IETCV},
  author       = {Yue Hu},
  doi          = {10.1049/cvi2.12335},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12335},
  shortjournal = {IET Comput. Vis.},
  title        = {Unlocking the power of multi-modal fusion in 3D object tracking},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic segmentation of urban airborne LiDAR data of
varying landcover diversity using XGBoost. <em>IETCV</em>,
<em>19</em>(1), e12334. (<a
href="https://doi.org/10.1049/cvi2.12334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of aerial LiDAR dataset is a crucial step for accurate identification of urban objects for various applications pertaining to sustainable urban development. However, this task becomes more complex in urban areas characterised by the coexistence of modern developments and natural vegetation. The unstructured nature of point cloud data, along with data sparsity, irregular point distribution, and varying sizes of urban objects, presents challenges in point cloud classification. To address these challenges, development of robust algorithmic approach encompassing efficient feature sets and classification model are essential. This study incorporates point-wise features to capture the local spatial context of points in datasets. Furthermore, an ensemble machine learning model based on extreme boosting is utilised, which integrates sequential training for weak learners, to enhance the model’s resilience. To thoroughly investigate the efficacy of the proposed approach, this study utilises three distinct datasets from diverse geographical locations, each presenting unique challenges related to class distribution, 3D terrain intricacies, and geographical variations. The Land-cover Diversity Index is introduced to quantify the complexity of landcover in 3D by measuring the degree of class heterogeneity and the frequency of class variation in the dataset. The proposed approach achieved an accuracy of 90% on the regionally complex, higher landcover diversity dataset, Trivandrum Aerial LiDAR Dataset. Furthermore, the results of the study demonstrate improved overall predictive accuracy of 91% and 87% on data segments from two benchmark datasets, DALES and Vaihingen 3D.},
  archive      = {J_IETCV},
  author       = {Jayati Vijaywargiya and Anandakumar M. Ramiya},
  doi          = {10.1049/cvi2.12334},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12334},
  shortjournal = {IET Comput. Vis.},
  title        = {Semantic segmentation of urban airborne LiDAR data of varying landcover diversity using XGBoost},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoencoder-based unsupervised one-class learning for
abnormal activity detection in egocentric videos. <em>IETCV</em>,
<em>19</em>(1), e12333. (<a
href="https://doi.org/10.1049/cvi2.12333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, abnormal human activity detection has become an important research topic. However, most existing methods focus on detecting abnormal activities of pedestrians in surveillance videos; even those methods using egocentric videos deal with the activities of pedestrians around the camera wearer. In this paper, the authors present an unsupervised auto-encoder-based network trained by one-class learning that inputs RGB image sequences recorded by egocentric cameras to detect abnormal activities of the camera wearers themselves. To improve the performance of network, the authors introduce a ‘re-encoding’ architecture and a regularisation loss function term, minimising the KL divergence between the distributions of features extracted by the first and second encoders. Unlike the common use of KL divergence loss to obtain a feature distribution close to an already-known distribution, the aim is to encourage the features extracted by the second encoder to have a close distribution to those extracted from the first encoder. The authors evaluate the proposed method on the Epic-Kitchens-55 dataset and conduct an ablation study to analyse the functions of different components. Experimental results demonstrate that the method outperforms the comparison methods in all cases and demonstrate the effectiveness of the proposed re-encoding architecture and the regularisation term.},
  archive      = {J_IETCV},
  author       = {Haowen Hu and Ryo Hachiuma and Hideo Saito},
  doi          = {10.1049/cvi2.12333},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12333},
  shortjournal = {IET Comput. Vis.},
  title        = {Autoencoder-based unsupervised one-class learning for abnormal activity detection in egocentric videos},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised bounding-box generation for camera-trap
image based animal detection. <em>IETCV</em>, <em>19</em>(1), e12332.
(<a href="https://doi.org/10.1049/cvi2.12332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ecology, deep learning is improving the performance of camera-trap image based wild animal analysis. However, high labelling cost becomes a big challenge, as it requires involvement of huge human annotation. For example, the Snapshot Serengeti (SS) dataset contains over 900,000 images, while only 322,653 contains valid animals, 68,000 volunteers were recruited to provide image level labels such as species, the no. of animals and five behaviour attributes such as standing, resting and moving etc. In contrast, the Gold Standard SS Bounding-Box Coordinates (GSBBC for short) contains only 4011 images for training of object detection algorithms, as the annotation of bounding-box for animals in the image, is much more costive. Such a no. of training images, is obviously insufficient. To address this, the authors propose a method to generate bounding-boxes for a larger dataset using limited manually labelled images. To achieve this, the authors first train a wild animal detector using a small dataset (e.g. GSBBC) that is manually labelled to locate animals in images; then apply this detector to a bigger dataset (e.g. SS) for bounding-box generation; finally, we remove false detections according to the existing label information of the images. Experiments show that detector trained with images whose bounding-boxes are generated using the proposal, outperformed the existing camera-trap image based animal detection, in terms of mean average precision (mAP). Compared with the traditional data augmentation method, our method improved the mAP by 21.3% and 44.9% for rare species, also alleviating the long-tail issue in data distribution. In addition, detectors trained with the proposed method also achieve promising results when applied to classification and counting tasks, which are commonly required in wildlife research.},
  archive      = {J_IETCV},
  author       = {Puxuan Xie and Renwu Gao and Weizeng Lu and Linlin Shen},
  doi          = {10.1049/cvi2.12332},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12332},
  shortjournal = {IET Comput. Vis.},
  title        = {Weakly supervised bounding-box generation for camera-trap image based animal detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Representation alignment contrastive regularisation for
multi-object tracking. <em>IETCV</em>, <em>19</em>(1), e12331. (<a
href="https://doi.org/10.1049/cvi2.12331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving high-performance in multi-object tracking algorithms heavily relies on modelling spatial-temporal relationships during the data association stage. Mainstream approaches encompass rule-based and deep learning-based methods for spatial-temporal relationship modelling. While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs. This work aims to simplify deep learning-based spatial-temporal relationship models and introduce interpretability into features for data association. Specifically, a lightweight single-layer transformer encoder is utilised to model spatial-temporal relationships. To make features more interpretative, two contrastive regularisation losses based on representation alignment are proposed, derived from spatial-temporal consistency rules. By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow. Experimental results showcase that our model enhances the majority of existing tracking networks&#39; performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs.},
  archive      = {J_IETCV},
  author       = {Shujie Chen and Zhonglin Liu and Jianfeng Dong and Xun Wang and Di Zhou},
  doi          = {10.1049/cvi2.12331},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12331},
  shortjournal = {IET Comput. Vis.},
  title        = {Representation alignment contrastive regularisation for multi-object tracking},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outliers rejection for robust camera pose estimation using
graduated non-convexity. <em>IETCV</em>, <em>19</em>(1), e12330. (<a
href="https://doi.org/10.1049/cvi2.12330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera pose estimation plays a crucial role in computer vision, which is widely used in augmented reality, robotics and autonomous driving. However, previous studies have neglected the presence of outliers in measurements, so that even a small percentage of outliers will significantly degrade precision. In order to deal with outliers, this paper proposes using a graduated non-convexity (GNC) method to suppress outliers in robust camera pose estimation, which serves as the core of GNCPnP. The authors first reformulate the camera pose estimation problem using a non-convex cost, which is less affected by outliers. Then, to apply a non-minimum solver to solve the reformulated problem, the authors use the Black-Rangarajan duality theory to transform it. Finally, to address the dependence of non-convex optimisation on initial values, the GNC method was customised according to the truncated least squares cost. The results of simulation and real experiments show that GNCPnP can effectively handle the interference of outliers and achieve higher accuracy compared to existing state-of-the-art algorithms. In particular, the camera pose estimation accuracy of GNCPnP in the case of a low percentage of outliers is almost comparable to that of the state-of-the-art algorithm in the case of no outliers.},
  archive      = {J_IETCV},
  author       = {Hao Yi and Bo Liu and Bin Zhao and Enhai Liu},
  doi          = {10.1049/cvi2.12330},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12330},
  shortjournal = {IET Comput. Vis.},
  title        = {Outliers rejection for robust camera pose estimation using graduated non-convexity},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid feature-based moving cast shadow detection.
<em>IETCV</em>, <em>19</em>(1), e12328. (<a
href="https://doi.org/10.1049/cvi2.12328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate detection of moving objects is essential in various applications of artificial intelligence, particularly in the field of intelligent surveillance systems. However, the moving cast shadow detection significantly decreases the precision of moving object detection because they share similar motion characteristics. To address the issue, the authors propose an innovative approach to detect moving cast shadows by combining the hybrid feature with a broad learning system (BLS). The approach involves extracting low-level features from the input and background images based on colour constancy and texture consistency principles that are shown to be highly effective in moving cast shadow detection. The authors then utilise the BLS to create a hybrid feature and BLS uses the extracted low-level features as input instead of the original data. BLS is an innovative form of deep learning that can map input to feature nodes and further enhance them by enhancement nodes, resulting in more compact features for classification. Finally, the authors develop an efficient and straightforward post-processing technique to improve the accuracy of moving object detection. To evaluate the effectiveness and generalisation ability, the authors conduct extensive experiments on public ATON-CVRR and CDnet datasets to verify the superior performance of our method by comparing with representative approaches.},
  archive      = {J_IETCV},
  author       = {Jiangyan Dai and Huihui Zhang and Jin Gao and Chunlei Chen and Yugen Yi},
  doi          = {10.1049/cvi2.12328},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12328},
  shortjournal = {IET Comput. Vis.},
  title        = {Hybrid feature-based moving cast shadow detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Category-instance distillation based on visual-language
models for rehearsal-free class incremental learning. <em>IETCV</em>,
<em>19</em>(1), e12327. (<a
href="https://doi.org/10.1049/cvi2.12327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, visual-language models (VLMs) have displayed potent capabilities in the field of computer vision. Their emerging trend as the backbone of visual tasks necessitates studying class incremental learning (CIL) issues within the VLM architecture. However, the pre-training data for many VLMs is proprietary, and during the incremental phase, old task data may also raise privacy issues. Moreover, replay-based methods can introduce new problems like class imbalance, the selection of data for replay and a trade-off between replay cost and performance. Therefore, the authors choose the more challenging rehearsal-free settings. In this paper, the authors study class-incremental tasks based on the large pre-trained vision-language models like CLIP model. Initially, at the category level, the authors combine traditional optimisation and distillation techniques, utilising both pre-trained models and models trained in previous incremental stages to jointly guide the training of the new model. This paradigm effectively balances the stability and plasticity of the new model, mitigating the issue of catastrophic forgetting. Moreover, utilising the VLM infrastructure, the authors redefine the relationship between instances. This allows us to glean fine-grained instance relational information from the a priori knowledge provided during pre-training. The authors supplement this approach with an entropy-balancing method that allows the model to adaptively distribute optimisation weights across training samples. The authors’ experimental results validate that their method, within the framework of VLMs, outperforms traditional CIL methods.},
  archive      = {J_IETCV},
  author       = {Weilong Jin and Zilei Wang and Yixin Zhang},
  doi          = {10.1049/cvi2.12327},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12327},
  shortjournal = {IET Comput. Vis.},
  title        = {Category-instance distillation based on visual-language models for rehearsal-free class incremental learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMEF-net: Towards an attention and multi-level enhancement
fusion for medical image classification in parkinson’s aided diagnosis.
<em>IETCV</em>, <em>19</em>(1), e12324. (<a
href="https://doi.org/10.1049/cvi2.12324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson&#39;s disease (PD) is a neurodegenerative disorder primarily affecting middle-aged and elderly populations. Its insidious onset, high disability rate, long diagnostic cycle, and high diagnostic costs impose a heavy burden on patients and their families. Leveraging artificial intelligence, with its rapid diagnostic speed, high accuracy, and fatigue resistance, to achieve intelligent assisted diagnosis of PD holds significant promise for alleviating patients&#39; financial stress, reducing diagnostic cycles, and helping patients seize the golden period for early treatment. This paper proposes an Attention and Multi-level Enhancement Fusion Network (AMEF-Net) based on the characteristics of three-dimensional medical imaging and the specific manifestations of PD in medical images. The focus is on small lesion areas and structural lesion areas that are often overlooked in traditional deep learning models, achieving multi-level attention and processing of imaging information. The model achieved a diagnostic accuracy of 98.867%, a precision of 99.830%, a sensitivity of 99.182%, and a specificity of 99.384% on Magnetic Resonance Images from the Parkinson&#39;s Progression Markers Initiative dataset. On Diffusion Tensor Images, it achieved a diagnostic accuracy of 99.602%, a precision of 99.930%, a sensitivity of 99.463%, and a specificity of 99.877%. The relevant code has been placed in https://github.com/EdwardTj/AMEF-NET .},
  archive      = {J_IETCV},
  author       = {Qingyan Ding and Yu Pan and Jianxin Liu and Lianxin Li and Nan Liu and Na Li and Wan Zheng and Xuecheng Dong},
  doi          = {10.1049/cvi2.12324},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12324},
  shortjournal = {IET Comput. Vis.},
  title        = {AMEF-net: Towards an attention and multi-level enhancement fusion for medical image classification in parkinson&#39;s aided diagnosis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metric-guided class-level alignment for domain adaptation.
<em>IETCV</em>, <em>19</em>(1), e12322. (<a
href="https://doi.org/10.1049/cvi2.12322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilisation of domain adaptation methods facilitates the resolution of classification challenges in an unlabelled target domain by capitalising on the labelled information from source domains. Unfortunately, previous domain adaptation methods have focused mostly on global domain adaptation and have not taken into account class-specific data, which leads to poor knowledge transfer performance. The study of class-level domain adaptation, which aims to precisely match the distributions of different domains, has garnered attention in recent times. However, existing investigations into class-level alignment frequently align domain features either directly on or in close proximity to classification boundaries, resulting in the creation of uncertain samples that could potentially impair classification accuracy. To address the aforementioned problem, we propose a new approach called metric-guided class-level alignment (MCA) as a solution to this problem. Specifically, we employ different metrics to enable the network to acquire supplementary information, thereby enhancing class-level alignment. Moreover, MCA can be effectively combined with existing domain-level alignment methods to successfully mitigate the challenges posed by domain shift. Extensive testing on commonly-used public datasets shows that our method outperforms many other cutting-edge domain adaptation methods, showing significant gains over baseline performance.},
  archive      = {J_IETCV},
  author       = {Xiaoshun Wang and Yunhan Li},
  doi          = {10.1049/cvi2.12322},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12322},
  shortjournal = {IET Comput. Vis.},
  title        = {Metric-guided class-level alignment for domain adaptation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMSFU: A hierarchical multi-scale fusion unit for video
prediction and beyond. <em>IETCV</em>, <em>19</em>(1), e12312. (<a
href="https://doi.org/10.1049/cvi2.12312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction is the process of learning necessary information from historical frames to predict future video frames. Learning features from historical frames is a crucial step in this process. However, most current methods have a relatively single-scale learning approach, even if they learn features at different scales, they cannot fully integrate and utilise them, resulting in unsatisfactory prediction results. To address this issue, a hierarchical multi-scale fusion unit (HMSFU) is proposed. By using a hierarchical multi-scale architecture, each layer predicts future frames at different granularities using different convolutional scales. The abstract features from different layers can be fused, enabling the model not only to capture rich contextual information but also to expand the model&#39;s receptive field, enhance its expressive power, and improve its applicability to complex prediction scenarios. To fully utilise the expanded receptive field, HMSFU incorporates three fusion modules. The first module is the single-layer historical attention fusion module, which uses an attention mechanism to fuse the features from historical frames into the current frame at each layer. The second module is the single-layer spatiotemporal fusion module, which fuses complementary temporal and spatial features at each layer. The third module is the multi-layer spatiotemporal fusion module, which fuses spatiotemporal features from different layers. Additionally, the authors not only focus on the frame-level error using mean squared error loss, but also introduce the novel use of Kullback–Leibler (KL) divergence to consider inter-frame variations. Experimental results demonstrate that our proposed HMSFU model achieves the best performance on popular video prediction datasets, showcasing its remarkable competitiveness in the field.},
  archive      = {J_IETCV},
  author       = {Hongchang Zhu and Faming Fang},
  doi          = {10.1049/cvi2.12312},
  journal      = {IET Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {e12312},
  shortjournal = {IET Comput. Vis.},
  title        = {HMSFU: A hierarchical multi-scale fusion unit for video prediction and beyond},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
