<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm---13">WIDM - 13</h2>
<ul>
<li><details>
<summary>
(2025). Survey on latest advances in natural language processing
applications of generative adversarial networks. <em>WIDM</em>,
<em>15</em>(1), e70004. (<a
href="https://doi.org/10.1002/widm.70004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining and natural language processing (NLP) are fundamental fields that interact in many ways. Text mining shares many topics, such as sentiment analysis and content understanding. Combining these two fields enables more efficient mining of text data and the extraction of valuable information. In particular, the GAN (Generative Adversarial Network) architecture has achieved success in image generation and has started to be used on text data. However, training GANs is fraught with difficulties due to the complexity of text data. Linguistic studies show important differences between languages. Language is characterized by fluidity, ambiguity, and context-sensitive interpretations, and text-generating GAN models can struggle to deal with these complexities. The interaction between data quality, language structure, and complex interpretation can lead to inconsistency and ambiguity in the text production of GAN models. These problems are particularly pronounced when complexities such as semantic subtleties, idiomatic expressions, and context-dependent usages come into play. Text generation is an area of GAN models used in NLP to generate language and enrich text-based applications. Work in this area can contribute to analyzing, classifying, and processing text data. Many methods and techniques have been proposed to improve the performance of text GANs. However, some problems may be encountered in the optimization of these methods. Therefore, it is essential to use optimized methods. In conclusion, GANs can be an important tool to improve text generation in NLP. Still, they require continuous research and innovation to deal with factors such as language complexity and data quality.},
  archive      = {J_WIDM},
  author       = {Canan Koç and Fatih Özyurt and Laszlo Barna Iantovics},
  doi          = {10.1002/widm.70004},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70004},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Survey on latest advances in natural language processing applications of generative adversarial networks},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated detection of neurological and mental health
disorders using EEG signals and artificial intelligence: A systematic
review. <em>WIDM</em>, <em>15</em>(1), e70002. (<a
href="https://doi.org/10.1002/widm.70002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental and neurological disorders significantly impact global health. This systematic review examines the use of artificial intelligence (AI) techniques to automatically detect these conditions using electroencephalography (EEG) signals. Guided by Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA), we reviewed 74 carefully selected studies published between 2013 and August 2024 that used machine learning (ML), deep learning (DL), or both of these two methods to detect neurological and mental health disorders automatically using EEG signals. The most common and most prevalent neurological and mental health disorder types were sourced from major databases, including Scopus, Web of Science, Science Direct, PubMed, and IEEE Xplore. Epilepsy, depression, and Alzheimer&#39;s disease are the most studied conditions that meet our evaluation criteria, 32, 12, and 10 studies were identified on these topics, respectively. Conversely, the number of studies meeting our criteria regarding stress, schizophrenia, Parkinson&#39;s disease, and autism spectrum disorders was relatively more average: 6, 4, 3, and 3, respectively. The diseases that least met our evaluation conditions were one study each of seizure, stroke, anxiety diseases, and one study examining Alzheimer&#39;s disease and epilepsy together. Support Vector Machines (SVM) were most widely used in ML methods, while Convolutional Neural Networks (CNNs) dominated DL approaches. DL methods generally outperformed traditional ML, as they yielded higher performance using huge EEG data. We observed that the complex decision process during feature extraction from EEG signals in ML-based models significantly impacted results, while DL-based models handled this more efficiently. AI-based EEG analysis shows promise for automated detection of neurological and mental health conditions. Future research should focus on multi-disease studies, standardizing datasets, improving model interpretability, and developing clinical decision support systems to assist in the diagnosis and treatment of these disorders.},
  archive      = {J_WIDM},
  author       = {Hakan Uyanik and Abdulkadir Sengur and Massimo Salvi and Ru-San Tan and Jen Hong Tan and U. Rajendra Acharya},
  doi          = {10.1002/widm.70002},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70002},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Automated detection of neurological and mental health disorders using EEG signals and artificial intelligence: A systematic review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of approaches to early rumor detection on
microblogging platforms: Computational and socio-psychological insights.
<em>WIDM</em>, <em>15</em>(1), e70001. (<a
href="https://doi.org/10.1002/widm.70001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media, particularly microblogging platforms, are essential for rapid information sharing and public discussion but often allow rumors, that is, unverified information, to spread rapidly during events or persist over time. These platforms also offer opportunities to study the dynamics of rumors and develop computational methods to assess their veracity. In this paper, we provide a comprehensive review of existing theoretical foundations, interdisciplinary challenges, and emerging advancements in rumor detection research, with a focus on integrating theoretical and computational approaches. Drawing on insights from computer science, cognitive psychology, and sociology, we explore methodologies, such as multimodal fusion, graph-based models, and attention mechanisms, while highlighting gaps in real-world scalability, ethical transparency, and cross-platform adaptability. Using a systematic literature review and bibliometric analysis, we identify trends, methods, and gaps in current research. Our findings emphasize interdisciplinary collaboration to develop adaptable, efficient, and ethical rumor detection strategies. We also highlight the critical role of combining socio-psychological insights with advanced computational techniques to address the human factors in rumor spread. Furthermore, we emphasize the importance of designing systems that remain effective across diverse cultural and linguistic contexts, enhancing their global applicability. We propose a conceptual framework integrating diverse theories and computational techniques, offering a roadmap for improving detection systems and addressing misinformation challenges on microblogging platforms.},
  archive      = {J_WIDM},
  author       = {Lazarus Kwao and Yang Yang and Jie Zou and Jing Ma},
  doi          = {10.1002/widm.70001},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70001},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of approaches to early rumor detection on microblogging platforms: Computational and socio-psychological insights},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICT-driven data mining analysis in civil engineering: A
scientometric review. <em>WIDM</em>, <em>15</em>(1), e70000. (<a
href="https://doi.org/10.1002/widm.70000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary landscape, the remarkable evolution of civil engineering is being driven by the pervasive integration of Information and Communication Technology (ICT). ICT-driven innovations are playing a crucial role in advancing sustainable development goals by promoting energy efficiency, minimizing resource consumption, and fostering resilient infrastructure. Solutions such as smart grids, intelligent transportation systems, and sustainable urban planning are integral to this progress to address global challenges. The goal of the current study is to conduct a scientometric analysis of scholarly literature published in the recent decade within the domain of ICT-assisted civil engineering. To achieve this, the study categorizes the civil engineering field into seven major subfields. It includes structural engineering, geotechnical engineering, transportation engineering, water resources engineering, environmental engineering, construction management, and urban planning and design. Employing CiteSpace as the analytical tool, the research offers insights into the intellectual foundations of the civil engineering. This is accomplished through reference co-citation analysis, cluster analysis, and burst reference analysis. The results demonstrate the adoption of advanced technologies such as Internet of Things (IoT), Machine Learning (ML), Extreme Gradient Boosting (XGBoost), and artificial neural networks in resolving complex civil engineering challenges that reflect the dynamism and diversity of the field. Moreover, it addresses current research challenges within this knowledge domain and explores potential research prospects. The findings emphasize the importance of collaborative efforts among academia, industry stakeholders, and government entities.},
  archive      = {J_WIDM},
  author       = {Kashvi Sood},
  doi          = {10.1002/widm.70000},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70000},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {ICT-driven data mining analysis in civil engineering: A scientometric review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on medical image segmentation: Datasets, technical
models, challenges and solutions. <em>WIDM</em>, <em>15</em>(1), e1574.
(<a href="https://doi.org/10.1002/widm.1574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is prerequisite in computer-aided diagnosis. As the field experiences tremendous paradigm changes since the introduction of foundation models, technicality of deep medical segmentation model is no longer a privilege limited to computer science researchers. A comprehensive educational resource suitable for researchers of broad, different backgrounds such as biomedical and medicine, is needed. This review strategically covers the evolving trends that happens to different fundamental components of medical image segmentation such as the emerging of multimodal medical image datasets, updates on deep learning libraries, classical-to-contemporary development in deep segmentation models and latest challenges with focus on enhancing the interpretability and generalizability of model. Last, the conclusion section highlights on future trends in deep medical segmentation that worth further attention and investigations.},
  archive      = {J_WIDM},
  author       = {Hong-Seng Gan and Muhammad Hanif Ramlee and Zimu Wang and Akinobu Shimizu},
  doi          = {10.1002/widm.1574},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1574},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review on medical image segmentation: Datasets, technical models, challenges and solutions},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trace encoding techniques for multi-perspective process
mining: A comparative study. <em>WIDM</em>, <em>15</em>(1), e1573. (<a
href="https://doi.org/10.1002/widm.1573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining (PM) comprises a variety of methods for discovering information about processes from their execution logs. Some of them, such as trace clustering, trace classification, and anomalous trace detection require a preliminary preprocessing step in which the raw data is encoded into a numerical feature space. To this end, encoding techniques are used to generate vectorial representations of process traces. Most of the PM literature provides trace encoding techniques that look at the control flow, that is, only encode the sequence of activities that characterize a process trace disregarding other process data that is fundamental for effectively describing the process behavior. To fill this gap, in this article we show 19 trace encoding methods that work in a multi-perspective manner, that is, by embedding events and trace attributes in addition to activity names into the vectorial representations of process traces. We also provide an extensive experimental study where these techniques are applied to real-life datasets and compared to each other.},
  archive      = {J_WIDM},
  author       = {Antonino Rullo and Farhana Alam and Edoardo Serra},
  doi          = {10.1002/widm.1573},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1573},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Trace encoding techniques for multi-perspective process mining: A comparative study},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-parameter optimization of kernel functions on
multi-class text categorization: A comparative evaluation.
<em>WIDM</em>, <em>15</em>(1), e1572. (<a
href="https://doi.org/10.1002/widm.1572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning (ML) has witnessed a paradigm shift in kernel function selection, which is pivotal in optimizing various ML models. Despite multiple studies about its significance, a comprehensive understanding of kernel function selection, particularly about model performance, still needs to be explored. Challenges remain in selecting and optimizing kernel functions to improve model performance and efficiency. The study investigates how gamma parameter and cost parameter influence performance metrics in multi-class classification tasks using various kernel-based algorithms. Through sensitivity analysis, the impact of these parameters on classification performance and computational efficiency is assessed. The experimental setup involves deploying ML models using four kernel-based algorithms: Support Vector Machine, Radial Basis Function, Polynomial Kernel, and Sigmoid Kernel. Data preparation includes text processing, categorization, and feature extraction using TfidfVectorizer, followed by model training and validation. Results indicate that Support Vector Machine with default settings and Radial Basis Function kernel consistently outperforms polynomial and sigmoid kernels. Adjusting gamma improves model accuracy and precision, highlighting its role in capturing complex relationships. Regularization cost parameters, however, show minimal impact on performance. The study also reveals that configurations with moderate gamma values achieve better balance between performance and computational time compared to higher gamma values or no gamma adjustment. The findings underscore the delicate balance between model performance and computational efficiency by highlighting the trade-offs between model complexity and efficiency.},
  archive      = {J_WIDM},
  author       = {Michael Loki and Agnes Mindila and Wilson Cheruiyot},
  doi          = {10.1002/widm.1572},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1572},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Hyper-parameter optimization of kernel functions on multi-class text categorization: A comparative evaluation},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Business analytics in customer lifetime value: An overview
analysis. <em>WIDM</em>, <em>15</em>(1), e1571. (<a
href="https://doi.org/10.1002/widm.1571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In customer-oriented systems, customer lifetime value (CLV) has been of significant importance for academia and marketing practitioners, especially within the scope of analytical modeling. CLV is a critical approach to managing and organizing a company&#39;s profitability. With the vast availability of consumer data, business analytics (BA) tools and approaches, alongside CLV models, have been applied to gain deeper insights into customer behaviors and decision-making processes. Despite the recognized importance of CLV, there is a noticeable gap in comprehensive analyses and reviews of BA techniques applied to CLV. This study aims to fill this gap by conducting a thorough survey of the state-of-the-art investigations on CLV models integrated with BA approaches, thereby contributing to a research agenda in this field. The review methodology consists of three main steps: identification of relevant studies, creating a coding plan, and ensuring coding reliability. First, relevant studies were identified using predefined keywords. Next, a coding plan—one of the study&#39;s significant contributions—was developed to evaluate these studies comprehensively. Finally, the coding plan&#39;s reliability was tested by three experts before being applied to the selected studies. Additionally, specific evaluation criteria in the coding plan were implemented to introduce new insights. This study presents exciting and valuable results from various perspectives, providing a crucial reference for academic researchers and marketing practitioners interested in the intersection of BA and CLV.},
  archive      = {J_WIDM},
  author       = {Onur Dogan and Abdulkadir Hiziroglu and Ali Pisirgen and Omer Faruk Seymen},
  doi          = {10.1002/widm.1571},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1571},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Business analytics in customer lifetime value: An overview analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph for solubility big data: Construction and
applications. <em>WIDM</em>, <em>15</em>(1), e1570. (<a
href="https://doi.org/10.1002/widm.1570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dissolution refers to the process in which solvent molecules and solute molecules attract and combine with each other. The extensive solubility data generated from the dissolution of various compounds under different conditions, is distributed across structured or semi-structured formats in various media, such as text, web pages, tables, images, and databases. These data exhibit multi-source and unstructured features, aligning with the typical 5 V characteristics of big data. A solubility big data technology system has emerged under the fusion of solubility data and big data technologies. However, the acquisition, fusion, storage, representation, and utilization of solubility big data are encountering new challenges. Knowledge Graphs, known as extensive systems for representing and applying knowledge, can effectively describe entities, concepts, and relations across diverse domains. The construction of solubility big data knowledge graph holds substantial value in the retrieval, analysis, utilization, and visualization of solubility knowledge. Throwing out a brick to attract a jade, this paper focuses on the solubility big data knowledge graph and, firstly, summarizes the architecture of solubility knowledge graph construction. Secondly, the key technologies such as knowledge extraction, knowledge fusion, and knowledge reasoning of solubility big data are emphasized, along with summarizing the common machine learning methods in knowledge graph construction. Furthermore, this paper explores application scenarios, such as knowledge question answering and recommender systems for solubility big data. Finally, it presents a prospective view of the shortcomings, challenges, and future directions related to the construction of solubility big data knowledge graph. This article proposes the research direction of solubility big data knowledge graph, which can provide technical references for constructing a solubility knowledge graph. At the same time, it serves as a comprehensive medium for describing data, resources, and their applications across diverse fields such as chemistry, materials, biology, energy, medicine, and so on. It further aids in knowledge retrieval and mining, analysis and utilization, and visualization across various disciplines.},
  archive      = {J_WIDM},
  author       = {Xiao Haiyang and Yan Ruomei and Wu Yan and Guan Lixin and Li Mengshan},
  doi          = {10.1002/widm.1570},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1570},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Knowledge graph for solubility big data: Construction and applications},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using machine learning for systematic literature review case
in point: Agile software development. <em>WIDM</em>, <em>15</em>(1),
e1569. (<a href="https://doi.org/10.1002/widm.1569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic literature reviews (SLRs) are essential for researchers to keep up with past and recent research in their domains. However, the rapid growth in knowledge creation and the rising number of publications have made this task increasingly complex and challenging. Moreover, most systematic literature reviews are performed manually, which requires significant effort and creates potential bias. The risk of bias is particularly relevant in the data synthesis task, where researchers interpret each study&#39;s evidence and summarize the results. This study uses an experimental approach to explore using machine learning (ML) techniques in the SLR process. Specifically, this study replicates a study that manually performed sentiment analysis for the data synthesis step to determine the polarity (negative or positive) of evidence extracted from studies in the field of agile methodology. This study employs a lexicon-based approach to sentiment analysis and achieves an accuracy rate of approximately 86.5% in identifying study evidence polarity.},
  archive      = {J_WIDM},
  author       = {Itzik David and Roy Gelbard},
  doi          = {10.1002/widm.1569},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1569},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Using machine learning for systematic literature review case in point: Agile software development},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensionality reduction for data analysis with quantum
feature learning. <em>WIDM</em>, <em>15</em>(1), e1568. (<a
href="https://doi.org/10.1002/widm.1568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve data analysis and feature learning, this study compares the effectiveness of quantum dimensionality reduction (qDR) techniques to classical ones. In this study, we investigate several qDR techniques on a variety of datasets such as quantum Gaussian distribution adaptation (qGDA), quantum principal component analysis (qPCA), quantum linear discriminant analysis (qLDA), and quantum t-SNE (qt-SNE). The Olivetti Faces, Wine, Breast Cancer, Digits, and Iris are among the datasets used in this investigation. Through comparison evaluations against well-established classical approaches, such as classical PCA (cPCA), classical LDA (cLDA), and classical GDA (cGDA), and using well-established metrics like loss, fidelity, and processing time, the effectiveness of these techniques is assessed. The findings show that cPCA produced positive results with the lowest loss and highest fidelity when used on the Iris dataset. On the other hand, quantum uniform manifold approximation and projection (qUMAP) performs well and shows strong fidelity when tested against the Wine dataset, but ct-SNE shows mediocre performance against the Digits dataset. Isomap and locally linear embedding (LLE) function differently depending on the dataset. Notably, LLE showed the largest loss and lowest fidelity on the Olivetti Faces dataset. The hypothesis testing findings showed that the qDR strategies did not significantly outperform the classical techniques in terms of maintaining pertinent information from quantum datasets. More specifically, the outcomes of paired t -tests show that when it comes to the ability to capture complex patterns, there are no statistically significant differences between the cPCA and qPCA, the cLDA and qLDA, and the cGDA and qGDA. According to the findings of the assessments of mutual information (MI) and clustering accuracy, qPCA may be able to recognize patterns more clearly than standardized cPCA. Nevertheless, there is no discernible improvement between the qLDA and qGDA approaches and their classical counterparts.},
  archive      = {J_WIDM},
  author       = {Shyam R. Sihare},
  doi          = {10.1002/widm.1568},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1568},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Dimensionality reduction for data analysis with quantum feature learning},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial attacks in explainable machine learning: A
survey of threats against models and humans. <em>WIDM</em>,
<em>15</em>(1), e1567. (<a
href="https://doi.org/10.1002/widm.1567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this paper, we comprehensively review the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios where a human assesses not only the input and the output classification, but also the explanation of the model&#39;s decision. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment. Based on this framework, we provide a structured review of the diverse attack paradigms existing in this domain, identify current gaps and future research directions, and illustrate the main attack paradigms discussed. Furthermore, our framework considers a wide range of relevant yet often ignored factors such as the type of problem, the user expertise or the objective of the explanations, in order to identify the attack strategies that should be adopted in each scenario to successfully deceive the model (and the human). The intention of these contributions is to serve as a basis for a more rigorous and realistic study of adversarial examples in the field of explainable machine learning.},
  archive      = {J_WIDM},
  author       = {Jon Vadillo and Roberto Santana and Jose A. Lozano},
  doi          = {10.1002/widm.1567},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1567},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Adversarial attacks in explainable machine learning: A survey of threats against models and humans},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application-based review of soft computational methods to
enhance industrial practices abetted by the patent landscape analysis.
<em>WIDM</em>, <em>15</em>(1), e1564. (<a
href="https://doi.org/10.1002/widm.1564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft computing is a collective methodology that touches all engineering and technology fields owing to its easiness in solving various problems while comparing the conventional methods. Many analytical methods are taken over by this soft computing technique and resolve it accurately and the soft computing has given a paradigm shift. The flexibility in soft computing results in swift knowledge acquisition processing and the information supply renders versatile and affordable technological system. Besides, the accuracy with which the soft computing technique predicts the parameters has transformed the industrial productivity to a whole new level. The interest of this article focuses on versatile applications of SC methods to forecast the technological changes which intend to reorient the progress of various industries, and this is ascertained by a patent landscape analysis. The patent landscape revealed the players who are in the segment consistently and this also provides how this field moves on in the future and who could be a dominant country for a specific technology. Alongside, the accuracy of the soft computing method for a particular practice has also been mentioned indicating the feasibility of the technique. The novel part of this article lies in patent landscape analysis compared with the other data while the other part is the discussion of application of computational techniques to various industrial practices. The progress of various engineering applications integrating them with the patent landscape analysis must be envisaged for a better understanding of the future of all these applications resulting in an improved productivity.},
  archive      = {J_WIDM},
  author       = {S. Tamilselvan and G. Dhanalakshmi and D. Balaji and L. Rajeshkumar},
  doi          = {10.1002/widm.1564},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1564},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Application-based review of soft computational methods to enhance industrial practices abetted by the patent landscape analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
