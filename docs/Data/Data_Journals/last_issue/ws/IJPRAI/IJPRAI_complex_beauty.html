<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJPRAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijprai---11">IJPRAI - 11</h2>
<ul>
<li><details>
<summary>
(2025). Multi-target detection method of intelligent driving traffic
scene based on faster r-CNN++. <em>IJPRAI</em>, <em>39</em>(1), 2459019.
(<a href="https://doi.org/10.1142/S0218001424590195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current multi-objective detection methods are plagued by several issues, including insufficient detection efficiency, slow speeds, and difficulties in deciphering driving rules. To address these challenges, we introduce a traffic scene classification method that utilizes Improved Faster Regions with Convolutional Neural Network features (Faster R-CNN++). Initially, the shared convolutional layer and the Recurrent Criss-Cross Attention (RCCA) layer are employed to extract features from the input image. Subsequently, the derived feature map is fed into the Region Proposal Network (RPN) to generate detection boxes, and the Region of Interest Align (RoI Align) layer selects features corresponding to each Region of Interest (RoI) on the feature map, guided by the RPN’s output. Ultimately, we adopt an alternating training approach. Simulation experiments confirm the effectiveness of our method in multi-scene object detection. Our method has demonstrated significant improvements over existing algorithms and has delivered outstanding performance on the BDD-100 K, ApolloScape, and NuScenes datasets. The results indicate that our deep learning-based traffic scene classification method can accurately discern the behavioral characteristics of various traffic participants.},
  archive      = {J_IJPRAI},
  author       = {Qiangqiang Xu and Junhua Guo},
  doi          = {10.1142/S0218001424590195},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2459019},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Multi-target detection method of intelligent driving traffic scene based on faster R-CNN++},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance improvement in MIMO-OFDM VLC systems through
combined adaptive modulation and coding with symbol decomposition.
<em>IJPRAI</em>, <em>39</em>(1), 2458007. (<a
href="https://doi.org/10.1142/S0218001424580072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the visible light communication (VLC) system, adaptive symbol decomposition technology is one of the methods to reduce peak-to-average power ratio (PAPR) and suppress LED nonlinear distortion. However, at high symbol variances, there still exists a high PAPR, leading to a higher number of decomposed symbols, which in turn reduces the information rate and increases the bit error rate (BER) performance. Therefore, a combined approach of adaptive modulation, adaptive symbol decomposition serial transmission (ASDST), and space–time block code and orthogonal cyclic matrix transform (STBC-OCT) coding is proposed. The characteristics of PAPR, symbol decomposition and BER under different approaches through Monte Carlo simulation were analyzed. The simulation results show that when using 4-Quadrate Amplitude Modulation (4QAM) modulation and CCDF= 2 × 1 0 − 4 , the STBC-OCT-ASDST scheme gains a 5 dB improvement compared to ASDST alone. At a BER of 3 × 1 0 − 2 , STBC-OCT-ASDST achieves a 10 dBm gain in symbol variance compared to ASDST. Moreover, when the symbol variance of STBC-OCT-ASDST is less than 34 dBm, the BER remains below the 7% threshold of forward error correction (FEC) error rate.},
  archive      = {J_IJPRAI},
  author       = {Na Zhang and JianQiang He and Jiao Liu and YuanYuan Wang},
  doi          = {10.1142/S0218001424580072},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2458007},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Performance improvement in MIMO-OFDM VLC systems through combined adaptive modulation and coding with symbol decomposition},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing brain tumor diagnosis: A comprehensive model
integrating VGG19 and LSTM for accurate MRI classification.
<em>IJPRAI</em>, <em>39</em>(1), 2457015. (<a
href="https://doi.org/10.1142/S0218001424570155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing brain tumors is particularly difficult because they can grow in unpredictable ways and look very different on MRI scans. The current methods used to automatically identify these tumors often struggle because of the wide variety of tumor types and the complex structure of the brain. As a result, these methods don’t always classify tumors accurately, which can affect patient treatment and outcomes. The main problems with these methods are that they find it hard to distinguish between different types of tumors accurately and to deal with the various ways tumors can appear on MRI scans. To improve this situation, our study integrates the robust image classification capabilities of VGG19 with the sequential data processing strengths of LSTM. This synergistic approach enhances our model’s ability to accurately classify various types of brain tumors from MRI scans, addressing the inherent challenges associated with tumor heterogeneity in medical imaging. VGG19, a deep convolutional neural network, is employed to extract detailed features from MRI scans, facilitating precise tumor characterization based on visual patterns and LSTM complements VGG19 by capturing temporal dependencies in the sequential data of MRI scans, enabling the model to discern subtle variations in tumor appearances over time. By leveraging the combined power of VGG19 and LSTM architectures, our study achieves significant advancements in the accurate classification of brain tumors from MRI images. This approach not only enhances diagnostic precision but also lays the groundwork for future improvements in neuro-oncological imaging diagnostics. Our study includes 1000 patients evaluated with MRI for brain tumors. We achieved an overall accuracy of 98.32% demonstrating the efficacy of our VGG19-LSTM model in accurate tumor classification. By using both, our model aims to get better at understanding MRI scans and, as a result, be more accurate at identifying brain tumors. This combination is a new step forward in making brain tumor diagnosis more precise through a detailed and cooperative approach using neural networks.},
  archive      = {J_IJPRAI},
  author       = {Chandrasekar Venkatachalam and M. Umamaheswari and Priyanka Shah and Arastu Thakur},
  doi          = {10.1142/S0218001424570155},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2457015},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Revolutionizing brain tumor diagnosis: A comprehensive model integrating VGG19 and LSTM for accurate MRI classification},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-frequency based EEG features for classification of
human emotions. <em>IJPRAI</em>, <em>39</em>(1), 2457014. (<a
href="https://doi.org/10.1142/S0218001424570143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotion classification without bias and unfairness is challenging because most existing image-based methods are directly or indirectly affected by subjectivity. Therefore, we propose an EEG (Electroencephalogram) based model for an accurate emotion classification without the effect of subjectivity. The captured EEG signals are converted into Delta, Theta, Alpha, Beta, and Gama frequency bands. As emotions change, the frequency bands change and provide unique patterns for each emotion irrespective of different persons. With this observation, the statical features, namely, mean, standard deviation, variance, and kurtosis, and frequency-based features, namely, Power Spectral Density (PSD) and Petrosian Fractal Dimension (PFD) are extracted. To integrate the strength of spatial and frequency-based features, the features are supplied to quadratic discriminative analysis for the final classification. The experiments on the benchmark datasets, DEAP and SEED-IV, achieve 99.40% and 91.97% accuracy, respectively. A comparison with state-of-the-art methods shows that the method performs very well on some datasets.},
  archive      = {J_IJPRAI},
  author       = {Shivanand S. Gornale and Shivakumara Palaiahnakote and Amruta Unki and Sunil Vadera},
  doi          = {10.1142/S0218001424570143},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2457014},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Spatial-frequency based EEG features for classification of human emotions},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization-enabled shepard convolutional quantum neural
network for brain tumor detection using MRI image. <em>IJPRAI</em>,
<em>39</em>(1), 2457013. (<a
href="https://doi.org/10.1142/S0218001424570131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors develop when abnormal cells grow within or near the brain. Determining the extent of the tumor is crucial for effective treatment. Magnetic Resonance Imaging (MRI) has emerged as a non-ionizing radiation tool for diagnosing brain tumors. Segmenting brain tumors manually is a tedious process so the performance depends on the experience of the operator. To overcome the above-mentioned problem, in this research project, brain tumor detection is classified by the proposed Shepard Convolutional Quantum Neural Network (ShCQNN) using MRI images. Initially, pre-processing of the input image is carried out with a Mean filter and the process of segmentation is executed by LinkNet. Here, the proposed Chicken Swarm Stock Exchange Trading Optimization (CSSETO) is used to train LinkNet. This CSSETO is formed from Stock Exchange Trading Optimization (SETO) and Chicken Swarm Optimization (CSO). Further, image augmentation includes rotation, random erasing, brightness or contrast adjustment, and shearing. Moreover, the extraction of features is done next to image augmentation, where some important features such as Local Ternary Pattern (LTP), Convolutional Neural Network (CNN), and Local Optimal Oriented Pattern (LOOP) are obtained. In the last stage, a brain tumor is detected using ShCQNN which is the amalgamation of Shepard Convolutional Neural Network (ShCNN) along with Quantum Neural Network (QNN). The two benchmark datasets, namely Multimodal Brain Tumor Segmentation Challenge 2018 (BraTS2018) database and the Figshare dataset are used to assess the performance of the proposed model using performance measures, such as specificity, accuracy, and sensitivity. Also, the performance of the proposed method has been compared with existing models, such as VGG Stacked Classifier Network (VGG-SCNet), Whale Harris Hawks Optimization (WHHO), CNN model, and EfficientNet-B0 and the results revealed that the proposed method provided superior performance than other existing methods. The proposed method obtains the accuracy of 0.925, sensitivity of 0.915, and specificity of 0.915. Regarding the accuracy, the performance improvement of the devised ShCQNN technique is 19.14%, 18.60%, 10.81%, and 2.70% higher than the existing methods VGG-SCNet, WHHO, CNN model, and EfficientNet-B0.},
  archive      = {J_IJPRAI},
  author       = {Swaminathan Balasubramanian and Veerraju Gampala and Alok Misra and Telu Venkata Madhusudhana Rao},
  doi          = {10.1142/S0218001424570131},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2457013},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Optimization-enabled shepard convolutional quantum neural network for brain tumor detection using MRI image},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and efficient wheat spike detection for small
targets. <em>IJPRAI</em>, <em>39</em>(1), 2455014. (<a
href="https://doi.org/10.1142/S0218001424550140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wheat spike detection is a crucial component of wheat yield prediction. In this study, n lightweight and efficient wheat spike detection model is proposed. The model employs a novel Wheat Spike Net Block (WSNB) within a lightweight network architecture, integrating Depth-Wise Convolution (DW-Conv) and Efficient Window Multi-Head Self-Attention (EW-MHSA) to rapidly process images and accurately identify wheat spikes, even under compact small target conditions. The model is equipped with four detection heads to effectively handle targets of varying scales and incorporates the innovative EMF-IOU loss function for refined bounding box estimation. Tested on a self-constructed Shangluo winter wheat dataset, the model achieves a detection speed of 96.1 FPS on NVIDIA Tesla V100 and mAP@0.5 of 95.3%, surpassing YOLOv5, EfficientV2, YOlOX,transformer, and MobileVIt3 in terms of accuracy and efficiency. The model’s performance across diverse hardware platforms highlights its potential for practical implementation in real-time wheat yield estimation and precision agriculture.},
  archive      = {J_IJPRAI},
  author       = {Bo Wang and Yawen Li and Jun Zhang and Liqiong Huang},
  doi          = {10.1142/S0218001424550140},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2455014},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Lightweight and efficient wheat spike detection for small targets},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel infogain and multi-axial wavelet-based transformer
for personality trait question answering. <em>IJPRAI</em>,
<em>39</em>(1), 2451023. (<a
href="https://doi.org/10.1142/S0218001424510236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is one of the attractive topics in the field of multimedia, affective, and empathic computing to garner user interest. Unlike existing models which aim at addressing challenges of VQA for the scene images, this work aims at developing a new model for Personality Trait Question Answering (PQA). It uses Twitter account information, which includes shared images, profile pictures, banners, text in the images, and descriptions of the images. Motivated by the accomplishments of the transformer, for encoding visual features of the images, a new InfoGain Multi-Axial Wavelet Vision Transformer (IgMaWaViT) is explored here. For encoding textual features in the images and descriptions, a new Information Gain BERT (InfoBert) method is introduced, which can handle the variable length encoding of text by choosing the optimal discriminator. Furthermore, the model fuses encodings of images and text according to the questions on different personality traits for question answering. The model is called InfoGain Multi-Axial Wavelet Vision Transformer for Personality Traits Question Answering (IgMaWaViT-PQA). To validate the efficacy of the proposed model, a dataset has been constructed, and it is used along with standard datasets for experimentation. Comprehensive experiments show that the proposed model is better than the state-of-the-art models. The code is available at the link: https://github.com/biswaskunal29/InfoGain_MultiAxial_PQA .},
  archive      = {J_IJPRAI},
  author       = {Kunal Biswas and Shivakumara Palaiahnakote and Saumik Bhattacharya and Umapada Pal and Ram Sarkar},
  doi          = {10.1142/S0218001424510236},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2451023},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {A novel infogain and multi-axial wavelet-based transformer for personality trait question answering},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of specialized deep-learning models for crop
freshness assessment to mitigate post-harvest loss. <em>IJPRAI</em>,
<em>39</em>(1), 2451022. (<a
href="https://doi.org/10.1142/S0218001424510224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for evaluating crop ripeness are critiqued for their inefficiency and potential harm to produce. The use of image-processing and deep-learning techniques can solve these issues as a trend in non-destructive methods. However, an overfitting problem arises when optimization and generalization are used to estimate the parameters of the next epoch. In this paper, we develop specialized models with a high volume of training images for a single type of crop to achieve the goal of 100% accuracy for both test and validation datasets. This development contributes insights into leveraging deep learning for crop assessment, emphasizing its potential application in diverse agricultural scenarios. Experimental results show that the proposed models are superior to several existing available methods.},
  archive      = {J_IJPRAI},
  author       = {Wellington Cunha and Arashdeep Kaur and Frank Y. Shih},
  doi          = {10.1142/S0218001424510224},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2451022},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Development of specialized deep-learning models for crop freshness assessment to mitigate post-harvest loss},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XAttentionHAR ensemble: Leveraging cross-modal attention for
enhanced activity recognition. <em>IJPRAI</em>, <em>39</em>(1), 2450026.
(<a href="https://doi.org/10.1142/S0218001424500265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) is pivotal in ubiquitous computing, offering benefits to human-centric services such as health monitoring, smart homes, and eldercare systems. HAR leverages smartphones, smartwatches, and other wearable devices to collect sensory data annotated with activity labels, which are then used to train machine learning or deep learning models for automatic activity recognition. Effective HAR systems must integrate information from multiple modalities to accurately assist users. This paper introduces the XAttentionHAR model, an innovative cross-modality attention-based ensemble model for HAR. Our approach utilizes a self-attention module to extract features within each modality and an inter-domain cross-attention module to capture and integrate long-term dependencies across domains. The cross-modality attention mechanism enhances the fusion of diverse modalities, enriching the semantic information. We conducted extensive experiments on the WISDM public dataset, which includes accelerometer and gyroscope data from smartwatches and smartphones. Our results demonstrate that XAttentionHAR outperforms other state-of-the-art methods in activity recognition, achieving 98.48% accuracy for smartphone-based HAR and 98.73% accuracy for smartwatch-based HAR, paving the way for improved human-centric services.},
  archive      = {J_IJPRAI},
  author       = {Sarita Sahni and Sweta Jain and Sri Khetwat Saritha},
  doi          = {10.1142/S0218001424500265},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2450026},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {XAttentionHAR ensemble: Leveraging cross-modal attention for enhanced activity recognition},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A newly adopted YOLOv9 model for detecting mould regions
inside of buildings. <em>IJPRAI</em>, <em>39</em>(1), 2450025. (<a
href="https://doi.org/10.1142/S0218001424500253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molds on wall and ceiling surfaces in damp indoor environments especially in houses with poor insulation and ventilation are common in the UK. Since it releases toxic chemicals as it grows, it is a serious health hazard for occupants who live in such houses. For example, eye irritation, sneezing, nose bleeds, respiratory infections, and skin irritations. Furthermore, there are chances of developing serious medical conditions like lung infections and respiratory diseases which may even lead to death. The main challenge here is that due to their irregular patterns, camouflaged with the background, it is not so easy to detect with our naked eyes in the early stage and often confused as stains. Therefore, inspired by the accomplishments of the Yolo architecture for object detection, the Yolov9 model is explored for mold detection by considering mold region as an object in this work. The overall result shows a promising 76% average classification rate. Since the mold does not have a shape, specific pattern, or color, adapting the Yolov9 for accurate mold detection is challenging. To the best of our knowledge, this is the first of its kind compared to existing methods. Since it is the first work, we constructed a dataset to perform experiments and evaluate the proposed method. To demonstrate the proposed method’s effectiveness, the results were also compared with the results of the Yolov8 and Yolov10 models.},
  archive      = {J_IJPRAI},
  author       = {Taha Mansouri and Md. Shadab Mashuk and Shivakumara Palaiahnakote and Aaron Chacko and Lawrence Sykes and Ali Alameer},
  doi          = {10.1142/S0218001424500253},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2450025},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {A newly adopted YOLOv9 model for detecting mould regions inside of buildings},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight safety activity recognition algorithm for
railway field personnel based on portable cards. <em>IJPRAI</em>,
<em>39</em>(1), 2450024. (<a
href="https://doi.org/10.1142/S0218001424500241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, railway construction has been plagued by frequent safety accidents, with workers’ safety during the construction process remaining a major concern. To mitigate this issue, intelligent monitoring of railway workers’ activity has been proposed as a means of improving the safety coefficient of construction. Human activity recognition (HAR) based on wearable devices holds significant application value in areas such as health monitoring, motion analysis, and intelligent assistance. Recently, convolutional neural networks (CNNs) have gained extensive adoption and demonstrated outstanding performance in HAR. However, current HAR research still faces some challenges, including problems with establishing spatial–temporal dependencies and addressing the demand for lightweight models. To address the above issues, we propose a lightweight dual-stream convolution model (LDSC) based on deformable convolution and hierarchical segmentation. The model adaptively captures significant variations in sensor readings over time from portable cards of railway personnel through a temporal stream and learns the interactive information among sensor channels over a spatial stream. LDSC consists of three lightweight convolutional modules that combine deep convolution and point convolution to reduce model parameters, thus meeting the demand for a lightweight model. Experiments and ablation studies are conducted on three available datasets (UCI-HAR, UniMiB-SHAR, and WISDM) to evaluate the proposed model. The experimental results indicate that our model outperforms existing state-of-the-art methods in terms of recognition accuracy, validating the effectiveness and feasibility of LDSC. In addition, theoretical analysis and ablation experiments demonstrate that the proposed LDSC embodies lightweight characteristics.},
  archive      = {J_IJPRAI},
  author       = {Hailu Zuo and Jiukai Deng and Zihan Liu and Guangjun Tian and Shuo Xiao},
  doi          = {10.1142/S0218001424500241},
  journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
  number       = {1},
  pages        = {2450024},
  shortjournal = {Int. J. Pattern Recognit. Artif. Intell.},
  title        = {Lightweight safety activity recognition algorithm for railway field personnel based on portable cards},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
