<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa---267">JASA - 267</h2>
<ul>
<li><details>
<summary>
(2024). Corrections to “spatio-temporal cross-covariance functions
under the lagrangian framework with multiple advections.” <em>JASA</em>,
<em>119</em>(548), 3189. (<a
href="https://doi.org/10.1080/01621459.2024.2412190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2024.2412190},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3189},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Corrections to “Spatio-temporal cross-covariance functions under the lagrangian framework with multiple advections”},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction. <em>JASA</em>, <em>119</em>(548), 3188. (<a
href="https://doi.org/10.1080/01621459.2024.2402959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2024.2402959},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3188},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial statistics for data science: Theory and practice
with r.,. <em>JASA</em>, <em>119</em>(548), 3186–3187. (<a
href="https://doi.org/10.1080/01621459.2024.2406581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Chae Young Lim},
  doi          = {10.1080/01621459.2024.2406581},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3186-3187},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatial statistics for data science: Theory and practice with r.,},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Models for multi-state survival data: Rates, risks, and
pseudo-values. <em>JASA</em>, <em>119</em>(548), 3184–3186. (<a
href="https://doi.org/10.1080/01621459.2024.2395590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ross L. Prentice},
  doi          = {10.1080/01621459.2024.2395590},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3184-3186},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Models for multi-state survival data: Rates, risks, and pseudo-values},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structural equation modeling using r/SAS: A step-by-step
approach with real data analysis. <em>JASA</em>, <em>119</em>(548),
3183–3184. (<a
href="https://doi.org/10.1080/01621459.2024.2309339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Lifeng Lin},
  doi          = {10.1080/01621459.2024.2309339},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3183-3184},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Structural equation modeling using R/SAS: A step-by-step approach with real data analysis},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian structure learning in undirected gaussian graphical
models: Literature review with empirical comparison. <em>JASA</em>,
<em>119</em>(548), 3164–3182. (<a
href="https://doi.org/10.1080/01621459.2024.2395504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models provide a powerful framework to reveal the conditional dependency structure between multivariate variables. The process of uncovering the conditional dependency network is known as structure learning. Bayesian methods can measure the uncertainty of conditional relationships and include prior information. However, frequentist methods are often preferred due to the computational burden of the Bayesian approach. Over the last decade, Bayesian methods have seen substantial improvements, with some now capable of generating accurate estimates of graphs up to a thousand variables in mere minutes. Despite these advancements, a comprehensive review or empirical comparison of all recent methods has not been conducted. This article delves into a wide spectrum of Bayesian approaches used for structure learning and evaluates their efficacy through a comprehensive simulation study. We also demonstrate how to apply Bayesian structure learning to a real-world dataset and provide directions for future research. This study gives an exhaustive overview of this dynamic field for newcomers, practitioners, and experts.},
  archive      = {J_JASA},
  author       = {Lucas Vogels and Reza Mohammadi and Marit Schoonhoven and Ş. İlker Birbil},
  doi          = {10.1080/01621459.2024.2395504},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3164-3182},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian structure learning in undirected gaussian graphical models: Literature review with empirical comparison},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated offline reinforcement learning. <em>JASA</em>,
<em>119</em>(548), 3152–3163. (<a
href="https://doi.org/10.1080/01621459.2024.2310287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based or data-driven dynamic treatment regimes are essential for personalized medicine, which can benefit from offline reinforcement learning (RL). Although massive healthcare data are available across medical institutions, they are prohibited from sharing due to privacy constraints. Besides, heterogeneity exists in different sites. As a result, federated offline RL algorithms are necessary and promising to deal with the problems. In this article, we propose a multi-site Markov decision process model that allows for both homogeneous and heterogeneous effects across sites. The proposed model makes the analysis of the site-level features possible. We design the first federated policy optimization algorithm for offline RL with sample complexity. The proposed algorithm is communication-efficient, which requires only a single round of communication interaction by exchanging summary statistics. We give a theoretical guarantee for the proposed algorithm, where the suboptimality for the learned policies is comparable to the rate as if data is not distributed. Extensive simulations demonstrate the effectiveness of the proposed algorithm. The method is applied to a sepsis dataset in multiple sites to illustrate its use in clinical settings. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Doudou Zhou and Yufeng Zhang and Aaron Sonabend-W and Zhaoran Wang and Junwei Lu and Tianxi Cai},
  doi          = {10.1080/01621459.2024.2310287},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3152-3163},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Federated offline reinforcement learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent community detection in inter-layer dependent
multi-layer networks. <em>JASA</em>, <em>119</em>(548), 3141–3151. (<a
href="https://doi.org/10.1080/01621459.2024.2308848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in multi-layer networks, which aims at finding groups of nodes with similar connective patterns among all layers, has attracted tremendous interests in multi-layer network analysis. Most existing methods are extended from those for single-layer networks, which assume that different layers are independent. In this article, we propose a novel community detection method in multi-layer networks with inter-layer dependence, which integrates the stochastic block model (SBM) and the Ising model. The community structure is modeled by the SBM model and the inter-layer dependence is incorporated via the Ising model. An efficient alternative updating algorithm is developed to tackle the resultant optimization task. Moreover, the asymptotic consistencies of the proposed method in terms of both parameter estimation and community detection are established, which are supported by extensive simulated examples and a real example on a multi-layer malaria parasite gene network. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jingnan Zhang and Junhui Wang and Xueqin Wang},
  doi          = {10.1080/01621459.2024.2308848},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3141-3151},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Consistent community detection in inter-layer dependent multi-layer networks},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric bayesian inference for local extrema of
functions in the presence of noise. <em>JASA</em>, <em>119</em>(548),
3127–3140. (<a
href="https://doi.org/10.1080/01621459.2024.2308333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a wide range of applications where the local extrema of a function are the key quantity of interest. However, there is surprisingly little work on methods to infer local extrema with uncertainty quantification in the presence of noise. By viewing the function as an infinite-dimensional nuisance parameter, a semiparametric formulation of this problem poses daunting challenges, both methodologically and theoretically, as (i) the number of local extrema may be unknown, and (ii) the induced shape constraints associated with local extrema are highly irregular. In this article, we build upon a derivative-constrained Gaussian process prior recently proposed by Yu et al. to derive what we call an encompassing approach that indexes possibly multiple local extrema by a single parameter. We provide closed-form characterization of the posterior distribution and study its large sample behavior under this unconventional encompassing regime. We show that the posterior measure converges to a mixture of Gaussians with the number of components matching the underlying truth, leading to posterior exploration that accounts for multi-modality. Point and interval estimates of local extrema with frequentist properties are also provided. The encompassing approach leads to a remarkably simple, fast semiparametric approach for inference on local extrema. We illustrate the method through simulations and a real data application to event-related potential analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Meng Li and Zejian Liu and Cheng-Han Yu and Marina Vannucci},
  doi          = {10.1080/01621459.2024.2308333},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3127-3140},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric bayesian inference for local extrema of functions in the presence of noise},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning in latent heterogeneous environments.
<em>JASA</em>, <em>119</em>(548), 3113–3126. (<a
href="https://doi.org/10.1080/01621459.2024.2308317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning holds great promise for data-driven decision-making in various social contexts, including healthcare, education, and business. However, classical methods that focus on the mean of the total return may yield misleading results when dealing with heterogeneous populations typically found in large-scale datasets. To address this issue, we introduce the K -Value Heterogeneous Markov Decision Process, a framework designed to handle sequential decision problems with latent population heterogeneity. Within this framework, we propose auto-clustered policy evaluation for estimating the value of a given policy and auto-clustered policy iteration for estimating the optimal policy within a parametric policy class. Our auto-clustered algorithms can automatically identify homogeneous subpopulations while simultaneously estimating the action value function and the optimal policy for each subgroup. We establish convergence rates and construct confidence intervals for the estimators. Simulation results support our theoretical findings, and an empirical study conducted on a real medical dataset confirms the presence of value heterogeneity and validates the advantages of our novel approach. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Elynn Y. Chen and Rui Song and Michael I. Jordan},
  doi          = {10.1080/01621459.2024.2308317},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3113-3126},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Reinforcement learning in latent heterogeneous environments},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifiability and consistent estimation for gaussian chain
graph models. <em>JASA</em>, <em>119</em>(548), 3101–3112. (<a
href="https://doi.org/10.1080/01621459.2024.2304692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The chain graph model admits both undirected and directed edges in one graph, where symmetric conditional dependencies are encoded via undirected edges and asymmetric causal relations are encoded via directed edges. Though frequently encountered in practice, the chain graph model has been largely under investigated in the literature, possibly due to the lack of identifiability conditions between undirected and directed edges. In this article, we first establish a set of novel identifiability conditions for the Gaussian chain graph model, exploiting a low rank plus sparse decomposition of the precision matrix. Further, an efficient learning algorithm is built upon the identifiability conditions to fully recover the chain graph structure. Theoretical analysis on the proposed method is conducted, assuring its asymptotic consistency in recovering the exact chain graph structure. The advantage of the proposed method is also supported by numerical experiments on both simulated examples and a real application on the Standard &amp; Poor 500 index data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ruixuan Zhao and Haoran Zhang and Junhui Wang},
  doi          = {10.1080/01621459.2024.2304692},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3101-3112},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Identifiability and consistent estimation for gaussian chain graph models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A composite likelihood-based approach for change-point
detection in spatio-temporal processes. <em>JASA</em>,
<em>119</em>(548), 3086–3100. (<a
href="https://doi.org/10.1080/01621459.2024.2302200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a unified and computationally efficient method for change-point estimation along the time dimension in a nonstationary spatio-temporal process. By modeling a nonstationary spatio-temporal process as a piecewise stationary spatio-temporal process, we consider simultaneous estimation of the number and locations of change-points, and model parameters in each segment. A composite likelihood-based criterion is developed for change-point and parameter estimation. Under the framework of increasing domain asymptotics, theoretical results including consistency and distribution of the estimators are derived under mild conditions. In contrast to classical results in fixed dimensional time series that the localization error of change-point estimator is O p ( 1 ) , exact recovery of true change-points is possible in the spatio-temporal setting. More surprisingly, the consistency of change-point estimation can be achieved without any penalty term in the criterion function. In addition, we further establish consistency of the change-point estimator under the infill asymptotics framework where the time domain is increasing while the spatial sampling domain is fixed. A computationally efficient pruned dynamic programming algorithm is developed for the challenging criterion optimization problem. Extensive simulation studies and an application to the U.S. precipitation data are provided to demonstrate the effectiveness and practicality of the proposed method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zifeng Zhao and Ting Fung Ma and Wai Leong Ng and Chun Yip Yau},
  doi          = {10.1080/01621459.2024.2302200},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3086-3100},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A composite likelihood-based approach for change-point detection in spatio-temporal processes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphical principal component analysis of multivariate
functional time series. <em>JASA</em>, <em>119</em>(548), 3073–3085. (<a
href="https://doi.org/10.1080/01621459.2024.2302198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider multivariate functional time series with a two-way dependence structure: a serial dependence across time points and a graphical interaction among the multiple functions within each time point. We develop the notion of dynamic weak separability, a more general condition than those assumed in literature, and use it to characterize the two-way structure in multivariate functional time series. Based on the proposed weak separability, we develop a unified framework for functional graphical models and dynamic principal component analysis, and further extend it to optimally reconstruct signals from contaminated functional data using graphical-level information. We investigate asymptotic properties of the resulting estimators and illustrate the effectiveness of our proposed approach through extensive simulations. We apply our method to hourly air pollution data that were collected from a monitoring network in China. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianbin Tan and Decai Liang and Yongtao Guan and Hui Huang},
  doi          = {10.1080/01621459.2024.2302198},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3073-3085},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Graphical principal component analysis of multivariate functional time series},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extremal random forests. <em>JASA</em>, <em>119</em>(548),
3059–3072. (<a
href="https://doi.org/10.1080/01621459.2023.2300522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical methods for quantile regression fail in cases where the quantile of interest is extreme and only few or no training data points exceed it. Asymptotic results from extreme value theory can be used to extrapolate beyond the range of the data, and several approaches exist that use linear regression, kernel methods or generalized additive models. Most of these methods break down if the predictor space has more than a few dimensions or if the regression function of extreme quantiles is complex. We propose a method for extreme quantile regression that combines the flexibility of random forests with the theory of extrapolation. Our extremal random forest (ERF) estimates the parameters of a generalized Pareto distribution, conditional on the predictor vector, by maximizing a local likelihood with weights extracted from a quantile random forest. We penalize the shape parameter in this likelihood to regularize its variability in the predictor space. Under general domain of attraction conditions, we show consistency of the estimated parameters in both the unpenalized and penalized case. Simulation studies show that our ERF outperforms both classical quantile regression methods and existing regression approaches from extreme value theory. We apply our methodology to extreme quantile prediction for U.S. wage data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Nicola Gnecco and Edossa Merga Terefe and Sebastian Engelke},
  doi          = {10.1080/01621459.2023.2300522},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3059-3072},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Extremal random forests},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy learning with asymmetric counterfactual utilities.
<em>JASA</em>, <em>119</em>(548), 3045–3058. (<a
href="https://doi.org/10.1080/01621459.2023.2300507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven decision making plays an important role even in high stakes settings like medicine and public policy. Learning optimal policies from observed data requires a careful formulation of the utility function whose expected value is maximized across a population. Although researchers typically use utilities that depend on observed outcomes alone, in many settings the decision maker’s utility function is more properly characterized by the joint set of potential outcomes under all actions. For example, the Hippocratic principle to “do no harm” implies that the cost of causing death to a patient who would otherwise survive without treatment is greater than the cost of forgoing life-saving treatment. We consider optimal policy learning with asymmetric counterfactual utility functions of this form that consider the joint set of potential outcomes. We show that asymmetric counterfactual utilities lead to an unidentifiable expected utility function, and so we first partially identify it. Drawing on statistical decision theory, we then derive minimax decision rules by minimizing the maximum expected utility loss relative to different alternative policies. We show that one can learn minimax loss decision rules from observed data by solving intermediate classification problems, and establish that the finite sample excess expected utility loss of this procedure is bounded by the regret of these intermediate classifiers. We apply this conceptual framework and methodology to the decision about whether or not to use right heart catheterization for patients with possible pulmonary hypertension. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Eli Ben-Michael and Kosuke Imai and Zhichao Jiang},
  doi          = {10.1080/01621459.2023.2300507},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3045-3058},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Policy learning with asymmetric counterfactual utilities},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust validation: Confident predictions even when
distributions shift. <em>JASA</em>, <em>119</em>(548), 3033–3044. (<a
href="https://doi.org/10.1080/01621459.2023.2298037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the traditional viewpoint in machine learning and statistics assumes training and testing samples come from the same population, practice belies this fiction. One strategy—coming from robust statistics and optimization—is thus to build a model robust to distributional perturbations. In this article, we take a different approach to describe procedures for robust predictive inference, where a model provides uncertainty estimates on its predictions rather than point predictions. We present a method that produces prediction sets (almost exactly) giving the right coverage level for any test distribution in an f -divergence ball around the training population. The method, based on conformal inference, achieves (nearly) valid coverage in finite samples, under only the condition that the training data be exchangeable. An essential component of our methodology is to estimate the amount of expected future data shift and build robustness to it; we develop estimators and prove their consistency for protection and validity of uncertainty estimates under shifts. By experimenting on several large-scale benchmark datasets, including Recht et al.’s CIFAR-v4 and ImageNet-V2 datasets, we provide complementary empirical results that highlight the importance of robust predictive validity. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Maxime Cauchois and Suyash Gupta and Alnur Ali and John C. Duchi},
  doi          = {10.1080/01621459.2023.2298037},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3033-3044},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust validation: Confident predictions even when distributions shift},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A kernel measure of dissimilarity between m distributions.
<em>JASA</em>, <em>119</em>(548), 3020–3032. (<a
href="https://doi.org/10.1080/01621459.2023.2298036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given 𝑀 ≥ 2 distributions defined on a general measurable space, we introduce a nonparametric (kernel) measure of multi-sample dissimilarity (KMD)—a parameter that quantifies the difference between the M distributions. The population KMD, which takes values between 0 and 1, is 0 if and only if all the M distributions are the same, and 1 if and only if all the distributions are mutually singular. Moreover, KMD possesses many properties commonly associated with f -divergences such as the data processing inequality and invariance under bijective transformations. The sample estimate of KMD, based on independent observations from the M distributions, can be computed in near linear time (up to logarithmic factors) using k -nearest neighbor graphs (for k ≥ 1 fixed). We develop an easily implementable test for the equality of M distributions based on the sample KMD that is consistent against all alternatives where at least two distributions are not equal. We prove central limit theorems for the sample KMD, and provide a complete characterization of the asymptotic power of the test, as well as its detection threshold. The usefulness of our measure is demonstrated via real and synthetic data examples; our method is also implemented in an R package. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhen Huang and Bodhisattva Sen},
  doi          = {10.1080/01621459.2023.2298036},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3020-3032},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A kernel measure of dissimilarity between m distributions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering high-dimensional noisy categorical data.
<em>JASA</em>, <em>119</em>(548), 3008–3019. (<a
href="https://doi.org/10.1080/01621459.2023.2298028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a widely used unsupervised learning technique that groups data into homogeneous clusters. However, when dealing with real-world data that contain categorical values, existing algorithms can be computationally costly in high dimensions and can struggle with noisy data that has missing values. Furthermore, except for one algorithm, no others provide theoretical guarantees of clustering accuracy. In this article, we propose a general categorical data encoding method and a computationally efficient spectral-based algorithm to cluster high-dimensional noisy categorical data (nominal or ordinal). Under a statistical model for data on m attributes from n subjects in r clusters with missing probability ϵ , we show that our algorithm exactly recovers the true clusters with high probability when m n ( 1 − ϵ ) ≥ C M r 2 log 3 M , with M = max ( n , m ) and a fixed constant C . In addition, we show that m n ( 1 − ϵ ) 2 ≥ r δ / 2 with 0 &lt; δ &lt; 1 is necessary for any algorithm to succeed with probability at least ( 1 + δ ) / 2 . In cases where m = n and r are fixed, the sufficient condition matches with the necessary condition up to a polylog ( n ) factor. In numerical studies our algorithm outperforms several existing algorithms in both clustering accuracy and computational efficiency. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhiyi Tian and Jiaming Xu and Jen Tang},
  doi          = {10.1080/01621459.2023.2298028},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {3008-3019},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Clustering high-dimensional noisy categorical data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic matrix recovery. <em>JASA</em>, <em>119</em>(548),
2996–3007. (<a
href="https://doi.org/10.1080/01621459.2023.2297468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix recovery from sparse observations is an extensively studied topic emerging in various applications, such as recommendation system and signal processing, which includes the matrix completion and compressed sensing models as special cases. In this article, we propose a general framework for dynamic matrix recovery of low-rank matrices that evolve smoothly over time. We start from the setting that the observations are independent across time, then extend to the setting that both the design matrix and noise possess certain temporal correlation via modified concentration inequalities. By pooling neighboring observations, we obtain sharp estimation error bounds of both settings, showing the influence of the underlying smoothness, the dependence and effective samples. We propose a dynamic fast iterative shrinkage-thresholding algorithm that is computationally efficient, and characterize the interplay between algorithmic and statistical convergence. Simulated and real data examples are provided to support such findings. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ziyuan Chen and Ying Yang and Fang Yao},
  doi          = {10.1080/01621459.2023.2297468},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2996-3007},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Dynamic matrix recovery},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fitting latent non-gaussian models using variational bayes
and laplace approximations. <em>JASA</em>, <em>119</em>(548), 2983–2995.
(<a href="https://doi.org/10.1080/01621459.2023.2296704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Gaussian models (LGMs) are perhaps the most commonly used class of models in statistical applications. Nevertheless, in areas ranging from longitudinal studies in biostatistics to geostatistics, it is easy to find datasets that contain inherently non-Gaussian features, such as sudden jumps or spikes, that adversely affect the inferences and predictions made using an LGM. These datasets require more general latent non-Gaussian models (LnGMs) that can handle automatically these non-Gaussian features. However, fast implementation and easy-to-use software are lacking, preventing the broad applicability of LnGMs. In this article, we derive variational Bayes algorithms for fast and scalable inference of LnGMs. The approximation leads to an LGM that downweights extreme events in the latent process, reducing their influence and leading to more robust inferences. It can be applied to a wide range of models, such as autoregressive processes for time series, simultaneous autoregressive models for areal data, and spatial Matérn models. To facilitate Bayesian inference, we introduce the ngvb package, where LGMs implemented in R-INLA can be easily extended to LnGMs by adding a single line of code. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Rafael Cabral and David Bolin and Håvard Rue},
  doi          = {10.1080/01621459.2023.2296704},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2983-2995},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fitting latent non-gaussian models using variational bayes and laplace approximations},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online statistical inference for stochastic optimization via
kiefer-wolfowitz methods. <em>JASA</em>, <em>119</em>(548), 2972–2982.
(<a href="https://doi.org/10.1080/01621459.2023.2296703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of online statistical inference of model parameters in stochastic optimization problems via the Kiefer-Wolfowitz algorithm with random search directions. We first present the asymptotic distribution for the Polyak-Ruppert-averaging type Kiefer-Wolfowitz (AKW) estimators, whose asymptotic covariance matrices depend on the distribution of search directions and the function-value query complexity. The distributional result reflects the tradeoff between statistical efficiency and function query complexity. We further analyze the choice of random search directions to minimize certain summary statistics of the asymptotic covariance matrix. Based on the asymptotic distribution, we conduct online statistical inference by providing two construction procedures of valid confidence intervals. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xi Chen and Zehua Lai and He Li and Yichen Zhang},
  doi          = {10.1080/01621459.2023.2296703},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2972-2982},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Online statistical inference for stochastic optimization via kiefer-wolfowitz methods},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-robust and efficient covariate adjustment for
cluster-randomized experiments. <em>JASA</em>, <em>119</em>(548),
2959–2971. (<a
href="https://doi.org/10.1080/01621459.2023.2289693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster-randomized experiments are increasingly used to evaluate interventions in routine practice conditions, and researchers often adopt model-based methods with covariate adjustment in the statistical analyses. However, the validity of model-based covariate adjustment remains unclear when the working models are misspecified, leading to ambiguity of estimands and risk of bias. In this article, we first adapt two model-based methods—generalized estimating equations and linear mixed models—with weighted g-computation to achieve robust inference for cluster-average and individual-average treatment effects. To further overcome the limitations of model-based covariate adjustment methods, we propose efficient estimators for each estimand that allow for flexible covariate adjustment and additionally address cluster size variation dependent on treatment assignment and other cluster characteristics. Such cluster size variations often occur post-randomization and, if ignored, can lead to bias of model-based estimators. For our proposed covariate-adjusted estimators, we prove that when the nuisance functions are consistently estimated by machine learning algorithms, the estimators are consistent, asymptotically normal, and efficient. When the nuisance functions are estimated via parametric working models, the estimators are triply-robust. Simulation studies and analyses of three real-world cluster-randomized experiments demonstrate that the proposed methods are superior to existing alternatives. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Bingkai Wang and Chan Park and Dylan S. Small and Fan Li},
  doi          = {10.1080/01621459.2023.2289693},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2959-2971},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-robust and efficient covariate adjustment for cluster-randomized experiments},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounding wasserstein distance with couplings. <em>JASA</em>,
<em>119</em>(548), 2947–2958. (<a
href="https://doi.org/10.1080/01621459.2023.2287773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates of intractable posterior expectations as the number of iterations tends to infinity. However, in large data applications, MCMC can be computationally expensive per iteration. This has catalyzed interest in approximating MCMC in a manner that improves computational speed per iteration but does not produce asymptotically consistent estimates. In this article, we propose estimators based on couplings of Markov chains to assess the quality of such asymptotically biased sampling methods. The estimators give empirical upper bounds of the Wasserstein distance between the limiting distribution of the asymptotically biased sampling method and the original target distribution of interest. We establish theoretical guarantees for our upper bounds and show that our estimators can remain effective in high dimensions. We apply our quality measures to stochastic gradient MCMC, variational Bayes, and Laplace approximations for tall data and to approximate MCMC for Bayesian logistic regression in 4500 dimensions and Bayesian linear regression in 50,000 dimensions.},
  archive      = {J_JASA},
  author       = {Niloy Biswas and Lester Mackey},
  doi          = {10.1080/01621459.2023.2287773},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2947-2958},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bounding wasserstein distance with couplings},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balancing covariates in randomized experiments with the
gram–schmidt walk design. <em>JASA</em>, <em>119</em>(548), 2934–2946.
(<a href="https://doi.org/10.1080/01621459.2023.2285474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of experiments involves a compromise between covariate balance and robustness. This article provides a formalization of this tradeoff and describes an experimental design that allows experimenters to navigate it. The design is specified by a robustness parameter that bounds the worst-case mean squared error of an estimator of the average treatment effect. Subject to the experimenter’s desired level of robustness, the design aims to simultaneously balance all linear functions of potentially many covariates. Less robustness allows for more balance. We show that the mean squared error of the estimator is bounded in finite samples by the minimum of the loss function of an implicit ridge regression of the potential outcomes on the covariates. Asymptotically, the design perfectly balances all linear functions of a growing number of covariates with a diminishing reduction in robustness, effectively allowing experimenters to escape the compromise between balance and robustness in large samples. Finally, we describe conditions that ensure asymptotic normality and provide a conservative variance estimator, which facilitate the construction of asymptotically valid confidence intervals. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Christopher Harshaw and Fredrik Sävje and Daniel A. Spielman and Peng Zhang},
  doi          = {10.1080/01621459.2023.2285474},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2934-2946},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Balancing covariates in randomized experiments with the Gram–Schmidt walk design},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-distance correlation and its applications.
<em>JASA</em>, <em>119</em>(548), 2919–2933. (<a
href="https://doi.org/10.1080/01621459.2023.2284988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new measure of dependence between a categorical random variable and a random vector with potentially high dimensions, named semi-distance correlation. It is an interesting extension of distance correlation to accommodate the information of the categorical random variable. It equals zero if and only if the categorical random variable and the other random vector are independent. Two important applications of semi-distance correlation are considered. First, we develop a semi-distance independence test between a categorical random variable and a random vector and derive its asymptotic distributions. When the dimension of the random vector tends to infinity, we derive the explicit asymptotic normal distribution of the test statistic under the null hypothesis, which allows us to compute p -values in an efficient and fast way for high dimensional data. Second, we propose to use the semi-distance correlation as a marginal utility between the response and a group of covariates to do groupwise variable screening for ultrahigh dimensional classification problems. The sure screening property has also been established. Monte Carlo simulations and a real data application are presented to demonstrate the excellent finite sample property of the proposed procedures. A new R package semidist is also developed to implement the proposed methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wei Zhong and Zhuoxi Li and Wenwen Guo and Hengjian Cui},
  doi          = {10.1080/01621459.2023.2284988},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2919-2933},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semi-distance correlation and its applications},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap inference in the presence of bias. <em>JASA</em>,
<em>119</em>(548), 2908–2918. (<a
href="https://doi.org/10.1080/01621459.2023.2284980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider bootstrap inference for estimators which are (asymptotically) biased. We show that, even when the bias term cannot be consistently estimated, valid inference can be obtained by proper implementations of the bootstrap. Specifically, we show that the prepivoting approach of Beran, originally proposed to deliver higher-order refinements, restores bootstrap validity by transforming the original bootstrap p -value into an asymptotically uniform random variable. We propose two different implementations of prepivoting (plug-in and double bootstrap), and provide general high-level conditions that imply validity of bootstrap inference. To illustrate the practical relevance and implementation of our results, we discuss five examples: (i) inference on a target parameter based on model averaging; (ii) ridge-type regularized estimators; (iii) nonparametric regression; (iv) a location model for infinite variance data; and (v) dynamic panel data models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Giuseppe Cavaliere and Sílvia Gonçalves and Morten Ørregaard Nielsen and Edoardo Zanelli},
  doi          = {10.1080/01621459.2023.2284980},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2908-2918},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bootstrap inference in the presence of bias},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online regularization toward always-valid high-dimensional
dynamic pricing. <em>JASA</em>, <em>119</em>(548), 2895–2907. (<a
href="https://doi.org/10.1080/01621459.2023.2284979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {– Devising a dynamic pricing policy with always valid online statistical learning procedures is an important and as yet unresolved problem. Most existing dynamic pricing policies, which focus on the faithfulness of adopted customer choice models, exhibit a limited capability for adapting to the online uncertainty of learned statistical models during the pricing process. In this article, we propose a novel approach for designing a dynamic pricing policy based on regularized online statistical learning with theoretical guarantees. The new approach overcomes the challenge of continuous monitoring of the online Lasso procedure and possesses several appealing properties. In particular, we make the decisive observation that the always-validity of pricing decisions builds and thrives on the online regularization scheme. Our proposed online regularization scheme equips the proposed optimistic online regularized maximum likelihood pricing ( OORMLP ) pricing policy with three major advantages: encode market noise knowledge into pricing process optimism; empower online statistical learning with always-validity overall decision points; envelope prediction error process with time-uniform non-asymptotic oracle inequalities. This type of non-asymptotic inference results allows us to design more sample-efficient and robust dynamic pricing algorithms in practice. In theory, the proposed OORMLP algorithm exploits the sparsity structure of high-dimensional models and secures a logarithmic regret in a decision horizon. These theoretical advances are made possible by proposing an optimistic online Lasso procedure that resolves dynamic pricing problems at the process level, based on a novel use of non-asymptotic martingale concentration. In experiments, we evaluate OORMLP in different synthetic and real pricing problem settings and demonstrate that OORMLP advances the state-of-the-art methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chi-Hua Wang and Zhanyu Wang and Will Wei Sun and Guang Cheng},
  doi          = {10.1080/01621459.2023.2284979},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2895-2907},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Online regularization toward always-valid high-dimensional dynamic pricing},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimum resource threshold policy under partial
interference. <em>JASA</em>, <em>119</em>(548), 2881–2894. (<a
href="https://doi.org/10.1080/01621459.2023.2284422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When developing policies for prevention of infectious diseases, policymakers often set specific, outcome-oriented targets to achieve. For example, when developing a vaccine allocation policy, policymakers may want to distribute them so that at least a certain fraction of individuals in a census block are disease-free and spillover effects due to interference within blocks are accounted for. The article proposes methods to estimate a block-level treatment policy that achieves a predefined, outcome-oriented target while accounting for spillover effects due to interference. Our policy, the minimum resource threshold policy (MRTP), suggests the minimum fraction of treated units required within a block to meet or exceed the target level of the outcome. We estimate the MRTP from empirical risk minimization using a novel, nonparametric, doubly robust loss function. We then characterize statistical properties of the estimated MRTP in terms of the excess risk bound. We apply our methodology to design a water, sanitation, and hygiene allocation policy for Senegal with the goal of increasing the proportion of households with no children experiencing diarrhea to a level exceeding a specified threshold. Our policy outperforms competing policies and offers new approaches to design allocation policies, especially in international development for communicable diseases. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chan Park and Guanhua Chen and Menggang Yu and Hyunseung Kang},
  doi          = {10.1080/01621459.2023.2284422},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2881-2894},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Minimum resource threshold policy under partial interference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference with noncompliance and unknown
interference. <em>JASA</em>, <em>119</em>(548), 2869–2880. (<a
href="https://doi.org/10.1080/01621459.2023.2284413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a causal inference model in which individuals interact in a social network and they may not comply with the assigned treatments. In particular, we suppose that the form of network interference is unknown to researchers. To estimate meaningful causal parameters in this situation, we introduce a new concept of exposure mapping, which summarizes potentially complicated spillover effects into a fixed dimensional statistic of instrumental variables. We investigate identification conditions for the intention-to-treat effects and the average treatment effects for compliers, while explicitly considering the possibility of misspecification of exposure mapping. Based on our identification results, we develop nonparametric estimation procedures via inverse probability weighting. Their asymptotic properties, including consistency and asymptotic normality, are investigated using an approximate neighborhood interference framework. For an empirical illustration, we apply our method to experimental data on the anti-conflict intervention school program. The proposed methods are readily available with the companion R package latenetwork . Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tadao Hoshino and Takahide Yanagi},
  doi          = {10.1080/01621459.2023.2284413},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2869-2880},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Causal inference with noncompliance and unknown interference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decorrelating and debiasing approach to simultaneous
inference for high-dimensional confounded models. <em>JASA</em>,
<em>119</em>(548), 2857–2868. (<a
href="https://doi.org/10.1080/01621459.2023.2283938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the simultaneous association analysis with the presence of latent confounders, this article studies the large-scale hypothesis testing problem for the high-dimensional confounded linear models with both non-asymptotic and asymptotic false discovery control. Such model covers a wide range of practical settings where both the response and the predictors may be confounded. In the presence of the high-dimensional predictors and the unobservable confounders, the simultaneous inference with provable guarantees becomes highly challenging, and the unknown strong dependence among the confounded covariates makes the challenge even more pronounced. This article first introduces a decorrelating procedure that shrinks the confounding effect and weakens the correlations among the predictors, then performs debiasing under the decorrelated design based on some biased initial estimator. Following that, an asymptotic normality result for the debiased estimator is established and standardized test statistics are then constructed. Furthermore, a simultaneous inference procedure is proposed to identify significant associations, and both the finite-sample and asymptotic false discovery bounds are provided. The non-asymptotic result is general and model-free, and is of independent interest. We also prove that, under minimal signal strength condition, all associations can be successfully detected with probability tending to one. Simulation and real data studies are carried out to evaluate the performance of the proposed approach and compare it with other competing methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yinrui Sun and Li Ma and Yin Xia},
  doi          = {10.1080/01621459.2023.2283938},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2857-2868},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A decorrelating and debiasing approach to simultaneous inference for high-dimensional confounded models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal subsampling via predictive inference. <em>JASA</em>,
<em>119</em>(548), 2844–2856. (<a
href="https://doi.org/10.1080/01621459.2023.2282644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the big data era, subsampling or sub-data selection techniques are often adopted to extract a fraction of informative individuals from the massive data. Existing subsampling algorithms focus mainly on obtaining a representative subset to achieve the best estimation accuracy under a given class of models. In this article, we consider a semi-supervised setting wherein a small or moderate sized “labeled” data is available in addition to a much larger sized “unlabeled” data. The goal is to sample from the unlabeled data with a given budget to obtain informative individuals that are characterized by their unobserved responses. We propose an optimal subsampling procedure that is able to maximize the diversity of the selected subsample and control the false selection rate (FSR) simultaneously, allowing us to explore reliable information as much as possible. The key ingredients of our method are the use of predictive inference for quantifying the uncertainty of response predictions and a reformulation of the objective into a constrained optimization problem. We show that the proposed method is asymptotically optimal in the sense that the diversity of the subsample converges to its oracle counterpart with FSR control. Numerical simulations and a real-data example validate the superior performance of the proposed strategy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiaoyang Wu and Yuyang Huo and Haojie Ren and Changliang Zou},
  doi          = {10.1080/01621459.2023.2282644},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2844-2856},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal subsampling via predictive inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian spline-based hidden markov models with applications
to actimetry data and sleep analysis. <em>JASA</em>, <em>119</em>(548),
2833–2843. (<a
href="https://doi.org/10.1080/01621459.2023.2279707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {B-spline-based hidden Markov models employ B-splines to specify the emission distributions, offering a more flexible modeling approach to data than conventional parametric HMMs. We introduce a Bayesian framework for inference, enabling the simultaneous estimation of all unknown model parameters including the number of states. A parsimonious knot configuration of the B-splines is identified by the use of a trans-dimensional Markov chain sampling algorithm, while model selection regarding the number of states can be performed based on the marginal likelihood within a parallel sampling framework. Using extensive simulation studies, we demonstrate the superiority of our methodology over alternative approaches as well as its robustness and scalability. We illustrate the explorative use of our methods for data on activity in animals, that is whitetip-sharks. The flexibility of our Bayesian approach also facilitates the incorporation of more realistic assumptions and we demonstrate this by developing a novel hierarchical conditional HMM to analyse human activity for circadian and sleep modeling. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sida Chen and Bärbel Finkenstädt},
  doi          = {10.1080/01621459.2023.2279707},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2833-2843},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian spline-based hidden markov models with applications to actimetry data and sleep analysis},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Split knockoffs for multiple comparisons: Controlling the
directional false discovery rate. <em>JASA</em>, <em>119</em>(548),
2822–2832. (<a
href="https://doi.org/10.1080/01621459.2023.2279292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple comparisons in hypothesis testing often encounter structural constraints in various applications. For instance, in structural Magnetic Resonance Imaging for Alzheimer’s Disease, the focus extends beyond examining atrophic brain regions to include comparisons of anatomically adjacent regions. These constraints can be modeled as linear transformations of parameters, where the sign patterns play a crucial role in estimating directional effects. This class of problems, encompassing total variations, wavelet transforms, fused LASSO, trend filtering, and more, presents an open challenge in effectively controlling the directional false discovery rate. In this article, we propose an extended Split Knockoff method specifically designed to address the control of directional false discovery rate under linear transformations. Our proposed approach relaxes the stringent linear manifold constraint to its neighborhood, employing a variable splitting technique commonly used in optimization. This methodology yields an orthogonal design that benefits both power and directional false discovery rate control. By incorporating a sample splitting scheme, we achieve effective control of the directional false discovery rate, with a notable reduction to zero as the relaxed neighborhood expands. To demonstrate the efficacy of our method, we conduct simulation experiments and apply it to two real-world scenarios: Alzheimer’s Disease analysis and human age comparisons. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yang Cao and Xinwei Sun and Yuan Yao},
  doi          = {10.1080/01621459.2023.2279292},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2822-2832},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Split knockoffs for multiple comparisons: Controlling the directional false discovery rate},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust interval estimation for optimal policy
evaluation in online learning. <em>JASA</em>, <em>119</em>(548),
2811–2821. (<a
href="https://doi.org/10.1080/01621459.2023.2279289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the performance of an ongoing policy plays a vital role in many areas such as medicine and economics, to provide crucial instructions on the early-stop of the online experiment and timely feedback from the environment. Policy evaluation in online learning thus attracts increasing attention by inferring the mean outcome of the optimal policy (i.e., the value) in real-time. Yet, such a problem is particularly challenging due to the dependent data generated in the online environment, the unknown optimal policy, and the complex exploration and exploitation tradeoff in the adaptive experiment. In this article, we aim to overcome these difficulties in policy evaluation for online learning. We explicitly derive the probability of exploration that quantifies the probability of exploring nonoptimal actions under commonly used bandit algorithms. We use this probability to conduct valid inference on the online conditional mean estimator under each action and develop the d oubly r obust int e rv a l esti m ation (DREAM) method to infer the value under the estimated optimal policy in online learning. The proposed value estimator provides double protection for consistency and is asymptotically normal with a Wald-type confidence interval provided. Extensive simulation studies and real data applications are conducted to demonstrate the empirical validity of the proposed DREAM method. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Ye Shen and Hengrui Cai and Rui Song},
  doi          = {10.1080/01621459.2023.2279289},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2811-2821},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly robust interval estimation for optimal policy evaluation in online learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Testing general linear hypotheses under a high-dimensional
multivariate regression model with spiked noise covariance.
<em>JASA</em>, <em>119</em>(548), 2799–2810. (<a
href="https://doi.org/10.1080/01621459.2023.2278825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of testing linear hypotheses under a multivariate regression model with a high-dimensional response and spiked noise covariance. The proposed family of tests consists of test statistics based on a weighted sum of projections of the data onto the estimated latent factor directions, with the weights acting as the regularization parameters. We establish asymptotic normality of the test statistics under the null hypothesis. We also establish the power characteristics of the tests and propose a data-driven choice of the regularization parameters under a family of local alternatives. The performance of the proposed tests is evaluated through a simulation study. Finally, the proposed tests are applied to the Human Connectome Project data to test for the presence of associations between volumetric measurements of human brain and behavioral variables. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Haoran Li and Alexander Aue and Debashis Paul and Jie Peng},
  doi          = {10.1080/01621459.2023.2278825},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2799-2810},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing general linear hypotheses under a high-dimensional multivariate regression model with spiked noise covariance},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In nonparametric and high-dimensional models, bayesian
ignorability is an informative prior. <em>JASA</em>, <em>119</em>(548),
2785–2798. (<a
href="https://doi.org/10.1080/01621459.2023.2278202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In problems with large amounts of missing data one must model two distinct data generating processes: the outcome process, which generates the response, and the missing data mechanism, which determines the data we observe. Under the ignorability condition of Rubin, however, likelihood-based inference for the outcome process does not depend on the missing data mechanism so that only the former needs to be estimated; partially because of this simplification, ignorability is often used as a baseline assumption. We study the implications of Bayesian ignorability in the presence of high-dimensional nuisance parameters and argue that ignorability is typically incompatible with sensible prior beliefs about the amount of confounding bias. We show that, for many problems, ignorability directly implies that the prior on the selection bias is tightly concentrated around zero. This is demonstrated on several models of practical interest, and the effect of ignorability on the posterior distribution is characterized for high-dimensional linear models with a ridge regression prior. We then show both how to build high-dimensional models that encode sensible beliefs about the confounding bias and also show that under certain narrow circumstances ignorability is less problematic. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Antonio R. Linero},
  doi          = {10.1080/01621459.2023.2278202},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2785-2798},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {In nonparametric and high-dimensional models, bayesian ignorability is an informative prior},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric statistical inference via metric distribution
function in metric spaces. <em>JASA</em>, <em>119</em>(548), 2772–2784.
(<a href="https://doi.org/10.1080/01621459.2023.2277417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution function is essential in statistical inference and connected with samples to form a directed closed loop by the correspondence theorem in measure theory and the Glivenko-Cantelli and Donsker properties. This connection creates a paradigm for statistical inference. However, existing distribution functions are defined in Euclidean spaces and are no longer convenient to use in rapidly evolving data objects of complex nature. It is imperative to develop the concept of the distribution function in a more general space to meet emerging needs. Note that the linearity allows us to use hypercubes to define the distribution function in a Euclidean space. Still, without the linearity in a metric space, we must work with the metric to investigate the probability measure. We introduce a class of metric distribution functions through the metric only. We overcome this challenging step by proving the correspondence theorem and the Glivenko-Cantelli theorem for metric distribution functions in metric spaces, laying the foundation for conducting rational statistical inference for metric space-valued data. Then, we develop a homogeneity test and a mutual independence test for non-Euclidean random objects and present comprehensive empirical evidence to support the performance of our proposed methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xueqin Wang and Jin Zhu and Wenliang Pan and Junhao Zhu and Heping Zhang},
  doi          = {10.1080/01621459.2023.2277417},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2772-2784},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric statistical inference via metric distribution function in metric spaces},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Higher-order expansions and inference for panel data models.
<em>JASA</em>, <em>119</em>(548), 2760–2771. (<a
href="https://doi.org/10.1080/01621459.2023.2277411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a simple inferential method for a wide class of panel data models with a focus on such cases that have both serial correlation and cross-sectional dependence. In order to establish an asymptotic theory to support the inferential method, we develop some new and useful higher-order expansions, such as Berry-Esseen bound and Edgeworth Expansion, under a set of simple and general conditions. We further demonstrate the usefulness of these theoretical results by explicitly investigating a panel data model with interactive effects which nests many traditional panel data models as special cases. Finally, we show the superiority of our approach over several natural competitors using extensive numerical studies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiti Gao and Bin Peng and Yayi Yan},
  doi          = {10.1080/01621459.2023.2277411},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2760-2771},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Higher-order expansions and inference for panel data models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal and safe estimation for high-dimensional
semi-supervised learning. <em>JASA</em>, <em>119</em>(548), 2748–2759.
(<a href="https://doi.org/10.1080/01621459.2023.2277409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation problem in high-dimensional semi-supervised learning. Our goal is to investigate when and how the unlabeled data can be exploited to improve the estimation of the regression parameters of linear model in light of the fact that such linear models may be misspecified in data analysis. We first establish the minimax lower bound for parameter estimation in the semi-supervised setting, and show that this lower bound cannot be achieved by supervised estimators using the labeled data only. We propose an optimal semi-supervised estimator that can attain this lower bound and therefore improves the supervised estimators, provided that the conditional mean function can be consistently estimated with a proper rate. We further propose a safe semi-supervised estimator. We view it safe, because this estimator is always at least as good as the supervised estimators. We also extend our idea to the aggregation of multiple semi-supervised estimators caused by different misspecifications of the conditional mean function. Extensive numerical simulations and a real data analysis are conducted to illustrate our theoretical results. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Siyi Deng and Yang Ning and Jiwei Zhao and Heping Zhang},
  doi          = {10.1080/01621459.2023.2277409},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2748-2759},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal and safe estimation for high-dimensional semi-supervised learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dimension reduction for fréchet regression. <em>JASA</em>,
<em>119</em>(548), 2733–2747. (<a
href="https://doi.org/10.1080/01621459.2023.2277406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of data collection techniques, complex data objects that are not in the Euclidean space are frequently encountered in new statistical applications. Fréchet regression model (Petersen and Müller) provides a promising framework for regression analysis with metric space-valued responses. In this article, we introduce a flexible sufficient dimension reduction (SDR) method for Fréchet regression to achieve two purposes: to mitigate the curse of dimensionality caused by high-dimensional predictors, and to provide a visual inspection tool for Fréchet regression. Our approach is flexible enough to turn any existing SDR method for Euclidean ( X , Y ) into one for Euclidean X and metric space-valued Y . The basic idea is to first map the metric space-valued random object Y to a real-valued random variable f ( Y ) using a class of functions, and then perform classical SDR to the transformed data. If the class of functions is sufficiently rich, then we are guaranteed to uncover the Fréchet SDR space. We showed that such a class, which we call an ensemble, can be generated by a universal kernel (cc-universal kernel). We established the consistency and asymptotic convergence rate of the proposed methods. The finite-sample performance of the proposed methods is illustrated through simulation studies for several commonly encountered metric spaces that include Wasserstein space, the space of symmetric positive definite matrices, and the sphere. We illustrated the data visualization aspect of our method by the human mortality distribution data from the United Nations Databases. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qi Zhang and Lingzhou Xue and Bing Li},
  doi          = {10.1080/01621459.2023.2277406},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2733-2747},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Dimension reduction for fréchet regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enveloped huber regression. <em>JASA</em>,
<em>119</em>(548), 2722–2732. (<a
href="https://doi.org/10.1080/01621459.2023.2277403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huber regression (HR) is a popular flexible alternative to the least squares regression when the error follows a heavy-tailed distribution. We propose a new method called the enveloped Huber regression (EHR) by considering the envelope assumption that there exists some subspace of the predictors that has no association with the response, which is referred to as the immaterial part. More efficient estimation is achieved via the removal of the immaterial part. Different from the envelope least squares (ENV) model whose estimation is based on maximum normal likelihood, the estimation of the EHR model is through Generalized Method of Moments. The asymptotic normality of the EHR estimator is established, and it is shown that EHR is more efficient than HR. Moreover, EHR is more efficient than ENV when the error distribution is heavy-tailed, while maintaining a small efficiency loss when the error distribution is normal. Moreover, our theory also covers the heteroscedastic case in which the error may depend on the covariates. The envelope dimension in EHR is a tuning parameter to be determined by the data in practice. We further propose a novel generalized information criterion (GIC) for dimension selection and establish its consistency. Extensive simulation studies confirm the messages from our theory. EHR is further illustrated on a real dataset. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Le Zhou and R. Dennis Cook and Hui Zou},
  doi          = {10.1080/01621459.2023.2277403},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2722-2732},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Enveloped huber regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework for circular local likelihood
regression. <em>JASA</em>, <em>119</em>(548), 2709–2721. (<a
href="https://doi.org/10.1080/01621459.2023.2272786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a general framework for the estimation of regression models with circular covariates, where the conditional distribution of the response given the covariate can be specified through a parametric model. The estimation of a conditional characteristic is carried out nonparametrically, by maximizing the circular local likelihood, and the estimator is shown to be asymptotically normal. The problem of selecting the smoothing parameter is also addressed, as well as bias and variance computation. The performance of the estimation method in practice is studied through an extensive simulation study, where we cover the cases of Gaussian, Bernoulli, Poisson, and Gamma distributed responses. The generality of our approach is illustrated with several real-data examples from different fields. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {María Alonso-Pena and Irène Gijbels and Rosa M. Crujeiras},
  doi          = {10.1080/01621459.2023.2272786},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2709-2721},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A general framework for circular local likelihood regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uniform inference for kernel density estimators with dyadic
data. <em>JASA</em>, <em>119</em>(548), 2695–2708. (<a
href="https://doi.org/10.1080/01621459.2023.2272785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dyadic data is often encountered when quantities of interest are associated with the edges of a network. As such it plays an important role in statistics, econometrics and many other data science disciplines. We consider the problem of uniformly estimating a dyadic Lebesgue density function, focusing on nonparametric kernel-based estimators taking the form of dyadic empirical processes. Our main contributions include the minimax-optimal uniform convergence rate of the dyadic kernel density estimator, along with strong approximation results for the associated standardized and Studentized t -processes. A consistent variance estimator enables the construction of valid and feasible uniform confidence bands for the unknown density function. We showcase the broad applicability of our results by developing novel counterfactual density estimation and inference methodology for dyadic data, which can be used for causal inference and program evaluation. A crucial feature of dyadic distributions is that they may be “degenerate” at certain points in the support of the data, a property making our analysis somewhat delicate. Nonetheless our methods for uniform inference remain robust to the potential presence of such points. For implementation purposes, we discuss inference procedures based on positive semidefinite covariance estimators, mean squared error optimal bandwidth selectors and robust bias correction techniques. We illustrate the empirical finite-sample performance of our methods both in simulations and with real-world trade data, for which we make comparisons between observed and counterfactual trade distributions in different years. Our technical results concerning strong approximations and maximal inequalities are of potential independent interest. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matias D. Cattaneo and Yingjie Feng and William G. Underwood},
  doi          = {10.1080/01621459.2023.2272785},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2695-2708},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Uniform inference for kernel density estimators with dyadic data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor augmented sparse throughput deep ReLU neural networks
for high dimensional regression. <em>JASA</em>, <em>119</em>(548),
2680–2694. (<a
href="https://doi.org/10.1080/01621459.2023.2271605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a Factor Augmented Sparse Throughput (FAST) model that uses both latent factors and sparse idiosyncratic components for nonparametric regression. It contains many popular statistical models. The FAST model bridges factor models on one end and sparse nonparametric models on the other end. It encompasses structured nonparametric models such as factor augmented additive models and sparse low-dimensional nonparametric interaction models and covers the cases where the covariates do not admit factor structures. This model allows us to conduct high-dimensional nonparametric model selection for both strong dependent and weak dependent covariates and hence contributes to interpretable machine learning, particularly to the feature selections for neural networks. Via diversified projections as estimation of latent factor space, we employ truncated deep ReLU networks to nonparametric factor regression without regularization and to a more general FAST model using nonconvex regularization, resulting in factor augmented regression using neural network (FAR-NN) and FAST-NN estimators, respectively. We show that FAR-NN and FAST-NN estimators adapt to the unknown low-dimensional structure using hierarchical composition models in nonasymptotic minimax rates. We also study statistical learning for the factor augmented sparse additive model using a more specific neural network architecture. Our results are applicable to the weak dependent cases without factor structures. In proving the main technical result for FAST-NN, we establish a new deep ReLU network approximation result that contributes to the foundation of neural network theory. Numerical studies further support our theory and methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Yihong Gu},
  doi          = {10.1080/01621459.2023.2271605},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2680-2694},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Factor augmented sparse throughput deep ReLU neural networks for high dimensional regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New estimands for experiments with strong interference.
<em>JASA</em>, <em>119</em>(548), 2670–2679. (<a
href="https://doi.org/10.1080/01621459.2023.2271205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In experiments that study social phenomena, such as peer influence or herd immunity, the treatment of one unit may influence the outcomes of others. Such “interference between units” violates traditional approaches for causal inference, so that additional assumptions are often imposed to model or limit the underlying social mechanism. For binary outcomes, we propose new estimands that can be estimated without such assumptions, allowing for interval estimates that assume only the randomization of treatment. However, the causal implications of these estimands are more limited than those attainable under stronger assumptions. The estimand shows whether the treatment effects under the observed assignment varied systematically as a function of each unit’s direct and indirect exposure to treatment, while also lower bounding the number of units affected. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {David Choi},
  doi          = {10.1080/01621459.2023.2271205},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2670-2679},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {New estimands for experiments with strong interference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A projection space-filling criterion and related optimality
results. <em>JASA</em>, <em>119</em>(548), 2658–2669. (<a
href="https://doi.org/10.1080/01621459.2023.2271203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer experiments call for space-filling designs. Recently, a minimum aberration type space-filling criterion was proposed to rank and assess a family of space-filling designs including Latin hypercubes and strong orthogonal arrays. It aims at capturing the space-filling properties of a design when projected onto subregions of various sizes. In this article, we also consider the dimension aside from the sizes of subregions by proposing first an expanded space-filling hierarchy principle and then a projection space-filling criterion as per the new principle. When projected onto subregions of the specific size, the proposed criterion ranks designs via sequentially maximizing the space-filling properties on equally sized subregions in lower dimensions to higher dimensions, while the minimum aberration type space-filling criterion compares designs by maximizing the aggregate space-filling properties on multidimensional subregions of the same size. We present illustrative examples to demonstrate two criteria and conduct simulations as evidence of the utility of our criterion in terms of selecting efficient space-filling designs to build statistical surrogate models. We further consider the construction of the optimal space-filling designs under the proposed criterion. Although many algorithms have been proposed for generating space-filling designs, it is well-known that they often deteriorate rapidly in performance for large designs. In this article, we develop some theoretical optimality results and characterize several classes of strong orthogonal arrays of strength three that are the most space-filling. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chenlu Shi and Hongquan Xu},
  doi          = {10.1080/01621459.2023.2271203},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2658-2669},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A projection space-filling criterion and related optimality results},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimax quasi-bayesian estimation in sparse canonical
correlation analysis via a rayleigh quotient function. <em>JASA</em>,
<em>119</em>(548), 2647–2657. (<a
href="https://doi.org/10.1080/01621459.2023.2271199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical correlation analysis (CCA) is a popular statistical technique for exploring relationships between datasets. In recent years, the estimation of sparse canonical vectors has emerged as an important but challenging variant of the CCA problem, with widespread applications. Unfortunately, existing rate-optimal estimators for sparse canonical vectors have high computational cost. We propose a quasi-Bayesian estimation procedure that not only achieves the minimax estimation rate, but also is easy to compute by Markov chain Monte Carlo (MCMC). The method builds on (Tan et al.) and uses a rescaled Rayleigh quotient function as the quasi-log-likelihood. However, unlike (Tan et al.), we adopt a Bayesian framework that combines this quasi-log-likelihood with a spike-and-slab prior to regularize the inference and promote sparsity. We investigate the empirical behavior of the proposed method on both continuous and truncated data, and we demonstrate that it outperforms several state-of-the-art methods. As an application, we use the proposed methodology to maximally correlate clinical variables and proteomic data for better understanding the Covid-19 disease. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qiuyun Zhu and Yves Atchadé},
  doi          = {10.1080/01621459.2023.2271199},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2647-2657},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Minimax quasi-bayesian estimation in sparse canonical correlation analysis via a rayleigh quotient function},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent space modeling of hypergraph data. <em>JASA</em>,
<em>119</em>(548), 2634–2646. (<a
href="https://doi.org/10.1080/01621459.2023.2270750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of relational data describing interactions among a target population has motivated a wide literature on statistical network analysis. In many applications, interactions may involve more than two members of the population and this data is more appropriately represented by a hypergraph. In this article, we present a model for hypergraph data that extends the well-established latent space approach for graphs and, by drawing a connection to constructs from computational topology, we develop a model whose likelihood is inexpensive to compute. A delayed acceptance MCMC scheme is proposed to obtain posterior samples and we rely on Bookstein coordinates to remove the identifiability issues associated with the latent representation. We theoretically examine the degree distribution of hypergraphs generated under our framework and, through simulation, we investigate the flexibility of our model and consider estimation of predictive distributions. Finally, we explore the application of our model to two real-world datasets. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kathryn Turnbull and Simón Lunagómez and Christopher Nemeth and Edoardo Airoldi},
  doi          = {10.1080/01621459.2023.2270750},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2634-2646},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Latent space modeling of hypergraph data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlling cumulative adverse risk in learning optimal
dynamic treatment regimens. <em>JASA</em>, <em>119</em>(548), 2622–2633.
(<a href="https://doi.org/10.1080/01621459.2023.2270637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimen (DTR) is one of the most important tools to tailor treatment in personalized medicine. For many diseases such as cancer and type 2 diabetes mellitus (T2D), more aggressive treatments can lead to a higher efficacy but may also increase risk. However, few methods for estimating DTRs can take into account both cumulative benefit and risk. In this work, we propose a general statistical learning framework to learn optimal DTRs that maximize the reward outcome while controlling the cumulative adverse risk to be below a pre-specified threshold. We convert this constrained optimization problem into an unconstrained optimization using a Lagrange function. We then solve the latter using either backward learning algorithms or simultaneously over all stages based on constructing a novel multistage ramp loss. Theoretically, we establish Fisher consistency of the proposed method and further obtain non-asymptotic convergence rates for both reward and risk outcomes under the estimated DTRs. The finite sample performance of the proposed method is demonstrated via simulation studies and through an application to a two-stage clinical trial for T2D patients. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Mochuan Liu and Yuanjia Wang and Haoda Fu and Donglin Zeng},
  doi          = {10.1080/01621459.2023.2270637},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2622-2633},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlling cumulative adverse risk in learning optimal dynamic treatment regimens},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The cellwise minimum covariance determinant estimator.
<em>JASA</em>, <em>119</em>(548), 2610–2621. (<a
href="https://doi.org/10.1080/01621459.2023.2267777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usual Minimum Covariance Determinant (MCD) estimator of a covariance matrix is robust against casewise outliers. These are cases (that is, rows of the data matrix) that behave differently from the majority of cases, raising suspicion that they might belong to a different population. On the other hand, cellwise outliers are individual cells in the data matrix. When a row contains one or more outlying cells, the other cells in the same row still contain useful information that we wish to preserve. We propose a cellwise robust version of the MCD method, called cellMCD. Its main building blocks are observed likelihood and a penalty term on the number of flagged cellwise outliers. It possesses good breakdown properties. We construct a fast algorithm for cellMCD based on concentration steps (C-steps) that always lower the objective. The method performs well in simulations with cellwise outliers, and has high finite-sample efficiency on clean data. It is illustrated on real data with visualizations of the results. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jakob Raymaekers and Peter J. Rousseeuw},
  doi          = {10.1080/01621459.2023.2267777},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2610-2621},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The cellwise minimum covariance determinant estimator},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Copula modeling of serially correlated multivariate data
with hidden structures. <em>JASA</em>, <em>119</em>(548), 2598–2609. (<a
href="https://doi.org/10.1080/01621459.2023.2263202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a copula-based extension of the hidden Markov model (HMM) which applies when the observations recorded at each time in the sample are multivariate. The joint model produced by the copula extension allows decoding of the hidden states based on information from multiple observations. However, unlike the case of independent marginals, the copula dependence structure embedded into the likelihood poses additional computational challenges. We tackle the latter using a theoretically-justified variation of the EM algorithm developed within the framework of inference functions for margins. We illustrate the method using numerical experiments and an analysis of room occupancy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Robert Zimmerman and Radu V. Craiu and Vianey Leos-Barajas},
  doi          = {10.1080/01621459.2023.2263202},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2598-2609},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Copula modeling of serially correlated multivariate data with hidden structures},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A regression-based approach to robust estimation and
inference for genetic covariance. <em>JASA</em>, <em>119</em>(548),
2585–2597. (<a
href="https://doi.org/10.1080/01621459.2023.2261669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genome-wide association studies (GWAS) have identified thousands of genetic variants associated with complex traits, and some variants are shown to be associated with multiple complex traits. Genetic covariance between two traits is defined as the underlying covariance of genetic effects and can be used to measure the shared genetic architecture. The data used to estimate such a genetic covariance can be from the same group or different groups of individuals, and the traits can be of different types or collected based on different study designs. This article proposes a unified regression-based approach to robust estimation and inference for genetic covariance of general traits that may be associated with genetic variants nonlinearly. The asymptotic properties of the proposed estimator are provided and are shown to be robust under certain model misspecification. Our method under linear working models provides a robust inference for the narrow-sense genetic covariance, even when both linear models are mis-specified. Numerical experiments are performed to support the theoretical results. Our method is applied to an outbred mice GWAS dataset to study the overlapping genetic effects between the behavioral and physiological phenotypes. The real data results reveal interesting genetic covariance among different mice developmental traits. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqiao Wang and Sai Li and Hongzhe Li},
  doi          = {10.1080/01621459.2023.2261669},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2585-2597},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A regression-based approach to robust estimation and inference for genetic covariance},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovery and inference of a causal network with hidden
confounding. <em>JASA</em>, <em>119</em>(548), 2572–2584. (<a
href="https://doi.org/10.1080/01621459.2023.2261658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel causal discovery and inference method called GrIVET for a Gaussian directed acyclic graph with unmeasured confounders. GrIVET consists of an order-based causal discovery method and a likelihood-based inferential procedure. For causal discovery, we generalize the existing peeling algorithm to estimate the ancestral relations and candidate instruments in the presence of hidden confounders. Based on this, we propose a new procedure for instrumental variable estimation of each direct effect by separating it from any mediation effects. For inference, we develop a new likelihood ratio test of multiple causal effects that is able to account for the unmeasured confounders. Theoretically, we prove that the proposed method has desirable guarantees, including robustness to invalid instruments and uncertain interventions, estimation consistency, low-order polynomial time complexity, and validity of asymptotic inference. Numerically, GrIVET performs well and compares favorably against state-of-the-art competitors. Furthermore, we demonstrate the utility and effectiveness of the proposed method through an application inferring regulatory pathways from Alzheimer’s disease gene expression data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Li Chen and Chunlin Li and Xiaotong Shen and Wei Pan},
  doi          = {10.1080/01621459.2023.2261658},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2572-2584},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discovery and inference of a causal network with hidden confounding},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian inference on high-dimensional multivariate binary
responses. <em>JASA</em>, <em>119</em>(548), 2560–2571. (<a
href="https://doi.org/10.1080/01621459.2023.2260053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become increasingly common to collect high-dimensional binary response data; for example, with the emergence of new sampling techniques in ecology. In smaller dimensions, multivariate probit (MVP) models are routinely used for inferences. However, algorithms for fitting such models face issues in scaling up to high dimensions due to the intractability of the likelihood, involving an integral over a multivariate normal distribution having no analytic form. Although a variety of algorithms have been proposed to approximate this intractable integral, these approaches are difficult to implement and/or inaccurate in high dimensions. Our main focus is in accommodating high-dimensional binary response data with a small-to-moderate number of covariates. We propose a two-stage approach for inference on model parameters while taking care of uncertainty propagation between the stages. We use the special structure of latent Gaussian models to reduce the highly expensive computation involved in joint parameter estimation to focus inference on marginal distributions of model parameters. This essentially makes the method embarrassingly parallel for both stages. We illustrate performance in simulations and applications to joint species distribution modeling in ecology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Antik Chakraborty and Rihui Ou and David B. Dunson},
  doi          = {10.1080/01621459.2023.2260053},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2560-2571},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian inference on high-dimensional multivariate binary responses},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ultimate pólya gamma samplers–efficient MCMC for possibly
imbalanced binary and categorical data. <em>JASA</em>,
<em>119</em>(548), 2548–2559. (<a
href="https://doi.org/10.1080/01621459.2023.2259030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling binary and categorical data is one of the most commonly encountered tasks of applied statisticians and econometricians. While Bayesian methods in this context have been available for decades now, they often require a high level of familiarity with Bayesian statistics or suffer from issues such as low sampling efficiency. To contribute to the accessibility of Bayesian models for binary and categorical data, we introduce novel latent variable representations based on Pólya-Gamma random variables for a range of commonly encountered logistic regression models. From these latent variable representations, new Gibbs sampling algorithms for binary, binomial, and multinomial logit models are derived. All models allow for a conditionally Gaussian likelihood representation, rendering extensions to more complex modeling frameworks such as state space models straightforward. However, sampling efficiency may still be an issue in these data augmentation based estimation frameworks. To counteract this, novel marginal data augmentation strategies are developed and discussed in detail. The merits of our approach are illustrated through extensive simulations and real data applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gregor Zens and Sylvia Frühwirth-Schnatter and Helga Wagner},
  doi          = {10.1080/01621459.2023.2259030},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2548-2559},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ultimate pólya gamma Samplers–Efficient MCMC for possibly imbalanced binary and categorical data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional integrative bayesian analysis of high-dimensional
multiplatform clinicogenomic data. <em>JASA</em>, <em>119</em>(548),
2533–2547. (<a
href="https://doi.org/10.1080/01621459.2024.2388909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancements in collection and dissemination of multi-platform molecular and genomics data has resulted in enormous opportunities to aggregate such data in order to understand, prevent, and treat human diseases. While significant improvements have been made in multi-omic data integration methods to discover biological markers and mechanisms underlying both prognosis and treatment, the precise cellular functions governing these complex mechanisms still need detailed and data-driven de-novo evaluations. We propose a framework called Functional Integrative Bayesian Analysis of High-dimensional Multiplatform Genomic Data (fiBAG), that allows simultaneous identification of upstream functional evidence of proteogenomic biomarkers and the incorporation of such knowledge in Bayesian variable selection models to improve signal detection. fiBAG employs a conflation of Gaussian process models to quantify (possibly nonlinear) functional evidence via Bayes factors, which are then mapped to a novel calibrated spike-and-slab prior, thus, guiding selection and providing functional relevance to the associations with patient outcomes. Using simulations, we illustrate how integrative methods with functional calibration have higher power to detect disease related markers than non-integrative approaches. We demonstrate the profitability of fiBAG via a pan-cancer analysis of 14 cancer types to identify and assess the cellular mechanisms of proteogenomic markers associated with cancer stemness and patient survival. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Rupam Bhattacharyya and Nicholas C. Henderson and Veerabhadran Baladandayuthapani},
  doi          = {10.1080/01621459.2024.2388909},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2533-2547},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional integrative bayesian analysis of high-dimensional multiplatform clinicogenomic data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference of cell-type proportions estimated
from bulk expression data. <em>JASA</em>, <em>119</em>(548), 2521–2532.
(<a href="https://doi.org/10.1080/01621459.2024.2382435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in cell-type-specific analysis from bulk samples with a mixture of different cell types. A critical first step in such analyses is the accurate estimation of cell-type proportions in a bulk sample. Although many methods have been proposed recently, quantifying the uncertainties associated with the estimated cell-type proportions has not been well studied. Lack of consideration of these uncertainties can lead to missed or false findings in downstream analyses. In this article, we introduce a flexible statistical deconvolution framework that allows a general and subject-specific covariance of bulk gene expressions. Under this framework, we propose a decorrelated constrained least squares method called DECALS that estimates cell-type proportions as well as the sampling distribution of the estimates. Simulation studies demonstrate that DECALS can accurately quantify the uncertainties in the estimated proportions whereas other methods fail. Applying DECALS to analyze bulk gene expression data of post mortem brain samples from the ROSMAP and GTEx projects, we show that taking into account the uncertainties in the estimated cell-type proportions can lead to more accurate identifications of cell-type-specific differentially expressed genes and transcripts between different subject groups, such as between Alzheimer’s disease patients and controls and between males and females. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Biao Cai and Jingfei Zhang and Hongyu Li and Chang Su and Hongyu Zhao},
  doi          = {10.1080/01621459.2024.2382435},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2521-2532},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference of cell-type proportions estimated from bulk expression data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse independent component analysis with an application to
cortical surface fMRI data in autism. <em>JASA</em>, <em>119</em>(548),
2508–2520. (<a
href="https://doi.org/10.1080/01621459.2024.2370593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent component analysis (ICA) is widely used to estimate spatial resting-state networks and their time courses in neuroimaging studies. It is thought that independent components correspond to sparse patterns of co-activating brain locations. Previous approaches for introducing sparsity to ICA replace the non-smooth objective function with smooth approximations, resulting in components that do not achieve exact zeros. We propose a novel Sparse ICA method that enables sparse estimation of independent source components by solving a non-smooth non-convex optimization problem via the relax-and-split framework. The proposed Sparse ICA method balances statistical independence and sparsity simultaneously and is computationally fast. In simulations, we demonstrate improved estimation accuracy of both source signals and signal time courses compared to existing approaches. We apply our Sparse ICA to cortical surface resting-state fMRI in school-aged autistic children. Our analysis reveals differences in brain activity between certain regions in autistic children compared to children without autism. Sparse ICA selects coactivating locations, which we argue is more interpretable than dense components from popular approaches. Sparse ICA is fast and easy to apply to big data. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Zihang Wang and Irina Gaynanova and Aleksandr Aravkin and Benjamin B. Risk},
  doi          = {10.1080/01621459.2024.2370593},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2508-2520},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse independent component analysis with an application to cortical surface fMRI data in autism},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient stochastic generators with spherical harmonic
transformation for high-resolution global climate simulations from
CESM2-LENS2. <em>JASA</em>, <em>119</em>(548), 2493–2507. (<a
href="https://doi.org/10.1080/01621459.2024.2360666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earth system models (ESMs) are fundamental for understanding Earth’s complex climate system. However, the computational demands and storage requirements of ESM simulations limit their utility. For the newly published CESM2-LENS2 data, which suffer from this issue, we propose a novel stochastic generator (SG) as a practical complement to the CESM2, capable of rapidly producing emulations closely mirroring training simulations. Our SG leverages the spherical harmonic transformation (SHT) to shift from spatial to spectral domains, enabling efficient low-rank approximations that significantly reduce computational and storage costs. By accounting for axial symmetry and retaining distinct ranks for land and ocean regions, our SG captures intricate nonstationary spatial dependencies. Additionally, a modified Tukey g-and-h (TGH) transformation accommodates non-Gaussianity in high-temporal-resolution data. We apply the proposed SG to generate emulations for surface temperature simulations from the CESM2-LENS2 data across various scales, marking the first attempt of reproducing daily data. These emulations are then meticulously validated against training simulations. This work offers a promising complementary pathway for efficient climate modeling and analysis while overcoming computational and storage limitations. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yan Song and Zubair Khalid and Marc G. Genton},
  doi          = {10.1080/01621459.2024.2360666},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2493-2507},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient stochastic generators with spherical harmonic transformation for high-resolution global climate simulations from CESM2-LENS2},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizing the intention-to-treat effect of an active
control from historical placebo-controlled trials: A case study of the
efficacy of daily oral TDF/FTC in the HPTN 084 study. <em>JASA</em>,
<em>119</em>(548), 2478–2492. (<a
href="https://doi.org/10.1080/01621459.2024.2360643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many clinical settings, an active-controlled trial design (e.g., a non-inferiority or superiority design) is often used to compare an experimental medicine to an active control (e.g., an FDA-approved, standard therapy). One prominent example is a recent phase 3 efficacy trial, HIV Prevention Trials Network Study 084 (HPTN 084), comparing long-acting cabotegravir, a new HIV pre-exposure prophylaxis (PrEP) agent, to the FDA-approved daily oral tenofovir disoproxil fumarate plus emtricitabine (TDF/FTC) in a population of heterosexual women in 7 African countries. One key complication of interpreting study results in an active-controlled trial like HPTN 084 is that the placebo arm is not present and the efficacy of the active control (and hence the experimental drug) compared to the placebo can only be inferred by leveraging other data sources. In this article, we study statistical inference for the intention-to-treat (ITT) effect of the active control using relevant historical placebo-controlled trials data under the potential outcomes (PO) framework. We highlight the role of adherence and unmeasured confounding, discuss in detail identification assumptions and two modes of inference (point vs. partial identification), propose estimators under identification assumptions permitting point identification, and lay out sensitivity analyses needed to relax identification assumptions. We applied our framework to estimating the intention-to-treat effect of daily oral TDF/FTC versus placebo in HPTN 084 using data from an earlier Phase 3, placebo-controlled trial of daily oral TDF/FTC (Partners PrEP). Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Qijia He and Fei Gao and Oliver Dukes and Sinead Delany-Moretlwe and Bo Zhang},
  doi          = {10.1080/01621459.2024.2360643},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2478-2492},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalizing the intention-to-treat effect of an active control from historical placebo-controlled trials: A case study of the efficacy of daily oral TDF/FTC in the HPTN 084 study},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint tensor modeling of single cell 3D genome and
epigenetic data with muscle. <em>JASA</em>, <em>119</em>(548),
2464–2477. (<a
href="https://doi.org/10.1080/01621459.2024.2358557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging single cell technologies that simultaneously capture long-range interactions of genomic loci together with their DNA methylation levels are advancing our understanding of three-dimensional genome structure and its interplay with the epigenome at the single cell level. While methods to analyze data from single cell high throughput chromatin conformation capture (scHi-C) experiments are maturing, methods that can jointly analyze multiple single cell modalities with scHi-C data are lacking. Here, we introduce Muscle, a semi-nonnegative joint decomposition of Multiple single cell tensors, to jointly analyze 3D conformation and DNA methylation data at the single cell level. Muscle takes advantage of the inherent tensor structure of the scHi-C data, and integrates this modality with DNA methylation. We developed an alternating least squares algorithm for estimating Muscle parameters and established its optimality properties. Parameters estimated by Muscle directly align with the key components of the downstream analysis of scHi-C data in a cell type specific manner. Evaluations with data-driven experiments and simulations demonstrate the advantages of the joint modeling framework of Muscle over single modality modeling and a baseline multi modality modeling for cell type delineation and elucidating associations between modalities. Muscle is publicly available at https://github.com/keleslab/muscle . Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Kwangmoon Park and Sündüz Keleş},
  doi          = {10.1080/01621459.2024.2358557},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2464-2477},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Joint tensor modeling of single cell 3D genome and epigenetic data with muscle},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dissecting gene expression heterogeneity: Generalized
pearson correlation squares and the k-lines clustering algorithm.
<em>JASA</em>, <em>119</em>(548), 2450–2463. (<a
href="https://doi.org/10.1080/01621459.2024.2342639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the pressing needs for dissecting heterogeneous relationships in gene expression data, here we generalize the squared Pearson correlation to capture a mixture of linear dependences between two real-valued variables, with or without an index variable that specifies the line memberships. We construct the generalized Pearson correlation squares by focusing on three aspects: variable exchangeability, no parametric model assumptions, and inference of population-level parameters. To compute the generalized Pearson correlation square from a sample without a line-membership specification, we develop a K -lines clustering algorithm to find K clusters that exhibit distinct linear dependences, where K can be chosen in a data-adaptive way. To infer the population-level generalized Pearson correlation squares, we derive the asymptotic distributions of the sample-level statistics to enable efficient statistical inference. Simulation studies verify the theoretical results and show the power advantage of the generalized Pearson correlation squares in capturing mixtures of linear dependences. Gene expression data analyses demonstrate the effectiveness of the generalized Pearson correlation squares and the K -lines clustering algorithm in dissecting complex but interpretable relationships. The estimation and inference procedures are implemented in the R package gR2 ( https://github.com/lijy03/gR2 ). Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jingyi Jessica Li and Heather J. Zhou and Peter J. Bickel and Xin Tong},
  doi          = {10.1080/01621459.2024.2342639},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2450-2463},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Dissecting gene expression heterogeneity: Generalized pearson correlation squares and the K-lines clustering algorithm},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient coalescent model for heterochronously sampled
molecular data. <em>JASA</em>, <em>119</em>(548), 2437–2449. (<a
href="https://doi.org/10.1080/01621459.2024.2330732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular sequence variation at a locus informs about the evolutionary history of the sample and past population size dynamics. The Kingman coalescent is used in a generative model of molecular sequence variation to infer evolutionary parameters. However, it is well understood that inference under this model does not scale well with sample size. Here, we build on recent work based on a lower resolution coalescent process, the Tajima coalescent, to model longitudinal samples. While the Kingman coalescent models the ancestry of labeled individuals, we model the ancestry of individuals labeled by their sampling time. We propose a new inference scheme for the reconstruction of effective population size trajectories based on this model and the infinite-sites mutation model. Modeling of longitudinal samples is necessary for applications (e.g., ancient DNA and RNA from rapidly evolving pathogens like viruses) and statistically desirable (variance reduction and parameter identifiability). We propose an efficient algorithm to calculate the likelihood and employ a Bayesian nonparametric procedure to infer the population size trajectory. We provide a new MCMC sampler to explore the space of heterochronous Tajima’s genealogies and model parameters. We compare our procedure with state-of-the-art methodologies in simulations and an application to ancient bison DNA sequences. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Lorenzo Cappello and Amandine Véber and Julia A. Palacios},
  doi          = {10.1080/01621459.2024.2330732},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2437-2449},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An efficient coalescent model for heterochronously sampled molecular data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical foundations driving 21st century innovation.
<em>JASA</em>, <em>119</em>(548), 2427–2436. (<a
href="https://doi.org/10.1080/01621459.2024.2412464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical foundations are without question at the core of modern innovation. In today’s economy, a common phrase is “data is the new gold.” Certainly, we live in an age where data is large, ubiquitous, and comes in many forms. The contributions from the statistical sciences go beyond data. We are emerging from a pandemic where statisticians around the globe saved lives by contributing critical understanding to vaccines, treatments, pandemic policies, and management. The contributions from statistical foundations are universal—from self-driving cars to Mars rovers, to sustainable and improved infrastructure, to clean energy and environmental stewardship, to financial markets and investing, to advances in medicine and medical practices, and toward a better understanding of the communities in which we live, work, learn, and play. This article summarizes my 2022 Presidential speech given at the Joint Statistical Meetings in Washington, DC. I highlight these important contributions and the innovations they made possible and will look to new opportunities on the horizon. Further, I reflect on advances the ASA has taken as an organization in data science and AI, establishing the ASA leadership institute, and third a specific focus on community analytics.},
  archive      = {J_JASA},
  author       = {Katherine B. Ensor},
  doi          = {10.1080/01621459.2024.2412464},
  journal      = {Journal of the American Statistical Association},
  month        = {10},
  number       = {548},
  pages        = {2427-2436},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical foundations driving 21st century innovation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum to maximum likelihood estimation of the
multivariate normal mixture model. <em>JASA</em>, <em>119</em>(547),
2423. (<a href="https://doi.org/10.1080/01621459.2024.2370355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2024.2370355},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2423},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Corrigendum to maximum likelihood estimation of the multivariate normal mixture model},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical methods in health disparity research.
<em>JASA</em>, <em>119</em>(547), 2421. (<a
href="https://doi.org/10.1080/01621459.2024.2344715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Susan M. Paddock},
  doi          = {10.1080/01621459.2024.2344715},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2421},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical methods in health disparity research},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse graphical modeling for high dimensional data: A
paradigm of conditional independence tests. <em>JASA</em>,
<em>119</em>(547), 2421–2422. (<a
href="https://doi.org/10.1080/01621459.2024.2375035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Reza Mohammadi},
  doi          = {10.1080/01621459.2024.2375035},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2421-2422},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse graphical modeling for high dimensional data: A paradigm of conditional independence tests},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controlled epidemiological studies. <em>JASA</em>,
<em>119</em>(547), 2419–2420. (<a
href="https://doi.org/10.1080/01621459.2024.2303317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Kaushik Ghosh},
  doi          = {10.1080/01621459.2024.2303317},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2419-2420},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Controlled epidemiological studies},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introduction to environmental data science. <em>JASA</em>,
<em>119</em>(547), 2418–2419. (<a
href="https://doi.org/10.1080/01621459.2024.2343459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Timothée Poisot},
  doi          = {10.1080/01621459.2024.2343459},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2418-2419},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Introduction to environmental data science},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoothness-penalized deconvolution (SPeD) of a density
estimate. <em>JASA</em>, <em>119</em>(547), 2407–2417. (<a
href="https://doi.org/10.1080/01621459.2023.2259028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the deconvolution problem of estimating a square-integrable probability density from observations contaminated with additive measurement errors having a known density. The estimator begins with a density estimate of the contaminated observations and minimizes a reconstruction error penalized by an integrated squared m th derivative. Theory for deconvolution has mainly focused on kernel- or wavelet-based techniques, but other methods including spline-based techniques and this smoothness-penalized estimator have been found to outperform kernel methods in simulation studies. This article fills in some of these gaps by establishing asymptotic guarantees for the smoothness-penalized approach. Consistency is established in mean integrated squared error, and rates of convergence are derived for Gaussian, Cauchy, and Laplace error densities, attaining some lower bounds already in the literature. The assumptions are weak for most results; the estimator can be used with a broader class of error densities than the deconvoluting kernel. Our application example estimates the density of the mean cytotoxicity of certain bacterial isolates under random sampling; this mean cytotoxicity can only be measured experimentally with additive error, leading to the deconvolution problem. We also describe a method for approximating the solution by a cubic spline, which reduces to a quadratic program. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {David Kent and David Ruppert},
  doi          = {10.1080/01621459.2023.2259028},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2407-2417},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Smoothness-penalized deconvolution (SPeD) of a density estimate},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A wasserstein index of dependence for random measures.
<em>JASA</em>, <em>119</em>(547), 2396–2406. (<a
href="https://doi.org/10.1080/01621459.2023.2258596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal transport and Wasserstein distances are flourishing in many scientific fields as a means for comparing and connecting random structures. Here we pioneer the use of an optimal transport distance between Lévy measures to solve a statistical problem. Dependent Bayesian nonparametric models provide flexible inference on distinct, yet related, groups of observations. Each component of a vector of random measures models a group of exchangeable observations, while their dependence regulates the borrowing of information across groups. We derive the first statistical index of dependence in [ 0 , 1 ] for (completely) random measures that accounts for their whole infinite-dimensional distribution, which is assumed to be equal across different groups. This is accomplished by using the geometric properties of the Wasserstein distance to solve a max–min problem at the level of the underlying Lévy measures. The Wasserstein index of dependence sheds light on the models’ deep structure and has desirable properties: (i) it is 0 if and only if the random measures are independent; (ii) it is 1 if and only if the random measures are completely dependent; (iii) it simultaneously quantifies the dependence of d ≥ 2 random measures, avoiding the need for pairwise comparisons; (iv) it can be evaluated numerically. Moreover, the index allows for informed prior specifications and fair model comparisons for Bayesian nonparametric models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Marta Catalano and Hugo Lavenant and Antonio Lijoi and Igor Prünster},
  doi          = {10.1080/01621459.2023.2258596},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2396-2406},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A wasserstein index of dependence for random measures},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal clustering with neyman-scott processes via
connections to bayesian nonparametric mixture models. <em>JASA</em>,
<em>119</em>(547), 2382–2395. (<a
href="https://doi.org/10.1080/01621459.2023.2257896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neyman-Scott processes (NSPs) are point process models that generate clusters of points in time or space. They are natural models for a wide range of phenomena, ranging from neural spike trains to document streams. The clustering property is achieved via a doubly stochastic formulation: first, a set of latent events is drawn from a Poisson process; then, each latent event generates a set of observed data points according to another Poisson process. This construction is similar to Bayesian nonparametric mixture models like the Dirichlet process mixture model (DPMM) in that the number of latent events (i.e., clusters) is a random variable, but the point process formulation makes the NSP especially well suited to modeling spatiotemporal data. While many specialized algorithms have been developed for DPMMs, comparatively fewer works have focused on inference in NSPs. Here, we present novel connections between NSPs and DPMMs, with the key link being a third class of Bayesian mixture models called mixture of finite mixture models (MFMMs). Leveraging this connection, we adapt the standard collapsed Gibbs sampling algorithm for DPMMs to enable scalable Bayesian inference on NSP models. We demonstrate the potential of Neyman-Scott processes on a variety of applications including sequence detection in neural spike trains and event detection in document streams. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yixin Wang and Anthony Degleris and Alex Williams and Scott W. Linderman},
  doi          = {10.1080/01621459.2023.2257896},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2382-2395},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spatiotemporal clustering with neyman-scott processes via connections to bayesian nonparametric mixture models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust leave-one-out cross-validation for high-dimensional
bayesian models. <em>JASA</em>, <em>119</em>(547), 2369–2381. (<a
href="https://doi.org/10.1080/01621459.2023.2257893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leave-one-out cross-validation (LOO-CV) is a popular method for estimating out-of-sample predictive accuracy. However, computing LOO-CV criteria can be computationally expensive due to the need to fit the model multiple times. In the Bayesian context, importance sampling provides a possible solution but classical approaches can easily produce estimators whose asymptotic variance is infinite, making them potentially unreliable. Here we propose and analyze a novel mixture estimator to compute Bayesian LOO-CV criteria. Our method retains the simplicity and computational convenience of classical approaches, while guaranteeing finite asymptotic variance of the resulting estimators. Both theoretical and numerical results are provided to illustrate the improved robustness and efficiency. The computational benefits are particularly significant in high-dimensional problems, allowing to perform Bayesian LOO-CV for a broader range of models, and datasets with highly influential observations. The proposed methodology is easily implementable in standard probabilistic programming software and has a computational cost roughly equivalent to fitting the original model once. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Luca Alessandro Silva and Giacomo Zanella},
  doi          = {10.1080/01621459.2023.2257893},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2369-2381},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust leave-one-out cross-validation for high-dimensional bayesian models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random fixed boundary flows. <em>JASA</em>,
<em>119</em>(547), 2356–2368. (<a
href="https://doi.org/10.1080/01621459.2023.2257892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider fixed boundary flows with canonical interpretability as principal components extended on nonlinear Riemannian manifolds. We aim to find a flow with fixed start and end points for noisy multivariate datasets lying near an embedded nonlinear Riemannian manifold. In geometric terms, the fixed boundary flow is defined as an optimal curve that moves in the data cloud with two fixed end points. At any point on the flow, we maximize the inner product of the vector field, which is calculated locally, and the tangent vector of the flow. The rigorous definition is derived from an optimization problem using the intrinsic metric on the manifolds. For random datasets, we name the fixed boundary flow the random fixed boundary flow and analyze its limiting behavior under noisy observed samples. We construct a high-level algorithm to compute the random fixed boundary flow, and provide the convergence of the algorithm. We show that the fixed boundary flow yields a concatenate of three segments, one of which coincides with the usual principal flow when the manifold is reduced to the Euclidean space. We further prove that the random fixed boundary flow converges largely to the population fixed boundary flow with high probability. Finally, we illustrate how the random fixed boundary flow can be used and interpreted, and demonstrate its application in real datasets. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhigang Yao and Yuqing Xia and Zengyan Fan},
  doi          = {10.1080/01621459.2023.2257892},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2356-2368},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Random fixed boundary flows},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized bayesian inference for discrete intractable
likelihood. <em>JASA</em>, <em>119</em>(547), 2345–2355. (<a
href="https://doi.org/10.1080/01621459.2023.2257891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete state spaces represent a major computational challenge to statistical inference, since the computation of normalization constants requires summation over large or possibly infinite sets, which can be impractical. This article addresses this computational challenge through the development of a novel generalized Bayesian inference procedure suitable for discrete intractable likelihood. Inspired by recent methodological advances for continuous data, the main idea is to update beliefs about model parameters using a discrete Fisher divergence, in lieu of the problematic intractable likelihood. The result is a generalized posterior that can be sampled from using standard computational tools, such as Markov chain Monte Carlo, circumventing the intractable normalizing constant. The statistical properties of the generalized posterior are analyzed, with sufficient conditions for posterior consistency and asymptotic normality established. In addition, a novel and general approach to calibration of generalized posteriors is proposed. Applications are presented on lattice models for discrete spatial data and on multivariate models for count data, where in each case the methodology facilitates generalized Bayesian inference at low computational cost. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Takuo Matsubara and Jeremias Knoblauch and François-Xavier Briol and Chris. J. Oates},
  doi          = {10.1080/01621459.2023.2257891},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2345-2355},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generalized bayesian inference for discrete intractable likelihood},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group network hawkes process. <em>JASA</em>,
<em>119</em>(547), 2328–2344. (<a
href="https://doi.org/10.1080/01621459.2023.2257889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study the event occurrences of individuals interacting in a network. To characterize the dynamic interactions among the individuals, we propose a group network Hawkes process (GNHP) model whose network structure is observed and fixed. In particular, we introduce a latent group structure among individuals to account for the heterogeneous user-specific characteristics. A maximum likelihood approach is proposed to simultaneously cluster individuals in the network and estimate model parameters. A fast EM algorithm is subsequently developed by using the branching representation of the proposed GNHP model. Theoretical properties of the resulting estimators of group memberships and model parameters are investigated under both settings when the number of latent groups G is over-specified or correctly specified. A data-driven criterion that can consistently identify the true G under mild conditions is derived. Extensive simulation studies and an application to a dataset collected from Sina Weibo are used to illustrate the effectiveness of the proposed methodology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Guanhua Fang and Ganggang Xu and Haochen Xu and Xuening Zhu and Yongtao Guan},
  doi          = {10.1080/01621459.2023.2257889},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2328-2344},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Group network hawkes process},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A randomized pairwise likelihood method for complex
statistical inferences. <em>JASA</em>, <em>119</em>(547), 2317–2327. (<a
href="https://doi.org/10.1080/01621459.2023.2257367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise likelihood methods are commonly used for inference in parametric statistical models in cases where the full likelihood is too complex to be used, such as multivariate count data. Although pairwise likelihood methods represent a useful solution to perform inference for intractable likelihoods, several computational challenges remain. The pairwise likelihood function still requires the computation of a sum over all pairs of variables and all observations, which may be prohibitive in high dimensions. Moreover, it may be difficult to calculate confidence intervals of the resulting estimators, as they involve summing all pairs of pairs and all of the four-dimensional marginals. To alleviate these issues, we consider a randomized pairwise likelihood approach, where only summands randomly sampled across observations and pairs are used for the estimation. In addition to the usual tradeoff between statistical and computational efficiency, it is shown that, under a condition on the sampling parameter, this two-way random sampling mechanism makes the individual bivariate likelihood scores become asymptotically independent, allowing more computationally efficient confidence intervals to be constructed. The proposed approach is illustrated in tandem with copula-based models for multivariate count data in simulations, and in real data from a transcriptome study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gildas Mazo and Dimitris Karlis and Andrea Rau},
  doi          = {10.1080/01621459.2023.2257367},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2317-2327},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A randomized pairwise likelihood method for complex statistical inferences},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAC covariance matrix estimation in quantile regression.
<em>JASA</em>, <em>119</em>(547), 2305–2316. (<a
href="https://doi.org/10.1080/01621459.2023.2257365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers an estimator for the asymptotic variance-covariance matrix in time-series quantile regression models which is robust to the presence of heteroscedasticity and autocorrelation. When regression errors are serially correlated, the conventional quantile regression standard errors are invalid. The proposed solution is a quantile analogue of the Newey-West robust standard errors. We establish the asymptotic properties of the heteroscedasticity and autocorrelation consistent (HAC) covariance matrix estimator and provide an optimal bandwidth selection rule. The quantile sample autocorrelation coefficient is biased toward zero in finite sample which adversely affects the optimal bandwidth estimation. We propose a simple alternative estimator that effectively reduces the finite sample bias. Numerical simulations provide evidence that the proposed HAC covariance matrix estimator significantly improves the size distortion problem. To illustrate the usefulness of the proposed robust standard error, we examine the impacts of the expansion of renewable energy resources on electricity prices. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Antonio F. Galvao and Jungmo Yoon},
  doi          = {10.1080/01621459.2023.2257365},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2305-2316},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {HAC covariance matrix estimation in quantile regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast approximation of the shapley values based on
order-of-addition experimental designs. <em>JASA</em>,
<em>119</em>(547), 2294–2304. (<a
href="https://doi.org/10.1080/01621459.2023.2257364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapley value is originally a concept in econometrics to fairly distribute both gains and costs to players in a coalition game. In the recent decades, its application has been extended to other areas such as marketing, engineering and machine learning. For example, it produces reasonable solutions for problems in sensitivity analysis, local model explanation toward the interpretable machine learning, node importance in social network, attribution models, etc. However, it could be very expensive to compute the Shapley value. Specifically, in a d -player coalition game, calculating a Shapley value requires the evaluation of d ! or 2 d marginal contribution values, depending on whether we are taking the permutation or combination formulation of the Shapley value. Hence, it becomes infeasible to calculate the Shapley value when d is reasonably large. A common remedy is to take a random sample of the permutations to surrogate for the complete list of permutations. We find an advanced sampling scheme can be designed to yield much more accurate estimation of the Shapley value than the simple random sampling (SRS). Our sampling scheme is based on combinatorial structures in the field of design of experiments (DOE), particularly the order-of-addition experimental designs for the study of how the orderings of components would affect the output. We show that the obtained estimates are unbiased, and can sometimes deterministically recover the original Shapley value. Both theoretical and simulations results show that our DOE-based sampling scheme outperforms SRS in terms of estimation accuracy. Surprisingly, it is also slightly faster than SRS. Lastly, real data analysis is conducted for the C. elegans nervous system and the 9/11 terrorist network. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Liuqing Yang and Yongdao Zhou and Haoda Fu and Min-Qian Liu and Wei Zheng},
  doi          = {10.1080/01621459.2023.2257364},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2294-2304},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast approximation of the shapley values based on order-of-addition experimental designs},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphical model inference with erosely measured data.
<em>JASA</em>, <em>119</em>(547), 2282–2293. (<a
href="https://doi.org/10.1080/01621459.2023.2256503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the Gaussian graphical model inference problem in a novel setting that we call erose measurements, referring to irregularly measured or observed data. For graphs, this results in different node pairs having vastly different sample sizes which frequently arises in data integration, genomics, neuroscience, and sensor networks. Existing works characterize the graph selection performance using the minimum pairwise sample size, which provides little insights for erosely measured data, and no existing inference method is applicable. We aim to fill in this gap by proposing the first inference method that characterizes the different uncertainty levels over the graph caused by the erose measurements, named GI-JOE (Graph Inference when Joint Observations are Erose). Specifically, we develop an edge-wise inference method and an affiliated FDR control procedure, where the variance of each edge depends on the sample sizes associated with corresponding neighbors. We prove statistical validity under erose measurements, thanks to careful localized edge-wise analysis and disentangling the dependencies across the graph. Finally, through simulation studies and a real neuroscience data example, we demonstrate the advantages of our inference methods for graph selection from erosely measured data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lili Zheng and Genevera I. Allen},
  doi          = {10.1080/01621459.2023.2256503},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2282-2293},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Graphical model inference with erosely measured data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric finite mixture: Applications in overcoming
misclassification bias. <em>JASA</em>, <em>119</em>(547), 2269–2281. (<a
href="https://doi.org/10.1080/01621459.2023.2256501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating the differential effect of treatments in groups defined by patient characteristics is of paramount importance in personalized medicine research. In some studies, participants are first classified as having or not of the characteristic of interest by diagnostic tools, but such classifiers may not be perfectly accurate. The impact of diagnostic misclassification in statistical inference has been recently investigated in parametric model contexts and shown to introduce severe bias in estimating treatment effects and give grossly inaccurate inferences. The article aims to address these problems in a fully nonparametric setting. Methods for consistently estimating and testing meaningful yet nonparametric treatment effects are developed. Along the way, we also construct estimators for misclassification error rates and investigate their asymptotic properties. The proposed methods are applicable for outcomes measured in ordinal, discrete, or continuous scales. They do not require any assumptions, such as the existence of moments. Simulation results show significant advantages of the proposed methods in bias reduction, coverage probability, and power. The applications of the proposed methods are illustrated with gene expression profiling of bronchial airway brushing in asthmatic and healthy control subjects. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zi Ye and Solomon W. Harrar},
  doi          = {10.1080/01621459.2023.2256501},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2269-2281},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric finite mixture: Applications in overcoming misclassification bias},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A negative correlation strategy for bracketing in
difference-in-differences. <em>JASA</em>, <em>119</em>(547), 2256–2268.
(<a href="https://doi.org/10.1080/01621459.2023.2252576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of difference-in-differences (DID) is widely used to study the causal effect of policy interventions in observational studies. DID employs a before and after comparison of the treated and control units to remove bias due to time-invariant unmeasured confounders under the parallel trends assumption. Estimates from DID, however, will be biased if the outcomes for the treated and control units evolve differently in the absence of treatment, namely if the parallel trends assumption is violated. We propose a general identification strategy that leverages two groups of control units whose outcomes relative to the treated units exhibit a negative correlation, and achieves partial identification of the average treatment effect for the treated. The identified set is of a union bounds form that involves the minimum and maximum operators, which makes the canonical bootstrap generally inconsistent and naive methods overly conservative. By using the directional inconsistency of the bootstrap distribution, we develop a novel bootstrap method to construct confidence intervals for the identified set and parameter of interest when the identified set is of a union bounds form, and we theoretically establish the asymptotic validity of the proposed method. We develop a simple falsification test and sensitivity analysis. We apply the proposed strategy for bracketing to study whether minimum wage laws affect employment levels. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ting Ye and Luke Keele and Raiden Hasegawa and Dylan S. Small},
  doi          = {10.1080/01621459.2023.2252576},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2256-2268},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A negative correlation strategy for bracketing in difference-in-differences},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact decoding of a sequentially markov coalescent model in
genetics. <em>JASA</em>, <em>119</em>(547), 2242–2255. (<a
href="https://doi.org/10.1080/01621459.2023.2252570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistical genetics, the sequentially Markov coalescent (SMC) is an important family of models for approximating the distribution of genetic variation data under complex evolutionary models. Methods based on SMC are widely used in genetics and evolutionary biology, with significant applications to genotype phasing and imputation, recombination rate estimation, and inferring population history. SMC allows for likelihood-based inference using hidden Markov models (HMMs), where the latent variable represents a genealogy. Because genealogies are continuous, while HMMs are discrete, SMC requires discretizing the space of trees in a way that is awkward and creates bias. In this work, we propose a method that circumvents this requirement, enabling SMC-based inference to be performed in the natural setting of a continuous state space. We derive fast, exact procedures for frequentist and Bayesian inference using SMC. Compared to existing methods, ours requires minimal user intervention or parameter tuning, no numerical optimization or E-M, and is faster and more accurate. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Caleb Ki and Jonathan Terhorst},
  doi          = {10.1080/01621459.2023.2252570},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2242-2255},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Exact decoding of a sequentially markov coalescent model in genetics},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified nonparametric fiducial approach to
interval-censored data. <em>JASA</em>, <em>119</em>(547), 2230–2241. (<a
href="https://doi.org/10.1080/01621459.2023.2252143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censored data, where the event time is partially observed, are challenging for survival probability estimation. In this article, we introduce a novel nonparametric fiducial approach to interval-censored data, including right-censored, current status, case II censored, and mixed case censored data. The proposed approach leveraging a simple Gibbs sampler has a useful property of being “one size fits all,” that is, the proposed approach automatically adapts to all types of noninformative censoring mechanisms. As shown in the extensive simulations, the proposed fiducial confidence intervals significantly outperform existing methods in terms of both coverage and length. In addition, the proposed fiducial point estimator has much smaller estimation errors than the nonparametric maximum likelihood estimator. Furthermore, we apply the proposed method to Austrian rubella data and a study of hemophiliacs infected with the human immunodeficiency virus. The strength of the proposed fiducial approach is not only estimation and uncertainty quantification but also its automatic adaptation to a variety of censoring mechanisms. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yifan Cui and Jan Hannig and Michael R. Kosorok},
  doi          = {10.1080/01621459.2023.2252143},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2230-2241},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A unified nonparametric fiducial approach to interval-censored data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent multimodal functional graphical model estimation.
<em>JASA</em>, <em>119</em>(547), 2217–2229. (<a
href="https://doi.org/10.1080/01621459.2023.2252142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is provably efficient, with the estimator converging to a stationary point with quantifiable statistical error. Furthermore, we show recovery of the latent graph under mild conditions. Our work is applied to analyze simultaneously acquired multimodal brain imaging data where the graph indicates functional connectivity of the brain. We present simulation and empirical results that support the benefits of joint estimation. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Katherine Tsai and Boxin Zhao and Sanmi Koyejo and Mladen Kolar},
  doi          = {10.1080/01621459.2023.2252142},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2217-2229},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Latent multimodal functional graphical model estimation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation and inference of extremal quantile treatment
effects for heavy-tailed distributions. <em>JASA</em>,
<em>119</em>(547), 2206–2216. (<a
href="https://doi.org/10.1080/01621459.2023.2252141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference for extreme events has many potential applications in fields such as climate science, medicine, and economics. We study the extremal quantile treatment effect of a binary treatment on a continuous, heavy-tailed outcome. Existing methods are limited to the case where the quantile of interest is within the range of the observations. For applications in risk assessment, however, the most relevant cases relate to extremal quantiles that go beyond the data range. We introduce an estimator of the extremal quantile treatment effect that relies on asymptotic tail approximation, and use a new causal Hill estimator for the extreme value indices of potential outcome distributions. We establish asymptotic normality of the estimators and propose a consistent variance estimator to achieve valid statistical inference. We illustrate the performance of our method in simulation studies, and apply it to a real dataset to estimate the extremal quantile treatment effect of college education on wage. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {David Deuber and Jinzhou Li and Sebastian Engelke and Marloes H. Maathuis},
  doi          = {10.1080/01621459.2023.2252141},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2206-2216},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation and inference of extremal quantile treatment effects for heavy-tailed distributions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network estimation by mixing: Adaptivity and more.
<em>JASA</em>, <em>119</em>(547), 2190–2205. (<a
href="https://doi.org/10.1080/01621459.2023.2252137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks analysis has been commonly used to study the interactions between units of complex systems. One problem of particular interest is learning the network’s underlying connection pattern given a single and noisy instantiation. While many methods have been proposed to address this problem in recent years, they usually assume that the true model belongs to a known class, which is not verifiable in most real-world applications. Consequently, network modeling based on these methods either suffers from model misspecification or relies on additional model selection procedures that are not well understood in theory and can potentially be unstable in practice. To address this difficulty, we propose a mixing strategy that leverages available arbitrary models to improve their individual performances. The proposed method is computationally efficient and almost tuning-free for network modeling. We show that the proposed method performs equally well as the oracle estimate when the true model is included as individual candidates. More importantly, the method remains robust and outperforms all current estimates even when the models are misspecified. Extensive simulation examples are used to verify the advantage of the proposed mixing method. Evaluation of link prediction performance on more than 500 real-world networks from different domains also demonstrates the universal competitiveness of the mixing method across multiple domains. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tianxi Li and Can M. Le},
  doi          = {10.1080/01621459.2023.2252137},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2190-2205},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Network estimation by mixing: Adaptivity and more},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unveiling the unobservable: Causal inference on multiple
derived outcomes. <em>JASA</em>, <em>119</em>(547), 2178–2189. (<a
href="https://doi.org/10.1080/01621459.2023.2252135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, the interest is in treatment effects on random quantities of subjects, where those random quantities are not directly observable but can be estimated based on data from each subject. In this article, we propose a general framework for conducting causal inference in a hierarchical data generation setting. The identifiability of causal parameters of interest is shown under a condition on the biasedness of subject level estimates and an ignorability condition on the treatment assignment. Estimation of the treatment effects is constructed by inverse propensity score weighting on the estimated subject level parameters. A multiple testing procedure able to control the false discovery proportion is proposed to identify the nonzero treatment effects. Theoretical results are developed to investigate the proposed procedure, and numerical simulations are carried out to evaluate its empirical performance. A case study of medication effects on brain functional connectivity of patients with Autism spectrum disorder (ASD) using fMRI data is conducted to demonstrate the utility of the proposed method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yumou Qiu and Jiarui Sun and Xiao-Hua Zhou},
  doi          = {10.1080/01621459.2023.2252135},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2178-2189},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Unveiling the unobservable: Causal inference on multiple derived outcomes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametrically efficient method for enveloped central
space. <em>JASA</em>, <em>119</em>(547), 2166–2177. (<a
href="https://doi.org/10.1080/01621459.2023.2252134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the central space is at the core of the sufficient dimension reduction (SDR) literature. However, it is well known that the finite-sample estimation suffers from collinearity among predictors. Cook, Helland, and Su proposed the predictor envelope method under linear models that can alleviate the problem by targeting a bigger space—which not only envelopes the central information, but also partitions the predictors by finding an uncorrelated set of material and immaterial predictors. One limitation of the predictor envelope is that it has strong distributional and modeling assumptions and therefore, it cannot be readily used in semiparametric settings where SDR usually nests. In this article, we generalize the envelope model by defining the enveloped central space and propose a semiparametric method to estimate it. We derive the entire class of regular and asymptotically linear (RAL) estimators as well as the locally and globally semiparametrically efficient estimators for the enveloped central space. Based on the connection between predictor envelope and partial least square (PLS), our methods can also be used to calculate the PLS space beyond linearity. In the simulations, our methods are shown to be both robust and accurate for estimating the enveloped central space under different settings. Moreover, the downstream analysis using state-of-the-art methods such as machine learning (ML) methods has the potential to achieve much better predictions. We further illustrate our methods in a heart failure study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Linquan Ma and Jixin Wang and Han Chen and Lan Liu},
  doi          = {10.1080/01621459.2023.2252134},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2166-2177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametrically efficient method for enveloped central space},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified unconditional regression for multivariate quantiles,
m-quantiles, and expectiles. <em>JASA</em>, <em>119</em>(547),
2154–2165. (<a
href="https://doi.org/10.1080/01621459.2023.2250512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a unified regression approach to model unconditional quantiles, M-quantiles and expectiles of multivariate dependent variables exploiting the multidimensional Huber’s function. To assess the impact of changes in the covariates across the entire unconditional distribution of the responses, we extend the work of Firpo, Fortin, and Lemieux by running a mean regression of the recentered influence function on the explanatory variables. We discuss the estimation procedure and establish the asymptotic properties of the derived estimators. A data-driven procedure is also presented to select the tuning constant of the Huber’s function. The validity of the proposed methodology is explored with simulation studies and through an application using the Survey of Household Income and Wealth 2016 conducted by the Bank of Italy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Luca Merlo and Lea Petrella and Nicola Salvati and Nikos Tzavidis},
  doi          = {10.1080/01621459.2023.2250512},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2154-2165},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Unified unconditional regression for multivariate quantiles, M-quantiles, and expectiles},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral clustering, bayesian spanning forest, and forest
process. <em>JASA</em>, <em>119</em>(547), 2140–2153. (<a
href="https://doi.org/10.1080/01621459.2023.2250098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering views the similarity matrix as a weighted graph, and partitions the data by minimizing a graph-cut loss. Since it minimizes the across-cluster similarity, there is no need to model the distribution within each cluster. As a result, one reduces the chance of model misspecification, which is often a risk in mixture model-based clustering. Nevertheless, compared to the latter, spectral clustering has no direct ways of quantifying the clustering uncertainty (such as the assignment probability), or allowing easy model extensions for complicated data applications. To fill this gap, we propose the Bayesian forest model as a generative graphical model for spectral clustering. This is motivated by our discovery that the posterior connecting matrix in a forest model has almost the same leading eigenvectors, as the ones used by normalized spectral clustering. To induce a distribution for the forest, we develop a “forest process” as a graph extension to the urn process, while we carefully characterize the differences in the partition probability. We derive a simple Markov chain Monte Carlo algorithm for posterior estimation, and demonstrate superior performance compared to existing algorithms. We illustrate several model-based extensions useful for data applications, including high-dimensional and multi-view clustering for images. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Leo L. Duan and Arkaprava Roy and For the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1080/01621459.2023.2250098},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2140-2153},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spectral clustering, bayesian spanning forest, and forest process},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive bayesian framework for envelope models.
<em>JASA</em>, <em>119</em>(547), 2129–2139. (<a
href="https://doi.org/10.1080/01621459.2023.2250096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The envelope model aims to increase efficiency in multivariate analysis by using dimension reduction techniques. It has been used in many contexts including linear regression, generalized linear models, matrix/tensor variate regression, reduced rank regression, and quantile regression, and has shown the potential to provide substantial efficiency gains. Virtually all of these advances, however, have been made from a frequentist perspective, and the literature addressing envelope models from a Bayesian point of view is sparse. The objective of this article is to propose a Bayesian framework that is applicable across various envelope model contexts. The proposed framework aids straightforward interpretation of model parameters and allows easy incorporation of prior information. We provide a simple block Metropolis-within-Gibbs MCMC sampler for practical implementations of our method. Simulations and data examples are included for illustration. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Saptarshi Chakraborty and Zhihua Su},
  doi          = {10.1080/01621459.2023.2250096},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2129-2139},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A comprehensive bayesian framework for envelope models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical likelihood for network data. <em>JASA</em>,
<em>119</em>(547), 2117–2128. (<a
href="https://doi.org/10.1080/01621459.2023.2250091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a concept of nonparametric likelihood for network data based on network moments, and proposes general inference methods by adapting the theory of jackknife empirical likelihood. Our methodology can be used not only to conduct inference on population network moments and parameters in network formation models, but also to implement goodness-of-fit testing, such as testing block size for stochastic block models. Theoretically we show that the jackknife empirical likelihood statistic for acyclic or cyclic subgraph moments loses its asymptotic pivotalness in severely or moderately sparse cases, respectively, and develop a modified statistic to recover pivotalness in such cases. The main advantage of our modified jackknife empirical likelihood method is its validity under weaker sparsity conditions than existing methods although it is computationally more demanding than the unmodified version. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yukitoshi Matsushita and Taisuke Otsu},
  doi          = {10.1080/01621459.2023.2250091},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2117-2128},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Empirical likelihood for network data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonstationary soft partitioned gaussian process model via
random spanning trees. <em>JASA</em>, <em>119</em>(547), 2105–2116. (<a
href="https://doi.org/10.1080/01621459.2023.2249642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a long-standing challenge in developing locally stationary Gaussian process models concerning how to obtain flexible partitions and make predictions near boundaries. In this work, we develop a new class of locally stationary stochastic processes, where local partitions are modeled by a soft partition process via predictive random spanning trees that leads to highly flexible spatially contiguous subregion shapes. This valid nonstationary process model knits together local models such that both parameter estimation and prediction can be performed under a unified and coherent framework, and it captures both discontinuities/abrupt changes and local smoothness in a spatial random field. We propose a theoretical framework to study the Bayesian posterior concentration concerning the behavior of this Bayesian nonstationary process model. The performance of the proposed model is illustrated with simulation studies and real data analysis of precipitation rates over the contiguous United States. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhao Tang Luo and Huiyan Sang and Bani Mallick},
  doi          = {10.1080/01621459.2023.2249642},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2105-2116},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A nonstationary soft partitioned gaussian process model via random spanning trees},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCABM: Pairwise covariates-adjusted block model for
community detection. <em>JASA</em>, <em>119</em>(547), 2092–2104. (<a
href="https://doi.org/10.1080/01621459.2023.2244731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most fundamental problems in network study is community detection. The stochastic block model (SBM) is a widely used model, and various estimation methods have been developed with their community detection consistency results unveiled. However, the SBM is restricted by the strong assumption that all nodes in the same community are stochastically equivalent, which may not be suitable for practical applications. We introduce a pairwise covariates-adjusted stochastic block model (PCABM), a generalization of SBM that incorporates pairwise covariate information. We study the maximum likelihood estimators of the coefficients for the covariates as well as the community assignments, and show they are consistent under suitable sparsity conditions. Spectral clustering with adjustment (SCWA) is introduced to efficiently solve PCABM. Under certain conditions, we derive the error bound of community detection for SCWA and show that it is community detection consistent. In addition, we investigate model selection in terms of the number of communities and feature selection for the pairwise covariates, and propose two corresponding algorithms. PCABM compares favorably with the SBM or degree-corrected stochastic block model (DCBM) under a wide range of simulated and real networks when covariate information is accessible. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sihan Huang and Jiajin Sun and Yang Feng},
  doi          = {10.1080/01621459.2023.2244731},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2092-2104},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {PCABM: Pairwise covariates-adjusted block model for community detection},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ideal bayesian spatial adaptation. <em>JASA</em>,
<em>119</em>(547), 2078–2091. (<a
href="https://doi.org/10.1080/01621459.2023.2241705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-life applications involve estimation of curves that exhibit complicated shapes including jumps or varying-frequency oscillations. Practical methods have been devised that can adapt to a locally varying complexity of an unknown function (e.g., variable-knot splines, sparse wavelet reconstructions, kernel methods or trees/forests). However, the overwhelming majority of existing asymptotic minimaxity theory is predicated on homogeneous smoothness assumptions. Focusing on locally Hölder functions, we provide new locally adaptive posterior concentration rate results under the supremum loss for widely used Bayesian machine learning techniques in white noise and nonparametric regression. In particular, we show that popular spike-and-slab priors and Bayesian CART are uniformly locally adaptive. In addition, we propose a new class of repulsive partitioning priors which relate to variable knot splines and which are exact-rate adaptive. For uncertainty quantification, we construct locally adaptive confidence bands whose width depends on the local smoothness and which achieve uniform asymptotic coverage under local self-similarity. To illustrate that spatial adaptation is not at all automatic, we provide lower-bound results showing that popular hierarchical Gaussian process priors fall short of spatial adaptation. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Veronika Ročková and Judith Rousseau},
  doi          = {10.1080/01621459.2023.2241705},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2078-2091},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ideal bayesian spatial adaptation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference in high-dimensional multivariate response
regression with hidden variables. <em>JASA</em>, <em>119</em>(547),
2066–2077. (<a
href="https://doi.org/10.1080/01621459.2023.2241701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the inference of the regression coefficient matrix under multivariate response linear regressions in the presence of hidden variables. A novel procedure for constructing confidence intervals of entries of the coefficient matrix is proposed. Our method first uses the multivariate nature of the responses by estimating and adjusting the hidden effect to construct an initial estimator of the coefficient matrix. By further deploying a low-dimensional projection procedure to reduce the bias introduced by the regularization in the previous step, a refined estimator is proposed and shown to be asymptotically normal. The asymptotic variance of the resulting estimator is derived with closed-form expression and can be consistently estimated. In addition, we propose a testing procedure for the existence of hidden effects and provide its theoretical justification. Both our procedures and their analyses are valid even when the feature dimension and the number of responses exceed the sample size. Our results are further backed up via extensive simulations and a real data analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xin Bing and Wei Cheng and Huijie Feng and Yang Ning},
  doi          = {10.1080/01621459.2023.2241701},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2066-2077},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference in high-dimensional multivariate response regression with hidden variables},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). De-confounding causal inference using latent
multiple-mediator pathways. <em>JASA</em>, <em>119</em>(547), 2051–2065.
(<a href="https://doi.org/10.1080/01621459.2023.2240461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal effect estimation from observational data is one of the essential problems in causal inference. However, most estimation methods rely on the strong assumption that all confounders are observed, which is impractical and untestable in the real world. We develop a mediation analysis framework inferring the latent confounder for debiasing both direct and indirect causal effects. Specifically, we introduce generalized structural equation modeling that incorporates structured latent factors to improve the goodness-of-fit of the model to observed data, and deconfound the mediators and outcome simultaneously. One major advantage of the proposed framework is that it uses the causal pathway structure from cause to outcome via multiple mediators to debias the causal effect without requiring external information on latent confounders. In addition, the proposed framework is flexible in terms of integrating powerful nonparametric prediction algorithms while retaining interpretable mediation effects. In theory, we establish the identification of both causal and mediation effects based on the proposed deconfounding method. Numerical experiments on both simulation settings and a normative aging study indicate that the proposed approach reduces the estimation bias of both causal and mediation effects. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yubai Yuan and Annie Qu},
  doi          = {10.1080/01621459.2023.2240461},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2051-2065},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {De-confounding causal inference using latent multiple-mediator pathways},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional time series segmentation via
factor-adjusted vector autoregressive modeling. <em>JASA</em>,
<em>119</em>(547), 2038–2050. (<a
href="https://doi.org/10.1080/01621459.2023.2240054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector autoregressive (VAR) models are popularly adopted for modeling high-dimensional time series, and their piecewise extensions allow for structural changes in the data. In VAR modeling, the number of parameters grow quadratically with the dimensionality which necessitates the sparsity assumption in high dimensions. However, it is debatable whether such an assumption is adequate for handling datasets exhibiting strong serial and cross-sectional correlations. We propose a piecewise stationary time series model that simultaneously allows for strong correlations as well as structural changes, where pervasive serial and cross-sectional correlations are accounted for by a time-varying factor structure, and any remaining idiosyncratic dependence between the variables is handled by a piecewise stationary VAR model. We propose an accompanying two-stage data segmentation methodology which fully addresses the challenges arising from the latency of the component processes. Its consistency in estimating both the total number and the locations of the change points in the latent components, is established under conditions considerably more general than those in the existing literature. We demonstrate the competitive performance of the proposed methodology on simulated datasets and an application to U.S. blue chip stocks data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Haeran Cho and Hyeyoung Maeng and Idris A. Eckley and Paul Fearnhead},
  doi          = {10.1080/01621459.2023.2240054},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2038-2050},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional time series segmentation via factor-adjusted vector autoregressive modeling},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity to unobserved confounding in studies with
factor-structured outcomes. <em>JASA</em>, <em>119</em>(547), 2026–2037.
(<a href="https://doi.org/10.1080/01621459.2023.2240053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose an approach for assessing sensitivity to unobserved confounding in studies with multiple outcomes. We demonstrate how prior knowledge unique to the multi-outcome setting can be leveraged to strengthen causal conclusions beyond what can be achieved from analyzing individual outcomes in isolation. We argue that it is often reasonable to make a shared confounding assumption, under which residual dependence amongst outcomes can be used to simplify and sharpen sensitivity analyses. We focus on a class of factor models for which we can bound the causal effects for all outcomes conditional on a single sensitivity parameter that represents the fraction of treatment variance explained by unobserved confounders. We characterize how causal ignorance regions shrink under additional prior assumptions about the presence of null control outcomes, and provide new approaches for quantifying the robustness of causal effect estimates. Finally, we illustrate our sensitivity analysis workflow in practice, in an analysis of both simulated data and a case study with data from the National Health and Nutrition Examination Survey (NHANES). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiajing Zheng and Jiaxi Wu and Alexander D’Amour and Alexander Franks},
  doi          = {10.1080/01621459.2023.2240053},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2026-2037},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sensitivity to unobserved confounding in studies with factor-structured outcomes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value enhancement of reinforcement learning via efficient
and robust trust region optimization. <em>JASA</em>, <em>119</em>(547),
2011–2025. (<a
href="https://doi.org/10.1080/01621459.2023.2238942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is a powerful machine learning technique that enables an intelligent agent to learn an optimal policy that maximizes the cumulative rewards in sequential decision making. Most of methods in the existing literature are developed in online settings where the data are easy to collect or simulate. Motivated by high stake domains such as mobile health studies with limited and pre-collected data, in this article, we study offline reinforcement learning methods. To efficiently use these datasets for policy optimization, we propose a novel value enhancement method to improve the performance of a given initial policy computed by existing state-of-the-art RL algorithms. Specifically, when the initial policy is not consistent, our method will output a policy whose value is no worse and often better than that of the initial policy. When the initial policy is consistent, under some mild conditions, our method will yield a policy whose value converges to the optimal one at a faster rate than the initial policy, achieving the desired “value enhancement” property. The proposed method is generally applicable to any parameterized policy that belongs to certain pre-specified function class (e.g., deep neural networks). Extensive numerical studies are conducted to demonstrate the superior performance of our method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Zhengling Qi and Jianing Wang and Fan Zhou},
  doi          = {10.1080/01621459.2023.2238942},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2011-2025},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Value enhancement of reinforcement learning via efficient and robust trust region optimization},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal dynamic treatment regimes and partial welfare
ordering. <em>JASA</em>, <em>119</em>(547), 2000–2010. (<a
href="https://doi.org/10.1080/01621459.2023.2238941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes are treatment allocations tailored to heterogeneous individuals (e.g., via previous outcomes and covariates). The optimal dynamic treatment regime is a regime that maximizes counterfactual welfare. We introduce a framework in which we can partially learn the optimal dynamic regime from observational data, relaxing the sequential randomization assumption commonly employed in the literature but instead using (binary) instrumental variables. We propose the notion of sharp partial ordering of counterfactual welfares with respect to dynamic regimes and establish mapping from data to partial ordering via a set of linear programs. We then characterize the identified set of the optimal regime as the set of maximal elements associated with the partial ordering. We relate the notion of partial ordering with a more conventional notion of partial identification using topological sorts. Practically, topological sorts can be served as a policy benchmark for a policymaker. We apply our method to understand returns to schooling and post-school training as a sequence of treatments by combining data from multiple sources. The framework of this article can be used beyond the current context, for example, in establishing rankings of multiple treatments or policies across different counterfactual scenarios. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sukjin Han},
  doi          = {10.1080/01621459.2023.2238941},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {2000-2010},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal dynamic treatment regimes and partial welfare ordering},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariate-assisted bayesian graph learning for heterogeneous
data. <em>JASA</em>, <em>119</em>(547), 1985–1999. (<a
href="https://doi.org/10.1080/01621459.2023.2233744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a traditional Gaussian graphical model, data homogeneity is routinely assumed with no extra variables affecting the conditional independence. In modern genomic datasets, there is an abundance of auxiliary information, which often gets under-utilized in determining the joint dependency structure. In this article, we consider a Bayesian approach to model undirected graphs underlying heterogeneous multivariate observations with additional assistance from covariates. Building on product partition models, we propose a novel covariate-dependent Gaussian graphical model that allows graphs to vary with covariates so that observations whose covariates are similar share a similar undirected graph. To efficiently embed Gaussian graphical models into our proposed framework, we explore both Gaussian likelihood and pseudo-likelihood functions. For Gaussian likelihood, a G-Wishart distribution is used as a natural conjugate prior, and for the pseudo-likelihood, a product of Gaussian-conditionals is used. Moreover, the proposed model has large prior support and is flexible to approximate any ν -Hölder conditional variance-covariance matrices with ν ∈ ( 0 , 1 ] . We further show that based on the theory of fractional likelihood, the rate of posterior contraction is minimax optimal assuming the true density to be a Gaussian mixture with a known number of components. The efficacy of the approach is demonstrated via simulation studies and an analysis of a protein network for a breast cancer dataset assisted by mRNA gene expression as covariates. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yabo Niu and Yang Ni and Debdeep Pati and Bani K. Mallick},
  doi          = {10.1080/01621459.2023.2233744},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1985-1999},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Covariate-assisted bayesian graph learning for heterogeneous data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for maximin effects: Identifying
stable associations across multiple studies. <em>JASA</em>,
<em>119</em>(547), 1968–1984. (<a
href="https://doi.org/10.1080/01621459.2023.2233162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative analysis of data from multiple sources is critical to making generalizable discoveries. Associations consistently observed across multiple source populations are more likely to be generalized to target populations with possible distributional shifts. In this article, we model the heterogeneous multi-source data with multiple high-dimensional regressions and make inferences for the maximin effect (Meinshausen and Bühlmann, AoS, 43(4), 1801–1830). The maximin effect provides a measure of stable associations across multi-source data. A significant maximin effect indicates that a variable has commonly shared effects across multiple source populations, and these shared effects may be generalized to a broader set of target populations. There are challenges associated with inferring maximin effects because its point estimator can have a nonstandard limiting distribution. We devise a novel sampling method to construct valid confidence intervals for maximin effects. The proposed confidence interval attains a parametric length. This sampling procedure and the related theoretical analysis are of independent interest for solving other nonstandard inference problems. Using genetic data on yeast growth in multiple environments, we demonstrate that the genetic variants with significant maximin effects have generalizable effects under new environments. The proposed method is implemented in the R package MaximinInfer available from CRAN. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zijian Guo},
  doi          = {10.1080/01621459.2023.2233162},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1968-1984},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for maximin effects: Identifying stable associations across multiple studies},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor augmented inverse regression and its application to
microbiome data analysis. <em>JASA</em>, <em>119</em>(547), 1957–1967.
(<a href="https://doi.org/10.1080/01621459.2023.2231577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the relationship between count data that inform the relative abundance of features of a composition, and factors that influence the composition. Our work is motivated from microbiome studies aiming to extract microbial signatures that are predictive of host phenotypes based on data collected from a group of individuals harboring radically different microbial communities. We introduce multinomial Factor Augmented Inverse Regression (FAIR) of the count vector onto response factors as a general framework for obtaining low-dimensional summaries of the count vector that preserve information relevant to the response. By augmenting known response factors with random latent factors, FAIR extends multinomial logistic regression to account for overdispersion and general correlations among counts. The projections of the count vector onto the loading vectors represent additional contribution, in addition to the projections that result from response factors. The method of maximum variational likelihood and a fast variational expectation-maximization algorithm are proposed for approximate inference based on variational approximation, and the asymptotic properties of the resulting estimator are derived. Moreover, a hybrid information criterion and a group-lasso penalized criterion are proposed for model selection. The effectiveness of FAIR is illustrated through simulations and application to a microbiome dataset. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Daolin Pang and Hongyu Zhao and Tao Wang},
  doi          = {10.1080/01621459.2023.2231577},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1957-1967},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Factor augmented inverse regression and its application to microbiome data analysis},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frequency detection and change point estimation for time
series of complex oscillation. <em>JASA</em>, <em>119</em>(547),
1945–1956. (<a
href="https://doi.org/10.1080/01621459.2023.2229486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider detecting the evolutionary oscillatory pattern of a signal when it is contaminated by nonstationary noises with complexly time-varying data generating mechanism. A high-dimensional dense progressive periodogram test is proposed to accurately detect all oscillatory frequencies. A further phase-adjusted local change point detection algorithm is applied in the frequency domain to detect the locations at which the oscillatory pattern changes. Our method is shown to be able to detect all oscillatory frequencies and the corresponding change points within an accurate range with a prescribed probability asymptotically. A Gaussian approximation scheme and an overlapping-block multiplier bootstrap methodology for sums of complex-valued high dimensional nonstationary time series without variance lower bounds are established, which could be of independent interest. This study is motivated by oscillatory frequency estimation and change point detection problems encountered in physiological time series analysis. An application to spindle detection and estimation in electroencephalogram recorded during sleep is used to illustrate the usefulness of the proposed methodology. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Hau-Tieng Wu and Zhou Zhou},
  doi          = {10.1080/01621459.2023.2229486},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1945-1956},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Frequency detection and change point estimation for time series of complex oscillation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DDAC-SpAM: A distributed algorithm for fitting
high-dimensional sparse additive models with feature division and
decorrelation. <em>JASA</em>, <em>119</em>(547), 1933–1944. (<a
href="https://doi.org/10.1080/01621459.2023.2225743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Distributed statistical learning has become a popular technique for large-scale data analysis. Most existing work in this area focuses on dividing the observations, but we propose a new algorithm, DDAC-SpAM, which divides the features under a high-dimensional sparse additive model. Our approach involves three steps: divide, decorrelate, and conquer. The decorrelation operation enables each local estimator to recover the sparsity pattern for each additive component without imposing strict constraints on the correlation structure among variables. The effectiveness and efficiency of the proposed algorithm are demonstrated through theoretical analysis and empirical results on both synthetic and real data. The theoretical results include both the consistent sparsity pattern recovery as well as statistical inference for each additive functional component. Our approach provides a practical solution for fitting sparse additive models, with promising applications in a wide range of domains. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yifan He and Ruiyang Wu and Yong Zhou and Yang Feng},
  doi          = {10.1080/01621459.2023.2225743},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1933-1944},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {DDAC-SpAM: A distributed algorithm for fitting high-dimensional sparse additive models with feature division and decorrelation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral embedding of weighted graphs. <em>JASA</em>,
<em>119</em>(547), 1923–1932. (<a
href="https://doi.org/10.1080/01621459.2023.2225239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When analyzing weighted networks using spectral embedding, a judicious transformation of the edge weights may produce better results. To formalize this idea, we consider the asymptotic behavior of spectral embedding for different edge-weight representations, under a generic low rank model. We measure the quality of different embeddings—which can be on entirely different scales—by how easy it is to distinguish communities, in an information-theoretical sense. For common types of weighted graphs, such as count networks or p -value networks, we find that transformations such as tempering or thresholding can be highly beneficial, both in theory and in practice. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ian Gallagher and Andrew Jones and Anna Bertiger and Carey E. Priebe and Patrick Rubin-Delanchy},
  doi          = {10.1080/01621459.2023.2225239},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1923-1932},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Spectral embedding of weighted graphs},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate sparse clustering for extremes. <em>JASA</em>,
<em>119</em>(547), 1911–1922. (<a
href="https://doi.org/10.1080/01621459.2023.2224517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying directions where extreme events occur is a significant challenge in multivariate extreme value analysis. In this article, we use the concept of sparse regular variation introduced by Meyer and Wintenberger to infer the tail dependence of a random vector X. This approach relies on the Euclidean projection onto the simplex which better exhibits the sparsity structure of the tail of X than the standard methods. Our procedure based on a rigorous methodology aims at capturing clusters of extremal coordinates of X. It also includes the identification of the threshold above which the values taken by X are considered extreme. We provide an efficient and scalable algorithm called MUSCLE and apply it to numerical examples to highlight the relevance of our findings. Finally, we illustrate our approach with financial return data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Nicolas Meyer and Olivier Wintenberger},
  doi          = {10.1080/01621459.2023.2224517},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1911-1922},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multivariate sparse clustering for extremes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the estimation of the number of communities for sparse
networks. <em>JASA</em>, <em>119</em>(547), 1895–1910. (<a
href="https://doi.org/10.1080/01621459.2023.2223793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the nonparametric methods of estimating the number of communities ( K ) in a community detection problem, methods based on the spectrum of the Bethe Hessian matrices ( H ζ H ζ Hζ with the scalar parameter ζ ) have garnered much popularity for their simplicity, computational efficiency, and robustness to the sparsity of data. For certain heuristic choices of ζ , such methods have been shown to be consistent for networks with N nodes with a common expected degree of ω ( log N ) . In this article, we obtain several finite sample results to show that if the input network is generated from either stochastic block models or degree-corrected block models, and if ζ is chosen from a certain interval, then the associated spectral methods based on H ζ is consistent for estimating K for the sub-logarithmic sparse regime, when the expected maximum degree is both o ( log N ) and ω ( 1 ) , under some mild conditions even in the situation when K increases with N . We also propose a method to estimate the aforementioned interval empirically, which enables us to develop a consistent K estimation procedure in the sparse regime. We evaluate the performance of the resulting estimation procedure theoretically, also empirically through extensive simulation studies and application to a comprehensive collection of real-world network data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Neil Hwang and Jiarui Xu and Shirshendu Chatterjee and Sharmodeep Bhattacharyya},
  doi          = {10.1080/01621459.2023.2223793},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1895-1910},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On the estimation of the number of communities for sparse networks},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact bayesian inference for diffusion-driven cox processes.
<em>JASA</em>, <em>119</em>(547), 1882–1894. (<a
href="https://doi.org/10.1080/01621459.2023.2223791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel methodology to perform Bayesian inference for Cox processes in which the intensity function is driven by a diffusion process. The novelty lies in the fact that no discretization error is involved, despite the non-tractability of both the likelihood function and the transition density of the diffusion. The methodology is based on an MCMC algorithm and its exactness is built on retrospective sampling techniques. The efficiency of the methodology is investigated in some simulated examples and its applicability is illustrated in some real data analyzes. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Flávio B. Gonçalves and Krzysztof G. Łatuszyński and Gareth O. Roberts},
  doi          = {10.1080/01621459.2023.2223791},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1882-1894},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Exact bayesian inference for diffusion-driven cox processes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bahadur efficiency of observational block designs.
<em>JASA</em>, <em>119</em>(547), 1871–1881. (<a
href="https://doi.org/10.1080/01621459.2023.2221402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To be convincing, an observational or nonrandomized study of causal effects must demonstrate that its conclusions cannot be readily explained by a small unmeasured bias in the way individuals were assigned to treatment or control. The Bahadur relative efficiency of a sensitivity analysis compares the performance of different test statistics or different research designs when sensitivity to unmeasured bias is appraised: better statistics and better designs exhibit insensitivity to larger biases. Bahadur efficiency is relevant here because, unlike Pitman efficiency, it can depict efficiency with an effect that is not trivially small: every trivially small treatment effect is sensitive to trivially small biases. The Bahadur efficiency of a sensitivity analysis has been used by various authors in the simple case of matched pairs, but the technical issues are more complex in the case of blocks larger than pairs, and they are developed here for the first time. Choosing a better test statistic for a block design, or choosing a better block size—larger than pairs—can result in enormous increases in the efficiency of a sensitivity analysis. An adaptive choice of test statistic can ensure the better Bahadur efficiency of two competing statistics. An R package weightedRank implements the methods, contains the example and reproduces its analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Paul R. Rosenbaum},
  doi          = {10.1080/01621459.2023.2221402},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1871-1881},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bahadur efficiency of observational block designs},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double negative control inference in test-negative design
studies of vaccine effectiveness. <em>JASA</em>, <em>119</em>(547),
1859–1870. (<a
href="https://doi.org/10.1080/01621459.2023.2220935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The test-negative design (TND) has become a standard approach to evaluate vaccine effectiveness against the risk of acquiring infectious diseases in real-world settings, such as Influenza, Rotavirus, Dengue fever, and more recently COVID-19. In a TND study, individuals who experience symptoms and seek care are recruited and tested for the infectious disease which defines cases and controls. Despite TND’s potential to reduce unobserved differences in healthcare seeking behavior (HSB) between vaccinated and unvaccinated subjects, it remains subject to various potential biases. First, residual confounding may remain due to unobserved HSB, occupation as healthcare worker, or previous infection history. Second, because selection into the TND sample is a common consequence of infection and HSB, collider stratification bias may exist when conditioning the analysis on tested samples, which further induces confounding by latent HSB. In this article, we present a novel approach to identify and estimate vaccine effectiveness in the target population by carefully leveraging a pair of negative control exposure and outcome variables to account for potential hidden bias in TND studies. We illustrate our proposed method with extensive simulations and an application to study COVID-19 vaccine effectiveness using data from the University of Michigan Health System. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kendrick Qijun Li and Xu Shi and Wang Miao and Eric Tchetgen Tchetgen},
  doi          = {10.1080/01621459.2023.2220935},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1859-1870},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Double negative control inference in test-negative design studies of vaccine effectiveness},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian inference using the proximal mapping: Uncertainty
quantification under varying dimensionality. <em>JASA</em>,
<em>119</em>(547), 1847–1858. (<a
href="https://doi.org/10.1080/01621459.2023.2220170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistical applications, it is common to encounter parameters supported on a varying or unknown dimensional space. Examples include the fused lasso regression, the matrix recovery under an unknown low rank, etc. Despite the ease of obtaining a point estimate via optimization, it is much more challenging to quantify their uncertainty. In the Bayesian framework, a major difficulty is that if assigning the prior associated with a p -dimensional measure, then there is zero posterior probability on any lower-dimensional subset with dimension d &lt; p . To avoid this caveat, one needs to choose another dimension-selection prior on d , which often involves a highly combinatorial problem. To significantly reduce the modeling burden, we propose a new generative process for the prior: starting from a continuous random variable such as multivariate Gaussian, we transform it into a varying-dimensional space using the proximal mapping. This leads to a large class of new Bayesian models that can directly exploit the popular frequentist regularizations and their algorithms, such as the nuclear norm penalty and the alternating direction method of multipliers, while providing a principled and probabilistic uncertainty estimation. We show that this framework is well justified in the geometric measure theory, and enjoys a convenient posterior computation via the standard Hamiltonian Monte Carlo. We demonstrate its use in the analysis of the dynamic flow network data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Maoran Xu and Hua Zhou and Yujie Hu and Leo L. Duan},
  doi          = {10.1080/01621459.2023.2220170},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1847-1858},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian inference using the proximal mapping: Uncertainty quantification under varying dimensionality},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing directed acyclic graph via structural, supervised
and generative adversarial learning. <em>JASA</em>, <em>119</em>(547),
1833–1846. (<a
href="https://doi.org/10.1080/01621459.2023.2220169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new hypothesis testing method for directed acyclic graph (DAG). While there is a rich class of DAG estimation methods, there is a relative paucity of DAG inference solutions. Moreover, the existing methods often impose some specific model structures such as linear models or additive models, and assume independent data observations. Our proposed test instead allows the associations among the random variables to be nonlinear and the data to be time-dependent. We build the test based on some highly flexible neural networks learners. We establish the asymptotic guarantees of the test, while allowing either the number of subjects or the number of time points for each subject to diverge to infinity. We demonstrate the efficacy of the test through simulations and a brain connectivity network analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Yunzhe Zhou and Lexin Li},
  doi          = {10.1080/01621459.2023.2220169},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1833-1846},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing directed acyclic graph via structural, supervised and generative adversarial learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian approximation and spatially dependent wild
bootstrap for high-dimensional spatial data. <em>JASA</em>,
<em>119</em>(547), 1820–1832. (<a
href="https://doi.org/10.1080/01621459.2023.2218578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish a high-dimensional CLT for the sample mean of p -dimensional spatial data observed over irregularly spaced sampling sites in R d , allowing the dimension p to be much larger than the sample size n . We adopt a stochastic sampling scheme that can generate irregularly spaced sampling sites in a flexible manner and include both pure increasing domain and mixed increasing domain frameworks. To facilitate statistical inference, we develop the spatially dependent wild bootstrap (SDWB) and justify its asymptotic validity in high dimensions by deriving error bounds that hold almost surely conditionally on the stochastic sampling sites. Our dependence conditions on the underlying random field cover a wide class of random fields such as Gaussian random fields and continuous autoregressive moving average random fields. Through numerical simulations and a real data analysis, we demonstrate the usefulness of our bootstrap-based inference in several applications, including joint confidence interval construction for high-dimensional spatial data and change-point detection for spatio-temporal data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Daisuke Kurisu and Kengo Kato and Xiaofeng Shao},
  doi          = {10.1080/01621459.2023.2218578},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1820-1832},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Gaussian approximation and spatially dependent wild bootstrap for high-dimensional spatial data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classified mixed model projections. <em>JASA</em>,
<em>119</em>(547), 1805–1819. (<a
href="https://doi.org/10.1080/01621459.2023.2218041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical problems, there is interest in the estimation of mixed effect projections for new data that are outside the range of the training data. Examples include predicting extreme small area means for rare populations or making treatment decisions for patients who do not fit typical risk profiles. Standard methods have long been known to struggle with such problems since the training data may not provide enough information about potential model changes for these new data values (extrapolation bias). We propose a new framework called Prediction Using Random-effect Extrapolation (PURE) which involves constructing a generalized independent variable hull (gIVH) to isolate a minority training set which is “close” to the prediction space, followed by a regrouping of the minority data according to the response variable which results in a new (but misspecified) random effect distribution. This misspecification reflects “extrapolated random effects” which prove vital to capture information that is needed for accurate model projections. Projections are then made using classified mixed model prediction (CMMP) (?) with the regrouped minority data. Comprehensive simulation studies and analysis of data from the National Longitudinal Mortality Study (NLMS) demonstrate superior predictive performance in these very challenging paradigms. An asymptotic analysis reveals why PURE results in more accurate projections. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {J. Sunil Rao and Mengying Li and Jiming Jiang},
  doi          = {10.1080/01621459.2023.2218041},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1805-1819},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Classified mixed model projections},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic distribution-free independence test for
high-dimension data. <em>JASA</em>, <em>119</em>(547), 1794–1804. (<a
href="https://doi.org/10.1080/01621459.2023.2218030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test of independence is of fundamental importance in modern data analysis, with broad applications in variable selection, graphical models, and causal inference. When the data is high dimensional and the potential dependence signal is sparse, independence testing becomes very challenging without distributional or structural assumptions. In this article, we propose a general framework for independence testing by first fitting a classifier that distinguishes the joint and product distributions, and then testing the significance of the fitted classifier. This framework allows us to borrow the strength of the most advanced classification algorithms developed from the modern machine learning community, making it applicable to high dimensional, complex data. By combining a sample split and a fixed permutation, our test statistic has a universal, fixed Gaussian null distribution that is independent of the underlying data distribution. Extensive simulations demonstrate the advantages of the newly proposed test compared with existing methods. We further apply the new test to a single cell dataset to test the independence between two types of single cell sequencing measurements, whose high dimensionality and sparsity make existing methods hard to apply. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhanrui Cai and Jing Lei and Kathryn Roeder},
  doi          = {10.1080/01621459.2023.2218030},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1794-1804},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Asymptotic distribution-free independence test for high-dimension data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated approach to causal inference in discrete
settings. <em>JASA</em>, <em>119</em>(547), 1778–1793. (<a
href="https://doi.org/10.1080/01621459.2023.2216909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applied research conditions often make it impossible to point-identify causal estimands without untenable assumptions. Partial identification —bounds on the range of possible solutions—is a principled alternative, but the difficulty of deriving bounds in idiosyncratic settings has restricted its application. We present a general, automated numerical approach to causal inference in discrete settings. We show causal questions with discrete data reduce to polynomial programming problems, then present an algorithm to automatically bound causal effects using efficient dual relaxation and spatial branch-and-bound techniques. The user declares an estimand, states assumptions, and provides data—however incomplete or mismeasured. The algorithm then searches over admissible data-generating processes and outputs the most precise possible range consistent with available information—that is, sharp bounds—including a point-identified solution if one exists. Because this search can be computationally intensive, our procedure reports and continually refines non-sharp ranges guaranteed to contain the truth at all times, even when the algorithm is not run to completion. Moreover, it offers an ε -sharpness guarantee, characterizing the worst-case looseness of the incomplete bounds. These techniques are implemented in our Python package, autobounds . Analytically validated simulations show the method accommodates classic obstacles—including confounding, selection, measurement error, noncompliance, and nonresponse. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Guilherme Duarte and Noam Finkelstein and Dean Knox and Jonathan Mummolo and Ilya Shpitser},
  doi          = {10.1080/01621459.2023.2216909},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1778-1793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An automated approach to causal inference in discrete settings},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tree-guided rare feature selection and logic aggregation
with electronic health records data. <em>JASA</em>, <em>119</em>(547),
1765–1777. (<a
href="https://doi.org/10.1080/01621459.2024.2326621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning with a large number of rare binary features is commonly encountered in analyzing electronic health records (EHR) data, especially in the modeling of disease onset with prior medical diagnoses and procedures. Dealing with the resulting highly sparse and large-scale binary feature matrix is notoriously challenging as conventional methods may suffer from a lack of power in testing and inconsistency in model fitting, while machine learning methods may suffer from the inability of producing interpretable results or clinically-meaningful risk factors. To improve EHR-based modeling and use the natural hierarchical structure of disease classification, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features, in which dimension reduction is achieved through not only a sparsity pursuit but also an aggregation promoter with the logic operator of “or”. We convert the combinatorial problem into a convex linearly-constrained regularized estimation, which enables scalable computation with theoretical guarantees. In a suicide risk study with EHR data, our approach is able to select and aggregate prior mental health diagnoses as guided by the diagnosis hierarchy of the International Classification of Diseases. By balancing the rarity and specificity of the EHR diagnosis records, our strategy improves both prediction and interpretation. We identify important higher-level categories and subcategories of mental health conditions and simultaneously determine the level of specificity needed for each of them in associating with suicide risk. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Jianmin Chen and Robert H. Aseltine and Fei Wang and Kun Chen},
  doi          = {10.1080/01621459.2024.2326621},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1765-1777},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tree-guided rare feature selection and logic aggregation with electronic health records data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coexchangeable process modeling for uncertainty
quantification in joint climate reconstruction. <em>JASA</em>,
<em>119</em>(547), 1751–1764. (<a
href="https://doi.org/10.1080/01621459.2024.2325705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Any experiment with climate models relies on a potentially large set of spatio-temporal boundary conditions. These can represent both the initial state of the system and/or forcings driving the model output throughout the experiment. These boundary conditions are typically fixed using available reconstructions in climate modeling studies; however, in reality they are highly uncertain, that uncertainty is unquantified, and the effect on the output of the experiment can be considerable. We develop efficient quantification of these uncertainties that combines relevant data from multiple models and observations. Starting from the coexchangeability model, we develop a coexchangeable process model to capture multiple correlated spatio-temporal fields of variables. We demonstrate that further exchangeability judgments over the parameters within this representation lead to a Bayes linear analogy of a hierarchical model. We use the framework to provide a joint reconstruction of sea-surface temperature and sea-ice concentration boundary conditions at the last glacial maximum (23–19 kya) and use it to force an ensemble of ice-sheet simulations using the FAMOUS-Ice coupled atmosphere and ice-sheet model. We demonstrate that existing boundary conditions typically used in these experiments are implausible given our uncertainties and demonstrate the impact of using more plausible boundary conditions on ice-sheet simulation. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Lachlan Astfalck and Daniel Williamson and Niall Gandy and Lauren Gregoire and Ruza Ivanovic},
  doi          = {10.1080/01621459.2024.2325705},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1751-1764},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Coexchangeable process modeling for uncertainty quantification in joint climate reconstruction},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Evaluating dynamic conditional quantile treatment effects
with applications in ridesharing. <em>JASA</em>, <em>119</em>(547),
1736–1750. (<a
href="https://doi.org/10.1080/01621459.2024.2314316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many modern tech companies, such as Google, Uber, and Didi, use online experiments (also known as A/B testing) to evaluate new policies against existing ones. While most studies concentrate on average treatment effects, situations with skewed and heavy-tailed outcome distributions may benefit from alternative criteria, such as quantiles. However, assessing dynamic quantile treatment effects (QTE) remains a challenge, particularly when dealing with data from ride-sourcing platforms that involve sequential decision-making across time and space. In this article, we establish a formal framework to calculate QTE conditional on characteristics independent of the treatment. Under specific model assumptions, we demonstrate that the dynamic conditional QTE (CQTE) equals the sum of individual CQTEs across time, even though the conditional quantile of cumulative rewards may not necessarily equate to the sum of conditional quantiles of individual rewards. This crucial insight significantly streamlines the estimation and inference processes for our target causal estimand. We then introduce two varying coefficient decision process (VCDP) models and devise an innovative method to test the dynamic CQTE. Moreover, we expand our approach to accommodate data from spatiotemporal dependent experiments and examine both conditional quantile direct and indirect effects. To showcase the practical utility of our method, we apply it to three real-world datasets from a ride-sourcing platform. Theoretical findings and comprehensive simulation studies further substantiate our proposal. Supplementary materials for this article are available online Code implementing the proposed method is also available at: https://github.com/BIG-S2/CQSTVCM .},
  archive      = {J_JASA},
  author       = {Ting Li and Chengchun Shi and Zhaohua Lu and Yi Li and Hongtu Zhu},
  doi          = {10.1080/01621459.2024.2314316},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1736-1750},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Evaluating dynamic conditional quantile treatment effects with applications in ridesharing},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing COVID-19 prevalence in austria with infection
surveys and case count data as auxiliary information. <em>JASA</em>,
<em>119</em>(547), 1722–1735. (<a
href="https://doi.org/10.1080/01621459.2024.2313790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Countries officially record the number of COVID-19 cases based on medical tests of a subset of the population. These case count data obviously suffer from participation bias, and for prevalence estimation, these data are typically discarded in favor of infection surveys, or possibly also completed with auxiliary information. One exception is the series of infection surveys recorded by the Statistics Austria Federal Institute to study the prevalence of COVID-19 in Austria in April, May, and November 2020. In these infection surveys, participants were additionally asked if they were simultaneously recorded as COVID-19 positive in the case count data. In this article, we analyze the benefits of properly combining the outcomes from the infection survey with the case count data, to analyze the prevalence of COVID-19 in Austria in 2020, from which the case ascertainment rate can be deduced. The results show that our approach leads to a significant efficiency gain. Indeed, considerably smaller infection survey samples suffice to obtain the same level of estimation accuracy. Our estimation method can also handle measurement errors due to the sensitivity and specificity of medical testing devices and to the nonrandom sample weighting scheme of the infection survey. The proposed estimators and associated confidence intervals are implemented in the companion open source R package pempi available on the Comprehensive R Archive Network ( CRAN ). Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Stéphane Guerrier and Christoph Kuzmics and Maria-Pia Victoria-Feser},
  doi          = {10.1080/01621459.2024.2313790},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1722-1735},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Assessing COVID-19 prevalence in austria with infection surveys and case count data as auxiliary information},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian integrative region segmentation in spatially
resolved transcriptomic studies. <em>JASA</em>, <em>119</em>(547),
1709–1721. (<a
href="https://doi.org/10.1080/01621459.2024.2308323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatially resolved transcriptomic study is a recently developed biological experiment that can measure gene expressions and retain spatial information simultaneously, opening a new avenue to characterize fine-grained tissue structures. In this article, we propose a nonparametric Bayesian method named BINRES to carry out the region segmentation for a tissue section by integrating all the three types of data generated during the study—gene expressions, spatial coordinates, and the histology image. BINRES is able to capture more subtle regions than existing statistical partitioning models that only partially make use of the three data modes and is more interpretable than neural-network-based region segmentation approaches. Specifically, due to a nonparametric spatial prior, BINRES does not require a prespecified region number and can learn it automatically. BINRES also combines the image and the gene expressions in the Bayesian consensus clustering framework and thus flexibly adjusts their label alignment contribution weights in a data-adaptive manner. A computationally scalable extension is developed for large-scale studies. Both simulation studies and the real application to three mouse spatial transcriptomic datasets demonstrate that BINRES outperforms the competing methods and easily achieves the uncertainty quantification of the integrative partition. The R package of the proposed method is publicly available at https://github.com/yinqiaoyan/BINRES . Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yinqiao Yan and Xiangyu Luo},
  doi          = {10.1080/01621459.2024.2308323},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1709-1721},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian integrative region segmentation in spatially resolved transcriptomic studies},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted functional data analysis for the calibration of a
ground motion model in italy. <em>JASA</em>, <em>119</em>(547),
1697–1708. (<a
href="https://doi.org/10.1080/01621459.2023.2300506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the crucial implications of Ground Motion Models in terms of seismic hazard analysis and civil protection planning, this work extends a scalar Ground Motion Model for Italy to the framework of Functional Data Analysis. The inherent characteristic of seismic data to be incomplete over the observation domain of oscillation periods entails embedding the analysis in the context of partially observed functional data and performing data reconstruction. This work proposes a novel methodology that accounts for the fact that parts of the curves are directly observed and other parts are reconstructed, thus, characterized by greater uncertainty. The method defines observation-specific functional weights, which enter the estimation process to reduce the impact that the less reliable portions of the curves have on the final estimates. The classical methods of smoothing and concurrent functional regression are extended to include weights. The advantages of the proposed methodology are assessed on synthetic data. Eventually, the weighted functional analysis performed on seismological data is shown to provide a natural smoothing and stabilization of the spectral estimates of the Ground Motion Model considered. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Teresa Bortolotti and Riccardo Peli and Giovanni Lanzano and Sara Sgobba and Alessandra Menafoglio},
  doi          = {10.1080/01621459.2023.2300506},
  journal      = {Journal of the American Statistical Association},
  month        = {7},
  number       = {547},
  pages        = {1697-1708},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Weighted functional data analysis for the calibration of a ground motion model in italy},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction. <em>JASA</em>, <em>119</em>(546), 1694–1695. (<a
href="https://doi.org/10.1080/01621459.2024.2344624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note provides correction to some numerical results in Krivitsky P. N., Coletti, P., and Hens, N. (2023), “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks,” Journal of the American Statistical Association , 118, 2213–2224.},
  archive      = {J_JASA},
  author       = {Pavel N. Krivitsky and Pietro Coletti and Niel Hens},
  doi          = {10.1080/01621459.2024.2344624},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1694-1695},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data science and predictive analytics, 2nd
ed. <em>JASA</em>, <em>119</em>(546), 1692–1693. (<a
href="https://doi.org/10.1080/01621459.2024.2303323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Xing Qiu},
  doi          = {10.1080/01621459.2024.2303323},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1692-1693},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Data science and predictive analytics, 2nd ed.},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical modeling with r: A dual frequentist and bayesian
approach for life scientists. <em>JASA</em>, <em>119</em>(546),
1691–1692. (<a
href="https://doi.org/10.1080/01621459.2024.2303316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Christian P. Robert},
  doi          = {10.1080/01621459.2024.2303316},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1691-1692},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical modeling with r: A dual frequentist and bayesian approach for life scientists},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantitative methods for precision medicine:
Pharmacogenomics in action. <em>JASA</em>, <em>119</em>(546), 1690–1691.
(<a href="https://doi.org/10.1080/01621459.2024.2303313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Arthur Berg},
  doi          = {10.1080/01621459.2024.2303313},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1690-1691},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Quantitative methods for precision medicine: Pharmacogenomics in action},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Theory of statistical inference. <em>JASA</em>,
<em>119</em>(546), 1689–1690. (<a
href="https://doi.org/10.1080/01621459.2024.2314719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Somabha Mukherjee},
  doi          = {10.1080/01621459.2024.2314719},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1689-1690},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Theory of statistical inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mathematical foundations of infinite-dimensional statistical
models. <em>JASA</em>, <em>119</em>(546), 1687–1689. (<a
href="https://doi.org/10.1080/01621459.2024.2308799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Bodhisattva Sen},
  doi          = {10.1080/01621459.2024.2308799},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1687-1689},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mathematical foundations of infinite-dimensional statistical models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Markov bases: A 25 year update. <em>JASA</em>,
<em>119</em>(546), 1671–1686. (<a
href="https://doi.org/10.1080/01621459.2024.2310181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we evaluate the challenges and best practices associated with the Markov bases approach to sampling from conditional distributions. We provide insights and clarifications after 25 years of the publication of the Fundamental theorem for Markov bases by Diaconis and Sturmfels. In addition to a literature review, we prove three new results on the complexity of Markov bases in hierarchical models, relaxations of the fibers in log-linear models, and limitations of partial sets of moves in providing an irreducible Markov chain. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Félix Almendra-Hernández and Jesús A. De Loera and Sonja Petrović},
  doi          = {10.1080/01621459.2024.2310181},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1671-1686},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Markov bases: A 25 year update},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Independence weights for causal inference with continuous
treatments. <em>JASA</em>, <em>119</em>(546), 1657–1670. (<a
href="https://doi.org/10.1080/01621459.2023.2213485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying causal effects of continuous treatments is important for gaining a deeper understanding of many interventions, policies, or medications, yet researchers are often left with observational studies for doing so. In the observational setting, confounding is a barrier to the estimation of causal effects. Weighting approaches seek to control for confounding by reweighting samples so that confounders are comparable across different treatment values. Yet, for continuous treatments, weighting methods are highly sensitive to model misspecification. In this article we elucidate the key property that makes weights effective in estimating causal quantities involving continuous treatments. We show that to eliminate confounding, weights should make treatment and confounders independent on the weighted scale. We develop a measure that characterizes the degree to which a set of weights induces such independence. Further, we propose a new model-free method for weight estimation by optimizing our measure. We study the theoretical properties of our measure and our weights, and prove that our weights can explicitly mitigate treatment-confounder dependence. The empirical effectiveness of our approach is demonstrated in a suite of challenging numerical experiments, where we find that our weights are quite robust and work well under a broad range of settings. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jared D. Huling and Noah Greifer and Guanhua Chen},
  doi          = {10.1080/01621459.2023.2213485},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1657-1670},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Independence weights for causal inference with continuous treatments},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survival mixed membership blockmodel. <em>JASA</em>,
<em>119</em>(546), 1647–1656. (<a
href="https://doi.org/10.1080/01621459.2023.2213466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whenever we send a message via a channel such as E-mail, Facebook, WhatsApp, WeChat, or LinkedIn, we care about the response rate—the probability that our message will receive a response—and the response time—how long it will take to receive a reply. Recent studies have made considerable efforts to model the sending behaviors of messages in social networks with point processes. However, statistical research on modeling response rates and response times on social networks is still lacking. Compared with sending behaviors, which are often determined by the sender’s characteristics, response rates and response times further depend on the relationship between the sender and the receiver. Here, we develop a survival mixed membership blockmodel (SMMB) that integrates semiparametric cure rate models with a mixed membership stochastic blockmodel to analyze time-to-event data observed for node pairs in a social network, and we are able to prove its model identifiability without the pure node assumption. We develop a Markov chain Monte Carlo algorithm to conduct posterior inference and select the number of social clusters in the network according to the conditional deviance information criterion. The application of the SMMB to the Enron E-mail corpus offers novel insights into the company’s organization and power relations. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fangda Song and Jing Chu and Shuangge Ma and Yingying Wei},
  doi          = {10.1080/01621459.2023.2213466},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1647-1656},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Survival mixed membership blockmodel},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Narrowest significance pursuit: Inference for multiple
change-points in linear models. <em>JASA</em>, <em>119</em>(546),
1633–1646. (<a
href="https://doi.org/10.1080/01621459.2023.2211733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Narrowest Significance Pursuit (NSP), a general and flexible methodology for automatically detecting localized regions in data sequences which each must contain a change-point (understood as an abrupt change in the parameters of an underlying linear model), at a prescribed global significance level. NSP works with a wide range of distributional assumptions on the errors, and guarantees important stochastic bounds which directly yield exact desired coverage probabilities, regardless of the form or number of the regressors. In contrast to the widely studied “post-selection inference” approach, NSP paves the way for the concept of “post-inference selection.” An implementation is available in the R package nsp. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Piotr Fryzlewicz},
  doi          = {10.1080/01621459.2023.2211733},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1633-1646},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Narrowest significance pursuit: Inference for multiple change-points in linear models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical expected improvement method for bayesian
optimization. <em>JASA</em>, <em>119</em>(546), 1619–1632. (<a
href="https://doi.org/10.1080/01621459.2023.2210803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Expected Improvement (EI) method, proposed by Jones, Schonlau, andWelch, is a widely used Bayesian optimization method, which makes use of a fitted Gaussian process model for efficient black-box optimization. However, one key drawback of EI is that it is overly greedy in exploiting the fitted Gaussian process model for optimization, which results in suboptimal solutions even with large sample sizes. To address this, we propose a new hierarchical EI (HEI) framework, which makes use of a hierarchical Gaussian process model. HEI preserves a closed-form acquisition function, and corrects the over-greediness of EI by encouraging exploration of the optimization space. We then introduce hyperparameter estimation methods which allow HEI to mimic a fully Bayesian optimization procedure, while avoiding expensive Markov-chain Monte Carlo sampling steps. We prove the global convergence of HEI over a broad function space, and establish near-minimax convergence rates under certain prior specifications. Numerical experiments show the improvement of HEI over existing Bayesian optimization methods, for synthetic functions and a semiconductor manufacturing optimization problem. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhehui Chen and Simon Mak and C. F. Jeff Wu},
  doi          = {10.1080/01621459.2023.2210803},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1619-1632},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A hierarchical expected improvement method for bayesian optimization},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale two-sample comparison of support sets.
<em>JASA</em>, <em>119</em>(546), 1604–1618. (<a
href="https://doi.org/10.1080/01621459.2023.2210337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sample multiple testing has a wide range of applications. Most of the literature considers simultaneous tests of equality of parameters. The article takes a different perspective and investigates the null hypotheses that the two support sets are equal. This formulation of the testing problem is motivated by the fact that in many applications where the two parameter vectors being compared are both sparse, one might be more concerned about the detection of differential sparsity structures rather than the difference in parameter magnitudes. Focusing on this type of problem, we develop a general approach, which adapts the newly proposed symmetry data aggregation tool combined with a novel double thresholding (DT) filter. The DT filter first constructs a sequence of pairs of ranking statistics that fulfill global symmetry properties and then chooses two data-driven thresholds along the ranking to simultaneously control the False Discovery Rate (FDR) and maximize the number of rejections. Several applications of the methodology are given including high-dimensional linear models and Gaussian graphical models. We show that the proposed method is able to asymptotically control the FDR and have power guarantee under certain conditions. Numerical results confirm the effectiveness and robustness of DT in FDR control and detection ability. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Haoyu Geng and Xiaolong Cui and Haojie Ren and Changliang Zou},
  doi          = {10.1080/01621459.2023.2210337},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1604-1618},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large-scale two-sample comparison of support sets},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Censored interquantile regression model with time-dependent
covariates. <em>JASA</em>, <em>119</em>(546), 1592–1603. (<a
href="https://doi.org/10.1080/01621459.2023.2208389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventionally, censored quantile regression stipulates a specific, pointwise conditional quantile of the survival time given covariates. Despite its model flexibility and straightforward interpretation, the pointwise formulation oftentimes yields rather unstable estimates across neighboring quantile levels with large variances. In view of this phenomenon, we propose a new class of quantile-based regression models with time-dependent covariates for censored data. The models proposed aim to capture the relationship between the failure time and the covariate processes of a target population that falls within a specific quantile bracket. The pooling of information within a homogeneous neighborhood facilitates more efficient estimates hence, more consistent conclusion on statistical significances of the variables concerned. This new formulation can also be regarded as a generalization of the accelerated failure time model for survival data in the sense that it relaxes the assumption of global homogeneity for the error at all quantile levels. By introducing a class of weighted rank-based estimation procedure, our framework allows a quantile-based inference on the covariate effect with a less restrictive set of assumptions. Numerical studies demonstrate that the proposed estimator outperforms existing alternatives under various settings in terms of smaller empirical biases and standard deviations. A perturbation-based resampling method is also developed to reconcile the asymptotic distribution of the parameter estimates. Finally, consistency and weak convergence of the proposed estimator are established via empirical process theory. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chi Wing Chu and Tony Sit},
  doi          = {10.1080/01621459.2023.2208389},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1592-1603},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Censored interquantile regression model with time-dependent covariates},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of linear functionals in high-dimensional linear
models: From sparsity to nonsparsity. <em>JASA</em>, <em>119</em>(546),
1579–1591. (<a
href="https://doi.org/10.1080/01621459.2023.2206084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional linear models are commonly used in practice. In many applications, one is interested in linear transformations β ⊤ x β ⊤ x β⊤x of regression coefficients β ∈ R p , where x is a specific point and is not required to be identically distributed as the training data. One common approach is the plug-in technique which first estimates β , then plugs the estimator in the linear transformation for prediction. Despite its popularity, estimation of β can be difficult for high-dimensional problems. Commonly used assumptions in the literature include that the signal of coefficients β is sparse and predictors are weakly correlated. These assumptions, however, may not be easily verified, and can be violated in practice. When β is non-sparse or predictors are strongly correlated, estimation of β can be very difficult. In this article, we propose a novel pointwise estimator for linear transformations of β . This new estimator greatly relaxes the common assumptions for high-dimensional problems, and is adaptive to the degree of sparsity of β and strength of correlations among the predictors. In particular, β can be sparse or nonsparse and predictors can be strongly or weakly correlated. The proposed method is simple for implementation. Numerical and theoretical results demonstrate the competitive advantages of the proposed method for a wide range of problems. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Junlong Zhao and Yang Zhou and Yufeng Liu},
  doi          = {10.1080/01621459.2023.2206084},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1579-1591},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of linear functionals in high-dimensional linear models: From sparsity to nonsparsity},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ridge regression under dense factor augmented models.
<em>JASA</em>, <em>119</em>(546), 1566–1578. (<a
href="https://doi.org/10.1080/01621459.2023.2206082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article establishes a comprehensive theory of the optimality, robustness, and cross-validation selection consistency for the ridge regression under factor-augmented models with possibly dense idiosyncratic information. Using spectral analysis for random matrices, we show that the ridge regression is asymptotically efficient in capturing both factor and idiosyncratic information by minimizing the limiting predictive loss among the entire class of spectral regularized estimators under large-dimensional factor models and mixed-effects hypothesis. We derive an asymptotically optimal ridge penalty in closed form and prove that a bias-corrected k -fold cross-validation procedure can adaptively select the best ridge penalty in large samples. We extend the theory to the autoregressive models with many exogenous variables and establish a consistent cross-validation procedure using the what-we-called double ridge regression method. Our results allow for nonparametric distributions for, possibly heavy-tailed, martingale difference errors and idiosyncratic random coefficients and adapt to the cross-sectional and temporal dependence structures of the large-dimensional predictors. We demonstrate the performance of our ridge estimators in simulated examples as well as an economic dataset. All the proofs are available in the supplementary materials , which also includes more technical discussions and remarks, extra simulation results, and useful lemmas that may be of independent interest.},
  archive      = {J_JASA},
  author       = {Yi He},
  doi          = {10.1080/01621459.2023.2206082},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1566-1578},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ridge regression under dense factor augmented models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anytime-valid tests of conditional independence under
model-x. <em>JASA</em>, <em>119</em>(546), 1554–1565. (<a
href="https://doi.org/10.1080/01621459.2023.2205607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sequential, anytime-valid method to test the conditional independence of a response Y and a predictor X given a random vector Z . The proposed test is based on e -statistics and test martingales, which generalize likelihood ratios and allow valid inference at arbitrary stopping times. In accordance with the recently introduced model-X setting, our test depends on the availability of the conditional distribution of X given Z , or at least a sufficiently sharp approximation thereof. Within this setting, we derive a general method for constructing e -statistics for testing conditional independence, show that it leads to growth-rate optimal e -statistics for simple alternatives, and prove that our method yields tests with asymptotic power one in the special case of a logistic regression model. A simulation study is done to demonstrate that the approach is competitive in terms of power when compared to established sequential and nonsequential testing methods, and robust with respect to violations of the model-X assumption. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Peter Grünwald and Alexander Henzi and Tyron Lardy},
  doi          = {10.1080/01621459.2023.2205607},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1554-1565},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Anytime-valid tests of conditional independence under model-X},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference for treatment-specific survival curves using
machine learning. <em>JASA</em>, <em>119</em>(546), 1541–1553. (<a
href="https://doi.org/10.1080/01621459.2023.2205060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of data from a randomized trial, researchers may aim to use observational data to draw causal inference about the effect of a treatment on a time-to-event outcome. In this context, interest often focuses on the treatment-specific survival curves, that is, the survival curves were the population under study to be assigned to receive the treatment or not. Under certain conditions, including that all confounders of the treatment-outcome relationship are observed, the treatment-specific survival curve can be identified with a covariate-adjusted survival curve. In this article, we propose a novel cross-fitted doubly-robust estimator that incorporates data-adaptive (e.g., machine learning) estimators of the conditional survival functions. We establish conditions on the nuisance estimators under which our estimator is consistent and asymptotically linear, both pointwise and uniformly in time. We also propose a novel ensemble learner for combining multiple candidate estimators of the conditional survival estimators. Notably, our methods and results accommodate events occurring in discrete or continuous time, or an arbitrary mix of the two. We investigate the practical performance of our methods using numerical studies and an application to the effect of a surgical treatment to prevent metastases of parotid carcinoma on mortality. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ted Westling and Alex Luedtke and Peter B. Gilbert and Marco Carone},
  doi          = {10.1080/01621459.2023.2205060},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1541-1553},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference for treatment-specific survival curves using machine learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified inference for predictive quantile regression.
<em>JASA</em>, <em>119</em>(546), 1526–1540. (<a
href="https://doi.org/10.1080/01621459.2023.2203354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The asymptotic behavior of quantile regression inference becomes dramatically different when it involves a persistent predictor with zero or nonzero intercept. Distinguishing various properties of a predictor is empirically challenging. In this article, we develop a unified predictability test for quantile regression regardless of the presence of intercept and persistence of a predictor. The developed test is a novel combination of data splitting, weighted inference, and a random weighted bootstrap method. Monte Carlo simulations show that the new approach displays significantly better size and power performance than other competing methods in various scenarios, particularly when the predictive regressor contains a nonzero intercept. In an empirical application, we revisit the quantile predictability of the monthly S&amp;P 500 returns between 1980 and 2019. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiaohui Liu and Wei Long and Liang Peng and Bingduo Yang},
  doi          = {10.1080/01621459.2023.2203354},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1526-1540},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A unified inference for predictive quantile regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing simultaneous diagonalizability. <em>JASA</em>,
<em>119</em>(546), 1513–1525. (<a
href="https://doi.org/10.1080/01621459.2023.2202435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes novel methods to test for simultaneous diagonalization of possibly asymmetric matrices. Motivated by various applications, a two-sample test as well as a generalization for multiple matrices are proposed. A partial version of the test is also studied to check whether a partial set of eigenvectors is shared across samples. Additionally, a novel algorithm for the considered testing methods is introduced. Simulation studies demonstrate favorable performance for all designs. Finally, the theoretical results are used to decouple multiple vector autoregression models into univariate time series, and to test for the same stationary distribution in recurrent Markov chains. These applications are demonstrated using macroeconomic indices of eight countries and streamflow data, respectively. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yuchen Xu and Marie-Christine Düker and David S. Matteson},
  doi          = {10.1080/01621459.2023.2202435},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1513-1525},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing simultaneous diagonalizability},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse convoluted rank regression in high dimensions.
<em>JASA</em>, <em>119</em>(546), 1500–1512. (<a
href="https://doi.org/10.1080/01621459.2023.2202433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wang et al. studied the high-dimensional sparse penalized rank regression and established its nice theoretical properties. Compared with the least squares, rank regression can have a substantial gain in estimation efficiency while maintaining a minimal relative efficiency of 86.4 % . However, the computation of penalized rank regression can be very challenging for high-dimensional data, due to the highly nonsmooth rank regression loss. In this work we view the rank regression loss as a nonsmooth empirical counterpart of a population level quantity, and a smooth empirical counterpart is derived by substituting a kernel density estimator for the true distribution in the expectation calculation. This view leads to the convoluted rank regression loss and consequently the sparse penalized convoluted rank regression (CRR) for high-dimensional data. We prove some interesting asymptotic properties of CRR. Under the same key assumptions for sparse rank regression, we establish the rate of convergence of the l 1 -penalized CRR for a tuning free penalization parameter and prove the strong oracle property of the folded concave penalized CRR. We further propose a high-dimensional Bayesian information criterion for selecting the penalization parameter in folded concave penalized CRR and prove its selection consistency. We derive an efficient algorithm for solving sparse convoluted rank regression that scales well with high dimensions. Numerical examples demonstrate the promising performance of the sparse convoluted rank regression over the sparse rank regression. Our theoretical and numerical results suggest that sparse convoluted rank regression enjoys the best of both sparse least squares regression and sparse rank regression. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Le Zhou and Boxiang Wang and Hui Zou},
  doi          = {10.1080/01621459.2023.2202433},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1500-1512},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sparse convoluted rank regression in high dimensions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inferences for complex dependence of multimodal
imaging data. <em>JASA</em>, <em>119</em>(546), 1486–1499. (<a
href="https://doi.org/10.1080/01621459.2023.2200610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical analysis of multimodal imaging data is a challenging task, since the data involves high-dimensionality, strong spatial correlations and complex data structures. In this article, we propose rigorous statistical testing procedures for making inferences on the complex dependence of multimodal imaging data. Motivated by the analysis of multi-task fMRI data in the Human Connectome Project (HCP) study, we particularly address three hypothesis testing problems: (a) testing independence among imaging modalities over brain regions, (b) testing independence between brain regions within imaging modalities, and (c) testing independence between brain regions across different modalities. Considering a general form for all the three tests, we develop a global testing procedure and a multiple testing procedure controlling the false discovery rate. We study theoretical properties of the proposed tests and develop a computationally efficient distributed algorithm. The proposed methods and theory are general and relevant for many statistical problems of testing independence structure among the components of high-dimensional random vectors with arbitrary dependence structures. We also illustrate our proposed methods via extensive simulations and analysis of five task fMRI contrast maps in the HCP study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jinyuan Chang and Jing He and Jian Kang and Mingcong Wu},
  doi          = {10.1080/01621459.2023.2200610},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1486-1499},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inferences for complex dependence of multimodal imaging data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive functional thresholding for sparse covariance
function estimation in high dimensions. <em>JASA</em>,
<em>119</em>(546), 1473–1485. (<a
href="https://doi.org/10.1080/01621459.2023.2200522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance function estimation is a fundamental task in multivariate functional data analysis and arises in many applications. In this article, we consider estimating sparse covariance functions for high-dimensional functional data, where the number of random functions p is comparable to, or even larger than the sample size n . Aided by the Hilbert–Schmidt norm of functions, we introduce a new class of functional thresholding operators that combine functional versions of thresholding and shrinkage, and propose the adaptive functional thresholding estimator by incorporating the variance effects of individual entries of the sample covariance function into functional thresholding. To handle the practical scenario where curves are partially observed with errors, we also develop a nonparametric smoothing approach to obtain the smoothed adaptive functional thresholding estimator and its binned implementation to accelerate the computation. We investigate the theoretical properties of our proposals when p grows exponentially with n under both fully and partially observed functional scenarios. Finally, we demonstrate that the proposed adaptive functional thresholding estimators significantly outperform the competitors through extensive simulations and the functional connectivity analysis of two neuroimaging datasets. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qin Fang and Shaojun Guo and Xinghao Qiao},
  doi          = {10.1080/01621459.2023.2200522},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1473-1485},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive functional thresholding for sparse covariance function estimation in high dimensions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference in high-dimensional online changepoint detection.
<em>JASA</em>, <em>119</em>(546), 1461–1472. (<a
href="https://doi.org/10.1080/01621459.2023.2199962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and study two new inferential challenges associated with the sequential detection of change in a high-dimensional mean vector. First, we seek a confidence interval for the changepoint, and second, we estimate the set of indices of coordinates in which the mean changes. We propose an online algorithm that produces an interval with guaranteed nominal coverage, and whose length is, with high probability, of the same order as the average detection delay, up to a logarithmic factor. The corresponding support estimate enjoys control of both false negatives and false positives. Simulations confirm the effectiveness of our methodology, and we also illustrate its applicability on the U.S. excess deaths data from 2017 to 2020. The supplementary material , which contains the proofs of our theoretical results, is available online.},
  archive      = {J_JASA},
  author       = {Yudong Chen and Tengyao Wang and Richard J. Samworth},
  doi          = {10.1080/01621459.2023.2199962},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1461-1472},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference in high-dimensional online changepoint detection},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multimodal sampling via tempered distribution
flow. <em>JASA</em>, <em>119</em>(546), 1446–1460. (<a
href="https://doi.org/10.1080/01621459.2023.2198059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling from high-dimensional distributions is a fundamental problem in statistical research and practice. However, great challenges emerge when the target density function is unnormalized and contains isolated modes. We tackle this difficulty by fitting an invertible transformation mapping, called a transport map, between a reference probability measure and the target distribution, so that sampling from the target distribution can be achieved by pushing forward a reference sample through the transport map. We theoretically analyze the limitations of existing transport-based sampling methods using the Wasserstein gradient flow theory, and propose a new method called TemperFlow that addresses the multimodality issue. TemperFlow adaptively learns a sequence of tempered distributions to progressively approach the target distribution, and we prove that it overcomes the limitations of existing methods. Various experiments demonstrate the superior performance of this novel sampler compared to traditional methods, and we show its applications in modern deep learning tasks such as image generation. The programming code for the numerical experiments is available in the supplementary material .},
  archive      = {J_JASA},
  author       = {Yixuan Qiu and Xiao Wang},
  doi          = {10.1080/01621459.2023.2198059},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1446-1460},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficient multimodal sampling via tempered distribution flow},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-validation: What does it estimate and how well does it
do it? <em>JASA</em>, <em>119</em>(546), 1434–1445. (<a
href="https://doi.org/10.1080/01621459.2023.2197686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-validation is a widely used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow’s C p . Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not refit the model on the combined data, since this invalidates the confidence intervals. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Stephen Bates and Trevor Hastie and Robert Tibshirani},
  doi          = {10.1080/01621459.2023.2197686},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1434-1445},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cross-validation: What does it estimate and how well does it do it?},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tail spectral density estimation and its uncertainty
quantification: Another look at tail dependent time series analysis.
<em>JASA</em>, <em>119</em>(546), 1424–1433. (<a
href="https://doi.org/10.1080/01621459.2023.2197159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation and uncertainty quantification of the tail spectral density, which provide a foundation for tail spectral analysis of tail dependent time series. The tail spectral density has a particular focus on serial dependence in the tail, and can reveal dependence information that is otherwise not discoverable by the traditional spectral analysis. Understanding the convergence rate of tail spectral density estimators and finding rigorous ways to quantify their statistical uncertainty, however, still stand as a somewhat open problem. The current article aims to fill this gap by providing a novel asymptotic theory on quadratic forms of tail statistics in the double asymptotic setting, based on which we develop the consistency and the long desired central limit theorem for tail spectral density estimators. The results are then used to devise a clean and effective method for constructing confidence intervals to gauge the statistical uncertainty of tail spectral density estimators, and it can be turned into a visualization tool to aid practitioners in examining the tail dependence for their data of interest. Numerical examples including data applications are presented to illustrate the developed results. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ting Zhang and Beibei Xu},
  doi          = {10.1080/01621459.2023.2197159},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1424-1433},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tail spectral density estimation and its uncertainty quantification: Another look at tail dependent time series analysis},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable bayesian transport maps for high-dimensional
non-gaussian spatial fields. <em>JASA</em>, <em>119</em>(546),
1409–1423. (<a
href="https://doi.org/10.1080/01621459.2023.2197158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multivariate distribution can be described by a triangular transport map from the target distribution to a simple reference distribution. We propose Bayesian nonparametric inference on the transport map by modeling its components using Gaussian processes. This enables regularization and uncertainty quantification of the map estimation, while resulting in a closed-form and invertible posterior map. We then focus on inferring the distribution of a nonstationary spatial field from a small number of replicates. We develop specific transport-map priors that are highly flexible and are motivated by the behavior of a large class of stochastic processes. Our approach is scalable to high-dimensional distributions due to data-dependent sparsity and parallel computations. We also discuss extensions, including Dirichlet process mixtures for flexible marginals. We present numerical results to demonstrate the accuracy, scalability, and usefulness of our methods, including statistical emulation of non-Gaussian climate-model output. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matthias Katzfuss and Florian Schäfer},
  doi          = {10.1080/01621459.2023.2197158},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1409-1423},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Scalable bayesian transport maps for high-dimensional non-gaussian spatial fields},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test of significance for high-dimensional thresholds with
application to individualized minimal clinically important difference.
<em>JASA</em>, <em>119</em>(546), 1396–1408. (<a
href="https://doi.org/10.1080/01621459.2023.2195977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is motivated by learning the individualized minimal clinically important difference, a vital concept to assess clinical importance in various biomedical studies. We formulate the scientific question into a high-dimensional statistical problem where the parameter of interest lies in an individualized linear threshold. The goal is to develop a hypothesis testing procedure for the significance of a single element in this parameter as well as of a linear combination of this parameter. The difficulty dues to the high-dimensional nuisance in developing such a testing procedure, and also stems from the fact that this high-dimensional threshold model is nonregular and the limiting distribution of the corresponding estimator is nonstandard. To deal with these challenges, we construct a test statistic via a new bias-corrected smoothed decorrelated score approach, and establish its asymptotic distributions under both null and local alternative hypotheses. We propose a double-smoothing approach to select the optimal bandwidth in our test statistic and provide theoretical guarantees for the selected bandwidth. We conduct simulation studies to demonstrate how our proposed procedure can be applied in empirical studies. We apply the proposed method to a clinical trial where the scientific goal is to assess the clinical importance of a surgery procedure. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Huijie Feng and Jingyi Duan and Yang Ning and Jiwei Zhao},
  doi          = {10.1080/01621459.2023.2195977},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1396-1408},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Test of significance for high-dimensional thresholds with application to individualized minimal clinically important difference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature screening with conditional rank utility for big-data
classification. <em>JASA</em>, <em>119</em>(546), 1385–1395. (<a
href="https://doi.org/10.1080/01621459.2023.2195976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature screening is a commonly used strategy to eliminate irrelevant features in high-dimensional classification. When one encounters big datasets with both high dimensionality and huge sample size, the conventional screening methods become computationally costly or even infeasible. In this article, we introduce a novel screening utility, Conditional Rank Utility (CRU), and propose a distributed feature screening procedure for the big-data classification. The proposed CRU effectively quantifies the significance of a numerical feature on the categorical response. Since CRU is constructed based on the ratio of the mean conditional rank to the mean unconditional rank of a feature, it is robust against model misspecification and the presence of outliers. Structurally, CRU can be expressed as a simple function of a few component parameters, each of which can be distributively estimated using a natural unbiased estimator from the data segments. Under mild conditions, we show that the distributed estimator of CRU is fully efficient in terms of the probability convergence bound and the mean squared error rate; the corresponding distributed screening procedure enjoys the sure screening and ranking properties. The promising performances of the CRU-based screening are supported by extensive numerical examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xingxiang Li and Chen Xu},
  doi          = {10.1080/01621459.2023.2195976},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1385-1395},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Feature screening with conditional rank utility for big-data classification},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cohesion and repulsion in bayesian distance clustering.
<em>JASA</em>, <em>119</em>(546), 1374–1384. (<a
href="https://doi.org/10.1080/01621459.2023.2191821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering in high-dimensions poses many statistical challenges. While traditional distance-based clustering methods are computationally feasible, they lack probabilistic interpretation and rely on heuristics for estimation of the number of clusters. On the other hand, probabilistic model-based clustering techniques often fail to scale and devising algorithms that are able to effectively explore the posterior space is an open problem. Based on recent developments in Bayesian distance-based clustering, we propose a hybrid solution that entails defining a likelihood on pairwise distances between observations. The novelty of the approach consists in including both cohesion and repulsion terms in the likelihood, which allows for cluster identifiability. This implies that clusters are composed of objects which have small dissimilarities among themselves (cohesion) and similar dissimilarities to observations in other clusters (repulsion). We show how this modeling strategy has interesting connection with existing proposals in the literature. The proposed method is computationally efficient and applicable to a wide variety of scenarios. We demonstrate the approach in simulation and an application in digital numismatics. Supplementary Material with code is available online.},
  archive      = {J_JASA},
  author       = {Abhinav Natarajan and Maria De Iorio and Andreas Heinecke and Emanuel Mayer and Simon Glenn},
  doi          = {10.1080/01621459.2023.2191821},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1374-1384},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cohesion and repulsion in bayesian distance clustering},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian conditional transformation models. <em>JASA</em>,
<em>119</em>(546), 1360–1373. (<a
href="https://doi.org/10.1080/01621459.2023.2191820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in statistical regression methodology shift away from pure mean regression toward distributional regression models. One important strand thereof is that of conditional transformation models (CTMs). CTMs infer the entire conditional distribution directly by applying a transformation function to the response conditionally on a set of covariates toward a simple log-concave reference distribution. Thereby, CTMs allow not only variance, kurtosis or skewness but the complete conditional distribution to depend on the explanatory variables. We propose a Bayesian notion of conditional transformation models (BCTMs) focusing on exactly observed continuous responses, but also incorporating extensions to randomly censored and discrete responses. Rather than relying on Bernstein polynomials that have been considered in likelihood-based CTMs, we implement a spline-based parameterization for monotonic effects that are supplemented with smoothness priors. Furthermore, we are able to benefit from the Bayesian paradigm via easily obtainable credible intervals and other quantities without relying on large sample approximations. A simulation study demonstrates the competitiveness of our approach against its likelihood-based counterpart but also Bayesian additive models of location, scale and shape and Bayesian quantile regression. Two applications illustrate the versatility of BCTMs in problems involving real world data, again including the comparison with various types of competitors. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Manuel Carlan and Thomas Kneib and Nadja Klein},
  doi          = {10.1080/01621459.2023.2191820},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1360-1373},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian conditional transformation models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric proximal causal inference. <em>JASA</em>,
<em>119</em>(546), 1348–1359. (<a
href="https://doi.org/10.1080/01621459.2023.2191817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skepticism about the assumption of no unmeasured confounding, also known as exchangeability, is often warranted in making causal inferences from observational data; because exchangeability hinges on an investigator’s ability to accurately measure covariates that capture all potential sources of confounding. In practice, the most one can hope for is that covariate measurements are at best proxies of the true underlying confounding mechanism operating in a given observational study. In this article, we consider the framework of proximal causal inference introduced by Miao, Geng, and Tchetgen Tchetgen and Tchetgen Tchetgen et al. which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. We make a number of contributions to proximal inference including (i) an alternative set of conditions for nonparametric proximal identification of the average treatment effect; (ii) general semiparametric theory for proximal estimation of the average treatment effect including efficiency bounds for key semiparametric models of interest; (iii) a characterization of proximal doubly robust and locally efficient estimators of the average treatment effect. Moreover, we provide analogous identification and efficiency results for the average treatment effect on the treated. Our approach is illustrated via simulation studies and a data application on evaluating the effectiveness of right heart catheterization in the intensive care unit of critically ill patients. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yifan Cui and Hongming Pu and Xu Shi and Wang Miao and Eric Tchetgen Tchetgen},
  doi          = {10.1080/01621459.2023.2191817},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1348-1359},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Semiparametric proximal causal inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixed-domain posterior contraction rates for spatial
gaussian process model with nugget. <em>JASA</em>, <em>119</em>(546),
1336–1347. (<a
href="https://doi.org/10.1080/01621459.2023.2191380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial Gaussian process regression models typically contain finite dimensional covariance parameters that need to be estimated from the data. We study the Bayesian estimation of covariance parameters including the nugget parameter in a general class of stationary covariance functions under fixed-domain asymptotics, which is theoretically challenging due to the increasingly strong dependence among spatial observations. We propose a novel adaptation of the Schwartz’s consistency theorem for showing posterior contraction rates of the covariance parameters including the nugget. We derive a new polynomial evidence lower bound, and propose consistent higher-order quadratic variation estimators that satisfy concentration inequalities with exponentially small tails. Our Bayesian fixed-domain asymptotics theory leads to explicit posterior contraction rates for the microergodic and nugget parameters in the isotropic Matérn covariance function under a general stratified sampling design. We verify our theory and the Bayesian predictive performance in simulation studies and an application to sea surface temperature data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Cheng Li and Saifei Sun and Yichen Zhu},
  doi          = {10.1080/01621459.2023.2191380},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1336-1347},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fixed-domain posterior contraction rates for spatial gaussian process model with nugget},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection for high-dimensional nodal attributes in
social networks with degree heterogeneity. <em>JASA</em>,
<em>119</em>(546), 1322–1335. (<a
href="https://doi.org/10.1080/01621459.2023.2187815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of network models, in which the connection probability depends on ultrahigh-dimensional nodal covariates ( homophily ) and node-specific popularity ( degree heterogeneity ). A Bayesian method is proposed to select nodal features in both dense and sparse networks under a mild assumption on popularity parameters. The proposed approach is implemented via Gibbs sampling. To alleviate the computational burden for large sparse networks, we further develop a working model in which parameters are updated based on a dense sub-graph at each step. Model selection consistency is established for both models, in the sense that the probability of the true model being selected converges to one asymptotically, even when the dimension grows with the network size at an exponential rate. The performance of the proposed models and estimation procedures are illustrated through Monte Carlo studies and three real world examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jia Wang and Xizhen Cai and Xiaoyue Niu and Runze Li},
  doi          = {10.1080/01621459.2023.2187815},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1322-1335},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variable selection for high-dimensional nodal attributes in social networks with degree heterogeneity},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Doubly robust capture-recapture methods for estimating
population size. <em>JASA</em>, <em>119</em>(546), 1309–1321. (<a
href="https://doi.org/10.1080/01621459.2023.2187814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of population size using incomplete lists has a long history across many biological and social sciences. For example, human rights groups often construct partial lists of victims of armed conflicts, to estimate the total number of victims. Earlier statistical methods for this setup often use parametric assumptions, or rely on suboptimal plug-in-type nonparametric estimators; but both approaches can lead to substantial bias, the former via model misspecification and the latter via smoothing. Under an identifying assumption that two lists are conditionally independent given measured covariates, we make several contributions. First, we derive the nonparametric efficiency bound for estimating the capture probability, which indicates the best possible performance of any estimator, and sheds light on the statistical limits of capture-recapture methods. Then we present a new estimator, that has a double robustness property new to capture-recapture, and is near-optimal in a nonasymptotic sense, under relatively mild nonparametric conditions. Next, we give a confidence interval construction method for total population size from generic capture probability estimators, and prove nonasymptotic near-validity. Finally, we apply them to estimate the number of killings and disappearances in Peru during its internal armed conflict between 1980 and 2000. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Manjari Das and Edward H. Kennedy and Nicholas P. Jewell},
  doi          = {10.1080/01621459.2023.2187814},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1309-1321},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly robust capture-recapture methods for estimating population size},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed inference for spatial extremes modeling in high
dimensions. <em>JASA</em>, <em>119</em>(546), 1297–1308. (<a
href="https://doi.org/10.1080/01621459.2023.2186886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme environmental events frequently exhibit spatial and temporal dependence. These data are often modeled using Max Stable Processes (MSPs) that are computationally prohibitive to fit for as few as a dozen observations. Supposed computationally-efficient approaches like the composite likelihood remain computationally burdensome with a few hundred observations. In this article, we propose a spatial partitioning approach based on local modeling of subsets of the spatial domain that delivers computationally and statistically efficient inference. Marginal and dependence parameters of the MSP are estimated locally on subsets of observations using censored pairwise composite likelihood, and combined using a modified generalized method of moments procedure. The proposed distributed approach is extended to estimate inverted MSP models, and to estimate spatially varying coefficient models to deliver computationally efficient modeling of spatial variation in marginal parameters. We demonstrate consistency and asymptotic normality of estimators, and show empirically that our approach leads to statistically efficient estimation of model parameters. We illustrate the flexibility and practicability of our approach through simulations and the analysis of streamflow data from the U.S. Geological Survey. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Emily C. Hector and Brian J. Reich},
  doi          = {10.1080/01621459.2023.2186886},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1297-1308},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distributed inference for spatial extremes modeling in high dimensions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On learning and testing of counterfactual fairness through
data preprocessing. <em>JASA</em>, <em>119</em>(546), 1286–1296. (<a
href="https://doi.org/10.1080/01621459.2023.2186885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has become more important in real-life decision-making but people are concerned about the ethical problems it may bring when used improperly. Recent work brings the discussion of machine learning fairness into the causal framework and elaborates on the concept of Counterfactual Fairness. In this article, we develop the Fair Learning through dAta Preprocessing (FLAP) algorithm to learn counterfactually fair decisions from biased training data and formalize the conditions where different data preprocessing procedures should be used to guarantee counterfactual fairness. We also show that Counterfactual Fairness is equivalent to the conditional independence of the decisions and the sensitive attributes given the processed nonsensitive attributes, which enables us to detect discrimination in the original decision using the processed data. The performance of our algorithm is illustrated using simulated data and real-world applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Haoyu Chen and Wenbin Lu and Rui Song and Pulak Ghosh},
  doi          = {10.1080/01621459.2023.2186885},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1286-1296},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On learning and testing of counterfactual fairness through data preprocessing},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation and inference for high-dimensional generalized
linear models with knowledge transfer. <em>JASA</em>, <em>119</em>(546),
1274–1285. (<a
href="https://doi.org/10.1080/01621459.2023.2184373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning provides a powerful tool for incorporating data from related studies into a target study of interest. In epidemiology and medical studies, the classification of a target disease could borrow information across other related diseases and populations. In this work, we consider transfer learning for high-dimensional Generalized Linear Models (GLMs). A novel algorithm, TransHDGLM, that integrates data from the target study and the source studies is proposed. Minimax rate of convergence for estimation is established and the proposed estimator is shown to be rate-optimal. Statistical inference for the target regression coefficients is also studied. Asymptotic normality for a debiased estimator is established, which can be used for constructing coordinate-wise confidence intervals of the regression coefficients. Numerical studies show significant improvement in estimation and inference accuracy over GLMs that only use the target data. The proposed methods are applied to a real data study concerning the classification of colorectal cancer using gut microbiomes, and are shown to enhance the classification accuracy in comparison to methods that only use the target data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sai Li and Linjun Zhang and T. Tony Cai and Hongzhe Li},
  doi          = {10.1080/01621459.2023.2184373},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1274-1285},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation and inference for high-dimensional generalized linear models with knowledge transfer},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network inference using the hub model and variants.
<em>JASA</em>, <em>119</em>(546), 1264–1273. (<a
href="https://doi.org/10.1080/01621459.2023.2183133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical network analysis primarily focuses on inferring the parameters of an observed network. In many applications, especially in the social sciences, the observed data is the groups formed by individual subjects. In these applications, the network is itself a parameter of a statistical model. Zhao and Weko propose a model-based approach, called the hub model , to infer implicit networks from grouping behavior. The hub model assumes that each member of the group is brought together by a member of the group called the hub . The set of members which can serve as a hub is called the hub set . The hub model belongs to the family of Bernoulli mixture models. Identifiability of Bernoulli mixture model parameters is a notoriously difficult problem. This article proves identifiability of the hub model parameters and estimation consistency under mild conditions. Furthermore, this article generalizes the hub model by introducing a model component that allows hubless groups in which individual nodes spontaneously appear independent of any other individual. We refer to this additional component as the null component . The new model bridges the gap between the hub model and the degenerate case of the mixture model—the Bernoulli product. Identifiability and consistency are also proved for the new model. In addition, a penalized likelihood approach is proposed to estimate the hub set when it is unknown. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhibing He and Yunpeng Zhao and Peter Bickel and Charles Weko and Dan Cheng and Jirui Wang},
  doi          = {10.1080/01621459.2023.2183133},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1264-1273},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Network inference using the hub model and variants},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor modeling for clustering high-dimensional time series.
<em>JASA</em>, <em>119</em>(546), 1252–1263. (<a
href="https://doi.org/10.1080/01621459.2023.2183132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new unsupervised learning method for clustering a large number of time series based on a latent factor structure. Each cluster is characterized by its own cluster-specific factors in addition to some common factors which impact on all the time series concerned. Our setting also offers the flexibility that some time series may not belong to any clusters. The consistency with explicit convergence rates is established for the estimation of the common factors, the cluster-specific factors, and the latent clusters. Numerical illustration with both simulated data as well as a real data example is also reported. As a spin-off, the proposed new approach also advances significantly the statistical inference for the factor model of Lam and Yao. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Bo Zhang and Guangming Pan and Qiwei Yao and Wang Zhou},
  doi          = {10.1080/01621459.2023.2183132},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1252-1263},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Factor modeling for clustering high-dimensional time series},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instrumental variable estimation of marginal structural mean
models for time-varying treatment. <em>JASA</em>, <em>119</em>(546),
1240–1251. (<a
href="https://doi.org/10.1080/01621459.2023.2183131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robins introduced Marginal Structural Models (MSMs), a general class of counterfactual models for the joint effects of time-varying treatment regimes in complex longitudinal studies subject to time-varying confounding. In his work, identification of MSM parameters is established under a Sequential Randomization Assumption (SRA), which rules out unmeasured confounding of treatment assignment over time. We consider sufficient conditions for identification of the parameters of a subclass, Marginal Structural Mean Models (MSMMs), when sequential randomization fails to hold due to unmeasured confounding, using instead a time-varying instrumental variable. Our identification conditions require that no unobserved confounder predicts compliance type for the time-varying treatment. We describe a simple weighted estimator and examine its finite-sample properties in a simulation study. We apply the proposed estimator to examine the effect of delivery hospital type on neonatal survival probability. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Haben Michael and Yifan Cui and Scott A. Lorch and Eric J. Tchetgen Tchetgen},
  doi          = {10.1080/01621459.2023.2183131},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1240-1251},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Instrumental variable estimation of marginal structural mean models for time-varying treatment},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypotheses testing from complex survey data using bootstrap
weights: A unified approach. <em>JASA</em>, <em>119</em>(546),
1229–1239. (<a
href="https://doi.org/10.1080/01621459.2023.2183130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard statistical methods without taking proper account of the complexity of a survey design can lead to erroneous inferences when applied to survey data due to unequal selection probabilities, clustering, and other design features. In particular, the Type I error rates of hypotheses tests using standard methods can be much larger than the nominal significance level. Methods incorporating design features in testing hypotheses have been proposed, including Wald tests and quasi-score tests that involve estimated covariance matrices of parameter estimates. In this article, we present a unified approach to hypothesis testing without requiring estimated covariance matrices or design effects, by constructing bootstrap approximations to quasi-likelihood ratio statistics and quasi-score statistics and establishing its asymptotic validity. The proposed method can be easily implemented without a specific software designed for complex survey sampling. We also consider hypothesis testing for categorical data and present a bootstrap procedure for testing simple goodness of fit and independence in a two-way table. In simulation studies, the Type I error rates of the proposed approach are much closer to their nominal significance level compared with the naive likelihood ratio test and quasi-score test. An application to an educational survey under a logistic regression model is also presented. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jae Kwang Kim and J. N. K. Rao and Zhonglei Wang},
  doi          = {10.1080/01621459.2023.2183130},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1229-1239},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hypotheses testing from complex survey data using bootstrap weights: A unified approach},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online smooth backfitting for generalized additive models.
<em>JASA</em>, <em>119</em>(546), 1215–1228. (<a
href="https://doi.org/10.1080/01621459.2023.2182213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an online smoothing backfitting method for generalized additive models coupled with local linear estimation. The idea can be extended to general nonlinear optimization problems. The strategy is to use an appropriate-order expansion to approximate the nonlinear equations and store the coefficients as sufficient statistics which can be updated in an online manner by the dynamic candidate bandwidth method. We investigate the statistical and algorithmic convergences of the proposed method. By defining the relative statistical efficiency and computational cost, we further establish a framework to characterize the tradeoff between estimation performance and computation performance. Simulations and real data examples are provided to illustrate the proposed method and algorithm. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ying Yang and Fang Yao and Peng Zhao},
  doi          = {10.1080/01621459.2023.2182213},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1215-1228},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Online smooth backfitting for generalized additive models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear causal discovery with confounders. <em>JASA</em>,
<em>119</em>(546), 1205–1214. (<a
href="https://doi.org/10.1080/01621459.2023.2179490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a causal discovery method to learn nonlinear relationships in a directed acyclic graph with correlated Gaussian errors due to confounding. First, we derive model identifiability under the sublinear growth assumption. Then, we propose a novel method, named the Deconfounded Functional Structure Estimation (DeFuSE), consisting of a deconfounding adjustment to remove the confounding effects and a sequential procedure to estimate the causal order of variables. We implement DeFuSE via feedforward neural networks for scalable computation. Moreover, we establish the consistency of DeFuSE under an assumption called the strong causal minimality. In simulations, DeFuSE compares favorably against state-of-the-art competitors that ignore confounding or nonlinearity. Finally, we demonstrate the utility and effectiveness of the proposed approach with an application to gene regulatory network analysis. The Python implementation is available at https://github.com/chunlinli/defuse . Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chunlin Li and Xiaotong Shen and Wei Pan},
  doi          = {10.1080/01621459.2023.2179490},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1205-1214},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonlinear causal discovery with confounders},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Crowdsourcing utilizing subgroup structure of latent factor
modeling. <em>JASA</em>, <em>119</em>(546), 1192–1204. (<a
href="https://doi.org/10.1080/01621459.2023.2178925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing has emerged as an alternative solution for collecting large scale labels. However, the majority of recruited workers are not domain experts, so their contributed labels could be noisy. In this article, we propose a two-stage model to predict the true labels for multicategory classification tasks in crowdsourcing. In the first stage, we fit the observed labels with a latent factor model and incorporate subgroup structures for both tasks and workers through a multi-centroid grouping penalty. Group-specific rotations are introduced to align workers with different task categories to solve multicategory crowdsourcing tasks. In the second stage, we propose a concordance-based approach to identify high-quality worker subgroups who are relied upon to assign labels to tasks. In theory, we show the estimation consistency of the latent factors and the prediction consistency of the proposed method. The simulation studies show that the proposed method outperforms the existing competitive methods, assuming the subgroup structures within tasks and workers. We also demonstrate the application of the proposed method to real world problems and show its superiority. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qi Xu and Yubai Yuan and Junhui Wang and Annie Qu},
  doi          = {10.1080/01621459.2023.2178925},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1192-1204},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Crowdsourcing utilizing subgroup structure of latent factor modeling},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intraday periodic volatility curves. <em>JASA</em>,
<em>119</em>(546), 1181–1191. (<a
href="https://doi.org/10.1080/01621459.2023.2177546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volatility of financial asset returns displays pronounced variation over the trading day. Our goal is nonparametric inference for the average intraday volatility pattern, viewed as a function of time-of-day. The functional inference is based on a long span of high-frequency return data. Our setup allows for general forms of volatility dynamics, including time-variation in the intraday pattern. The estimation is based on forming local volatility estimates from the high-frequency returns over overlapping blocks of asymptotically shrinking size, and then averaging these estimates across days in the sample. The block-based estimation of volatility renders the error in the estimation due to the martingale return innovation asymptotically negligible. As a result, the centered and scaled calendar volatility effect estimator converges to a Gaussian process determined by the empirical process error associated with estimating average volatility across the trading day. Feasible inference is obtained by consistently estimating the limiting covariance operator. Simulation results corroborate our theoretical findings. In an application to S&amp;P 500 futures data, we find evidence for a shift in the intraday volatility pattern over time, including a more pronounced role for volatility outside U.S. trading hours in the latter part of the sample. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Torben G. Andersen and Tao Su and Viktor Todorov and Zhiyuan Zhang},
  doi          = {10.1080/01621459.2023.2177546},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1181-1191},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Intraday periodic volatility curves},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving estimating equations with copulas. <em>JASA</em>,
<em>119</em>(546), 1168–1180. (<a
href="https://doi.org/10.1080/01621459.2023.2177545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to their ability to capture complex dependence structures, copulas are frequently used to glue random variables into a joint model with arbitrary marginal distributions. More recently, they have been applied to solve statistical learning problems such as regression or classification. Framing such approaches as solutions of estimating equations, we generalize them in a unified framework. We can then obtain simultaneous, coherent inferences across multiple regression-like problems. We derive consistency, asymptotic normality, and validity of the bootstrap for corresponding estimators. The conditions allow for both continuous and discrete data as well as parametric, nonparametric, and semiparametric estimators of the copula and marginal distributions. The versatility of this methodology is illustrated by several theoretical examples, a simulation study, and an application to financial portfolio allocation. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Thomas Nagler and Thibault Vatter},
  doi          = {10.1080/01621459.2023.2177545},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1168-1180},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Solving estimating equations with copulas},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian modeling with spatial curvature processes.
<em>JASA</em>, <em>119</em>(546), 1155–1167. (<a
href="https://doi.org/10.1080/01621459.2023.2177166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial process models are widely used for modeling point-referenced variables arising from diverse scientific domains. Analyzing the resulting random surface provides deeper insights into the nature of latent dependence within the studied response. We develop Bayesian modeling and inference for rapid changes on the response surface to assess directional curvature along a given trajectory. Such trajectories or curves of rapid change, often referred to as wombling boundaries, occur in geographic space in the form of rivers in a flood plain, roads, mountains or plateaus or other topographic features leading to high gradients on the response surface. We demonstrate fully model based Bayesian inference on directional curvature processes to analyze differential behavior in responses along wombling boundaries. We illustrate our methodology with a number of simulated experiments followed by multiple applications featuring the Boston Housing data; Meuse river data; and temperature data from the Northeastern United States. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Aritra Halder and Sudipto Banerjee and Dipak K. Dey},
  doi          = {10.1080/01621459.2023.2177166},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1155-1167},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian modeling with spatial curvature processes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-sample conditional distribution test using conformal
prediction and weighted rank sum. <em>JASA</em>, <em>119</em>(546),
1136–1154. (<a
href="https://doi.org/10.1080/01621459.2023.2177165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of testing the equality of conditional distributions of a response variable given a vector of covariates between two populations. Such a hypothesis testing problem can be motivated from various machine learning and statistical inference scenarios, including transfer learning and causal predictive inference. We develop a nonparametric test procedure inspired from the conformal prediction framework. The construction of our test statistic combines recent developments in conformal prediction with a novel choice of conformity score, resulting in a weighted rank-sum test statistic that is valid and powerful under general settings. To our knowledge, this is the first successful attempt of using conformal prediction for testing statistical hypotheses beyond exchangeability. Our method is suitable for modern machine learning scenarios where the data has high dimensionality and large sample sizes, and can be effectively combined with existing classification algorithms to find good conformity score functions. The performance of the proposed method is demonstrated in various numerical examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiaoyu Hu and Jing Lei},
  doi          = {10.1080/01621459.2023.2177165},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1136-1154},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A two-sample conditional distribution test using conformal prediction and weighted rank sum},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton clustering: Dimension-free density-aided
clustering. <em>JASA</em>, <em>119</em>(546), 1124–1135. (<a
href="https://doi.org/10.1080/01621459.2023.2174122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a density-aided clustering method called Skeleton Clustering that can detect clusters in multivariate and even high-dimensional data with irregular shapes. To bypass the curse of dimensionality, we propose surrogate density measures that are less dependent on the dimension but have intuitive geometric interpretations. The clustering framework constructs a concise representation of the given data as an intermediate step and can be thought of as a combination of prototype methods, density-based clustering, and hierarchical clustering. We show by theoretical analysis and empirical studies that the skeleton clustering leads to reliable clusters in multivariate and high-dimensional scenarios. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zeyu Wei and Yen-Chi Chen},
  doi          = {10.1080/01621459.2023.2174122},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1124-1135},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Skeleton clustering: Dimension-free density-aided clustering},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian robustness: A nonasymptotic viewpoint.
<em>JASA</em>, <em>119</em>(546), 1112–1123. (<a
href="https://doi.org/10.1080/01621459.2023.2174121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of robustly estimating the posterior distribution for the setting where observed data can be contaminated with potentially adversarial outliers. We propose Rob-ULA, a robust variant of the Unadjusted Langevin Algorithm (ULA), and provide a finite-sample analysis of its sampling distribution. In particular, we show that after T = O ˜ ( d / ε acc ) iterations, we can sample from p T such that dist ( p T , p * ) ≤ ε acc + O ˜ ( ϵ ) , where ϵ is the fraction of corruptions and dist represents the squared 2-Wasserstein distance metric. Our results for the class of posteriors p * which satisfy log-concavity and smoothness assumptions. We corroborate our theoretical analysis with experiments on both synthetic and real-world datasets for mean estimation, regression and binary classification. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kush Bhatia and Yi-An Ma and Anca D. Dragan and Peter L. Bartlett and Michael I. Jordan},
  doi          = {10.1080/01621459.2023.2174121},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1112-1123},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian robustness: A nonasymptotic viewpoint},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel estimation of bivariate time-varying coefficient
model for longitudinal data with terminal event. <em>JASA</em>,
<em>119</em>(546), 1102–1111. (<a
href="https://doi.org/10.1080/01621459.2023.2169702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nonparametric bivariate time-varying coefficient model for longitudinal measurements with the occurrence of a terminal event that is subject to right censoring. The time-varying coefficients capture the longitudinal trajectories of covariate effects along with both the followup time and the residual lifetime. The proposed model extends the parametric conditional approach given terminal event time in recent literature, and thus avoids potential model misspecification. We consider a kernel smoothing method for estimating regression coefficients in our model and use cross-validation for bandwidth selection, applying undersmoothing in the final analysis to eliminate the asymptotic bias of the kernel estimator. We show that the kernel estimates follow a finite-dimensional normal distribution asymptotically under mild regularity conditions, and provide an easily computed sandwich covariance matrix estimator. We conduct extensive simulations that show desirable performance of the proposed approach, and apply the method to analyzing the medical cost data for patients with end-stage renal disease. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yue Wang and Bin Nan and John D. Kalbfleisch},
  doi          = {10.1080/01621459.2023.2169702},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1102-1111},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Kernel estimation of bivariate time-varying coefficient model for longitudinal data with terminal event},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational bayes for fast and accurate empirical likelihood
inference. <em>JASA</em>, <em>119</em>(546), 1089–1101. (<a
href="https://doi.org/10.1080/01621459.2023.2169701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a fast and accurate approach to approximate posterior distributions in the Bayesian empirical likelihood framework. Bayesian empirical likelihood allows for the use of Bayesian shrinkage without specification of a full likelihood but is notorious for leading to several computational difficulties. By coupling the stochastic variational Bayes procedure with an adjusted empirical likelihood framework, the proposed method overcomes the intractability of both the exact posterior and the arising evidence lower bound objective, and the mismatch between the exact posterior support and the variational posterior support. The optimization algorithm achieves fast algorithmic convergence by using the variational expected gradient of the log adjusted empirical likelihood function. We prove the consistency of the proposed approximate posterior distribution and an empirical likelihood analogue of the variational Bernstein-von-Mises theorem. Through several numerical examples, we confirm the accuracy and quick algorithmic convergence of our proposed method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Weichang Yu and Howard D. Bondell},
  doi          = {10.1080/01621459.2023.2169701},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1089-1101},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variational bayes for fast and accurate empirical likelihood inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are latent factor regression and sparse regression adequate?
<em>JASA</em>, <em>119</em>(546), 1076–1088. (<a
href="https://doi.org/10.1080/01621459.2023.2169700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the Factor Augmented (sparse linear) Regression Model (FARM) that not only admits both the latent factor regression and sparse linear regression as special cases but also bridges dimension reduction and sparse regression together. We provide theoretical guarantees for the estimation of our model under the existence of sub-Gaussian and heavy-tailed noises (with bounded ( 1 + ϑ ) th moment, for all ϑ &gt; 0 ), respectively. In addition, the existing works on supervised learning often assume the latent factor regression or sparse linear regression is the true underlying model without justifying its adequacy. To fill in such an important gap on high-dimensional inference, we also leverage our model as the alternative model to test the sufficiency of the latent factor regression and the sparse linear regression models. To accomplish these goals, we propose the Factor-Adjusted deBiased Test (FabTest) and a two-stage ANOVA type test, respectively. We also conduct large-scale numerical experiments including both synthetic and FRED macroeconomics data to corroborate the theoretical properties of our methods. Numerical results illustrate the robustness and effectiveness of our model against latent factor regression and sparse linear regression models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Zhipeng Lou and Mengxin Yu},
  doi          = {10.1080/01621459.2023.2169700},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1076-1088},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Are latent factor regression and sparse regression adequate?},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general m-estimation theory in semi-supervised framework.
<em>JASA</em>, <em>119</em>(546), 1065–1075. (<a
href="https://doi.org/10.1080/01621459.2023.2169699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of general M-estimators in the semi-supervised setting, wherein the data are typically a combination of a relatively small labeled dataset and large amounts of unlabeled data. A new estimator, which efficiently uses the useful information contained in the unlabeled data, is proposed via a projection technique. We prove consistency and asymptotic normality, and provide an inference procedure based on K -fold cross-validation. The optimal weights are derived to balance the contributions of the labeled and unlabeled data. It is shown that the proposed method, by taking advantage of the unlabeled data, produces asymptotically more efficient estimation of the target parameters than the supervised counterpart. Supportive numerical evidence is shown in simulation studies. Applications are illustrated in analysis of the homeless data in Los Angeles. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shanshan Song and Yuanyuan Lin and Yong Zhou},
  doi          = {10.1080/01621459.2023.2169699},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1065-1075},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A general M-estimation theory in semi-supervised framework},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal linear discriminant analysis for high-dimensional
functional data. <em>JASA</em>, <em>119</em>(546), 1055–1064. (<a
href="https://doi.org/10.1080/01621459.2022.2164288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing methods of functional data classification deal with one or a few processes. In this work we tackle classification of high-dimensional functional data, in which each observation is potentially associated with a large number of functional processes, p , which is comparable to or even much larger than the sample size n . The challenge arises from the complex inter-correlation structures among multiple functional processes, instead of a diagonal correlation for a single process. Since truncation is often needed for approximation in functional data, another difficulty stems from the fact that the discriminant set of the infinite-dimensional optimal classifier may be different from that of the truncated optimal classifier, when multiple (especially a large number of) processes are involved. We bridge the gap by proposing a penalized classifier that achieves both near-perfect classification that is unique to functional data, and discriminant set inclusion consistency in the sense that the classification-responsible functional predictors include those of the underlying optimal classifier. Simulation study and real data application are carried out to demonstrate its favorable performance. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kaijie Xue and Jin Yang and Fang Yao},
  doi          = {10.1080/01621459.2022.2164288},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1055-1064},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal linear discriminant analysis for high-dimensional functional data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Copula based cox proportional hazards models for dependent
censoring. <em>JASA</em>, <em>119</em>(546), 1044–1054. (<a
href="https://doi.org/10.1080/01621459.2022.2161387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing copula models for dependent censoring in the literature assume that the parameter defining the copula is known. However, prior knowledge on this dependence parameter is often unavailable. In this article we propose a novel model under which the copula parameter does not need to be known. The model is based on a parametric copula model for the relation between the survival time ( T ) and the censoring time ( C ), whereas the marginal distributions of T and C follow a semiparametric Cox proportional hazards model and a parametric model, respectively. We show that this model is identified, and propose estimators of the nonparametric cumulative hazard and the finite-dimensional parameters. It is shown that the estimators of the model parameters and the cumulative hazard function are consistent and asymptotically normal. We also investigate the performance of the proposed method using finite-sample simulations. Finally, we apply our model and estimation procedure to a follicular cell lymphoma dataset. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Negera Wakgari Deresa and Ingrid Van Keilegom},
  doi          = {10.1080/01621459.2022.2161387},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1044-1054},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Copula based cox proportional hazards models for dependent censoring},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On semiparametrically dynamic functional-coefficient
autoregressive spatio-temporal models with irregular location wide
nonstationarity. <em>JASA</em>, <em>119</em>(546), 1032–1043. (<a
href="https://doi.org/10.1080/01621459.2022.2161386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear dynamic modeling of spatio-temporal data is often a challenge, especially due to irregularly observed locations and location-wide nonstationarity. In this article we propose a semiparametric family of Dynamic Functional-coefficient Autoregressive Spatio-Temporal (DyFAST) models to address the difficulties. We specify the autoregressive smoothing coefficients depending dynamically on both a concerned regime and location so that the models can characterize not only the dynamic regime-switching nature but also the location-wide nonstationarity in real data. Different smoothing schemes are then proposed to model the dynamic neighboring-time interaction effects with irregular locations incorporated by (spatial) weight matrices. The first scheme popular in econometrics supposes that the weight matrix is pre-specified. We show that locally optimal bandwidths by a greedy idea popular in machine learning should be cautiously applied. Moreover, many weight matrices can be generated differently by data location features. Model selection is popular, but may suffer from loss of different candidate features. Our second scheme is thus to suggest a weight matrix fusion to let data combine or select the candidates with estimation done simultaneously. Both theoretical properties and Monte Carlo simulations are investigated. The empirical application to an EU energy market dataset further demonstrates the usefulness of our DyFAST models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zudi Lu and Xiaohang Ren and Rongmao Zhang},
  doi          = {10.1080/01621459.2022.2161386},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1032-1043},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On semiparametrically dynamic functional-coefficient autoregressive spatio-temporal models with irregular location wide nonstationarity},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Higher-order least squares: Assessing partial goodness of
fit of linear causal models. <em>JASA</em>, <em>119</em>(546),
1019–1031. (<a
href="https://doi.org/10.1080/01621459.2022.2157728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a simple diagnostic test for assessing the overall or partial goodness of fit of a linear causal model with errors being independent of the covariates. In particular, we consider situations where hidden confounding is potentially present. We develop a method and discuss its capability to distinguish between covariates that are confounded with the response by latent variables and those that are not. Thus, we provide a test and methodology for partial goodness of fit. The test is based on comparing a novel higher-order least squares principle with ordinary least squares. In spite of its simplicity, the proposed method is extremely general and is also proven to be valid for high-dimensional settings. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Christoph Schultheiss and Peter Bühlmann and Ming Yuan},
  doi          = {10.1080/01621459.2022.2157728},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1019-1031},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Higher-order least squares: Assessing partial goodness of fit of linear causal models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A random projection approach to hypothesis tests in
high-dimensional single-index models. <em>JASA</em>, <em>119</em>(546),
1008–1018. (<a
href="https://doi.org/10.1080/01621459.2022.2156350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the problem of hypothesis testing in high-dimensional single-index models. First, we study the feasibility of applying the classical F-test to a single-index model when the dimension of covariate vector and sample size are of the same order, and derive its asymptotic null distribution and asymptotic local power function. For the ultrahigh-dimensional single-index model, we construct F-statistics based on lower-dimensional random projections of the data, and establish the asymptotic null distribution and the asymptotic local power function of the proposed test statistics for the hypothesis testing of global and partial parameters. The new proposed test possesses the advantages of having a simple structure as well as being easy to compute. We compare the proposed test with other high-dimensional tests and provide sufficient conditions under which the proposed tests are more efficient. We conduct simulation studies to evaluate the finite-sample performances of the proposed tests and demonstrate that it has higher power than some existing methods in the models we consider. The application of real high-dimensional gene expression data is also provided to illustrate the effectiveness of the method. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Changyu Liu and Xingqiu Zhao and Jian Huang},
  doi          = {10.1080/01621459.2022.2156350},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {1008-1018},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A random projection approach to hypothesis tests in high-dimensional single-index models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guaranteed functional tensor singular value decomposition.
<em>JASA</em>, <em>119</em>(546), 995–1007. (<a
href="https://doi.org/10.1080/01621459.2022.2153689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the functional tensor singular value decomposition (FTSVD), a novel dimension reduction framework for tensors with one functional mode and several tabular modes. The problem is motivated by high-order longitudinal data analysis. Our model assumes the observed data to be a random realization of an approximate CP low-rank functional tensor measured on a discrete time grid. Incorporating tensor algebra and the theory of reproducing kernel Hilbert space (RKHS), we propose a novel RKHS-based constrained power iteration with spectral initialization. Our method can successfully estimate both singular vectors and functions of the low-rank structure in the observed data. With mild assumptions, we establish the non-asymptotic contractive error bounds for the proposed algorithm. The superiority of the proposed framework is demonstrated via extensive experiments on both simulated and real data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Rungang Han and Pixu Shi and Anru R. Zhang},
  doi          = {10.1080/01621459.2022.2153689},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {995-1007},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Guaranteed functional tensor singular value decomposition},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidently comparing estimates with the c-value.
<em>JASA</em>, <em>119</em>(546), 983–994. (<a
href="https://doi.org/10.1080/01621459.2022.2153688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern statistics provides an ever-expanding toolkit for estimating unknown parameters. Consequently, applied statisticians frequently face a difficult decision: retain a parameter estimate from a familiar method or replace it with an estimate from a newer or more complex one. While it is traditional to compare estimates using risk, such comparisons are rarely conclusive in realistic settings. In response, we propose the “c-value” as a measure of confidence that a new estimate achieves smaller loss than an old estimate on a given dataset. We show that it is unlikely that a large c-value coincides with a larger loss for the new estimate. Therefore, just as a small p -value supports rejecting a null hypothesis, a large c-value supports using a new estimate in place of the old. For a wide class of problems and estimates, we show how to compute a c-value by first constructing a data-dependent high-probability lower bound on the difference in loss. The c-value is frequentist in nature, but we show that it can provide validation of shrinkage estimates derived from Bayesian models in real data applications involving hierarchical models and Gaussian processes. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Brian L. Trippe and Sameer K. Deshpande and Tamara Broderick},
  doi          = {10.1080/01621459.2022.2153688},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {983-994},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Confidently comparing estimates with the c-value},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive algorithm for multi-armed bandit problem with
high-dimensional covariates. <em>JASA</em>, <em>119</em>(546), 970–982.
(<a href="https://doi.org/10.1080/01621459.2022.2152343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies an important sequential decision making problem known as the multi-armed stochastic bandit problem with covariates. Under a linear bandit framework with high-dimensional covariates, we propose a general multi-stage arm allocation algorithm that integrates both arm elimination and randomized assignment strategies. By employing a class of high-dimensional regression methods for coefficient estimation, the proposed algorithm is shown to have near optimal finite-time regret performance under a new study scope that requires neither a margin condition nor a reward gap condition for competitive arms. Based on the synergistically verified benefit of the margin, our algorithm exhibits adaptive performance that automatically adapts to the margin and gap conditions, and attains optimal regret rates simultaneously for both study scopes, without or with the margin, up to a logarithmic factor. Besides the desirable regret performance, the proposed algorithm simultaneously generates useful coefficient estimation output for competitive arms and is shown to achieve both estimation consistency and variable selection consistency. Promising empirical performance is demonstrated through extensive simulation and two real data evaluation examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wei Qian and Ching-Kang Ing and Ji Liu},
  doi          = {10.1080/01621459.2022.2152343},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {970-982},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive algorithm for multi-armed bandit problem with high-dimensional covariates},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous decorrelation of matrix time series.
<em>JASA</em>, <em>119</em>(546), 957–969. (<a
href="https://doi.org/10.1080/01621459.2022.2151448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a contemporaneous bilinear transformation for a p × q matrix time series to alleviate the difficulties in modeling and forecasting matrix time series when p and/or q are large. The resulting transformed matrix assumes a block structure consisting of several small matrices, and those small matrix series are uncorrelated across all times. Hence, an overall parsimonious model is achieved by modeling each of those small matrix series separately without the loss of information on the linear dynamics. Such a parsimonious model often has better forecasting performance, even when the underlying true dynamics deviates from the assumed uncorrelated block structure after transformation. The uniform convergence rates of the estimated transformation are derived, which vindicate an important virtue of the proposed bilinear transformation, that is, it is technically equivalent to the decorrelation of a vector time series of dimension max( p , q ) instead of p × q . The proposed method is illustrated numerically via both simulated and real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yuefeng Han and Rong Chen and Cun-Hui Zhang and Qiwei Yao},
  doi          = {10.1080/01621459.2022.2151448},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {957-969},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simultaneous decorrelation of matrix time series},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partially linear additive regression with a general
hilbertian response. <em>JASA</em>, <em>119</em>(546), 942–956. (<a
href="https://doi.org/10.1080/01621459.2022.2149407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we develop semiparametric regression techniques for fitting partially linear additive models. The methods are for a general Hilbert-space-valued response. They use a powerful technique of additive regression in profiling out the additive nonparametric components of the models, which necessarily involves additive regression of the nonadditive effects of covariates. We show that the estimators of the parametric components are n -consistent and asymptotically Gaussian under weak conditions. We also prove that the estimators of the nonparametric components, which are random elements taking values in a space of Hilbert-space-valued maps, achieve the univariate rate of convergence regardless of the dimension of covariates. We present some numerical evidence for the success of the proposed method and discuss real data applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Sungho Cho and Jeong Min Jeon and Dongwoo Kim and Kyusang Yu and Byeong U. Park},
  doi          = {10.1080/01621459.2022.2149407},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {942-956},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Partially linear additive regression with a general hilbertian response},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-dimensional discrete random structures and bayesian
clustering. <em>JASA</em>, <em>119</em>(546), 929–941. (<a
href="https://doi.org/10.1080/01621459.2022.2149406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete random probability measures stand out as effective tools for Bayesian clustering. The investigation in the area has been very lively, with a strong emphasis on nonparametric procedures based on either the Dirichlet process or on more flexible generalizations, such as the normalized random measures with independent increments (NRMI). The literature on finite-dimensional discrete priors is much more limited and mostly confined to the standard Dirichlet-multinomial model. While such a specification may be attractive due to conjugacy, it suffers from considerable limitations when it comes to addressing clustering problems. In order to overcome these, we introduce a novel class of priors that arise as the hierarchical compositions of finite-dimensional random discrete structures. Despite the analytical hurdles such a construction entails, we are able to characterize the induced random partition and determine explicit expressions of the associated urn scheme and of the posterior distribution. A detailed comparison with (infinite-dimensional) NRMIs is also provided: indeed, informative bounds for the discrepancy between the partition laws are obtained. Finally, the performance of our proposal over existing methods is assessed on a real application where we study a publicly available dataset from the Italian education system comprising the scores of a mandatory nationwide test.},
  archive      = {J_JASA},
  author       = {Antonio Lijoi and Igor Prünster and Tommaso Rigon},
  doi          = {10.1080/01621459.2022.2149406},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {929-941},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Finite-dimensional discrete random structures and bayesian clustering},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proximal learning for individualized treatment regimes under
unmeasured confounding. <em>JASA</em>, <em>119</em>(546), 915–928. (<a
href="https://doi.org/10.1080/01621459.2022.2147841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven individualized decision making has recently received increasing research interest. However, most existing methods rely on the assumption of no unmeasured confounding, which cannot be ensured in practice especially in observational studies. Motivated by the recently proposed proximal causal inference, we develop several proximal learning methods to estimate optimal individualized treatment regimes (ITRs) in the presence of unmeasured confounding. Explicitly, in terms of two types of proxy variables, we are able to establish several identification results for different classes of ITRs respectively, exhibiting the tradeoff between the risk of making untestable assumptions and the potential improvement of the value function in decision making. Based on these identification results, we propose several classification-based approaches to finding a variety of restricted in-class optimal ITRs and establish their theoretical properties. The appealing numerical performance of our proposed methods is demonstrated via extensive simulation experiments and a real data application. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhengling Qi and Rui Miao and Xiaoke Zhang},
  doi          = {10.1080/01621459.2022.2147841},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {915-928},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Proximal learning for individualized treatment regimes under unmeasured confounding},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Valid model-free spatial prediction. <em>JASA</em>,
<em>119</em>(546), 904–914. (<a
href="https://doi.org/10.1080/01621459.2022.2147531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the response at an unobserved location is a fundamental problem in spatial statistics. Given the difficulty in modeling spatial dependence, especially in nonstationary cases, model-based prediction intervals are at risk of misspecification bias that can negatively affect their validity. Here we present a new approach for model-free nonparametric spatial prediction based on the conformal prediction machinery. Our key observation is that spatial data can be treated as exactly or approximately exchangeable in a wide range of settings. In particular, under an infill asymptotic regime, we prove that the response values are, in a certain sense, locally approximately exchangeable for a broad class of spatial processes, and we develop a local spatial conformal prediction algorithm that yields valid prediction intervals without strong model assumptions like stationarity. Numerical examples with both real and simulated data confirm that the proposed conformal prediction intervals are valid and generally more efficient than existing model-based procedures for large datasets across a range of nonstationary and non-Gaussian settings.},
  archive      = {J_JASA},
  author       = {Huiying Mao and Ryan Martin and Brian J. Reich},
  doi          = {10.1080/01621459.2022.2147531},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {904-914},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Valid model-free spatial prediction},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-way truncated linear regression models with extremely
thresholding penalization. <em>JASA</em>, <em>119</em>(546), 887–903.
(<a href="https://doi.org/10.1080/01621459.2022.2147074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new type of linear regression model with regularization. Each predictor is conditionally truncated through the presence of unknown thresholds. The new model, called the two-way truncated linear regression model (TWT-LR), is not only viewed as a nonlinear generalization of a linear model but is also a much more flexible model with greatly enhanced interpretability and applicability. The TWT-LR model performs classifications through thresholds similar to the tree-based methods and conducts inferences that are the same as the classical linear model on different segments. In addition, the innovative penalization, called the extremely thresholding penalty (ETP), is applied to thresholds. The ETP is independent of the values of regression coefficients and does not require any normalizations of regressors. The TWT-LR-ETP model detects thresholds at a wide range, including the two extreme ends where data are sparse. Under suitable conditions, both the estimators for coefficients and thresholds are consistent, with the convergence rate for threshold estimators being faster than n . Furthermore, the estimators for coefficients are asymptotically normal for fixed dimension p . It is demonstrated in simulations and real data analyses that the TWT-LR-ETP model illustrates various threshold features and provides better estimation and prediction results than existing models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hao Yang Teng and Zhengjun Zhang},
  doi          = {10.1080/01621459.2022.2147074},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {887-903},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Two-way truncated linear regression models with extremely thresholding penalization},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal design of experiments on riemannian manifolds.
<em>JASA</em>, <em>119</em>(546), 875–886. (<a
href="https://doi.org/10.1080/01621459.2022.2146587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theory of optimal design of experiments has been traditionally developed on an Euclidean space. In this article, new theoretical results and an algorithm for finding the optimal design of an experiment located on a Riemannian manifold are provided. It is shown that analogously to the results in Euclidean spaces, D-optimal and G-optimal designs are equivalent on manifolds, and we provide a lower bound for the maximum prediction variance of the response evaluated over the manifold. In addition, a converging algorithm that finds the optimal experimental design on manifold data is proposed. Numerical experiments demonstrate the importance of considering the manifold structure in a designed experiment when present, and the superiority of the proposed algorithm. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hang Li and Enrique Del Castillo},
  doi          = {10.1080/01621459.2022.2146587},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {875-886},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal design of experiments on riemannian manifolds},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Addressing multiple detection limits with semiparametric
cumulative probability models. <em>JASA</em>, <em>119</em>(546),
864–874. (<a
href="https://doi.org/10.1080/01621459.2024.2315667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection limits (DLs), where a variable cannot be measured outside of a certain range, are common in research. DLs may vary across study sites or over time. Most approaches to handling DLs in response variables implicitly make strong parametric assumptions on the distribution of data outside DLs. We propose a new approach to deal with multiple DLs based on a widely used ordinal regression model, the cumulative probability model (CPM). The CPM is a rank-based, semiparametric linear transformation model that can handle mixed distributions of continuous and discrete outcome variables. These features are key for analyzing data with DLs because while observations inside DLs are continuous, those outside DLs are censored and generally put into discrete categories. With a single lower DL, CPMs assign values below the DL as having the lowest rank. With multiple DLs, the CPM likelihood can be modified to appropriately distribute probability mass. We demonstrate the use of CPMs with DLs via simulations and a data example. This work is motivated by a study investigating factors associated with HIV viral load 6 months after starting antiretroviral therapy in Latin America; 56% of observations are below lower DLs that vary across study sites and over time. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Yuqi Tian and Chun Li and Shengxin Tu and Nathan T. James and FrankE. Harrell and BryanE. Shepherd},
  doi          = {10.1080/01621459.2024.2315667},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {864-874},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Addressing multiple detection limits with semiparametric cumulative probability models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneity analysis on multi-state brain functional
connectivity and adolescent neurocognition. <em>JASA</em>,
<em>119</em>(546), 851–863. (<a
href="https://doi.org/10.1080/01621459.2024.2311363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain functional connectivity or connectome, a unique measure for brain functional organization, provides a great potential to explain the neurobiological underpinning of behavioral profiles. Existing connectome-based analyses highly concentrate on brain activities under a single cognitive state, and fail to consider heterogeneity when attempting to characterize brain-to-behavior relationships. In this work, we study the complex impact of multi-state functional connectivity on behaviors by analyzing the data from a recent landmark brain development and child health study. We propose a nonparametric, Bayesian supervised heterogeneity analysis to uncover neurodevelopmental subtypes with distinct effect mechanisms. We impose stochastic block structures to identify network-based functional phenotypes and develop a variational expectation-maximization algorithm to facilitate an efficient posterior computation. Through integrating resting-state and task-related functional connectomes, we dissect heterogeneous effect mechanisms on children’s fluid intelligence from the functional network phenotypes, including Fronto-parietal Network and Default Mode Network, under different cognitive states. Based on extensive simulations, we further confirm the superior performance of our method on uncovering brain-to-behavior relationships. Supplementary materials for this article are available online including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Shiying Wang and Todd Constable and Heping Zhang and Yize Zhao},
  doi          = {10.1080/01621459.2024.2311363},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {851-863},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Heterogeneity analysis on multi-state brain functional connectivity and adolescent neurocognition},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating trans-ancestry genetic correlation with
unbalanced data resources. <em>JASA</em>, <em>119</em>(546), 839–850.
(<a href="https://doi.org/10.1080/01621459.2024.2344703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to propose a novel method for estimating trans-ancestry genetic correlations in genome-wide association studies (GWAS) using genetically predicted observations. These correlations describe how genetic architecture of complex traits varies among populations. Our new estimator corrects for biases arising from prediction errors in high-dimensional weak GWAS signals, while addressing the ethnic diversity inherent in GWAS data, such as linkage disequilibrium (LD) differences. A distinguishing feature of our approach is its flexibility regarding sample sizes: it necessitates a large GWAS sample only from one population, while the secondary population may have a much smaller cohort, even in the hundreds. This design directly addresses the existing imbalance in GWAS data resources, where datasets for European populations typically outnumber those of non-European ancestries. Through extensive simulations and real data analysis from the UK Biobank study encompassing 26 complex traits, we validate the reliability of our method. Our results illuminate the broader implications of transferring genetic findings across diverse populations. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
  archive      = {J_JASA},
  author       = {Bingxin Zhao and Xiaochen Yang and Hongtu Zhu},
  doi          = {10.1080/01621459.2024.2344703},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {839-850},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating trans-ancestry genetic correlation with unbalanced data resources},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging weather dynamics in insurance claims triage using
deep learning. <em>JASA</em>, <em>119</em>(546), 825–838. (<a
href="https://doi.org/10.1080/01621459.2024.2308314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In property insurance claims triage, insurers often use static information to assess the severity of a claim and to identify the subsequent actions. We hypothesize that the pattern of weather conditions throughout the course of a loss event is predictive of the insured losses, and hence appropriate use of weather dynamics improves the operation of insurers’ claim management. To test this hypothesis, we propose a deep learning method to incorporate dynamic weather information in the predictive modeling of the insured losses for reported claims. The proposed method features a hierarchical network architecture to address the challenges in claims triage due to the nature of weather dynamics. In the empirical analysis, we examine a portfolio of hail damage property insurance claims obtained from a major U.S. insurance carrier. When supplemented by dynamic weather information, the deep learning method exhibits substantial improvement in the hold-out predictive performance. We further design a cost-conscious decision strategy for triaging claims using the probabilistic forecasts of the insurance claim amounts. We show that leveraging weather dynamics in claims triage leads to a reduction of up to 9% and 6% in operational costs compared to when the triaging decision is based on forecasts without any weather information and with only static weather information, respectively. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Peng Shi and Wei Zhang and Kun Shi},
  doi          = {10.1080/01621459.2024.2308314},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {825-838},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Leveraging weather dynamics in insurance claims triage using deep learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating cell-type-specific gene co-expression networks
from bulk gene expression data with an application to alzheimer’s
disease. <em>JASA</em>, <em>119</em>(546), 811–824. (<a
href="https://doi.org/10.1080/01621459.2023.2297467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring and characterizing gene co-expression networks has led to important insights on the molecular mechanisms of complex diseases. Most co-expression analyses to date have been performed on gene expression data collected from bulk tissues with different cell type compositions across samples. As a result, the co-expression estimates only offer an aggregated view of the underlying gene regulations and can be confounded by heterogeneity in cell type compositions, failing to reveal gene coordination that may be distinct across different cell types. In this article, we introduce a flexible framework for estimating cell-type-specific gene co-expression networks from bulk sample data, without making specific assumptions on the distributions of gene expression profiles in different cell types. We develop a novel sparse least squares estimator, referred to as CSNet, that is efficient to implement and has good theoretical properties. Using CSNet, we analyzed the bulk gene expression data from a cohort study on Alzheimer’s disease and identified previously unknown cell-type-specific co-expressions among Alzheimer’s disease risk genes, suggesting cell-type-specific disease mechanisms. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chang Su and Jingfei Zhang and Hongyu Zhao},
  doi          = {10.1080/01621459.2023.2297467},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {811-824},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating cell-type-specific gene co-expression networks from bulk gene expression data with an application to alzheimer’s disease},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian landmark-based shape analysis of tumor pathology
images. <em>JASA</em>, <em>119</em>(546), 798–810. (<a
href="https://doi.org/10.1080/01621459.2023.2298031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging is a form of technology that has revolutionized the medical field over the past decades. Digital pathology imaging, which captures histological details at the cellular level, is rapidly becoming a routine clinical procedure for cancer diagnosis support and treatment planning. Recent developments in deep-learning methods have facilitated tumor region segmentation from pathology images. The traditional shape descriptors that characterize tumor boundary roughness at the anatomical level are no longer suitable. New statistical approaches to model tumor shapes are in urgent need. In this article, we consider the problem of modeling a tumor boundary as a closed polygonal chain. A Bayesian landmark-based shape analysis model is proposed. The model partitions the polygonal chain into mutually exclusive segments, accounting for boundary roughness. Our Bayesian inference framework provides uncertainty estimations on both the number and locations of landmarks, while outputting metrics that can be used to quantify boundary roughness. The performance of our model is comparable with that of a recently developed landmark detection model for planar elastic curves. In a case study of 143 consecutive patients with stage I to IV lung cancer, we demonstrated the heterogeneity of tumor boundary roughness derived from our model effectively predicted patient prognosis ( p -value &lt; 0.001 ). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Cong Zhang and Tejasv Bedi and Chul Moon and Yang Xie and Min Chen and Qiwei Li},
  doi          = {10.1080/01621459.2023.2298031},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {798-810},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian landmark-based shape analysis of tumor pathology images},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partnering with authors to enhance reproducibility at JASA.
<em>JASA</em>, <em>119</em>(546), 795–797. (<a
href="https://doi.org/10.1080/01621459.2024.2340557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Julia Wrobel and Emily C. Hector and Lorin Crawford and Lucy D’Agostino McGowan and Natalia da Silva and Jeff Goldsmith and Stephanie Hicks and Michael Kane and Youjin Lee and Vinicius Mayrink and Christopher J. Paciorek and Therri Usher and Julian Wolfson},
  doi          = {10.1080/01621459.2024.2340557},
  journal      = {Journal of the American Statistical Association},
  month        = {4},
  number       = {546},
  pages        = {795-797},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Partnering with authors to enhance reproducibility at JASA},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The journal of the american statistical association 2023
associate editors. <em>JASA</em>, <em>119</em>(545), 792–793. (<a
href="https://doi.org/10.1080/01621459.2024.2303300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2024.2303300},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {792-793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The journal of the american statistical association 2023 associate editors},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handbook of matching and weighting adjustments for causal
inference. <em>JASA</em>, <em>119</em>(545), 791. (<a
href="https://doi.org/10.1080/01621459.2023.2293811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Raymond K. W. Wong},
  doi          = {10.1080/01621459.2023.2293811},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {791},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of matching and weighting adjustments for causal inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fundamentals of causal inference: With r. <em>JASA</em>,
<em>119</em>(545), 790–791. (<a
href="https://doi.org/10.1080/01621459.2023.2287599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ting Ye},
  doi          = {10.1080/01621459.2023.2287599},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {790-791},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fundamentals of causal inference: With r},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable lévy processes via lamperti-type representations.
<em>JASA</em>, <em>119</em>(545), 789–790. (<a
href="https://doi.org/10.1080/01621459.2023.2286293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Giacomo Bormetti},
  doi          = {10.1080/01621459.2023.2286293},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {789-790},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stable lévy processes via lamperti-type representations},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Martingale methods in statistics. <em>JASA</em>,
<em>119</em>(545), 787–789. (<a
href="https://doi.org/10.1080/01621459.2023.2276742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Insuk Seo},
  doi          = {10.1080/01621459.2023.2276742},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {787-789},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Martingale methods in statistics},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical analytics for health data science with SAS and
r. <em>JASA</em>, <em>119</em>(545), 786–787. (<a
href="https://doi.org/10.1080/01621459.2023.2273403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ali Rahnavard},
  doi          = {10.1080/01621459.2023.2273403},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {786-787},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical analytics for health data science with SAS and r},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recommender systems: A review. <em>JASA</em>,
<em>119</em>(545), 773–785. (<a
href="https://doi.org/10.1080/01621459.2023.2279695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are the engine of online advertising. Not only do they suggest movies, music, or romantic partners, but they also are used to select which advertisements to show to users. This paper reviews the basics of recommender system methodology and then looks at the emerging arena of active recommender systems.},
  archive      = {J_JASA},
  author       = {Patrick M. LeBlanc and David Banks and Linhui Fu and Mingyan Li and Zhengyu Tang and Qiuyi Wu},
  doi          = {10.1080/01621459.2023.2279695},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {773-785},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Recommender systems: A review},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matching on generalized propensity scores with continuous
exposures. <em>JASA</em>, <em>119</em>(545), 757–772. (<a
href="https://doi.org/10.1080/01621459.2022.2144737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of a binary treatment, matching is a well-established approach in causal inference. However, in the context of a continuous treatment or exposure, matching is still underdeveloped. We propose an innovative matching approach to estimate an average causal exposure-response function under the setting of continuous exposures that relies on the generalized propensity score (GPS). Our approach maintains the following attractive features of matching: (a) clear separation between the design and the analysis; (b) robustness to model misspecification or to the presence of extreme values of the estimated GPS; (c) straightforward assessments of covariate balance. We first introduce an assumption of identifiability, called local weak unconfoundedness. Under this assumption and mild smoothness conditions, we provide theoretical guarantees that our proposed matching estimator attains point-wise consistency and asymptotic normality. In simulations, our proposed matching approach outperforms existing methods under settings with model misspecification or in the presence of extreme values of the estimated GPS. We apply our proposed method to estimate the average causal exposure-response function between long-term PM 2.5 exposure and all-cause mortality among 68.5 million Medicare enrollees, 2000–2016. We found strong evidence of a harmful effect of long-term PM 2.5 exposure on mortality. Code for the proposed matching approach is provided in the CausalGPS R package, which is available on CRAN and provides a computationally efficient implementation. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiao Wu and Fabrizia Mealli and Marianthi-Anna Kioumourtzoglou and Francesca Dominici and Danielle Braun},
  doi          = {10.1080/01621459.2022.2144737},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {757-772},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Matching on generalized propensity scores with continuous exposures},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projection test for mean vector in high dimensions.
<em>JASA</em>, <em>119</em>(545), 744–756. (<a
href="https://doi.org/10.1080/01621459.2022.2142592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the projection test for high-dimensional mean vectors via optimal projection. The idea of projection test is to project high-dimensional data onto a space of low dimension such that traditional methods can be applied. We first propose a new estimation for the optimal projection direction by solving a constrained and regularized quadratic programming. Then two tests are constructed using the estimated optimal projection direction. The first one is based on a data-splitting procedure, which achieves an exact t -test under normality assumption. To mitigate the power loss due to data-splitting, we further propose an online framework, which iteratively updates the estimation of projection direction when new observations arrive. We show that this online-style projection test asymptotically converges to the standard normal distribution. Various simulation studies as well as a real data example show that the proposed online-style projection test retains the Type I error rate well and is more powerful than other existing tests. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wanjun Liu and Xiufan Yu and Wei Zhong and Runze Li},
  doi          = {10.1080/01621459.2022.2142592},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {744-756},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Projection test for mean vector in high dimensions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair policy targeting. <em>JASA</em>, <em>119</em>(545),
730–743. (<a
href="https://doi.org/10.1080/01621459.2022.2142591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major concerns of targeting interventions on individuals in social welfare programs is discrimination: individualized treatments may induce disparities across sensitive attributes such as age, gender, or race. This article addresses the question of the design of fair and efficient treatment allocation rules. We adopt the nonmaleficence perspective of “first do no harm”: we select the fairest allocation within the Pareto frontier. We cast the optimization into a mixed-integer linear program formulation, which can be solved using off-the-shelf algorithms. We derive regret bounds on the unfairness of the estimated policy function and small sample guarantees on the Pareto frontier under general notions of fairness. Finally, we illustrate our method using an application from education economics. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Davide Viviano and Jelena Bradic},
  doi          = {10.1080/01621459.2022.2142591},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {730-743},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fair policy targeting},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust high-dimensional regression with coefficient
thresholding and its application to imaging data analysis.
<em>JASA</em>, <em>119</em>(545), 715–729. (<a
href="https://doi.org/10.1080/01621459.2022.2142590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to develop statistical techniques to analyze high-dimensional data in the presence of both complex dependence and possible heavy tails and outliers in real-world applications such as imaging data analyses. We propose a new robust high-dimensional regression with coefficient thresholding, in which an efficient nonconvex estimation procedure is proposed through a thresholding function and the robust Huber loss. The proposed regularization method accounts for complex dependence structures in predictors and is robust against heavy tails and outliers in outcomes. Theoretically, we rigorously analyze the landscape of the population and empirical risk functions for the proposed method. The fine landscape enables us to establish both statistical consistency and computational convergence under the high-dimensional setting. We also present an extension to incorporate spatial information into the proposed method. Finite-sample properties of the proposed methods are examined by extensive simulation studies. An application concerns a scalar-on-image regression analysis for an association of psychiatric disorder measured by the general factor of psychopathology with features extracted from the task functional MRI data in the Adolescent Brain Cognitive Development (ABCD) study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Bingyuan Liu and Qi Zhang and Lingzhou Xue and Peter X.-K. Song and Jian Kang},
  doi          = {10.1080/01621459.2022.2142590},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {715-729},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust high-dimensional regression with coefficient thresholding and its application to imaging data analysis},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric two-sample tests of high dimensional mean
vectors via random integration. <em>JASA</em>, <em>119</em>(545),
701–714. (<a
href="https://doi.org/10.1080/01621459.2022.2141636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the equality of the means in two samples is a fundamental statistical inferential problem. Most of the existing methods are based on the sum-of-squares or supremum statistics. They are possibly powerful in some situations, but not in others, and they do not work in a unified way. Using random integration of the difference, we develop a framework that includes and extends many existing methods, especially in high-dimensional settings, without restricting the same covariance matrices or sparsity. Under a general multivariate model, we can derive the asymptotic properties of the proposed test statistic without specifying a relationship between the data dimension and sample size explicitly. Specifically, the new framework allows us to better understand the test’s properties and select a powerful procedure accordingly. For example, we prove that our proposed test can achieve the power of 1 when nonzero signals in the true mean differences are weakly dense with nearly the same sign. In addition, we delineate the conditions under which the asymptotic relative Pitman efficiency of our proposed test to its competitor is greater than or equal to 1. Extensive numerical studies and a real data example demonstrate the potential of our proposed test. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yunlu Jiang and Xueqin Wang and Canhong Wen and Yukang Jiang and Heping Zhang},
  doi          = {10.1080/01621459.2022.2141636},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {701-714},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric two-sample tests of high dimensional mean vectors via random integration},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic convergence rates and applications of adaptive
quadrature in bayesian inference. <em>JASA</em>, <em>119</em>(545),
690–700. (<a
href="https://doi.org/10.1080/01621459.2022.2141635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide the first stochastic convergence rates for a family of adaptive quadrature rules used to normalize the posterior distribution in Bayesian models. Our results apply to the uniform relative error in the approximate posterior density, the coverage probabilities of approximate credible sets, and approximate moments and quantiles, therefore, guaranteeing fast asymptotic convergence of approximate summary statistics used in practice. The family of quadrature rules includes adaptive Gauss-Hermite quadrature, and we apply this rule in two challenging low-dimensional examples. Further, we demonstrate how adaptive quadrature can be used as a crucial component of a modern approximate Bayesian inference procedure for high-dimensional additive models. The method is implemented and made publicly available in the aghq package for the R language, available on CRAN. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Blair Bilodeau and Alex Stringer and Yanbo Tang},
  doi          = {10.1080/01621459.2022.2141635},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {690-700},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Stochastic convergence rates and applications of adaptive quadrature in bayesian inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust inference and modeling of mean and dispersion for
generalized linear models. <em>JASA</em>, <em>119</em>(545), 678–689.
(<a href="https://doi.org/10.1080/01621459.2022.2140054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Linear Models (GLMs) are a popular class of regression models when the responses follow a distribution in the exponential family. In real data the variability often deviates from the relation imposed by the exponential family distribution, which results in over- or underdispersion. Dispersion effects may even vary in the data. Such datasets do not follow the traditional GLM distributional assumptions, leading to unreliable inference. Therefore, the family of double exponential distributions has been proposed, which models both the mean and the dispersion as a function of covariates in the GLM framework. Since standard maximum likelihood inference is highly susceptible to the possible presence of outliers, we propose the robust double exponential (RDE) estimator. Asymptotic properties and robustness of the RDE estimator are discussed. A generalized robust quasi-deviance measure is introduced which constitutes the basis for a stable robust test. Simulations for binomial and Poisson models show the excellent performance of the RDE estimator and corresponding robust tests. Penalized versions of the RDE estimator are developed for sparse estimation with high-dimensional data and for flexible estimation via generalized additive models (GAMs). Real data applications illustrate the relevance of robust inference for dispersion effects in GLMs and GAMs. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jolien Ponnet and Pieter Segaert and Stefan Van Aelst and Tim Verdonck},
  doi          = {10.1080/01621459.2022.2140054},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {678-689},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust inference and modeling of mean and dispersion for generalized linear models},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling point referenced spatial count data: A poisson
process approach. <em>JASA</em>, <em>119</em>(545), 664–677. (<a
href="https://doi.org/10.1080/01621459.2022.2140053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random fields are useful mathematical tools for representing natural phenomena with complex dependence structures in space and/or time. In particular, the Gaussian random field is commonly used due to its attractive properties and mathematical tractability. However, this assumption seems to be restrictive when dealing with counting data. To deal with this situation, we propose a random field with a Poisson marginal distribution considering a sequence of independent copies of a random field with an exponential marginal distribution as “inter-arrival times” in the counting renewal processes framework. Our proposal can be viewed as a spatial generalization of the Poisson counting process. Unlike the classical hierarchical Poisson Log-Gaussian model, our proposal generates a (non)-stationary random field that is mean square continuous and with Poisson marginal distributions. For the proposed Poisson spatial random field, analytic expressions for the covariance function and the bivariate distribution are provided. In an extensive simulation study, we investigate the weighted pairwise likelihood as a method for estimating the Poisson random field parameters. Finally, the effectiveness of our methodology is illustrated by an analysis of reindeer pellet-group survey data, where a zero-inflated version of the proposed model is compared with zero-inflated Poisson Log-Gaussian and Poisson Gaussian copula models. Supplementary materials for this article, including technical proofs and R code for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Diego Morales-Navarrete and Moreno Bevilacqua and Christian Caamaño-Carrillo and Luis M. Castro},
  doi          = {10.1080/01621459.2022.2140053},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {664-677},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling point referenced spatial count data: A poisson process approach},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task learning with high-dimensional noisy images.
<em>JASA</em>, <em>119</em>(545), 650–663. (<a
href="https://doi.org/10.1080/01621459.2022.2140052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent medical imaging studies have given rise to distinct but inter-related datasets corresponding to multiple experimental tasks or longitudinal visits. Standard scalar-on-image regression models that fit each dataset separately are not equipped to leverage information across inter-related images, and existing multi-task learning approaches are compromised by the inability to account for the noise that is often observed in images. We propose a novel joint scalar-on-image regression framework involving wavelet-based image representations with grouped penalties that are designed to pool information across inter-related images for joint learning, and which explicitly accounts for noise in high-dimensional images via a projection-based approach. In the presence of nonconvexity arising due to noisy images, we derive nonasymptotic error bounds under nonconvex as well as convex grouped penalties, even when the number of voxels increases exponentially with sample size. A projected gradient descent algorithm is used for computation, which is shown to approximate the optimal solution via well-defined nonasymptotic optimization error bounds under noisy images. Extensive simulations and application to a motivating longitudinal Alzheimer’s disease study illustrate significantly improved predictive ability and greater power to detect true signals, that are simply missed by existing methods without noise correction due to the attenuation to null phenomenon. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xin Ma and Suprateek Kundu and for the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1080/01621459.2022.2140052},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {650-663},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multi-task learning with high-dimensional noisy images},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical learning for individualized asset allocation.
<em>JASA</em>, <em>119</em>(545), 639–649. (<a
href="https://doi.org/10.1080/01621459.2022.2139265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish a high-dimensional statistical learning framework for individualized asset allocation. Our proposed methodology addresses continuous-action decision-making with a large number of characteristics. We develop a discretization approach to model the effect of continuous actions and allow the discretization frequency to be large and diverge with the number of observations. We estimate the value function of continuous-action using penalized regression with our proposed generalized penalties that are imposed on linear transformations of the model coefficients. We show that our proposed Discretization and Regression with generalized fOlded concaVe penalty on Effect discontinuity (DROVE) approach enjoys desirable theoretical properties and allows for statistical inference of the optimal value associated with optimal decision-making. Empirically, the proposed framework is exercised with the Health and Retirement Study data in finding individualized optimal asset allocation. The results show that our individualized optimal strategy improves the financial well-being of the population. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yi Ding and Yingying Li and Rui Song},
  doi          = {10.1080/01621459.2022.2139265},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {639-649},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical learning for individualized asset allocation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating optimal infinite horizon dynamic treatment
regimes via pT-learning. <em>JASA</em>, <em>119</em>(545), 625–638. (<a
href="https://doi.org/10.1080/01621459.2022.2138760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in mobile health (mHealth) technology provide an effective way to monitor individuals’ health statuses and deliver just-in-time personalized interventions. However, the practical use of mHealth technology raises unique challenges to existing methodologies on learning an optimal dynamic treatment regime. Many mHealth applications involve decision-making with large numbers of intervention options and under an infinite time horizon setting where the number of decision stages diverges to infinity. In addition, temporary medication shortages may cause optimal treatments to be unavailable, while it is unclear what alternatives can be used. To address these challenges, we propose a Proximal Temporal consistency Learning (pT-Learning) framework to estimate an optimal regime that is adaptively adjusted between deterministic and stochastic sparse policy models. The resulting minimax estimator avoids the double sampling issue in the existing algorithms. It can be further simplified and can easily incorporate off-policy data without mismatched distribution corrections. We study theoretical properties of the sparse policy and establish finite-sample bounds on the excess risk and performance error. The proposed method is provided in our proximalDTR package and is evaluated through extensive simulation studies and the OhioT1DM mHealth dataset. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wenzhuo Zhou and Ruoqing Zhu and Annie Qu},
  doi          = {10.1080/01621459.2022.2138760},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {625-638},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating optimal infinite horizon dynamic treatment regimes via pT-learning},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the spectral density at frequencies near zero.
<em>JASA</em>, <em>119</em>(545), 612–624. (<a
href="https://doi.org/10.1080/01621459.2022.2133719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the spectral density function f ( w ) for some w ∈ [ − π , π ] w ∈ [ − π , π ] w∈[−π,π] has been traditionally performed by kernel smoothing the periodogram and related techniques. Kernel smoothing is tantamount to local averaging, that is, approximating f ( w ) by a constant over a window of small width. Although f ( w ) is uniformly continuous and periodic with period 2 π , in this article we recognize the fact that w = 0 effectively acts as a boundary point in the underlying kernel smoothing problem, and the same is true for w = ± π . It is well-known that local averaging may be suboptimal in kernel regression at (or near) a boundary point. As an alternative, we propose a local polynomial regression of the periodogram or log-periodogram when w is at (or near) the points 0 or ± π . The case w = 0 is of particular importance since f (0) is the large-sample variance of the sample mean; hence, estimating f (0) is crucial in order to conduct any sort of inference on the mean. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tucker McElroy and Dimitris N. Politis},
  doi          = {10.1080/01621459.2022.2133719},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {612-624},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating the spectral density at frequencies near zero},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference for social network data. <em>JASA</em>,
<em>119</em>(545), 597–611. (<a
href="https://doi.org/10.1080/01621459.2022.2131557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe semiparametric estimation and inference for causal effects using observational data from a single social network. Our asymptotic results are the first to allow for dependence of each observation on a growing number of other units as sample size increases. In addition, while previous methods have implicitly permitted only one of two possible sources of dependence among social network observations, we allow for both dependence due to transmission of information across network ties and for dependence due to latent similarities among nodes sharing ties. We propose new causal effects that are specifically of interest in social network settings, such as interventions on network ties and network structure. We use our methods to reanalyze an influential and controversial study that estimated causal peer effects of obesity using social network data from the Framingham Heart Study; after accounting for network structure we find no evidence for causal peer effects. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Elizabeth L. Ogburn and Oleg Sofrygin and Iván Díaz and Mark J. van der Laan},
  doi          = {10.1080/01621459.2022.2131557},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {597-611},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Causal inference for social network data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mass-shifting phenomenon of truncated multivariate normal
priors. <em>JASA</em>, <em>119</em>(545), 582–596. (<a
href="https://doi.org/10.1080/01621459.2022.2129059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that lower-dimensional marginal densities of dependent zero-mean normal distributions truncated to the positive orthant exhibit a mass-shifting phenomenon. Despite the truncated multivariate normal density having a mode at the origin, the marginal density assigns increasingly small mass near the origin as the dimension increases. The phenomenon accentuates with stronger correlation between the random variables. This surprising behavior has serious implications toward Bayesian constrained estimation and inference, where the prior, in addition to having a full support, is required to assign a substantial probability near the origin to capture flat parts of the true function of interest. A precise quantification of the mass-shifting phenomenon for both the prior and the posterior, characterizing the role of the dimension as well as the dependence, is provided under a variety of correlation structures. Without further modification, we show that truncated normal priors are not suitable for modeling flat regions and propose a novel alternative strategy based on shrinking the coordinates using a multiplicative scale parameter. The proposed shrinkage prior is shown to achieve optimal posterior contraction around true functions with potentially flat regions. Synthetic and real data studies demonstrate how the modification guards against the mass shifting phenomenon while retaining computational efficiency. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shuang Zhou and Pallavi Ray and Debdeep Pati and Anirban Bhattacharya},
  doi          = {10.1080/01621459.2022.2129059},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {582-596},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A mass-shifting phenomenon of truncated multivariate normal priors},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference in heavy-tailed nonstationary multivariate time
series. <em>JASA</em>, <em>119</em>(545), 565–581. (<a
href="https://doi.org/10.1080/01621459.2022.2128807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study inference on the common stochastic trends in a nonstationary, N -variate time series y t , in the possible presence of heavy tails. We propose a novel methodology which does not require any knowledge or estimation of the tail index, or even knowledge as to whether certain moments (such as the variance) exist or not, and develop an estimator of the number of stochastic trends m based on the eigenvalues of the sample second moment matrix of y t . We study the rates of such eigenvalues, showing that the first m ones diverge, as the sample size T passes to infinity, at a rate faster by O ( T ) than the remaining N – m ones, irrespective of the tail index. We thus exploit this eigen-gap by constructing, for each eigenvalue, a test statistic which diverges to positive infinity or drifts to zero according to whether the relevant eigenvalue belongs to the set of the first m eigenvalues or not. We then construct a randomized statistic based on this, using it as part of a sequential testing procedure, ensuring consistency of the resulting estimator of m . We also discuss an estimator of the common trends based on principal components and show that, up to a an invertible linear transformation, such estimator is consistent in the sense that the estimation error is of smaller order than the trend itself. Importantly, we present the case in which we relax the standard assumption of iid innovations, by allowing for heterogeneity of a very general form in the scale of the innovations. Finally, we develop an extension to the large dimensional case. A Monte Carlo study shows that the proposed estimator for m performs particularly well, even in samples of small size. We complete the article by presenting two illustrative applications covering commodity prices and interest rates data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matteo Barigozzi and Giuseppe Cavaliere and Lorenzo Trapani},
  doi          = {10.1080/01621459.2022.2128807},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {565-581},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Inference in heavy-tailed nonstationary multivariate time series},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy optimization using semiparametric models for dynamic
pricing. <em>JASA</em>, <em>119</em>(545), 552–564. (<a
href="https://doi.org/10.1080/01621459.2022.2128359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the contextual dynamic pricing problem where the market value of a product is linear in its observed features plus some market noise. Products are sold one at a time, and only a binary response indicating success or failure of a sale is observed. Our model setting is similar to the work by? except that we expand the demand curve to a semiparametric model and learn dynamically both parametric and nonparametric components. We propose a dynamic statistical learning and decision making policy that minimizes regret (maximizes revenue) by combining semiparametric estimation for a generalized linear model with unknown link and online decision making. Under mild conditions, for a market noise cdf F ( · ) with m th order derivative ( m ≥ 2 ), our policy achieves a regret upper bound of O ˜ d ( T 2 m + 1 4 m − 1 ) , where T is the time horizon and O ˜ d is the order hiding logarithmic terms and the feature dimension d . The upper bound is further reduced to O ˜ d ( T ) if F is super smooth. These upper bounds are close to Ω ( T ) , the lower bound where F belongs to a parametric class. We further generalize these results to the case with dynamic dependent product features under the strong mixing condition. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jianqing Fan and Yongyi Guo and Mengxin Yu},
  doi          = {10.1080/01621459.2022.2128359},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {552-564},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Policy optimization using semiparametric models for dynamic pricing},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution of distances based object matching: Asymptotic
inference. <em>JASA</em>, <em>119</em>(545), 538–551. (<a
href="https://doi.org/10.1080/01621459.2022.2127360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we aim to provide a statistical theory for object matching based on a lower bound of the Gromov-Wasserstein distance related to the distribution of (pairwise) distances of the considered objects. To this end, we model general objects as metric measure spaces. Based on this, we propose a simple and efficiently computable asymptotic statistical test for pose invariant object discrimination. This is based on a ( β -trimmed) empirical version of the afore-mentioned lower bound. We derive the distributional limits of this test statistic for the trimmed and untrimmed case. For this purpose, we introduce a novel U -type process indexed in β and show its weak convergence. The theory developed is investigated in Monte Carlo simulations and applied to structural protein comparisons. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Christoph Alexander Weitkamp and Katharina Proksch and Carla Tameling and Axel Munk},
  doi          = {10.1080/01621459.2022.2127360},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {538-551},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distribution of distances based object matching: Asymptotic inference},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large scale prediction with decision trees. <em>JASA</em>,
<em>119</em>(545), 525–537. (<a
href="https://doi.org/10.1080/01621459.2022.2126782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article shows that decision trees constructed with Classification and Regression Trees (CART) and C4.5 methodology are consistent for regression and classification tasks, even when the number of predictor variables grows sub-exponentially with the sample size, under natural 0-norm and 1-norm sparsity constraints. The theory applies to a wide range of models, including (ordinary or logistic) additive regression models with component functions that are continuous, of bounded variation, or, more generally, Borel measurable. Consistency holds for arbitrary joint distributions of the predictor variables, thereby accommodating continuous, discrete, and/or dependent data. Finally, we show that these qualitative properties of individual trees are inherited by Breiman’s random forests. A key step in the analysis is the establishment of an oracle inequality, which allows for a precise characterization of the goodness of fit and complexity tradeoff for a mis-specified model. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jason M. Klusowski and Peter M. Tian},
  doi          = {10.1080/01621459.2022.2126782},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {525-537},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Large scale prediction with decision trees},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fisher’s combined probability test for high-dimensional
covariance matrices. <em>JASA</em>, <em>119</em>(545), 511–524. (<a
href="https://doi.org/10.1080/01621459.2022.2126781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing large covariance matrices is of fundamental importance in statistical analysis with high-dimensional data. In the past decade, three types of test statistics have been studied in the literature: quadratic form statistics, maximum form statistics, and their weighted combination. It is known that quadratic form statistics would suffer from low power against sparse alternatives and maximum form statistics would suffer from low power against dense alternatives. The weighted combination methods were introduced to enhance the power of quadratic form statistics or maximum form statistics when the weights are appropriately chosen. In this article, we provide a new perspective to exploit the full potential of quadratic form statistics and maximum form statistics for testing high-dimensional covariance matrices. We propose a scale-invariant power-enhanced test based on Fisher’s method to combine the p -values of quadratic form statistics and maximum form statistics. After carefully studying the asymptotic joint distribution of quadratic form statistics and maximum form statistics, we first prove that the proposed combination method retains the correct asymptotic size under the Gaussian assumption, and we also derive a new Lyapunov-type bound for the joint distribution and prove the correct asymptotic size of the proposed method without requiring the Gaussian assumption. Moreover, we show that the proposed method boosts the asymptotic power against more general alternatives. Finally, we demonstrate the finite-sample performance in simulation studies and a real application. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiufan Yu and Danning Li and Lingzhou Xue},
  doi          = {10.1080/01621459.2022.2126781},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {511-524},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fisher’s combined probability test for high-dimensional covariance matrices},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An algebraic estimator for large spectral density matrices.
<em>JASA</em>, <em>119</em>(545), 498–510. (<a
href="https://doi.org/10.1080/01621459.2022.2126780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new estimator of high-dimensional spectral density matrices, called ALgebraic Spectral Estimator (ALSE), under the assumption of an underlying low rank plus sparse structure, as typically assumed in dynamic factor models. The ALSE is computed by minimizing a quadratic loss under a nuclear norm plus l 1 norm constraint to control the latent rank and the residual sparsity pattern. The loss function requires as input the classical smoothed periodogram estimator and two threshold parameters, the choice of which is thoroughly discussed. We prove consistency of ALSE as both the dimension p and the sample size T diverge to infinity, as well as the recovery of latent rank and residual sparsity pattern with probability one. We then propose the UNshrunk ALgebraic Spectral Estimator (UNALSE), which is designed to minimize the Frobenius loss with respect to the pre-estimator while retaining the optimality of the ALSE. When applying UNALSE to a standard U.S. quarterly macroeconomic dataset, we find evidence of two main sources of comovements: a real factor driving the economy at business cycle frequencies, and a nominal factor driving the higher frequency dynamics. The article is also complemented by an extensive simulation exercise. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matteo Barigozzi and Matteo Farnè},
  doi          = {10.1080/01621459.2022.2126780},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {498-510},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An algebraic estimator for large spectral density matrices},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning coefficient heterogeneity over networks: A
distributed spanning-tree-based fused-lasso regression. <em>JASA</em>,
<em>119</em>(545), 485–497. (<a
href="https://doi.org/10.1080/01621459.2022.2126363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the latent cluster structure based on model heterogeneity is a fundamental but challenging task arises in many machine learning applications. In this article, we study the clustered coefficient regression problem in the distributed network systems, where the data are locally collected and held by nodes. Our work aims to improve the regression estimation efficiency by aggregating the neighbors’ information while also identifying the cluster membership for nodes. To achieve efficient estimation and clustering, we develop a distributed spanning-tree-based fused-lasso regression (DTFLR) approach. In particular, we propose an adaptive spanning-tree-based fusion penalty for the low-complexity clustered coefficient regression. We show that our proposed estimator satisfies statistical oracle properties. Additionally, to solve the problem parallelly, we design a distributed generalized alternating direction method of multiplier algorithm, which has a simple node-based implementation scheme and enjoys a linear convergence rate. Collectively, our results in this article contribute to the theories of low-complexity clustered coefficient regression and distributed optimization over networks. Thorough numerical experiments and real-world data analysis are conducted to verify our theoretical results, which show that our approach outperforms existing works in terms of estimation accuracy, computation speed, and communication costs. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xin Zhang and Jia Liu and Zhengyuan Zhu},
  doi          = {10.1080/01621459.2022.2126363},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {485-497},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Learning coefficient heterogeneity over networks: A distributed spanning-tree-based fused-lasso regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assumption-lean cox regression. <em>JASA</em>,
<em>119</em>(545), 475–484. (<a
href="https://doi.org/10.1080/01621459.2022.2126362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference for the conditional association between an exposure and a time-to-event endpoint, given covariates, is routinely based on partial likelihood estimators for hazard ratios indexing Cox proportional hazards models. This approach is flexible and makes testing straightforward, but is nonetheless not entirely satisfactory. First, there is no good understanding of what it infers when the model is misspecified. Second, it is common to employ variable selection procedures when deciding which model to use. However, the bias and uncertainty that imperfect variable selection adds to the analysis is rarely acknowledged, rendering standard inferences biased and overly optimistic. To remedy this, we propose a nonparametric estimand which reduces to the main exposure effect parameter in a (partially linear) Cox model when that model is correct, but continues to capture the (conditional) association of interest in a well understood way, even when this model is misspecified in an arbitrary manner. We achieve an assumption-lean inference for this estimand based on its influence function under the nonparametric model. This has the further advantage that it makes the proposed approach amenable to the use of data-adaptive procedures (e.g., variable selection, machine learning), which we find to work well in simulation studies and a data analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Stijn Vansteelandt and Oliver Dukes and Kelly Van Lancker and Torben Martinussen},
  doi          = {10.1080/01621459.2022.2126362},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {475-484},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Assumption-lean cox regression},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection for a large number of streams: A
permutation-based higher criticism approach. <em>JASA</em>,
<em>119</em>(545), 461–474. (<a
href="https://doi.org/10.1080/01621459.2022.2126361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection when observing a large number of data streams is essential in a variety of applications, ranging from epidemiological studies to monitoring of complex systems. High-dimensional scenarios are usually tackled with scan-statistics and related methods, requiring stringent modeling assumptions for proper calibration. In this work we take a nonparametric stance, and propose a permutation-based variant of the higher criticism statistic not requiring knowledge of the null distribution. This results in an exact test in finite samples which is asymptotically optimal in the wide class of exponential models. We demonstrate the power loss in finite samples is minimal with respect to the oracle test. Furthermore, since the proposed statistic does not rely on asymptotic approximations it typically performs better than popular variants of higher criticism that rely on such approximations. We include recommendations such that the test can be readily applied in practice, and demonstrate its applicability in monitoring the content uniformity of an active ingredient for a batch-produced drug product. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ivo V. Stoepker and Rui M. Castro and Ery Arias-Castro and Edwin van den Heuvel},
  doi          = {10.1080/01621459.2022.2126361},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {461-474},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Anomaly detection for a large number of streams: A permutation-based higher criticism approach},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To adjust or not to adjust? Estimating the average treatment
effect in randomized experiments with missing covariates. <em>JASA</em>,
<em>119</em>(545), 450–460. (<a
href="https://doi.org/10.1080/01621459.2022.2123814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized experiments allow for consistent estimation of the average treatment effect based on the difference in mean outcomes without strong modeling assumptions. Appropriate use of pretreatment covariates can further improve the estimation efficiency. Missingness in covariates is nevertheless common in practice, and raises an important question: should we adjust for covariates subject to missingness, and if so, how? The unadjusted difference in means is always unbiased. The complete-covariate analysis adjusts for all completely observed covariates, and is asymptotically more efficient than the difference in means if at least one completely observed covariate is predictive of the outcome. Then what is the additional gain of adjusting for covariates subject to missingness? To reconcile the conflicting recommendations in the literature, we analyze and compare five strategies for handling missing covariates in randomized experiments under the design-based framework, and recommend the missingness-indicator method, as a known but not so popular strategy in the literature, due to its multiple advantages. First, it removes the dependence of the regression-adjusted estimators on the imputed values for the missing covariates. Second, it does not require modeling the missingness mechanism, and yields consistent estimators even when the missingness mechanism is related to the missing covariates and unobservable potential outcomes. Third, it ensures large-sample efficiency over the complete-covariate analysis and the analysis based on only the imputed covariates. Lastly, it is easy to implement via least squares. We also propose modifications to it based on asymptotic and finite sample considerations. Importantly, our theory views randomization as the basis for inference, and does not impose any modeling assumptions on the data-generating process or missingness mechanism. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Anqi Zhao and Peng Ding},
  doi          = {10.1080/01621459.2022.2123814},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {450-460},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {To adjust or not to adjust? estimating the average treatment effect in randomized experiments with missing covariates},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using SVD for topic modeling. <em>JASA</em>,
<em>119</em>(545), 434–449. (<a
href="https://doi.org/10.1080/01621459.2022.2123813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probabilistic topic model imposes a low-rank structure on the expectation of the corpus matrix. Therefore, singular value decomposition (SVD) is a natural tool of dimension reduction. We propose an SVD-based method for estimating a topic model. Our method constructs an estimate of the topic matrix from only a few leading singular vectors of the data matrix, and has a great advantage in memory use and computational cost for large-scale corpora. The core ideas behind our method include a pre-SVD normalization to tackle severe word frequency heterogeneity, a post-SVD normalization to create a low-dimensional word embedding that manifests a simplex geometry, and a post-SVD procedure to construct an estimate of the topic matrix directly from the embedded word cloud. We provide the explicit rate of convergence of our method. We show that our method attains the optimal rate in the case of long and moderately long documents, and it improves the rates of existing methods in the case of short documents. The key of our analysis is a sharp row-wise large-deviation bound for empirical singular vectors, which is technically demanding to derive and potentially useful for other problems. We apply our method to a corpus of Associated Press news articles and a corpus of abstracts of statistical papers. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zheng Tracy Ke and Minzhe Wang},
  doi          = {10.1080/01621459.2022.2123813},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {434-449},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Using SVD for topic modeling},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian spatial blind source separation via the thresholded
gaussian process. <em>JASA</em>, <em>119</em>(545), 422–433. (<a
href="https://doi.org/10.1080/01621459.2022.2123336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind source separation (BSS) aims to separate latent source signals from their mixtures. For spatially dependent signals in high-dimensional and large-scale data, such as neuroimaging, most existing BSS methods do not take into account the spatial dependence and the sparsity of the latent source signals. To address these major limitations, we propose a Bayesian spatial blind source separation (BSP-BSS) approach for neuroimaging data analysis. We assume the expectation of the observed images as a linear mixture of multiple sparse and piece-wise smooth latent source signals, for which we construct a new class of Bayesian nonparametric prior models by thresholding Gaussian processes. We assign the vMF priors to mixing coefficients in the model. Under some regularity conditions, we show that the proposed method has several desirable theoretical properties including the large support for the priors, the consistency of joint posterior distribution of the latent source intensity functions and the mixing coefficients, and the selection consistency on the number of latent sources. We use extensive simulation studies and an analysis of the resting-state fMRI data in the Autism Brain Imaging Data Exchange (ABIDE) study to demonstrate that BSP-BSS outperforms the existing method for separating latent brain networks and detecting activated brain activation in the latent sources. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ben Wu and Ying Guo and Jian Kang},
  doi          = {10.1080/01621459.2022.2123336},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {422-433},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian spatial blind source separation via the thresholded gaussian process},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and active learning for experiments with
quantitative-sequence factors. <em>JASA</em>, <em>119</em>(545),
407–421. (<a
href="https://doi.org/10.1080/01621459.2022.2123335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new type of experiment that aims to determine the optimal quantities of a sequence of factors is eliciting considerable attention in medical science, bioengineering, and many other disciplines. Such studies require the simultaneous optimization of both quantities and sequence orders of several components which are called quantitative-sequence (QS) factors. Given the large and semi-discrete solution spaces in such experiments, efficiently identifying optimal or near-optimal solutions by using a small number of experimental trials is a nontrivial task. To address this challenge, we propose a novel active learning approach, called QS-learning, to enable effective modeling and efficient optimization for experiments with QS factors. QS-learning consists of three parts: a novel mapping-based additive Gaussian process (MaGP) model, an efficient global optimization scheme (QS-EGO), and a new class of optimal designs (QS-design). The theoretical properties of the proposed method are investigated, and optimization techniques using analytical gradients are developed. The performance of the proposed method is demonstrated via a real drug experiment on lymphoma treatment and several simulation studies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qian Xiao and Yaping Wang and Abhyuday Mandal and Xinwei Deng},
  doi          = {10.1080/01621459.2022.2123335},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {407-421},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling and active learning for experiments with quantitative-sequence factors},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fisher-pitman permutation tests based on nonparametric
poisson mixtures with application to single cell genomics.
<em>JASA</em>, <em>119</em>(545), 394–406. (<a
href="https://doi.org/10.1080/01621459.2022.2120401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the theoretical and empirical performance of Fisher-Pitman-type permutation tests for assessing the equality of unknown Poisson mixture distributions. Building on nonparametric maximum likelihood estimators (NPMLEs) of the mixing distribution, these tests are theoretically shown to be able to adapt to complicated unspecified structures of count data and also consistent against their corresponding ANOVA-type alternatives; the latter is a result in parallel to classic claims made by Robinson. The studied methods are then applied to a single-cell RNA-seq data obtained from different cell types from brain samples of autism subjects and healthy controls; empirically, they unveil genes that are differentially expressed between autism and control subjects yet are missed using common tests. For justifying their use, rate optimality of NPMLEs is also established in settings similar to nonparametric Gaussian (Wu and Yang) and binomial mixtures (Tian, Kong, and Valiant; Vinayak et al.). Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhen Miao and Weihao Kong and Ramya Korlakai Vinayak and Wei Sun and Fang Han},
  doi          = {10.1080/01621459.2022.2120401},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {394-406},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fisher-pitman permutation tests based on nonparametric poisson mixtures with application to single cell genomics},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrapping extreme value estimators. <em>JASA</em>,
<em>119</em>(545), 382–393. (<a
href="https://doi.org/10.1080/01621459.2022.2120400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a bootstrap analogue of the well-known asymptotic expansion of the tail quantile process in extreme value theory. One application of this result is to construct confidence intervals for estimators of the extreme value index such as the Probability Weighted Moment (PWM) estimator. For the peaks-over-threshold method, we show the bootstrap consistency of the confidence intervals. By contrast, the asymptotic expansion of the quantile process of the bootstrapped block maxima does not lead to a similar consistency result for the PWM estimator using the block maxima method. For both methods, We show by simulations that the sample variance of bootstrapped estimates can be a good approximation for the asymptotic variance of the original estimator. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Laurens de Haan and Chen Zhou},
  doi          = {10.1080/01621459.2022.2120400},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {382-393},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bootstrapping extreme value estimators},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An additive graphical model for discrete data.
<em>JASA</em>, <em>119</em>(545), 368–381. (<a
href="https://doi.org/10.1080/01621459.2022.2119983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a nonparametric graphical model for discrete node variables based on additive conditional independence. Additive conditional independence is a three-way statistical relation that shares similar properties with conditional independence by satisfying the semi-graphoid axioms. Based on this relation we build an additive graphical model for discrete variables that does not suffer from the restriction of a parametric model such as the Ising model. We develop an estimator of the new graphical model via the penalized estimation of the discrete version of the additive precision operator and establish the consistency of the estimator under the ultrahigh-dimensional setting. Along with these methodological developments, we also exploit the properties of discrete random variables to uncover a deeper relation between additive conditional independence and conditional independence than previously known. The new graphical model reduces to a conditional independence graphical model under certain sparsity conditions. We conduct simulation experiments and analysis of an HIV antiretroviral therapy dataset to compare the new method with existing ones. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jun Tao and Bing Li and Lingzhou Xue},
  doi          = {10.1080/01621459.2022.2119983},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {368-381},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An additive graphical model for discrete data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and numerically stable particle-based online additive
smoothing: The AdaSmooth algorithm. <em>JASA</em>, <em>119</em>(545),
356–367. (<a
href="https://doi.org/10.1080/01621459.2022.2118602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel sequential Monte Carlo approach to online smoothing of additive functionals in a very general class of path-space models. Hitherto, the solutions proposed in the literature suffer from either long-term numerical instability due to particle-path degeneracy or, in the case that degeneracy is remedied by particle approximation of the so-called backward kernel, high computational demands. In order to balance optimally computational speed against numerical stability, we propose to furnish a (fast) naive particle smoother, propagating recursively a sample of particles and associated smoothing statistics, with an adaptive backward-sampling-based updating rule which allows the number of (costly) backward samples to be kept at a minimum. This yields a new, function-specific additive smoothing algorithm, AdaSmooth, which is computationally fast, numerically stable and easy to implement. The algorithm is provided with rigorous theoretical results guaranteeing its consistency, asymptotic normality and long-term stability as well as numerical results demonstrating empirically the clear superiority of AdaSmooth to existing algorithms. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Alessandro Mastrototaro and Jimmy Olsson and Johan Alenlöv},
  doi          = {10.1080/01621459.2022.2118602},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {356-367},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast and numerically stable particle-based online additive smoothing: The AdaSmooth algorithm},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subspace estimation with automatic dimension and variable
selection in sufficient dimension reduction. <em>JASA</em>,
<em>119</em>(545), 343–355. (<a
href="https://doi.org/10.1080/01621459.2022.2118601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) methods target finding lower-dimensional representations of a multivariate predictor to preserve all the information about the conditional distribution of the response given the predictor. The reduction is commonly achieved by projecting the predictor onto a low-dimensional subspace. The smallest such subspace is known as the Central Subspace (CS) and is the key parameter of interest for most SDR methods. In this article, we propose a unified and flexible framework for estimating the CS in high dimensions. Our approach generalizes a wide range of model-based and model-free SDR methods to high-dimensional settings, where the CS is assumed to involve only a subset of the predictors. We formulate the problem as a quadratic convex optimization so that the global solution is feasible. The proposed estimation procedure simultaneously achieves the structural dimension selection and coordinate-independent variable selection of the CS. Theoretically, our method achieves dimension selection, variable selection, and subspace estimation consistency at a high convergence rate under mild conditions. We demonstrate the effectiveness and efficiency of our method with extensive simulation studies and real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jing Zeng and Qing Mai and Xin Zhang},
  doi          = {10.1080/01621459.2022.2118601},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {343-355},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Subspace estimation with automatic dimension and variable selection in sufficient dimension reduction},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective inference for hierarchical clustering.
<em>JASA</em>, <em>119</em>(545), 332–342. (<a
href="https://doi.org/10.1080/01621459.2022.2116331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical tests for a difference in means control the Type I error rate when the groups are defined a priori. However, when the groups are instead defined via clustering, then applying a classical test yields an extremely inflated Type I error rate. Notably, this problem persists even if two separate and independent datasets are used to define the groups and to test for a difference in their means. To address this problem, in this article, we propose a selective inference approach to test for a difference in means between two clusters. Our procedure controls the selective Type I error rate by accounting for the fact that the choice of null hypothesis was made based on the data. We describe how to efficiently compute exact p -values for clusters obtained using agglomerative hierarchical clustering with many commonly used linkages. We apply our method to simulated data and to single-cell RNA-sequencing data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lucy L. Gao and Jacob Bien and Daniela Witten},
  doi          = {10.1080/01621459.2022.2116331},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {332-342},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Selective inference for hierarchical clustering},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scaled process priors for bayesian nonparametric estimation
of the unseen genetic variation. <em>JASA</em>, <em>119</em>(545),
320–331. (<a
href="https://doi.org/10.1080/01621459.2022.2115918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in the estimation of the number of unseen features, mostly driven by biological applications. A recent work brought out a peculiar property of the popular completely random measures (CRMs) as prior models in Bayesian nonparametric (BNP) inference for the unseen-features problem: for fixed prior’s parameters, they all lead to a Poisson posterior distribution for the number of unseen features, which depends on the sampling information only through the sample size. CRMs are thus not a flexible prior model for the unseen-features problem and, while the Poisson posterior distribution may be appealing for analytical tractability and ease of interpretability, its independence from the sampling information makes the BNP approach a questionable oversimplification, with posterior inferences being completely determined by the estimation of unknown prior’s parameters. In this article, we introduce the stable-Beta scaled process (SB-SP) prior, and we show that it allows to enrich the posterior distribution of the number of unseen features arising under CRM priors, while maintaining its analytical tractability and interpretability. That is, the SB-SP prior leads to a negative Binomial posterior distribution, which depends on the sampling information through the sample size and the number of distinct features, with corresponding estimates being simple, linear in the sampling information and computationally efficient. We apply our BNP approach to synthetic data and to real cancer genomic data, showing that: (i) it outperforms the most popular parametric and nonparametric competitors in terms of estimation accuracy; (ii) it provides improved coverage for the estimation with respect to a BNP approach under CRM priors. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Federico Camerlenghi and Stefano Favaro and Lorenzo Masoero and Tamara Broderick},
  doi          = {10.1080/01621459.2022.2115918},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {320-331},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Scaled process priors for bayesian nonparametric estimation of the unseen genetic variation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic principal component analysis in high dimensions.
<em>JASA</em>, <em>119</em>(545), 308–319. (<a
href="https://doi.org/10.1080/01621459.2022.2115917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis is a versatile tool to reduce dimensionality which has wide applications in statistics and machine learning. It is particularly useful for modeling data in high-dimensional scenarios where the number of variables p is comparable to, or much larger than the sample size n . Despite an extensive literature on this topic, researchers have focused on modeling static principal eigenvectors, which are not suitable for stochastic processes that are dynamic in nature. To characterize the change in the entire course of high-dimensional data collection, we propose a unified framework to directly estimate dynamic eigenvectors of covariance matrices. Specifically, we formulate an optimization problem by combining the local linear smoothing and regularization penalty together with the orthogonality constraint, which can be effectively solved by manifold optimization algorithms. We show that our method is suitable for high-dimensional data observed under both common and irregular designs, and theoretical properties of the estimators are investigated under l q ( 0 ≤ q ≤ 1 ) sparsity. Extensive experiments demonstrate the effectiveness of the proposed method in both simulated and real data examples.},
  archive      = {J_JASA},
  author       = {Xiaoyu Hu and Fang Yao},
  doi          = {10.1080/01621459.2022.2115917},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {308-319},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Dynamic principal component analysis in high dimensions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal nonparametric inference with two-scale
distributional nearest neighbors. <em>JASA</em>, <em>119</em>(545),
297–307. (<a
href="https://doi.org/10.1080/01621459.2022.2115375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weighted nearest neighbors (WNN) estimator has been popularly used as a flexible and easy-to-implement nonparametric tool for mean regression estimation. The bagging technique is an elegant way to form WNN estimators with weights automatically generated to the nearest neighbors (Steele Citation 2009 ; Biau, Cérou, and Guyader Citation 2010 ); we name the resulting estimator as the distributional nearest neighbors (DNN) for easy reference. Yet, there is a lack of distributional results for such estimator, limiting its application to statistical inference. Moreover, when the mean regression function has higher-order smoothness, DNN does not achieve the optimal nonparametric convergence rate, mainly because of the bias issue. In this work, we provide an in-depth technical analysis of the DNN, based on which we suggest a bias reduction approach for the DNN estimator by linearly combining two DNN estimators with different subsampling scales, resulting in the novel two-scale DNN (TDNN) estimator. The two-scale DNN estimator has an equivalent representation of WNN with weights admitting explicit forms and some being negative. We prove that, thanks to the use of negative weights, the two-scale DNN estimator enjoys the optimal nonparametric rate of convergence in estimating the regression function under the fourth-order smoothness condition. We further go beyond estimation and establish that the DNN and two-scale DNN are both asymptotically normal as the subsampling scales and sample size diverge to infinity. For the practical implementation, we also provide variance estimators and a distribution estimator using the jackknife and bootstrap techniques for the two-scale DNN. These estimators can be exploited for constructing valid confidence intervals for nonparametric inference of the regression function. The theoretical results and appealing finite-sample performance of the suggested two-scale DNN method are illustrated with several simulation examples and a real data application.},
  archive      = {J_JASA},
  author       = {Emre Demirkaya and Yingying Fan and Lan Gao and Jinchi Lv and Patrick Vossler and Jingbo Wang},
  doi          = {10.1080/01621459.2022.2115375},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {297-307},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal nonparametric inference with two-scale distributional nearest neighbors},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal one-pass nonparametric estimation under memory
constraint. <em>JASA</em>, <em>119</em>(545), 285–296. (<a
href="https://doi.org/10.1080/01621459.2022.2115374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For nonparametric regression in the streaming setting, where data constantly flow in and require real-time analysis, a main challenge is that data are cleared from the computer system once processed due to limited computer memory and storage. We tackle the challenge by proposing a novel one-pass estimator based on penalized orthogonal basis expansions and developing a general framework to study the interplay between statistical efficiency and memory consumption of estimators. We show that, the proposed estimator is statistically optimal under memory constraint, and has asymptotically minimal memory footprints among all one-pass estimators of the same estimation quality. Numerical studies demonstrate that the proposed one-pass estimator is nearly as efficient as its nonstreaming counterpart that has access to all historical data.},
  archive      = {J_JASA},
  author       = {Mingxue Quan and Zhenhua Lin},
  doi          = {10.1080/01621459.2022.2115374},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {285-296},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal one-pass nonparametric estimation under memory constraint},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Off-policy confidence interval estimation with confounded
markov decision process. <em>JASA</em>, <em>119</em>(545), 273–284. (<a
href="https://doi.org/10.1080/01621459.2022.2110878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned with constructing a confidence interval for a target policy’s value offline based on a pre-collected observational data in infinite horizon settings. Most of the existing works assume no unmeasured variables exist that confound the observed actions. This assumption, however, is likely to be violated in real applications such as healthcare and technological industries. In this article, we show that with some auxiliary variables that mediate the effect of actions on the system dynamics, the target policy’s value is identifiable in a confounded Markov decision process. Based on this result, we develop an efficient off-policy value estimator that is robust to potential model misspecification and provide rigorous uncertainty quantification. Our method is justified by theoretical results, simulated and real datasets obtained from ridesharing companies. A Python implementation of the proposed procedure is available at https://github.com/Mamba413/cope .},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Jin Zhu and Ye Shen and Shikai Luo and Hongtu Zhu and Rui Song},
  doi          = {10.1080/01621459.2022.2110878},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {273-284},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Off-policy confidence interval estimation with confounded markov decision process},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical bayes approach to shrinkage estimation on the
manifold of symmetric positive-definite matrices. <em>JASA</em>,
<em>119</em>(545), 259–272. (<a
href="https://doi.org/10.1080/01621459.2022.2110877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The James–Stein estimator is an estimator of the multivariate normal mean and dominates the maximum likelihood estimator (MLE) under squared error loss. The original work inspired great interest in developing shrinkage estimators for a variety of problems. Nonetheless, research on shrinkage estimation for manifold-valued data is scarce. In this article, we propose shrinkage estimators for the parameters of the Log-Normal distribution defined on the manifold of N × N symmetric positive-definite matrices. For this manifold, we choose the Log-Euclidean metric as its Riemannian metric since it is easy to compute and has been widely used in a variety of applications. By using the Log-Euclidean distance in the loss function, we derive a shrinkage estimator in an analytic form and show that it is asymptotically optimal within a large class of estimators that includes the MLE, which is the sample Fréchet mean of the data. We demonstrate the performance of the proposed shrinkage estimator via several simulated data experiments. Additionally, we apply the shrinkage estimator to perform statistical inference in both diffusion and functional magnetic resonance imaging problems. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chun-Hao Yang and Hani Doss and Baba C. Vemuri},
  doi          = {10.1080/01621459.2022.2110877},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {259-272},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An empirical bayes approach to shrinkage estimation on the manifold of symmetric positive-definite matrices},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nearly dimension-independent sparse linear bandit over small
action spaces via best subset selection. <em>JASA</em>,
<em>119</em>(545), 246–258. (<a
href="https://doi.org/10.1080/01621459.2022.2108816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the stochastic contextual bandit problem under the high dimensional linear model. We focus on the case where the action space is finite and random, with each action associated with a randomly generated contextual covariate. This setting finds essential applications such as personalized recommendations, online advertisements, and personalized medicine. However, it is very challenging to balance the exploration and exploitation tradeoff. We modify the L in UCB algorithm in doubly growing epochs and estimate the parameter using the best subset selection method, which is easy to implement in practice. This approach achieves O ( s T ) regret with high probability, which is nearly independent of the “ambient” regression model dimension d . We further attain a sharper O ( s T ) regret by using the S up L in UCB framework and match the minimax lower bound of the low-dimensional linear stochastic bandit problem. Finally, we conduct extensive numerical experiments to empirically demonstrate our algorithms’ applicability and robustness. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yi Chen and Yining Wang and Ethan X. Fang and Zhaoran Wang and Runze Li},
  doi          = {10.1080/01621459.2022.2108816},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {246-258},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nearly dimension-independent sparse linear bandit over small action spaces via best subset selection},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistically efficient advantage learning for offline
reinforcement learning in infinite horizons. <em>JASA</em>,
<em>119</em>(545), 232–245. (<a
href="https://doi.org/10.1080/01621459.2022.2106868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider reinforcement learning (RL) methods in offline domains without additional online data collection, such as mobile health applications. Most of existing policy optimization algorithms in the computer science literature are developed in online settings where data are easy to collect or simulate. Their generalizations to mobile health applications with a pre-collected offline dataset remain are less explored. The aim of this article is to develop a novel advantage learning framework in order to efficiently use pre-collected data for policy optimization. The proposed method takes an optimal Q-estimator computed by any existing state-of-the-art RL algorithms as input, and outputs a new policy whose value is guaranteed to converge at a faster rate than the policy derived based on the initial Q-estimator. Extensive numerical experiments are conducted to back up our theoretical findings. A Python implementation of our proposed method is available at https://github.com/leyuanheart/SEAL . Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Shikai Luo and Yuan Le and Hongtu Zhu and Rui Song},
  doi          = {10.1080/01621459.2022.2106868},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {232-245},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistically efficient advantage learning for offline reinforcement learning in infinite horizons},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bias-correction and test for mark-point dependence with
replicated marked point processes. <em>JASA</em>, <em>119</em>(545),
217–231. (<a
href="https://doi.org/10.1080/01621459.2022.2106234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mark-point dependence plays a critical role in research problems that can be fitted into the general framework of marked point processes. In this work, we focus on adjusting for mark-point dependence when estimating the mean and covariance functions of the mark process, given independent replicates of the marked point process. We assume that the mark process is a Gaussian process and the point process is a log-Gaussian Cox process, where the mark-point dependence is generated through the dependence between two latent Gaussian processes. Under this framework, naive local linear estimators ignoring the mark-point dependence can be severely biased. We show that this bias can be corrected using a local linear estimator of the cross-covariance function and establish uniform convergence rates of the bias-corrected estimators. Furthermore, we propose a test statistic based on local linear estimators for mark-point independence, which is shown to converge to an asymptotic normal distribution in a parametric n -convergence rate. Model diagnostics tools are developed for key model assumptions and a robust functional permutation test is proposed for a more general class of mark-point processes. The effectiveness of the proposed methods is demonstrated using extensive simulations and applications to two real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ganggang Xu and Jingfei Zhang and Yehua Li and Yongtao Guan},
  doi          = {10.1080/01621459.2022.2106234},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {217-231},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bias-correction and test for mark-point dependence with replicated marked point processes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-rank regression models for multiple binary responses and
their applications to cancer cell-line encyclopedia data. <em>JASA</em>,
<em>119</em>(545), 202–216. (<a
href="https://doi.org/10.1080/01621459.2022.2105704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study high-dimensional multivariate logistic regression models in which a common set of covariates is used to predict multiple binary outcomes simultaneously. Our work is primarily motivated from many biomedical studies with correlated multiple responses such as the cancer cell-line encyclopedia project. We assume that the underlying regression coefficient matrix is simultaneously low-rank and row-wise sparse. We propose an intuitively appealing selection and estimation framework based on marginal model likelihood, and we develop an efficient computational algorithm for inference. We establish a novel high-dimensional theory for this nonlinear multivariate regression. Our theory is general, allowing for potential correlations between the binary responses. We propose a new type of nuclear norm penalty using the smooth clipped absolute deviation, filling the gap in the related non-convex penalization literature. We theoretically demonstrate that the proposed approach improves estimation accuracy by considering multiple responses jointly through the proposed estimator when the underlying coefficient matrix is low-rank and row-wise sparse. In particular, we establish the non-asymptotic error bounds, and both rank and row support consistency of the proposed method. Moreover, we develop a consistent rule to simultaneously select the rank and row dimension of the coefficient matrix. Furthermore, we extend the proposed methods and theory to a joint Ising model, which accounts for the dependence relationships. In our analysis of both simulated data and the cancer cell line encyclopedia data, the proposed methods outperform the existing methods in better predicting responses. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Seyoung Park and Eun Ryung Lee and Hongyu Zhao},
  doi          = {10.1080/01621459.2022.2105704},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {202-216},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Low-rank regression models for multiple binary responses and their applications to cancer cell-line encyclopedia data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hidden markov pólya trees for high-dimensional
distributions. <em>JASA</em>, <em>119</em>(545), 189–201. (<a
href="https://doi.org/10.1080/01621459.2022.2105223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Pólya tree (PT) process is a general-purpose Bayesian nonparametric model that has found wide application in a range of inference problems. It has a simple analytic form and the posterior computation boils down to beta-binomial conjugate updates along a partition tree over the sample space. Recent development in PT models shows that performance of these models can be substantially improved by (i) allowing the partition tree to adapt to the structure of the underlying distributions and (ii) incorporating latent state variables that characterize local features of the underlying distributions. However, important limitations of the PT remain, including (i) the sensitivity in the posterior inference with respect to the choice of the partition tree, and (ii) the lack of scalability with respect to dimensionality of the sample space. We consider a modeling strategy for PT models that incorporates a flexible prior on the partition tree along with latent states with Markov dependency. We introduce a hybrid algorithm combining sequential Monte Carlo (SMC) and recursive message passing for posterior sampling that can scale up to 100 dimensions. While our description of the algorithm assumes a single computer environment, it has the potential to be implemented on distributed systems to further enhance the scalability. Moreover, we investigate the large sample properties of the tree structures and latent states under the posterior model. We carry out extensive numerical experiments in density estimation and two-group comparison, which show that flexible partitioning can substantially improve the performance of PT models in both inference tasks. We demonstrate an application to a mass cytometry dataset with 19 dimensions and over 200,000 observations. Supplementary Materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Naoki Awaya and Li Ma},
  doi          = {10.1080/01621459.2022.2105223},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {189-201},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hidden markov pólya trees for high-dimensional distributions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric estimation of repeated densities with
heterogeneous sample sizes. <em>JASA</em>, <em>119</em>(545), 176–188.
(<a href="https://doi.org/10.1080/01621459.2022.2104728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation of densities in multiple subpopulations, where the available sample size in each subpopulation greatly varies. This problem occurs in epidemiology, for example, where different diseases may share similar pathogenic mechanism but differ in their prevalence. Without specifying a parametric form, our proposed method pools information from the population and estimate the density in each subpopulation in a data-driven fashion. Drawing from functional data analysis, low-dimensional approximating density families in the form of exponential families are constructed from the principal modes of variation in the log-densities. Subpopulation densities are subsequently fitted in the approximating families based on likelihood principles and shrinkage. The approximating families increase in their flexibility as the number of components increases and can approximate arbitrary infinite-dimensional densities. We also derive convergence results of the density estimates formed with discrete observations. The proposed methods are shown to be interpretable and efficient in simulation studies as well as applications to electronic medical record and rainfall data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jiaming Qiu and Xiongtao Dai and Zhengyuan Zhu},
  doi          = {10.1080/01621459.2022.2104728},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {176-188},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric estimation of repeated densities with heterogeneous sample sizes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heavy-tailed density estimation. <em>JASA</em>,
<em>119</em>(545), 163–175. (<a
href="https://doi.org/10.1080/01621459.2022.2104727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel statistical method is proposed and investigated for estimating a heavy tailed density under mild smoothness assumptions. Statistical analyses of heavy-tailed distributions are susceptible to the problem of sparse information in the tail of the distribution getting washed away by unrelated features of a hefty bulk. The proposed Bayesian method avoids this problem by incorporating smoothness and tail regularization through a carefully specified semiparametric prior distribution, and is able to consistently estimate both the density function and its tail index at near minimax optimal rates of contraction. A joint, likelihood driven estimation of the bulk and the tail is shown to help improve uncertainty assessment in estimating the tail index parameter and offer more accurate and reliable estimates of the high tail quantiles compared to thresholding methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Surya T. Tokdar and Sheng Jiang and Erika L. Cunningham},
  doi          = {10.1080/01621459.2022.2104727},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {163-175},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Heavy-tailed density estimation},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new and unified family of covariate adaptive randomization
procedures and their properties. <em>JASA</em>, <em>119</em>(545),
151–162. (<a
href="https://doi.org/10.1080/01621459.2022.2102986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials and other comparative studies, covariate balance is crucial for credible and efficient assessment of treatment effects. Covariate adaptive randomization (CAR) procedures are extensively used to reduce the likelihood of covariate imbalances occurring. In the literature, most studies have focused on balancing of discrete covariates. Applications of CAR with continuous covariates remain rare, especially when the interest goes beyond balancing only the first moment. In this article, we propose a family of CAR procedures that can balance general covariate features, such as quadratic and interaction terms. Our framework not only unifies many existing methods, but also introduces a much broader class of new and useful CAR procedures. We show that the proposed procedures have superior balancing properties; in particular, the convergence rate of imbalance vectors is O P ( n ϵ ) for any ϵ &gt; 0 if all of the moments are finite for the covariate features, relative to O P ( n ) under complete randomization, where n is the sample size. Both the resulting convergence rate and its proof are novel. These favorable balancing properties lead to increased precision of treatment effect estimation in the presence of nonlinear covariate effects. The framework is applied to balance covariate means and covariance matrices simultaneously. Simulation and empirical studies demonstrate the excellent and robust performance of the proposed procedures. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wei Ma and Ping Li and Li-Xin Zhang and Feifang Hu},
  doi          = {10.1080/01621459.2022.2102986},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {151-162},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A new and unified family of covariate adaptive randomization procedures and their properties},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomization-based joint central limit theorem and
efficient covariate adjustment in randomized block 2K factorial
experiments. <em>JASA</em>, <em>119</em>(545), 136–150. (<a
href="https://doi.org/10.1080/01621459.2022.2102985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized block factorial experiments are widely used in industrial engineering, clinical trials, and social science. Researchers often use a linear model and analysis of covariance to analyze experimental results; however, limited studies have addressed the validity and robustness of the resulting inferences because assumptions for a linear model might not be justified by randomization in randomized block factorial experiments. In this article, we establish a new finite population joint central limit theorem for usual (unadjusted) factorial effect estimators in randomized block 2 K factorial experiments. Our theorem is obtained under a randomization-based inference framework, making use of an extension of the vector form of the Wald–Wolfowitz–Hoeffding theorem for a linear rank statistic. It is robust to model misspecification, numbers of blocks, block sizes, and propensity scores across blocks. To improve the estimation and inference efficiency, we propose four covariate adjustment methods. We show that under mild conditions, the resulting covariate-adjusted factorial effect estimators are consistent, jointly asymptotically normal, and generally more efficient than the unadjusted estimator. In addition, we propose Neyman-type conservative estimators for the asymptotic covariances to facilitate valid inferences. Simulation studies and a clinical trial data analysis demonstrate the benefits of the covariate adjustment methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hanzhong Liu and Jiyang Ren and Yuehan Yang},
  doi          = {10.1080/01621459.2022.2102985},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {136-150},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Randomization-based joint central limit theorem and efficient covariate adjustment in randomized block 2K factorial experiments},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal sensitivity analysis for individual treatment
effects. <em>JASA</em>, <em>119</em>(545), 122–135. (<a
href="https://doi.org/10.1080/01621459.2022.2102503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating an individual treatment effect (ITE) is essential to personalized decision making. However, existing methods for estimating the ITE often rely on unconfoundedness, an assumption that is fundamentally untestable with observed data. To assess the robustness of individual-level causal conclusion with unconfoundedness, this article proposes a method for sensitivity analysis of the ITE, a way to estimate a range of the ITE under unobserved confounding. The method we develop quantifies unmeasured confounding through a marginal sensitivity model, and adapts the framework of conformal inference to estimate an ITE interval at a given confounding strength. In particular, we formulate this sensitivity analysis as a conformal inference problem under distribution shift, and we extend existing methods of covariate-shifted conformal inference to this more general setting. The resulting predictive interval has guaranteed nominal coverage of the ITE and provides this coverage with distribution-free and nonasymptotic guarantees. We evaluate the method on synthetic data and illustrate its application in an observational study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Mingzhang Yin and Claudia Shi and Yixin Wang and David M. Blei},
  doi          = {10.1080/01621459.2022.2102503},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {122-135},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Conformal sensitivity analysis for individual treatment effects},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian markov-switching tensor regression for time-varying
networks. <em>JASA</em>, <em>119</em>(545), 109–121. (<a
href="https://doi.org/10.1080/01621459.2022.2102502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling time series of multilayer network data is challenging due to the peculiar characteristics of real-world networks, such as sparsity and abrupt structural changes. Moreover, the impact of external factors on the network edges is highly heterogeneous due to edge- and time-specific effects. Capturing all these features results in a very high-dimensional inference problem. A novel tensor-on-tensor regression model is proposed, which integrates zero-inflated logistic regression to deal with the sparsity, and Markov-switching coefficients to account for structural changes. A tensor representation and decomposition of the regression coefficients are used to tackle the high-dimensionality and account for the heterogeneous impact of the covariate tensor across the response variables. The inference is performed following a Bayesian approach, and an efficient Gibbs sampler is developed for posterior approximation. Our methodology applied to financial and email networks detects different connectivity regimes and uncovers the role of covariates in the edge-formation process, which are relevant in risk and resource management. Code is available on GitHub. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Monica Billio and Roberto Casarin and Matteo Iacopini},
  doi          = {10.1080/01621459.2022.2102502},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {109-121},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian markov-switching tensor regression for time-varying networks},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent network structure learning from high-dimensional
multivariate point processes. <em>JASA</em>, <em>119</em>(545), 95–108.
(<a href="https://doi.org/10.1080/01621459.2022.2102019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the latent network structure from large scale multivariate point process data is an important task in a wide range of scientific and business applications. For instance, we might wish to estimate the neuronal functional connectivity network based on spiking times recorded from a collection of neurons. To characterize the complex processes underlying the observed data, we propose a new and flexible class of nonstationary Hawkes processes that allow both excitatory and inhibitory effects. We estimate the latent network structure using an efficient sparse least squares estimation approach. Using a thinning representation, we establish concentration inequalities for the first and second order statistics of the proposed Hawkes process. Such theoretical results enable us to establish the non-asymptotic error bound and the selection consistency of the estimated parameters. Furthermore, we describe a least squares loss based statistic for testing if the background intensity is constant in time. We demonstrate the efficacy of our proposed method through simulation studies and an application to a neuron spike train dataset. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Biao Cai and Jingfei Zhang and Yongtao Guan},
  doi          = {10.1080/01621459.2022.2102019},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {95-108},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Latent network structure learning from high-dimensional multivariate point processes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Operator-induced structural variable selection for
identifying materials genes. <em>JASA</em>, <em>119</em>(545), 81–94.
(<a href="https://doi.org/10.1080/01621459.2023.2294527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the emerging field of materials informatics, a fundamental task is to identify physicochemically meaningful descriptors, or materials genes, which are engineered from primary features and a set of elementary algebraic operators through compositions. Standard practice directly analyzes the high-dimensional candidate predictor space in a linear model; statistical analyses are then substantially hampered by the daunting challenge posed by the astronomically large number of correlated predictors with limited sample size. We formulate this problem as variable selection with operator-induced structure (OIS) and propose a new method to achieve unconventional dimension reduction by using the geometry embedded in OIS. Although the model remains linear, we iterate nonparametric variable selection for effective dimension reduction. This enables variable selection based on ab initio primary features, leading to a method that is orders of magnitude faster than existing methods, with improved accuracy. To select the nonparametric module, we discuss a desired performance criterion that is uniquely induced by variable selection with OIS; in particular, we propose to employ a Bayesian Additive Regression Trees (BART)-based variable selection method. Numerical studies show superiority of the proposed method, which continues to exhibit robust performance when the input dimension is out of reach of existing methods. Our analysis of single-atom catalysis identifies physical descriptors that explain the binding energy of metal-support pairs with high explanatory power, leading to interpretable insights to guide the prevention of a notorious problem called sintering and aid catalysis design. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shengbin Ye and Thomas P. Senftle and Meng Li},
  doi          = {10.1080/01621459.2023.2294527},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {81-94},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Operator-induced structural variable selection for identifying materials genes},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian lesion estimation with a structured spike-and-slab
prior. <em>JASA</em>, <em>119</em>(545), 66–80. (<a
href="https://doi.org/10.1080/01621459.2023.2278201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural demyelination and brain damage accumulated in white matter appear as hyperintense areas on T2-weighted MRI scans in the form of lesions. Modeling binary images at the population level, where each voxel represents the existence of a lesion, plays an important role in understanding aging and inflammatory diseases. We propose a scalable hierarchical Bayesian spatial model, called BLESS, capable of handling binary responses by placing continuous spike-and-slab mixture priors on spatially varying parameters and enforcing spatial dependency on the parameter dictating the amount of sparsity within the probability of inclusion. The use of mean-field variational inference with dynamic posterior exploration, which is an annealing-like strategy that improves optimization, allows our method to scale to large sample sizes. Our method also accounts for underestimation of posterior variance due to variational inference by providing an approximate posterior sampling approach based on Bayesian bootstrap ideas and spike-and-slab priors with random shrinkage targets. Besides accurate uncertainty quantification, this approach is capable of producing novel cluster size based imaging statistics, such as credible intervals of cluster size, and measures of reliability of cluster occurrence. Lastly, we validate our results via simulation studies and an application to the UK Biobank, a large-scale lesion mapping study with a sample size of 40,000 subjects. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Anna Menacher and Thomas E. Nichols and Chris Holmes and Habib Ganjgahi},
  doi          = {10.1080/01621459.2023.2278201},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {66-80},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian lesion estimation with a structured spike-and-slab prior},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feasibility study of differentially private summary
statistics and regression analyses with evaluations on administrative
and survey data. <em>JASA</em>, <em>119</em>(545), 52–65. (<a
href="https://doi.org/10.1080/01621459.2023.2270795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federal administrative data, such as tax data, are invaluable for research, but because of privacy concerns, access to these data is typically limited to select agencies and a few individuals. An alternative to sharing microlevel data is to allow individuals to query statistics without directly accessing the confidential data. This article studies the feasibility of using differentially private (DP) methods to make certain queries while preserving privacy. We also include new methodological adaptations to existing DP regression methods for using new data types and returning standard error estimates. We define feasibility as the impact of DP methods on analyses for making public policy decisions and the queries accuracy according to several utility metrics. We evaluate the methods using Internal Revenue Service data and public-use Current Population Survey data and identify how specific data features might challenge some of these methods. Our findings show that DP methods are feasible for simple, univariate statistics but struggle to produce accurate regression estimates and confidence intervals. To the best of our knowledge, this is the first comprehensive statistical study of DP regression methodology on real, complex datasets, and the findings have significant implications for the direction of a growing research field and public policy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Andrés F. Barrientos and Aaron R. Williams and Joshua Snoke and Claire McKay Bowen},
  doi          = {10.1080/01621459.2023.2270795},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {52-65},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A feasibility study of differentially private summary statistics and regression analyses with evaluations on administrative and survey data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical neyman-pearson classification for prioritizing
severe disease categories in COVID-19 patient data. <em>JASA</em>,
<em>119</em>(545), 39–51. (<a
href="https://doi.org/10.1080/01621459.2023.2270657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 has a spectrum of disease severity, ranging from asymptomatic to requiring hospitalization. Understanding the mechanisms driving disease severity is crucial for developing effective treatments and reducing mortality rates. One way to gain such understanding is using a multi-class classification framework, in which patients’ biological features are used to predict patients’ severity classes. In this severity classification problem, it is beneficial to prioritize the identification of more severe classes and control the “under-classification” errors, in which patients are misclassified into less severe categories. The Neyman-Pearson (NP) classification paradigm has been developed to prioritize the designated type of error. However, current NP procedures are either for binary classification or do not provide high probability controls on the prioritized errors in multi-class classification. Here, we propose a hierarchical NP (H-NP) framework and an umbrella algorithm that generally adapts to popular classification methods and controls the under-classification errors with high probability. On an integrated collection of single-cell RNA-seq (scRNA-seq) datasets for 864 patients, we explore ways of featurization and demonstrate the efficacy of the H-NP algorithm in controlling the under-classification errors regardless of featurization. Beyond COVID-19 severity classification, the H-NP algorithm generally applies to multi-class classification problems, where classes have a priority order. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lijia Wang and Y. X. Rachel Wang and Jingyi Jessica Li and Xin Tong},
  doi          = {10.1080/01621459.2023.2270657},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {39-51},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical neyman-pearson classification for prioritizing severe disease categories in COVID-19 patient data},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semiparametric inverse reinforcement learning approach to
characterize decision making for mental disorders. <em>JASA</em>,
<em>119</em>(545), 27–38. (<a
href="https://doi.org/10.1080/01621459.2023.2261184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD) is one of the leading causes of disability-adjusted life years. Emerging evidence indicates the presence of reward processing abnormalities in MDD. An important scientific question is whether the abnormalities are due to reduced sensitivity to received rewards or reduced learning ability. Motivated by the probabilistic reward task (PRT) experiment in the EMBARC study, we propose a semiparametric inverse reinforcement learning (RL) approach to characterize the reward-based decision-making of MDD patients. The model assumes that a subject’s decision-making process is updated based on a reward prediction error weighted by the subject-specific learning rate. To account for the fact that one favors a decision leading to a potentially high reward, but this decision process is not necessarily linear, we model reward sensitivity with a nondecreasing and nonlinear function. For inference, we estimate the latter via approximation by I-splines and then maximize the joint conditional log-likelihood. We show that the resulting estimators are consistent and asymptotically normal. Through extensive simulation studies, we demonstrate that under different reward-generating distributions, the semiparametric inverse RL outperforms the parametric inverse RL. We apply the proposed method to EMBARC and find that MDD and control groups have similar learning rates but different reward sensitivity functions. There is strong statistical evidence that reward sensitivity functions have nonlinear forms. Using additional brain imaging data in the same study, we find that both reward sensitivity and learning rate are associated with brain activities in the negative affect circuitry under an emotional conflict task. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xingche Guo and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1080/01621459.2023.2261184},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {27-38},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A semiparametric inverse reinforcement learning approach to characterize decision making for mental disorders},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous distributed lag models to estimate
personalized effects of maternal exposures to air pollution.
<em>JASA</em>, <em>119</em>(545), 14–26. (<a
href="https://doi.org/10.1080/01621459.2023.2258595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children’s health studies support an association between maternal environmental exposures and children’s birth outcomes. A common goal is to identify critical windows of susceptibility—periods during gestation with increased association between maternal exposures and a future outcome. The timing of the critical windows and magnitude of the associations are likely heterogeneous across different levels of individual, family, and neighborhood characteristics. Using an administrative Colorado birth cohort we estimate the individualized relationship between weekly exposures to fine particulate matter (PM 2.5 ) during gestation and birth weight. To achieve this goal, we propose a statistical learning method combining distributed lag models and Bayesian additive regression trees to estimate critical windows at the individual level and identify characteristics that induce heterogeneity from a high-dimensional set of potential modifying factors. We find evidence of heterogeneity in the PM 2.5 —birth weight relationship, with some mother—child dyads showing a three times larger decrease in birth weight for an IQR increase in exposure (5.9–8.5 μ g / m 3 PM 2.5 ) compared to the population average. Specifically, we find increased vulnerability for non-Hispanic mothers who are either younger, have higher body mass index or lower educational attainment. Our case study is the first precision health study of critical windows. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Daniel Mork and Marianthi-Anna Kioumourtzoglou and Marc Weisskopf and Brent A. Coull and Ander Wilson},
  doi          = {10.1080/01621459.2023.2258595},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {14-26},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Heterogeneous distributed lag models to estimate personalized effects of maternal exposures to air pollution},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming repeated testing schedule bias in estimates of
disease prevalence. <em>JASA</em>, <em>119</em>(545), 1–13. (<a
href="https://doi.org/10.1080/01621459.2023.2238943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the COVID-19 pandemic, many institutions such as universities and workplaces implemented testing regimens with every member of some population tested longitudinally, and those testing positive isolated for some time. Although the primary purpose of such regimens was to suppress disease spread by identifying and isolating infectious individuals, testing results were often also used to obtain prevalence and incidence estimates. Such estimates are helpful in risk assessment and institutional planning and various estimation procedures have been implemented, ranging from simple test-positive rates to complex dynamical modeling. Unfortunately, the popular test-positive rate is a biased estimator of prevalence under many seemingly innocuous longitudinal testing regimens with isolation. We illustrate how such bias arises and identify conditions under which the test-positive rate is unbiased. Further, we identify weaker conditions under which prevalence is identifiable and propose a new estimator of prevalence under longitudinal testing. We evaluate the proposed estimation procedure via simulation study and illustrate its use on a dataset derived by anonymizing testing data from The Ohio State University. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Patrick M. Schnell and Matthew Wascher and Grzegorz A. Rempala},
  doi          = {10.1080/01621459.2023.2238943},
  journal      = {Journal of the American Statistical Association},
  month        = {1},
  number       = {545},
  pages        = {1-13},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Overcoming repeated testing schedule bias in estimates of disease prevalence},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
