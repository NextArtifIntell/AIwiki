<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="oms---56">OMS - 56</h2>
<ul>
<li><details>
<summary>
(2024). Least squares monotonic unimodal approximations to
successively updated data and an application to a covid-19 outbreak.
<em>OMS</em>, <em>39</em>(6), 1464‚Äì1494. (<a
href="https://doi.org/10.1080/10556788.2024.2428475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximations to noisy data are constructed to obtain unimodality of successively updated data. Precisely, a method is developed that calculates least squares monotonic increasing - decreasing approximations to n data, for successive values of n . Having fixed n , the statement of the increasing-‚Äìdecreasing constraints in terms of first differences of the data subject to one sign change gives rise to a discrete search calculation, because the position of the sign change is also an unknown of the optimization problem. Although this problem may have many local minima for each value of n , the method is made quite efficient by taking advantage of two properties of the optimization calculation. The first property, for fixed n , provides necessary and sufficient conditions that reduce the optimal selection of the position to solving two sequences of monotonic approximation problems to subranges of data in the least complexity O ( n ) . The sufficiency conditions is a new result. The second property shows that the optimal position increases as n increases. Hence, if this position of a fit to the first n ‚àí1 data is available, then in order to calculate the required position of a fit to the first n data there is no need to consider data that are before the current position. A Fortran program has been written and some timings of numerical results are given for up to n = 30,000 data and various levels of noise. They seem to be proportional to n . The method is applied to data of daily Covid-19 deaths of the United Kingdom for a period that exhibits a major outbreak, and obtains insights of the evolution of the process as new data enter the calculation. The results reveal certain features of the calculation that may be helpful to supporting policy making, when used in modelling epidemic processes. Further, a procedure is proposed where our method initializes the nonlinear least squares calculation of Richards epidemiological model, and the numerical results confirm some useful advantages over the plain use of this model. One benefit of our method for unimodal smoothing of successive data and estimation of the associated turning points is that it provides a property that appears in a wide range of processes from biology, economics, finance and social sciences, for instance.},
  archive      = {J_OMS},
  author       = {I. C. Demetriou},
  doi          = {10.1080/10556788.2024.2428475},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1464-1494},
  shortjournal = {Optim. Methods Softw.},
  title        = {Least squares monotonic unimodal approximations to successively updated data and an application to a covid-19 outbreak},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two efficient spectral hybrid CG methods based on memoryless
BFGS direction and dai‚Äìliao conjugacy condition. <em>OMS</em>,
<em>39</em>(6), 1445‚Äì1463. (<a
href="https://doi.org/10.1080/10556788.2024.2364203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral conjugate gradient (CG) method is one of the effective methods for solving unconstrained optimization problems. In this work, we introduce a composite hybrid CG parameter which is a convex combination of two new adaptive hybrid CG parameters. To derive the optimal choice of the combination coefficient in our proposed composite CG parameter, two approaches to calculating it are introduced. One is to minimize the distance between the hybrid CG direction and the self-scaling memoryless BFGS direction and the other is to apply the Dai‚ÄìLiao conjugacy condition. Further, to make the search direction have better theoretical performance, two effective spectral hybrid CG methods are generated. Our proposed methods ensure the sufficient descent property regardless of the line search. And the global convergence results for general non-convex functions are established under some fundamental assumptions and Wolfe line search. Numerical experiments on solving unconstrained optimization problems illustrate the effectiveness of our proposed methods.},
  archive      = {J_OMS},
  author       = {Pengjie Liu and Zihang Yuan and Yue Zhuo and Hu Shao},
  doi          = {10.1080/10556788.2024.2364203},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1445-1463},
  shortjournal = {Optim. Methods Softw.},
  title        = {Two efficient spectral hybrid CG methods based on memoryless BFGS direction and Dai‚ÄìLiao conjugacy condition},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mixed-integer programming formulation for optimizing the
double row layout problem. <em>OMS</em>, <em>39</em>(6), 1428‚Äì1444. (<a
href="https://doi.org/10.1080/10556788.2024.2349093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Double Row Layout Problem (DRLP) asks for an arrangement of machines on both sides of a straight line corridor so as to minimize the total cost for transferring materials among machines. The DRLP is NP-Hard and has practical relevance, specially in manufacturing systems design. In this paper, we drastically reduce the time required to solve the problem by constructing a new and effective mixed-integer linear programming (MILP) model of the DRLP. The new model was obtained by reformulating an existing MILP model. This includes tightening some constraints, introducing new variables, implementing constraints to link the new and original variables; and adding valid inequalities and a valid system of equations. To reduce the size of the reformulated model, we eliminate several of the new introduced variables by a substitution using the system of equations. The computational results demonstrate that the proposed model requires considerably smaller computational times compared to the ones in the literature. As a consequence, optimal solutions can now be efficiently found for larger instances of the problem. Previous studies have been able to optimally solve, within reasonable time, instances with size up to 16 machines, while with the new model four instances with 20 machines could be optimally solved.},
  archive      = {J_OMS},
  author       = {Andr√© R. S. Amaral},
  doi          = {10.1080/10556788.2024.2349093},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1428-1444},
  shortjournal = {Optim. Methods Softw.},
  title        = {A mixed-integer programming formulation for optimizing the double row layout problem},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximizing the number of rides served for time-limited
dial-a-ride*. <em>OMS</em>, <em>39</em>(6), 1407‚Äì1427. (<a
href="https://doi.org/10.1080/10556788.2024.2349091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a variant of the offline Dial-a-Ride problem on weighted graphs with a single server where each request has a source and destination. The server&#39;s goal is to serve requests so as to maximize the total number of requests served within a given time limit. We first prove that no polynomial-time algorithm will always serve the optimal number of requests, even when the algorithm&#39;s time limit is augmented by any factor c ‚â• 1 , unless P¬†=¬†NP. We also show that the approximation ratio is unbounded for a reasonable class of algorithms for this problem. We then present k -Sequence, an algorithm that repeatedly serves the fastest set of k remaining requests. We show that k -Sequence has approximation ratio at most 2 + ‚åà Œª ‚åâ / k and at least 1 + Œª / k , where Œª denotes the aspect ratio of the graph, and that the ratio 1 + Œª / k is tight when 1 + Œª / k ‚â• k . We also show that even as k grows beyond the size of Œª , the ratio never improves below 9/7.},
  archive      = {J_OMS},
  author       = {Barbara M. Anthony and Christine Chung and Ananya Das and David Yuen},
  doi          = {10.1080/10556788.2024.2349091},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1407-1427},
  shortjournal = {Optim. Methods Softw.},
  title        = {Maximizing the number of rides served for time-limited dial-a-ride*},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust reverse 1-center problems on trees with interval
costs. <em>OMS</em>, <em>39</em>(6), 1383‚Äì1406. (<a
href="https://doi.org/10.1080/10556788.2024.2346642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the problem of reducing the edge lengths of a tree in a minimum cost way such that the 1-center function at a predetermined vertex is at most a given threshold value. Moreover, it is assumed that the costs of reducing edge lengths are not exactly determined, but they are estimated within intervals and their total deviation is limited within a given bound. This problem is called the robust reverse 1-center problem on trees with interval costs. We first reformulate the problem as a Stackelberg game and translate it into a univariate optimization problem. Based on the special properties of the feasible set, we can state the corresponding problem as a parameterized minimum cost flow problem. Then we develop an algorithm that solves the problem in O ( n 2 log ‚Å° n ) time due to the convexity of the univariate objective function. We finally discuss the problem of improving the international transportation network in Iran as a real-life application.},
  archive      = {J_OMS},
  author       = {Javad Tayyebi and Kien Trung Nguyen},
  doi          = {10.1080/10556788.2024.2346642},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1383-1406},
  shortjournal = {Optim. Methods Softw.},
  title        = {Robust reverse 1-center problems on trees with interval costs},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximating hessian matrices using bayesian inference: A
new approach for quasi-newton methods in stochastic optimization.
<em>OMS</em>, <em>39</em>(6), 1352‚Äì1382. (<a
href="https://doi.org/10.1080/10556788.2024.2339226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using quasi-Newton methods in stochastic optimization is not a trivial task given the difficulty of extracting curvature information from the noisy gradients. Moreover, pre-conditioning noisy gradient observations tend to amplify the noise. We propose a Bayesian approach to obtain a Hessian matrix approximation for stochastic optimization that minimizes the secant equations residue while retaining the extreme eigenvalues between a specified range. Thus, the proposed approach assists stochastic gradient descent to converge to local minima without augmenting gradient noise. We propose maximizing the log posterior using the Newton-CG method. Numerical results on a stochastic quadratic function and an ‚Ñì 2 -regularized logistic regression problem are presented. In all the cases tested, our approach improves the convergence of stochastic gradient descent, compensating for the overhead of solving the log posterior maximization. In particular, pre-conditioning the stochastic gradient with the inverse of our Hessian approximation becomes more advantageous the larger the condition number of the problem is.},
  archive      = {J_OMS},
  author       = {Andr√© Gustavo Carlon and Luis Espath and Ra√∫l Tempone},
  doi          = {10.1080/10556788.2024.2339226},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1352-1382},
  shortjournal = {Optim. Methods Softw.},
  title        = {Approximating hessian matrices using bayesian inference: A new approach for quasi-newton methods in stochastic optimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing subgradients of convex relaxations for solutions
of parametric ordinary differential equations. <em>OMS</em>,
<em>39</em>(6), 1309‚Äì1351. (<a
href="https://doi.org/10.1080/10556788.2024.2346641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel subgradient evaluation method is proposed for nonsmooth convex relaxations of parametric solutions of ordinary differential equations (ODEs) arising in global dynamic optimization, assuming that the relaxations always lie strictly within interval bounds during integration. We argue that this assumption is reasonable in practice. These subgradients are computed as the unique solution of an auxiliary parametric affine ODE, analogous to classical forward/tangent sensitivity evaluation methods for smooth dynamic systems. Unlike established subgradient evaluation approaches for nonsmooth dynamic systems, this new method does not require smoothness or transversality assumptions, and is compatible with existing subgradient evaluation methods for closed-form convex functions, as implemented in subgradient evaluation software such as EAGO.jl and MC ++ . Moreover, we show that a subgradient for a lower-bounding problem in global dynamic optimization can be directly evaluated using reverse/adjoint sensitivity analysis, which may reduce the overall computational effort for an overarching global optimization method. Numerical examples are presented, based on a proof-of-concept implementation in Julia.},
  archive      = {J_OMS},
  author       = {Yingkai Song and Kamil A. Khan},
  doi          = {10.1080/10556788.2024.2346641},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1309-1351},
  shortjournal = {Optim. Methods Softw.},
  title        = {Computing subgradients of convex relaxations for solutions of parametric ordinary differential equations},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix extreme points and free extreme points of free
spectrahedra. <em>OMS</em>, <em>39</em>(6), 1263‚Äì1308. (<a
href="https://doi.org/10.1080/10556788.2024.2339221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free spectrahedra are dimension free solution sets to linear matrix inequalities of the form ùêø ùê¥ ‚Å° ( ùëã ) = ùêº ùëë ‚äó ùêº ùëõ + ùê¥ 1 ‚äó ùëã 1 + ùê¥ 2 ‚äó ùëã 2 + ‚ãØ + ùê¥ ùëî ‚äó ùëã ùëî ‚™∞ 0 , where the ùê¥ ùëñ and ùëã ùëñ are symmetric matrices and the X i have any size n √ó n . Free spectrahedra are ubiquitous in systems engineering, operator algebras, and the theory of matrix convex sets. Matrix and free extreme points of free spectrahedra are particularly important. We present theoretical, algorithmic, and experimental results illuminating basic properties of extreme points. For example, though many authors have studied matrix and free extreme points, it has until now been unknown if these two types of extreme points are actually different. This paper settles that issue. We also present and analyze several algorithms. Namely, we perfect an algorithm for computing an expansion of an element of a free spectrahedron in terms of free extreme points. We also give algorithms for testing if a point is matrix extreme and for computing matrix extreme points that are not free extreme.},
  archive      = {J_OMS},
  author       = {Aidan Epperly and Eric Evert and J. William Helton and Igor Klep},
  doi          = {10.1080/10556788.2024.2339221},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1263-1308},
  shortjournal = {Optim. Methods Softw.},
  title        = {Matrix extreme points and free extreme points of free spectrahedra},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A family of limited memory three term conjugate gradient
methods. <em>OMS</em>, <em>39</em>(6), 1241‚Äì1262. (<a
href="https://doi.org/10.1080/10556788.2024.2329591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on several modified Hestenes‚ÄìStiefel and Polak‚ÄìRibi√®re‚ÄìPolyak nonlinear conjugate gradient methods, a family of three term limited memory CG methods are developed. When the current search direction falls into the subspace spanned by the previous m directions, the algorithm branches to optimize the objective function on the subspace by using L-BFGS method. We use this strategy to avoid potential local loops and accelerate the convergence. The proposed methods are sufficient descent. When the steplength is determined by Wolfe or Armijo line search, we establish the global convergence of the methods in a concise way. Numerical results verify the efficiency of the proposed method for the unconstrained optimization problems in the CUTEst library.},
  archive      = {J_OMS},
  author       = {Min Li and Minru Bai},
  doi          = {10.1080/10556788.2024.2329591},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1241-1262},
  shortjournal = {Optim. Methods Softw.},
  title        = {A family of limited memory three term conjugate gradient methods},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An active set method for bound-constrained optimization.
<em>OMS</em>, <em>39</em>(6), 1216‚Äì1240. (<a
href="https://doi.org/10.1080/10556788.2024.2339215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a class of algorithms is developed for bound-constrained optimization. The new scheme uses the gradient-free line search along bent search paths. Unlike traditional algorithms for bound-constrained optimization, our algorithm ensures that the reduced gradient becomes arbitrarily small. It is also proved that all strongly active variables are found and fixed after finitely many iterations. A Matlab implementation of a bound-constrained solver LMBOPT based on the new theory was discussed by the present authors in a companion paper ( Math. Program. Comput. 14 (2022), 271‚Äì318).},
  archive      = {J_OMS},
  author       = {A. Neumaier and B. Azmi and M. Kimiaei},
  doi          = {10.1080/10556788.2024.2339215},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1216-1240},
  shortjournal = {Optim. Methods Softw.},
  title        = {An active set method for bound-constrained optimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Loraine ‚Äì an interior-point solver for low-rank semidefinite
programming. <em>OMS</em>, <em>39</em>(6), 1185‚Äì1215. (<a
href="https://doi.org/10.1080/10556788.2023.2250522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to introduce a new code for the solution of large-and-sparse linear semidefinite programs (SDPs) with low-rank solutions or solutions with few outlying eigenvalues, and/or problems with low-rank data. We propose to use a preconditioned conjugate gradient method within an interior-point SDP algorithm and an efficient preconditioner fully utilizing the low-rank information. The efficiency is demonstrated by numerical experiments using the truss topology optimization problems, Lasserre relaxations of the MAXCUT problems and the sensor network localization problems.},
  archive      = {J_OMS},
  author       = {Soodeh Habibi and Michal Koƒçvara and Michael Stingl},
  doi          = {10.1080/10556788.2023.2250522},
  journal      = {Optimization Methods and Software},
  month        = {11},
  number       = {6},
  pages        = {1185-1215},
  shortjournal = {Optim. Methods Softw.},
  title        = {Loraine ‚Äì an interior-point solver for low-rank semidefinite programming},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linear programming sensitivity measured by the optimal value
worst-case analysis. <em>OMS</em>, <em>39</em>(5), 1168‚Äì1184. (<a
href="https://doi.org/10.1080/10556788.2024.2329590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the concept of a derivative of the optimal value function in linear programming (LP). Basically, it is the worst case optimal value of an interval LP problem when the nominal data are inflated to intervals according to given perturbation patterns. By definition, the derivative expresses how the optimal value can worsen when the data are subject to variations. In addition, it also gives a certain sensitivity measure or condition number of an LP problem. If the LP problem is nondegenerate, the derivatives are easy to calculate from the computed primal and dual optimal solutions. For degenerate problems, the computation is more difficult. We propose an upper bound and some kind of characterization, but there are many open problems remaining. We carried out numerical experiments with specific LP problems and with real LP data from Netlib repository. They show that the derivatives give a suitable sensitivity measure of LP problems. It remains an open problem how to efficiently and rigorously handle degenerate problems.},
  archive      = {J_OMS},
  author       = {Milan Hlad√≠k},
  doi          = {10.1080/10556788.2024.2329590},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1168-1184},
  shortjournal = {Optim. Methods Softw.},
  title        = {Linear programming sensitivity measured by the optimal value worst-case analysis},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AN-SPS: Adaptive sample size nonmonotone line search
spectral projected subgradient method for convex constrained
optimization problems. <em>OMS</em>, <em>39</em>(5), 1143‚Äì1167. (<a
href="https://doi.org/10.1080/10556788.2024.2324920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider convex optimization problems with a possibly nonsmooth objective function in the form of a mathematical expectation. The proposed framework (AN-SPS) employs Sample Average Approximations (SAA) to approximate the objective function, which is either unavailable or too costly to compute. The sample size is chosen in an adaptive manner, which eventually pushes the SAA error to zero almost surely (a.s.). The search direction is based on a scaled subgradient and a spectral coefficient, both related to the SAA function. The step size is obtained via a nonmonotone line search over a predefined interval, which yields a theoretically sound and practically efficient algorithm. The method retains feasibility by projecting the resulting points onto a feasible set. The a.s. convergence of AN-SPS method is proved without the assumption of a bounded feasible set or bounded iterates. Preliminary numerical results on Hinge loss problems reveal the advantages of the proposed adaptive scheme. In addition, a study of different nonmonotone line search strategies in combination with different spectral coefficients within AN-SPS framework is also conducted, yielding some hints for future work.},
  archive      = {J_OMS},
  author       = {Nata≈°a Krklec Jerinkiƒá and Tijana Ostojiƒá},
  doi          = {10.1080/10556788.2024.2324920},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1143-1167},
  shortjournal = {Optim. Methods Softw.},
  title        = {AN-SPS: Adaptive sample size nonmonotone line search spectral projected subgradient method for convex constrained optimization problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential hierarchical least-squares programming for
prioritized non-linear optimal control. <em>OMS</em>, <em>39</em>(5),
1104‚Äì1142. (<a
href="https://doi.org/10.1080/10556788.2024.2307467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a sequential hierarchical least-squares programming solver with trust-region and hierarchical step-filter with application to prioritized discrete non-linear optimal control. It is based on a hierarchical step-filter which resolves each priority level of a non-linear hierarchical least-squares programming via a globally convergent sequential quadratic programming step-filter. Leveraging a condition on the trust-region or the filter initialization, our hierarchical step-filter maintains this global convergence property. The hierarchical least-squares programming sub-problems are solved via a sparse reduced Hessian based interior point method. It leverages an efficient implementation of the turnback algorithm for the computation of nullspace bases for banded matrices. We propose a nullspace trust region adaptation method embedded within the sub-problem solver towards a comprehensive hierarchical step-filter. We demonstrate the computational efficiency of the hierarchical solver on typical test functions like the Rosenbrock and Himmelblau&#39;s functions, inverse kinematics problems and prioritized discrete non-linear optimal control.},
  archive      = {J_OMS},
  author       = {Kai Pfeiffer and Abderrahmane Kheddar},
  doi          = {10.1080/10556788.2024.2307467},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1104-1142},
  shortjournal = {Optim. Methods Softw.},
  title        = {Sequential hierarchical least-squares programming for prioritized non-linear optimal control},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Near-optimal tensor methods for minimizing the gradient norm
of convex functions and accelerated primal‚Äìdual tensor methods.
<em>OMS</em>, <em>39</em>(5), 1068‚Äì1103. (<a
href="https://doi.org/10.1080/10556788.2023.2296443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated, in particular, by the entropy-regularized optimal transport problem, we consider convex optimization problems with linear equality constraints, where the dual objective has Lipschitz p th order derivatives, and develop two approaches for solving such problems. The first approach is based on the minimization of the norm of the gradient in the dual problem and then the reconstruction of an approximate primal solution. Recently, Grapiglia and Nesterov [ Tensor methods for finding approximate stationary points of convex functions , Optim. Methods Softw. (2020), pp. 1‚Äì34] showed lower complexity bounds for the problem of minimizing the gradient norm of the function with Lipschitz p th order derivatives. Still, the question of optimal or near-optimal methods remained open as the algorithms presented in [Grapiglia and Nesterov, Tensor methods for finding approximate stationary points of convex functions , Optim. Methods Softw. (2020), pp. 1‚Äì34] achieve suboptimal bounds only. We close this gap by proposing two near-optimal (up to logarithmic factors) methods with complexity bounds O ~ ( Œµ ‚àí 2 ( p + 1 ) / ( 3 p + 1 ) ) and O ~ ( Œµ ‚àí 2 / ( 3 p + 1 ) ) with respect to the initial objective residual and the distance between the starting point and solution, respectively. We then apply these results (having independent interest) to our primal‚Äìdual setting. As the second approach, we propose a direct accelerated primal‚Äìdual tensor method for convex problems with linear equality constraints, where the dual objective has Lipschitz p th order derivatives. For this algorithm, we prove O ~ ( Œµ ‚àí 1 / ( p + 1 ) ) complexity in terms of the duality gap and the residual in the constraints. We illustrate the practical performance of the proposed algorithms in experiments on logistic regression, entropy-regularized optimal transport problem, and the minimal mutual information problem.},
  archive      = {J_OMS},
  author       = {Pavel Dvurechensky and Petr Ostroukhov and Alexander Gasnikov and C√©sar A. Uribe and Anastasiya Ivanova},
  doi          = {10.1080/10556788.2023.2296443},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1068-1103},
  shortjournal = {Optim. Methods Softw.},
  title        = {Near-optimal tensor methods for minimizing the gradient norm of convex functions and accelerated primal‚Äìdual tensor methods},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closing duality gaps of SDPs completely through perturbation
when singularity degree is one. <em>OMS</em>, <em>39</em>(5), 1040‚Äì1067.
(<a href="https://doi.org/10.1080/10556788.2024.2409710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let ( ùêè , ùêÉ ) be a primal-dual pair of SDPs with a nonzero finite duality gap. Under such circumstances, ùêè and ùêÉ are weakly feasible and if we perturb the problem data to recover strong feasibility, the (common) optimal value function v as a function of the perturbation is not well-defined at zero (i.e. at the unperturbed data). Nevertheless, under the assumption that both P and D have singularity degree one, we show that a limiting version v a of v is a well-defined monotone decreasing continuous bijective function with domain [ 0 , œÄ / 2 ] and image [ v ( P ) , v ( D ) ] , where v ( P ) and v ( D ) are the optimal values of P and D , respectively. The domain [ 0 , œÄ / 2 ] corresponds to directions of perturbation defined in a certain manner. Thus, v a ‚Äòcompletely fills‚Äô the nonzero duality gap under a mild regularity condition. Our result is tight in that there exists an instance with singularity degree two for which v a is not continuous.},
  archive      = {J_OMS},
  author       = {Takashi Tsuchiya and Bruno F. Louren√ßo and Masakazu Muramatsu and Takayuki Okuno},
  doi          = {10.1080/10556788.2024.2409710},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1040-1067},
  shortjournal = {Optim. Methods Softw.},
  title        = {Closing duality gaps of SDPs completely through perturbation when singularity degree is one},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A meta-heuristic extension of the lagrangian heuristic
framework. <em>OMS</em>, <em>39</em>(5), 1008‚Äì1039. (<a
href="https://doi.org/10.1080/10556788.2024.2404094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lagrangian heuristics for discrete optimization work by modifying Lagrangian relaxed solutions into feasible solutions to an original problem. They are designed to identify feasible, and hopefully also near-optimal, solutions and have proven to be highly successful in many applications. Based on a primal-dual global optimality condition for non-convex optimization problems, we develop a meta-heuristic extension of the Lagrangian heuristic framework. The optimality condition characterizes (near-)optimal solutions in terms of near-optimality and near-complementarity measures for Lagrangian relaxed solutions. The meta-heuristic extension amounts to constructing a weighted combination of these measures, thus creating a parametric auxiliary objective function, which is a close relative to a Lagrangian function, and embedding a Lagrangian heuristic in a search procedure in the space of the weight parameters. We illustrate and make a first assessment of this meta-heuristic extension by applying it to the generalized assignment and set covering problems. Our computational experience show that the meta-heuristic extension of a standard Lagrangian heuristic can significantly improve upon solution quality.},
  archive      = {J_OMS},
  author       = {Uledi Ngulo and Torbj√∂rn Larsson and Nils-Hassan Quttineh},
  doi          = {10.1080/10556788.2024.2404094},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {1008-1039},
  shortjournal = {Optim. Methods Softw.},
  title        = {A meta-heuristic extension of the lagrangian heuristic framework},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shape-changing trust-region methods using multipoint
symmetric secant matrices. <em>OMS</em>, <em>39</em>(5), 990‚Äì1007. (<a
href="https://doi.org/10.1080/10556788.2023.2296441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider methods for large-scale and nonconvex unconstrained optimization. We propose a new trust-region method whose subproblem is defined using a so-called ‚Äòshape-changing‚Äô norm together with densely-initialized multipoint symmetric secant (MSS) matrices to approximate the Hessian. Shape-changing norms and dense initializations have been successfully used in the context of traditional quasi-Newton methods, but have yet to be explored in the case of MSS methods. Numerical results suggest that trust-region methods that use densely-initialized MSS matrices together with shape-changing norms outperform MSS with other trust-region methods.},
  archive      = {J_OMS},
  author       = {J. J. Brust and J. B. Erway and R. F. Marcia},
  doi          = {10.1080/10556788.2023.2296441},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {990-1007},
  shortjournal = {Optim. Methods Softw.},
  title        = {Shape-changing trust-region methods using multipoint symmetric secant matrices},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence rate analysis of the gradient descent‚Äìascent
method for convex‚Äìconcave saddle-point problems. <em>OMS</em>,
<em>39</em>(5), 967‚Äì989. (<a
href="https://doi.org/10.1080/10556788.2024.2360040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the gradient descent‚Äìascent method for convex‚Äìconcave saddle-point problems. We derive a new non-asymptotic global convergence rate in terms of distance to the solution set by using the semidefinite programming performance estimation method. The given convergence rate incorporates most parameters of the problem and it is exact for a large class of strongly convex-strongly concave saddle-point problems for one iteration. We also investigate the algorithm without strong convexity and we provide some necessary and sufficient conditions under which the gradient descent‚Äìascent enjoys linear convergence.},
  archive      = {J_OMS},
  author       = {Moslem Zamani and Hadi Abbaszadehpeivasti and Etienne de Klerk},
  doi          = {10.1080/10556788.2024.2360040},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {967-989},
  shortjournal = {Optim. Methods Softw.},
  title        = {Convergence rate analysis of the gradient descent‚Äìascent method for convex‚Äìconcave saddle-point problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An investigation of stochastic trust-region based algorithms
for finite-sum minimization. <em>OMS</em>, <em>39</em>(5), 937‚Äì966. (<a
href="https://doi.org/10.1080/10556788.2024.2346834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work elaborates on the TRust-region-ish (TRish) algorithm, a stochastic optimization method for finite-sum minimization problems proposed by Curtis et¬†al. in [F.E. Curtis, K. Scheinberg, and R. Shi, A stochastic trust region algorithm based on careful step normalization , INFORMS. J. Optim. 1(3) (2019), pp. 200‚Äì220; F.E. Curtis and R. Shi, A fully stochastic second-order trust region method , Optim. Methods Softw. 37(3) (2022), pp. 844‚Äì877]. A theoretical analysis that complements the results in the literature is presented, and the issue of tuning the involved hyper-parameters is investigated. Our study also focuses on a practical version of the method, which computes the stochastic gradient by means of the inner product test and the orthogonality test proposed by Bollapragada et¬†al. in [R. Bollapragada, R. Byrd, and J. Nocedal , Adaptive sampling strategies for stochastic optimization , SIAM. J. Optim. 28(4) (2018), pp. 3312‚Äì3343]. It is shown experimentally that this implementation improves the performance of TRish and reduces its sensitivity to the choice of the hyper-parameters.},
  archive      = {J_OMS},
  author       = {Stefania Bellavia and Benedetta Morini and Simone Rebegoldi},
  doi          = {10.1080/10556788.2024.2346834},
  journal      = {Optimization Methods and Software},
  month        = {9},
  number       = {5},
  pages        = {937-966},
  shortjournal = {Optim. Methods Softw.},
  title        = {An investigation of stochastic trust-region based algorithms for finite-sum minimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semismooth conjugate gradients method ‚Äì theoretical
analysis. <em>OMS</em>, <em>39</em>(4), 911‚Äì935. (<a
href="https://doi.org/10.1080/10556788.2024.2331068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale applications, deterministic and stochastic variants of Cauchy&#39;s steepest descent method are widely used for the minimization of objectives that are only piecewise smooth. In this paper, we analyse a deterministic descent method based on the generalization of rescaled conjugate gradients proposed by Philip Wolfe in 1975 for objectives that are convex. Without this assumption, the new method exploits semismoothness to obtain pairs of directionally active generalized gradients such that it can only converge to Clarke stationary points. Numerical results illustrate the theoretical findings.},
  archive      = {J_OMS},
  author       = {Franz Bethke and Andreas Griewank and Andrea Walther},
  doi          = {10.1080/10556788.2024.2331068},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {911-935},
  shortjournal = {Optim. Methods Softw.},
  title        = {A semismooth conjugate gradients method ‚Äì theoretical analysis},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimal interpolation set for model-based derivative-free
optimization methods. <em>OMS</em>, <em>39</em>(4), 898‚Äì910. (<a
href="https://doi.org/10.1080/10556788.2024.2330635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper demonstrates the optimality of an interpolation set employed in derivative-free trust-region methods. This set is optimal in the sense that it minimizes the constant of well-poisedness in a ball centred at the starting point. It is chosen as the default initial interpolation set by many derivative-free trust-region methods based on underdetermined quadratic interpolation, including NEWUOA, BOBYQA, LINCOA, and COBYQA. Our analysis provides a theoretical justification for this choice.},
  archive      = {J_OMS},
  author       = {Tom M. Ragonneau and Zaikun Zhang},
  doi          = {10.1080/10556788.2024.2330635},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {898-910},
  shortjournal = {Optim. Methods Softw.},
  title        = {An optimal interpolation set for model-based derivative-free optimization methods},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel interior-point solver for block-structured
nonlinear programs on SIMD/GPU architectures. <em>OMS</em>,
<em>39</em>(4), 874‚Äì897. (<a
href="https://doi.org/10.1080/10556788.2024.2329646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how to port the standard interior-point method to new exascale architectures for block-structured nonlinear programs with state equations. Computationally, we decompose the interior-point algorithm into two successive operations: the evaluation of the derivatives and the solution of the associated Karush-Kuhn-Tucker (KKT) linear system. Our method accelerates both operations using two levels of parallelism. First, we distribute the computations on multiple processes using coarse parallelism. Second, each process uses SIMD/GPU accelerators locally to accelerate the operations using fine-grained parallelism. The KKT system is reduced by eliminating the inequalities and the state variables from the corresponding equations. We demonstrate our method&#39;s capability on the supercomputer Polaris, a testbed for the future exascale Aurora system. Each node is equipped with four GPUs, a setup amenable to our two-level approach. Our experiments on the stochastic optimal power flow problem show that the reduction method is 50x faster than the sparse linear solver HSL MA57 running in serial on the CPU, and 6x faster than Pardiso running in parallel on CPU on the same number of processes.},
  archive      = {J_OMS},
  author       = {Fran√ßois Pacaud and Michel Schanen and Sungho Shin and Daniel Adrian Maldonado and Mihai Anitescu},
  doi          = {10.1080/10556788.2024.2329646},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {874-897},
  shortjournal = {Optim. Methods Softw.},
  title        = {Parallel interior-point solver for block-structured nonlinear programs on SIMD/GPU architectures},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Derivative-free bound-constrained optimization for solving
structured problems with surrogate models. <em>OMS</em>, <em>39</em>(4),
845‚Äì873. (<a
href="https://doi.org/10.1080/10556788.2024.2329588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and analyze a model-based derivative-free (DFO) algorithm for solving bound-constrained optimization problems where the objective function is the composition of a smooth function and a vector of black-box functions. We assume that the black-box functions are smooth and the evaluation of them is the computational bottleneck of the algorithm. The distinguishing feature of our algorithm is the use of approximate function values at interpolation points which can be obtained by an application-specific surrogate model that is cheap to evaluate. As an example, we consider the situation in which a sequence of related optimization problems is solved and present a regression-based approximation scheme that uses function values that were evaluated when solving prior problem instances. In addition, we propose and analyze a new algorithm for obtaining interpolation points that handles unrelaxable bound constraints. Our numerical results show that our algorithm outperforms a state-of-the-art DFO algorithm for solving a least-squares problem from a chemical engineering application when a history of black-box function evaluations is available.},
  archive      = {J_OMS},
  author       = {Frank E. Curtis and Shima Dezfulian and Andreas W√§chter},
  doi          = {10.1080/10556788.2024.2329588},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {845-873},
  shortjournal = {Optim. Methods Softw.},
  title        = {Derivative-free bound-constrained optimization for solving structured problems with surrogate models},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A proximal-gradient method for problems with overlapping
group-sparse regularization: Support identification complexity.
<em>OMS</em>, <em>39</em>(4), 810‚Äì844. (<a
href="https://doi.org/10.1080/10556788.2024.2346643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the proximal-gradient method for minimizing the sum of a smooth function and a convex non-smooth overlapping group- ‚Ñì 1 regularizer, which is known to promote sparse solutions with respect to its groups. A feature that distinguishes our work from most in the literature is that the proximal operator for the overlapping group- ‚Ñì 1 function does not admit a closed-form solution, which introduces challenges when proving convergence of the iterates and especially when proving that the iterates correctly identify the group-sparse structure of the optimal solution. To address these challenges, we present an implementable termination condition for the proximal-gradient algorithm, and a specialized primal-dual subproblem solver that is designed to ensure that a group-sparse structure identification property holds. In particular, we give an upper bound on the maximum number of iterations before the correct support (i.e. group-sparse structure) of an optimal solution is identified.},
  archive      = {J_OMS},
  author       = {Yutong Dai and Daniel P. Robinson},
  doi          = {10.1080/10556788.2024.2346643},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {810-844},
  shortjournal = {Optim. Methods Softw.},
  title        = {A proximal-gradient method for problems with overlapping group-sparse regularization: Support identification complexity},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On generalized nash equilibrium problems in
infinite-dimensional spaces using nikaido‚Äìisoda type functionals.
<em>OMS</em>, <em>39</em>(4), 779‚Äì809. (<a
href="https://doi.org/10.1080/10556788.2024.2320736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an analysis of generalized Nash equilibrium problems in infinite-dimensional spaces with possibly non-convex objective functions of the players. Such settings arise, for instance, in games that involve nonlinear partial differential equation constraints. Due to non-convexity, we work with equilibrium concepts that build on first order optimality conditions, especially Quasi-Nash Equilibria (QNE), i.e. first-order optimality conditions for (Generalized) Nash Equilibria, and Variational Equilibria (VE), i.e. first-order optimality conditions for Normalized Nash Equilibria. We prove existence of these types of equilibria and study characterizations of them via regularized (and localized) Nikaido-Isoda merit functions. We also develop continuity and (continuous) differentiability results for these merit functions under quite weak assumptions, using a generalization of Danskin&#39;s theorem. They provide a theoretical foundation for, e.g. using globalized descent methods for computing QNE or VE.},
  archive      = {J_OMS},
  author       = {Michael Ulbrich and Julia Fritz},
  doi          = {10.1080/10556788.2024.2320736},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {779-809},
  shortjournal = {Optim. Methods Softw.},
  title        = {On generalized nash equilibrium problems in infinite-dimensional spaces using Nikaido‚ÄìIsoda type functionals},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bilevel optimization with a multi-objective lower-level
problem: Risk-neutral and risk-averse formulations. <em>OMS</em>,
<em>39</em>(4), 756‚Äì778. (<a
href="https://doi.org/10.1080/10556788.2024.2318707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose different formulations and gradient-based algorithms for deterministic and stochastic bilevel problems with conflicting objectives in the lower level. Such problems have received little attention in the deterministic case and have never been studied from a stochastic approximation viewpoint despite the recent advances in stochastic methods for single-level, bilevel, and multi-objective optimization. To solve bilevel problems with a multi-objective lower level, different approaches can be considered depending on the interpretation of the lower-level optimality. An optimistic formulation that was previously introduced for the deterministic case consists of minimizing the upper-level function over all non-dominated lower-level solutions. In this paper, we develop new risk-neutral and risk-averse formulations, address their main computational challenges, and develop the corresponding deterministic and stochastic gradient-based algorithms.},
  archive      = {J_OMS},
  author       = {T. Giovannelli and G. D. Kent and L. N. Vicente},
  doi          = {10.1080/10556788.2024.2318707},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {756-778},
  shortjournal = {Optim. Methods Softw.},
  title        = {Bilevel optimization with a multi-objective lower-level problem: Risk-neutral and risk-averse formulations},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating linear, semidefinite, and second-order cone
optimization problems for numerical experiments. <em>OMS</em>,
<em>39</em>(4), 725‚Äì755. (<a
href="https://doi.org/10.1080/10556788.2024.2308677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The numerical performance of algorithms can be studied using test sets or procedures that generate such problems. This paper proposes various methods for generating linear, semidefinite, and second-order cone optimization problems. Specifically, we are interested in problem instances requiring a known optimal solution, a known optimal partition, a specific interior solution, or all these together. In the proposed problem generators, different characteristics of optimization problems, including dimension, size, condition number, degeneracy, optimal partition, and sparsity, can be chosen to facilitate comprehensive computational experiments. We also develop procedures to generate instances with a maximally complementary optimal solution with a predetermined optimal partition to generate challenging semidefinite and second-order cone optimization problems. Generated instances enable us to evaluate efficient interior-point methods for conic optimization problems.},
  archive      = {J_OMS},
  author       = {Mohammadhossein Mohammadisiahroudi and Ramin Fakhimi and Brandon Augustino and Tam√°s Terlaky},
  doi          = {10.1080/10556788.2024.2308677},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {725-755},
  shortjournal = {Optim. Methods Softw.},
  title        = {Generating linear, semidefinite, and second-order cone optimization problems for numerical experiments},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized convergence of stochastic gradient descent by
weighted averaging. <em>OMS</em>, <em>39</em>(4), 699‚Äì724. (<a
href="https://doi.org/10.1080/10556788.2024.2306383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under mild assumptions stochastic gradient methods asymptotically achieve an optimal rate of convergence if the arithmetic mean of all iterates is returned as an approximate optimal solution. However, in the absence of stochastic noise, the arithmetic mean of all iterates converges considerably slower to the optimal solution than the iterates themselves. And also in the presence of noise, when a termination of the stochastic gradient method after a finite number of steps is considered, the arithmetic mean is not necessarily the best possible approximation to the unknown optimal solution. This paper aims at identifying optimal strategies in a particularly simple case, the minimization of a strongly convex function with i. i. d. noise terms and termination after a finite number of steps. Explicit formulas for the stochastic error and the optimality error are derived in dependence of certain parameters of the SGD method. The aim was to choose parameters such that both stochastic error and optimality error are reduced compared to arithmetic averaging. This aim could not be achieved; however, by allowing a slight increase of the stochastic error it was possible to select the parameters such that a significant reduction of the optimality error could be achieved. This reduction of the optimality error has a strong effect on the approximate solution generated by the stochastic gradient method in case that only a moderate number of iterations is used or when the initial error is large. The numerical examples confirm the theoretical results and suggest that a generalization to non-quadratic objective functions may be possible.},
  archive      = {J_OMS},
  author       = {Melinda Hagedorn and Florian Jarre},
  doi          = {10.1080/10556788.2024.2306383},
  journal      = {Optimization Methods and Software},
  month        = {7},
  number       = {4},
  pages        = {699-724},
  shortjournal = {Optim. Methods Softw.},
  title        = {Optimized convergence of stochastic gradient descent by weighted averaging},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal inexactness schedules for tunable oracle-based
methods. <em>OMS</em>, <em>39</em>(3), 664‚Äì698. (<a
href="https://doi.org/10.1080/10556788.2023.2296982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several recent works address the impact of inexact oracles in the convergence analysis of modern first-order optimization techniques, e.g. Bregman Proximal Gradient and Prox-Linear methods as well as their accelerated variants, extending their field of applicability. In this paper, we consider situations where the oracle&#39;s inexactness can be chosen upon demand, more precision coming at a computational price counterpart. Our main motivations arise from oracles requiring the solving of auxiliary subproblems or the inexact computation of involved quantities, e.g. a mini-batch stochastic gradient as a full-gradient estimate. We propose optimal inexactness schedules according to presumed oracle cost models and patterns of worst-case guarantees, covering among others convergence results of the aforementioned methods under the presence of inexactness. Specifically, we detail how to choose the level of inexactness at each iteration to obtain the best trade-off between convergence and computational investments. Furthermore, we highlight the benefits one can expect by tuning those oracles&#39; accuracy¬†instead of keeping it constant throughout. Finally, we provide extensive numerical experiments that support the practical interest of our approach, both in offline and online settings, applied to the Fast Gradient algorithm.},
  archive      = {J_OMS},
  author       = {Guillaume Van Dessel and Fran√ßois Glineur},
  doi          = {10.1080/10556788.2023.2296982},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {664-698},
  shortjournal = {Optim. Methods Softw.},
  title        = {Optimal inexactness schedules for tunable oracle-based methods},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Second-order cone programming for frictional contact
mechanics using interior point algorithm. <em>OMS</em>, <em>39</em>(3),
634‚Äì663. (<a
href="https://doi.org/10.1080/10556788.2023.2296438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We report experiments of an implementation of a primal‚Äìdual interior point algorithm for solving mechanical models of one-sided contact problems with Coulomb friction. The objective is to recover an optimal solution with high accuracy and as quickly as possible. These developments are part of the design of Siconos, an open-source software for modelling and simulating non-smooth dynamical systems. Currently, Siconos uses mainly first-order methods for the numerical solution of these systems. These methods are very robust, but suffer from a linear rate of convergence and are therefore too slow to find accurate solutions in a reasonable time. Our main objective is to apply second-order methods to speed up convergence. In this paper, we focus on solving a relaxed formulation of the initial mechanical model, corresponding to the optimality conditions of a convex quadratic minimization problem with second-order cone constraints. We will present in detail a primal‚Äìdual interior point algorithm for solving this type of problem. The main difficulty in implementing this algorithm arises from the fact that at each iteration of the algorithm, a change of variable, called a scaling, has to be performed in order to guarantee the non-singularity of the linear system to be solved, as well as to recover a symmetric system. Although this scaling strategy is very nice from a theoretical point of view, it leads to an enormous deterioration of the conditioning of the linear system when approaching the optimal solution and therefore to all the numerical difficulties that result from it. We will describe in detail the numerical algebra that we have developed in our implementation, in order to overcome these problems of numerical instability. We will also present the solution of the models resulting from the problems of rolling friction, for which the cone of constraints is no longer self-dual like the Lorentz cone.},
  archive      = {J_OMS},
  author       = {Vincent Acary and Paul Armand and Hoang Minh Nguyen and Maksym Shpakovych},
  doi          = {10.1080/10556788.2023.2296438},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {634-663},
  shortjournal = {Optim. Methods Softw.},
  title        = {Second-order cone programming for frictional contact mechanics using interior point algorithm},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Barzilai‚Äìborwein-like rules in proximal gradient schemes for
‚Ñì1-regularized problems. <em>OMS</em>, <em>39</em>(3), 601‚Äì633. (<a
href="https://doi.org/10.1080/10556788.2023.2285489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel steplength selection rule in proximal gradient methods for minimizing the sum of a differentiable function plus an ‚Ñì 1 -norm penalty term. The proposed rule modifies one of the classical Barzilai‚ÄìBorwein steplength, extending analogous results obtained in the context of gradient projection methods for constrained optimization. We analyse the spectral properties of the Barzilai‚ÄìBorwein-like steplength when the differentiable part is quadratic, showing that its reciprocal lies in the spectrum of the submatrix of the Hessian that depends on both the nonzero and the nonoptimal zero components of the current iterate, allowing for acceleration effects when the optimal zero components start to be identified. Furthermore, we insert the modified rule into a proximal gradient method with a nonmonotone line search, for which we prove global convergence towards a stationary point. Numerical experiments show the ability of the proposed rule to sweep the spectrum of the reduced Hessian on a series of quadratic ‚Ñì 1 -regularized problems, as well as its effectiveness in recovering the ground truth in a least squares regularized problem arising in image restoration.},
  archive      = {J_OMS},
  author       = {Serena Crisci and Simone Rebegoldi and Gerardo Toraldo and Marco Viola},
  doi          = {10.1080/10556788.2023.2285489},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {601-633},
  shortjournal = {Optim. Methods Softw.},
  title        = {Barzilai‚ÄìBorwein-like rules in proximal gradient schemes for ‚Ñì1-regularized problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning graph laplacian with MCP. <em>OMS</em>,
<em>39</em>(3), 569‚Äì600. (<a
href="https://doi.org/10.1080/10556788.2023.2269594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning a graph under the Laplacian constraint with a non-convex penalty: minimax concave penalty (MCP). For solving the MCP penalized graphical model, we design an inexact proximal difference-of-convex algorithm (DCA) and prove its convergence to critical points. We note that each subproblem of the proximal DCA enjoys the nice property that the objective function in its dual problem is continuously differentiable with a semismooth gradient. Therefore, we apply an efficient semismooth Newton method to subproblems of the proximal DCA. Numerical experiments on various synthetic and real data sets demonstrate the effectiveness of the non-convex penalty MCP in promoting sparsity. Compared with the existing state-of-the-art method, our method is demonstrated to be more efficient and reliable for learning graph Laplacian with MCP.},
  archive      = {J_OMS},
  author       = {Yangjing Zhang and Kim-Chuan Toh and Defeng Sun},
  doi          = {10.1080/10556788.2023.2269594},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {569-600},
  shortjournal = {Optim. Methods Softw.},
  title        = {Learning graph laplacian with MCP},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid direct search and projected simplex gradient method
for convex constrained minimization. <em>OMS</em>, <em>39</em>(3),
534‚Äì568. (<a
href="https://doi.org/10.1080/10556788.2023.2263618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new Derivative-free Optimization (DFO) approach for solving convex constrained minimization problems. The feasible set is assumed to be the non-empty intersection of a finite collection of closed convex sets, such that the projection onto each of these individual convex sets is simple and inexpensive to compute. Our iterative approach alternates between steps that use Directional Direct Search (DDS), considering adequate poll directions, and a Spectral Projected Gradient (SPG) method, replacing the real gradient by a simplex gradient, under a DFO approach. In the SPG steps, if the convex feasible set is simple, then a direct projection is computed. If the feasible set is the intersection of finitely many convex simple sets, then Dykstra&#39;s alternating projection method is applied. Convergence properties are established under standard assumptions usually associated to DFO methods. Some preliminary numerical experiments are reported to illustrate the performance of the proposed algorithm, in particular by comparing it with a classical DDS method. Our results indicate that the hybrid algorithm is a robust and effective approach for derivative-free convex constrained optimization.},
  archive      = {J_OMS},
  author       = {A. L. Cust√≥dio and E. H. M. Krulikovski and M. Raydan},
  doi          = {10.1080/10556788.2023.2263618},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {534-568},
  shortjournal = {Optim. Methods Softw.},
  title        = {A hybrid direct search and projected simplex gradient method for convex constrained minimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). More on second-order properties of the moreau
regularization-approximation of a convex function. <em>OMS</em>,
<em>39</em>(3), 519‚Äì533. (<a
href="https://doi.org/10.1080/10556788.2023.2258258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We unify and improve existing results on the second-order differentiabilty of the so-called M OREAU &#39;s regularization of a convex function.},
  archive      = {J_OMS},
  author       = {J.-B. Hiriart-Urruty},
  doi          = {10.1080/10556788.2023.2258258},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {519-533},
  shortjournal = {Optim. Methods Softw.},
  title        = {More on second-order properties of the moreau regularization-approximation of a convex function},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A highly efficient algorithm for solving exclusive lasso
problems. <em>OMS</em>, <em>39</em>(3), 489‚Äì518. (<a
href="https://doi.org/10.1080/10556788.2023.2253356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exclusive lasso (also known as elitist lasso) regularizer has become popular recently due to its superior performance on intra-group feature selection. Its complex nature poses difficulties for the computation of high-dimensional machine learning models involving such a regularizer. In this paper, we propose a highly efficient dual Newton method based proximal point algorithm (PPDNA) for solving large-scale exclusive lasso models. As important ingredients, we systematically study the proximal mapping of the weighted exclusive lasso regularizer and the corresponding generalized Jacobian. These results also make popular first-order algorithms for solving exclusive lasso models more practical. Extensive numerical results are presented to demonstrate the superior performance of the PPDNA against other popular numerical algorithms for solving the exclusive lasso problems.},
  archive      = {J_OMS},
  author       = {Meixia Lin and Yancheng Yuan and Defeng Sun and Kim-Chuan Toh},
  doi          = {10.1080/10556788.2023.2253356},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {489-518},
  shortjournal = {Optim. Methods Softw.},
  title        = {A highly efficient algorithm for solving exclusive lasso problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A class of projected-search methods for bound-constrained
optimization. <em>OMS</em>, <em>39</em>(3), 459‚Äì488. (<a
href="https://doi.org/10.1080/10556788.2023.2241769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projected-search methods for bound-constrained optimization are based on performing a search along a piecewise-linear continuous path obtained by projecting a search direction onto the feasible region. A potential benefit of a projected-search method is that many changes to the active set can be made at the cost of computing a single search direction. As the objective function is not differentiable along the search path, it is not possible to use a projected-search method with a step that satisfies the Wolfe conditions, which require the directional derivative of the objective function at a point on the path. For this reason, methods based in full or in part on a simple backtracking procedure must be used to give a step that satisfies an ‚ÄòArmijo-like‚Äô sufficient decrease condition. As a consequence, conventional projected-search methods are unable to exploit sophisticated safeguarded polynomial interpolation techniques that have been shown to be effective for the unconstrained case. This paper describes a new framework for the development of a general class of projected-search methods for bound-constrained optimization. At each iteration, a descent direction is computed with respect to a certain extended active set. This direction is used to specify a search direction that is used in conjunction with a step length computed by a quasi- Wolfe search. The quasi-Wolfe search is designed specifically for use with a piecewise-linear search path and is similar to a conventional Wolfe line search, except that a step is accepted under a wider range of conditions. These conditions take into consideration steps at which the restriction of the objective function on the search path is not differentiable. Standard existence and convergence results associated with a conventional Wolfe line search are extended to the quasi-Wolfe case. In addition, it is shown that under a standard nondegeneracy assumption, any method within the framework will identify the optimal active set in a finite number of iterations. Computational results are given for a specific projected-search method that uses a limited-memory quasi-Newton approximation of the Hessian. The results show that, in this context, a quasi-Wolfe search is substantially more efficient and reliable than an Armijo-like search based on simple backtracking. Comparisons with a state-of-the-art bound-constrained optimization package are also presented.},
  archive      = {J_OMS},
  author       = {Michael W. Ferry and Philip E. Gill and Elizabeth Wong and Minxin Zhang},
  doi          = {10.1080/10556788.2023.2241769},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {459-488},
  shortjournal = {Optim. Methods Softw.},
  title        = {A class of projected-search methods for bound-constrained optimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preface for the special edition of optimization methods and
software. <em>OMS</em>, <em>39</em>(3), 457‚Äì458. (<a
href="https://doi.org/10.1080/10556788.2024.2406663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  author       = {Serge Gratton and Philippe Toint and Jacek Gondzio and Yurii Nesterov and Ya-xiang Yuan},
  doi          = {10.1080/10556788.2024.2406663},
  journal      = {Optimization Methods and Software},
  month        = {5},
  number       = {3},
  pages        = {457-458},
  shortjournal = {Optim. Methods Softw.},
  title        = {Preface for the special edition of optimization methods and software},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-operator reflected forward-backward splitting
algorithm with double inertial effects. <em>OMS</em>, <em>39</em>(2),
431‚Äì456. (<a
href="https://doi.org/10.1080/10556788.2024.2307470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a reflected forward-backward splitting algorithm with two different inertial extrapolations to find a zero of the sum of three monotone operators consisting of the maximal monotone operator, Lipschitz continuous monotone operator, and a cocoercive operator in real Hilbert spaces. One of the interesting features of the proposed algorithm is that both the Lipschitz continuous monotone operator and the cocoercive operator are computed explicitly each with one evaluation per iteration. We then obtain weak and strong convergence results under some mild conditions. We finally give a numerical illustration to show that our proposed algorithm is effective and competitive with other related algorithms in the literature.},
  archive      = {J_OMS},
  author       = {Qiao-Li Dong and Min Su and Yekini Shehu},
  doi          = {10.1080/10556788.2024.2307470},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {431-456},
  shortjournal = {Optim. Methods Softw.},
  title        = {Three-operator reflected forward-backward splitting algorithm with double inertial effects},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-step new modulus-based matrix splitting method for
vertical linear complementarity problem. <em>OMS</em>, <em>39</em>(2),
414‚Äì430. (<a
href="https://doi.org/10.1080/10556788.2023.2278090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, for solving the vertical linear complementarity problem (VLCP) effectively, a two-step new modulus-based matrix splitting (TNMMS) iteration method is introduced. Its convergence properties are discussed in depth. The efficiency of the proposed method is confirmed by some numerical experiments from the 2-D boundary problem. Numerical results show that the proposed method is superior to some existing methods, such as the two modulus-based matrix splitting (TMMS) iteration method (Numer. Algor., 2011, 57: 83‚Äì99) and the new modulus-based matrix splitting (NMMS) iteration method (Optim., 2023, 72:¬†2499‚Äì2516).},
  archive      = {J_OMS},
  author       = {Cuixia Li and Shiliang Wu},
  doi          = {10.1080/10556788.2023.2278090},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {414-430},
  shortjournal = {Optim. Methods Softw.},
  title        = {A two-step new modulus-based matrix splitting method for vertical linear complementarity problem},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence analysis of stochastic higher-order
majorization‚Äìminimization algorithms. <em>OMS</em>, <em>39</em>(2),
384‚Äì413. (<a
href="https://doi.org/10.1080/10556788.2023.2256447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Majorization‚Äìminimization schemes are a broad class of iterative methods targeting general optimization problems, including nonconvex, nonsmooth and stochastic. These algorithms minimize successively a sequence of upper bounds of the objective function so that along the iterations the objective value decreases. We present a stochastic higher-order algorithmic framework for minimizing the average of a very large number of sufficiently smooth functions. Our stochastic framework is based on the notion of stochastic higher-order upper bound approximations of the finite-sum objective function and minibatching. We derive convergence results for nonconvex and convex optimization problems when the higher-order approximation of the objective function yields an error that is p times differentiable and has Lipschitz continuous p derivative. More precisely, for general nonconvex problems we present asymptotic stationary point guarantees and under Kurdyka‚ÄìLojasiewicz property we derive local convergence rates ranging from sublinear to linear. For convex problems with uniformly convex objective function, we derive local (super)linear convergence results for our algorithm. Compared to existing stochastic (first-order) methods, our algorithm adapts to the problem&#39;s curvature and allows using any batch size. Preliminary numerical tests support the effectiveness of our algorithmic framework.},
  archive      = {J_OMS},
  author       = {Daniela Lupu and Ion Necoara},
  doi          = {10.1080/10556788.2023.2256447},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {384-413},
  shortjournal = {Optim. Methods Softw.},
  title        = {Convergence analysis of stochastic higher-order majorization‚Äìminimization algorithms},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The use of a family of gerstewitz scalarization functions in
the context of vector optimization with variable domination structures
to derive scalarization results. <em>OMS</em>, <em>39</em>(2), 368‚Äì383.
(<a href="https://doi.org/10.1080/10556788.2023.2296440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a nonlinear scalarization function for a variable domination structure in an arbitrary linear space without assuming any particular topology. Conditions are provided under which the nonlinear scalarization function possesses several useful properties such as finiteness, properness, positive homogeneity, subadditivity, (strict) monotonicity, convexity or continuity. These properties are employed to characterize approximate efficiency in linear spaces.},
  archive      = {J_OMS},
  author       = {Lam Quoc Anh and Tran Ngoc Tam},
  doi          = {10.1080/10556788.2023.2296440},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {368-383},
  shortjournal = {Optim. Methods Softw.},
  title        = {The use of a family of gerstewitz scalarization functions in the context of vector optimization with variable domination structures to derive scalarization results},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental quasi-newton algorithms for solving a nonconvex,
nonsmooth, finite-sum optimization problem. <em>OMS</em>,
<em>39</em>(2), 345‚Äì367. (<a
href="https://doi.org/10.1080/10556788.2023.2296432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms for solving certain nonconvex, nonsmooth, finite-sum optimization problems are proposed and tested. In particular, the algorithms are proposed and tested in the context of a transductive support vector machine (TSVM) problem formulation arising in semi-supervised machine learning. The common feature of all algorithms is that they employ an incremental quasi-Newton (IQN) strategy, specifically an incremental BFGS (IBFGS) strategy. One applies an IBFGS strategy to the problem directly, whereas the others apply an IBFGS strategy to a difference-of-convex reformulation, smoothed approximation, or (strongly) convex local approximation. Experiments show that all IBFGS approaches fare well in practice, and all outperform a state-of-the-art bundle method when solving TSVM problem instances.},
  archive      = {J_OMS},
  author       = {Gulcin Dinc Yalcin and Frank E. Curtis},
  doi          = {10.1080/10556788.2023.2296432},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {345-367},
  shortjournal = {Optim. Methods Softw.},
  title        = {Incremental quasi-newton algorithms for solving a nonconvex, nonsmooth, finite-sum optimization problem},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ADMM based method for underdetermined box-constrained
integer least squares problems. <em>OMS</em>, <em>39</em>(2), 321‚Äì344.
(<a href="https://doi.org/10.1080/10556788.2023.2285492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve underdetermined box-constrained integer least squares (UBILS) problems, we propose an integer-constrained alternating direction method of multipliers (IADMM), which can be much more accurate than the ADMM method. To guarantee to find the optimal solution, then we incorporate IADMM to DTS, a tree search method, to make the latter more efficient. Numerical tests show that the combined method IADMM-DTS can be much faster than the original DTS method. Finally, we apply the combined method to a practical communication problem. Numerical results indicate that IADMM-DTS typically performs better than the commercial solvers CPLEX and MOSEK in terms of both efficiency and accuracy, and it can be used as an alternative to the commercial solver Gurobi for UBILS problems.},
  archive      = {J_OMS},
  author       = {Xiao-Wen Chang and Tianchi Ma},
  doi          = {10.1080/10556788.2023.2285492},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {321-344},
  shortjournal = {Optim. Methods Softw.},
  title        = {An ADMM based method for underdetermined box-constrained integer least squares problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PathWyse: A flexible, open-source library for the resource
constrained shortest path problem. <em>OMS</em>, <em>39</em>(2),
298‚Äì320. (<a
href="https://doi.org/10.1080/10556788.2023.2296978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a fundamental and hard combinatorial problem: the Resource Constrained Shortest Path Problem (RCSPP). We describe the implementation of a flexible, open-source library for the solution of the RCSPP, called PathWyse , capable of tackling several variants of the problem. We designed PathWyse with the final user in mind, developing easy-to-use interfaces without compromising performance. We provide computational experiments on three classes of instances of the RCSPP, namely RCSPP on cyclic networks, RCSPP on large acyclic networks, and RCSPP on ad-hoc cyclic networks. We show that PathWyse is packed off-the-shelf with algorithms capable of tackling generic problems, and can exploit dedicated algorithms for specific classes. This paper represents the first step along the journey of devising and implementing a comprehensive open-source library for a large variety of RCSPPs. The current version of the library carries exact algorithms for the RCSPP but new algorithms, both heuristic and exact, will be added thanks to the flexible design. We also foresee PathWyse becoming a platform ready for data-driven and process-driven methodologies for these types of problems.},
  archive      = {J_OMS},
  author       = {Matteo Salani and Saverio Basso and Vincenzo Giuffrida},
  doi          = {10.1080/10556788.2023.2296978},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {298-320},
  shortjournal = {Optim. Methods Softw.},
  title        = {PathWyse: A flexible, open-source library for the resource constrained shortest path problem},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Study on precoding optimization algorithms in massive MIMO
system with multi-antenna users. <em>OMS</em>, <em>39</em>(2), 282‚Äì297.
(<a href="https://doi.org/10.1080/10556788.2022.2091564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper studies the multi-user precoding problem as a non-convex optimization problem for wireless multiple inputs and multiple outputs (MIMO) systems. In our work, we approximate the target Spectral Efficiency function with a novel computationally simpler function. Then, we reduce the precoding problem to an unconstrained optimization task using a special differential projection method and solve it by the Quasi-Newton L-BFGS iterative procedure to achieve gains in capacity. We are testing the proposed approach in several scenarios generated using Quadriga-open-source software for generating realistic radio channel impulse response. Our method shows monotonic improvement over heuristic methods with reasonable computation time. The proposed L-BFGS optimization scheme is novel in this area and shows a significant advantage over the standard approaches. The proposed method has simple implementation and can be a good reference for other heuristic algorithms in this field.},
  archive      = {J_OMS},
  author       = {Evgeny Bobrov and Dmitry Kropotov and Sergey Troshin and Danila Zaev},
  doi          = {10.1080/10556788.2022.2091564},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {282-297},
  shortjournal = {Optim. Methods Softw.},
  title        = {Study on precoding optimization algorithms in massive MIMO system with multi-antenna users},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Customized douglas-rachford splitting methods for structured
inverse variational inequality problems. <em>OMS</em>, <em>39</em>(2),
257‚Äì281. (<a
href="https://doi.org/10.1080/10556788.2023.2278092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, structured inverse variational inequality (SIVI) problems have attracted much attention. In this paper, we propose new splitting methods to solve SIVI problems by employing the idea of the classical Douglas-Rachford splitting method (DRSM). In particular, the proposed methods can be regarded as a novel application of the DRSM to SIVI problems by decoupling the linear equality constraint, leading to smaller and easier subproblems. The main computational tasks per iteration are the evaluations of certain resolvent operators, which are much cheaper than those methods without taking advantage of the problem structures. To make the methods more implementable in the general cases where the resolvent operator is evaluated in an iterative scheme, we further propose to solve the subproblems in an approximate manner. Under quite mild conditions, global convergence, sublinear rate of convergence, and linear rate of convergence results are established for both the exact and the inexact methods. Finally, we present preliminary numerical results to illustrate the performance of the proposed methods.},
  archive      = {J_OMS},
  author       = {Y. N. Jiang and X. J. Cai and D. R. Han and J. F. Yang},
  doi          = {10.1080/10556788.2023.2278092},
  journal      = {Optimization Methods and Software},
  month        = {3},
  number       = {2},
  pages        = {257-281},
  shortjournal = {Optim. Methods Softw.},
  title        = {Customized douglas-rachford splitting methods for structured inverse variational inequality problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GBOML: A structure-exploiting optimization modelling
language in python. <em>OMS</em>, <em>39</em>(1), 227‚Äì256. (<a
href="https://doi.org/10.1080/10556788.2023.2246169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed-Integer Linear Programs (MILPs) have many practical applications. Most modelling tools for MILPs fall in two broad categories. Indeed, tools such as algebraic modelling languages allow practitioners to compactly encode models using syntax close to mathematical notation but usually lack support for special structures, while other tools instead provide predefined components that can be easily assembled but modifying or adding new components is difficult. In this work, we present the inner workings of the Graph-Based Optimization Modelling Language (GBOML), an open-source modelling tool implemented in Python combining the strengths of both worlds. GBOML natively supports special structures that can be encoded by a hierarchical hypergraph, offers syntax close to mathematical notation and facilitates the modular construction and reuse of time-indexed models. We detail design choices enabling these features and show that they simplify problem encoding, lead to faster instance generation times and sometimes faster solve times. We benchmark the times taken by GBOML, JuMP, Plasmo, Pyomo and AMPL to generate instances of a structured MILP. We find that GBOML outperforms Plasmo and Pyomo, is tied with JuMP but is slower than AMPL. With parallel model generation, GBOML outperforms JuMP and closes the gap with AMPL. GBOML has the smallest memory footprint.},
  archive      = {J_OMS},
  author       = {Bardhyl Miftari and Mathias Berger and Guillaume Derval and Quentin Louveaux and Damien Ernst},
  doi          = {10.1080/10556788.2023.2246169},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {227-256},
  shortjournal = {Optim. Methods Softw.},
  title        = {GBOML: A structure-exploiting optimization modelling language in python},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new inertial projected reflected gradient method with
application to optimal control problems. <em>OMS</em>, <em>39</em>(1),
197‚Äì226. (<a
href="https://doi.org/10.1080/10556788.2023.2246168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The projected reflected gradient method has been shown to be a simple and elegant method for solving variational inequalities. The method involves one projection onto the feasible set and one evaluation of the cost operator per iteration and has been shown numerically to be more efficient than most available methods for solving variational inequalities. Convergence results for methods with similar elegant structures of projected reflected gradient method are still rare. In this paper, we present weak and linear convergence of a projected reflected gradient method with an inertial extrapolation step and give some applications arising from optimal control problems. We first obtain weak convergence result for the projected reflected gradient method with an inertial extrapolation step for solving variational inequalities under standard assumptions with self-adaptive step sizes. We further obtain a linear convergence rate when the cost operator is strongly monotone and Lipschitz continuous. Finally, we give some numerical applications arising from optimal control. Preliminary results show that our method is effective and efficient when compared to other related state-of-the-art methods in the literature and show the advantage gained by incorporating inertial terms into the projected reflected gradient methods.},
  archive      = {J_OMS},
  author       = {Chinedu Izuchukwu and Yekini Shehu},
  doi          = {10.1080/10556788.2023.2246168},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {197-226},
  shortjournal = {Optim. Methods Softw.},
  title        = {A new inertial projected reflected gradient method with application to optimal control problems},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IntSat: Integer linear programming by conflict-driven
constraint learning. <em>OMS</em>, <em>39</em>(1), 169‚Äì196. (<a
href="https://doi.org/10.1080/10556788.2023.2246167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art SAT solvers are nowadays able to handle huge real-world instances. The key to this success is the Conflict-Driven Clause-Learning (CDCL) scheme, which encompasses a number of techniques that exploit the conflicts that are encountered during the search for a solution. In this article, we extend these techniques to Integer Linear Programming (ILP), where variables may take general integer values instead of purely binary ones, constraints are more expressive than just propositional clauses, and there may be an objective function to optimize. We explain how these methods can be implemented efficiently and discuss possible improvements. Our work is backed with a basic implementation showing that, even in this far less mature stage, our techniques are already a useful complement to the state of the art in ILP.},
  archive      = {J_OMS},
  author       = {Robert Nieuwenhuis and Albert Oliveras and Enric Rodr√≠guez-Carbonell},
  doi          = {10.1080/10556788.2023.2246167},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {169-196},
  shortjournal = {Optim. Methods Softw.},
  title        = {IntSat: Integer linear programming by conflict-driven constraint learning},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributionally robust joint chance-constrained programming
with wasserstein metric. <em>OMS</em>, <em>39</em>(1), 134‚Äì168. (<a
href="https://doi.org/10.1080/10556788.2023.2241149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop an exact reformulation and a deterministic approximation for distributionally robust joint chance-constrained programmings ( DRCCPs ) ( DRCCPs ) (DRCCPs) with a general class of convex uncertain constraints under data-driven Wasserstein ambiguity sets. It is known that robust chance constraints can be conservatively approximated by worst-case conditional value-at-risk (CVaR) constraints. It is shown that the proposed worst-case CVaR approximation model can be reformulated as an optimization problem involving biconvex constraints for joint DRCCP. This approximation is essentially exact under certain conditions. We derive a convex relaxation of this approximation model by constructing new decision variables which allows us to eliminate biconvex terms. Specifically, when the constraint function is affine in both the decision variable and the uncertainty, the resulting approximation model is equivalent to a tractable mixed-integer convex reformulation for joint binary DRCCP. Numerical results illustrate the computational effectiveness and superiority of the proposed formulations.},
  archive      = {J_OMS},
  author       = {Yining Gu and Yanjun Wang},
  doi          = {10.1080/10556788.2023.2241149},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {134-168},
  shortjournal = {Optim. Methods Softw.},
  title        = {Distributionally robust joint chance-constrained programming with wasserstein metric},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensorial total variation-based image and video restoration
with optimized projection methods. <em>OMS</em>, <em>39</em>(1),
102‚Äì133. (<a
href="https://doi.org/10.1080/10556788.2022.2053971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The total variation regularization method was introduced by Rudin, Osher, and Fatemi as an efficient technique for regularizing grayscale images. In this work, we aimed to generalize the total variation method to regularize multidimensional problems such as colour image and video restoration. A degradation model in a tensor format is proposed to recover blurred and noisy colour images and videos. The alternating direction method for multipliers ( ADMM ) and an optimized form of projection methods have been employed to solve the tensorial total variation minimization problem. The structure of the developed approach allows the selection of the optimal parameter. We use the truncated SVD ( TSVD ) to reduce the size of the problem and to accelerate the convergence of the algorithm. The convergence analysis of the proposed method is proved using convex optimization. Numerical tests for image and video restoration are given showing the effectiveness of the proposed approaches.},
  archive      = {J_OMS},
  author       = {O. Benchettou and A. H. Bentbib and A. Bouhamidi},
  doi          = {10.1080/10556788.2022.2053971},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {102-133},
  shortjournal = {Optim. Methods Softw.},
  title        = {Tensorial total variation-based image and video restoration with optimized projection methods},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual formulation of the sparsity constrained optimization
problem: Application to classification. <em>OMS</em>, <em>39</em>(1),
84‚Äì101. (<a
href="https://doi.org/10.1080/10556788.2023.2278091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the sparsity constrained optimization problem by resorting to polyhedral k -norm as a valid tool to emulate the ‚Ñì 0 ‚Ñì 0 ‚Ñì0 -pseudo-norm. The main novelty of the approach is the use of the dual of the k -norm, which allows to obtain a formulation amenable for a relaxation that can be efficiently handled by block coordinate methods. The advantage of the approach is that it does not require the solution of difference-of-convex programmes, unlike other k -norm based methods available in the literature. In fact, our block coordinate approach requires, at each iteration, the solution of two convex programmes, one of which can be solved in O ( nlog ‚Å° n ) time. We apply the method to feature selection within the framework of Support Vector Machine classification, and we report the results obtained on some benchmark test problems.},
  archive      = {J_OMS},
  author       = {M. Gaudioso and G. Giallombardo and J.-B. Hiriart-Urruty},
  doi          = {10.1080/10556788.2023.2278091},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {84-101},
  shortjournal = {Optim. Methods Softw.},
  title        = {Dual formulation of the sparsity constrained optimization problem: Application to classification},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inexact tensor methods and their application to stochastic
convex optimization. <em>OMS</em>, <em>39</em>(1), 42‚Äì83. (<a
href="https://doi.org/10.1080/10556788.2023.2261604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose general non-accelerated [The results for non-accelerated methods first appeared in December 2020 in the preprint (A. Agafonov, D. Kamzolov, P. Dvurechensky, and A. Gasnikov, Inexact tensor methods and their application to stochastic convex optimization , preprint 2020. arXiv:2012.15636)] and accelerated tensor methods under inexact information on the derivatives of the objective, analyse their convergence rate. Further, we provide conditions for the inexactness in each derivative that is sufficient for each algorithm to achieve a desired accuracy. As a corollary, we propose stochastic tensor methods for convex optimization and obtain sufficient mini-batch sizes for each derivative.},
  archive      = {J_OMS},
  author       = {Artem Agafonov and Dmitry Kamzolov and Pavel Dvurechensky and Alexander Gasnikov and Martin Tak√°ƒç},
  doi          = {10.1080/10556788.2023.2261604},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {42-83},
  shortjournal = {Optim. Methods Softw.},
  title        = {Inexact tensor methods and their application to stochastic convex optimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Techniques for accelerating branch-and-bound algorithms
dedicated to sparse optimization. <em>OMS</em>, <em>39</em>(1), 4‚Äì41.
(<a href="https://doi.org/10.1080/10556788.2023.2241154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse optimization‚Äìfitting data with a low-cardinality linear model‚Äìis addressed through the minimization of a cardinality-penalized least-squares function, for which dedicated branch-and-bound algorithms clearly outperform generic mixed-integer-programming solvers. Three acceleration techniques are proposed for such algorithms. Convex relaxation problems at each node are addressed with dual approaches, which can early prune suboptimal nodes. Screening methods are implemented, which fix variables to their optimal value during the node evaluation, reducing the subproblem size. Numerical experiments show that the efficiency of such techniques depends on the node cardinality and on the structure of the problem matrix. Last, different exploration strategies are proposed to schedule the nodes. Best-first search is shown to outperform the standard depth-first search used in the related literature. A new strategy is proposed which first explores the nodes with the lowest least-squares value, which is shown to be the best at finding the optimal solution‚Äìwithout proving its optimality. A C++ solver with compiling and usage instructions is made available.},
  archive      = {J_OMS},
  author       = {Gwena√´l Samain and S√©bastien Bourguignon and Jordan Ninin},
  doi          = {10.1080/10556788.2023.2241154},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {4-41},
  shortjournal = {Optim. Methods Softw.},
  title        = {Techniques for accelerating branch-and-bound algorithms dedicated to sparse optimization},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foreword. <em>OMS</em>, <em>39</em>(1), 1‚Äì3. (<a
href="https://doi.org/10.1080/10556788.2024.2353488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  author       = {Giancarlo Bigi and Sonia Cafieri and Nicolas Cou√´llan and Sophie Jan and Fr√©d√©ric Messine},
  doi          = {10.1080/10556788.2024.2353488},
  journal      = {Optimization Methods and Software},
  month        = {1},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Optim. Methods Softw.},
  title        = {Foreword},
  volume       = {39},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
