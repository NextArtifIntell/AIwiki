<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joas---169">JOAS - 169</h2>
<ul>
<li><details>
<summary>
(2024). Review of the third edition of sampling: Design and
analysis. <em>JOAS</em>, <em>51</em>(16), 3486–3494. (<a
href="https://doi.org/10.1080/02664763.2024.2346350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rising response rates, increasing prevalence of nonprobability samples, and new technologies have generated important challenges for survey sampling in recent years. Simultaneously, governmental and private organizations continue to rely on probability samples to understand the characteristics of target populations. Therefore, a reliable textbook on survey statistics that also addresses contemporary issues is of critical importance for statistics students and practitioners. The third edition of Sampling: Design and Analysis fulfills this need. The text offers tools to confront modern statistical challenges, while providing a solid foundation in the pillars of survey statistics. The text is reliable, builds intuition, and contains a rich set of examples. These qualities make this book appropriate for a wide range of students and practitioners.},
  archive      = {J_JOAS},
  author       = {Emily Berg},
  doi          = {10.1080/02664763.2024.2346350},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3486-3494},
  shortjournal = {J. Appl. Stat.},
  title        = {Review of the third edition of sampling: Design and analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Urinary volatile organic compounds (VOCs) based prostate
cancer diagnosis via high-dimensional classification. <em>JOAS</em>,
<em>51</em>(16), 3468–3485. (<a
href="https://doi.org/10.1080/02664763.2024.2346355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of prostate cancer is critical for successful treatment and survival. However, current diagnostic methods such as prostate-specific antigen (PSA) testing and digital rectal examination (DRE) have limitations in accuracy, specificity, and sensitivity. Recent research suggests that urinary volatile organic compounds (VOCs) could serve as potential biomarkers for prostate cancer diagnosis. In this study, urine samples from 337 PCa-positive and 233 PCa-negative patients were collected to develop a diagnosis model. The study involves a high dimensional (HD) classification problem due to the vast number of measured VOCs. Our findings reveal that regularized logistic regression outperforms numerous other classifiers when analyzing the collected data. In particular, we have selected a regularized logistic model with the SCAD (smoothly clipped absolute deviation) penalty as the final model, which attains an AUC (area under the ROC curve) of 0.748, in contrast to a PSA-based AUC of 0.540. These results underscore the potential of VOC-based diagnosis as a clinically feasible approach for PCa screening.},
  archive      = {J_JOAS},
  author       = {George Ekow Quaye and Wen-Yee Lee and Elizabeth Noriega Landa and Sabur Badmos and Kiana L. Holbrook and Xiaogang Su},
  doi          = {10.1080/02664763.2024.2346355},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3468-3485},
  shortjournal = {J. Appl. Stat.},
  title        = {Urinary volatile organic compounds (VOCs) based prostate cancer diagnosis via high-dimensional classification},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal double acceptance sampling plans based on truncated
life tests for tsallis q-exponential distributions. <em>JOAS</em>,
<em>51</em>(16), 3456–3467. (<a
href="https://doi.org/10.1080/02664763.2024.2353365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal double sampling plans for lot acceptance based on truncated life tests are derived by minimizing the expected sample size when the lifetime variable follows a Tsallis q -exponential distribution. The proposed test plans significantly reduce the inspection effort with respect to the best single sampling schemes. Some tables and figures are presented to analyze the behavior of the suggested test plans. The results show that the proposed lot inspection scheme clearly outperforms the standard single sampling plan. A justification is provided using a comparative study for existing double sampling plans. Finally, several applications are provided for illustrative purposes.},
  archive      = {J_JOAS},
  author       = {M. Naghizadeh Qomi and Arturo J. Fernández},
  doi          = {10.1080/02664763.2024.2353365},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3456-3467},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal double acceptance sampling plans based on truncated life tests for tsallis q-exponential distributions},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative smoothing for change-point regression function
estimation. <em>JOAS</em>, <em>51</em>(16), 3431–3455. (<a
href="https://doi.org/10.1080/02664763.2024.2352759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding wildfire spread in Canada is critical to promoting forest health and protecting human life and infrastructure. Quantifying fire spread from noisy images, where change-point boundaries separate regions of fire, is critical to accurately estimating fire spread rates. The challenge lies in denoising the fire images and accurately identifying highly non-linear fire lines without smoothing over boundaries. In this paper, we develop an iterative smoothing algorithm for change-point data that utilizes oversmoothed estimates of the underlying data generating process to inform re-smoothing. We demonstrate its effectiveness on simulated one- and two-dimensional change-point data, and robustness to response outliers. Then, we apply the methodology to fire spread images from laboratory micro-fire experiments and show that the regions fuel, burning and burnt-out are smoothed while boundaries are preserved.},
  archive      = {J_JOAS},
  author       = {John R. J. Thompson},
  doi          = {10.1080/02664763.2024.2352759},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3431-3455},
  shortjournal = {J. Appl. Stat.},
  title        = {Iterative smoothing for change-point regression function estimation},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a unified test for the intercept of autoregressive
models. <em>JOAS</em>, <em>51</em>(16), 3407–3430. (<a
href="https://doi.org/10.1080/02664763.2024.2352756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has long been an open problem to provide a unified test for the intercept of autoregressive (AR) models. In this paper, we use the empirical likelihood method to solve this issue. It turns out that the resulting test statistic always converges in distribution to a standard chi-squared distribution under the null hypothesis, whether the AR process is stationary or nonstationary, and with or without an intercept. The asymptotic distribution under the local alternative hypothesis is also derived under some mild conditions. Several simulations as well as a real data example are used to show how well the suggested test performs in terms of size and power on a finite sample.},
  archive      = {J_JOAS},
  author       = {Jing Zhang and Yawen Fan and Yu Wang and Xiaohui Liu and Bo Li},
  doi          = {10.1080/02664763.2024.2352756},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3407-3430},
  shortjournal = {J. Appl. Stat.},
  title        = {Towards a unified test for the intercept of autoregressive models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating software reliability using size-biased modelling.
<em>JOAS</em>, <em>51</em>(16), 3386–3406. (<a
href="https://doi.org/10.1080/02664763.2024.2352751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software testing is an important step in software development where inputs are administered repeatedly to detect bugs present in the software. In this paper, we have considered the estimation of total number of bugs and software reliability as a size-biased sampling problem by introducing the concept of eventual bug size as a latent variable. We have developed a Bayesian generalised linear mixed model (GLMM) using software testing detection data to estimate software reliability and stopping phase. The model uses size-biased approach where the probability of detecting a bug is an increasing function of eventual size of the bug which is as an index for the potential number of inputs that may eventually pass through the bug. We have tested the sensitivity of the reliability estimates by varying the number of inputs and detection probability via a simulation study and have found that the key parameters could be accurately estimated. Further, we have applied our model to two empirical data sets – one from a commercial software and the other from ISRO launch mission software testing data set. The hierarchical modelling approach provides a unified modelling framework that may find applications in other fields (e.g. hydrocarbon explorations) apart from software management.},
  archive      = {J_JOAS},
  author       = {Soumen Dey and Ashis Kumar Chakraborty},
  doi          = {10.1080/02664763.2024.2352751},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3386-3406},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating software reliability using size-biased modelling},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying waves of COVID-19 mortality using skew normal
curves. <em>JOAS</em>, <em>51</em>(16), 3366–3385. (<a
href="https://doi.org/10.1080/02664763.2024.2351467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model for multiple waves of an epidemic that decomposes the health outcome of interest into the sum of scaled skew normal curves. When applied to daily COVID-19 mortality in six regions (Japan, Italy, Belgium, Ontario, Texas, and Peru), this model provides three notable results. First, when fit to data from early 2020 to May 31, 2022, the estimated skew normal curves substantially overlap with the dates of COVID-19 waves in Ontario and Belgium, as determined by their respective health authorities. Second, the asymmetry of the skew normal curves changes over time – they progress from increasing more quickly to decreasing more quickly, indicating changes in the relative speed that daily COVID-19 mortality rises and falls over time. Third, most regions have day-of-the-week effects, which suggests that day-of-the-week effects should be included when modeling daily COVID-19 mortality. We conclude by discussing limitations and possible extensions of this model and its results, including commenting on its applicability to potential future COVID-19 waves.},
  archive      = {J_JOAS},
  author       = {Kamal Rai and Patrick E. Brown},
  doi          = {10.1080/02664763.2024.2351467},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3366-3385},
  shortjournal = {J. Appl. Stat.},
  title        = {Identifying waves of COVID-19 mortality using skew normal curves},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A supervised weeding method to cluster high dimensional
predictors with application to job market analysis. <em>JOAS</em>,
<em>51</em>(16), 3350–3365. (<a
href="https://doi.org/10.1080/02664763.2024.2348634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The clustering of high-dimensional predictors draws increasing attention in various scientific areas, such as text mining and biological data analysis. In standard clustering procedures, when predictors are clustered, they only showcase the inherent patterns within the predictor set, lacking the capacity to predict the response variable. To this end, a new supervised weeding algorithm is advocated to address the dual requirement of detecting sparse clusters and capturing the prediction effects. The proposed algorithm is based on an iterative feature screening and coherence evaluation procedure. It iteratively weeds off the unimportant predictors in a backward fashion, forming sequences of nested sets to determine data-driven optimal cut-offs. This study uses Monte Carlo simulation to assess the finite-sample performance of the proposed method. The findings demonstrate that both the clustering and prediction performance of the proposed method are comparable to existing methods that concentrate solely on one aspect of the dual targets. An analysis of a job description dataset is conducted to explore significant groups of keywords that affect employees&#39; salaries.},
  archive      = {J_JOAS},
  author       = {Yuyang Li and Jianxin Bi and Jingyuan Liu and Ying Yang},
  doi          = {10.1080/02664763.2024.2348634},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3350-3365},
  shortjournal = {J. Appl. Stat.},
  title        = {A supervised weeding method to cluster high dimensional predictors with application to job market analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KCBC – a correlation-based method for co-localization
analysis of super-resolution microscopy images using bivariate ripley’s
k functions. <em>JOAS</em>, <em>51</em>(16), 3333–3349. (<a
href="https://doi.org/10.1080/02664763.2024.2346828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the high demand for co-localization analysis methods for super-resolution microscopy images which are featured with nanoscale precise locational information of molecules, this paper establishes a novel correlation-based method, KCBC, named after the Coordinated-Based Colocalization (CBC) method proposed by Malkusch et al. in 2012, by using bivariate Ripley&#39;s K functions. The local KCBC values are to quantify the local spatial co-localization of molecules between two species by measuring the correlation of bivariate Ripley&#39;s K functions over equal-area concentric rings around the base species within a near distance. The mean of local KCBC values is proposed to quantify the co-localization degree of cross-channel to base-channel molecules for the whole image. It could effectively correct the false positives with reduced variance and increased power within the user-defined proximity size. We provide extensive simulation studies under different scenarios to demonstrate the unbiasedness of the KCBC method, and its ability to filter noise signals and random over-counting. Our real data application for super-resolution mitochondria image data illustrates the applicability of our methods with increased effectiveness and power.},
  archive      = {J_JOAS},
  author       = {Xueyan Liu and Stephan Komladzei and Clifford Guy},
  doi          = {10.1080/02664763.2024.2346828},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3333-3349},
  shortjournal = {J. Appl. Stat.},
  title        = {KCBC – a correlation-based method for co-localization analysis of super-resolution microscopy images using bivariate ripley&#39;s k functions},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The weighted sum of powers in mean for estimating a change
point in linear processes with random coefficients. <em>JOAS</em>,
<em>51</em>(16), 3308–3332. (<a
href="https://doi.org/10.1080/02664763.2024.2346827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let { 𝑋 𝑖 , 1 ≤ 𝑖 ≤ 𝑛 } be a sequence of linear process based on dependent random variables with random coefficients, which has a mean shift at an unknown location. The weighted sum of powers in mean (WSPM, for short) estimator of the change point is proposed. The weak consistency, the rate of weak consistency and strong consistency for the WSPM estimator are established under some mild conditions. Simulation studies and two real data exercises are also provided to show the superiority of this new method to some existing methods.},
  archive      = {J_JOAS},
  author       = {Yi Wu and Wei Wang and Shipeng Wu and Xuejun Wang},
  doi          = {10.1080/02664763.2024.2346827},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3308-3332},
  shortjournal = {J. Appl. Stat.},
  title        = {The weighted sum of powers in mean for estimating a change point in linear processes with random coefficients},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On efficiency of locally d-optimal designs under
heteroscedasticity and non-gaussianity. <em>JOAS</em>, <em>51</em>(16),
3292–3307. (<a
href="https://doi.org/10.1080/02664763.2024.2346822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the classical theory of locally optimal designs, which is developed within the framework of the center+error model, the most efficient design is the one based on MGLE, the maximum Gaussian likelihood estimator. However, practical scenarios often lack of complete information as to the governing probability model for the response measure and deviate from Gaussianity and homoscedasticity assumptions, in which, MqLE, the maximum quasi-likelihood estimator, has been advocated in the literature. In this work, we examine the locally optimal design based on the novel oracle-SLSE, the second-order least-square estimator, in the case where the underlying probability model is incompletely specified. We find that in a general setting, our oracle SLSE-based optimal design, incorporating skewness and kurtosis information, outperforms those based on MqLE or MGLE. Our numerical experiment supports this, with locally D-optimal designs based on MqLE approaching the efficiency of oracle-SLSE designs in some cases. This research guides the choice of estimators in practical scenarios departing from ideal assumptions.},
  archive      = {J_JOAS},
  author       = {Xiao Zhang and Gang Shen},
  doi          = {10.1080/02664763.2024.2346822},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3292-3307},
  shortjournal = {J. Appl. Stat.},
  title        = {On efficiency of locally D-optimal designs under heteroscedasticity and non-gaussianity},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A doubly robust estimator for the mann whitney wilcoxon rank
sum test when applied for causal inference in observational studies.
<em>JOAS</em>, <em>51</em>(16), 3267–3291. (<a
href="https://doi.org/10.1080/02664763.2024.2346357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mann–Whitney–Wilcoxon rank sum test (MWWRST) is a widely used method for comparing two treatment groups in randomized control trials, particularly when dealing with highly skewed data. However, when applied to observational study data, the MWWRST often yields invalid results for causal inference. To address this limitation, Wu et al. ( Causal inference for Mann–Whitney–Wilcoxon rank sum and other nonparametric statistics , Stat. Med. 33 (2014), pp. 1261–1271) introduced an approach that incorporates inverse probability weighting (IPW) into this rank-based statistic to mitigate confounding effects. Subsequently, Mao ( On causal estimation using U-statistics , Biometrika 105 (2018), pp. 215–220), Zhang et al. ( Estimating Mann Whitney-type causal effects , J. Causal Inference 7 (2019), ARTICLE ID 20180010), and Ai et al. ( A Mann–Whitney test of distributional effects in a multivalued treatment , J. Stat. Plan. Inference 209 (2020), pp. 85–100) extended this IPW estimator to develop doubly robust estimators. Nevertheless, each of these approaches has notable limitations. Mao&#39;s method imposes stringent assumptions that may not align with real-world study data. Zhang et al. &#39;s ( Estimating Mann Whitney-type causal effects , J. Causal Inference 7 (2019), ARTICLE ID 20180010) estimators rely on bootstrap inference, which suffers from computational inefficiency and lacks known asymptotic properties. Meanwhile, Ai et al. ( A Mann–Whitney test of distributional effects in a multivalued treatment , J. Stat. Plan. Inference 209 (2020), pp. 85–100) primarily focus on testing the null hypothesis of equal distributions between two groups, which is a more stringent assumption that may not be well-suited to the primary practical application of MWWRST. In this paper, we aim to address these limitations by leveraging functional response models (FRM) to develop doubly robust estimators. We demonstrate the performance of our proposed approach using both simulated and real study data.},
  archive      = {J_JOAS},
  author       = {Ruohui Chen and Tuo Lin and Lin Liu and Jinyuan Liu and Ruifeng Chen and Jingjing Zou and Chenyu Liu and Loki Natarajan and Wan Tang and Xinlian Zhang and Xin Tu},
  doi          = {10.1080/02664763.2024.2346357},
  journal      = {Journal of Applied Statistics},
  month        = {12},
  number       = {16},
  pages        = {3267-3291},
  shortjournal = {J. Appl. Stat.},
  title        = {A doubly robust estimator for the mann whitney wilcoxon rank sum test when applied for causal inference in observational studies},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reparametrized generalized gamma partially linear regression
with application to breast cancer data. <em>JOAS</em>, <em>51</em>(15),
3248–3265. (<a
href="https://doi.org/10.1080/02664763.2024.2337086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a new partially linear regression based on a reparametrized generalized gamma distribution with two systematic components that can be easily interpreted. Its parameters are estimated by penalized maximum likelihood. For different parameter settings, sample sizes, and censoring percentages, some simulations are performed to examine the accuracy of the maximum likelihood estimators, and the empirical distribution of the residuals compared with the standard normal distribution. The methodology is applied to breast cancer data in the city of João Pessoa in the state of Paraíba in Brazil.},
  archive      = {J_JOAS},
  author       = {Cleanderson R. Fidelis and Edwin M. M. Ortega and Fábio Prataviera and Roberto Vila and Gauss M. Cordeiro},
  doi          = {10.1080/02664763.2024.2337086},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3248-3265},
  shortjournal = {J. Appl. Stat.},
  title        = {Reparametrized generalized gamma partially linear regression with application to breast cancer data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSE superiority of the unrestricted stein-rule estimator in
a regression model with a possible structural break. <em>JOAS</em>,
<em>51</em>(15), 3233–3247. (<a
href="https://doi.org/10.1080/02664763.2024.2346346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the estimation of a linear regression model with a possible structural break at a known point. We analytically derive the exact formulae of the MSE for the restricted SR and PSR estimators, which shrinks the OLS estimator toward the restriction of no structural break. We compare the MSE performance of restricted/unrestricted SR, PSR, and least squared estimators. We analytically show that the unrestricted SR estimator can have a smaller MSE than the restricted SR estimator even when the restriction is correct. Further, our numerical results show that the unrestricted PSR estimator has the best MSE performance over a wide region of parameter space. These results indicate that the use of the unrestricted PSR estimator is recommended even when a structural break may not exist, although the unrestricted PSR estimator does not take the possibility of no structural break into consideration.},
  archive      = {J_JOAS},
  author       = {Haifeng Xu and Akio Namba},
  doi          = {10.1080/02664763.2024.2346346},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3233-3247},
  shortjournal = {J. Appl. Stat.},
  title        = {MSE superiority of the unrestricted stein-rule estimator in a regression model with a possible structural break},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A principal-weighted penalized regression model and its
application in economic modeling. <em>JOAS</em>, <em>51</em>(15),
3215–3232. (<a
href="https://doi.org/10.1080/02664763.2024.2346343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Principal-Weighted Penalized (PWP) regression model, designed for dimensionality reduction in large datasets without sacrificing essential information. This new model retains the favorable features of the principal component analysis (PCA) technique and penalized regression models. It weighs the variables in a large data set based on their contributions to principal components identified by PCA, enhancing its capacity to uncover crucial hidden variables. The PWP model also efficiently performs variable selection and estimates regression coefficients through regularization. An application of the proposed model on high-dimensional economic data is studied. The results of comparative studies in simulations and a real example in economic modeling demonstrate its superior fitting and predictive abilities. The resulting model excels in accuracy and interpretability, outperforming existing methods.},
  archive      = {J_JOAS},
  author       = {Mingwei Sun and Murong Xu},
  doi          = {10.1080/02664763.2024.2346343},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3215-3232},
  shortjournal = {J. Appl. Stat.},
  title        = {A principal-weighted penalized regression model and its application in economic modeling},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Directional false discovery rate control in large-scale
multiple comparisons. <em>JOAS</em>, <em>51</em>(15), 3195–3214. (<a
href="https://doi.org/10.1080/02664763.2024.2344260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advance of high-throughput biomedical technology makes it possible to access massive measurements of gene expression levels. An important statistical issue is identifying both under-expressed and over-expressed genes for a disease. Most existing multiple-testing procedures focus on selecting only the non-null or significant genes without further identifying their expression type. Only limited methods are designed for the directional problem, and yet they fail to separately control the numbers of falsely discovered over-expressed and under-expressed genes with only a unified index combining all the false discoveries. In this paper, based on a three-classification multiple testing framework, we propose a practical data-driven procedure to control separately the two directions of false discoveries. The proposed procedure is theoretically valid and optimal in the sense that it maximizes the expected number of true discoveries while controlling the false discovery rates for under-expressed and over-expressed genes simultaneously. The procedure allows different nominal levels for the two directions, exhibiting high flexibility in practice. Extensive numerical results and analysis of two large-scale genomic datasets show the effectiveness of our procedure.},
  archive      = {J_JOAS},
  author       = {Wenjuan Liang and Dongdong Xiang and Yajun Mei and Wendong Li},
  doi          = {10.1080/02664763.2024.2344260},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3195-3214},
  shortjournal = {J. Appl. Stat.},
  title        = {Directional false discovery rate control in large-scale multiple comparisons},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New results on optimal conditional error functions for
adaptive two-stage designs. <em>JOAS</em>, <em>51</em>(15), 3178–3194.
(<a href="https://doi.org/10.1080/02664763.2024.2342424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unblinded interim analyses in clinical trials with adaptive designs are gaining increasing popularity. Here, the type I error rate is controlled by defining an appropriate conditional error function. Since various approaches to the selection of the conditional error function exist, the question of an optimal choice arises. In this article, we extend existing work on optimal conditional error functions by two results. Firstly, we prove that techniques from variational calculus can be applied to derive existing optimal conditional error functions. Secondly, we answer the question of optimizing the conditional error function of an optimal promising zone design and investigate the efficiency gain.},
  archive      = {J_JOAS},
  author       = {Maximilian Pilz and Meinhard Kieser},
  doi          = {10.1080/02664763.2024.2342424},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3178-3194},
  shortjournal = {J. Appl. Stat.},
  title        = {New results on optimal conditional error functions for adaptive two-stage designs},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goodness-of-fit tests for the one-sided lévy distribution
based on quantile conditional moments. <em>JOAS</em>, <em>51</em>(15),
3154–3177. (<a
href="https://doi.org/10.1080/02664763.2024.2340592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a novel statistical framework based on the first two quantile conditional moments that facilitates effective goodness-of-fit testing for one-sided Lévy distributions. The scale-ratio framework introduced in this paper extends our previous results in which we have shown how to extract unique distribution features using conditional variance ratio for the generic class of α -stable distributions. We show that the conditional moment-based goodness-of-fit statistics are a good alternative to other methods introduced in the literature tailored to the one-sided Lévy distributions. The usefulness of our approach is verified using an empirical test power study. For completeness, we also derive the asymptotic distributions of the test statistics and show how to apply our framework to real data.},
  archive      = {J_JOAS},
  author       = {Kewin Pączek and Damian Jelito and Marcin Pitera and Agnieszka Wyłomańska},
  doi          = {10.1080/02664763.2024.2340592},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3154-3177},
  shortjournal = {J. Appl. Stat.},
  title        = {Goodness-of-fit tests for the one-sided lévy distribution based on quantile conditional moments},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian extension of the weibull AFT shared frailty model
with generalized family of distributions for enhanced survival analysis
using censored data. <em>JOAS</em>, <em>51</em>(15), 3125–3153. (<a
href="https://doi.org/10.1080/02664763.2024.2338404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis, the Accelerated Failure Time (AFT) shared frailty model is a widely used framework for analyzing time-to-event data while accounting for unobserved heterogeneity among individuals. This paper extends the traditional Weibull AFT shared frailty model using half logistic-G family of distributions (Type I, Type II and Type II exponentiated) through Bayesian methods. This approach offers flexibility in capturing covariate influence and handling heavy-tailed frailty distributions. Bayesian inference with MCMC provides parameter estimates and credible intervals. Simulation studies show improved model predictive performance compared to existing models, and real-world applications demonstrate its practical utility. In summary, our Bayesian Weibull AFT shared frailty model with Type I, Type II and Type II exponentiated half logistic-G family distributions enhances time-to-event data analysis, making it a versatile tool for survival analysis in various fields using STAN in R .},
  archive      = {J_JOAS},
  author       = {Mohammad Parvej and Athar Ali Khan},
  doi          = {10.1080/02664763.2024.2338404},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3125-3153},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian extension of the weibull AFT shared frailty model with generalized family of distributions for enhanced survival analysis using censored data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matching a discrete distribution by poisson matching
quantiles estimation. <em>JOAS</em>, <em>51</em>(15), 3102–3124. (<a
href="https://doi.org/10.1080/02664763.2024.2337082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the data collected from different sources requires unpaired data analysis to account for the absence of correspondence between the random variable Y and the covariates 𝑿 . Several attempts have been made to analyze continuous Y , but it may follow a discrete distribution, which previous methodologies have overlooked. To address these limitations, we propose Poisson matching quantiles estimation (PMQE), the first unpaired data analysis method designed to examine the discrete Y and the unpaired continuous covariates X . Using their order statistics, the PMQE method matches the linear combination of random variables β T X to log ( Y ) . We further improve the performance of the proposed method by ℓ 1 penalizing β , leading to the PMQE LASSO. An effective algorithm and simulation results are presented, along with the convergence results. We illustrate the practical application of PMQE using real data.},
  archive      = {J_JOAS},
  author       = {Hyungjun Lim and Arlene K. H. Kim},
  doi          = {10.1080/02664763.2024.2337082},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3102-3124},
  shortjournal = {J. Appl. Stat.},
  title        = {Matching a discrete distribution by poisson matching quantiles estimation},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional sufficient dimension reduction through
information maximization with application to classification.
<em>JOAS</em>, <em>51</em>(15), 3059–3101. (<a
href="https://doi.org/10.1080/02664763.2024.2335570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the case where the response variable is a categorical variable and the predictor is a random function, two novel functional sufficient dimensional reduction (FSDR) methods are proposed based on mutual information and square loss mutual information. Compared to the classical FSDR methods, such as functional sliced inverse regression and functional sliced average variance estimation, the proposed methods are appealing because they are capable of estimating multiple effective dimension reduction directions in the case of a relatively small number of categories, especially for the binary response. Moreover, the proposed methods do not require the restrictive linear conditional mean assumption and the constant covariance assumption. They avoid the inverse problem of the covariance operator which is often encountered in the functional sufficient dimension reduction. The functional principal component analysis with truncation be used as a regularization mechanism. Under some mild conditions, the statistical consistency of the proposed methods is established. Simulation studies and real data analyzes are used to evaluate the finite sample properties of our methods.},
  archive      = {J_JOAS},
  author       = {Xinyu Li and Jianjun Xu and Haoyang Cheng},
  doi          = {10.1080/02664763.2024.2335570},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3059-3101},
  shortjournal = {J. Appl. Stat.},
  title        = {Functional sufficient dimension reduction through information maximization with application to classification},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of world seroprevalence of SARS-CoV-2 antibodies.
<em>JOAS</em>, <em>51</em>(15), 3039–3058. (<a
href="https://doi.org/10.1080/02664763.2024.2335569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we estimate the seroprevalence against COVID-19 by country and derive the seroprevalence over the world. To estimate seroprevalence among adults, we use serological surveys (also called the serosurveys) conducted within each country. When the serosurveys are incorporated to estimate world seroprevalence, there are two issues. First, there are countries in which a serological survey has not been conducted. Second, the sample collection dates differ from country to country. We attempt to tackle these problems using the vaccination data, confirmed cases data, and national statistics. We construct Bayesian models to estimate the numbers of people who have antibodies produced by infection or vaccination separately. For the number of people with antibodies due to infection, we develop a hierarchical model for combining the information included in both confirmed cases data and national statistics. At the same time, we propose regression models to estimate missing values in the vaccination data. As of 31st of July 2021, using the proposed methods, we obtain the 95 % credible interval of the world seroprevalence as [ 35.5 % , 56.8 % ] .},
  archive      = {J_JOAS},
  author       = {Kwangmin Lee and Seongmin Kim and Seongil Jo and Jaeyong Lee},
  doi          = {10.1080/02664763.2024.2335569},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3039-3058},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of world seroprevalence of SARS-CoV-2 antibodies},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian factor selection in a hybrid approach to
confirmatory factor analysis. <em>JOAS</em>, <em>51</em>(15), 3005–3038.
(<a href="https://doi.org/10.1080/02664763.2024.2335568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To investigate latent structures of measured variables, various factor structures are used for confirmatory factor analysis, including higher-order models and more flexible bifactor models. In practice, measured variables may also have relatively small or moderate non-zero loadings on multiple group factors, which form cross loadings. The selection of correct and ‘identifiable’ latent structures is important to evaluate an impact of constructs of interest in the confirmatory factor analysis model. Herein, we first discuss the identifiability condition that allows several cross loadings of the models with underlying bifactor structures. Then, we implement Bayesian variable selection allowing cross loadings on bifactor structures using the spike and slab prior. Our approaches evaluate the inclusion probability for all group factor loadings and utilize known underlying structural information, making our approaches not entirely exploratory. Through a Monte Carlo study, we demonstrate that our methods can provide more accurately identified results than other available methods. For the application, the SF-12 version 2 scale, a self-report health-related quality of life survey is used. The model selected by our proposed methods is more parsimonious and has a better fit index compared to other models including the ridge prior selection and strict bifactor model.},
  archive      = {J_JOAS},
  author       = {Junyu Nie and Jihnhee Yu},
  doi          = {10.1080/02664763.2024.2335568},
  journal      = {Journal of Applied Statistics},
  month        = {11},
  number       = {15},
  pages        = {3005-3038},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian factor selection in a hybrid approach to confirmatory factor analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing predictive precision medicine models by
exploiting real-world data using machine learning methods.
<em>JOAS</em>, <em>51</em>(14), 2980–3003. (<a
href="https://doi.org/10.1080/02664763.2024.2315451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational Medicine encompasses the application of Statistical Machine Learning and Artificial Intelligence methods on several traditional medical approaches, including biochemical testing which is extremely valuable both for early disease prognosis and long-term individual monitoring, as it can provide important information about a person&#39;s health status. However, using Statistical Machine Learning and Artificial Intelligence algorithms to analyze biochemical test data from Electronic Health Records requires several preparatory steps, such as data manipulation and standardization. This study presents a novel approach for utilizing Electronic Health Records from large, real-world databases to develop predictive precision medicine models by exploiting Artificial Intelligence. Furthermore, to demonstrate the effectiveness of this approach, we compare the performance of various traditional Statistical Machine Learning and Deep Learning algorithms in predicting individuals&#39; future biochemical test outcomes. Specifically, using data from a large real-world database, we exploit a longitudinal format of the data in order to predict the future values of 15 biochemical tests and identify individuals at high risk. The proposed approach and the extensive model comparison contribute to the personalized approach that modern medicine aims to achieve.},
  archive      = {J_JOAS},
  author       = {Panagiotis C. Theocharopoulos and Sotiris Bersimis and Spiros V. Georgakopoulos and Antonis Karaminas and Sotiris K. Tasoulis and Vassilis P. Plagianakos},
  doi          = {10.1080/02664763.2024.2315451},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2980-3003},
  shortjournal = {J. Appl. Stat.},
  title        = {Developing predictive precision medicine models by exploiting real-world data using machine learning methods},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A combined superiority and non-inferiority procedure for
comparing predictive values of two diagnostic tests. <em>JOAS</em>,
<em>51</em>(14), 2961–2979. (<a
href="https://doi.org/10.1080/02664763.2024.2335564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and negative predictive values are useful to quantify the performance of medical tests, and both are often used simultaneously. Although there are several methods to test the equality of these predictive values between two medical tests, these approaches separately compare positive and negative predictive values. Therefore, we propose a testing procedure that combines the approximate likelihood ratio test defined by Tang et al. with the non-inferiority test for predictive values. The procedure can confirm that compared to an existing test, a new medical test is non-inferior in terms of both positive and negative predictive values, as well as superior regarding at least one of these values. It can make a comprehensive judgment of the performance of the new test based on both measures. A simulation study showed that the performance of the proposed testing procedure is appropriate, and the procedure is considered useful for evaluating the performance of predictive values of medical tests.},
  archive      = {J_JOAS},
  author       = {Kanae Takahashi and Kouji Yamamoto and Ayumi Shintani},
  doi          = {10.1080/02664763.2024.2335564},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2961-2979},
  shortjournal = {J. Appl. Stat.},
  title        = {A combined superiority and non-inferiority procedure for comparing predictive values of two diagnostic tests},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The link between multiplicative competitive interaction
models and compositional data regression with a total. <em>JOAS</em>,
<em>51</em>(14), 2929–2960. (<a
href="https://doi.org/10.1080/02664763.2024.2329923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article sheds light on the relationship between compositional data (CoDa) regression models and multiplicative competitive interaction (MCI) models, which are two approaches for modeling shares. We demonstrate that MCI models are particular cases of CoDa models with a total and that a reparameterization links both. Recognizing this relation offers mutual benefits for the CoDa and MCI literature, each with its own rich tradition. The CoDa tradition, with its rigorous mathematical foundation, provides additional theoretical guarantees and mathematical tools that we apply to improve the estimation of MCI models. Simultaneously, the MCI model emerged from almost a century-long tradition in marketing research that may enrich the CoDa literature. One aspect is the grounding of the MCI specification in assumptions on the behavior of individuals. From this basis, the MCI tradition also provides credible justifications for heteroskedastic error structures – an idea we develop further and that is relevant to many CoDa models beyond the marketing context. Additionally, MCI models have always been interpreted in terms of elasticities, a method that has only recently emerged in CoDa. Regarding this interpretation, the CoDa perspective leads to a decomposition of the influence of the explanatory variables into contributions from relative and absolute information.},
  archive      = {J_JOAS},
  author       = {Lukas Dargel and Christine Thomas-Agnan},
  doi          = {10.1080/02664763.2024.2329923},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2929-2960},
  shortjournal = {J. Appl. Stat.},
  title        = {The link between multiplicative competitive interaction models and compositional data regression with a total},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The impact of misclassifications and outliers on imputation
methods. <em>JOAS</em>, <em>51</em>(14), 2894–2928. (<a
href="https://doi.org/10.1080/02664763.2024.2325969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many imputation methods have been developed over the years and tested mostly under ideal settings. Surprisingly, there is no detailed research on how imputation methods perform when the idealized assumptions about the distribution of data and/or model assumptions are partly not fulfilled. This research looks into the susceptibility of imputation techniques, particularly in relation to outliers, misclassifications, and incorrect model specifications. This is crucial knowledge about how well the methods convince in everyday life because, in reality, conditions are usually not ideal, and model assumptions may not hold. The data may not fit the defined models well. Outliers distort the estimates, and misclassifications reduce the quality of most imputation methods. Several different evaluation measures are discussed, from comparing imputed values with true values or comparing certain statistics, from the performance of classifiers to the variance of estimated parameters. Some well-known imputation methods are compared based on real data and simulations. It turns out that robust conditional imputation methods outperform other methods for real data and simulation settings.},
  archive      = {J_JOAS},
  author       = {M. Templ and Markus Ulmer},
  doi          = {10.1080/02664763.2024.2325969},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2894-2928},
  shortjournal = {J. Appl. Stat.},
  title        = {The impact of misclassifications and outliers on imputation methods},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor model for ordinal categorical data with latent
factors explained by auxiliary variables applied to the major depression
inventory. <em>JOAS</em>, <em>51</em>(14), 2866–2893. (<a
href="https://doi.org/10.1080/02664763.2024.2321913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In behavioral and social research, questionnaires are an important assessment tool, through which individuals can be categorized according to how they classify themselves in respect to a personal trait. One example is the Major Depression Inventory (MDI), which is widely used for the assessment of depression. It can also be used as a depression severity scale, with scores ranging from 0 to 50 constructed considering the same weight for each item in the MDI. However, the dependence among the items of the questionnaire suggests that a score with better properties could be obtained through factor models, which besides allowing to reduce the dimensionality of multivariate data, provides the estimation of common factors and factor loadings that often have an interesting theoretical interpretation. Additionally, auxiliary information could be available and, the effect of these variables in the latent factor could be estimated and provide interesting results. Thus, the main aim of this paper is to propose a factor model for ordered categorical data which incorporates auxiliary variables to explain the latent factors. The proposed model provides an alternative score to MDI based on the estimated latent factors that takes the uncertainty in the data and auxiliary information into account.},
  archive      = {J_JOAS},
  author       = {Alana Tavares Viana and Kelly Cristina Mota Gonçalves and Marina Silva Paez},
  doi          = {10.1080/02664763.2024.2321913},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2866-2893},
  shortjournal = {J. Appl. Stat.},
  title        = {Factor model for ordinal categorical data with latent factors explained by auxiliary variables applied to the major depression inventory},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust likelihood approach to inference for paired
multiple binary endpoints data. <em>JOAS</em>, <em>51</em>(14),
2851–2865. (<a
href="https://doi.org/10.1080/02664763.2024.2321904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a robust likelihood approach to inference for paired multiple binary endpoints data. One can easily implement the methodology without dealing with the model that incorporates a large number of joint probabilities of no direct relevance to the inference of interest. We present the robust score test statistic for testing the equality of two treatment effects to exemplify the utility and simplicity of the method. Our novel technique is applicable when patients have different numbers of endpoints and for unpaired endpoints. The extension of our robust approach to multiple endpoints data with more categories is straightforward. We use simulations and real data analysis to highlight the efficacy of our robust procedure.},
  archive      = {J_JOAS},
  author       = {Tsung-Shan Tsou and Wei-Cheng Hsiao},
  doi          = {10.1080/02664763.2024.2321904},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2851-2865},
  shortjournal = {J. Appl. Stat.},
  title        = {A robust likelihood approach to inference for paired multiple binary endpoints data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel m-lognormal–burr regression model with varying
threshold for modeling heavy-tailed claim severity data. <em>JOAS</em>,
<em>51</em>(14), 2832–2850. (<a
href="https://doi.org/10.1080/02664763.2024.2319232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we explore the potential of composite probability distributions in effectively modeling claim severity data, which encompasses a spectrum of losses, ranging from minor to substantial. Our approach incorporates the innovative Mode-Matching technique to introduce a novel composite Lognormal–Burr distribution family. To comprehensively address the diverse risk characteristics exhibited by policyholders, we develop a regression model based on the composite Lognormal–Burr distribution. Additionally, we delve into the details of the parameter estimation method required for precise model parameter estimation. The practical utility of our proposed composite regression model is substantiated through its application to real-world insurance data, serving as a compelling illustration of its effectiveness.},
  archive      = {J_JOAS},
  author       = {Girish Aradhye and Deepesh Bhati and George Tzougas},
  doi          = {10.1080/02664763.2024.2319232},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2832-2850},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel M-Lognormal–Burr regression model with varying threshold for modeling heavy-tailed claim severity data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anticipative bayesian classification for data streams with
verification latency. <em>JOAS</em>, <em>51</em>(14), 2812–2831. (<a
href="https://doi.org/10.1080/02664763.2024.2319222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing adaptive classification algorithms in non-stationary data streams require recent labelled data for their updates. Such recent labels are often missing. For stream classification under verification latency only few approaches exist. Most of them assume clustered data or homogeneous drift in all features, which limits their applicability. We address this by proposing Anticipative Bayesian stream Classifier ( ABClass ), an approach that is capable of integrating and automatically selecting from different components. In its Bayesian classification framework, ABClass combines density estimation techniques, extended to extrapolate drift patterns over time, with unsupervised parameter tuning and unsupervised model selection. ABClass allows for multivariate density estimation and extrapolation techniques. In this work, we assume conditional independence between features given the class label for modelling feature-specific drift patterns. ABClass is generative and can also be used for explaining and visualising concept drift patterns. It is generic, making it easy to include further types of drift models, both for the class-conditional feature distribution and for the class prior distribution. The experimental evaluation on several real-world data streams shows its competitiveness compared to other state-of-the-art approaches. ABClass is in most cases ten- to hundred-times faster than its competitors, both for model fitting and for prediction.},
  archive      = {J_JOAS},
  author       = {Vera Hofer and Georg Krempl and Dominik Lang},
  doi          = {10.1080/02664763.2024.2319222},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2812-2831},
  shortjournal = {J. Appl. Stat.},
  title        = {Anticipative bayesian classification for data streams with verification latency},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple observers ranked set samples for shrinkage
estimators. <em>JOAS</em>, <em>51</em>(14), 2779–2811. (<a
href="https://doi.org/10.1080/02664763.2024.2317312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranked set sampling (RSS) is used as a powerful data collection technique for situations where measuring the study variable requires a costly and/or tedious process while the sampling units can be ranked easily (e.g. osteoporosis research). In this paper, we develop ridge and Liu-type shrinkage estimators under RSS data from multiple observers to handle the collinearity problem in estimating coefficients of linear regression, stochastic restricted regression and logistic regression. Through extensive numerical studies, we show that shrinkage methods with the multi-observer RSS result in more efficient coefficient estimates. The developed methods are finally applied to bone mineral data for analysis of bone disorder status of women aged 50 and older.},
  archive      = {J_JOAS},
  author       = {Andrew David Pearce and Armin Hatefi},
  doi          = {10.1080/02664763.2024.2317312},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2779-2811},
  shortjournal = {J. Appl. Stat.},
  title        = {Multiple observers ranked set samples for shrinkage estimators},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new factor analysis model for factors obeying a gamma
distribution. <em>JOAS</em>, <em>51</em>(14), 2760–2778. (<a
href="https://doi.org/10.1080/02664763.2024.2317299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional factor analysis model assumes that the factors obey a normal distribution, which is not appropriate in fields whose data are nonnegative. For this kind of problem, we construct a more practical factor model, assuming that the factors obey a Gamma distribution. We develop a new factor analysis model and discuss its true loading matrix. Then we study its parameter estimation with the maximum likelihood estimation (MLE) method based on an Expectation-Maximization (EM) algorithm, where step E is realized by the Metropolis-Hastings (M-H) algorithm in the Markov Chain Monte Carlo (MCMC) method. We use the new model to empirically study real data, and evaluate its information extraction ability, using the defined true loading matrix to calculate the true loading of the factor. We compare the new model and traditional factor analysis models on simulated and real data, respectively, whose results show that the new model has better information extraction ability for nonnegative data when the number of factors is the same.},
  archive      = {J_JOAS},
  author       = {Guoqiong Zhou and Wenjiang Jiang and Shixun Lin},
  doi          = {10.1080/02664763.2024.2317299},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2760-2778},
  shortjournal = {J. Appl. Stat.},
  title        = {A new factor analysis model for factors obeying a gamma distribution},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vector time series modelling of turbidity in dublin bay.
<em>JOAS</em>, <em>51</em>(14), 2744–2759. (<a
href="https://doi.org/10.1080/02664763.2024.2315470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turbidity is commonly monitored as an important water quality index. Human activities, such as dredging and dumping operations, can disrupt turbidity levels and should be monitored and analysed for possible effects. In this paper, we model the variations of turbidity in Dublin Bay over space and time to investigate the effects of dumping and dredging while controlling for the effect of wind speed as a common atmospheric effect. We develop a Vector Auto-Regressive Integrated Conditional Heteroskedasticity (VARICH) approach to modelling the dynamical behaviour of turbidity over different locations and at different water depths. We use daily values of turbidity during the years 2017–2018 to fit the model. We show that the results of our fitted model are in line with the observed data and that the uncertainties, measured through Bayesian credible intervals, are well calibrated. Furthermore, we show that the daily effects of dredging and dumping on turbidity are negligible in comparison to that of wind speed.},
  archive      = {J_JOAS},
  author       = {Amin Shoari Nejad and Gerard D. McCarthy and Brian Kelleher and Anthony Grey and Andrew Parnell},
  doi          = {10.1080/02664763.2024.2315470},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2744-2759},
  shortjournal = {J. Appl. Stat.},
  title        = {Vector time series modelling of turbidity in dublin bay},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal poisson subsampling decorrelated score for
high-dimensional generalized linear models. <em>JOAS</em>,
<em>51</em>(14), 2719–2743. (<a
href="https://doi.org/10.1080/02664763.2024.2315467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For high-dimensional generalized linear models (GLMs) with massive data, this paper investigates a unified optimal Poisson subsampling scheme to conduct estimation and inference for prespecified low-dimensional partition of the whole parameter. A Poisson subsampling decorrelated score function is proposed such that the adverse effect of the less accurate nuisance parameter estimation with slow convergence rate can be mitigated. The resultant Poisson subsample estimator is proved to enjoy consistency and asymptotic normality, and a more general optimal subsampling criterion including A- and L-optimality criteria is formulated to improve estimation efficiency. We also propose a two-step algorithm for implementation and discuss some practical issues. The satisfactory performance of our method is validated through simulation studies and a real dataset.},
  archive      = {J_JOAS},
  author       = {Junhao Shan and Lei Wang},
  doi          = {10.1080/02664763.2024.2315467},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {14},
  pages        = {2719-2743},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal poisson subsampling decorrelated score for high-dimensional generalized linear models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion comments on “exponentiated teissier distribution
with increasing, decreasing and bathtub hazard functions.”
<em>JOAS</em>, <em>51</em>(13), 2715–2717. (<a
href="https://doi.org/10.1080/02664763.2023.2297153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we present some discussion comments on a note entitled ‘A note on the unimodality and log-concavity of the exponentiated Teissier distribution’ submitted in J. Appl. Stat. by some authors, about the paper by Sharma et al. (Exponentiated Teissier distribution with increasing, decreasing and bathtub hazard functions, J. Appl. Stat. 49 (2022), pp. 371–393).},
  archive      = {J_JOAS},
  author       = {Vikas Kumar Sharma and Sudhanshu V. Singh and Komal Shekhawat},
  doi          = {10.1080/02664763.2023.2297153},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2715-2717},
  shortjournal = {J. Appl. Stat.},
  title        = {Discussion comments on “Exponentiated teissier distribution with increasing, decreasing and bathtub hazard functions”},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A note on the unimodality and log-concavity of the
exponentiated teissier distribution. <em>JOAS</em>, <em>51</em>(13),
2709–2714. (<a
href="https://doi.org/10.1080/02664763.2023.2297149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Sharma et al. (Exponentiated Teissier distribution with increasing, decreasing and bathtub hazard functions, J. Appl. Stat. 49 (2022), pp. 371–393) introduced the exponentiated Teissier distribution. Unfortunately, the proof given for the log-concavity of the probability density function and log-concave for α ≥ 1 α ≥ 1 α≥1 only, are incorrect. Also, the information related to the unimodality of the distribution is incomplete. In this note, numerically, graphically and analytically, the necessary corrections have been made.},
  archive      = {J_JOAS},
  author       = {V. Kumaran and Vishwa Prakash Jha},
  doi          = {10.1080/02664763.2023.2297149},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2709-2714},
  shortjournal = {J. Appl. Stat.},
  title        = {A note on the unimodality and log-concavity of the exponentiated teissier distribution},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian parametric estimation based on left-truncated
competing risks data under bivariate clayton copula models.
<em>JOAS</em>, <em>51</em>(13), 2690–2708. (<a
href="https://doi.org/10.1080/02664763.2024.2315458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational/field studies, competing risks and left-truncation may co-exist, yielding ‘left-truncated competing risks’ settings. Under the assumption of independent competing risks, parametric estimation methods were developed for left-truncated competing risks data. However, competing risks may be dependent in real applications. In this paper, we propose a Bayesian estimator for both independent competing risks and copula-based dependent competing risks models under left-truncation. The simulations show that the Bayesian estimator for the copula-based dependent risks model yields the desired performance when competing risks are dependent. We also comprehensively explore the choice of the prior distributions (Gamma, Inverse-Gamma, Uniform, half Normal and half Cauchy) and hyperparameters via simulations. Finally, two real datasets are analyzed to demonstrate the proposed estimators.},
  archive      = {J_JOAS},
  author       = {Hirofumi Michimae and Takeshi Emura and Atsushi Miyamoto and Kazuma Kishi},
  doi          = {10.1080/02664763.2024.2315458},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2690-2708},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian parametric estimation based on left-truncated competing risks data under bivariate clayton copula models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing nonlinearity of heavy-tailed time series.
<em>JOAS</em>, <em>51</em>(13), 2672–2689. (<a
href="https://doi.org/10.1080/02664763.2024.2315450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A test statistic for nonlinearity of a given heavy-tailed time series process is constructed, based on the sub-sample stability of Gini-based sample autocorrelations. The finite-sample performance of the proposed test is evaluated in a Monte Carlo study and compared to a similar test based on the sub-sample stability of a heavy-tailed analogue of the conventional sample autocorrelation function. In terms of size and power properties, the quality of our test outperforms a nonlinearity test for heavy-tailed time series processes proposed by [S.I. Resnick and E. Van den Berg, A test for nonlinearity of time series with infinite variance , Extremes 3 (2000), pp. 145–172.]. A nonlinear Pareto-type autoregressive process and a nonlinear Pareto-type moving average process are used as alternative specifications when comparing the power of the proposed test statistic. The efficacy of the test is illustrated via the analysis of a heavy-tailed actuarial data set and two time series of Ethernet traffic.},
  archive      = {J_JOAS},
  author       = {Jan G. De Gooijer},
  doi          = {10.1080/02664763.2024.2315450},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2672-2689},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing nonlinearity of heavy-tailed time series},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating effects of time-varying exposures on mortality
risk. <em>JOAS</em>, <em>51</em>(13), 2652–2671. (<a
href="https://doi.org/10.1080/02664763.2024.2313459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Administrative databases have become an increasingly popular data source for population-based health research. We explore how mortality risk is associated with some health service utilization process via linked administrative data. A generalized Cox regression model is proposed using a time-dependent stratification variable to summarize lifetime service utilization. Recognizing the service utilization over time as an internal covariate in the survival analysis, conventional likelihood methods are inapplicable. We present an estimating function based procedure for estimating model parameters, and provide a testing procedure for updating the stratification levels. The proposed approach is examined both asymptotically and numerically via simulation. We motivate and illustrate the proposed approach using an on-going program pertaining to opioid agonist treatment (OAT) management for individuals identified with opioid use disorders. Our analysis of the OAT data indicates that the OAT effect on mortality risk decreases in successive OAT attempts, in which two risk classes based on an individual&#39;s treatment episode number are established: one with 1–3 OAT episodes, and the other with 4+ OAT episodes.},
  archive      = {J_JOAS},
  author       = {Trevor J. Thomson and X. Joan Hu and Bohdan Nosyk},
  doi          = {10.1080/02664763.2024.2313459},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2652-2671},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating effects of time-varying exposures on mortality risk},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modeling of an outcome variable and integrated omics
datasets using GLM-PO2PLS. <em>JOAS</em>, <em>51</em>(13), 2627–2651.
(<a href="https://doi.org/10.1080/02664763.2024.2313458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many studies of human diseases, multiple omics datasets are measured. Typically, these omics datasets are studied one by one with the disease, thus the relationship between omics is overlooked. Modeling the joint part of multiple omics and its association to the outcome disease will provide insights into the complex molecular base of the disease. Several dimension reduction methods which jointly model multiple omics and two-stage approaches that model the omics and outcome in separate steps are available. Holistic one-stage models for both omics and outcome are lacking. In this article, we propose a novel one-stage method that jointly models an outcome variable with omics. We establish the model identifiability and develop EM algorithms to obtain maximum likelihood estimators of the parameters for normally and Bernoulli distributed outcomes. Test statistics are proposed to infer the association between the outcome and omics, and their asymptotic distributions are derived. Extensive simulation studies are conducted to evaluate the proposed model. The method is illustrated by modeling Down syndrome as outcome and methylation and glycomics as omics datasets. Here we show that our model provides more insight by jointly considering methylation and glycomics.},
  archive      = {J_JOAS},
  author       = {Zhujie Gu and Hae-Won Uh and Jeanine Houwing-Duistermaat and Said el Bouhaddani},
  doi          = {10.1080/02664763.2024.2313458},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2627-2651},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint modeling of an outcome variable and integrated omics datasets using GLM-PO2PLS},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A method for optimizing text preprocessing and text
classification using multiple cycles of learning with an application on
shipbrokers emails. <em>JOAS</em>, <em>51</em>(13), 2592–2626. (<a
href="https://doi.org/10.1080/02664763.2024.2307535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing text preprocessing and text classification algorithms is an important, everyday task in large organizations and companies and it usually involves a labor-intensive and time-consuming effort. For example, the filtering and sorting of a large number of electronic mails (emails) are crucial to keeping track of the received information and converting it automatically into useful and profitable knowledge. Business emails are often unstructured, noisy, and with many abbreviations and acronyms, which makes their handling a challenging procedure. To overcome those challenges, a two-step classification approach is proposed, along with a two-cycle labeling procedure in order to speed up the labeling process. Every step incorporates a heuristic classification approach to assign emails to predefined classes by comparing several classification and text vectorization algorithms. These algorithms are compared and evaluated using the F1 score and balanced accuracy. The implementation of the proposed algorithm is demonstrated in a shipbroker agent operating in Greece with excellent performance, improving organization and administration while reducing expenses.},
  archive      = {J_JOAS},
  author       = {Grigorios Papageorgiou and Polychronis Economou and Sotirios Bersimis},
  doi          = {10.1080/02664763.2024.2307535},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2592-2626},
  shortjournal = {J. Appl. Stat.},
  title        = {A method for optimizing text preprocessing and text classification using multiple cycles of learning with an application on shipbrokers emails},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric shiryaev-roberts change-point detection
procedures based on modified empirical likelihood. <em>JOAS</em>,
<em>51</em>(13), 2558–2591. (<a
href="https://doi.org/10.1080/02664763.2024.2307532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential change-point analysis, which identifies a change of probability distribution in an infinite sequence of random observations, has important applications in many fields. A good method should detect a change point as soon as possible, and keep a low amount of false alarms. As one of the most popular methods, Shiryaev-Roberts (SR) procedure holds many optimalities. However, its implementation requires the pre-change and post-change distributions to be known, which is not achievable in practice. In this paper, we construct a nonparametric version of the SR procedure by embedding different versions of empirical likelihood, assuming two training samples, before and after change, are available for parameter estimations. Simulations are conducted to compare the performance of the proposed method with existing methods. The results show that when the underlying distribution is unknown, and training sample sizes are small, the proposed modified procedure shows advantage by giving a smaller delay of detection.},
  archive      = {J_JOAS},
  author       = {Peiyao Wang and Wei Ning},
  doi          = {10.1080/02664763.2024.2307532},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2558-2591},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonparametric shiryaev-roberts change-point detection procedures based on modified empirical likelihood},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved agricultural household margin insurance scheme
with insured–insurer risk protection: A time-varying copula approach.
<em>JOAS</em>, <em>51</em>(13), 2529–2557. (<a
href="https://doi.org/10.1080/02664763.2024.2302061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agricultural household margin insurance (AHMI) is a crop insurance scheme that considers a farmer&#39;s trading capability. The scheme examines farmers&#39; losses in terms of both agricultural input and output using the farmer exchange rate index. In this article, we improve the previous design of the AHMI [A. Ahdika, D. Rosadi, A.R. Effendie, and Gunardi, Household margin insurance of agricultural sector in Indonesia using a farmer exchange rate index , Agric. Finance Rev. 81 (2020), pp. 169–188.] proposed to be implemented in Indonesia – which only provided risk protection for companies – by broadening the formula of losses so that risk protection covers both farmers and companies. We also provide premium payment scenarios, both seasonally and non-seasonally. We employ a time-varying Student-t copula with the extended dynamic parameter to identify the dynamic dependency between the indexes involved in determining the loss variable. We also determine the premium rate for the insurance scheme with various payment scenarios and investigate the implications for farmer survival and company management. Examining the farmer exchange rate index data of Indonesia, the empirical results show that the improved AHMI using a time-varying copula approach produces more reasonable loss estimates and various premium payment options with low premium rates for farmers, compared to either the previous AHMI design or the current crop insurance program in Indonesia.},
  archive      = {J_JOAS},
  author       = {Atina Ahdika and Dedi Rosadi and Adhitya Ronnie Effendie and Gunardi},
  doi          = {10.1080/02664763.2024.2302061},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2529-2557},
  shortjournal = {J. Appl. Stat.},
  title        = {An improved agricultural household margin insurance scheme with insured–insurer risk protection: A time-varying copula approach},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric estimation of mean residual lifetime in ranked
set sampling with a concomitant variable. <em>JOAS</em>,
<em>51</em>(13), 2512–2528. (<a
href="https://doi.org/10.1080/02664763.2023.2301334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual lifetime (MRL) of a unit is its expected additional lifetime provided that it has survived until time t . The MRL estimation problem has been frequently addressed in the literature since it has wide applications in statistics, reliability and survival analysis. In this paper, we consider the problem of estimating the MRL in ranked set sampling when actual quantifications of a concomitant variable are available. To exploit the additional information of the concomitant variable, we introduce several MRL estimators based on some regression techniques. We then compare them with the standard MRL estimator in simple random sampling using Monte Carlo simulation and a real dataset from the Surveillance, Epidemiology, and End Results Program. Our results indicate the superiority of the procedures that we have developed when the quality of ranking is fairly good.},
  archive      = {J_JOAS},
  author       = {Ehsan Zamanzade and M. Mahdizadeh and Hani M. Samawi},
  doi          = {10.1080/02664763.2023.2301334},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2512-2528},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonparametric estimation of mean residual lifetime in ranked set sampling with a concomitant variable},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed-level designs with orthogonality and relatively
optimal run order. <em>JOAS</em>, <em>51</em>(13), 2493–2511. (<a
href="https://doi.org/10.1080/02664763.2023.2301323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonality and optimality of run order are two important and worthy to be considered criteria in design of experiment. For the mixed-level designs commonly used in the case of unequal levels of factors in the experiment, both orthogonality and optimality of run order are taken into account at the same time in this paper. The construction methods of mixed-level designs with orthogonality and relatively minimum or maximum level changes are respectively proposed. Based on such designs, the orthogonal main-effect plans with relatively minimum or maximum level changes are obtained by the method of collapsing factor. The designs with relatively minimum or maximum level changes constructed in this paper guarantee the cost minimum or the time trend most robust in the actual experiment arrangement.},
  archive      = {J_JOAS},
  author       = {Wenwen Hu and Zujun Ou and Qiao Peng},
  doi          = {10.1080/02664763.2023.2301323},
  journal      = {Journal of Applied Statistics},
  month        = {10},
  number       = {13},
  pages        = {2493-2511},
  shortjournal = {J. Appl. Stat.},
  title        = {Mixed-level designs with orthogonality and relatively optimal run order},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Post-shrinkage strategies in statistical and machine
learning for high dimensional data. <em>JOAS</em>, <em>51</em>(12),
2489–2491. (<a
href="https://doi.org/10.1080/02664763.2023.2286426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Shuangzhe Liu and Tiefeng Ma},
  doi          = {10.1080/02664763.2023.2286426},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2489-2491},
  shortjournal = {J. Appl. Stat.},
  title        = {Post-shrinkage strategies in statistical and machine learning for high dimensional data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Propensity score matching: A tool for consumer risk modeling
and portfolio underwriting. <em>JOAS</em>, <em>51</em>(12), 2481–2488.
(<a href="https://doi.org/10.1080/02664763.2024.2302058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers and practitioners in financial services utilize a wide range of empirical techniques to assess risk and value. In cases where known performance is used to predict future performance of a new asset, the risk of bias is present when samples are uncontrolled by the analyst. Propensity score matching is a statistical methodology commonly used in medical and social science research to address issues related to experimental design when random assignment of cases is not possible. This common method has been almost absent from financial risk modeling and portfolio underwriting, primarily due to the different objectives for this sector relative to medicine and social sciences. In this application note, we demonstrate how propensity score matching can be considered as a practical tool to inform portfolio underwriting outside of experimental design. Using a portfolio of distressed consumer credit accounts, we demonstrate that propensity score matching can be used to predict both account-level and portfolio-level risk and argue that propensity score matching should be included in the methodological toolbox of researchers and practitioners engaged in risk modeling and valuation activities of portfolios of consumer assets, particularly in contexts with limited observations, a large number of potential modeling features, or highly imbalanced covariates.},
  archive      = {J_JOAS},
  author       = {Jennifer Lewis Priestley and Eric VonDohlen},
  doi          = {10.1080/02664763.2024.2302058},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2481-2488},
  shortjournal = {J. Appl. Stat.},
  title        = {Propensity score matching: A tool for consumer risk modeling and portfolio underwriting},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of estimation and prediction methods for a
zero-inflated geometric INAR(1) process with random coefficients.
<em>JOAS</em>, <em>51</em>(12), 2457–2480. (<a
href="https://doi.org/10.1080/02664763.2023.2301321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores zero-inflated count time series models used to analyze data sets with characteristics such as overdispersion, excess zeros, and autocorrelation. Specifically, we investigate the ZIGINAR RC ( 1 ) ZIGINAR RC ( 1 ) ZIGINARRC(1) process, a first-order stationary integer-valued autoregressive model with random coefficients and a zero-inflated geometric marginal distribution. Our focus is on examining various estimation and prediction techniques for this model. We employ estimation methods, including Whittle, Taper Spectral Whittle, Maximum Empirical Likelihood, and Sieve Bootstrap estimators for parameter estimation. Additionally, we propose forecasting approaches, such as median, Bayesian, and Sieve Bootstrap methods, to predict future values of the series. We assess the performance of these methods through simulation studies and real-world data analysis, finding that all methods perform well, providing 95% highest predicted probability intervals that encompass the observed data. While Bayesian and Bootstrap methods require more time for execution, their superior predictive accuracy justifies their use in forecasting.},
  archive      = {J_JOAS},
  author       = {R. Nasirzadeh and H. Bakouch},
  doi          = {10.1080/02664763.2023.2301321},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2457-2480},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparison of estimation and prediction methods for a zero-inflated geometric INAR(1) process with random coefficients},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interval-specific censoring set adjusted kaplan–meier
estimator. <em>JOAS</em>, <em>51</em>(12), 2436–2456. (<a
href="https://doi.org/10.1080/02664763.2023.2298795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a non-parametric approach to reduce the overestimation of the Kaplan-Meier (KM) estimator when the event and censoring times are independent. We adjust the KM estimator based on the interval-specific censoring set, a collection of intervals where censored data are observed between two adjacent event times. The proposed interval-specific censoring set adjusted KM estimator reduces to the KM estimator if there are no censored observations or the sample size tends to infinity and the proposed estimator is consistent, as is the case for the KM estimator. We prove theoretically that the proposed estimator reduces the overestimation compared to the KM estimator and provide a mathematical formula to estimate the variance of the proposed estimator based on Greenwood&#39;s approach. We also provide a modified log-rank test based on the proposed estimator. We perform four simulation studies to compare the proposed estimator with the KM estimator when the failure rate is constant, decreasing, increasing, and based on the flexible hazard method. The bias reduction in median survival time and survival rate using the proposed estimator is considerably large, especially when the censoring rate is high. The standard deviations are comparable between the two estimators. We implement the proposed and KM estimator for the Nonalcoholic Fatty Liver Disease patients from a population study. The results show the proposed estimator substantially reduce the overestimation in the presence of high observed censoring rate.},
  archive      = {J_JOAS},
  author       = {Yaoshi Wu and John Kolassa},
  doi          = {10.1080/02664763.2023.2298795},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2436-2456},
  shortjournal = {J. Appl. Stat.},
  title        = {Interval-specific censoring set adjusted Kaplan–Meier estimator},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence intervals and prediction intervals for
two-parameter negative binomial distributions. <em>JOAS</em>,
<em>51</em>(12), 2420–2435. (<a
href="https://doi.org/10.1080/02664763.2023.2297157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems of finding confidence intervals (CIs) and prediction intervals (PIs) for two-parameter negative binomial distributions are considered. Simple CIs for the mean of a two-parameter negative binomial distribution based on some large sample methods are proposed and compared with the likelihood CIs. Proposed CIs are not only simple to compute, but also better than the likelihood CIs for moderate sample sizes. Prediction intervals for the mean of a future sample from a two-parameter negative binomial distribution are also proposed and evaluated for their accuracy. The methods are illustrated using two examples with real life data sets.},
  archive      = {J_JOAS},
  author       = {Md Mahadi Hasan and K. Krishnamoorthy},
  doi          = {10.1080/02664763.2023.2297157},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2420-2435},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence intervals and prediction intervals for two-parameter negative binomial distributions},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagnostic checks in time series models based on a new
correlation coefficient of residuals. <em>JOAS</em>, <em>51</em>(12),
2402–2419. (<a
href="https://doi.org/10.1080/02664763.2023.2297155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For checking time series models, the Ljung–Box, Li–Mak and Zhu–Wang statistics play an important role, which use the Pearson&#39;s correlation coefficient to implement (squared) residual (partial) autocorrelation tests. In this paper, we replace the Pearson&#39;s correlation coefficient with a new rank correlation coefficient and propose a new test statistic to conduct diagnostic checks for residuals in autoregressive moving average models, autoregressive conditional heteroscedasticity models and integer-valued time series models, respectively. We conduct simulations to assess the performance of the new test statistic, and compare it with existing ones, and the results show the superiority of the proposed one. We use three real examples to exhibit its usefulness.},
  archive      = {J_JOAS},
  author       = {Jian Pei and Fukang Zhu and Qi Li},
  doi          = {10.1080/02664763.2023.2297155},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2402-2419},
  shortjournal = {J. Appl. Stat.},
  title        = {Diagnostic checks in time series models based on a new correlation coefficient of residuals},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach for evaluating equivalence over multiple
groups, and comparison with frequentist tost. <em>JOAS</em>,
<em>51</em>(12), 2382–2401. (<a
href="https://doi.org/10.1080/02664763.2023.2297150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufacturing and testing of pharmaceutical products frequently occur in multiple facilities within a company’s network. It is of interest to demonstrate equivalence among the alternative testing/manufacturing facilities to ensure product consistency and quality regardless of the facility where it was manufactured/tested. In the Frequentist framework, equivalence testing is well established when comparing two labs or manufacturing facilities; however, when considering more than two labs or production sites, the Frequentist approach may not always offer appropriate or interpretable estimates for demonstrating equivalence among all of them simultaneously. This paper demonstrates the utility of Bayesian methods to the equivalence assessment of multiple groups means, with a comparison against traditional Frequentist methods. We conclude that a Bayesian strategy is very useful for addressing the problem of multi-group equivalence. While it is not our intention to argue that Bayesian methods should always replace Frequentist ones, we show that among the advantages of a Bayesian analysis is that it provides a more nuanced understanding of the degree of similarity among sites than the hypothesis testing underpinning the Frequentist approach.},
  archive      = {J_JOAS},
  author       = {Jos Weusten and Ji Young Kim and Katherine Giacoletti and Jorge Vázquez and Plinio De los Santos},
  doi          = {10.1080/02664763.2023.2297150},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2382-2401},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian approach for evaluating equivalence over multiple groups, and comparison with frequentist tost},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A characteristic function based circular distribution family
and its goodness of fit: The flexible wrapped linnik family.
<em>JOAS</em>, <em>51</em>(12), 2364–2381. (<a
href="https://doi.org/10.1080/02664763.2023.2283689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the primary aim is to introduce a new flexible family of circular distributions, namely the wrapped Linnik family which possesses the flexibility to model the inflection points and tail behavior often better than the existing popular flexible symmetric unimodal circular models. The second objective of this article is to obtain a simple and efficient estimator of the index parameter α of symmetric Linnik distribution exploiting the fact that it is preserved in the wrapped Linnik family. This is an interesting problem for highly volatile financial data as has been studied by several authors. Our final aim is to analytically derive the asymptotic distribution of our estimator, not available for other estimator. This estimator is shown to outperform the existing estimator over the range of the parameter for all sample sizes. The proposed wrapped Linnik distribution is applied to some real-life data. A measure of goodness of fit proposed in one of the authors&#39; previous works is used for the above family of distributions. The wrapped Linnik family was found to be preferable as it gave better fit to those data sets rather than the popular von-Mises distribution or the wrapped stable family of distributions.},
  archive      = {J_JOAS},
  author       = {Ashis SenGupta and Moumita Roy},
  doi          = {10.1080/02664763.2023.2283689},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2364-2381},
  shortjournal = {J. Appl. Stat.},
  title        = {A characteristic function based circular distribution family and its goodness of fit: The flexible wrapped linnik family},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Determination of the number of clusters through logistic
regression analysis. <em>JOAS</em>, <em>51</em>(12), 2344–2363. (<a
href="https://doi.org/10.1080/02664763.2023.2283687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We advise a novel measure to determine the unknown number of clusters underlying a designated sample through implementation of the parametric logistic regression model. The regression analysis is carried out to estimate the probabilities of inclusion for every individual member from data, irrespective of its parent distribution, to each of the clusters under existence. The proposed one is shown to be superior to its well-known rivals by means of both synthetic and real-world data sets, while designed to significantly reduce the computational burden serving our desired purpose.},
  archive      = {J_JOAS},
  author       = {Soumita Modak},
  doi          = {10.1080/02664763.2023.2283687},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2344-2363},
  shortjournal = {J. Appl. Stat.},
  title        = {Determination of the number of clusters through logistic regression analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GWR-assisted integrated estimator of finite population total
under two-phase sampling: A model-assisted approach. <em>JOAS</em>,
<em>51</em>(12), 2326–2343. (<a
href="https://doi.org/10.1080/02664763.2023.2280879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survey sampling, auxiliary information is used to precisely estimate the finite population parameters. There are several approaches available in the literature that provide a practical method for incorporating auxiliary information during the estimation stage. In order to effectively utilize the auxiliary information, a geographically weighted regression (GWR) model-assisted integrated estimator of finite population total under a two-phase sampling design has been proposed in this article. Spatial simulation studies have been conducted to empirically assess the statistical properties of the proposed estimator. In the presence of spatial non-stationarity, empirical findings reveal that the proposed estimator outperforms all existing estimators such as two-phase HT, ratio, and regression estimators, demonstrating the importance of spatial information in survey sampling.},
  archive      = {J_JOAS},
  author       = {Nobin Chandra Paul and Anil Rai and Tauqueer Ahmad and Ankur Biswas and Prachi Misra Sahoo},
  doi          = {10.1080/02664763.2023.2280879},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2326-2343},
  shortjournal = {J. Appl. Stat.},
  title        = {GWR-assisted integrated estimator of finite population total under two-phase sampling: A model-assisted approach},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase II control charts for monitoring the depth-ratio of
ball-bearings involving three normal variables. <em>JOAS</em>,
<em>51</em>(12), 2298–2325. (<a
href="https://doi.org/10.1080/02664763.2023.2279015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of monitoring the ratio involving three variables, jointly distributed as trivariate normal. The Shewhart-type and two exponentially weighted moving average (EWMA) type schemes for monitoring depth ratio are proposed. The ratio of a normal variable to the average of two other normal variables has wide applications in natural science, production, and engineering. It is defined with slightly different terminology in various contexts, such as depth or aspect ratios. In modern bearing manufacturing, the aspect ratio of width to the average of inner and outer diameters can be an essential indicator of product quality and process stability. While there are many helpful existing charts for monitoring the three components separately or jointly when these characteristics follow a normal distribution, the ratio aspect is often ignored. The Shewhart-type schemes&#39; exact and approximated control limits are considered and analyzed. Numerical results based on Monte-Carlo are conducted using the average run length as a metric with different values of in-control ratio and correlation between the three variables. An application based on the parts manufacturing data illustrates the implementation design of the two control charts. The real-life data analysis shows the efficacy of the proposed monitoring schemes in practice.},
  archive      = {J_JOAS},
  author       = {Li Jin and Amitava Mukherjee and Zhi Song and Jiujun Zhang},
  doi          = {10.1080/02664763.2023.2279015},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2298-2325},
  shortjournal = {J. Appl. Stat.},
  title        = {Phase II control charts for monitoring the depth-ratio of ball-bearings involving three normal variables},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of two statistical methodologies for a binary
classification problem of two-dimensional images. <em>JOAS</em>,
<em>51</em>(12), 2279–2297. (<a
href="https://doi.org/10.1080/02664763.2023.2279012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work intends to compare two statistical classification methods using images as covariates and under the comparison criterion of the ROC curve. The first implemented procedure is based on exploring a mathematical-statistical model using multidimensional arrangements, frequently known as tensors. It is based on the theoretical framework of the high-dimensional generalized linear model. The second methodology is situated in the field of functional data analysis, particularly in the space of functions that have a finite measure of the total variation. A simulation study is carried out to compare both classification methodologies using the area under the ROC curve (AUC). The model based on functional data had better performance than the tensor model. A real data application using medical images is presented.},
  archive      = {J_JOAS},
  author       = {Deniz A. Sanchez S. and Rubén D. Guevara G. and Sergio A. Calderón V.},
  doi          = {10.1080/02664763.2023.2279012},
  journal      = {Journal of Applied Statistics},
  month        = {9},
  number       = {12},
  pages        = {2279-2297},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparison of two statistical methodologies for a binary classification problem of two-dimensional images},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computationally efficient sequential regression imputation
algorithm for multilevel data. <em>JOAS</em>, <em>51</em>(11),
2258–2278. (<a
href="https://doi.org/10.1080/02664763.2023.2277669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the computational burden, especially in high-dimensional settings, sequential imputation may not be practical. In this paper, we adopt computationally advantageous methods by sampling the missing data from their perspective predictive distributions, which leads to significantly improved computation time in the class of variable-by-variable imputation algorithms. We assess the computational performance in a comprehensive simulation study. We then compare and contrast the performance of our algorithm with commonly used alternatives. The results show that our method has a significant advantage over the commonly used alternatives with respect to computational efficiency and inferential quality. Finally, we demonstrate our methods in a substantive problem aimed at investigating the effects of area-level behavioral, socioeconomic, and demographic characteristics on poor birth outcomes in New York State among singleton births.},
  archive      = {J_JOAS},
  author       = {Tugba Akkaya Hocagil and Recai M. Yucel},
  doi          = {10.1080/02664763.2023.2277669},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2258-2278},
  shortjournal = {J. Appl. Stat.},
  title        = {A computationally efficient sequential regression imputation algorithm for multilevel data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoothing level selection for density estimators based on
the moments. <em>JOAS</em>, <em>51</em>(11), 2232–2257. (<a
href="https://doi.org/10.1080/02664763.2023.2277125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an approach to select the bandwidth or smoothing parameter in multiresolution (MR) density estimation and nonparametric density estimation. It is based on the evolution of the second, third and fourth central moments and the shape of the estimated densities for different bandwidths and resolution levels. The proposed method has been applied to density estimation by means of multiresolution densities as well as kernel density estimation (MRDE and KDE respectively). The results of the simulations and the empirical application demonstrate that the level of resolution resulting from the moments method performs better with multimodal densities than the Bayesian Information Criterion (BIC) for multiresolution densities estimation and the plug-in for kernel densities estimation.},
  archive      = {J_JOAS},
  author       = {Rosa M. García-Fernández and Federico Palacios-González},
  doi          = {10.1080/02664763.2023.2277125},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2232-2257},
  shortjournal = {J. Appl. Stat.},
  title        = {Smoothing level selection for density estimators based on the moments},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exact projection pursuit-based algorithm for multivariate
two-sample nonparametric testing applicable to retrospective and group
sequential studies. <em>JOAS</em>, <em>51</em>(11), 2214–2231. (<a
href="https://doi.org/10.1080/02664763.2023.2277118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric tests for equality of multivariate distributions are frequently desired in research. It is commonly required that test-procedures based on relatively small samples of vectors accurately control the corresponding Type I Error (TIE) rates. Often, in the multivariate testing, extensions of null-distribution-free univariate methods, e.g., Kolmogorov-Smirnov and Cramér-von Mises type schemes, are not exact, since their null distributions depend on underlying data distributions. The present paper extends the density-based empirical likelihood technique in order to nonparametrically approximate the most powerful test for the multivariate two-sample (MTS) problem, yielding an exact finite-sample test statistic. We rigorously apply one-to-one-mapping between the equality of vectors&#39; distributions and the equality of distributions of relevant univariate linear projections. We establish a general algorithm that simplifies the use of projection pursuit, employing only a few of the infinitely many linear combinations of observed vectors&#39; components. The displayed distribution-free strategy is employed in retrospective and group sequential manners. A novel MTS nonparametric procedure in the group sequential manner is proposed. The asymptotic consistency of the proposed technique is shown. Monte Carlo studies demonstrate that the proposed procedures exhibit extremely high and stable power characteristics across a variety of settings. Supplementary materials for this article are available online.},
  archive      = {J_JOAS},
  author       = {Li Zou and Gregory Gurevich and Ablert Vexler},
  doi          = {10.1080/02664763.2023.2277118},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2214-2231},
  shortjournal = {J. Appl. Stat.},
  title        = {An exact projection pursuit-based algorithm for multivariate two-sample nonparametric testing applicable to retrospective and group sequential studies},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust estimation and bias-corrected empirical likelihood in
generalized linear models with right censored data. <em>JOAS</em>,
<em>51</em>(11), 2197–2213. (<a
href="https://doi.org/10.1080/02664763.2023.2277117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the robust estimation and empirical likelihood for the regression parameter in generalized linear models with right censored data. A robust estimating equation is proposed to estimate the regression parameter, and the resulting estimator has consistent and asymptotic normality. A bias-corrected empirical log-likelihood ratio statistic of the regression parameter is constructed, and it is shown that the statistic converges weakly to a standard χ 2 distribution. The result can be directly used to construct the confidence region of regression parameter. We use the bias correction method to directly calibrate the empirical log-likelihood ratio, which does not need to be multiplied by an adjustment factor. We also propose a method for selecting the tuning parameters in the loss function. Simulation studies show that the estimator of the regression parameter is robust and the bias-corrected empirical likelihood is better than the normal approximation method. An example of a real dataset from Alzheimer&#39;s disease studies shows that the proposed method can be applied in practical problems.},
  archive      = {J_JOAS},
  author       = {Liugen Xue and Junshan Xie and Xiaohui Yang},
  doi          = {10.1080/02664763.2023.2277117},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2197-2213},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust estimation and bias-corrected empirical likelihood in generalized linear models with right censored data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A longitudinal study of the influence of air pollutants on
children: A robust multivariate approach. <em>JOAS</em>,
<em>51</em>(11), 2178–2196. (<a
href="https://doi.org/10.1080/02664763.2023.2272228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to evaluate the statistical association between exposure to air pollution and forced expiratory volume in the first second (FEV 1 ) in both asthmatic and non-asthmatic children and teenagers, in which the response variable FEV 1 was repeatedly measured on a monthly basis, characterizing a longitudinal experiment. Due to the nature of the data, an robust linear mixed model (RLMM), combined with a robust principal component analysis (RPCA), is proposed to handle the multicollinearity among the covariates and the impact of extreme observations (high levels of air contaminants) on the estimates. The Huber and Tukey loss functions are considered to obtain robust estimators of the parameters in the linear mixed model (LMM). A finite sample size investigation is conducted under the scenario where the covariates follow linear time series models with and without additive outliers (AO). The impact of the time-correlation and the outliers on the estimates of the fixed effect parameters in the LMM is investigated. In the real data analysis, the robust model strategy evidenced that RPCA exhibits three principal component (PC), mainly related to relative humidity (Hmd), particulate matter with a diameter smaller than 10 μm (PM 10 ) and particulate matter with a diameter smaller than 2.5 μm (PM 2.5 ).},
  archive      = {J_JOAS},
  author       = {Ian Meneghel Danilevicz and Pascal Bondon and Valdério Anselmo Reisen and Faradiba Sarquis Serpa},
  doi          = {10.1080/02664763.2023.2272228},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2178-2196},
  shortjournal = {J. Appl. Stat.},
  title        = {A longitudinal study of the influence of air pollutants on children: A robust multivariate approach},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of time-varying kernel densities and chronology
of the impact of COVID-19 on financial markets. <em>JOAS</em>,
<em>51</em>(11), 2157–2177. (<a
href="https://doi.org/10.1080/02664763.2023.2272226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-varying kernel density estimation relies on two free parameters: the bandwidth and the discount factor. We propose to select these parameters so as to minimize a criterion consistent with the traditional requirements of the validation of a probability density forecast. These requirements are both the uniformity and the independence of the so-called probability integral transforms, which are the forecast time-varying cumulated distributions applied to the observations. We thus build a new numerical criterion incorporating both the uniformity and independence properties by the mean of an adapted Kolmogorov–Smirnov statistic. We apply this method to financial markets during the onset of the COVID-19 crisis. We determine the time-varying density of daily price returns of several stock indices and, using various divergence statistics, we are able to describe the chronology of the crisis as well as regional disparities. For instance, we observe a more limited impact of COVID-19 on financial markets in China, a strong impact in the US, and a slow recovery in Europe.},
  archive      = {J_JOAS},
  author       = {Matthieu Garcin and Jules Klein and Sana Laaribi},
  doi          = {10.1080/02664763.2023.2272226},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2157-2177},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of time-varying kernel densities and chronology of the impact of COVID-19 on financial markets},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian transformation model for spatial partly
interval-censored data. <em>JOAS</em>, <em>51</em>(11), 2139–2156. (<a
href="https://doi.org/10.1080/02664763.2023.2263819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformation model with partly interval-censored data offers a highly flexible modeling framework that can simultaneously support multiple common survival models and a wide variety of censored data types. However, the real data may contain unexplained heterogeneity that cannot be entirely explained by covariates and may be brought on by a variety of unmeasured regional characteristics. Due to this, we introduce the conditionally autoregressive prior into the transformation model with partly interval-censored data and take the spatial frailty into account. An efficient Markov chain Monte Carlo method is proposed to handle the posterior sampling and model inference. The approach is simple to use and does not include any challenging Metropolis steps owing to four-stage data augmentation. Through several simulations, the suggested method&#39;s empirical performance is assessed and then the method is used in a leukemia study.},
  archive      = {J_JOAS},
  author       = {Mingyue Qiu and Tao Hu},
  doi          = {10.1080/02664763.2023.2263819},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2139-2156},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian transformation model for spatial partly interval-censored data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating linear mixed effect models with non-normal random
effects through saddlepoint approximation and its application in retail
pricing analytics. <em>JOAS</em>, <em>51</em>(11), 2116–2138. (<a
href="https://doi.org/10.1080/02664763.2023.2260576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Mixed Effects (LME) models are powerful statistical tools that have been employed in many different real-world applications such as retail data analytics, marketing measurement, and medical research. Statistical inference is often conducted via maximum likelihood estimation with Normality assumptions on the random effects. Nevertheless, for many applications in the retail industry, it is often necessary to consider non-Normal distributions on the random effects when considering the unknown parameters&#39; business interpretations. Motivated by this need, a linear mixed effects model with possibly non-Normal distribution is studied in this research. We propose a general estimating framework based on a saddlepoint approximation (SA) of the probability density function of the dependent variable, which leads to constrained nonlinear optimization problems. The classical LME model with Normality assumption can then be viewed as a special case under the proposed general SA framework. Compared with the existing approach, the proposed method enhances the real-world interpretability of the estimates with satisfactory model fits.},
  archive      = {J_JOAS},
  author       = {Hao Chen and Lanshan Han and Alvin Lim},
  doi          = {10.1080/02664763.2023.2260576},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2116-2138},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating linear mixed effect models with non-normal random effects through saddlepoint approximation and its application in retail pricing analytics},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the prevalence of osteoporosis using ranked-based
methodologies and manitoba’s population-based BMD registry.
<em>JOAS</em>, <em>51</em>(11), 2090–2115. (<a
href="https://doi.org/10.1080/02664763.2023.2260572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoporosis is a metabolic bone disorder that is characterized by reduced bone mineral density (BMD) and deterioration of bone microarchitecture. Osteoporosis is highly prevalent among women over 50, leading to skeletal fragility and risk of fracture. Early diagnosis and treatment of those at high risk for fracture is very important in order to avoid morbidity, mortality and economic burden from preventable fractures. The province of Manitoba established a BMD testing program in 1997. The Manitoba BMD registry is now the largest population-based BMD registry in the world, and has detailed information on fracture outcomes and other covariates for over 160,000 BMD assessments. In this paper, we develop a number of methodologies based on ranked-set type sampling designs to estimate the prevalence of osteoporosis among women of age 50 and older in the province of Manitoba. We use a parametric approach based on finite mixture models, as well as the usual approaches using simple random and stratified sampling designs. Results are obtained under perfect and imperfect ranking scenarios while the sampling and ranking costs are incorporated into the study. We observe that rank-based methodologies can be used as cost-efficient methods to monitor the prevalence of osteoporosis.},
  archive      = {J_JOAS},
  author       = {Sedigheh Omidvar and Mohammad Jafari Jozani and Nader Nematollahi and Wiliam D. Leslie},
  doi          = {10.1080/02664763.2023.2260572},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2090-2115},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating the prevalence of osteoporosis using ranked-based methodologies and manitoba&#39;s population-based BMD registry},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The zero-and-plus/minus-one inflated extended-poisson
distribution. <em>JOAS</em>, <em>51</em>(11), 2062–2089. (<a
href="https://doi.org/10.1080/02664763.2023.2260570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new distribution defined on ℤ Z Z , called the ZPMOIEP ZPMOIEP ZPMOIEP distribution, which can be viewed as a natural extension of the zero-and-one-inflated Poisson ( ZOIP ZOIP ZOIP ) distribution. It is designed to fit the count data with potentially excess zeros and/or ones, and/or minus ones. We explore its various properties and investigate the estimation of the unknown parameters. Moreover, simulation experiments are carried out to attest to the performance of the estimation. Through the use of a useful data set on football scores, the applicability of the proposed distribution is examined.},
  archive      = {J_JOAS},
  author       = {Maher Kachour and Christophe Chesneau},
  doi          = {10.1080/02664763.2023.2260570},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2062-2089},
  shortjournal = {J. Appl. Stat.},
  title        = {The zero-and-plus/minus-one inflated extended-poisson distribution},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The spike-and-slab lasso and scalable algorithm to
accommodate multinomial outcomes in variable selection problems.
<em>JOAS</em>, <em>51</em>(11), 2039–2061. (<a
href="https://doi.org/10.1080/02664763.2023.2258301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike-and-slab prior distributions are used to impose variable selection in Bayesian regression-style problems with many possible predictors. These priors are a mixture of two zero-centered distributions with differing variances, resulting in different shrinkage levels on parameter estimates based on whether they are relevant to the outcome. The spike-and-slab lasso assigns mixtures of double exponential distributions as priors for the parameters. This framework was initially developed for linear models, later developed for generalized linear models, and shown to perform well in scenarios requiring sparse solutions. Standard formulations of generalized linear models cannot immediately accommodate categorical outcomes with &gt; 2 categories, i.e. multinomial outcomes, and require modifications to model specification and parameter estimation. Such modifications are relatively straightforward in a Classical setting but require additional theoretical and computational considerations in Bayesian settings, which can depend on the choice of prior distributions for the parameters of interest. While previous developments of the spike-and-slab lasso focused on continuous, count, and/or binary outcomes, we generalize the spike-and-slab lasso to accommodate multinomial outcomes, developing both the theoretical basis for the model and an expectation-maximization algorithm to fit the model. To our knowledge, this is the first generalization of the spike-and-slab lasso to allow for multinomial outcomes.},
  archive      = {J_JOAS},
  author       = {Justin M. Leach and Nengjun Yi and Inmaculada Aban and The Alzheimer&#39;s Disease Neuroimaging Initiative},
  doi          = {10.1080/02664763.2023.2258301},
  journal      = {Journal of Applied Statistics},
  month        = {8},
  number       = {11},
  pages        = {2039-2061},
  shortjournal = {J. Appl. Stat.},
  title        = {The spike-and-slab lasso and scalable algorithm to accommodate multinomial outcomes in variable selection problems},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel two-way functional linear model with applications in
human mortality data analysis. <em>JOAS</em>, <em>51</em>(10),
2025–2038. (<a
href="https://doi.org/10.1080/02664763.2023.2253379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, two-way or longitudinal functional data analysis has attracted much attention in many fields. However, little is known on how to appropriately characterize the association between two-way functional predictor and scalar response. Motivated by a mortality study, in this paper, we propose a novel two-way functional linear model, where the response is a scalar and functional predictor is two-way trajectory. The model is intuitive, interpretable and naturally captures relationship between each way of two-way functional predictor and scalar-type response. Further, we develop a new estimation method to estimate the regression functions in the framework of weak separability. The main technical tools for the construction of the regression functions are product functional principal component analysis and iterative least square procedure. The solid performance of our method is demonstrated in extensive simulation studies. We also analyze the mortality dataset to illustrate the usefulness of the proposed procedure.},
  archive      = {J_JOAS},
  author       = {Xingyu Yan and Jiaqian Yu and Weiyong Ding and Hao Wang and Peng Zhao},
  doi          = {10.1080/02664763.2023.2253379},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {2025-2038},
  shortjournal = {J. Appl. Stat.},
  title        = {A novel two-way functional linear model with applications in human mortality data analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction and model evaluation for space–time data.
<em>JOAS</em>, <em>51</em>(10), 2007–2024. (<a
href="https://doi.org/10.1080/02664763.2023.2252208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation metrics for prediction error, model selection and model averaging on space–time data are understudied and poorly understood. The absence of independent replication makes prediction ambiguous as a concept and renders evaluation procedures developed for independent data inappropriate for most space–time prediction problems. Motivated by air pollution data collected during California wildfires in 2008, this manuscript attempts a formalization of the true prediction error associated with spatial interpolation. We investigate a variety of cross-validation (CV) procedures employing both simulations and case studies to provide insight into the nature of the estimand targeted by alternative data partition strategies. Consistent with recent best practice, we find that location-based cross-validation is appropriate for estimating spatial interpolation error as in our analysis of the California wildfire data. Interestingly, commonly held notions of bias-variance trade-off of CV fold size do not trivially apply to dependent data, and we recommend leave-one-location-out (LOLO) CV as the preferred prediction error metric for spatial interpolation.},
  archive      = {J_JOAS},
  author       = {G. L. Watson and C. E. Reid and M. Jerrett and D. Telesca},
  doi          = {10.1080/02664763.2023.2252208},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {2007-2024},
  shortjournal = {J. Appl. Stat.},
  title        = {Prediction and model evaluation for space–time data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point estimation and related classification problems for
several lindley populations with application using COVID-19 data.
<em>JOAS</em>, <em>51</em>(10), 1976–2006. (<a
href="https://doi.org/10.1080/02664763.2023.2251100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problems of point estimation and classification under the assumption that the training data follow a Lindley distribution are considered. Bayes estimators are derived for the parameter of the Lindley distribution applying the Markov chain Monte Carlo (MCMC), and Tierney and Kadane&#39;s [Tierney and Kadane, Accurate approximations for posterior moments and marginal densities , J. Amer. Statist. Assoc. 81 (1986), pp. 82–86] methods. In the sequel, we prove that the Bayes estimators using Tierney and Kadane&#39;s approximation and Lindley&#39;s approximation both converge to the maximum likelihood estimator (MLE), as n → ∞ , where n is the sample size. The performances of all the proposed estimators are compared with some of the existing ones using bias and mean squared error (MSE), numerically. It has been noticed from our simulation study that the proposed estimators perform better than some of the existing ones. Applying these estimators, we construct several plug-in type classification rules and a rule that uses the likelihood accordance function. The performances of each of the rules are numerically evaluated using the expected probability of misclassification (EPM). Two real-life examples related to COVID-19 disease are considered for illustrative purposes.},
  archive      = {J_JOAS},
  author       = {Debasmita Bal and Manas Ranjan Tripathy and Somesh Kumar},
  doi          = {10.1080/02664763.2023.2251100},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1976-2006},
  shortjournal = {J. Appl. Stat.},
  title        = {Point estimation and related classification problems for several lindley populations with application using COVID-19 data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goodness-of-fit test for the one-sided lévy distribution.
<em>JOAS</em>, <em>51</em>(10), 1961–1975. (<a
href="https://doi.org/10.1080/02664763.2023.2251098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of this work is to develop a new goodness-of-fit test for the one-sided Lévy distribution. The proposed test is based on the scale-ratio approach in which two estimators of the scale parameter of one-sided Lévy distribution are confronted. The asymptotic distribution of the test statistic is obtained under null hypotheses. The performance of the test is demonstrated using simulated observations from various known distributions. Finally, two real-world datasets are analyzed.},
  archive      = {J_JOAS},
  author       = {Aditi Kumari and Deepesh Bhati},
  doi          = {10.1080/02664763.2023.2251098},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1961-1975},
  shortjournal = {J. Appl. Stat.},
  title        = {Goodness-of-fit test for the one-sided lévy distribution},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial analysis for interval-valued data. <em>JOAS</em>,
<em>51</em>(10), 1946–1960. (<a
href="https://doi.org/10.1080/02664763.2023.2249636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic data analysis deals with complex data with symbolic objects, such as lists, histograms, and intervals. Spatial analysis for symbolic data is relatively underexplored. To fill the gap, this paper proposes a statistical framework for spatial interval-valued data (SIVD) analysis. We provide geostatistical methods for spatial prediction, predictive performance measure for prediction assessment, and visualization for mapping SIVD. The proposed methods are illustrated with both simulated and real examples.},
  archive      = {J_JOAS},
  author       = {Austin Workman and Joon Jin Song},
  doi          = {10.1080/02664763.2023.2249636},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1946-1960},
  shortjournal = {J. Appl. Stat.},
  title        = {Spatial analysis for interval-valued data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GARTFIMA process and its empirical spectral density based
estimation. <em>JOAS</em>, <em>51</em>(10), 1919–1945. (<a
href="https://doi.org/10.1080/02664763.2023.2249270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a Gegenbauer autoregressive tempered fractionally integrated moving average process. We work on the spectral density and autocovariance function for the introduced process. The parameter estimation is done using the empirical spectral density with the help of the nonlinear least square technique and the Whittle likelihood estimation technique. The performance of the proposed estimation techniques is assessed on simulated data. Further, the introduced process is shown to better model the real-world data in comparison to other time series models.},
  archive      = {J_JOAS},
  author       = {Niharika Bhootna and Arun Kumar},
  doi          = {10.1080/02664763.2023.2249270},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1919-1945},
  shortjournal = {J. Appl. Stat.},
  title        = {GARTFIMA process and its empirical spectral density based estimation},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel mixed and kernel stochastic restricted ridge
predictions in the partially linear mixed measurement error models: An
application to COVID-19. <em>JOAS</em>, <em>51</em>(10), 1894–1918. (<a
href="https://doi.org/10.1080/02664763.2023.2248418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we define mixed predictor and stochastic restricted ridge predictor of partially linear mixed measurement error models by taking advantage of Kernel approximation. Under matrix mean square error criterion, we make the comparison of the superiorities the linear combinations of the new defined predictors. Then we investigate the asymptotic normality characteristics and the situation of the unknown covariance matrix of measurement errors. Finally, the study is ended with a Monte Carlo simulation study and COVID-19 data application.},
  archive      = {J_JOAS},
  author       = {Özge Kuran and Seçil Yalaz},
  doi          = {10.1080/02664763.2023.2248418},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1894-1918},
  shortjournal = {J. Appl. Stat.},
  title        = {Kernel mixed and kernel stochastic restricted ridge predictions in the partially linear mixed measurement error models: An application to COVID-19},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical clustering of documents via stochastic
blockmodels. <em>JOAS</em>, <em>51</em>(10), 1878–1893. (<a
href="https://doi.org/10.1080/02664763.2023.2247617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the online market grows rapidly, people are relying more on product review when they purchase the product. Hence, many companies and researchers are interested in analyzing product review which essentially a text data. In the current literature, it is common to use only text analysis tools to analyze text dataset. But in our work, we propose a method that utilizes both text analysis method such as topic modeling and statistical network model to build network among individuals and find interesting communities. We introduce a promising framework that incorporates topic modeling technique to define the edges among the individuals and form a network and uses stochastic blockmodels (SBM) to find the communities. The power of our proposed method is demonstrated in real-world application to Amazon product review dataset.},
  archive      = {J_JOAS},
  author       = {Paul H. Atandoh and Kevin H. Lee},
  doi          = {10.1080/02664763.2023.2247617},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1878-1893},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical clustering of documents via stochastic blockmodels},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple comparisons of treatment against control under
unequal variances using parametric bootstrap. <em>JOAS</em>,
<em>51</em>(10), 1861–1877. (<a
href="https://doi.org/10.1080/02664763.2023.2245179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In one-way analysis of variance models, performing simultaneous multiple comparisons of treatment groups with a control group may be of interest. Dunnett&#39;s test is used to test such differences and assumes equal variances of the response variable for each group. This assumption is not always met even after transformation. A parametric bootstrap (PB) method is developed here for comparing multiple treatment group means against the control group with unequal variances and unbalanced data. In simulation studies, the proposed method outperformed Dunnett&#39;s test in controlling the type I error under various settings, particularly when data have heteroscedastic variance and unbalanced design. Simulations show that power is often lower for the PB method than for Dunnett&#39;s test under equal variance, balanced data, or smaller sample size, but similar to or higher than for Dunnett&#39;s test with unequal variance, unbalanced data and larger sample size. The method is applied to a dataset concerning isotope levels found in elephant tusks from various geographical areas. These data have very unbalanced group sizes and unequal variances. This example illustrates that the PB method is easy to implement and avoids the need for transforming data to meet the equal variance assumption, simplifying interpretation of results.},
  archive      = {J_JOAS},
  author       = {Sarah Alver and Guoyi Zhang},
  doi          = {10.1080/02664763.2023.2245179},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1861-1877},
  shortjournal = {J. Appl. Stat.},
  title        = {Multiple comparisons of treatment against control under unequal variances using parametric bootstrap},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The cluster d-trace loss for differential network analysis.
<em>JOAS</em>, <em>51</em>(10), 1843–1860. (<a
href="https://doi.org/10.1080/02664763.2023.2245178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing literature suggests that gene expression can be greatly altered in disease conditions, and identifying those changes will improve the understanding of complex diseases such as cancers or diabetes. A prevailing direction in the analysis of gene expression studies the changes in gene pathways which include sets of related genes. Therefore, introducing structured exploration to differential analysis of gene expression networks may lead to meaningful discoveries. The topic of this paper is differential network analysis, which focuses on capturing the differences between two or more precision matrices. We discuss the connection between the thresholding method and the D-trace loss method on differential network analysis in the case that the precision matrices share the common connected components. Based on this connection, we further propose the cluster D-trace loss method which directly estimates the differential network and achieves model selection consistency. Simulation studies demonstrate its improved performance and computational efficiency. Finally, the usefulness of our proposed estimator is demonstrated by a real-data analysis on non-small cell lung cancer.},
  archive      = {J_JOAS},
  author       = {Han Yan and Shuhan Lu and Sanguo Zhang},
  doi          = {10.1080/02664763.2023.2245178},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {10},
  pages        = {1843-1860},
  shortjournal = {J. Appl. Stat.},
  title        = {The cluster D-trace loss for differential network analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive accuracy of time series models applied to
economic data: The european countries retail trade. <em>JOAS</em>,
<em>51</em>(9), 1818–1841. (<a
href="https://doi.org/10.1080/02664763.2023.2238249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and accurately forecasting trend and seasonal patterns of a time series is a crucial activity in economics. The main propose of this study is to evaluate and compare the performance of three traditional forecasting methods, namely the ARIMA models and their extensions, the classical decomposition time series associated with multiple linear regression models with correlated errors, and the Holt–Winters method. These methodologies are applied to retail time series from seven different European countries that present strong trend and seasonal fluctuations. In general, the results indicate that all the forecasting models somehow follow the seasonal pattern exhibited in the data. Based on mean squared error (MSE), root mean squared error (RMSE), mean absolute percentage error (MAPE), mean absolute scaled error (MASE) and U-Theil statistic, the results demonstrate the superiority of the ARIMA model over the other two forecasting approaches. Holt–Winters method also produces accurate forecasts, so it is considered a viable alternative to ARIMA. The performance of the forecasting methods in terms of coverage rates matches the results for accuracy measures.},
  archive      = {J_JOAS},
  author       = {S. Lima and A. M. Gonçalves and M. Costa},
  doi          = {10.1080/02664763.2023.2238249},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1818-1841},
  shortjournal = {J. Appl. Stat.},
  title        = {Predictive accuracy of time series models applied to economic data: The european countries retail trade},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lindley–binomial model for analyzing the proportions with
sparseness and excessive zeros. <em>JOAS</em>, <em>51</em>(9),
1792–1817. (<a
href="https://doi.org/10.1080/02664763.2023.2237212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proportional data arise frequently in a wide variety of fields of study. Such data often exhibit extra variation such as over/under dispersion, sparseness and zero inflation. For example, the hepatitis data present both sparseness and zero inflation with 19 contributing non-zero denominators of 5 or less and with 36 having zero seropositive out of 83 annual age groups. The whitefly data consists of 640 observations with 339 zeros (53%), which demonstrates extra zero inflation. The catheter management data involve excessive zeros with over 60% zeros averagely for outcomes of 193 urinary tract infections, 194 outcomes of catheter blockages and 193 outcomes of catheter displacements. However, the existing models cannot always address such features appropriately. In this paper, a new two-parameter probability distribution called Lindley–binomial (LB) distribution is proposed to analyze the proportional data with such features. The probabilistic properties of the distribution such as moment, moment generating function are derived. The Fisher scoring algorithm and EM algorithm are presented for the computation of estimates of parameters in the proposed LB regression model. The issues on goodness of fit for the LB model are discussed. A limited simulation study is also performed to evaluate the performance of derived EM algorithms for the estimation of parameters in the model with/without covariates. The proposed model is illustrated through three aforementioned proportional datasets.},
  archive      = {J_JOAS},
  author       = {Dianliang Deng and Xiaoqing Zhang},
  doi          = {10.1080/02664763.2023.2237212},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1792-1817},
  shortjournal = {J. Appl. Stat.},
  title        = {A lindley–binomial model for analyzing the proportions with sparseness and excessive zeros},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical bayesian analysis for bivariate weibull
distribution under left-censoring scheme. <em>JOAS</em>, <em>51</em>(9),
1772–1791. (<a
href="https://doi.org/10.1080/02664763.2023.2235093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for analyzing bivariate positive data, taking into account a covariate vector and left-censored observations, by introducing a hierarchical Bayesian analysis. The proposed method assumes marginal Weibull distributions and employs either a usual Weibull likelihood or Weibull–Tobit likelihood approaches. A latent variable or frailty is included in the model to capture the possible correlation between the bivariate responses for the same sampling unit. The posterior summaries of interest are obtained through Markov Chain Monte Carlo methods. To demonstrate the effectiveness of the proposed methodology, we apply it to a bivariate data set from stellar astronomy that includes left-censored observations and covariates. Our results indicate that the new bivariate model approach, which incorporates the latent factor to capture the potential dependence between the two responses of interest, produces accurate inference results. We also compare the two models using the different likelihood approaches (Weibull or Weibull–Tobit likelihoods) in the application. Overall, our findings suggest that the proposed hierarchical Bayesian analysis is a promising approach for analyzing bivariate positive data with left-censored observations and covariate information.},
  archive      = {J_JOAS},
  author       = {Danielle Peralta and Ricardo Puziol de Oliveira and Jorge Alberto Achcar},
  doi          = {10.1080/02664763.2023.2235093},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1772-1791},
  shortjournal = {J. Appl. Stat.},
  title        = {A hierarchical bayesian analysis for bivariate weibull distribution under left-censoring scheme},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting dichotomised outcomes from high-dimensional data
in biomedicine. <em>JOAS</em>, <em>51</em>(9), 1756–1771. (<a
href="https://doi.org/10.1080/02664763.2023.2233057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many biomedical applications, we are more interested in the predicted probability that a numerical outcome is above a threshold than in the predicted value of the outcome. For example, it might be known that antibody levels above a certain threshold provide immunity against a disease, or a threshold for a disease severity score might reflect conversion from the presymptomatic to the symptomatic disease stage. Accordingly, biomedical researchers often convert numerical to binary outcomes (loss of information) to conduct logistic regression (probabilistic interpretation). We address this bad statistical practice by modelling the binary outcome with logistic regression, modelling the numerical outcome with linear regression, transforming the predicted values from linear regression to predicted probabilities, and combining the predicted probabilities from logistic and linear regression. Analysing high-dimensional simulated and experimental data, namely clinical data for predicting cognitive impairment, we obtain significantly improved predictions of dichotomised outcomes. Thus, the proposed approach effectively combines binary with numerical outcomes to improve binary classification in high-dimensional settings. An implementation is available in the R package cornet on GitHub ( https://github.com/rauschenberger/cornet ) and CRAN ( https://CRAN.R-project.org/package=cornet ).},
  archive      = {J_JOAS},
  author       = {Armin Rauschenberger and Enrico Glaab},
  doi          = {10.1080/02664763.2023.2233057},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1756-1771},
  shortjournal = {J. Appl. Stat.},
  title        = {Predicting dichotomised outcomes from high-dimensional data in biomedicine},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Family of bivariate distributions on the unit square:
Theoretical properties and applications. <em>JOAS</em>, <em>51</em>(9),
1729–1755. (<a
href="https://doi.org/10.1080/02664763.2023.2232127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the bivariate unit-log-symmetric model based on the bivariate log-symmetric distribution (BLS) defined in Vila et al. [25] as a flexible family of bivariate distributions over the unit square. We then study its mathematical properties such as stochastic representations, quantiles, conditional distributions, independence of the marginal distributions and marginal moments. Maximum likelihood estimation method is discussed and examined through Monte Carlo simulation. Finally, the proposed model is used to analyze some soccer data sets.},
  archive      = {J_JOAS},
  author       = {Roberto Vila and Narayanaswamy Balakrishnan and Helton Saulo and Peter Zörnig},
  doi          = {10.1080/02664763.2023.2232127},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1729-1755},
  shortjournal = {J. Appl. Stat.},
  title        = {Family of bivariate distributions on the unit square: Theoretical properties and applications},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing serial correlation in a general d-factor model with
possible infinite variance. <em>JOAS</em>, <em>51</em>(9), 1709–1728.
(<a href="https://doi.org/10.1080/02664763.2023.2231175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-known that the presence of serial correlation may result in an inefficient or even biased estimation in time series analysis. In this paper, we consider testing serial correlation in a general d -factor model when the model errors follow the GARCH process, which is frequently used in modeling financial data. Two empirical likelihood-based testing statistics are suggested as a way to deal with problems that might come up with infinite variance. Both statistics are shown to be chi-squared distributed asymptotically under mild conditions. Simulations confirm the excellent finite-sample performance of both tests. Finally, to emphasize the importance of using our tests, we explore the impact of the exchange rate on the stock return using both monthly and daily data from eight countries.},
  archive      = {J_JOAS},
  author       = {Yawen Fan and Xiaohui Liu and Ting Luo and Yao Rao and Hanqing Li},
  doi          = {10.1080/02664763.2023.2231175},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1709-1728},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing serial correlation in a general d-factor model with possible infinite variance},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Current status data with two competing risks and
time-dependent missing failure types. <em>JOAS</em>, <em>51</em>(9),
1689–1708. (<a
href="https://doi.org/10.1080/02664763.2023.2231174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In competing risks data, in practice, there may be lack of information or uncertainty about the true failure type, termed as ‘missing failure type’, for some subjects. We consider a general pattern of missing failure type in which we observe, if not the true failure type, a set of possible failure types containing the true one. In this work, we focus on both parametric and non-parametric estimation based on current status data with two competing risks and the above-mentioned missing failure type. Here, the missing probabilities are assumed to be time-dependent, that is, dependent on both failure and monitoring time points, in addition to being dependent on the true failure type. This makes the missing mechanism non-ignorable. We carry out maximum likelihood estimation and obtain the asymptotic properties of the estimators. Simulation studies are conducted to investigate the finite sample properties of the estimators. Finally, the methods are illustrated through a data set on hearing loss.},
  archive      = {J_JOAS},
  author       = {Tamalika Koley and Anup Dewanji},
  doi          = {10.1080/02664763.2023.2231174},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1689-1708},
  shortjournal = {J. Appl. Stat.},
  title        = {Current status data with two competing risks and time-dependent missing failure types},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation procedures and optimal censoring schemes for an
improved adaptive progressively type-II censored weibull distribution.
<em>JOAS</em>, <em>51</em>(9), 1664–1688. (<a
href="https://doi.org/10.1080/02664763.2023.2230536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an effort to investigate the estimations of the Weibull distribution using an improved adaptive Type-II progressive censoring scheme. This scheme effectively guarantees that the experimental time will not exceed a pre-fixed time. The point and interval estimations using two classical estimation methods, namely maximum likelihood and maximum product of spacing, are considered to estimate the unknown parameters as well as the reliability and hazard rate functions. The approximate confidence intervals of these quantities are obtained based on the asymptotic normality of the maximum likelihood and maximum product of spacing methods. The Bayesian estimations are also considered using MCMC techniques based on the two classical approaches. An extensive simulation study is implemented to compare the performance of the different methods. Further, we propose the use of various optimality criteria to find the optimal sampling scheme. Finally, one real data set is applied to show how the proposed estimators and the optimality criteria work in real-life scenarios. The numerical outcomes demonstrated that the Bayesian estimates using the likelihood and product of spacing functions performed better than the classical estimates.},
  archive      = {J_JOAS},
  author       = {Mazen Nassar and Ahmed Elshahhat},
  doi          = {10.1080/02664763.2023.2230536},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1664-1688},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation procedures and optimal censoring schemes for an improved adaptive progressively type-II censored weibull distribution},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The generalized odd log-logistic-g regression with
interval-censored survival data. <em>JOAS</em>, <em>51</em>(9),
1642–1663. (<a
href="https://doi.org/10.1080/02664763.2023.2230533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes a new regression based on the generalized odd log-logistic family for interval-censored data. The survival times are not observed for this type of data, and the event of interest occurs at some random interval. This family can be used in interval modeling since it generalizes some popular lifetime distributions in addition to its ability to present various forms of the risk function. The estimation of the parameters is addressed by the classical and Bayesian methods. We examine the behavior of the estimates for some sample sizes and censorship percentages. Selection criteria, likelihood ratio tests, residual analysis, and graphical techniques assess the goodness of fit of the fitted models. The usefulness of the proposed models is red shown by means of two real data sets.},
  archive      = {J_JOAS},
  author       = {Valdemiro P. Vigas and Edwin M. M. Ortega and Adriano K. Suzuki and Gauss M. Cordeiro and Paulo C. dos Santos Junior},
  doi          = {10.1080/02664763.2023.2230533},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1642-1663},
  shortjournal = {J. Appl. Stat.},
  title        = {The generalized odd log-logistic-G regression with interval-censored survival data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling and monitoring social network change based on
exponential random graph models. <em>JOAS</em>, <em>51</em>(9),
1621–1641. (<a
href="https://doi.org/10.1080/02664763.2023.2230530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to detect anomalous changes in social network structure in real time and to offer early warnings by phase II monitoring social networks. First, the exponential random graph model is used to model social networks. Then, a test and online monitoring technique of the exponential random graph model is developed based on the split likelihood-ratio test after determining the model and its parameters for a specific data set. This proposed approach uses pseudo-maximum likelihood estimation and likelihood ratio to construct the test statistics, avoiding the several steps of discovering Monte Carlo Markov Chain maximum likelihood estimation through an iterative method. A bisection algorithm for the control limit is given. Simulations on three data sets Flobusiness, Kapferer and Faux.mesa.high are presented to study the performance of the procedure. Different change points and shift sizes are compared to see how they affect the average run length. A real application example on the MIT reality mining social proximity network is used to illustrate the proposed modelling and online monitoring methods.},
  archive      = {J_JOAS},
  author       = {Yantao Cai and Liu Liu and Zhonghua Li},
  doi          = {10.1080/02664763.2023.2230530},
  journal      = {Journal of Applied Statistics},
  month        = {7},
  number       = {9},
  pages        = {1621-1641},
  shortjournal = {J. Appl. Stat.},
  title        = {Modelling and monitoring social network change based on exponential random graph models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding groups in data: An introduction to cluster analysis.
<em>JOAS</em>, <em>51</em>(8), 1618–1620. (<a
href="https://doi.org/10.1080/02664763.2023.2220087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Dr. Soumita Modak},
  doi          = {10.1080/02664763.2023.2220087},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1618-1620},
  shortjournal = {J. Appl. Stat.},
  title        = {Finding groups in data: An introduction to cluster analysis},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating intracluster correlation for ordinal data.
<em>JOAS</em>, <em>51</em>(8), 1609–1617. (<a
href="https://doi.org/10.1080/02664763.2023.2280821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the estimation of intracluster correlation for ordinal data. We focus on pure-tone audiometry hearing threshold data, where thresholds are measured in 5 decibel increments. We estimate the intracluster correlation for tests from iPhone-based hearing assessment applications as a measure of test/retest reliability. We present a method to estimate the intracluster correlation using mixed effects cumulative logistic and probit models, which assume the outcome data are ordinal. This contrasts with using a mixed effects linear model which assumes that the outcome data are continuous. In simulation studies, we show that using a mixed effects linear model to estimate the intracluster correlation for ordinal data results in a negative finite sample bias, while using mixed effects cumulative logistic or probit models reduces this bias. The estimated intracluster correlation for the iPhone-based hearing assessment application is higher when using the mixed effects cumulative logistic and probit models compared to using a mixed effects linear model. When data are ordinal, using mixed effects cumulative logistic or probit models reduces the bias of intracluster correlation estimates relative to using a mixed effects linear model.},
  archive      = {J_JOAS},
  author       = {Benjamin W. Langworthy and Zhaoxun Hou and Gary C. Curhan and Sharon G. Curhan and Molin Wang},
  doi          = {10.1080/02664763.2023.2280821},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1609-1617},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimating intracluster correlation for ordinal data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust explicit estimators using the power-weighted repeated
medians. <em>JOAS</em>, <em>51</em>(8), 1590–1608. (<a
href="https://doi.org/10.1080/02664763.2023.2229969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper consists of two parts. The first part of the paper is to propose an explicit robust estimation method for the regression coefficients in simple linear regression based on the power-weighted repeated medians technique that has a tuning constant for dealing with the trade-offs between efficiency and robustness. We then investigate the lower and upper bounds of the finite-sample breakdown point of the proposed method. The second part of the paper is to show that based on the linearization of the cumulative distribution function, the proposed method can be applied to obtain robust parameter estimators for the Weibull and Birnbaum-Saunders distributions that are commonly used in both reliability and survival analysis. Numerical studies demonstrate that the proposed method performs well in a manner that is approximately comparable with the ordinary least squares method, whereas it is far superior in the presence of data contamination that occurs frequently in practice.},
  archive      = {J_JOAS},
  author       = {Chanseok Park and Xuehong Gao and Min Wang},
  doi          = {10.1080/02664763.2023.2229969},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1590-1608},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust explicit estimators using the power-weighted repeated medians},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instrumental variable estimation for functional concurrent
regression models. <em>JOAS</em>, <em>51</em>(8), 1570–1589. (<a
href="https://doi.org/10.1080/02664763.2023.2229968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose a functional concurrent regression model to estimate labor supply elasticities over the years 1988 through 2014 using Current Population Survey data. Assuming, as is common, that individuals&#39; wages are endogenous, we introduce instrumental variables in a two-stage least squares approach to estimate the desired labor supply elasticities. Furthermore, we tailor our estimation method to sparse functional data. Though recent work has incorporated instrumental variables into other functional regression models, to our knowledge this has not yet been done in the functional concurrent regression model, and most existing literature is not suited for sparse functional data. We show through simulations that this two-stage least squares approach greatly eliminates the bias introduced by a naive model (i.e. one that does not acknowledge endogeneity) and produces accurate coefficient estimates for moderate sample sizes.},
  archive      = {J_JOAS},
  author       = {Justin Petrovich and Bahaeddine Taoufik and Zachary George Davis},
  doi          = {10.1080/02664763.2023.2229968},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1570-1589},
  shortjournal = {J. Appl. Stat.},
  title        = {Instrumental variable estimation for functional concurrent regression models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conformal normal curvature and detection of masked
observations in multivariate null intercept measurement error models.
<em>JOAS</em>, <em>51</em>(8), 1545–1569. (<a
href="https://doi.org/10.1080/02664763.2023.2212332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement errors occur very commonly in practice. After fitting the model, influence diagnostics is an important step in statistical data analysis. The most frequently used diagnostic method for measurement error models is the local influence. However, this methodology may fail to detect masked influential observations. To overcome this limitation, we propose the use of the conformal normal curvature with the forward search algorithm. The results are presented through easy to interpret plots considering different perturbation schemes. The proposed methodology is illustrated with three real data sets and one simulated data set, two of which have been previously analyzed in the literature. The third data set deals with the stability of the hygroscopic solid dosage in pharmaceutical processes to ensure the maintenance of product safety quality. In this application, the analytical mass balance is subject to measurement errors, which require attention in the modeling process and diagnostic analysis.},
  archive      = {J_JOAS},
  author       = {Reiko Aoki and Juan P. Mamani Bustamante and Cibele M. Russo and Gilberto A. Paula},
  doi          = {10.1080/02664763.2023.2212332},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1545-1569},
  shortjournal = {J. Appl. Stat.},
  title        = {Conformal normal curvature and detection of masked observations in multivariate null intercept measurement error models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Full bayesian analysis of double seasonal autoregressive
models with real applications. <em>JOAS</em>, <em>51</em>(8), 1524–1544.
(<a href="https://doi.org/10.1080/02664763.2023.2211754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a full Bayesian analysis of multiplicative double seasonal autoregressive (DSAR) models in a unified way, considering identification (best subset selection), estimation, and prediction problems. We assume that the DSAR model errors are normally distributed and introduce latent variables for the model lags, and then we embed the DSAR model in a hierarchical Bayes normal mixture structure. By employing the Bernoulli prior for each latent variable and the mixture normal and inverse gamma priors for the DSAR model coefficients and variance, respectively, we derive the full conditional posterior and predictive distributions in closed form. Using these derived conditional posterior and predictive distributions, we present the full Bayesian analysis of DSAR models by proposing the Gibbs sampling algorithm to approximate the posterior and predictive distributions and provide multi-step-ahead predictions. We evaluate the efficiency of the proposed full Bayesian analysis of DSAR models using an extensive simulation study, and we then apply our work to several real-world hourly electricity load time series datasets in 16 European countries.},
  archive      = {J_JOAS},
  author       = {Ayman A. Amin},
  doi          = {10.1080/02664763.2023.2211754},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1524-1544},
  shortjournal = {J. Appl. Stat.},
  title        = {Full bayesian analysis of double seasonal autoregressive models with real applications},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining phenotypic and genomic data to improve prediction
of binary traits. <em>JOAS</em>, <em>51</em>(8), 1497–1523. (<a
href="https://doi.org/10.1080/02664763.2023.2208773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant breeders want to develop cultivars that outperform existing genotypes. Some characteristics (here ‘main traits’) of these cultivars are categorical and difficult to measure directly. It is important to predict the main trait of newly developed genotypes accurately. In addition to marker data, breeding programs often have information on secondary traits (or ‘phenotypes’) that are easy to measure. Our goal is to improve prediction of main traits with interpretable relations by combining the two data types using variable selection techniques. However, the genomic characteristics can overwhelm the set of secondary traits, so a standard technique may fail to select any phenotypic variables. We develop a new statistical technique that ensures appropriate representation from both the secondary traits and the genotypic variables for optimal prediction. When two data types (markers and secondary traits) are available, we achieve improved prediction of a binary trait by two steps that are designed to ensure that a significant intrinsic effect of a phenotype is incorporated in the relation before accounting for extra effects of genotypes. First, we sparsely regress the secondary traits on the markers and replace the secondary traits by their residuals to obtain the effects of phenotypic variables as adjusted by the genotypic variables. Then, we develop a sparse logistic classifier using the markers and residuals so that the adjusted phenotypes may be selected first to avoid being overwhelmed by the genotypic variables due to their numerical advantage. This classifier uses forward selection aided by a penalty term and can be computed effectively by a technique called the one-pass method. It compares favorably with other classifiers on simulated and real data.},
  archive      = {J_JOAS},
  author       = {D. Jarquin and A. Roy and B. Clarke and S. Ghosal},
  doi          = {10.1080/02664763.2023.2208773},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1497-1523},
  shortjournal = {J. Appl. Stat.},
  title        = {Combining phenotypic and genomic data to improve prediction of binary traits},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation for two gompertz populations under a balanced
joint progressive type-II censoring scheme. <em>JOAS</em>,
<em>51</em>(8), 1470–1496. (<a
href="https://doi.org/10.1080/02664763.2023.2207787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative lifetime experiments are remarkable when the study is to ascertain the relative merits of two competing products regarding the duration of their service life. This paper considers the comparative lifetime experiments of two Gompertz populations under a balanced joint progressive Type-II censoring scheme. The lifetime distributions of the units are assumed to follow the Gompertz distribution with a common shape but different scale parameters. The maximum likelihood estimates of the unknown parameters are derived. The existence of the maximum likelihood estimates is proved. Expectation-maximization and stochastic expectation-maximization algorithms are provided to calculate the estimates. The bootstrap-p, bootstrap-t, and approximate confidence intervals are established. To obtain the Bayesian estimates, it is assumed that the prior of scale parameters is a Beta-Gamma distribution and the prior of the common shape parameter is an independent Gamma distribution. Under squared error loss and LINEX loss functions, the Metropolis-Hastings algorithm is provided to compute the Bayes estimates and the credible intervals. Further, the statistical inferences with order restriction are studied when it is known a priori that the expectation of the lifespan of one population is shorter than that of the other population. A wide range of simulation experiments is conducted to evaluate the performance of the proposed methods. Finally, the lifetimes of white organic light-emitting diodes and the breaking strengths of jute fiber of gauge lengths are analyzed to illustrate the practical application of the proposed model and methods.},
  archive      = {J_JOAS},
  author       = {Weihua Shi and Wenhao Gui},
  doi          = {10.1080/02664763.2023.2207787},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1470-1496},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation for two gompertz populations under a balanced joint progressive type-II censoring scheme},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general class of trimodal distributions: Properties and
inference. <em>JOAS</em>, <em>51</em>(8), 1446–1469. (<a
href="https://doi.org/10.1080/02664763.2023.2207785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modality is an important topic for modelling. Using parametric models is an efficient way when real data set shows trimodality. In this paper, we propose a new class of trimodal probability distributions, that is, probability distributions that have up to three modes. Trimodality itself is achieved by applying a proper transformation to density function of certain continuous probability distributions. At first, we obtain preliminary results for an arbitrary density function g ( x ) and, next, we focus on the Gaussian case, studying trimodal Gaussian model more deeply. The Gaussian distribution is applied to produce the trimodal form of Gaussian known as normal distribution. The tractability of analytical expression of normal distribution and properties of the trimodal normal distribution are important reasons why we choose normal distribution. Furthermore, the existing distributions should be improved to be capable of modelling efficiently when there exists a trimodal form in a data set. After new density function is proposed, estimating its parameters is important. Since Mathematica 12.0 software has optimization tools and important modelling techniques, computational steps are performed using this software. The bootstrapped form of real data sets are applied to show the modelling ability of the proposed distribution when real data sets show trimodality.},
  archive      = {J_JOAS},
  author       = {Roberto Vila and Victor Serra and Mehmet Niyazi Çankaya and Felipe Quintino},
  doi          = {10.1080/02664763.2023.2207785},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1446-1469},
  shortjournal = {J. Appl. Stat.},
  title        = {A general class of trimodal distributions: Properties and inference},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and efficient subsampling algorithms for massive data
logistic regression. <em>JOAS</em>, <em>51</em>(8), 1427–1445. (<a
href="https://doi.org/10.1080/02664763.2023.2205611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets that are big with regard to their volume, variety and velocity are becoming increasingly common. However, limitations in computer processing can often restrict analysis performed on them. Nonuniform subsampling methods are effective in reducing computational loads for massive data. However, the variance of the estimator of nonuniform subsampling methods becomes large when the subsampling probabilities are highly heterogenous. To this end, we develop two new algorithms to improve the estimation method for massive data logistic regression based on a chosen hard threshold value and combining subsamples, respectively. The basic idea of the hard threshold method is to carefully select a threshold value and then replace subsampling probabilities lower than the threshold value with the chosen value itself. The main idea behind the combining subsamples method is to better exploit information in the data without hitting the computation bottleneck by generating many subsamples and then combining estimates constructed from the subsamples. The combining subsamples method obtains the standard error of the parameter estimator without estimating the sandwich matrix, which provides convenience for statistical inference in massive data, and can significantly improve the estimation efficiency. Asymptotic properties of the resultant estimators are established. Simulations and analysis of real data are conducted to assess and showcase the practical performance of the proposed methods.},
  archive      = {J_JOAS},
  author       = {Jun Jin and Shuangzhe Liu and Tiefeng Ma},
  doi          = {10.1080/02664763.2023.2205611},
  journal      = {Journal of Applied Statistics},
  month        = {6},
  number       = {8},
  pages        = {1427-1445},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust and efficient subsampling algorithms for massive data logistic regression},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting of the true satellite carbon monoxide data with
ensemble empirical mode decomposition, singular value decomposition and
moving average. <em>JOAS</em>, <em>51</em>(7), 1412–1426. (<a
href="https://doi.org/10.1080/02664763.2023.2277115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The forecasting of carbon monoxide in the atmosphere is essential as it causes the pollution of the atmosphere and hence severe health problems for humans. This study proposes a time-series prognosis EEMD-SVD-MA technique which incorporates Ensemble Empirical Mode Decomposition, Singular Value Decomposition and Moving Average, to predict the prospects of carbon monoxide data taken from the Indian region. The collected data are non-linear. The technique can be applied for non-stationary and non-linear data. In this approach, there are three levels: EEMD level, SVD level and MA level. The first level deploys EEMD to fragment data series into a limited number of Intrinsic Mode Function (IMF) components along with a residue. To denoise each IMF component, SVD is deployed in the second level. In the third level, each denoised IMF component is predicted by MA. The future values of the original data are obtained by adding all the predicted series of the components. In this study, we proposed two variants of the model: EEMD-SVD-MA(3) and EEMD-SVD-MA(4) and compared the results with other forecasting techniques, namely LSTM (Long Short Term Memory network), EMD-LSTM, EMD-MA, EEMD-MA and CEEMDAN-MA. The results show that the proposed EEMD-SVD-MA model is more efficient than other models.},
  archive      = {J_JOAS},
  author       = {Sameer Poongadan and M. C. Lineesh},
  doi          = {10.1080/02664763.2023.2277115},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1412-1426},
  shortjournal = {J. Appl. Stat.},
  title        = {Forecasting of the true satellite carbon monoxide data with ensemble empirical mode decomposition, singular value decomposition and moving average},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the hosmer-lemeshow goodness-of-fit test in large
models with replicated bernoulli trials. <em>JOAS</em>, <em>51</em>(7),
1399–1411. (<a
href="https://doi.org/10.1080/02664763.2023.2272223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hosmer-Lemeshow (HL) test is a commonly used global goodness-of-fit (GOF) test that assesses the quality of the overall fit of a logistic regression model. In this paper, we give results from simulations showing that the type I error rate (and hence power) of the HL test decreases as model complexity grows, provided that the sample size remains fixed and binary replicates (multiple Bernoulli trials) are present in the data. We demonstrate that a generalized version of the HL test (GHL) presented in previous work can offer some protection against this power loss. These results are also supported by application of both the HL and GHL test to a real-life data set. We conclude with a brief discussion explaining the behavior of the HL test, along with some guidance on how to choose between the two tests. In particular, we suggest the GHL test to be used when there are binary replicates or clusters in the covariate space, provided that the sample size is sufficiently large.},
  archive      = {J_JOAS},
  author       = {Nikola Surjanovic and Thomas M. Loughin},
  doi          = {10.1080/02664763.2023.2272223},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1399-1411},
  shortjournal = {J. Appl. Stat.},
  title        = {Improving the hosmer-lemeshow goodness-of-fit test in large models with replicated bernoulli trials},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new family of quantile regression models applied to
nutritional data. <em>JOAS</em>, <em>51</em>(7), 1378–1398. (<a
href="https://doi.org/10.1080/02664763.2023.2203882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new family of quantile regression models whose response variable follows a reparameterized Marshall-Olkin distribution indexed by quantile, scale, and asymmetry parameters. The family has arisen by applying the Marshall-Olkin approach to distributions belonging to the location-scale family. Models of higher flexibility and whose structure is similar to generalized linear models were generated by quantile reparameterization. The maximum likelihood (ML) method is presented for the estimation of the model parameters, and simulation studies evaluated the performance of the ML estimators. The advantages of the family are illustrated through an application to a set of nutritional data, whose results indicate it is a good alternative for modeling slightly asymmetric response variables with support on the real line.},
  archive      = {J_JOAS},
  author       = {Isaac E. Cortés and Mário de Castro and Diego I. Gallardo},
  doi          = {10.1080/02664763.2023.2203882},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1378-1398},
  shortjournal = {J. Appl. Stat.},
  title        = {A new family of quantile regression models applied to nutritional data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust scalar-on-function partial quantile regression.
<em>JOAS</em>, <em>51</em>(7), 1359–1377. (<a
href="https://doi.org/10.1080/02664763.2023.2202464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with the conditional mean regression-based scalar-on-function regression model, the scalar-on-function quantile regression is robust to outliers in the response variable. However, it is susceptible to outliers in the functional predictor (called leverage points). This is because the influence function of the regression quantiles is bounded in the response variable but unbounded in the predictor space. The leverage points may alter the eigenstructure of the predictor matrix, leading to poor estimation and prediction results. This study proposes a robust procedure to estimate the model parameters in the scalar-on-function quantile regression method and produce reliable predictions in the presence of both outliers and leverage points. The proposed method is based on a functional partial quantile regression procedure. We propose a weighted partial quantile covariance to obtain functional partial quantile components of the scalar-on-function quantile regression model. After the decomposition, the model parameters are estimated via a weighted loss function, where the robustness is obtained by iteratively reweighting the partial quantile components. The estimation and prediction performance of the proposed method is evaluated by a series of Monte-Carlo experiments and an empirical data example. The results are compared favorably with several existing methods. The method is implemented in an R package robfpqr .},
  archive      = {J_JOAS},
  author       = {Ufuk Beyaztas and Mujgan Tez and Han Lin Shang},
  doi          = {10.1080/02664763.2023.2202464},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1359-1377},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust scalar-on-function partial quantile regression},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of survival outcomes using likelihood ratio test in
trials incorporating patient’s treatment choice. <em>JOAS</em>,
<em>51</em>(7), 1344–1358. (<a
href="https://doi.org/10.1080/02664763.2023.2199177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for designing and analyzing multiple arms survival trials that incorporate patient&#39;s treatment choice are needed. In these trials, patients are randomized into two groups, random and choice. Participants in the choice group choose their treatment, which is not a current standard practice in randomized clinical trials. In this paper, we propose a new method based on the likelihood function to design and analyze these trials with time to event outcomes in the presence of non-informative right censoring. We use simulations to evaluate the methods for Weibull outcomes, complete and censored. Finally, we provide an illustration for designing a study in which we discuss some design considerations and demonstrate the methods.},
  archive      = {J_JOAS},
  author       = {Rouba A. Chahine and Inmaculada Aban},
  doi          = {10.1080/02664763.2023.2199177},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1344-1358},
  shortjournal = {J. Appl. Stat.},
  title        = {Analysis of survival outcomes using likelihood ratio test in trials incorporating patient&#39;s treatment choice},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust autoregressive modeling and its diagnostic analytics
with a COVID-19 related application. <em>JOAS</em>, <em>51</em>(7),
1318–1343. (<a
href="https://doi.org/10.1080/02664763.2023.2198178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive models in time series are useful in various areas. In this article, we propose a skew-t autoregressive model. We estimate its parameters using the expectation-maximization (EM) method and develop the influence methodology based on local perturbations for its validation. We obtain the normal curvatures for four perturbation strategies to identify influential observations, and then to assess their performance through Monte Carlo simulations. An example of financial data analysis is presented to study daily log-returns for Brent crude futures and investigate possible impact by the COVID-19 pandemic.},
  archive      = {J_JOAS},
  author       = {Yonghui Liu and Jing Wang and Víctor Leiva and Alejandra Tapia and Wei Tan and Shuangzhe Liu},
  doi          = {10.1080/02664763.2023.2198178},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1318-1343},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust autoregressive modeling and its diagnostic analytics with a COVID-19 related application},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional principal component models for sparse and
irregularly spaced data by bayesian inference. <em>JOAS</em>,
<em>51</em>(7), 1287–1317. (<a
href="https://doi.org/10.1080/02664763.2023.2197587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area of functional principal component analysis (FPCA) has seen relatively few contributions from the Bayesian inference. A Bayesian method in FPCA is developed under the cases of continuous and binary observations for sparse and irregularly spaced data. In the proposed Markov chain Monte Carlo (MCMC) method, Gibbs sampler approach is adopted to update the different variables based on their conditional posterior distributions. In FPCA, a set of eigenfunctions is suggested under Stiefel manifold, and samples are drawn from a Langevin–Bingham matrix variate distribution. Penalized splines are used to model mean trajectory and eigenfunction trajectories in generalized functional mixed models; and the proposed model is casted into a mixed-effects model framework for Bayesian inference. To determine the number of principal components, reversible jump Markov chain Monte Carlo (RJ-MCMC) algorithm is implemented. Four different simulation settings are conducted to demonstrate competitive performance against non-Bayesian approaches in FPCA. Finally, the proposed method is illustrated to the analysis of body mass index (BMI) data by gender and ethnicity.},
  archive      = {J_JOAS},
  author       = {Jun Ye},
  doi          = {10.1080/02664763.2023.2197587},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1287-1317},
  shortjournal = {J. Appl. Stat.},
  title        = {Functional principal component models for sparse and irregularly spaced data by bayesian inference},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian sample size determination for coefficient of
variation of normal distribution. <em>JOAS</em>, <em>51</em>(7),
1271–1286. (<a
href="https://doi.org/10.1080/02664763.2023.2197571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size determination is an active area of research in statistics. Generally, Bayesian methods provide relatively smaller sample sizes than the classical techniques, particularly average length criterion is more conventional and gives relatively small sample sizes under the given constraints. The objective of this study is to utilize major Bayesian sample size determination techniques for the coefficient of variation of normal distribution and assess their performance by comparing the results with the freqentist approach. To this end, we noticed that the average coverage criterion is the one that provides relatively smaller sample sizes than the worst outcome criterion. By comparing with the existing frequentist studies, we show that a smaller sample size is required in Bayesian methods to achieve the same efficiency.},
  archive      = {J_JOAS},
  author       = {Sajid Ali and Mariyam Waheed and Ismail Shah and Syed Muhammad Muslim Raza},
  doi          = {10.1080/02664763.2023.2197571},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1271-1286},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian sample size determination for coefficient of variation of normal distribution},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of accelerated hazards models based on case k
informatively interval-censored failure time data. <em>JOAS</em>,
<em>51</em>(7), 1251–1270. (<a
href="https://doi.org/10.1080/02664763.2023.2196752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated hazards model is one of the most commonly used models for regression analysis of failure time data and this is especially the case when, for example, the hazard functions may have monotonicity property. Correspondingly a large literature has been established for its estimation or inference when right-censored data are observed. Although several methods have also been developed for its inference based on interval-censored data, they apply only to limited situations or rely on some assumptions such as independent censoring. In this paper, we consider the situation where one observes case K interval-censored data, the type of failure time data that occur most in, for example, medical research such as clinical trials or periodical follow-up studies. For inference, we propose a sieve borrow-strength method and in particular, it allows for informative censoring. The asymptotic properties of the proposed estimators are established. Simulation studies demonstrate that the proposed inference procedure performs well. The method is applied to a set of real data set arising from an AIDS clinical trial.},
  archive      = {J_JOAS},
  author       = {Rui Ma and Shishun Zhao and Jianguo Sun and Shuying Wang},
  doi          = {10.1080/02664763.2023.2196752},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1251-1270},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation of accelerated hazards models based on case k informatively interval-censored failure time data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The balanced discrete burr–hatke model and mixing INAR(1)
process: Properties, estimation, forecasting and COVID-19 applications.
<em>JOAS</em>, <em>51</em>(7), 1227–1250. (<a
href="https://doi.org/10.1080/02664763.2023.2194582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main concern of this paper is providing a flexible discrete model that captures every kind of dispersion (equi-, over- and under-dispersion). Based on the balanced discretization method, a new discrete version of Burr–Hatke distribution is introduced with the partial moment-preserving property. Some statistical properties of the new distribution are introduced, and the applicability of proposed model is evaluated by considering counting series. A new integer-valued autoregressive (INAR) process based on the mixing Pegram and binomial thinning operators with discrete Burr–Hatke innovations is introduced, which can model contagious data properly. The different estimation approaches of parameters of the new process are provided and compared through the Monte Carlo simulation scheme. The performance of the proposed process is evaluated by four data sets of the daily death counts of the COVID-19 in Austria, Switzerland, Nigeria and Slovenia in comparison with some competitor INAR(1) models, along with the Pearson residual analysis of the assessing model. The goodness of fit measures affirm the adequacy of the proposed process in modeling all COVID-19 data sets. The fundamental prediction procedures are considered for new process by classic, modified Sieve bootstrap and Bayesian forecasting methods for all COVID-19 data sets, which is concluded that the Bayesian forecasting approach provides more reliable results.},
  archive      = {J_JOAS},
  author       = {Seyedeh Mahbubeh Hoseini Baladezaei and Einolah Deiri and Ezzatallah Baloui Jamkhaneh},
  doi          = {10.1080/02664763.2023.2194582},
  journal      = {Journal of Applied Statistics},
  month        = {5},
  number       = {7},
  pages        = {1227-1250},
  shortjournal = {J. Appl. Stat.},
  title        = {The balanced discrete Burr–Hatke model and mixing INAR(1) process: Properties, estimation, forecasting and COVID-19 applications},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classifying contaminated cell cultures using time series
features. <em>JOAS</em>, <em>51</em>(6), 1210–1226. (<a
href="https://doi.org/10.1080/02664763.2023.2248413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the use of time series data, derived from Electric Cell-substrate Impedance Sensing (ECIS), to differentiate between standard mammalian cell cultures and those infected with a mycoplasma organism. With the goal of easy visualization and interpretation, we perform low-dimensional feature-based classification, extracting application-relevant features from the ECIS time courses. We can achieve very high classification accuracy using only two features, which depend on the cell line under examination. Initial results also show the existence of experimental variation between plates and suggest types of features that may prove more robust to such variation. Our paper is the first to perform a broad examination of ECIS time course features in the context of detecting contamination; to combine different types of features to achieve classification accuracy while preserving interpretability; and to describe and suggest possibilities for ameliorating plate-to-plate variation.},
  archive      = {J_JOAS},
  author       = {Laura L. Tupper and Charles R. Keese and David S. Matteson},
  doi          = {10.1080/02664763.2023.2248413},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1210-1226},
  shortjournal = {J. Appl. Stat.},
  title        = {Classifying contaminated cell cultures using time series features},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric framework for statistical analysis of eye tracking
heat maps, with application to a tobacco waterpipe study. <em>JOAS</em>,
<em>51</em>(6), 1191–1209. (<a
href="https://doi.org/10.1080/02664763.2023.2233143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health warning labels have been found to increase awareness of the harmful effects of tobacco products. An eye tracking study was conducted to determine the optimal placement and type of a health warning label on tobacco waterpipes. Participants viewed images that contained one of (1) four waterpipes, (2) three different types of warning labels, (3) placed in three locations. Typically, statistical analysis of eye tracking data is conducted based on summary statistics such as total dwell time, duration score, and number of visits to an area of interest. However, these summary statistics fail to capture the complete variability in a participant&#39;s eye movement. Instead, we propose to estimate heat maps defined on the entire image domain using the raw two-dimensional coordinates of eye movement via kernel density estimation. For statistical analysis of heat maps, we adopt the Fisher–Rao Riemannian geometric framework, which enables computationally efficient comparisons of heat maps, statistical summarization and exploration of variability in a sample of heat maps, and metric-based hierarchical clustering. We apply this framework to eye tracking data from the tobacco waterpipe study and comment on the results in the context of the optimal placement and type of health warning labels on tobacco waterpipes.},
  archive      = {J_JOAS},
  author       = {David Angeles and Sebastian Kurtek and Elizabeth Klein and Marielle Brinkman and Amy Ferketich},
  doi          = {10.1080/02664763.2023.2233143},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1191-1209},
  shortjournal = {J. Appl. Stat.},
  title        = {Geometric framework for statistical analysis of eye tracking heat maps, with application to a tobacco waterpipe study},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distribution-free phase II triple EWMA control chart for
joint monitoring the process location and scale parameters.
<em>JOAS</em>, <em>51</em>(6), 1171–1190. (<a
href="https://doi.org/10.1080/02664763.2023.2189771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution-free or nonparametric control charts are used for monitoring the process parameters when there is a lack of knowledge about the underlying distribution. In this paper, we investigate a single distribution-free triple exponentially weighted moving average control chart based on the Lepage statistic (referred as TL chart) for simultaneously monitoring shifts in the unknown location and scale parameters of a univariate continuous distribution. The design and implementation of the proposed chart are discussed using time-varying and steady-state control limits for the zero-state case. The run-length distribution of the TL chart is evaluated by performing Monte Carlo simulations. The performance of the proposed chart is compared to those of the existing EWMA-Lepage (EL) and DEWMA-Lepage (DL) charts. It is observed that the TL chart with a time-varying control limit is superior to its competitors, especially for small to moderate shifts in the process parameters. We also provide a real example from a manufacturing process to illustrate the application of the proposed chart.},
  archive      = {J_JOAS},
  author       = {Vasileios Alevizakos and Kashinath Chatterjee and Christos Koukouvinos},
  doi          = {10.1080/02664763.2023.2189771},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1171-1190},
  shortjournal = {J. Appl. Stat.},
  title        = {Distribution-free phase II triple EWMA control chart for joint monitoring the process location and scale parameters},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalized robust learning for optimal treatment regimes with
heterogeneous individualized treatment effects. <em>JOAS</em>,
<em>51</em>(6), 1151–1170. (<a
href="https://doi.org/10.1080/02664763.2023.2180167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of personalized medicine motivates people to explore individualized treatment regimes according to heterogeneous characteristics of the patients. For the large-scale data analysis, however, the data are collected at different times and different locations, i.e. subjects are usually from a heterogeneous population, which causes that the optimal treatment regimes also vary for patients across different subgroups. In this paper, we mainly focus on the estimation of optimal treatment regimes for subjects come from a heterogeneous population with high-dimensional data. We first remove the main effects of the covariates for each subgroup to eliminate non-ignorable residual confounding. Based on the centralized outcome, we propose a penalized robust learning that estimates the coefficient matrix of the interactions between covariates and treatment by penalizing pairwise differences of the coefficients of any two subgroups for the same covariate, which can automatically identify the latent complex structure of the coefficient matrix with heterogeneous and homogeneous columns. At the same time, the penalized robust learning can also select the important variables that truly contribute to the individualized treatment decisions with commonly used sparsity structure penalty. Extensive simulation studies show that our proposed method outperforms current popular methods, and it is further illustrated in the real analysis of the Tamoxifen breast cancer data.},
  archive      = {J_JOAS},
  author       = {Canhui Li and Weirong Li and Wensheng Zhu},
  doi          = {10.1080/02664763.2023.2180167},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1151-1170},
  shortjournal = {J. Appl. Stat.},
  title        = {Penalized robust learning for optimal treatment regimes with heterogeneous individualized treatment effects},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Epidemic change-point detection in general integer-valued
time series. <em>JOAS</em>, <em>51</em>(6), 1131–1150. (<a
href="https://doi.org/10.1080/02664763.2023.2179567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the structural change in a class of discrete valued time series, where the true conditional distribution of the observations is assumed to be unknown. The conditional mean of the process depends on a parameter θ ∗ θ ∗ θ∗ which may change over time. We provide sufficient conditions for the consistency and the asymptotic normality of the Poisson quasi-maximum likelihood estimator (QMLE) of the model. We consider an epidemic change-point detection and propose a test statistic based on the QMLE of the parameter. Under the null hypothesis of a constant parameter (no change), the test statistic converges to a distribution obtained from increments of a Browninan bridge. The test statistic diverges to infinity under the epidemic alternative, which establishes that the proposed procedure is consistent in power. The effectiveness of the proposed procedure is illustrated by simulated and real data examples.},
  archive      = {J_JOAS},
  author       = {Mamadou Lamine Diop and William Kengne},
  doi          = {10.1080/02664763.2023.2179567},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1131-1150},
  shortjournal = {J. Appl. Stat.},
  title        = {Epidemic change-point detection in general integer-valued time series},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection for quantile autoregressive model:
Bayesian methods versus classical methods. <em>JOAS</em>,
<em>51</em>(6), 1098–1130. (<a
href="https://doi.org/10.1080/02664763.2023.2178642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce three Bayesian variable selection methods for the quantile autoregressive model with explanatory variables. The Gibbs sampling algorithms are developed for each method by setting different priors. The numerical simulations suggest that the Gibbs sampling algorithms converge fast and Bayesian variable selection methods are reliable. A real example is given to analysis the relationship between the count of total rental bikes and five explanatory variables. Both simulations and data example indicate that the proposed methods are feasible, reliable, and appropriate for analyzing the Bike Sharing data set.},
  archive      = {J_JOAS},
  author       = {Bo Peng and Kai Yang and Xiaogang Dong},
  doi          = {10.1080/02664763.2023.2178642},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1098-1130},
  shortjournal = {J. Appl. Stat.},
  title        = {Variable selection for quantile autoregressive model: Bayesian methods versus classical methods},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid statistical and machine learning modeling of
cognitive neuroscience data. <em>JOAS</em>, <em>51</em>(6), 1076–1097.
(<a href="https://doi.org/10.1080/02664763.2023.2176834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nested data structure is prevalent for cognitive measure experiments due to repeatedly taken observations from different brain locations within subjects. The analysis methods used for this data type should consider the dependency structure among the repeated measurements. However, the dependency assumption is mainly ignored in the cognitive neuroscience data analysis literature. We consider both statistical, and machine learning methods extended to repeated data analysis and compare distinct algorithms in terms of their advantage and disadvantages. Unlike basic algorithm comparison studies, this article analyzes novel neuroscience data considering the dependency structure for the first time with several statistical and machine learning methods and their hybrid forms. In addition, the fitting performances of different algorithms are compared using contaminated data sets, and the cross-validation approach. One of our findings suggests that the GLMM tree, including random term indices indicating the location of functional near-infrared spectroscopy optodes nested within experimental units, shows the best predictive performance with the lowest MSE, RMSE, and MAE model performance metrics. However, there is a trade-off between accuracy and speed since this algorithm is required the highest computational time.},
  archive      = {J_JOAS},
  author       = {Serenay Cakar and Fulya Gokalp Yavuz},
  doi          = {10.1080/02664763.2023.2176834},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1076-1097},
  shortjournal = {J. Appl. Stat.},
  title        = {Hybrid statistical and machine learning modeling of cognitive neuroscience data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imputed mean tensor regression for near-sited spatial
temporal data. <em>JOAS</em>, <em>51</em>(6), 1057–1075. (<a
href="https://doi.org/10.1080/02664763.2023.2176470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern spatial temporal data are often collected from sensor networks. Missing data problems are common to this kind of data. Making accurate imputation is important for many applications. In the unsupervised setting, one technique is to minimize the rank of a tensor or matrix. If we add related covariates, can we get more accurate imputation results? To address this, we transform the original sensor×time measurements to high order tensors by adding additional temporal dimensions and then integrate tensor regression with tensor completion using nuclear norm penalty. One advantage is we can simultaneously estimate parameters and impute missing values due to clear spatial consistency for near-sited spatial-temporal data. The proposed method doesn&#39;t assume missing mechanism of the response. Theoretical properties of the proposed estimator are investigated. Simulation studies and real data analysis are conducted to verify the efficiency of the estimation procedure.},
  archive      = {J_JOAS},
  author       = {Jinwen Liang and Maozai Tian},
  doi          = {10.1080/02664763.2023.2176470},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1057-1075},
  shortjournal = {J. Appl. Stat.},
  title        = {Imputed mean tensor regression for near-sited spatial temporal data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoothing regression and impact measures for accidents of
traffic flows. <em>JOAS</em>, <em>51</em>(6), 1041–1056. (<a
href="https://doi.org/10.1080/02664763.2023.2175799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic pattern identification and accident evaluation are essential for improving traffic planning, road safety, and traffic management. In this paper, we establish classification and regression models to characterize the relationship between traffic flows and different time points and identify different patterns of traffic flows by a negative binomial model with smoothing splines. It provides mean response curves and Bayesian credible bands for traffic flows, a single index, and the log-likelihood difference, for traffic flow pattern recognition. We further propose an impact measure for evaluating the influence of accidents on traffic flows based on the fitted negative binomial model. The proposed method has been successfully applied to real-world traffic flows, and it can be used for improving traffic management.},
  archive      = {J_JOAS},
  author       = {Zhou Yu and Jie Yang and Hsin-Hsiung Huang},
  doi          = {10.1080/02664763.2023.2175799},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1041-1056},
  shortjournal = {J. Appl. Stat.},
  title        = {Smoothing regression and impact measures for accidents of traffic flows},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A monotone single index model for missing-at-random
longitudinal proportion data. <em>JOAS</em>, <em>51</em>(6), 1023–1040.
(<a href="https://doi.org/10.1080/02664763.2023.2173156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beta distributions are commonly used to model proportion valued response variables, often encountered in longitudinal studies. In this article, we develop semi-parametric Beta regression models for proportion valued responses, where the aggregate covariate effect is summarized and flexibly modeled, using a interpretable monotone time-varying single index transform of a linear combination of the potential covariates. We utilize the potential of single index models, which are effective dimension reduction tools and accommodate link function misspecification in generalized linear mixed models. Our Bayesian methodology incorporates the missing-at-random feature of the proportion response and utilize Hamiltonian Monte Carlo sampling to conduct inference. We explore finite-sample frequentist properties of our estimates and assess the robustness via detailed simulation studies. Finally, we illustrate our methodology via application to a motivating longitudinal dataset on obesity research recording proportion body fat.},
  archive      = {J_JOAS},
  author       = {Satwik Acharyya and Debdeep Pati and Shumei Sun and Dipankar Bandyopadhyay},
  doi          = {10.1080/02664763.2023.2173156},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {6},
  pages        = {1023-1040},
  shortjournal = {J. Appl. Stat.},
  title        = {A monotone single index model for missing-at-random longitudinal proportion data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alternative statistical modeling for radical prostatectomy
data. <em>JOAS</em>, <em>51</em>(5), 1007–1022. (<a
href="https://doi.org/10.1080/02664763.2023.2229973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several statistical models have been proposed in recent years, among them is the semiparametric regression. In medicine, there are several situations in which it is impracticable to consider a linear regression for statistical modeling, especially when the data contain explanatory variables that present a nonlinear relationship with the response variable. Another common situation is when the response variable does not have a unimodal shape, and it is not possible to adopt distributions belonging to the symmetric or asymmetric classes. In this context, a semiparametric heteroskedastic regression is proposed based on an extension of the normal distribution. Then, we show the usefulness of this model to analyze the cost of prostate cancer surgery. The predictor variables refer to two groups of patients such that one group receives a multimodal local anesthetic solution (Preemptive Target Anesthetic Solution) and the second group is treated with neuraxial blockade (spinal anesthesia/traditional standard). The other relevant predictor variables are also evaluated, thus allowing for the in-depth interpretation of the predictor variables with a nonlinear effect on the dependent variable cost . The penalized maximum likelihood method is adopted to estimate the model parameters. The new regression is a useful statistical tool for analyzing medical data.},
  archive      = {J_JOAS},
  author       = {Julio C. S. Vasconcelos and Thiago da Costa Travassos and Edwin M. M. Ortega and Gauss M. Cordeiro and Leonardo Oliveira Reis},
  doi          = {10.1080/02664763.2023.2229973},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {1007-1022},
  shortjournal = {J. Appl. Stat.},
  title        = {Alternative statistical modeling for radical prostatectomy data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kppmenet: Combining the kppm and elastic net regularization
for inhomogeneous cox point process with correlated covariates.
<em>JOAS</em>, <em>51</em>(5), 993–1006. (<a
href="https://doi.org/10.1080/02664763.2023.2207786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 𝚔𝚙𝚙𝚖 kppm kppm is a standard procedure to estimate the parameters of the inhomogeneous Cox point process. However, the procedure cannot handle the problem when the models involve correlated covariates. In this study, we develop the 𝚔𝚙𝚙𝚖𝚎𝚗𝚎𝚝 kppmenet kppmenet , the modified version of the kppm , for the inhomogeneous Cox point process involving correlated covariates by considering elastic net regularization. We compare the methodology in a simulation study and apply it to model major-shallow earthquake distribution in Sumatra, Indonesia. We conclude that the kppmenet outperforms kppm when correlated covariates are involved.},
  archive      = {J_JOAS},
  author       = {Achmad Choiruddin and Tabita Yuni Susanto and Ahmad Husain and Yuniar Mega Kartikasari},
  doi          = {10.1080/02664763.2023.2207786},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {993-1006},
  shortjournal = {J. Appl. Stat.},
  title        = {Kppmenet: Combining the kppm and elastic net regularization for inhomogeneous cox point process with correlated covariates},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian adaptive selection of basis functions for
functional data representation. <em>JOAS</em>, <em>51</em>(5), 958–992.
(<a href="https://doi.org/10.1080/02664763.2023.2172143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the context of functional data analysis, we developed and applied a new Bayesian approach via the Gibbs sampler to select basis functions for a finite representation of functional data. The proposed methodology uses Bernoulli latent variables to assign zero to some of the basis function coefficients with a positive probability. This procedure allows for an adaptive basis selection since it can determine the number of bases and which ones should be selected to represent functional data. Moreover, the proposed procedure measures the uncertainty of the selection process and can be applied to multiple curves simultaneously. The methodology developed can deal with observed curves that may differ due to experimental error and random individual differences between subjects, which one can observe in a real dataset application involving daily numbers of COVID-19 cases in Brazil. Simulation studies show the main properties of the proposed method, such as its accuracy in estimating the coefficients and the strength of the procedure to find the true set of basis functions. Despite having been developed in the context of functional data analysis, we also compared the proposed model via simulation with the well-established LASSO and Bayesian LASSO, which are methods developed for non-functional data.},
  archive      = {J_JOAS},
  author       = {Pedro Henrique T. O. Sousa and Camila P. E. de Souza and Ronaldo Dias},
  doi          = {10.1080/02664763.2023.2172143},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {958-992},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian adaptive selection of basis functions for functional data representation},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aggregated parameter update schemes for monitoring binary
profiles. <em>JOAS</em>, <em>51</em>(5), 935–957. (<a
href="https://doi.org/10.1080/02664763.2023.2170991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Profile monitoring is one of the most important topics for statistical process control. Traditional self-starting profile monitoring schemes generally use all historical observations to estimate parameters. Because of the rapid increase in the complexity of modern statistical processes, the practitioners often need to deal with massive datasets in process monitoring. However, when observations of each period are of large sample size and the computation is of high complexity, the traditional method is not economical and urgently needs a parameter update strategy. Under the framework of binary profile monitoring, this paper proposes a novel recursive update strategy based on the aggregated estimation equation (AEE) for massive datasets and designs a self-starting control chart accordingly. Numerical simulation verifies that the proposed method performs better in parameter estimation and process monitoring. In addition, we give the asymptotic property of the proposed monitoring statistic and illustrate our method&#39;s superiority by a real-data example.},
  archive      = {J_JOAS},
  author       = {Yifan Li and Chunjie Wu and Zhijun Wang and Zhiming Hu},
  doi          = {10.1080/02664763.2023.2170991},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {935-957},
  shortjournal = {J. Appl. Stat.},
  title        = {Aggregated parameter update schemes for monitoring binary profiles},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating zero-state and steady-state performance of
MEWMA-CoDa control chart using variable sampling interval.
<em>JOAS</em>, <em>51</em>(5), 913–934. (<a
href="https://doi.org/10.1080/02664763.2023.2170336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional process monitoring control charts (CCs) focused on sampling methods using fixed sampling intervals ( FSI FSI FSI s). The variable sampling intervals ( VSI VSI VSI s) scheme is receiving increasing attention, in which the sampling interval ( SI SI SI ) length varies according to the process monitoring statistics. A shorter SI is considered when the process quality indicates the possibility of an out-of-control (OOC) situation; otherwise, a longer SI is preferred. The VSI multivariate exponentially moving average for compositional data ( VSI - MEWMA CoDa ) CC based on a coordinate representation using isometric log-ratio ( ilr ) transformation is proposed in this study. A methodology is proposed to obtain the optimal parameters by considering the zero-state ( ZS ) average time to signal ( ZATS ) and the steady-state ( SS ) average time to signal ( SATS ). The statistical performance of the proposed CC is evaluated based on a continuous-time Markov chain ( CTMC ) method for both cases, the ZS and the SS using a fixed value of in-control (IC) ATS 0 . Simulation results demonstrate that the VSI - MEWMA CoDa CC has significantly decreased the OOC average time to signal ( ATS ) than the FSI MEWMA CoDa CC. Moreover, it is found that the number of variables ( d ) has a negative impact on the ATS of the VSI - MEWMA CoDa CC, and the subgroup size ( n ) has a mildly positive impact on the ATS of the VSI - MEWMA CoDa CC. At the same time, the SATS of the VSI - MEWMA CoDa CC is less than the ZATS of the VSI - MEWMA CoDa CC for all the values of n and d . The proposed VSI - MEWMA CoDa CC under steady-State performs effectively compared to its competitors, such as the FSI - MEWMA CoDa CC, the VSI - T 2 CoDa CC and the FSI - T 2 CoDa CC. An example of an industrial problem from a plant in Europe is also given to study the statistical significance of the VSI - MEWMA CoDa CC.},
  archive      = {J_JOAS},
  author       = {Muhammad Imran and Jinsheng Sun and Xuelong Hu and Fatima Sehar Zaidi and Anan Tang},
  doi          = {10.1080/02664763.2023.2170336},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {913-934},
  shortjournal = {J. Appl. Stat.},
  title        = {Investigating zero-state and steady-state performance of MEWMA-CoDa control chart using variable sampling interval},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection of the optimal personalized treatment from
multiple treatments with right-censored multivariate outcome measures.
<em>JOAS</em>, <em>51</em>(5), 891–912. (<a
href="https://doi.org/10.1080/02664763.2022.2164759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel personalized concept for the optimal treatment selection for a situation where the response is a multivariate vector that could contain right-censored variables such as survival time. The proposed method can be applied with any number of treatments and outcome variables, under a broad set of models. Following a working semiparametric Single Index Model that relates covariates and responses, we first define a patient-specific composite score, constructed from individual covariates. We then estimate conditional means of each response, given the patient score, correspond to each treatment, using a nonparametric smooth estimator. Next, a rank aggregation technique is applied to estimate an ordering of treatments based on ranked lists of treatment performance measures given by conditional means. We handle the right-censored data by incorporating the inverse probability of censoring weighting to the corresponding estimators. An empirical study illustrates the performance of the proposed method in finite sample problems. To show the applicability of the proposed procedure for real data, we also present a data analysis using HIV clinical trial data, that contained a right-censored survival event as one of the endpoints.},
  archive      = {J_JOAS},
  author       = {Chathura Siriwardhana and K.B. Kulasekera and Somnath Datta},
  doi          = {10.1080/02664763.2022.2164759},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {891-912},
  shortjournal = {J. Appl. Stat.},
  title        = {Selection of the optimal personalized treatment from multiple treatments with right-censored multivariate outcome measures},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the spatial patterns of antenatal care utilization
in nigeria with inference based on pólya-gamma mixtures. <em>JOAS</em>,
<em>51</em>(5), 866–890. (<a
href="https://doi.org/10.1080/02664763.2022.2164561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the vast advantages of making antenatal care visits, the service utilization among pregnant women in Nigeria is suboptimal. A five-year monitoring estimate indicated that about 24% of the women who had live births made no visit. The non-utilization induced excessive zeroes in the outcome of interest. Thus, this study adopted a zero-inflated negative binomial model within a Bayesian framework to identify the spatial pattern and the key factors hindering antenatal care utilization in Nigeria. We overcome the intractability associated with posterior inference by adopting a Pólya-Gamma data-augmentation technique to facilitate inference. The Gibbs sampling algorithm was used to draw samples from the joint posterior distribution. Results revealed that type of place of residence, maternal level of education, access to mass media, household work index, and woman&#39;s working status have significant effects on the use of antenatal care services. Findings identified substantial state-level spatial disparity in antenatal care utilization across the country. Cost-effective techniques to achieve an acceptable frequency of utilization include the creation of a community-specific awareness to emphasize the importance and benefits of the appropriate utilization. Special consideration should be given to older pregnant women, women in poor antenatal utilization states, and women residing in poor road network regions.},
  archive      = {J_JOAS},
  author       = {Osafu Augustine Egbon and Ezra Gayawan},
  doi          = {10.1080/02664763.2022.2164561},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {866-890},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling the spatial patterns of antenatal care utilization in nigeria with inference based on pólya-gamma mixtures},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning the structure of the mTOR protein signaling pathway
from protein phosphorylation data. <em>JOAS</em>, <em>51</em>(5),
845–865. (<a
href="https://doi.org/10.1080/02664763.2022.2163379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning of the structures of cellular networks, such as protein signaling pathways, is a topical research field in computational systems biology. To get the most information out of experimental data, it is often required to develop a tailored statistical approach rather than applying one of the off-the-shelf network reconstruction methods. The focus of this paper is on learning the structure of the mTOR protein signaling pathway from immunoblotting protein phosphorylation data. Under two experimental conditions eleven phosphorylation sites of eight key proteins of the mTOR pathway were measured at ten non-equidistant time points. For the statistical analysis we propose a new advanced hierarchically coupled non-homogeneous dynamic Bayesian network (NH-DBN) model, and we consider various data imputation methods for dealing with non-equidistant temporal observations. Because of the absence of a true gold standard network, we propose to use predictive probabilities in combination with a leave-one-out cross validation strategy to objectively cross-compare the accuracies of different NH-DBN models and data imputation methods. Finally, we employ the best combination of model and data imputation method for predicting the structure of the mTOR protein signaling pathway.},
  archive      = {J_JOAS},
  author       = {Abdul Salam and Marco Grzegorczyk},
  doi          = {10.1080/02664763.2022.2163379},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {845-865},
  shortjournal = {J. Appl. Stat.},
  title        = {Learning the structure of the mTOR protein signaling pathway from protein phosphorylation data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new flexible regression model with application to recovery
probability covid-19 patients. <em>JOAS</em>, <em>51</em>(5), 826–844.
(<a href="https://doi.org/10.1080/02664763.2022.2163229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study is to propose a generalized odd log-logistic Maxwell mixture model to analyze the effect of gender and age groups on lifetimes and on the recovery probabilities of Chinese individuals with COVID-19. We add new properties of the generalized Maxwell model. The coefficients of the regression and the recovered fraction are estimated by maximum likelihood and Bayesian methods. Further, some simulation studies are done to compare the regressions for different scenarios. Model-checking techniques based on the quantile residuals are addressed. The estimated survival functions for the patients are reported by age range and sex. The simulation study showed that mean squared errors decay toward zero and the average estimates converge to the true parameters when sample size increases. According to the fitted model, there is a significant difference only in the age group on the lifetime of individuals with COVID-19. Women have higher probability of recovering than men and individuals aged ≥ 60 years have lower recovered probabilities than those who aged &lt; 60 years. The findings suggest that the proposed model could be a good alternative to analyze censored lifetime of individuals with COVID-19.},
  archive      = {J_JOAS},
  author       = {F. Prataviera and E. M. Hashimoto and E. M. M. Ortega and G. M. Cordeiro and V. G. Cancho and R. Vila},
  doi          = {10.1080/02664763.2022.2163229},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {826-844},
  shortjournal = {J. Appl. Stat.},
  title        = {A new flexible regression model with application to recovery probability covid-19 patients},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The optimal CUSUM control chart with a dynamic non-random
control limit and a given sampling strategy for small samples sequence.
<em>JOAS</em>, <em>51</em>(5), 809–825. (<a
href="https://doi.org/10.1080/02664763.2022.2162863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a performance measure to evaluate the detection performance of a control chart with a given sampling strategy for finite or small samples sequence and prove that the CUSUM control chart with dynamic non-random control limit and a given sampling strategy can be optimal under the measure. Numerical simulations and real data for an earthquake are provided to illustrate that for different sampling strategies, the CUSUM chart will have different monitoring performance in change-point detection. Among the six sampling strategies that take only a part of samples, the numerical comparing results illustrate that the uniform sampling strategy (uniformly dispersed sampling strategy) has the best monitoring effect.},
  archive      = {J_JOAS},
  author       = {Dong Han and Fugee Tsung and Lei Qiao},
  doi          = {10.1080/02664763.2022.2162863},
  journal      = {Journal of Applied Statistics},
  month        = {4},
  number       = {5},
  pages        = {809-825},
  shortjournal = {J. Appl. Stat.},
  title        = {The optimal CUSUM control chart with a dynamic non-random control limit and a given sampling strategy for small samples sequence},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering regions with dynamic time warping to model
obesity prevalence disparities in the united states. <em>JOAS</em>,
<em>51</em>(4), 793–807. (<a
href="https://doi.org/10.1080/02664763.2023.2192445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for clustering adult obesity prevalence by state focus on creating a single map of obesity prevalence for a given year in the United States. Comparing these maps for different years may limit our understanding of the progression of state and regional obesity prevalence over time for the purpose of developing targeted regional health policies. In this application note, we adopt the non-parametric Dynamic Time Warping method for clustering longitudinal time series of obesity prevalence by state. This method captures the lead and lag relationship between the time series as part of the temporal alignment, allowing us to produce a single map that captures the regional and temporal clusters of obesity prevalence from 1990 to 2019 in the United States. We identify six regions of obesity prevalence in the United States and forecast future estimates of obesity prevalence based on ARIMA models.},
  archive      = {J_JOAS},
  author       = {Katherine Vorpe and Sierra Hessinger and Rebekah Poth and Tatjana Miljkovic},
  doi          = {10.1080/02664763.2023.2192445},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {793-807},
  shortjournal = {J. Appl. Stat.},
  title        = {Clustering regions with dynamic time warping to model obesity prevalence disparities in the united states},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cox regression with doubly truncated responses and
time-dependent covariates: The impact of innovation on firm survival.
<em>JOAS</em>, <em>51</em>(4), 780–792. (<a
href="https://doi.org/10.1080/02664763.2023.2178641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of new firms is an important incentive for the economic growth of a country, since it generates employment, it encourages the competition, and promotes innovation. In this work, we investigate the survival of Spanish firms which were created since 2001 and closed down between 2004 and 2012. The information was gathered from Technological Innovation Panel (PITEC), a survey with a focus the technological innovation in Spanish firms. In particular, a Cox regression model with time-dependent covariates was used in order to identify and quantify the determinants of the risk of exit for the firm. The selection bias due to the interval sampling for the firms was corrected by using methods for doubly truncated lifetimes. Interestingly, it is seen how the correction for the selection bias changes both the size and the statistical significance of the effects provided by standard Cox regression.},
  archive      = {J_JOAS},
  author       = {J. de Uña-Álvarez and A. I. Martínez-Senra and M. S. Otero-Giráldez and M. A. Quintás},
  doi          = {10.1080/02664763.2023.2178641},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {780-792},
  shortjournal = {J. Appl. Stat.},
  title        = {Cox regression with doubly truncated responses and time-dependent covariates: The impact of innovation on firm survival},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The sparse estimation of the semiparametric linear
transformation model with dependent current status data. <em>JOAS</em>,
<em>51</em>(4), 759–779. (<a
href="https://doi.org/10.1080/02664763.2022.2161488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the sparse estimation under the semiparametric linear transformation models for the current status data, also called type I interval-censored data. For the problem, the failure time of interest may be dependent on the censoring time and the association parameter between them is left unspecified. To address this, we employ the copula model to describe the dependence between them and a two-stage estimation procedure to estimate both the association parameter and the regression parameter. In addition, we propose a penalized maximum likelihood estimation procedure based on the broken adaptive ridge regression, and Bernstein polynomials are used to approximate the nonparametric functions involved. The oracle property of the proposed method is established and the numerical studies suggest that the method works well for practical situations. Finally, the method is applied to an Alzheimer&#39;s disease study that motivated this investigation.},
  archive      = {J_JOAS},
  author       = {Lin Luo and Jinzhao Yu and Hui Zhao},
  doi          = {10.1080/02664763.2022.2161488},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {759-779},
  shortjournal = {J. Appl. Stat.},
  title        = {The sparse estimation of the semiparametric linear transformation model with dependent current status data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TreeKDE: Clustering multivariate data based on decision tree
and using one-dimensional kernel density estimation. <em>JOAS</em>,
<em>51</em>(4), 740–758. (<a
href="https://doi.org/10.1080/02664763.2022.2159339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an algorithm for clustering multidimensional data, which we named TreeKDE. It is based on a tree structure decision associated with the optimization of the one-dimensional kernel density estimator function constructed from the orthogonal projections of the data on the coordinate axes. Among the main features of the proposed algorithm, we highlight the automatic determination of the number of clusters and their insertion in a rectangular region. Comparative numerical experiments are presented to illustrate the performance of the proposed algorithm and the results indicate that the TreeKDE is efficient and competitive when compared to other algorithms from the literature. Features such as simplicity and efficiency make the proposed algorithm an attractive and promising research field, which can be used as a basis for its improvement, and also for the development of new clustering algorithms based on the association between decision tree and kernel density estimator.},
  archive      = {J_JOAS},
  author       = {D. Scaldelai and L. C. Matioli and S. R. Santos},
  doi          = {10.1080/02664763.2022.2159339},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {740-758},
  shortjournal = {J. Appl. Stat.},
  title        = {TreeKDE: Clustering multivariate data based on decision tree and using one-dimensional kernel density estimation},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian nonparametric model for bounded directional data
on the positive orthant of the unit sphere. <em>JOAS</em>,
<em>51</em>(4), 721–739. (<a
href="https://doi.org/10.1080/02664763.2022.2156485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional data appears in several branches of research. In some cases, those directional variables are only defined in subsets of the K-dimensional unit sphere. For example, in some applications, angles as measured responses are limited on the positive orthant. Analysis on subsets of the K-dimensional unit sphere is challenging and nowadays there are not many proposals that discuss this topic. Thus, from a methodological point of view, it is important to have probability distributions defined on bounded subsets of the K-dimensional unit sphere. Specifically, in this paper, we introduce a nonparametric Bayesian model to describe directional variables restricted to the first orthant. This model is based on a Dirichlet process mixture model with multivariate projected Gamma densities as kernel distributions. We show how to carry out inference for the proposed model based on a slice sampling scheme. The proposed methodology is illustrated using simulated data sets as well as a real data set.},
  archive      = {J_JOAS},
  author       = {Emiliano Geneyro and Gabriel Núñez-Antonio},
  doi          = {10.1080/02664763.2022.2156485},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {721-739},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian nonparametric model for bounded directional data on the positive orthant of the unit sphere},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regression models for the full distribution to exceedance
data. <em>JOAS</em>, <em>51</em>(4), 701–720. (<a
href="https://doi.org/10.1080/02664763.2022.2153812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The list of occurrences linked to significant climate change has grown in recent decades. These changes can be influenced by a set of covariates, such as temperature, location and period of the year. Analyzing the relation among elements and factors that influence the behavior of such events is extremely important for decision-making in order to minimize damages and losses. Exceedance analysis uses the tail of the distribution based on Extreme Value Theory (EVT). Extensions for these models have been proposed in literature, such as regression models for the tail parameters and a parametric or semi-parametric distribution for the part that comes before the tail (well known as bulk distribution). This work presents a new extension to exceedance model, in which the parameters for the bulk distribution capture the effect of covariates such as location and seasonality. We considered a Bayesian approach in the inference procedure. The estimation was done using MCMC -- Markov Chain Monte Carlo methods. Application results for modeling maximum and minimum temperature data showed an efficient estimation of extreme quantiles and a predictive advantage compared to models previously used in literature.},
  archive      = {J_JOAS},
  author       = {Fernando Ferraz do Nascimento and Aline Raquel Assunção Nunes},
  doi          = {10.1080/02664763.2022.2153812},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {701-720},
  shortjournal = {J. Appl. Stat.},
  title        = {Regression models for the full distribution to exceedance data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection of random coefficients in ordered response models:
A framework to detect heterogeneity in household surveys. <em>JOAS</em>,
<em>51</em>(4), 682–700. (<a
href="https://doi.org/10.1080/02664763.2022.2151989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a Bayesian method to detect heterogeneity in the relationship between covariates and the outcome in models with ordered responses. To this end, we construct an efficient Markov chain Monte Carlo algorithm for a hierarchical Bayesian model that selects random coefficients in ordered models. This method extends an approach for selecting random coefficients in linear mixed models into the ordered setting by adding two enhancements that are relevant to the latter category of models. First, we construct steps to efficiently estimate cut-points by addressing identification and ordering constraints. Second, we develop a framework to evaluate marginal effects that combine the fixed and random effects of each covariate. The marginal effects additionally allow for model uncertainty by averaging across models visited by the selection algorithm. Simulation studies demonstrate that this method detects random effects when they are present, estimates parameters accurately and efficiently samples from the posterior with low autocorrelations across successive draws. On applying this method on data from the survey of consumer expectations, we find clear support for the presence of household-level heterogeneity in relationships between demographic variables, and current as well as expected financial conditions.},
  archive      = {J_JOAS},
  author       = {Padma Sharma},
  doi          = {10.1080/02664763.2022.2151989},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {682-700},
  shortjournal = {J. Appl. Stat.},
  title        = {Selection of random coefficients in ordered response models: A framework to detect heterogeneity in household surveys},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A model for bimodal rates and proportions. <em>JOAS</em>,
<em>51</em>(4), 664–681. (<a
href="https://doi.org/10.1080/02664763.2022.2146661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The beta model is the most important distribution for fitting data with the unit interval. However, the beta distribution is not suitable to model bimodal unit interval data. In this paper, we propose a bimodal beta distribution constructed by using an approach based on the alpha-skew-normal model. We discuss several properties of this distribution, such as bimodality, real moments, entropies and identifiability. Furthermore, we propose a new regression model based on the proposed model and discuss residuals. Estimation is performed by maximum likelihood. A Monte Carlo experiment is conducted to evaluate the performances of these estimators in finite samples with a discussion of the results. An application is provided to show the modelling competence of the proposed distribution when the data sets show bimodality.},
  archive      = {J_JOAS},
  author       = {Roberto Vila and Lucas Alfaia and André F.B. Menezes and Mehmet N. Çankaya and Marcelo Bourguignon},
  doi          = {10.1080/02664763.2022.2146661},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {664-681},
  shortjournal = {J. Appl. Stat.},
  title        = {A model for bimodal rates and proportions},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The heterogeneity effect of surveillance intervals on
progression free survival. <em>JOAS</em>, <em>51</em>(4), 646–663. (<a
href="https://doi.org/10.1080/02664763.2022.2145272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progression-free survival (PFS) is an increasingly important surrogate endpoint in cancer clinical trials. However, the true time of progression is typically unknown if the evaluation of progression status is only scheduled at given surveillance intervals. In addition, comparison between treatment arms under different surveillance schema is not uncommon. Our aim is to explore whether the heterogeneity of the surveillance intervals may interfere with the validity of the conclusion of efficacy based on PFS, and the extent to which the variation would bias the results. We conduct comprehensive simulation studies to explore the aforementioned goals in a two-arm randomized control trial. We introduce three steps to simulate survival data with predefined surveillance intervals under different censoring rate considerations. We report the estimated hazard ratios and examine false positive rate, power and bias under different surveillance intervals, given different baseline median PFS, hazard ratio and censoring rate settings. Results show that larger heterogeneous lengths of surveillance intervals lead to higher false positive rate and overestimate the power, and the effect of the heterogeneous surveillance intervals may depend upon both the life expectancy of the tumor prognoses and the censoring proportion of the survival data. We also demonstrate such heterogeneity effect of surveillance intervals on PFS in a phase III metastatic colorectal cancer trial. In our opinions, adherence to consistent surveillance intervals should be favored in designing the comparative trials. Otherwise, it needs to be appropriately taken into account when analyzing data.},
  archive      = {J_JOAS},
  author       = {Zihang Zhong and Min Yang and Senmiao Ni and Lixin Cai and Jingwei Wu and Jianling Bai and Hao Yu},
  doi          = {10.1080/02664763.2022.2145272},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {646-663},
  shortjournal = {J. Appl. Stat.},
  title        = {The heterogeneity effect of surveillance intervals on progression free survival},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap-adjusted quasi-likelihood information criteria for
mixed model selection. <em>JOAS</em>, <em>51</em>(4), 621–645. (<a
href="https://doi.org/10.1080/02664763.2022.2143484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two model selection criteria relying on the bootstrap approach, denoted by QAICb1 and QAICb2, in the framework of linear mixed models. Similar to the justification of Akaike Information Criterion (AIC), the proposed QAICb1 and QAICb2 are proved as asymptotically unbiased estimators of the Kullback–Leibler discrepancy between a candidate model and the true model. However, they are defined on the quasi-likelihood function instead of the likelihood and are proven to be asymptotically equivalent. The proposed selection criteria are constructed by the quasi-likelihood of a candidate model and a bias estimation term in which the bootstrap method is adopted to improve the estimation for the bias caused by using the candidate model to estimate the true model. The simulations across a variety of mixed model settings are conducted to demonstrate that the proposed selection criteria outperform some other existing model selection criteria in selecting the true model. Generalized estimating equations (GEE) are utilized to calculate QAICb1 and QAICb2 in the simulations. The effectiveness of the proposed selection criteria is also demonstrated in an application of Parkinson&#39;s Progression Markers Initiative (PPMI) data.},
  archive      = {J_JOAS},
  author       = {Wentao Ge and Junfeng Shang},
  doi          = {10.1080/02664763.2022.2143484},
  journal      = {Journal of Applied Statistics},
  month        = {3},
  number       = {4},
  pages        = {621-645},
  shortjournal = {J. Appl. Stat.},
  title        = {Bootstrap-adjusted quasi-likelihood information criteria for mixed model selection},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partially constrained group variable selection to adjust for
complementary unit performance in american college football.
<em>JOAS</em>, <em>51</em>(3), 606–620. (<a
href="https://doi.org/10.1080/02664763.2023.2166905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the importance of accurate team rankings in American college football (CFB) – due to heavy title and playoff implications – strides have been made to improve metrics for team performance evaluation, going from basic averages (e.g. points scored per game) to metrics that adjust for a team&#39;s strength of schedule, but one aspect that&#39;s yet to be accounted for is the ability of team&#39;s offense and defense to complement one another, termed ‘complementary football’. American football is unique because the same team&#39;s offensive and defensive units typically consist of separate player sets that don&#39;t share the field simultaneously, which tempts one to evaluate them independently. Yet, some aspects of your team&#39;s defensive (offensive) performance may directly impact the complementary unit, e.g. turnovers forced by your defense could lead to easier scoring chances for your offense. Our main goal is to identify the most consistently influential features of complementary football in a data-driven way, subsequently adjusting each team&#39;s offensive (defensive) performance for that of their complementary unit. To achieve that, for the 2009–2019 CFB seasons, we incorporate natural splines with group penalty approaches, conducting partially constrained optimization to guarantee the full adjustment for the strength of schedule and home-field factor.},
  archive      = {J_JOAS},
  author       = {A. Skripnikov},
  doi          = {10.1080/02664763.2023.2166905},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {606-620},
  shortjournal = {J. Appl. Stat.},
  title        = {Partially constrained group variable selection to adjust for complementary unit performance in american college football},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of COVID-19 on public social life and mental health:
A statistical study of google trends data from the USA. <em>JOAS</em>,
<em>51</em>(3), 581–605. (<a
href="https://doi.org/10.1080/02664763.2022.2164562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has caused a significant disruption in the social lives and mental health of people across the world. This study aims to asses the effect of using internet search volume data. We categorize the widely searched keywords on the internet in several categories, which are relevant in analyzing the public mental health status. Corresponding to each category of keywords, we conduct an appropriate statistical analysis to identify significant changes in the search pattern during the course of the pandemic. Binary segmentation method of changepoint detection, along with the combination of ARMA-GARCH models are utilized in this analysis. It helps us detect how people&#39;s behavior changed in phases and whether the severity of the pandemic brought forth those shifts in behaviors. Interestingly, we find that rather than the severity of the outbreak, the long duration of the pandemic has affected the public health status more. The phases, however, align well with the so-called COVID-19 waves and are consistent for different aspects of social and mental health. We further observe that the results are typically similar for different states as well.},
  archive      = {J_JOAS},
  author       = {Archi Roy and Soudeep Deb and Divya Chakarwarti},
  doi          = {10.1080/02664763.2022.2164562},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {581-605},
  shortjournal = {J. Appl. Stat.},
  title        = {Impact of COVID-19 on public social life and mental health: A statistical study of google trends data from the USA},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust tests for multivariate repeated measures with small
samples. <em>JOAS</em>, <em>51</em>(3), 555–580. (<a
href="https://doi.org/10.1080/02664763.2022.2142537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate repeated measures data naturally arise in clinical trials and other fields such as biomedical science, public health, agriculture, social science and so on. For data of this type, the classical approach is to conduct multivariate analysis of variance (MANOVA) based on Wilks&#39; Lambda and other multivariate statistics, which require the assumptions of multivariate normality and homogeneity of within-cell covariance matrices. However, data being analyzed nowadays show marked departure from multivariate normality and homogeneity. This paper proposes a finite-sample test by modifying the sums of squares matrices to make them insensitive to the heterogeneity in MANOVA. The proposed test is invariant to affine transformation and robust against nonnormality. The proposed method can be used in various experimental designs, for example, factorial design and crossover design. Under various simulation settings, the proposed method outperforms the classical Doubly Multivariate Model and Multivariate Mixed Model proposed elsewhere, especially for unbalanced sample sizes with heteroscedasticity. The applications of the proposed method are illustrated with ophthalmology data in factorial and crossover designs. The proposed method successfully identified and validated a significant main effect and demonstrated that univariate analysis could be oversensitive to small but clinically unimportant interactions.},
  archive      = {J_JOAS},
  author       = {Ting Zeng and Solomon W. Harrar},
  doi          = {10.1080/02664763.2022.2142537},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {555-580},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust tests for multivariate repeated measures with small samples},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal design of stress levels in accelerated degradation
testing for multivariate linear degradation models. <em>JOAS</em>,
<em>51</em>(3), 534–554. (<a
href="https://doi.org/10.1080/02664763.2022.2140332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, more attention has been paid prominently to accelerated degradation testing (ADT) in order to characterize accurate estimation of reliability properties for systems that are designed to work properly for years or even decades. In this paper, we propose optimal experimental designs for repeated measures ADTs with competing failure modes that correspond to multiple response components. The marginal degradation paths are expressed using linear mixed effects models. The optimal design is obtained by minimizing the asymptotic variance of the estimator of some quantile of the failure time distribution at the normal use conditions. Numerical examples are introduced to ensure the robustness of the proposed optimal designs and compare their efficiency with standard experimental designs.},
  archive      = {J_JOAS},
  author       = {Helmi Shat},
  doi          = {10.1080/02664763.2022.2140332},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {534-554},
  shortjournal = {J. Appl. Stat.},
  title        = {Optimal design of stress levels in accelerated degradation testing for multivariate linear degradation models},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New nonparametric measures for instantaneous and
granger-causality tail co-dependence. <em>JOAS</em>, <em>51</em>(3),
515–533. (<a
href="https://doi.org/10.1080/02664763.2022.2138837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new methodology to asses risk spillovers in a time-series framework. Firstly, we introduce an explicit nonparametric measure of cross-sectional conditional tail co-movement, which is intuitively comparable to the Conditional Value-at-Risk (CoVaR). We show that nonlinear CoVaR (NCoVaR) is able to capture even highly nonlinear dependence structures. Secondly, for the purpose of potential contagion analysis, we adapt the measure to be informative about the causality direction between the variables in the Granger causality sense. By showing that the natural estimators of the two metrics are U-statistics, we construct formal nonparametric tests for independence and Granger non-causality. Numerical simulations confirm that in common situations the nonparametric tests have better size and power properties than their parametric counterparts. The methodology is illustrated empirically by assessing risk transmissions between sovereigns and banking sectors in the euro area, which observed highly irregular co-movements between asset prices after the global financial crisis. The new measures seem to be less susceptible to these irregularities than their parametric analogues, providing a clearer overview of the underlying sovereign-bank risk feedback loops.},
  archive      = {J_JOAS},
  author       = {Cees Diks and Marcin Wolski},
  doi          = {10.1080/02664763.2022.2138837},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {515-533},
  shortjournal = {J. Appl. Stat.},
  title        = {New nonparametric measures for instantaneous and granger-causality tail co-dependence},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Performance of diagnostic tests based on continuous
bivariate markers. <em>JOAS</em>, <em>51</em>(3), 497–514. (<a
href="https://doi.org/10.1080/02664763.2022.2137478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical diagnostic research, it is customary to collect multiple continuous biomarker measures to improve the accuracy of diagnostic tests. A prevalent practice is to combine the measurements of these biomarkers into one single composite score. However, incorporating those biomarker measurements into a single score depends on the combination of methods and may lose vital information needed to make an effective and accurate decision. Furthermore, a diagnostic cut-off is required for such a combined score, and it is difficult to interpret in actual clinical practice. The paper extends the classical biomarkers’ accuracy and predictive values from univariate to bivariate markers. Also, we will develop a novel pseudo-measures system to maximize the vital information from multiple biomarkers. We specified these pseudo-and-or classifiers for the true positive rate, true negative rate, false-positive rate, and false-negative rate. We used them to redefine classical measures such as the Youden index, diagnostics odds ratio, likelihood ratios, and predictive values. We provide optimal cut-off point selection based on the modified Youden index with numerical illustrations and real data analysis for this paper&#39;s newly developed pseudo measures.},
  archive      = {J_JOAS},
  author       = {Hani Samawi and Ding-Geng Chen and Jingjing Yin and Marwan Alsharman},
  doi          = {10.1080/02664763.2022.2137478},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {497-514},
  shortjournal = {J. Appl. Stat.},
  title        = {Performance of diagnostic tests based on continuous bivariate markers},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferential procedures based on the weighted pearson
correlation coefficient test statistic. <em>JOAS</em>, <em>51</em>(3),
481–496. (<a
href="https://doi.org/10.1080/02664763.2022.2137477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this note, we evaluated the type I error control of the commonly used t -test found in most statistical software packages for testing the hypothesis on H 0 : ρ = 0 H 0 : ρ = 0 H0:ρ=0 vs. H 1 : ρ &gt; 0 H 1 : ρ &gt; 0 H1:ρ&amp;gt;0 based on the sample weighted Pearson correlation coefficient. We found the type I error rate is severely inflated in general cases, even under bivariate normality. To address this issue, we derived the large sample variance of the weighted Pearson correlation. Based on this result, we proposed an asymptotic test and a set of studentized permutation tests. A comprehensive set of simulation studies with a range of sample sizes and a variety of underlying distributions were conducted. The studentized permutation test based on Fisher&#39;s Z statistic was shown to robustly control the type I error even in the small sample and non-normality settings. The method was demonstrated with an example data of country-level preterm birth rates.},
  archive      = {J_JOAS},
  author       = {Han Yu and Alan D. Hutson},
  doi          = {10.1080/02664763.2022.2137477},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {481-496},
  shortjournal = {J. Appl. Stat.},
  title        = {Inferential procedures based on the weighted pearson correlation coefficient test statistic},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for gompertz distribution under
adaptive type-II progressive hybrid censoring. <em>JOAS</em>,
<em>51</em>(3), 451–480. (<a
href="https://doi.org/10.1080/02664763.2022.2136147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gompertz distribution is a significant and commonly used lifetime distribution, which plays an important role in reliability engineering. In this paper, we study the statistical inference of Gompertz distribution based on adaptive Type-II hybrid progressive censored schemes. From the perspective of frequentist, we derive the point estimations through the method of maximum likelihood estimation (MLE) and the existence of MLE is proved. Besides MLE, we propose the stochastic EM algorithm to reduce complexity and simplify computing. We also apply the method of Bootstraps (Bootstrap-p and Bootstrap-t) to construct confidence intervals. From Bayesian aspect, the Bayes estimates of the unknown parameters are evaluated by applying the MCMC method, the average length and coverage rate of credible intervals are also carried out. The Bayes inference is based on the squared error loss function and LINEX loss function. Furthermore, a numerical simulation is conducted to assess the performance of the proposed methods. Finally, a real-life example is considered to illustrate the application and development of the inference methods. In summary, the Bayesian method seems to perform the best among all approaches, while other approaches also present different advantages.},
  archive      = {J_JOAS},
  author       = {Qi Lv and Yajie Tian and Wenhao Gui},
  doi          = {10.1080/02664763.2022.2136147},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {451-480},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference for gompertz distribution under adaptive type-II progressive hybrid censoring},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric partial linear modeling of risk factors for
ear infections: The early childhood longitudinal study. <em>JOAS</em>,
<em>51</em>(3), 430–450. (<a
href="https://doi.org/10.1080/02664763.2022.2134316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Early Childhood Longitudinal Study–Kindergarten Class of 2010–2011 (ECLS-K:2011) ascertained timing of ear infections within age specified intervals and parent&#39;s/caregiver&#39;s report of medically diagnosed hearing loss. In this nationally representative, school-based sample of children followed from kindergarten entry through fifth grade, academic performance in reading, mathematics, and science was assessed longitudinally. Prior investigations of this ECLS-K:2011 cohort showed that age has a non-linear, monotonically increasing functional relationship with academic performance. Because of this knowledge, a semiparametric partial linear model is proposed, in which the effect of age is modeled by an unknown monotonically increasing function along with other regression parameters. The parameters are estimated by a semiparametric maximum likelihood estimator. A test of a constant effect of age is also proposed. Simulation studies are conducted to evaluate the performance of the proposed method, as compared with the commonly used linear model; the former outperforms the latter based on several criteria. We then analyzed ECLS-K:2011 data to compare results of the partial linear parametric model estimation with that of classical linear regression models.},
  archive      = {J_JOAS},
  author       = {Le Chen and Ruochen Tian and Guanjie Chen and Ao Yuan and Chuan-Ming Li and Amy R. Bentley and Howard J. Hoffman and Charles Rotimi},
  doi          = {10.1080/02664763.2022.2134316},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {430-450},
  shortjournal = {J. Appl. Stat.},
  title        = {Semiparametric partial linear modeling of risk factors for ear infections: The early childhood longitudinal study},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alternative classification rules for two inverse gaussian
populations with a common mean and order restricted scale-like
parameters. <em>JOAS</em>, <em>51</em>(3), 407–429. (<a
href="https://doi.org/10.1080/02664763.2022.2129044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of classification into two inverse Gaussian populations with a common mean and ordered scale-like parameters is considered. Surprisingly, the maximum likelihood estimators (MLEs) of the associated model parameters have not been utilized for classification purposes. Note that the MLEs of the model parameters, including the MLE of the common mean, do not have closed-form expressions. In this paper, several classification rules are proposed that use the MLEs and some plug-in type estimators under order restricted scale-like parameters. In the sequel, the risk values of all the proposed estimators are compared numerically, which shows that the proposed plug-in type restricted MLE performs better than others, including the Graybill-Deal type estimator of the common mean. Further, the proposed classification rules are compared in terms of the expected probability of correct classification (EPC) numerically. It is seen that some of our proposed rules have better performance than the existing ones in most of the parameter space. Two real-life examples are considered for application purposes.},
  archive      = {J_JOAS},
  author       = {Pushkal Kumar and Manas Ranjan Tripathy and Somesh Kumar},
  doi          = {10.1080/02664763.2022.2129044},
  journal      = {Journal of Applied Statistics},
  month        = {2},
  number       = {3},
  pages        = {407-429},
  shortjournal = {J. Appl. Stat.},
  title        = {Alternative classification rules for two inverse gaussian populations with a common mean and order restricted scale-like parameters},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modelling of mental health markers through pregnancy:
A bayesian semi-parametric approach. <em>JOAS</em>, <em>51</em>(2),
388–405. (<a
href="https://doi.org/10.1080/02664763.2022.2154329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maternal depression and anxiety through pregnancy have lasting societal impacts. It is thus crucial to understand the trajectories of its progression from preconception to postnatal period, and the risk factors associated with it. Within the Bayesian framework, we propose to jointly model seven outcomes, of which two are physiological and five non-physiological indicators of maternal depression and anxiety over time. We model the former two by a Gaussian process and the latter by an autoregressive model, while imposing a multidimensional Dirichlet process prior on the subject-specific random effects to account for subject heterogeneity and induce clustering. The model allows for the inclusion of covariates through a regression term. Our findings reveal four distinct clusters of trajectories of the seven health outcomes, characterising women&#39;s mental health progression from before to after pregnancy. Importantly, our results caution against the loose use of hair corticosteroids as a biomarker, or even a causal factor, for pregnancy mental health progression. Additionally, the regression analysis reveals a range of preconception determinants and risk factors for depressive and anxiety symptoms during pregnancy.},
  archive      = {J_JOAS},
  author       = {Shengxiao Vincent Feng and Willem van den Boom and Maria De Iorio and Gladi J. Thng and Jerry K. Y. Chan and Helen Y. Chen and Kok Hian Tan and Michelle Z. L. Kee},
  doi          = {10.1080/02664763.2022.2154329},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {388-405},
  shortjournal = {J. Appl. Stat.},
  title        = {Joint modelling of mental health markers through pregnancy: A bayesian semi-parametric approach},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hidden markov modeling approach combining objective
measure of activity and subjective measure of self-reported sleep to
estimate the sleep-wake cycle. <em>JOAS</em>, <em>51</em>(2), 370–387.
(<a href="https://doi.org/10.1080/02664763.2022.2151576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing the sleep-wake cycle in adolescents is an important prerequisite to better understand the association of abnormal sleep patterns with subsequent clinical and behavioral outcomes. The aim of this research was to develop hidden Markov models (HMM) that incorporate both objective (actigraphy) and subjective (sleep log) measures to estimate the sleep-wake cycle using data from the NEXT longitudinal study, a large population-based cohort study. The model was estimated with a negative binomial distribution for the activity counts (1-minute epochs) to account for overdispersion relative to a Poisson process. Furthermore, self-reported measures were dichotomized (for each one-minute interval) and subject to misclassification. We assumed that the unobserved sleep-wake cycle follows a two-state Markov chain with transitional probabilities varying according to a circadian rhythm. Maximum-likelihood estimation using a backward–forward algorithm was applied to fit the longitudinal data on a subject by subject basis. The algorithm was used to reconstruct the sleep-wake cycle from sequences of self-reported sleep and activity data. Furthermore, we conduct simulations to examine the properties of this approach under different observational patterns including both complete and partially observed measurements on each individual.},
  archive      = {J_JOAS},
  author       = {Semhar B. Ogbagaber and Yifan Cui and Kaigang Li and Ronald J. Iannotti and Paul S. Albert},
  doi          = {10.1080/02664763.2022.2151576},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {370-387},
  shortjournal = {J. Appl. Stat.},
  title        = {A hidden markov modeling approach combining objective measure of activity and subjective measure of self-reported sleep to estimate the sleep-wake cycle},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A size-of-loss model for the negatively skewed insurance
claims data: Applications, risk analysis using different methods and
statistical forecasting. <em>JOAS</em>, <em>51</em>(2), 348–369. (<a
href="https://doi.org/10.1080/02664763.2022.2125936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future values of the expected claims are very important for the insurance companies for avoiding the big losses under uncertainty which may be produced from future claims. In this paper, we define a new size-of-loss distribution for the negatively skewed insurance claims data. Four key risk indicators are defined and analyzed under four estimation methods: maximum likelihood, ordinary least squares, weighted least squares, and Anderson Darling. The insurance claims data are modeled using many competitive models and comprehensive comparison is performed under nine statistical tests. The autoregressive model is proposed to analyze the insurance claims data and estimate the future values of the expected claims. The value-at-risk estimation and the peaks-over random threshold mean-of-order-p methodology are considered.},
  archive      = {J_JOAS},
  author       = {Heba Soltan Mohamed and Gauss M. Cordeiro and R. Minkah and Haitham M. Yousof and Mohamed Ibrahim},
  doi          = {10.1080/02664763.2022.2125936},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {348-369},
  shortjournal = {J. Appl. Stat.},
  title        = {A size-of-loss model for the negatively skewed insurance claims data: Applications, risk analysis using different methods and statistical forecasting},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). K-bessel regression model for speckled data. <em>JOAS</em>,
<em>51</em>(2), 324–347. (<a
href="https://doi.org/10.1080/02664763.2022.2125935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) provides an efficient way to monitor the Earth&#39;s surface. But the speckle noise that the SAR system generates when acquiring images makes it difficult to understand and interpret SAR intensity features. To automatically analyze SAR images, this paper presents a K-Bessel regression (KBR) model in which a function of the mean intensity response is explained by other features (or covariates) determined in parallel. Some mathematical properties of this regression are derived and discussed in the context of the physical origin of the SAR image. A maximum likelihood estimation procedure is planned and its performance is quantified by Monte Carlo experiments. An application to real data obtained from a polarimetric SAR image of San Francisco Bay is realized. Results show that both the KBR-based processing is more informative than the unconditional approach to describe SAR intensity and that our proposal can outperform the normal and gamma regression models. Finally, it is shown that the KBR model is useful to reproduce the relief signal of one channel from the intensity values of the other.},
  archive      = {J_JOAS},
  author       = {A. D. C. Nascimento and P. M. Almeida-Junior and J. M. Vasconcelos and A. P. M. Borges-Junior},
  doi          = {10.1080/02664763.2022.2125935},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {324-347},
  shortjournal = {J. Appl. Stat.},
  title        = {K-bessel regression model for speckled data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical methods for assessing drug interactions using
observational data. <em>JOAS</em>, <em>51</em>(2), 298–323. (<a
href="https://doi.org/10.1080/02664763.2022.2123460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in medicine, many drugs and treatments become available. On the one hand, polydrug use (i.e. using more than one drug at a time) has been used to treat patients with multiple morbid conditions, and polydrug use may cause severe side effects. On the other hand, combination treatments have been successfully developed to treat severe diseases such as cancer and chronic diseases. Observational data, such as electronic health record data, may provide useful information for assessing drug interactions. In this article, we propose using marginal structural models to assess the average treatment effect and causal interaction of two drugs by controlling confounding variables. The causal effect and the interaction of two drugs are assessed using the weighted likelihood approach, with weights being the inverse probability of the treatment assigned. Simulation studies were conducted to examine the performance of the proposed method, which showed that the proposed method was able to estimate the causal parameters consistently. Case studies were conducted to examine the joint effect of metformin and glyburide use on reducing the hospital readmission for type 2 diabetic patients, and to examine the joint effect of antecedent statins and opioids use on the immune and inflammatory biomarkers for COVID-19 hospitalized patients.},
  archive      = {J_JOAS},
  author       = {Qian Xu and Demetra Antimisiaris and Maiying Kong},
  doi          = {10.1080/02664763.2022.2123460},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {298-323},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical methods for assessing drug interactions using observational data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model averaging in calibration of near-infrared instruments
with correlated high-dimensional data. <em>JOAS</em>, <em>51</em>(2),
279–297. (<a
href="https://doi.org/10.1080/02664763.2022.2122947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model averaging (MA) is a modelling strategy where the uncertainty in the configuration of selected variables is taken into account by weight-combining each estimate of the so-called ‘candidate model’. Some studies have shown that MA enables better prediction, even in high-dimensional cases. However, little is known about the model prediction performance at different types of multicollinearity in high-dimensional data. Motivated by calibration of near-infrared (NIR) instruments,we focus on MA prediction performance in such data. The weighting schemes that we consider are based on the Akaike’s information criterion (AIC), Mallows’ C p , and cross-validation. For estimating the model parameters, we consider the standard least squares and the ridge regression methods. The results indicate that MA outperforms model selection methods such as LASSO and SCAD in high-correlation data. The use of Mallows’ C p and cross-validation for the weights tends to yield similar results in all structures of correlation, although the former is generally preferred. We also find that the ridge model averaging outperforms the least-squares model averaging. This research suggests ridge model averaging to build a relatively better prediction of the NIR calibration model.},
  archive      = {J_JOAS},
  author       = {Deiby Tineke Salaki and Anang Kurnia and Bagus Sartono and I Wayan Mangku and Arief Gusnanto},
  doi          = {10.1080/02664763.2022.2122947},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {279-297},
  shortjournal = {J. Appl. Stat.},
  title        = {Model averaging in calibration of near-infrared instruments with correlated high-dimensional data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of single- and double-threshold ROC plots for
mixture distributions. <em>JOAS</em>, <em>51</em>(2), 256–278. (<a
href="https://doi.org/10.1080/02664763.2022.2122027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The receiver operating characteristics (ROC) analysis is commonly used in clinical settings to check the performance of a single threshold for distinguishing population-wise bimodal-distributed test results. However, for population-wise three-modal distributed test results, a single threshold ROC (stROC) analysis showed poor discriminative performance. The purpose of this study is to use a double-threshold ROC analysis for the three-modal distributed test results to provide better discriminative performance than the stROC analysis. A double-threshold receiver operating characteristic plot (dtROC) is constructed by replacing the single threshold with a double threshold. The sensitivity and specificity coordinates are chosen to maximize sensitivity for a given specificity value. Besides a simulation study assuming a mixture of lognormal, Poisson, and Weibull distributions, a clinical application is examined by a secondary data analysis of palpation test results of the C7 spinous process using the modified thorax–rib static technique. For the assumed mixture models, the discrimination performance of dtROC analysis outperforms the stROC analysis (area under ROC (AUROC) increased from 0.436 to 0.983 for lognormal distributed test results, 0.676 to 0.752 for the Poisson distribution, and 0.674 to 0.804 for Weibull distribution).},
  archive      = {J_JOAS},
  author       = {Faryal Ibrar and Sajid Ali and Ismail Shah},
  doi          = {10.1080/02664763.2022.2122027},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {256-278},
  shortjournal = {J. Appl. Stat.},
  title        = {A comparison of single- and double-threshold ROC plots for mixture distributions},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrapping a powerful mixed portmanteau test for time
series. <em>JOAS</em>, <em>51</em>(2), 230–255. (<a
href="https://doi.org/10.1080/02664763.2022.2121384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new portmanteau test statistic is proposed for detecting nonlinearity in time series data. The new portmanteau statistic is calculated from the log of the determinant of a matrix comprised of the autocorrelations and cross-correlations of the residuals and squared residuals of a fitted time series. The asymptotic distribution of the proposed test statistic is derived as a linear combination of chi-square distributed random variables and can be approximated by a gamma distribution. A bootstrapping approach is shown to be robust when distributional assumptions are relaxed. The efficacy of the statistic is studied against linear and nonlinear dependency structures of some stationary time series models. It is shown that the new test can provide higher power than other tests in many situations. We demonstrate the advantages of the proposed test by investigating linear and nonlinear effects in an economic series and two environmental time series.},
  archive      = {J_JOAS},
  author       = {Esam Mahdi and Thomas J. Fisher},
  doi          = {10.1080/02664763.2022.2121384},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {230-255},
  shortjournal = {J. Appl. Stat.},
  title        = {Bootstrapping a powerful mixed portmanteau test for time series},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic model for NFL games and point spread
assessment. <em>JOAS</em>, <em>51</em>(2), 216–229. (<a
href="https://doi.org/10.1080/02664763.2022.2120973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modelling of sports data is indispensable to analyse the sports behaviour and apprehend significant inferences that are helpful to adopt decisive strategies before or during the sports events. This paper introduces a stochastic model as the distribution of difference derived from the Bivariate Affine-Linear Exponential distribution. The distribution of difference is first ever used to model the margin of victory that provides an adequate fitting on the observed data. A simulation study is carried out to observe the stability of the model parameters through their average estimated values, biases, standard errors, root mean square errors and confidence intervals. The performance of the proposed model is examined by applying it on the real data of the National Football League and comparing the results with those of the existing models. Finally, the quantile function of the proposed distribution is used to assess the possible range of point spreads for winning the bet in a particular game.},
  archive      = {J_JOAS},
  author       = {Muhammad Mohsin and Albrecht Gebhardt},
  doi          = {10.1080/02664763.2022.2120973},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {216-229},
  shortjournal = {J. Appl. Stat.},
  title        = {A stochastic model for NFL games and point spread assessment},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach for de-duplication in the presence of
relational data. <em>JOAS</em>, <em>51</em>(2), 197–215. (<a
href="https://doi.org/10.1080/02664763.2022.2118678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the impact of combining profile and network data in solving record de-duplication problems. We also assess the influence of a range of prior distributions on the linkage structure, and explore the use of stochastic gradient Hamiltonian Monte Carlo methods as a faster alternative to obtain samples from the posterior distribution for network parameters. Our methodology is evaluated using the RLdata500 data, which is a popular dataset in the record linkage literature.},
  archive      = {J_JOAS},
  author       = {Juan Sosa and Abel Rodríguez},
  doi          = {10.1080/02664763.2022.2118678},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {2},
  pages        = {197-215},
  shortjournal = {J. Appl. Stat.},
  title        = {A bayesian approach for de-duplication in the presence of relational data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New advances in statistics and data science. <em>JOAS</em>,
<em>51</em>(1), 193–195. (<a
href="https://doi.org/10.1080/02664763.2022.2125502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {Jianguo Sun},
  doi          = {10.1080/02664763.2022.2125502},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {193-195},
  shortjournal = {J. Appl. Stat.},
  title        = {New advances in statistics and data science},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage latent factor regression method to model the
common and unique effects of multiple highly correlated exposure
variables. <em>JOAS</em>, <em>51</em>(1), 168–192. (<a
href="https://doi.org/10.1080/02664763.2022.2138838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many epidemiological and environmental health studies, developing an accurate exposure assessment of multiple exposures on a health outcome is often of interest. However, the problem is challenging in the presence of multicollinearity, which can lead to biased estimates of regression coefficients and inflated variance estimators. Selecting one exposure variable as a surrogate of multiple highly correlated exposure variables is often suggested in the literature as a solution to handle the multicollinearity problem. However, this may lead to loss of information, since the exposure variables that are highly correlated tend to have not only common but also additional effects on the outcome variable. In this study, a two-stage latent factor regression method is proposed. The key idea is to regress the dependent variable not only on the common latent factor(s) of the explanatory variables, but also on the residuals terms from the factor analysis as the explanatory variables. The proposed method is compared to the traditional latent factor regression and principal component regression for their performance of handling multicollinearity. Two case studies are presented. Simulation studies are performed to assess their performances in terms of the epidemiological interpretation and stability of parameter estimates.},
  archive      = {J_JOAS},
  author       = {Cindy Feng and Xi Chen},
  doi          = {10.1080/02664763.2022.2138838},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {168-192},
  shortjournal = {J. Appl. Stat.},
  title        = {A two-stage latent factor regression method to model the common and unique effects of multiple highly correlated exposure variables},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dealing with missing data under stratified sampling designs
where strata are study domains. <em>JOAS</em>, <em>51</em>(1), 153–167.
(<a href="https://doi.org/10.1080/02664763.2022.2112937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quick count seeks to estimate the voting trends of an election and communicate them to the population on the evening of the same day of the election. In quick counts, the sampling is based on a stratified design of polling stations. Voting information is gathered gradually, often with no guarantee of obtaining the complete sample or even information in all the strata. However, accurate interval estimates with partial information must be obtained. Furthermore, this becomes more challenging if the strata are additionally study domains. To produce partial estimates, two strategies are proposed: (1) a Bayesian model using a dynamic post-stratification strategy and a single imputation process defined after a thorough analysis of historic voting information; additionally, a credibility level correction is included to solve the underestimation of the variance and (2) a frequentist alternative that combines standard multiple imputation ideas with classic sampling techniques to obtain estimates under a missing information framework. Both solutions are illustrated and compared using information from the 2021 quick count. The aim was to estimate the composition of the Chamber of Deputies in Mexico.},
  archive      = {J_JOAS},
  author       = {Carlos E. Rodríguez and Luis E. Nieto-Barajas and Carlos S. Pérez-Pérez},
  doi          = {10.1080/02664763.2022.2112937},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {153-167},
  shortjournal = {J. Appl. Stat.},
  title        = {Dealing with missing data under stratified sampling designs where strata are study domains},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuity corrected score confidence interval for the
difference in proportions in paired data. <em>JOAS</em>, <em>51</em>(1),
139–152. (<a
href="https://doi.org/10.1080/02664763.2022.2118245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For paired binary data, the hybrid method and the score method are often recommended for use to calculate the confidence interval for risk difference. These asymptotic intervals do not control the coverage probability. We propose to develop a new score interval with continuity correction to further improve the performance of the existing intervals. The traditional correction value may be too large which leads to a wide interval. For that reason, we propose three different correction values to identify the optimal correction interval with balanced coverage probability and interval width. From simulation studies, we find that a small correction value for the score interval has good performance. In addition, we derive the non-iterative solutions for the developed continuity correction score intervals.},
  archive      = {J_JOAS},
  author       = {Peter Chang and Rongzi Liu and Tingting Hou and Xinyu Yan and Guogen Shan},
  doi          = {10.1080/02664763.2022.2118245},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {139-152},
  shortjournal = {J. Appl. Stat.},
  title        = {Continuity corrected score confidence interval for the difference in proportions in paired data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian dynamic network modelling: An application to
metabolic associations in cardiovascular diseases. <em>JOAS</em>,
<em>51</em>(1), 114–138. (<a
href="https://doi.org/10.1080/02664763.2022.2116746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach to the estimation of multiple Graphical Models to analyse temporal patterns of association among a set of metabolites over different groups of patients. Our motivating application is the Southall And Brent REvisited (SABRE) study, a tri-ethnic cohort study conducted in the UK. We are interested in identifying potential ethnic differences in metabolite levels and associations as well as their evolution over time, with the aim of gaining a better understanding of different risk of cardio-metabolic disorders across ethnicities. Within a Bayesian framework, we employ a nodewise regression approach to infer the structure of the graphs, borrowing information across time as well as across ethnicities. The response variables of interest are metabolite levels measured at two time points and for two ethnic groups, Europeans and South-Asians. We use nodewise regression to estimate the high-dimensional precision matrices of the metabolites, imposing sparsity on the regression coefficients through the dynamic horseshoe prior, thus favouring sparser graphs. We provide the code to fit the proposed model using the software Stan , which performs posterior inference using Hamiltonian Monte Carlo sampling, as well as a detailed description of a block Gibbs sampling scheme.},
  archive      = {J_JOAS},
  author       = {Marco Molinari and Andrea Cremaschi and Maria De Iorio and Nishi Chaturvedi and Alun Hughes and Therese Tillin},
  doi          = {10.1080/02664763.2022.2116746},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {114-138},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian dynamic network modelling: An application to metabolic associations in cardiovascular diseases},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A test for comparing conditional ROC curves with
multidimensional covariates. <em>JOAS</em>, <em>51</em>(1), 87–113. (<a
href="https://doi.org/10.1080/02664763.2022.2116409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The comparison of Receiver Operating Characteristic (ROC) curves is frequently used in the literature to compare the discriminatory capability of different classification procedures based on diagnostic variables. The performance of these variables can be sometimes influenced by the presence of other covariates, and thus they should be taken into account when making the comparison. A new non-parametric test is proposed here for testing the equality of two or more dependent ROC curves conditioned to the value of a multidimensional covariate. Projections are used for transforming the problem into a one-dimensional approach easier to handle. Simulations are carried out to study the practical performance of the new methodology. The procedure is then used to analyse a real data set of patients with Pleural Effusion to compare the diagnostic capability of different markers.},
  archive      = {J_JOAS},
  author       = {A. Fanjul-Hevia and J. C. Pardo-Fernández and I. Van Keilegom and W. González-Manteiga},
  doi          = {10.1080/02664763.2022.2116409},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {87-113},
  shortjournal = {J. Appl. Stat.},
  title        = {A test for comparing conditional ROC curves with multidimensional covariates},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A markov random field model with cumulative logistic
functions for spatially dependent ordinal data. <em>JOAS</em>,
<em>51</em>(1), 70–86. (<a
href="https://doi.org/10.1080/02664763.2022.2115985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a class of regression models with cumulative logistic functions that are chiefly designed to analyse spatially dependent ordinal data. In contrast to previous works, the proposed model requires neither the sites to be regularly spaced nor the assumption of an underlying continuous variable. It belongs to a more general class of Markov random field models, and can be considered an extension of the ordinal regression model with the proportional odds link function. Our proposed model allows practitioners to interpret the model parameters using odds ratios. Apart from the theoretical developments, this work also highlights the practical aspects of model fitting, including parameterisation, selection of neighbourhood, and calculation of standard errors. Simulation studies with regularly and irregularly spaced sites were conducted. Modelling strategies including pseudo-likelihood methods were found to be useful in both settings. The proposed model and the non-spatial counterpart were applied to the daily air quality index measured in the United Kingdom. The results indicate the presence of spatial effects and the incorporation of spatial effects led to better model performance in terms of various goodness-of-fit measures.},
  archive      = {J_JOAS},
  author       = {Ryan H.L. Ip and K.Y.K. Wu},
  doi          = {10.1080/02664763.2022.2115985},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {70-86},
  shortjournal = {J. Appl. Stat.},
  title        = {A markov random field model with cumulative logistic functions for spatially dependent ordinal data},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model averaging estimation of panel data models with many
instruments and boosting. <em>JOAS</em>, <em>51</em>(1), 53–69. (<a
href="https://doi.org/10.1080/02664763.2022.2114432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applied researchers often confront two issues when using the fixed effect-two-stage least squares (FE-2SLS) estimator for panel data models. One is that it may lose its consistency due to too many instruments. The other is that the gain of using FE-2SLS may not exceed its loss when the endogeneity is weak. In this paper, an L 2 L 2 L2 Boosting regularization procedure for panel data models is proposed to tackle the many instruments issue. We then construct a Stein-like model-averaging estimator to take advantage of FE and FE-2SLS-Boosting estimators. Finite sample properties are examined in Monte Carlo and an empirical application is presented.},
  archive      = {J_JOAS},
  author       = {Hao Hao and Bai Huang and Tae-hwy Lee},
  doi          = {10.1080/02664763.2022.2114432},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {53-69},
  shortjournal = {J. Appl. Stat.},
  title        = {Model averaging estimation of panel data models with many instruments and boosting},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new method for estimating sharpe ratio function via local
maximum likelihood. <em>JOAS</em>, <em>51</em>(1), 34–52. (<a
href="https://doi.org/10.1080/02664763.2022.2114431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sharpe ratio function is a commonly used risk/return measure in financial econometrics. To estimate this function, most existing methods take a two-step procedure that first estimates the mean and volatility functions separately and then applies the plug-in method. In this paper, we propose a direct method via local maximum likelihood to simultaneously estimate the Sharpe ratio function and the negative log-volatility function as well as their derivatives. We establish the joint limiting distribution of the proposed estimators, and moreover extend the proposed method to estimate the multivariate Sharpe ratio function. We also evaluate the numerical performance of the proposed estimators through simulation studies, and compare them with existing methods. Finally, we apply the proposed method to the three-month US Treasury bill data and that captures a well-known covariate-dependent effect on the Sharpe ratio.},
  archive      = {J_JOAS},
  author       = {Wenchao Xu and Hongmei Lin and Tiejun Tong and Riquan Zhang},
  doi          = {10.1080/02664763.2022.2114431},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {34-52},
  shortjournal = {J. Appl. Stat.},
  title        = {A new method for estimating sharpe ratio function via local maximum likelihood},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation and prediction for burr type III distribution
based on unified progressive hybrid censoring scheme. <em>JOAS</em>,
<em>51</em>(1), 1–33. (<a
href="https://doi.org/10.1080/02664763.2022.2113865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present communication develops the tools for estimation and prediction of the Burr-III distribution under unified progressive hybrid censoring scheme. The maximum likelihood estimates of model parameters are obtained. It is shown that the maximum likelihood estimates exist uniquely. Expectation maximization and stochastic expectation maximization methods are employed to compute the point estimates of unknown parameters. Based on the asymptotic distribution of the maximum likelihood estimators, approximate confidence intervals are proposed. In addition, the bootstrap confidence intervals are constructed. Furthermore, the Bayes estimates are derived with respect to squared error and LINEX loss functions. To compute the approximate Bayes estimates, Metropolis–Hastings algorithm is adopted. The highest posterior density credible intervals are obtained. Further, maximum a posteriori estimates of the model parameters are computed. The Bayesian predictive point, as well as interval estimates, are proposed. A Monte Carlo simulation study is employed in order to evaluate the performance of the proposed statistical procedures. Finally, two real data sets are considered and analysed to illustrate the methodologies established in this paper.},
  archive      = {J_JOAS},
  author       = {Subhankar Dutta and Suchandan Kayal},
  doi          = {10.1080/02664763.2022.2113865},
  journal      = {Journal of Applied Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation and prediction for burr type III distribution based on unified progressive hybrid censoring scheme},
  volume       = {51},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
