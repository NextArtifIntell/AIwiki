<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JBES_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jbes---108">JBES - 108</h2>
<ul>
<li><details>
<summary>
(2024). Robust narrowest significance pursuit: Inference for
multiple change-points in the median. <em>JBES</em>, <em>42</em>(4),
1389–1402. (<a
href="https://doi.org/10.1080/07350015.2024.2316103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Robust Narrowest Significance Pursuit (RNSP), a methodology for detecting localized regions in data sequences which each must contain a change-point in the median, at a prescribed global significance level. RNSP works by fitting the postulated constant model over many regions of the data using a new sign-multiresolution sup-norm-type loss, and greedily identifying the shortest intervals on which the constancy is significantly violated. By working with the signs of the data around fitted model candidates, RNSP fulfils its coverage promises under minimal assumptions, requiring only sign-symmetry and serial independence of the signs of the true residuals. In particular, it permits their heterogeneity and arbitrarily heavy tails. The intervals of significance returned by RNSP have a finite-sample character, are unconditional in nature and do not rely on any assumptions on the true signal. Code implementing RNSP is available at https://github.com/pfryz/nsp .},
  archive      = {J_JBES},
  author       = {Piotr Fryzlewicz},
  doi          = {10.1080/07350015.2024.2316103},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1389-1402},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Robust narrowest significance pursuit: Inference for multiple change-points in the median},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Should humans lie to machines? The incentive compatibility
of lasso and GLM structured sparsity estimators. <em>JBES</em>,
<em>42</em>(4), 1379–1388. (<a
href="https://doi.org/10.1080/07350015.2024.2316102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider situations where a user feeds her attributes to a machine learning method that tries to predict her best option based on a random sample of other users. The predictor is incentive-compatible if the user has no incentive to misreport her covariates. Focusing on the popular Lasso estimation technique, we borrow tools from high-dimensional statistics to characterize sufficient conditions that ensure that Lasso is incentive compatible in the asymptotic case. We extend our results to a new nonlinear machine learning technique, Generalized Linear Model Structured Sparsity estimators. Our results show that incentive compatibility is achieved if the tuning parameter is kept above some threshold in the case of asymptotics.},
  archive      = {J_JBES},
  author       = {Mehmet Caner and Kfir Eliaz},
  doi          = {10.1080/07350015.2024.2316102},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1379-1388},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Should humans lie to machines? the incentive compatibility of lasso and GLM structured sparsity estimators},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing quantile forecast optimality. <em>JBES</em>,
<em>42</em>(4), 1367–1378. (<a
href="https://doi.org/10.1080/07350015.2024.2316091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile forecasts made across multiple horizons have become an important output of many financial institutions, central banks and international organizations. This article proposes misspecification tests for such quantile forecasts that assess optimality over a set of multiple forecast horizons and/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile regressions cast in a moment equality framework. Our main test is for the null hypothesis of autocalibration, a concept which assesses optimality with respect to the information contained in the forecasts themselves. We provide an extension that allows to test for optimality with respect to larger information sets and a multivariate extension. Importantly, our tests do not just inform about general violations of optimality, but may also provide useful insights into specific forms of sub-optimality. A simulation study investigates the finite sample performance of our tests, and two empirical applications to financial returns and U.S. macroeconomic series illustrate that our tests can yield interesting insights into quantile forecast sub-optimality and its causes.},
  archive      = {J_JBES},
  author       = {Jack Fosten and Daniel Gutknecht and Marc-Oliver Pohle},
  doi          = {10.1080/07350015.2024.2316091},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1367-1378},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing quantile forecast optimality},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive testing for alphas in conditional factor models
with high dimensional assets. <em>JBES</em>, <em>42</em>(4), 1356–1366.
(<a href="https://doi.org/10.1080/07350015.2024.2313543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on testing for the presence of alpha in time-varying factor pricing models, specifically when the number of securities N is larger than the time dimension of the return series T . We introduce a maximum-type test that performs well in scenarios where the alternative hypothesis is sparse. We establish the limit null distribution of the proposed maximum-type test statistic and demonstrate its asymptotic independence from the sum-type test statistics proposed by Ma et al. Additionally, we propose an adaptive test by combining the maximum-type test and sum-type test, and we show its advantages under various alternative hypotheses through simulation studies and two real data applications.},
  archive      = {J_JBES},
  author       = {Huifang Ma and Long Feng and Zhaojun Wang and Jigang Bao},
  doi          = {10.1080/07350015.2024.2313543},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1356-1366},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Adaptive testing for alphas in conditional factor models with high dimensional assets},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum-subsampling test of equal predictive ability.
<em>JBES</em>, <em>42</em>(4), 1344–1355. (<a
href="https://doi.org/10.1080/07350015.2024.2311196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparing the accuracy of any two competing forecasts, under the assumption that the loss differentials are covariance stationary, Diebold and Mariano (DM) proposed a test statistic that is asymptotically normal. This test is further studied by Giacomini and White (GW). However, under the DM-GW framework, the estimator of the variance of the DM test can be inaccurate in small samples, which yields size distortions; see, for example, Coroneo and Iacone (CI). To alleviate this challenge, we propose a maximum-subsampling (MS) test that does not require estimating the long-run variance induced by a number of autocovariances. Accordingly, the MS test is applicable for comparing the predictive ability between two competing forecasts even when the loss differentials are serially correlated with arbitrary autocovariance structures. We demonstrate that the MS test converges to the type I extreme value distribution under the DM-GW null hypothesis with proper conditions. In addition, under a set of alternative hypotheses, we show the MS test is consistent. We further compare the MS test with the DM test and two CI tests via five simulation settings, which are either modified or adapted from McCracken and Coroneo and Iacone. Simulation results show that the MS test performs satisfactorily.},
  archive      = {J_JBES},
  author       = {Wei Lan and Bo Lei and Long Feng and Chih-Ling Tsai},
  doi          = {10.1080/07350015.2024.2311196},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1344-1355},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Maximum-subsampling test of equal predictive ability},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection based testing for parameter changes in
regression with autoregressive dependence. <em>JBES</em>,
<em>42</em>(4), 1331–1343. (<a
href="https://doi.org/10.1080/07350015.2024.2310025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a regression model with autoregressive terms and propose significance tests for the detection of change points in this model. Our tests are applicable to both low- or moderate dimension and to high-dimension with sparse regressors. The dimension may be high from the practical point of view of economic and business applications, but in our theoretical framework it is fixed. To accommodate practically high dimension, variable selection is incorporated as an integral part of our approach. The regressors and the errors can exhibit general nonlinear dependence and the model incorporates autoregressive dependence. We develop asymptotic justification and evaluate the performance of the tests both on simulated and real economic data. We test for and estimate changes in responses to risk factors of a U.S. energy stocks portfolio and the Industrial Production index. We relate our findings to macroeconomic policy changes and global impact events.},
  archive      = {J_JBES},
  author       = {Lajos Horváth and Piotr Kokoszka and Shanglin Lu},
  doi          = {10.1080/07350015.2024.2310025},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1331-1343},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Variable selection based testing for parameter changes in regression with autoregressive dependence},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification and auto-debiased machine learning for
outcome-conditioned average structural derivatives. <em>JBES</em>,
<em>42</em>(4), 1318–1330. (<a
href="https://doi.org/10.1080/07350015.2024.2310022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new class of heterogeneous causal quantities, referred to as outcome-conditioned average structural derivatives (OASDs), in a general nonseparable model. An OASD is the average partial effect of a marginal change in a continuous treatment on individuals located on different parts of an outcome distribution, irrespective of individuals’ characteristics. We show that OASDs extend the unconditional quantile partial effects (UQPE) proposed by Firpo, Fortin, and Lemieux to that conditional on a set of outcome values by effectively integrating the UQPE. Exploiting such relationship brings about two merits. First, unlike UQPE that is generally not n -estimable, OASD is shown to be n -estimable. Second, our estimator achieves semiparametric efficiency bound which is a new result in the literature. We propose a novel, automatic, debiased machine-learning estimator for an OASD, and present asymptotic statistical guarantees for it. The estimator is proven to be n -consistent, asymptotically normal, and semi-parametrically efficient. We also prove the validity of the bootstrap procedure for uniform inference for the OASD process. We apply the method to Imbens, Rubin, and Sacerdote’s lottery data.},
  archive      = {J_JBES},
  author       = {Zequn Jin and Lihua Lin and Zhengyu Zhang},
  doi          = {10.1080/07350015.2024.2310022},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1318-1330},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Identification and auto-debiased machine learning for outcome-conditioned average structural derivatives},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating growth-at-risk using a multicountry
nonparametric quantile factor model. <em>JBES</em>, <em>42</em>(4),
1302–1317. (<a
href="https://doi.org/10.1080/07350015.2024.2310020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a nonparametric quantile panel regression model. Within each quantile, the quantile function is a combination of linear and nonlinear parts, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information is captured through a conditionally heteroscedastic latent factor. The nonparametric feature enhances flexibility, while the panel feature increases the number of observations in the tails. We develop Bayesian methods for inference and apply several versions of the model to study growth-at-risk dynamics in a panel of 11 advanced economies. Our framework usually improves upon single-country quantile models in recursive growth forecast comparisons.},
  archive      = {J_JBES},
  author       = {Todd E. Clark and Florian Huber and Gary Koop and Massimiliano Marcellino and Michael Pfarrhofer},
  doi          = {10.1080/07350015.2024.2310020},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1302-1317},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Investigating growth-at-risk using a multicountry nonparametric quantile factor model},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for equivalence of pre-trends in
difference-in-differences estimation. <em>JBES</em>, <em>42</em>(4),
1289–1301. (<a
href="https://doi.org/10.1080/07350015.2024.2308121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The plausibility of the “parallel trends assumption” in Difference-in-Differences estimation is usually assessed by a test of the null hypothesis that the difference between the average outcomes of both groups is constant over time before the treatment. However, failure to reject the null hypothesis does not imply the absence of differences in time trends between both groups. We provide equivalence tests that allow researchers to find evidence in favor of the parallel trends assumption and thus increase the credibility of their treatment effect estimates. While we motivate our tests in the standard two-way fixed effects model, we discuss simple extensions to settings in which treatment adoption is staggered over time.},
  archive      = {J_JBES},
  author       = {Holger Dette and Martin Schumann},
  doi          = {10.1080/07350015.2024.2308121},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1289-1301},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing for equivalence of pre-trends in difference-in-differences estimation},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The block-correlated pseudo marginal sampler for state space
models. <em>JBES</em>, <em>42</em>(4), 1276–1288. (<a
href="https://doi.org/10.1080/07350015.2024.2308109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo Marginal Metropolis-Hastings (PMMH) is a general approach to Bayesian inference when the likelihood is intractable, but can be estimated unbiasedly. Our article develops an efficient PMMH method that scales up better to higher dimensional state vectors than previous approaches. The improvement is achieved by the following innovations. First, a novel block version of PMMH that works with multiple particle filters is proposed. Second, the trimmed mean of the unbiased likelihood estimates of the multiple particle filters is used. Third, the article develops an efficient auxiliary disturbance particle filter, which is necessary when the bootstrap disturbance filter is inefficient, but the state transition density cannot be expressed in closed form. Fourth, a novel sorting algorithm, which is as effective as previous approaches but significantly faster than them, is developed to preserve the correlation between the logs of the likelihood estimates at the current and proposed parameter values. The performance of the sampler is investigated empirically by applying it to nonlinear Dynamic Stochastic General Equilibrium models with relatively high state dimensions and with intractable state transition densities and to multivariate GARCH diffusion-driven volatility in the mean models. Although we only apply the method to state space models, the approach will be useful in a wide range of applications such as large panel data models and stochastic differential equation models with mixed effects.},
  archive      = {J_JBES},
  author       = {David Gunawan and Pratiti Chatterjee and Robert Kohn},
  doi          = {10.1080/07350015.2024.2308109},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1276-1288},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The block-correlated pseudo marginal sampler for state space models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Policy analysis using multilevel regression models with
group interactive fixed effects. <em>JBES</em>, <em>42</em>(4),
1264–1275. (<a
href="https://doi.org/10.1080/07350015.2024.2308108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of multilevel regression models is prevalent in policy analysis to estimate the effect of group level policies on individual outcomes. In order to allow for the time varying effect of group heterogeneity and the group specific impact of time effects, we propose a group interactive fixed effects approach that employs interaction terms of group factor loadings and common factors in this model. For this approach, we consider the least squares estimator and associated inference procedure. We examine their properties under the large n and fixed T asymptotics. The number of groups, G , is allowed to grow but at a slower rate. We also propose a test for the level of grouping to specify group factor loadings, and a GMM approach to address policy endogeneity with respect to idiosyncratic errors. Finally, we provide empirical illustrations of the proposed approach using two empirical examples.},
  archive      = {J_JBES},
  author       = {Zhenhao Gong and Min Seong Kim},
  doi          = {10.1080/07350015.2024.2308108},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1264-1275},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Policy analysis using multilevel regression models with group interactive fixed effects},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting multiple level shifts in bounded time series.
<em>JBES</em>, <em>42</em>(4), 1250–1263. (<a
href="https://doi.org/10.1080/07350015.2024.2308107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes a sequential statistical procedure to test for the presence of level shifts affecting bounded time series, regardless of their order of integration. The article shows that bounds are relevant for the statistic that assumes that the time series are integrated of order one. In contrast, they do not affect the limiting distribution of the statistic that is defined for time series that are integrated of order zero. The article proposes a union rejection statistic for bounded processes that does not require information about the order of integration of the stochastic processes. The model specification is general enough to consider the existence of structural breaks that can affect either the level of the time series and/or the bounds that limit its evolution. Monte Carlo simulations indicate that the procedure works well in finite samples. An empirical application that focuses on the Swiss franc against the euro exchange rate evolution illustrates the usefulness of the proposal.},
  archive      = {J_JBES},
  author       = {Josep Lluís Carrion-i-Silvestre and María Dolores Gadea},
  doi          = {10.1080/07350015.2024.2308107},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1250-1263},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Detecting multiple level shifts in bounded time series},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic realized minimum variance portfolio models.
<em>JBES</em>, <em>42</em>(4), 1238–1249. (<a
href="https://doi.org/10.1080/07350015.2024.2308106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a dynamic minimum variance portfolio (MVP) model using nonlinear volatility dynamic models, based on high-frequency financial data. Specifically, we impose an autoregressive dynamic structure on MVP processes, which helps capture the MVP dynamics directly. To evaluate the dynamic MVP model, we estimate the inverse volatility matrix using the constrained l 1 -minimization for inverse matrix estimation (CLIME) and calculate daily realized non-normalized MVP weights. Based on the realized non-normalized MVP weight estimator, we propose the dynamic MVP model, which we call the dynamic realized minimum variance portfolio (DR-MVP) model. To estimate a large number of parameters, we employ the least absolute shrinkage and selection operator (LASSO) and predict the future MVP and establish its asymptotic properties. Using high-frequency trading data, we apply the proposed method to MVP prediction.},
  archive      = {J_JBES},
  author       = {Donggyu Kim and Minseog Oh},
  doi          = {10.1080/07350015.2024.2308106},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1238-1249},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Dynamic realized minimum variance portfolio models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noncommon breaks. <em>JBES</em>, <em>42</em>(4), 1223–1237.
(<a href="https://doi.org/10.1080/07350015.2024.2301969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new Bayesian approach to estimate noncommon structural breaks in panel regression models. Any subset of the cross-section may be hit at different times within a break window. Break-specific parameters are learned from the cross-section. They reflect whether (i) breaks hit many or few series and (ii) there is a long or short lag between the first and final series hit by a break. In an empirical application to international stock return predictability, the method generates significantly more accurate forecasts than several benchmarks that yield economically meaningful utility gains for a risk averse investor with power utility.},
  archive      = {J_JBES},
  author       = {Simon C. Smith},
  doi          = {10.1080/07350015.2024.2301969},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1223-1237},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Noncommon breaks},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model checking in partially linear spatial autoregressive
models. <em>JBES</em>, <em>42</em>(4), 1210–1222. (<a
href="https://doi.org/10.1080/07350015.2024.2301958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes two new classes of nonparametric tests for the correct specification of linear spatial autoregressive models based on the “integrated conditional moment” approach. Our test statistics are constructed as continuous functionals of a residual marked empirical process as well as its projected version. We derive asymptotic properties of the test statistics under the null hypothesis, the alternative hypothesis, and a sequence of local alternatives. The proposed tests do not involve the selection of tuning parameters such as bandwidths and are able to detect a broad class of local alternatives converging to the null at the parametric rate n − 1 / 2 , with n being the sample size. We also propose a multiplier bootstrap procedure that is computationally simple to approximate the critical values. Monte Carlo simulations illustrate that our tests have a reasonable size and satisfactory power for different types of data-generating processes. Finally, an empirical analysis of growth convergence is carried out to demonstrate the usefulness of the tests.},
  archive      = {J_JBES},
  author       = {Zixin Yang and Xiaojun Song and Jihai Yu},
  doi          = {10.1080/07350015.2024.2301958},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1210-1222},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Model checking in partially linear spatial autoregressive models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized spectral tests for multivariate martingale
difference hypotheses. <em>JBES</em>, <em>42</em>(4), 1195–1209. (<a
href="https://doi.org/10.1080/07350015.2024.2301954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes new generalized spectral tests for multivariate martingale difference hypotheses, specifically geared toward high-dimensionality scenarios where the dimension of the time series is comparable or even larger than the sample size in practice. We develop an asymptotic theory and a valid wild bootstrapping procedure for the new test statistics, in which the dimension of the time series is fixed. We demonstrate that a bias-reduced version of the test statistics effectively addresses the high-dimensionality concerns. Comprehensive Monte Carlo simulations reveal that the bias-reduced statistic performs substantially better than its competitors. The application to testing the efficient market hypothesis on the U.S. stock market illustrates the usefulness of our proposal.},
  archive      = {J_JBES},
  author       = {Xuexin Wang},
  doi          = {10.1080/07350015.2024.2301954},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1195-1209},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Generalized spectral tests for multivariate martingale difference hypotheses},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric identification and inference of first-price
auctions with heterogeneous bidders. <em>JBES</em>, <em>42</em>(4),
1185–1194. (<a
href="https://doi.org/10.1080/07350015.2023.2299432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the auction literature, it is well established that bidders’ asymmetry plays an important role in determining auction revenues. In this article, we propose a nonparametric methodology to analyze first-price auctions with two popularly adopted asymmetries: heterogeneous risk preferences and asymmetric value distributions. We find that the two competing models provide distinct implications for the bid distributions conditional on heterogeneity. By modeling bidders’ asymmetry as unobserved heterogeneity, we show that the conditional bid distributions are identified nonparametrically. These results enable researchers to test the two competing models. In an application using the US Forest Service timber auction data, we find that the data supports the model with heterogeneity in risk preference.},
  archive      = {J_JBES},
  author       = {Zheng Li},
  doi          = {10.1080/07350015.2023.2299432},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1185-1194},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Nonparametric identification and inference of first-price auctions with heterogeneous bidders},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oracle efficient estimation of heterogeneous dynamic panel
data models with interactive fixed effects. <em>JBES</em>,
<em>42</em>(4), 1169–1184. (<a
href="https://doi.org/10.1080/07350015.2023.2294124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a two-step procedure to estimate a heterogeneous dynamic panel data model with interactive fixed effects. We establish the asymptotic properties of the estimators and show that the final estimator is oracle efficient. We also propose a specification test for the null hypothesis of homogeneous slopes and study the asymptotic properties of the test statistic under both local and global alternatives. Simulations demonstrate the fine performance of the estimator and test statistic. The new estimation and inference methods are applied to study the heterogeneous effects of minimum wage on employment across different counties in the United States. Our dynamic model suggests that the changes of employment range from about–1% to 1% when the minimum wage increases by 1%.},
  archive      = {J_JBES},
  author       = {Yiqiu Cao and Sainan Jin and Xun Lu and Liangjun Su},
  doi          = {10.1080/07350015.2023.2294124},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1169-1184},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Oracle efficient estimation of heterogeneous dynamic panel data models with interactive fixed effects},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linking frequentist and bayesian change-point methods.
<em>JBES</em>, <em>42</em>(4), 1155–1168. (<a
href="https://doi.org/10.1080/07350015.2023.2293166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the two-stage minimum description length (MDL) criterion widely used to estimate linear change-point (CP) models corresponds to the marginal likelihood of a Bayesian model with a specific class of prior distributions. This allows results from the frequentist and Bayesian paradigms to be bridged together. Thanks to this link, one can rely on the consistency of the number and locations of the estimated CPs and the computational efficiency of frequentist methods, and obtain a probability of observing a CP at a given time, compute model posterior probabilities, and select or combine CP methods via Bayesian posteriors. Furthermore, we adapt several CP methods to take advantage of the MDL probabilistic representation. Based on simulated data, we show that the adapted CP methods can improve structural break detection compared to state-of-the-art approaches. Finally, we empirically illustrate the usefulness of combining CP detection methods when dealing with long time series and forecasting.},
  archive      = {J_JBES},
  author       = {David Ardia and Arnaud Dufays and Carlos Ordás Criado},
  doi          = {10.1080/07350015.2023.2293166},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1155-1168},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Linking frequentist and bayesian change-point methods},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unconditional quantile regression for streaming datasets.
<em>JBES</em>, <em>42</em>(4), 1143–1154. (<a
href="https://doi.org/10.1080/07350015.2023.2293162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Unconditional Quantile Regression (UQR) method, initially introduced by Firpo et al. has gained significant traction as a popular approach for modeling and analyzing data. However, much like Conditional Quantile Regression (CQR), UQR encounters computational challenges when it comes to obtaining parameter estimates for streaming datasets. This is attributed to the involvement of unknown parameters in the logistic regression loss function used in UQR, which presents obstacles in both computational execution and theoretical development. To address this, we present a novel approach involving smoothing logistic regression estimation. Subsequently, we propose a renewable estimator tailored for UQR with streaming data, relying exclusively on current data and summary statistics derived from historical data. Theoretically, our proposed estimators exhibit equivalent asymptotic properties to the standard version computed directly on the entire dataset, without any additional constraints. Both simulations and real data analysis are conducted to illustrate the finite sample performance of the proposed methods.},
  archive      = {J_JBES},
  author       = {Rong Jiang and Keming Yu},
  doi          = {10.1080/07350015.2023.2293162},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1143-1154},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Unconditional quantile regression for streaming datasets},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Imputation of counterfactual outcomes when the errors are
predictable: rejoinder. <em>JBES</em>, <em>42</em>(4), 1140–1142. (<a
href="https://doi.org/10.1080/07350015.2024.2393722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {Sílvia Gonçalves and Serena Ng},
  doi          = {10.1080/07350015.2024.2393722},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1140-1142},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Imputation of counterfactual outcomes when the errors are predictable: Rejoinder},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discussion of “imputation of counterfactual outcomes when
the errors are predictable” by sílvia gonçalves and serena ng.
<em>JBES</em>, <em>42</em>(4), 1137–1139. (<a
href="https://doi.org/10.1080/07350015.2024.2362922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I argue that the PUP approach proposed by Goncalves and Ng is not only useful for reducing the mean squared error of treatment effect estimators but can also be motivated from an identification perspective when units select into treatment based on past shocks.},
  archive      = {J_JBES},
  author       = {Kaspar Wüthrich},
  doi          = {10.1080/07350015.2024.2362922},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1137-1139},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Discussion of “Imputation of counterfactual outcomes when the errors are predictable” by sílvia gonçalves and serena ng},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On “imputation of counterfactual outcomes when the errors
are predictable”: Viewing the PUP as the DID and the LDV. <em>JBES</em>,
<em>42</em>(4), 1133–1136. (<a
href="https://doi.org/10.1080/07350015.2024.2351883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I discuss the practical unbiased predictor (PUP; Gonçalves and Ng) from the viewpoint of the literature on identification in event studies. The PUP can be seen as the prediction based on a generalized estimand that encompasses both the difference-in-differences (DID) and the lagged dependent variable (LDV). This feature of the PUP allows for a doubly robust property that the identification is achieved when either the parallel trend assumption or the LDV assumption holds at the expense of richer data. Furthermore, in this case, the bracketing property implies that the PUP identifying the true causal effect is bounded below by the LDV and above by the DID.},
  archive      = {J_JBES},
  author       = {Yuya Sasaki},
  doi          = {10.1080/07350015.2024.2351883},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1133-1136},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {On “Imputation of counterfactual outcomes when the errors are predictable”: Viewing the PUP as the DID and the LDV},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counterfactual imputation: Comments on “imputation of
counterfactual outcomes when the errors are predictable” by silvia
gonçalves and serena ng. <em>JBES</em>, <em>42</em>(4), 1128–1132. (<a
href="https://doi.org/10.1080/07350015.2024.2368805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The measurement of treatment (intervention) effects on a single (or just a few) treated unit(s) based on counterfactuals constructed from artificial controls has become a popular practice in applied statistics and economics since the proposal of the synthetic control method. However, most of the literature has ignored the time-series properties of the data. The work of Gonçalves and Ng fills this gap by proposing a simple correction for existing estimators to take into account serial and cross-correlation in the data. This note provides some thoughts on Gonçalves and Ng’s method.},
  archive      = {J_JBES},
  author       = {Marcelo C. Medeiros},
  doi          = {10.1080/07350015.2024.2368805},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1128-1132},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Counterfactual imputation: Comments on “Imputation of counterfactual outcomes when the errors are predictable” by silvia gonçalves and serena ng},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On “imputation of counterfactual outcomes when the errors
are predictable”: Discussions on misspecification and suggestions of
sensitivity analyses. <em>JBES</em>, <em>42</em>(4), 1123–1127. (<a
href="https://doi.org/10.1080/07350015.2024.2359594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gonçalves and Ng propose an interesting and simple way to improve counterfactual imputation methods when errors are predictable. For unconditional analyses, this approach yields smaller mean-squared error and tighter prediction intervals in large samples, even if the dependence of the errors is misspecified. For conditional analyses, this approach corrects the bias of standard methods, and provides valid asymptotic inference, if the dependence of the errors is correctly specified. In this comment, we first discuss how the assumptions imposed on the errors depend on the model and estimator adopted. This enables researchers to assess the validity of the assumptions imposed on the structure of the errors, and the relevant information set for conditional analyses. We then propose a simple sensitivity analysis in order to quantify the amount of misspecification on the dependence structure of the errors required for the conclusions of conditional analyses to be changed.},
  archive      = {J_JBES},
  author       = {Luis A. F. Alvarez and Bruno Ferman},
  doi          = {10.1080/07350015.2024.2359594},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1123-1127},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {On “Imputation of counterfactual outcomes when the errors are predictable”: Discussions on misspecification and suggestions of sensitivity analyses},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Imputation of counterfactual outcomes when the errors are
predictable. <em>JBES</em>, <em>42</em>(4), 1107–1122. (<a
href="https://doi.org/10.1080/07350015.2024.2358900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial input into causal inference is the imputed counterfactual outcome. Imputation error can arise because of sampling uncertainty from estimating the prediction model using the untreated observations, or from out-of-sample information not captured by the model. While the literature has focused on sampling uncertainty, it vanishes with the sample size. Often overlooked is the possibility that the out-of-sample error can be informative about the missing counterfactual outcome if it is mutually or serially correlated. Motivated by the best linear unbiased predictor (BLUP) of Goldberger in a time series setting, we propose an improved predictor of potential outcome when the errors are correlated. The proposed PUP is practical as it is not restricted to linear models, can be used with consistent estimators already developed, and improves mean-squared error for a large class of strong mixing error processes. Ignoring predictability in the errors can distort conditional inference. However, the precise impact will depend on the choice of estimator as well as the realized values of the residuals.},
  archive      = {J_JBES},
  author       = {Sílvia Gonçalves and Serena Ng},
  doi          = {10.1080/07350015.2024.2358900},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {1107-1122},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Imputation of counterfactual outcomes when the errors are predictable},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An econometric analysis of volatility discovery.
<em>JBES</em>, <em>42</em>(3), 1095–1106. (<a
href="https://doi.org/10.1080/07350015.2023.2292178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate information processing in the stochastic process driving stock’s volatility (volatility discovery). We apply fractionally cointegration techniques to decompose the estimates of the market-specific integrated variances into an estimate of the common integrated variance of the efficient price and a transitory component. The market weights on the common integrated variance of the efficient price are the volatility discovery measures. We relate the volatility discovery measure to the price discovery framework and formally show their roles on the identification of the integrated variance of the efficient price. We establish the limiting distribution of the volatility discovery measures by resorting to both long span and in-fill asymptotics. The empirical application is in line with our theoretical results, as it reveals that trading venues incorporate new information into the stochastic volatility process in an individual manner and that the volatility discovery analysis identifies a distinct information process than that based on the price discovery analysis.},
  archive      = {J_JBES},
  author       = {Gustavo Fruet Dias and Fotis Papailias and Cristina Scherrer},
  doi          = {10.1080/07350015.2023.2292178},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1095-1106},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An econometric analysis of volatility discovery},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ridge-regularized jackknifed anderson-rubin test.
<em>JBES</em>, <em>42</em>(3), 1083–1094. (<a
href="https://doi.org/10.1080/07350015.2023.2290739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider hypothesis testing in instrumental variable regression models with few included exogenous covariates but many instruments—possibly more than the number of observations. We show that a ridge-regularized version of the jackknifed Anderson and Rubin (henceforth AR) test controls asymptotic size in the presence of heteroscedasticity, and when the instruments may be arbitrarily weak. Asymptotic size control is established under weaker assumptions than those imposed for recently proposed jackknifed AR tests in the literature. Furthermore, ridge-regularization extends the scope of jackknifed AR tests to situations in which there are more instruments than observations. Monte Carlo simulations indicate that our method has favorable finite-sample size and power properties compared to recently proposed alternative approaches in the literature. An empirical application on the elasticity of substitution between immigrants and natives in the United States illustrates the usefulness of the proposed method for practitioners.},
  archive      = {J_JBES},
  author       = {Max-Sebastian Dovì and Anders Bredahl Kock and Sophocles Mavroeidis},
  doi          = {10.1080/07350015.2023.2290739},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1083-1094},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A ridge-regularized jackknifed anderson-rubin test},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational inference for large bayesian vector
autoregressions. <em>JBES</em>, <em>42</em>(3), 1066–1082. (<a
href="https://doi.org/10.1080/07350015.2023.2290716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel variational Bayes approach to estimate high-dimensional Vector Autoregressive (VAR) models with hierarchical shrinkage priors. Our approach does not rely on a conventional structural representation of the parameter space for posterior inference. Instead, we elicit hierarchical shrinkage priors directly on the matrix of regression coefficients so that (a) the prior structure maps into posterior inference on the reduced-form transition matrix and (b) posterior estimates are more robust to variables permutation. An extensive simulation study provides evidence that our approach compares favorably against existing linear and nonlinear Markov chain Monte Carlo and variational Bayes methods. We investigate the statistical and economic value of the forecasts from our variational inference approach for a mean-variance investor allocating her wealth to different industry portfolios. The results show that more accurate estimates translate into substantial out-of-sample gains across hierarchical shrinkage priors and model dimensions.},
  archive      = {J_JBES},
  author       = {Mauro Bernardi and Daniele Bianchi and Nicolas Bianco},
  doi          = {10.1080/07350015.2023.2290716},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1066-1082},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Variational inference for large bayesian vector autoregressions},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and robust estimation of the generalized LATE
model. <em>JBES</em>, <em>42</em>(3), 1053–1065. (<a
href="https://doi.org/10.1080/07350015.2023.2282497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the estimation of causal parameters in the generalized local average treatment effect (GLATE) model, which expands upon the traditional LATE model to include multivalued treatments. We derive the efficient influence function (EIF) and the semiparametric efficiency bound (SPEB) for two types of causal parameters: the local average structural function (LASF) and the local average structural function for the treated (LASFT). The moment conditions generated by the EIF satisfy two robustness properties: double robustness and Neyman orthogonality. Based on the robust moment conditions, we propose the double/debiased machine learning (DML) estimator for estimating the LASF. The DML estimator is well-suited for high dimensional settings. We also propose null-restricted inference methods that are robust against weak identification issues. As an empirical application of these methods, we examine the potential health outcome across different types of health insurance plans using data from the Oregon Health Insurance Experiment.},
  archive      = {J_JBES},
  author       = {Haitian Xie},
  doi          = {10.1080/07350015.2023.2282497},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1053-1065},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Efficient and robust estimation of the generalized LATE model},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical bayes approach to controlling the false
discovery exceedance. <em>JBES</em>, <em>42</em>(3), 1041–1052. (<a
href="https://doi.org/10.1080/07350015.2023.2277857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale multiple hypothesis testing problems, the false discovery exceedance (FDX) provides a desirable alternative to the widely used false discovery rate (FDR) when the false discovery proportion (FDP) is highly variable. We develop an empirical Bayes approach to control the FDX. We show that, for independent hypotheses from a two-group model and dependent hypotheses from a Gaussian model fulfilling the exchangeability condition, an oracle decision rule based on ranking and thresholding the local false discovery rate ( lfdr ) is optimal in the sense that the power is maximized subject to the FDX constraint. We propose a data-driven FDX procedure that uses carefully designed computational shortcuts to emulate the oracle rule. We investigate the empirical performance of the proposed method using both simulated and real data and study the merits of FDX control through an application for identifying abnormal stock trading strategies.},
  archive      = {J_JBES},
  author       = {Pallavi Basu and Luella Fu and Alessio Saretto and Wenguang Sun},
  doi          = {10.1080/07350015.2023.2277857},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1041-1052},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An empirical bayes approach to controlling the false discovery exceedance},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional-coefficient quantile regression for panel data
with latent group structure. <em>JBES</em>, <em>42</em>(3), 1026–1040.
(<a href="https://doi.org/10.1080/07350015.2023.2277172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers estimating functional-coefficient models in panel quantile regression with individual effects, allowing the cross-sectional and temporal dependence for large panel observations. A latent group structure is imposed on the heterogeneous quantile regression models so that the number of nonparametric functional coefficients to be estimated can be reduced considerably. With the preliminary local linear quantile estimates of the subject-specific functional coefficients, a classic agglomerative clustering algorithm is used to estimate the unknown group structure and an easy-to-implement ratio criterion is proposed to determine the group number. The estimated group number and structure are shown to be consistent. Furthermore, a post-grouping local linear smoothing method is introduced to estimate the group-specific functional coefficients, and the relevant asymptotic normal distribution theory is derived with a normalization rate comparable to that in the literature. The developed methodologies and theory are verified through a simulation study and showcased with an application to house price data from U.K. local authority districts, which reveals different homogeneity structures at different quantile levels.},
  archive      = {J_JBES},
  author       = {Xiaorong Yang and Jia Chen and Degui Li and Runze Li},
  doi          = {10.1080/07350015.2023.2277172},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1026-1040},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Functional-coefficient quantile regression for panel data with latent group structure},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and forecasting macroeconomic downside risk.
<em>JBES</em>, <em>42</em>(3), 1010–1025. (<a
href="https://doi.org/10.1080/07350015.2023.2277171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model permanent and transitory changes of the predictive density of U.S. GDP growth. A substantial increase in downside risk to U.S. economic growth emerges over the last 30 years, associated with the long-run growth slowdown started in the early 2000s. Conditional skewness moves procyclically, implying negatively skewed predictive densities ahead and during recessions, often anticipated by deteriorating financial conditions. Conversely, positively skewed distributions characterize expansions. The modeling framework ensures robustness to tail events, allows for both dense or sparse predictor designs, and delivers competitive out-of-sample (point, density and tail) forecasts, improving upon standard benchmarks.},
  archive      = {J_JBES},
  author       = {Davide Delle Monache and Andrea De Polis and Ivan Petrella},
  doi          = {10.1080/07350015.2023.2277171},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {1010-1025},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Modeling and forecasting macroeconomic downside risk},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal inference under outcome-based sampling with
monotonicity assumptions. <em>JBES</em>, <em>42</em>(3), 998–1009. (<a
href="https://doi.org/10.1080/07350015.2023.2277164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study causal inference under case-control and case-population sampling. Specifically, we focus on the binary-outcome and binary-treatment case, where the parameters of interest are causal relative and attributable risks defined via the potential outcome framework. It is shown that strong ignorability is not always as powerful as it is under random sampling and that certain monotonicity assumptions yield comparable results in terms of sharp identified intervals. Specifically, the usual odds ratio is shown to be a sharp identified upper bound on causal relative risk under the monotone treatment response and monotone treatment selection assumptions. We offer algorithms for inference on the causal parameters that are aggregated over the true population distribution of the covariates. We show the usefulness of our approach by studying three empirical examples: the benefit of attending private school for entering a prestigious university in Pakistan; the relationship between staying in school and getting involved with drug-trafficking gangs in Brazil; and the link between physicians’ hours and size of the group practice in the United States.},
  archive      = {J_JBES},
  author       = {Sung Jae Jun and Sokbae Lee},
  doi          = {10.1080/07350015.2023.2277164},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {998-1009},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Causal inference under outcome-based sampling with monotonicity assumptions},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDP solera: The ideal vintage mix. <em>JBES</em>,
<em>42</em>(3), 984–997. (<a
href="https://doi.org/10.1080/07350015.2023.2273622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use the information in the successive vintages of GDE and GDI to obtain an improved timely measure of U.S. aggregate output by exploiting cointegration between the different measures taking seriously their monthly release calendar. We also combine all existing overlapping comprehensive revisions to achieve further improvements. We pay particular attention to the Great Recession and the COVID-19 pandemic, which, despite producing dramatic fluctuations, did not generate noticeable revisions in previous growth rates. Our results suggest that revised GDE estimates, unlike GDI ones, are increasingly precise and receive higher weights, but early estimates retain some influence.},
  archive      = {J_JBES},
  author       = {Martín Almuzara and Dante Amengual and Gabriele Fiorentini and Enrique Sentana},
  doi          = {10.1080/07350015.2023.2273622},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {984-997},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {GDP solera: The ideal vintage mix},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap inference in cointegrating regressions:
Traditional and self-normalized test statistics. <em>JBES</em>,
<em>42</em>(3), 970–983. (<a
href="https://doi.org/10.1080/07350015.2023.2271538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tests of hypotheses on the cointegrating vector are well known to suffer from severe size distortions in finite samples, especially when the data are characterized by large levels of endogeneity or error serial correlation. To address this issue, we combine a vector autoregressive (VAR) sieve bootstrap to construct critical values with a self-normalization approach that avoids direct estimation of long-run variance parameters when computing test statistics. To asymptotically justify this method, we prove bootstrap consistency for the self-normalized test statistics under mild conditions. In addition, the underlying bootstrap invariance principle allows us to prove bootstrap consistency also for traditional test statistics based on popular modified OLS estimators. Simulation results show that using bootstrap critical values instead of asymptotic critical values reduces size distortions associated with traditional test statistics considerably, but combining the VAR sieve bootstrap with self-normalization can lead to even less size distorted tests at the cost of only small power losses. We illustrate the usefulness of the VAR sieve bootstrap in empirical applications by analyzing the validity of the Fisher effect in 19 OECD countries.},
  archive      = {J_JBES},
  author       = {Karsten Reichold and Carsten Jentsch},
  doi          = {10.1080/07350015.2023.2271538},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {970-983},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bootstrap inference in cointegrating regressions: Traditional and self-normalized test statistics},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Double machine learning for sample selection models.
<em>JBES</em>, <em>42</em>(3), 958–969. (<a
href="https://doi.org/10.1080/07350015.2023.2271071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the evaluation of discretely distributed treatments when outcomes are only observed for a subpopulation due to sample selection or outcome attrition. For identification, we combine a selection-on-observables assumption for treatment assignment with either selection-on-observables or instrumental variable assumptions concerning the outcome attrition/sample selection process. We also consider dynamic confounding, meaning that covariates that jointly affect sample selection and the outcome may (at least partly) be influenced by the treatment. To control in a data-driven way for a potentially high dimensional set of pre- and/or post-treatment covariates, we adapt the double machine learning framework for treatment evaluation to sample selection problems. We make use of (a) Neyman-orthogonal, doubly robust, and efficient score functions, which imply the robustness of treatment effect estimation to moderate regularization biases in the machine learning-based estimation of the outcome, treatment, or sample selection models and (b) sample splitting (or cross-fitting) to prevent overfitting bias. We demonstrate that the proposed estimators are asymptotically normal and root-n consistent and investigate their finite sample properties in a simulation study. We also apply our proposed methodology to the Job Corps data. The estimator is available in the causalweight package for the statistical software R.},
  archive      = {J_JBES},
  author       = {Michela Bia and Martin Huber and Lukáš Lafférs},
  doi          = {10.1080/07350015.2023.2271071},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {958-969},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Double machine learning for sample selection models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tests for jumps in yield spreads. <em>JBES</em>,
<em>42</em>(3), 946–957. (<a
href="https://doi.org/10.1080/07350015.2023.2271039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies high-frequency econometric methods to test for a jump in the spread of bond yields. We propose a coherent inference procedure that detects a jump in the yield spread only if at least one of the two underlying bonds displays a jump. Ignoring this inherent connection by basing inference only on a univariate jump test applied to the spread tends to overestimate the number of jumps in yield spreads and puts the coherence of test results at risk. We formalize the statistical approach in the context of an intersection union test in multiple testing. We document the relevance of coherent tests and their practicability via simulations and real data examples.},
  archive      = {J_JBES},
  author       = {Lars Winkelmann and Wenying Yao},
  doi          = {10.1080/07350015.2023.2271039},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {946-957},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Tests for jumps in yield spreads},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified randomization test for the level of clustering.
<em>JBES</em>, <em>42</em>(3), 933–945. (<a
href="https://doi.org/10.1080/07350015.2023.2261567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose a researcher observes individuals within a county within a state. Given concerns about correlation across individuals, it is common to group observations into clusters and conduct inference treating observations across clusters as independent. However, a researcher that has chosen to cluster at the county level may be unsure of their decision, given knowledge that observations are independent across states. This article proposes a modified randomization test as a robustness check for the chosen level of clustering in a linear regression setting. Existing tests require either the number of states or number of counties to be large. Our method is designed for settings with few states and few counties. While the method is conservative, it has competitive power in settings that may be relevant to empirical work.},
  archive      = {J_JBES},
  author       = {Yong Cai},
  doi          = {10.1080/07350015.2023.2261567},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {933-945},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A modified randomization test for the level of clustering},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reduced-rank envelope vector autoregressive model.
<em>JBES</em>, <em>42</em>(3), 918–932. (<a
href="https://doi.org/10.1080/07350015.2023.2260862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard vector autoregressive (VAR) models suffer from overparameterization which is a serious issue for high-dimensional time series data as it restricts the number of variables and lags that can be incorporated into the model. Several statistical methods, such as the reduced-rank model for multivariate (multiple) time series (Velu, Reinsel, and Wichern; Reinsel and Velu; Reinsel, Velu, and Chen) and the Envelope VAR model (Wang and Ding), provide solutions for achieving dimension reduction of the parameter space of the VAR model. However, these methods can be inefficient in extracting relevant information from complex data, as they fail to distinguish between relevant and irrelevant information, or they are inefficient in addressing the rank deficiency problem. We put together the idea of envelope models into the reduced-rank VAR model to simultaneously tackle these challenges, and propose a new parsimonious version of the classical VAR model called the reduced-rank envelope VAR (REVAR) model. Our proposed REVAR model incorporates the strengths of both reduced-rank VAR and envelope VAR models and leads to significant gains in efficiency and accuracy. The asymptotic properties of the proposed estimators are established under different error assumptions. Simulation studies and real data analysis are conducted to evaluate and illustrate the proposed method.},
  archive      = {J_JBES},
  author       = {S. Yaser Samadi and H. M. Wiranthe B. Herath},
  doi          = {10.1080/07350015.2023.2260862},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {918-932},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Reduced-rank envelope vector autoregressive model},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling extreme events: Time-varying extreme tail shape.
<em>JBES</em>, <em>42</em>(3), 903–917. (<a
href="https://doi.org/10.1080/07350015.2023.2260439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a dynamic semiparametric framework to study time variation in tail parameters. The framework builds on the Generalized Pareto Distribution (GPD) for modeling peaks over thresholds as in Extreme Value Theory, but casts the model in a conditional framework to allow for time-variation in the tail parameters. We establish parameter regions for stationarity and ergodicity and for the existence of (unconditional) moments and consider conditions for consistency and asymptotic normality of the maximum likelihood estimator for the deterministic parameters in the model. Two empirical datasets illustrate the usefulness of the approach: daily U.S. equity returns, and 15-min euro area sovereign bond yield changes.},
  archive      = {J_JBES},
  author       = {Enzo D’Innocenzo and André Lucas and Bernd Schwaab and Xin Zhang},
  doi          = {10.1080/07350015.2023.2260439},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {903-917},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Modeling extreme events: Time-varying extreme tail shape},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FNETS: Factor-adjusted network estimation and forecasting
for high-dimensional time series. <em>JBES</em>, <em>42</em>(3),
890–902. (<a
href="https://doi.org/10.1080/07350015.2023.2257270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose FNETS, a methodology for network estimation and forecasting of high-dimensional time series exhibiting strong serial- and cross-sectional correlations. We operate under a factor-adjusted vector autoregressive (VAR) model which, after accounting for pervasive co-movements of the variables by common factors, models the remaining idiosyncratic dynamic dependence between the variables as a sparse VAR process. Network estimation of FNETS consists of three steps: (i) factor-adjustment via dynamic principal component analysis, (ii) estimation of the latent VAR process via l 1 -regularized Yule-Walker estimator, and (iii) estimation of partial correlation and long-run partial correlation matrices. In doing so, we learn three networks underpinning the VAR process, namely a directed network representing the Granger causal linkages between the variables, an undirected one embedding their contemporaneous relationships and finally, an undirected network that summarizes both lead-lag and contemporaneous linkages. In addition, FNETS provides a suite of methods for forecasting the factor-driven and the idiosyncratic VAR processes. Under general conditions permitting tails heavier than the Gaussian one, we derive uniform consistency rates for the estimators in both network estimation and forecasting, which hold as the dimension of the panel and the sample size diverge. Simulation studies and real data application confirm the good performance of FNETS.},
  archive      = {J_JBES},
  author       = {Matteo Barigozzi and Haeran Cho and Dom Owens},
  doi          = {10.1080/07350015.2023.2257270},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {890-902},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {FNETS: Factor-adjusted network estimation and forecasting for high-dimensional time series},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the combination of naive and mean-variance portfolio
strategies. <em>JBES</em>, <em>42</em>(3), 875–889. (<a
href="https://doi.org/10.1080/07350015.2023.2256801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how to best combine the sample mean-variance portfolio with the naive equally weighted portfolio to optimize out-of-sample performance. We show that the seemingly natural convexity constraint—the two combination coefficients must sum to one—is undesirable because it severely constrains the allocation to the risk-free asset relative to the unconstrained portfolio combination. However, we demonstrate that relaxing the convexity constraint inflates estimation errors in combination coefficients, which we alleviate using a shrinkage estimator of the unconstrained combination scheme. Empirically, the constrained combination outperforms the unconstrained one in a range of generally small degrees of risk aversion, but severely deteriorates otherwise. In contrast, the shrinkage unconstrained combination enjoys the best of both strategies and performs consistently well for all levels of risk aversion.},
  archive      = {J_JBES},
  author       = {Nathan Lassance and Rodolphe Vanderveken and Frédéric Vrins},
  doi          = {10.1080/07350015.2023.2256801},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {875-889},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {On the combination of naive and mean-variance portfolio strategies},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Powerful backtests for historical simulation expected
shortfall models. <em>JBES</em>, <em>42</em>(3), 864–874. (<a
href="https://doi.org/10.1080/07350015.2023.2252881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2016, the Basel Committee on Banking Supervision has regulated banks to switch from a Value-at-Risk (VaR) to an Expected Shortfall (ES) approach to measuring the market risk and calculating the capital requirement. In the transition from VaR to ES, the major challenge faced by financial institutions is the lack of simple but powerful tools for evaluating ES forecasts (i.e., backtesting ES). This article first shows that the unconditional backtest is inconsistent in evaluating the most popular Historical Simulation (HS) and Filtered Historical Simulation (FHS) E S models, with power even less than the nominal level in large samples. To overcome this problem, we propose a new class of conditional backtests for E S that are powerful against a large class of alternatives. We establish the asymptotic properties of the tests, and investigate their finite sample performance through some Monte Carlo simulations. An empirical application to stock indices data highlights the merits of our method.},
  archive      = {J_JBES},
  author       = {Zaichao Du and Pei Pei and Xuhui Wang and Tao Yang},
  doi          = {10.1080/07350015.2023.2252881},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {864-874},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Powerful backtests for historical simulation expected shortfall models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correcting for endogeneity in models with bunching.
<em>JBES</em>, <em>42</em>(3), 851–863. (<a
href="https://doi.org/10.1080/07350015.2023.2252471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel control function approach in models where the treatment variable has bunching at one corner of its support. This situation typically arises when the treatment variable is a constrained choice and some observations choose the corner solution. The method exploits distributional shape restrictions but makes no exclusion restrictions. We provide estimators and establish their asymptotic behavior, prove the convergence of the bootstrap, and develop tests of the identification assumptions. An application reveals that watching television has no effect on cognitive skills and a negative effect on noncognitive skills in children.},
  archive      = {J_JBES},
  author       = {Carolina Caetano and Gregorio Caetano and Eric Nielsen},
  doi          = {10.1080/07350015.2023.2252471},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {851-863},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Correcting for endogeneity in models with bunching},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference on consensus ranking of distributions.
<em>JBES</em>, <em>42</em>(3), 839–850. (<a
href="https://doi.org/10.1080/07350015.2023.2252040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instead of testing for unanimous agreement, I propose learning how broad of a consensus favors one distribution over another (of earnings, productivity, asset returns, test scores, etc.). Specifically, given a sample from each of two distributions, I propose statistical inference methods to learn about the set of utility functions for which the first distribution has higher expected utility than the second distribution. With high probability, an “inner” confidence set is contained within this true set, while an “outer” confidence set contains the true set. Such confidence sets can be formed by inverting a proposed multiple testing procedure that controls the familywise error rate. Theoretical justification comes from empirical process results, given that very large classes of utility functions are generally Donsker (subject to finite moments). The theory additionally justifies a uniform (over utility functions) confidence band of expected utility differences, as well as tests with a utility-based “restricted stochastic dominance” as either the null or alternative hypothesis. Simulated and empirical examples illustrate the methodology.},
  archive      = {J_JBES},
  author       = {David M. Kaplan},
  doi          = {10.1080/07350015.2023.2252040},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {839-850},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Inference on consensus ranking of distributions},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large order-invariant bayesian VARs with stochastic
volatility. <em>JBES</em>, <em>42</em>(2), 825–837. (<a
href="https://doi.org/10.1080/07350015.2023.2252039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many popular specifications for Vector Autoregressions (VARs) with multivariate stochastic volatility are not invariant to the way the variables are ordered due to the use of a lower triangular parameterization of the error covariance matrix. We show that the order invariance problem in existing approaches is likely to become more serious in large VARs. We propose the use of a specification which avoids the use of this lower triangular parameterization. We show that the presence of multivariate stochastic volatility allows for identification of the proposed model and prove that it is invariant to ordering. We develop a Markov chain Monte Carlo algorithm which allows for Bayesian estimation and prediction. In exercises involving artificial and real macroeconomic data, we demonstrate that the choice of variable ordering can have non-negligible effects on empirical results when using the nonorder invariant approach. In a macroeconomic forecasting exercise involving VARs with 20 variables we find that our order-invariant approach leads to the best forecasts and that some choices of variable ordering can lead to poor forecasts using a conventional, non-order invariant, approach.},
  archive      = {J_JBES},
  author       = {Joshua C. C. Chan and Gary Koop and Xuewen Yu},
  doi          = {10.1080/07350015.2023.2252039},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {825-837},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Large order-invariant bayesian VARs with stochastic volatility},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extreme changes in changes. <em>JBES</em>, <em>42</em>(2),
812–824. (<a
href="https://doi.org/10.1080/07350015.2023.2249509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy analysts are often interested in treating the units with extreme outcomes, such as infants with extremely low birth weights. Existing changes-in-changes (CIC) estimators are tailored to middle quantiles and do not work well for such subpopulations. This article proposes a new CIC estimator to accurately estimate treatment effects at extreme quantiles. With its asymptotic normality, we also propose a method of statistical inference, which is simple to implement. Based on simulation studies, we propose to use our extreme CIC estimator for extreme quantiles, while the conventional CIC estimator should be used for intermediate quantiles. Applying the proposed method, we study the effects of income gains from the 1993 EITC reform on infant birth weights for those in the most critical conditions. This article is accompanied by a Stata command.},
  archive      = {J_JBES},
  author       = {Yuya Sasaki and Yulong Wang},
  doi          = {10.1080/07350015.2023.2249509},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {812-824},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Extreme changes in changes},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalizing the results from social experiments: Theory and
evidence from india. <em>JBES</em>, <em>42</em>(2), 801–811. (<a
href="https://doi.org/10.1080/07350015.2023.2241529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How informative are treatment effects estimated in one region or time period for another region or time? In this article, I derive bounds on the average treatment effect in a context of interest using experimental evidence from another context. The bounds are based on (a) the information identified about treatment effect heterogeneity due to unobservables in the experiment and (b) using differences in outcome distributions across contexts to learn about differences in distributions of unobservables. Empirically, using data from a pair of remedial education experiments carried out in India, I show the bounds are able to recover average treatment effects in one location using results from the other while the benchmark method cannot.},
  archive      = {J_JBES},
  author       = {Michael Gechter},
  doi          = {10.1080/07350015.2023.2241529},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {801-811},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Generalizing the results from social experiments: Theory and evidence from india},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized autoregressive positive-valued processes.
<em>JBES</em>, <em>42</em>(2), 786–800. (<a
href="https://doi.org/10.1080/07350015.2023.2239869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce generalized autoregressive positive-valued (GARP) processes, a class of autoregressive and moving-average processes that extends the class of existing autoregressive positive-valued (ARP) processes in one important dimension: each conditional moment dynamic is driven by a different and identifiable moving average of the variable of interest. The article provides ergodicity conditions for GARP processes and derives closed-form conditional and unconditional moments. The article also presents estimation and inference methods, illustrated by an application to European option pricing where the daily realized variance follows a GARP dynamic. Our results show that using GARP processes reduces pricing errors by substantially more than using ARP processes.},
  archive      = {J_JBES},
  author       = {Bruno Feunou},
  doi          = {10.1080/07350015.2023.2239869},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {786-800},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Generalized autoregressive positive-valued processes},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic autoregressive liquidity (DArLiQ). <em>JBES</em>,
<em>42</em>(2), 774–785. (<a
href="https://doi.org/10.1080/07350015.2023.2238790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of semiparametric dynamic autoregressive models for the Amihud illiquidity measure, which captures both the long-run trend in the illiquidity series with a nonparametric component and the short-run dynamics with an autoregressive component. We develop a generalized method of moments (GMM) estimator based on conditional moment restrictions and an efficient semiparametric maximum likelihood (ML) estimator based on an iid assumption. We derive large sample properties for our estimators. Finally, we demonstrate the model fitting performance and its empirical relevance on an application. We investigate how the different components of the illiquidity process obtained from our model relate to the stock market risk premium using data on the S&amp;P 500 stock market index.},
  archive      = {J_JBES},
  author       = {Christian M. Hafner and Oliver B. Linton and Linqi Wang},
  doi          = {10.1080/07350015.2023.2238790},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {774-785},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Dynamic autoregressive liquidity (DArLiQ)},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A design-based perspective on synthetic control methods.
<em>JBES</em>, <em>42</em>(2), 762–773. (<a
href="https://doi.org/10.1080/07350015.2023.2238788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since their introduction by Abadie and Gardeazabal, Synthetic Control (SC) methods have quickly become one of the leading methods for estimating causal effects in observational studies in settings with panel data. Formal discussions often motivate SC methods by the assumption that the potential outcomes were generated by a factor model. Here we study SC methods from a design-based perspective, assuming a model for the selection of the treated unit(s) and period(s). We show that the standard SC estimator is generally biased under random assignment. We propose a Modified Unbiased Synthetic Control (MUSC) estimator that guarantees unbiasedness under random assignment and derive its exact, randomization-based, finite-sample variance. We also propose an unbiased estimator for this variance. We document in settings with real data that under random assignment, SC-type estimators can have root mean-squared errors that are substantially lower than that of other common estimators. We show that such an improvement is weakly guaranteed if the treated period is similar to the other periods, for example, if the treated period was randomly selected. While our results only directly apply in settings where treatment is assigned randomly, we believe that they can complement model-based approaches even for observational studies.},
  archive      = {J_JBES},
  author       = {Lea Bottmer and Guido W. Imbens and Jann Spiess and Merrill Warnick},
  doi          = {10.1080/07350015.2023.2238788},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {762-773},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A design-based perspective on synthetic control methods},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An LM test for the conditional independence between
regressors and factor loadings in panel data models with interactive
effects. <em>JBES</em>, <em>42</em>(2), 743–761. (<a
href="https://doi.org/10.1080/07350015.2023.2238774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A huge literature on modeling cross-sectional dependence in panels has been developed using interactive effects (IE). One area of contention is the hypothesis concerned with whether the regressors and factor loadings are correlated or not. Under the null hypothesis that they are conditionally independent, we can still apply the consistent and robust two-way fixed effects estimator. As an important specification test we develop an LM test for both static and dynamic panels with IE. Simulation results confirm the satisfactory performance of the LM test in small samples. We demonstrate its usefulness with an application to a total of 22 datasets, including static panels with a small T and dynamic panels with serially correlated factors, providing convincing evidence that the null hypothesis is not rejected in},
  archive      = {J_JBES},
  author       = {George Kapetanios and Laura Serlenga and Yongcheol Shin},
  doi          = {10.1080/07350015.2023.2238774},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {743-761},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {An LM test for the conditional independence between regressors and factor loadings in panel data models with interactive effects},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instrumental variable estimation of dynamic treatment
effects on a duration outcome. <em>JBES</em>, <em>42</em>(2), 732–742.
(<a href="https://doi.org/10.1080/07350015.2023.2231053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers identification and estimation of the causal effect of the time Z until a subject is treated on a duration T . The time-to-treatment is not randomly assigned, T is randomly right censored by a random variable C , and the time-to-treatment Z is right censored by T ∧ C . The endogeneity issue is treated using an instrumental variable explaining Z and independent of the error term of the model. We study identification in a fully nonparametric framework. We show that our specification generates an integral equation, of which the regression function of interest is a solution. We provide identification conditions that rely on this identification equation. We assume that the regression function follows a parametric model for estimation purposes. We propose an estimation procedure and give conditions under which the estimator is asymptotically normal. The estimators exhibit good finite sample properties in simulations. Our methodology is applied to evaluate the effect of the timing of a therapy for burnout.},
  archive      = {J_JBES},
  author       = {Jad Beyhum and Samuele Centorrino and Jean-Pierre Florens and Ingrid Van Keilegom},
  doi          = {10.1080/07350015.2023.2231053},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {732-742},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Instrumental variable estimation of dynamic treatment effects on a duration outcome},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework for constructing locally self-normalized
multiple-change-point tests. <em>JBES</em>, <em>42</em>(2), 719–731. (<a
href="https://doi.org/10.1080/07350015.2023.2231041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general framework to construct self-normalized multiple-change-point tests with time series data. The only building block is a user-specified single-change-detecting statistic, which covers a large class of popular methods, including the cumulative sum process, outlier-robust rank statistics, and order statistics. The proposed test statistic does not require robust and consistent estimation of nuisance parameters, selection of bandwidth parameters, nor pre-specification of the number of change points. The finite-sample performance shows that the proposed test is size-accurate, robust against misspecification of the alternative hypothesis, and more powerful than existing methods. Case studies of the Shanghai-Hong Kong Stock Connect turnover are provided.},
  archive      = {J_JBES},
  author       = {Cheuk Hin Cheng and Kin Wai Chan},
  doi          = {10.1080/07350015.2023.2231041},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {719-731},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A general framework for constructing locally self-normalized multiple-change-point tests},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-assisted complier average treatment effect estimates
in randomized experiments with noncompliance. <em>JBES</em>,
<em>42</em>(2), 707–718. (<a
href="https://doi.org/10.1080/07350015.2023.2224851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noncompliance is a common problem in randomized experiments in various fields. Under certain assumptions, the complier average treatment effect is identifiable and equal to the ratio of the intention-to-treat effects of the potential outcomes to that of the treatment received. To improve the estimation efficiency, we propose three model-assisted estimators for the complier average treatment effect in randomized experiments with a binary outcome. We study their asymptotic properties, compare their efficiencies with that of the Wald estimator, and propose the Neyman-type conservative variance estimators to facilitate valid inferences. Moreover, we extend our methods and theory to estimate the multiplicative complier average treatment effect. Our analysis is randomization-based, allowing the working models to be misspecified. Finally, we conduct simulation studies to illustrate the advantages of the model-assisted methods and apply these analysis methods in a randomized experiment to evaluate the effect of academic services or incentives on academic performance.},
  archive      = {J_JBES},
  author       = {Jiyang Ren},
  doi          = {10.1080/07350015.2023.2224851},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {707-718},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Model-assisted complier average treatment effect estimates in randomized experiments with noncompliance},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent estimation of multiple breakpoints in dependence
measures. <em>JBES</em>, <em>42</em>(2), 695–706. (<a
href="https://doi.org/10.1080/07350015.2023.2224850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes different methods to consistently detect multiple breaks in copula-based dependence measures. Starting with the classical binary segmentation, also the more recent wild binary segmentation (WBS) is considered. For binary segmentation, consistency of the estimators for the location of the breakpoints as well as the number of breaks is proved, taking filtering effects from AR-GARCH models explicitly into account. Monte Carlo simulations based on a factor copula as well as on a Clayton copula model illustrate the strengths and limitations of the procedures. A real data application on recent Euro Stoxx 50 data reveals some interpretable breaks in the dependence structure.},
  archive      = {J_JBES},
  author       = {Marvin Borsch and Alexander Mayer and Dominik Wied},
  doi          = {10.1080/07350015.2023.2224850},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {695-706},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Consistent estimation of multiple breakpoints in dependence measures},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asset pricing via the conditional quantile variational
autoencoder. <em>JBES</em>, <em>42</em>(2), 681–694. (<a
href="https://doi.org/10.1080/07350015.2023.2223683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new asset pricing model that is applicable to the big panel of return data. The main idea of this model is to learn the conditional distribution of the return, which is approximated by a step distribution function constructed from conditional quantiles of the return. To study conditional quantiles of the return, we propose a new conditional quantile variational autoencoder (CQVAE) network. The CQVAE network specifies a factor structure for conditional quantiles with latent factors learned from a VAE network and nonlinear factor loadings learned from a “multi-head” network. Under the CQVAE network, we allow the observed covariates such as asset characteristics to guide the structure of latent factors and factor loadings. Furthermore, we provide a two-step estimation procedure for the CQVAE network. Using the learned conditional distribution of return from the CQVAE network, we propose our asset pricing model from the mean of this distribution, and additionally, we use both the mean and variance of this distribution to select portfolios. Finally, we apply our CQVAE asset pricing model to analyze a large 60-year US equity return dataset. Compared with the benchmark conditional autoencoder model, the CQVAE model not only delivers much larger values of out-of-sample total and predictive R 2 ’s, but also earns at least 30.9% higher values of Sharpe ratios for both long-short and long-only portfolios.},
  archive      = {J_JBES},
  author       = {Xuanling Yang and Zhoufan Zhu and Dong Li and Ke Zhu},
  doi          = {10.1080/07350015.2023.2223683},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {681-694},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Asset pricing via the conditional quantile variational autoencoder},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple correction for misspecification in trend-cycle
decompositions with an application to estimating r*. <em>JBES</em>,
<em>42</em>(2), 665–680. (<a
href="https://doi.org/10.1080/07350015.2023.2221974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple correction for misspecification in trend-cycle decompositions when the stochastic trend is assumed to be a random walk process but the estimated trend displays some serial correlation in first differences. Possible sources of misspecification that would otherwise be hard to detect and correct for include a small amount of measurement error, omitted variables, or minor approximation errors in model dynamics when estimating trend. Our proposed correction is conducted via application of a univariate Beveridge-Nelson decomposition to the preliminary estimated trend and we show with Monte Carlo analysis that our approach can work as well as if the original model used to estimate trend were correctly specified. We demonstrate the empirical relevance of the correction in an application to estimating r * as the trend of a risk-free short-term real interest rate. We find that our corrected estimate of r * is considerably smoother than the preliminary estimate from a multivariate Beveridge-Nelson decomposition based on a vector error correction model, consistent with the presence of at least a small amount of measurement error in some of the variables included in the multivariate model.},
  archive      = {J_JBES},
  author       = {James Morley and Trung Duc Tran and Benjamin Wong},
  doi          = {10.1080/07350015.2023.2221974},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {665-680},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A simple correction for misspecification in trend-cycle decompositions with an application to estimating r*},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uniform nonparametric inference for spatially dependent
panel data. <em>JBES</em>, <em>42</em>(2), 654–664. (<a
href="https://doi.org/10.1080/07350015.2023.2219283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a uniform functional inference method for nonparametric regressions in a panel-data setting that features general unknown forms of spatio-temporal dependence. The method requires a long time span, but does not impose any restriction on the size of the cross section or the strength of spatial correlation. The uniform inference is justified via a new growing-dimensional Gaussian coupling theory for spatio-temporally dependent panels. We apply the method in two empirical settings. One concerns the nonparametric relationship between asset price volatility and trading volume as depicted by the mixture of distribution hypothesis. The other pertains to testing the rationality of survey-based forecasts, in which we document nonparametric evidence for information rigidity among professional forecasters, offering new support for sticky-information and noisy-information models in macroeconomics.},
  archive      = {J_JBES},
  author       = {Jia Li and Zhipeng Liao and Wenyu Zhou},
  doi          = {10.1080/07350015.2023.2219283},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {654-664},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Uniform nonparametric inference for spatially dependent panel data},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive testing for alphas in high-dimensional factor
pricing models. <em>JBES</em>, <em>42</em>(2), 640–653. (<a
href="https://doi.org/10.1080/07350015.2023.2217871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new procedure to validate the multi-factor pricing theory by testing the presence of alpha in linear factor pricing models with a large number of assets. Because the market’s inefficient pricing is likely to occur to a small fraction of exceptional assets, we develop a testing procedure that is particularly powerful against sparse signals. Based on the high-dimensional Gaussian approximation theory, we propose a simulation-based approach to approximate the limiting null distribution of the test. Our numerical studies show that the new procedure can deliver a reasonable size and achieve substantial power improvement compared to the existing tests under sparse alternatives, and especially for weak signals.},
  archive      = {J_JBES},
  author       = {Qiang Xia and Xianyang Zhang},
  doi          = {10.1080/07350015.2023.2217871},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {640-653},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Adaptive testing for alphas in high-dimensional factor pricing models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap inference for panel data quantile regression.
<em>JBES</em>, <em>42</em>(2), 628–639. (<a
href="https://doi.org/10.1080/07350015.2023.2210189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops bootstrap methods for practical statistical inference in panel data quantile regression models with fixed effects. We consider random-weighted bootstrap resampling and formally establish its validity for asymptotic inference. The bootstrap algorithm is simple to implement in practice by using a weighted quantile regression estimation for fixed effects panel data. We provide results under conditions that allow for temporal dependence of observations within individuals, thus, encompassing a large class of possible empirical applications. Monte Carlo simulations provide numerical evidence the proposed bootstrap methods have correct finite sample properties. Finally, we provide an empirical illustration using the environmental Kuznets curve.},
  archive      = {J_JBES},
  author       = {Antonio F. Galvao and Thomas Parker and Zhijie Xiao},
  doi          = {10.1080/07350015.2023.2210189},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {628-639},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bootstrap inference for panel data quantile regression},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tie-break bootstrap for nonparametric rank statistics.
<em>JBES</em>, <em>42</em>(2), 615–627. (<a
href="https://doi.org/10.1080/07350015.2023.2210181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new bootstrap procedure for the empirical copula process. The procedure involves taking pseudo samples of normalized ranks in the same fashion as the classical bootstrap and applying small perturbations to break ties in the normalized ranks. Our procedure is a simple modification of the usual bootstrap based on sampling with replacement, yet it provides noticeable improvement in the finite sample performance. We also discuss how to incorporate our procedure into the time series framework. Since nonparametric rank statistics can be treated as functionals of the empirical copula, our proposal is useful in approximating the distribution of rank statistics in general. As an empirical illustration, we apply our bootstrap procedure to test the null hypotheses of positive quadrant dependence, tail monotonicity, and stochastic monotonicity, using U.S. Census data on spousal incomes in the past 15 years.},
  archive      = {J_JBES},
  author       = {Juwon Seo},
  doi          = {10.1080/07350015.2023.2210181},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {615-627},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Tie-break bootstrap for nonparametric rank statistics},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks for partially linear quantile regression.
<em>JBES</em>, <em>42</em>(2), 603–614. (<a
href="https://doi.org/10.1080/07350015.2023.2208183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has enjoyed tremendous success in a variety of applications but its application to quantile regression remains scarce. A major advantage of the deep learning approach is its flexibility to model complex data in a more parsimonious way than nonparametric smoothing methods. However, while deep learning brought breakthroughs in prediction, it is not well suited for statistical inference due to its black box nature. In this article, we leverage the advantages of deep learning and apply it to quantile regression where the goal is to produce interpretable results and perform statistical inference. We achieve this by adopting a semiparametric approach based on the partially linear quantile regression model, where covariates of primary interest for statistical inference are modeled linearly and all other covariates are modeled nonparametrically by means of a deep neural network. In addition to the new methodology, we provide theoretical justification for the proposed model by establishing the root- n consistency and asymptotically normality of the parametric coefficient estimator and the minimax optimal convergence rate of the neural nonparametric function estimator. Across several simulated and real data examples, the proposed model empirically produces superior estimates and more accurate predictions than various alternative approaches.},
  archive      = {J_JBES},
  author       = {Qixian Zhong and Jane-Ling Wang},
  doi          = {10.1080/07350015.2023.2208183},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {603-614},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Neural networks for partially linear quantile regression},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating a continuous treatment model with spillovers: A
control function approach. <em>JBES</em>, <em>42</em>(2), 591–602. (<a
href="https://doi.org/10.1080/07350015.2023.2207617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a continuous treatment effect model in the presence of treatment spillovers through social networks. We assume that one’s outcome is affected not only by his/her own treatment but also by a (weighted) average of his/her neighbors’ treatments, both of which are treated as endogenous variables. Using a control function approach with appropriate instrumental variables, we show that the conditional mean potential outcome can be nonparametrically identified. We also consider a more empirically tractable semiparametric model and develop a three-step estimation procedure for this model. As an empirical illustration, we investigate the causal effect of the regional unemployment rate on the crime rate.},
  archive      = {J_JBES},
  author       = {Tadao Hoshino},
  doi          = {10.1080/07350015.2023.2207617},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {591-602},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimating a continuous treatment model with spillovers: A control function approach},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Links and legibility: Making sense of historical u.s. Census
automated linking methods. <em>JBES</em>, <em>42</em>(2), 579–590. (<a
href="https://doi.org/10.1080/07350015.2023.2205918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How does handwriting legibility affect the performance of algorithms that link individuals across census rounds? We propose a measure of legibility, which we implement at scale for the 1940 U.S. Census, and find strikingly wide variation in enumeration-district-level legibility. Using boundary discontinuities in enumeration districts, we estimate the causal effect of low legibility on the quality of linked samples, measured by linkage rates and share of validated links. Our estimates imply that, across eight linking algorithms, perfect legibility would increase the linkage rate by 5–10 percentage points. Improvements in transcription could substantially increase the quality of linked samples.},
  archive      = {J_JBES},
  author       = {Arkadev Ghosh and Sam Il Myoung Hwang and Munir Squires},
  doi          = {10.1080/07350015.2023.2205918},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {579-590},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Links and legibility: Making sense of historical U.S. census automated linking methods},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple inference on functionals of set-identified parameters
defined by linear moments. <em>JBES</em>, <em>42</em>(2), 563–578. (<a
href="https://doi.org/10.1080/07350015.2023.2203768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new approach to obtain uniformly valid inference for linear functionals or scalar subvectors of a partially identified parameter defined by linear moment inequalities. The procedure amounts to bootstrapping the value functions of randomly perturbed linear programming problems, and does not require the researcher to grid over the parameter space. The low-level conditions for uniform validity rely on genericity results for linear programs. The unconventional perturbation approach produces a confidence set with a coverage probability of 1 over the identified set, but obtains exact coverage on an outer set, is valid under weak assumptions, and is computationally simple to implement.},
  archive      = {J_JBES},
  author       = {JoonHwan Cho and Thomas M. Russell},
  doi          = {10.1080/07350015.2023.2203768},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {563-578},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Simple inference on functionals of set-identified parameters defined by linear moments},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The leverage effect puzzle under semi-nonparametric
stochastic volatility models. <em>JBES</em>, <em>42</em>(2), 548–562.
(<a href="https://doi.org/10.1080/07350015.2023.2203756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends the solution proposed by Aït-Sahalia, Fan, and Li for the leverage effect puzzle, which refers to a fact that empirical correlation between daily asset returns and the changes of daily volatility estimated from high frequency data is nearly zero. Complementing the analysis in Aït-Sahalia, Fan, and Li via the Heston model, we work with a generic semi-nonparametric stochastic volatility model via an operator-based expansion method. Under such a general setup, we identify a new source of bias due to the flexibility of variance dynamics, distinguishing the leverage effect parameter from the instantaneous correlation parameter. For estimating the leverage effect parameter, we show that the main results on analyzing the various sources of biases as well as the resulting statistical procedures for biases correction in Aït-Sahalia, Fan, and Li hold true and are thus indeed theoretically robust. For estimating the instantaneous correlation parameter, we developed a new nonparametric estimation method.},
  archive      = {J_JBES},
  author       = {Dachuan Chen and Chenxu Li and Cheng Yong Tang and Jun Yan},
  doi          = {10.1080/07350015.2023.2203756},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {548-562},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The leverage effect puzzle under semi-nonparametric stochastic volatility models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation and inference on time-varying FAVAR models.
<em>JBES</em>, <em>42</em>(2), 533–547. (<a
href="https://doi.org/10.1080/07350015.2023.2203726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a time-varying (TV) factor-augmented vector autoregressive (FAVAR) model to capture the TV behavior in the factor loadings and the VAR coefficients. To consistently estimate the TV parameters, we first obtain the unobserved common factors via the local principal component analysis (PCA) and then estimate the TV-FAVAR model via a local smoothing approach. The limiting distribution of the proposed estimators is established. To gauge possible sources of TV features in the FAVAR model, we propose three L 2 -distance-based test statistics and study their asymptotic properties under the null and local alternatives. Simulation studies demonstrate the excellent finite sample performance of the proposed estimators and tests. In an empirical application to the U.S. macroeconomic dataset, we document overwhelming evidence of structural changes in the FAVAR model and show that the TV-FAVAR model outperforms the conventional time-invariant FAVAR model in predicting certain key macroeconomic series.},
  archive      = {J_JBES},
  author       = {Zhonghao Fu and Liangjun Su and Xia Wang},
  doi          = {10.1080/07350015.2023.2203726},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {533-547},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimation and inference on time-varying FAVAR models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jumps or staleness? <em>JBES</em>, <em>42</em>(2), 516–532.
(<a href="https://doi.org/10.1080/07350015.2023.2203207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even moderate amounts of zero returns in financial data, associated with stale prices, are heavily detrimental for reliable jump inference. We harness staleness-robust estimators to reappraise the statistical features of jumps in financial markets. We find that jumps are much less frequent and much less contributing to price variation than what found by the empirical literature so far. In particular, the empirical finding that volatility is driven by a pure jump process is actually shown to be an artifact due to staleness.},
  archive      = {J_JBES},
  author       = {Aleksey Kolokolov and Roberto Renò},
  doi          = {10.1080/07350015.2023.2203207},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {516-532},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Jumps or staleness?},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bonferroni type tests for return predictability and the
initial condition. <em>JBES</em>, <em>42</em>(2), 499–515. (<a
href="https://doi.org/10.1080/07350015.2023.2201313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop tests for predictability that are robust to both the magnitude of the initial condition and the degree of persistence of the predictor. While the popular Bonferroni Q test of Campbell and Yogo displays excellent power properties for strongly persistent predictors with an asymptotically negligible initial condition, it can suffer from severe size distortions and power losses when either the initial condition is asymptotically non-negligible or the predictor is weakly persistent. The Bonferroni t test of Elliott, and Stock, although displaying power well below that of the Bonferroni Q test for strongly persistent predictors with an asymptotically negligible initial condition, displays superior size control and power when the initial condition is asymptotically nonnegligible. In the case where the predictor is weakly persistent, a conventional regression t test comparing to standard normal quantiles is known to be asymptotically optimal under Gaussianity. Based on these properties, we propose two asymptotically size controlled hybrid tests that are functions of the Bonferroni Q , Bonferroni t , and conventional t tests. Our proposed hybrid tests exhibit very good power regardless of the magnitude of the initial condition or the persistence degree of the predictor. An empirical application to the data originally analyzed by Campbell and Yogo shows our new hybrid tests are much more likely to find evidence of predictability than the Bonferroni Q test when the initial condition of the predictor is estimated to be large in magnitude.},
  archive      = {J_JBES},
  author       = {Sam Astill and David I. Harvey and Stephen J. Leybourne and A. M. Robert Taylor},
  doi          = {10.1080/07350015.2023.2201313},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {499-515},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bonferroni type tests for return predictability and the initial condition},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Backtesting systemic risk forecasts using multi-objective
elicitability. <em>JBES</em>, <em>42</em>(2), 485–498. (<a
href="https://doi.org/10.1080/07350015.2023.2200514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systemic risk measures such as CoVaR, CoES, and MES are widely-used in finance, macroeconomics and by regulatory bodies. Despite their importance, we show that they fail to be elicitable and identifiable. This renders forecast comparison and validation, commonly summarized as “backtesting,” impossible. The novel notion of multi-objective elicitability solves this problem by relying on bivariate scores equipped with the lexicographic order. Based on this concept, we propose Diebold–Mariano type tests with suitable bivariate scores to compare systemic risk forecasts. We illustrate the test decisions by an easy-to-apply traffic-light approach. Finally, we apply our traffic-light approach to DAX 30 and S&amp;P 500 returns, and infer some recommendations for regulators.},
  archive      = {J_JBES},
  author       = {Tobias Fissler and Yannick Hoga},
  doi          = {10.1080/07350015.2023.2200514},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {485-498},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Backtesting systemic risk forecasts using multi-objective elicitability},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of matrix exponential unbalanced panel data
models with fixed effects: An application to US outward FDI stock.
<em>JBES</em>, <em>42</em>(2), 469–484. (<a
href="https://doi.org/10.1080/07350015.2023.2200486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider a matrix exponential unbalanced panel data model that allows for (i) spillover effects using matrix exponential terms, (ii) unobserved heterogeneity across entities and time, and (iii) potential heteroscedasticity in the error terms across entities and time. We adopt a likelihood based direct estimation approach in which we jointly estimate the common parameters and fixed effects. To ensure that our estimator has the standard large sample properties, we show how the score functions should be suitably adjusted under both homoscedasticity and heteroscedasticity. We define our suggested estimator as the root of the adjusted score functions, and therefore our approach can be called the M -estimation approach. For inference, we suggest an analytical bias correction approach involving the sample counterpart and plug-in methods to consistently estimate the variance-covariance matrix of the suggested M -estimator. Through an extensive Monte Carlo study, we show that the suggested M -estimator has good finite sample properties. In an empirical application, we use our model to investigate the third country effects on the U.S. outward foreign direct investment (FDI) stock at the industry level.},
  archive      = {J_JBES},
  author       = {Ye Yang and Osman Doğan and Süleyman Taşp Inar},
  doi          = {10.1080/07350015.2023.2200486},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {469-484},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimation of matrix exponential unbalanced panel data models with fixed effects: An application to US outward FDI stock},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for global covariate effects in dynamic interaction
event networks. <em>JBES</em>, <em>42</em>(2), 457–468. (<a
href="https://doi.org/10.1080/07350015.2023.2263537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistical network analysis it is common to observe so called interaction data. Such data is characterized by actors forming the vertices and interacting along edges of the network, where edges are randomly formed and dissolved over the observation horizon. In addition, covariates are observed and the goal is to model the impact of the covariates on the interactions. We distinguish two types of covariates: global, system-wide covariates (i.e., covariates taking the same value for all individuals, such as seasonality) and local, dyadic covariates modeling interactions between two individuals in the network. Existing continuous time network models are extended to allow for comparing a completely parametric model and a model that is parametric only in the local covariates but has a global nonparametric time component. This allows, for instance, to test whether global time dynamics can be explained by simple global covariates like weather, seasonality etc. The procedure is applied to a bike-sharing network by using weather and weekdays as global covariates and distances between the bike stations as local covariates.},
  archive      = {J_JBES},
  author       = {Alexander Kreiss and Enno Mammen and Wolfgang Polonik},
  doi          = {10.1080/07350015.2023.2263537},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {457-468},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing for global covariate effects in dynamic interaction event networks},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A time-varying network for cryptocurrencies. <em>JBES</em>,
<em>42</em>(2), 437–456. (<a
href="https://doi.org/10.1080/07350015.2022.2146695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrencies return cross-predictability and technological similarity yield information on risk propagation and market segmentation. To investigate these effects, we build a time-varying network for cryptocurrencies, based on the evolution of return cross-predictability and technological similarities. We develop a dynamic covariate-assisted spectral clustering method to consistently estimate the latent community structure of cryptocurrencies network that accounts for both sets of information. We demonstrate that investors can achieve better risk diversification by investing in cryptocurrencies from different communities. A cross-sectional portfolio that implements an inter-crypto momentum trading strategy earns a 1.08% daily return. By dissecting the portfolio returns on behavioral factors, we confirm that our results are not driven by behavioral mechanisms.},
  archive      = {J_JBES},
  author       = {Li Guo and Wolfgang Karl Härdle and Yubo Tao},
  doi          = {10.1080/07350015.2022.2146695},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {437-456},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A time-varying network for cryptocurrencies},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large spillover networks of nonstationary systems.
<em>JBES</em>, <em>42</em>(2), 422–436. (<a
href="https://doi.org/10.1080/07350015.2022.2099870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a vector error correction framework for constructing large consistent spillover networks of nonstationary systems grounded in the network theory of Diebold and Y ilmaz. We aim to provide a tailored methodology for the large nonstationary (macro)economic and financial system application settings avoiding technical and often hard to verify assumptions for general statistical high-dimensional approaches where the dimension can also increase with sample size. To achieve this, we propose an elementwise Lasso-type technique for consistent and numerically efficient model selection of VECM, and relate the resulting forecast error variance decomposition to the network topology representation. We also derive the corresponding asymptotic results for model selection and network estimation under standard assumptions. Moreover, we develop a refinement strategy for efficient estimation and show implications and modifications for general dependent innovations. In a comprehensive simulation study, we show convincing finite sample performance of our technique in all cases of moderate and low dimensions. In an application to a system of FX rates, the proposed method leads to novel insights on the connectedness and spillover effects in the FX market among the OECD countries.},
  archive      = {J_JBES},
  author       = {Shi Chen and Melanie Schienle},
  doi          = {10.1080/07350015.2022.2099870},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {422-436},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Large spillover networks of nonstationary systems},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic network quantile regression model. <em>JBES</em>,
<em>42</em>(2), 407–421. (<a
href="https://doi.org/10.1080/07350015.2022.2093882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a dynamic network quantile regression model to investigate the quantile connectedness using a predetermined network information. We extend the existing network quantile autoregression model of Zhu et al. by explicitly allowing the contemporaneous network effects and controlling for the common factors across quantiles. To cope with the endogeneity issue due to simultaneous network spillovers, we adopt the instrumental variable quantile regression (IVQR) estimation and derive the consistency and asymptotic normality of the IVQR estimator using the near epoch dependence property of the network process. Via Monte Carlo simulations, we confirm the satisfactory performance of the IVQR estimator across different quantiles under the different network structures. Finally, we demonstrate the usefulness of our proposed approach with an application to the dataset on the stocks traded in NYSE and NASDAQ in 2016.},
  archive      = {J_JBES},
  author       = {Xiu Xu and Weining Wang and Yongcheol Shin and Chaowen Zheng},
  doi          = {10.1080/07350015.2022.2093882},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {407-421},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Dynamic network quantile regression model},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monitoring network changes in social media. <em>JBES</em>,
<em>42</em>(2), 391–406. (<a
href="https://doi.org/10.1080/07350015.2021.2016425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Econometricians are increasingly working with high-dimensional networks and their dynamics. Econometricians, however, are often confronted with unforeseen changes in network dynamics. In this article, we develop a method and the corresponding algorithm for monitoring changes in dynamic networks. We characterize two types of changes, edge-initiated and node-initiated, to feature the complexity of networks. The proposed approach accounts for three potential challenges in the analysis of networks. First, networks are high-dimensional objects causing the standard statistical tools to suffer from the curse of dimensionality. Second, any potential changes in social networks are likely driven by a few nodes or edges in the network. Third, in many dynamic network applications such as monitoring network connectedness or its centrality, it will be more practically applicable to detect the change in an online fashion than the offline version. The proposed detection method at each time point projects the entire network onto a low-dimensional vector by taking the sparsity into account, then sequentially detects the change by comparing consecutive estimates of the optimal projection direction. As long as the change is sizeable and persistent, the projected vectors will converge to the optimal one, leading to a jump in the sine angle distance between them. A change is therefore declared. Strong theoretical guarantees on both the false alarm rate and detection delays are derived in a sub-Gaussian setting, even under spatial and temporal dependence in the data stream. Numerical studies and an application to the social media messages network support the effectiveness of our method.},
  archive      = {J_JBES},
  author       = {Cathy Yi-Hsuan Chen and Yarema Okhrin and Tengyao Wang},
  doi          = {10.1080/07350015.2021.2016425},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {391-406},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Monitoring network changes in social media},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic peer groups of arbitrage characteristics.
<em>JBES</em>, <em>42</em>(2), 367–390. (<a
href="https://doi.org/10.1080/07350015.2021.2011736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an asset pricing factor model constructed with semiparametric characteristics-based mispricing and factor loading functions. We approximate the unknown functions by B-splines sieve where the number of B-splines coefficients is diverging. We estimate this model and test the existence of the mispricing function by a power enhanced hypothesis test. The enhanced test solves the low power problem caused by diverging B-splines coefficients, with the strengthened power approaching one asymptotically. We also investigate the structure of mispricing components through Hierarchical K-means Clusterings. We apply our methodology to CRSP (Center for Research in Security Prices) and Compustat data for the U.S. stock market with one-year rolling windows during 1967–2017. This empirical study shows the presence of mispricing functions in certain time blocks. We also find that distinct clusters of the same characteristics lead to similar arbitrage returns, forming a “peer group” of arbitrage characteristics.},
  archive      = {J_JBES},
  author       = {Shuyi Ge and Shaoran Li and Oliver Linton},
  doi          = {10.1080/07350015.2021.2011736},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {367-390},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Dynamic peer groups of arbitrage characteristics},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling functional time series and mixed-type predictors
with partially functional autoregressions. <em>JBES</em>,
<em>42</em>(2), 349–366. (<a
href="https://doi.org/10.1080/07350015.2021.2011299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many business and economics studies, researchers have sought to measure the dynamic dependence of curves with high-dimensional mixed-type predictors. We propose a partially functional autoregressive model (pFAR) where the serial dependence of curves is controlled by coefficient operators that are defined on a two-dimensional surface, and the individual and group effects of mixed-type predictors are estimated with a two-layer regularization. We develop an efficient estimation with the proven asymptotic properties of consistency and sparsity. We show how to choose the sieve and tuning parameters in regularization based on a forward-looking criterion. In addition to the asymptotic properties, numerical validation suggests that the dependence structure is accurately detected. The implementation of the pFAR within a real-world analysis of dependence in German daily natural gas flow curves, with seven lagged curves and 85 scalar predictors, produces superior forecast accuracy and an insightful understanding of the dynamics of natural gas supply and demand for the municipal, industry, and border nodes, respectively.},
  archive      = {J_JBES},
  author       = {Xiaofei Xu and Ying Chen and Ge Zhang and Thorsten Koch},
  doi          = {10.1080/07350015.2021.2011299},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {349-366},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Modeling functional time series and mixed-type predictors with partially functional autoregressions},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introduction to the special issue on statistics of dynamic
networks. <em>JBES</em>, <em>42</em>(2), 347–348. (<a
href="https://doi.org/10.1080/07350015.2024.2326778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {Wolfgang Härdle and Melanie Schienled},
  doi          = {10.1080/07350015.2024.2326778},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {347-348},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Introduction to the special issue on statistics of dynamic networks},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Associate editors. <em>JBES</em>, <em>42</em>(1), i. (<a
href="https://doi.org/10.1080/07350015.2024.2291309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  doi          = {10.1080/07350015.2024.2291309},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {i},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Associate editors},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic binary probit model with time-varying parameters
and shrinkage prior. <em>JBES</em>, <em>42</em>(1), 335–346. (<a
href="https://doi.org/10.1080/07350015.2023.2200458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a time series binary probit model in which the underlying latent variable depends on its lag and exogenous regressors. The regression coefficients for the latent variable are allowed to vary over time to capture possible model instability. Bayesian shrinkage priors are applied to automatically differentiate fixed and truly time-varying coefficients and thus avoid unnecessary model complexity. I develop an MCMC algorithm for model estimation that exploits parameter blocking to boost sampling efficiency. An efficient Monte Carlo approximation based on the Kalman filter is developed to improve the numerical stability for computing the predictive likelihood of the binary outcome. Benefits of the proposed model are illustrated in a simulation study and an application to forecast economic recessions.},
  archive      = {J_JBES},
  author       = {Zhongfang He},
  doi          = {10.1080/07350015.2023.2200458},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {335-346},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A dynamic binary probit model with time-varying parameters and shrinkage prior},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix factor analysis: From least squares to iterative
projection. <em>JBES</em>, <em>42</em>(1), 322–334. (<a
href="https://doi.org/10.1080/07350015.2023.2191676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study large-dimensional matrix factor models and estimate the factor loading matrices and factor score matrix by minimizing square loss function. Interestingly, the resultant estimators coincide with the Projected Estimators (PE) in Yu et al. which was proposed from the perspective of simultaneous reduction of the dimensionality and the magnitudes of the idiosyncratic error matrix. In other word, we provide a least-square interpretation of the PE for the matrix factor model, which parallels to the least-square interpretation of the PCA for the vector factor model. We derive the convergence rates of the theoretical minimizers under sub-Gaussian tails. Considering the robustness to the heavy tails of the idiosyncratic errors, we extend the least squares to minimizing the Huber loss function, which leads to a weighted iterative projection approach to compute and learn the parameters. We also derive the convergence rates of the theoretical minimizers of the Huber loss function under bounded fourth or even ( 2 + ϵ ) th moment of the idiosyncratic errors. We conduct extensive numerical studies to investigate the empirical performance of the proposed Huber estimators relative to the state-of-the-art ones. The Huber estimators perform robustly and much better than existing ones when the data are heavy-tailed, and as a result can be used as a safe replacement in practice. An application to a Fama-French financial portfolio dataset demonstrates the empirical advantage of the Huber estimator.},
  archive      = {J_JBES},
  author       = {Yong He and Xinbing Kong and Long Yu and Xinsheng Zhang and Changwei Zhao},
  doi          = {10.1080/07350015.2023.2191676},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {322-334},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Matrix factor analysis: From least squares to iterative projection},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation, inference, and empirical analysis for
time-varying VAR models. <em>JBES</em>, <em>42</em>(1), 310–321. (<a
href="https://doi.org/10.1080/07350015.2023.2191673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector autoregressive (VAR) models are widely used in practical studies, for example, forecasting, modeling policy transmission mechanism, and measuring connection of economic agents. To better capture the dynamics, this article introduces a new class of time-varying VAR models in which the coefficients and covariance matrix of the error innovations are allowed to change smoothly over time. Accordingly, we establish a set of asymptotic properties including the impulse response analyses subject to structural VAR identification conditions, an information criterion to select the optimal lag, and a Wald-type test to determine the constant coefficients. Simulation studies are conducted to evaluate the theoretical findings. Finally, we demonstrate the empirical relevance and usefulness of the proposed methods through an application on U.S. government spending multipliers.},
  archive      = {J_JBES},
  author       = {Jiti Gao and Bin Peng and Yayi Yan},
  doi          = {10.1080/07350015.2023.2191673},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {310-321},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimation, inference, and empirical analysis for time-varying VAR models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-directional simultaneous inference for high-dimensional
models. <em>JBES</em>, <em>42</em>(1), 298–309. (<a
href="https://doi.org/10.1080/07350015.2023.2191672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a general two-directional simultaneous inference (TOSI) framework for high-dimensional models with a manifest variable or latent variable structure, for example, high-dimensional mean models, high-dimensional sparse regression models, and high-dimensional latent factors models. TOSI performs simultaneous inference on a set of parameters from two directions, one to test whether the assumed zero parameters indeed are zeros and one to test whether exist zeros in the parameter set of nonzeros. As a result, we can better identify whether the parameters are zeros, thereby keeping the data structure fully and parsimoniously expressed. We theoretically prove that the single-split TOSI is asymptotically unbiased and the multi-split version of TOSI can control the Type I error below the prespecified significance level. Simulations are conducted to examine the performance of the proposed method in finite sample situations and two real datasets are analyzed. The results show that the TOSI method can provide more predictive and more interpretable estimators than existing methods.},
  archive      = {J_JBES},
  author       = {Wei Liu and Huazhen Lin and Jin Liu and Shurong Zheng},
  doi          = {10.1080/07350015.2023.2191672},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {298-309},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Two-directional simultaneous inference for high-dimensional models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional censored regression via the penalized tobit
likelihood. <em>JBES</em>, <em>42</em>(1), 286–297. (<a
href="https://doi.org/10.1080/07350015.2023.2182309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional regression and regression with a left-censored response are each well-studied topics. In spite of this, few methods have been proposed which deal with both of these complications simultaneously. The Tobit model—long the standard method for censored regression in economics—has not been adapted for high-dimensional regression at all. To fill this gap and bring up-to-date techniques from high-dimensional statistics to the field of high-dimensional left-censored regression, we propose several penalized Tobit models. We develop a fast algorithm which combines quadratic majorization with coordinate descent to compute the penalized Tobit solution path. Theoretically, we analyze the Tobit lasso and Tobit with a folded concave penalty, bounding the l 2 estimation loss for the former and proving that a local linear approximation estimator for the latter possesses the strong oracle property. Through an extensive simulation study, we find that our penalized Tobit models provide more accurate predictions and parameter estimates than other methods on high-dimensional left-censored data. We use a penalized Tobit model to analyze high-dimensional left-censored HIV viral load data from the AIDS Clinical Trials Group and identify potential drug resistance mutations in the HIV genome. A supplementary file contains intermediate theoretical results and technical proofs.},
  archive      = {J_JBES},
  author       = {Tate Jacobson and Hui Zou},
  doi          = {10.1080/07350015.2023.2182309},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {286-297},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {High-dimensional censored regression via the penalized tobit likelihood},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic forecast reconciliation under the gaussian
framework. <em>JBES</em>, <em>42</em>(1), 272–285. (<a
href="https://doi.org/10.1080/07350015.2023.2181176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecast reconciliation of multivariate time series maps a set of incoherent forecasts into coherent forecasts to satisfy a given set of linear constraints. Available methods in the literature either follow a projection matrix-based approach or an empirical copula-based reordering approach to revise the incoherent future sample paths to obtain reconciled probabilistic forecasts. The projection matrices are estimated either by optimizing a scoring rule such as energy or variogram score or simply using a projection matrix derived for point forecast reconciliation. This article proves that (a) if the incoherent predictive distribution is jointly Gaussian, then MinT (minimum trace) minimizes the logarithmic scoring rule for the hierarchy; and (b) the logarithmic score of MinT for each marginal predictive density is smaller than that of OLS (ordinary least squares). We illustrate these theoretical results using a set of simulation studies and the Australian domestic tourism dataset. The estimation of MinT needs to estimate the covariance matrix of the base forecast errors. We have evaluated the performance using the sample covariance matrix and shrinkage estimator. It was observed that the theoretical properties noted above are greatly impacted by the covariance matrix used and highlighted the importance of estimating it reliably, especially with high dimensional data.},
  archive      = {J_JBES},
  author       = {Shanika L. Wickramasuriya},
  doi          = {10.1080/07350015.2023.2181176},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {272-285},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Probabilistic forecast reconciliation under the gaussian framework},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A one-sided refined symmetrized data aggregation approach to
robust mutual fund selection. <em>JBES</em>, <em>42</em>(1), 257–271.
(<a href="https://doi.org/10.1080/07350015.2023.2174549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of identifying skilled funds among a large number of candidates under the linear factor pricing models containing both observable and latent market factors. Motivated by the existence of non-strong potential factors and diversity of error distribution types of the linear factor pricing models, we develop a distribution-free multiple testing procedure to solve this problem. The proposed procedure is established based on the statistical tool of symmetrized data aggregation, which makes it robust to the strength of potential factors and distribution type of the error terms. We then establish the asymptotic validity of the proposed procedure in terms of both the false discovery rate and true discovery proportion under some mild regularity conditions. Furthermore, we demonstrate the advantages of the proposed procedure over some existing methods through extensive Monte Carlo experiments. In an empirical application, we illustrate the practical utility of the proposed procedure in the context of selecting skilled funds, which clearly has much more satisfactory performance than its main competitors.},
  archive      = {J_JBES},
  author       = {Long Feng and Binghui Liu and Yanyuan Ma},
  doi          = {10.1080/07350015.2023.2174549},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {257-271},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A one-sided refined symmetrized data aggregation approach to robust mutual fund selection},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimations and tests for generalized mediation models with
high-dimensional potential mediators. <em>JBES</em>, <em>42</em>(1),
243–256. (<a
href="https://doi.org/10.1080/07350015.2023.2174548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an empirical analysis of stock reaction to COVID-19 pandemic, we propose a generalized mediation model with high-dimensional potential mediators to study the mediation effects of financial metrics that bridge company’s sector and stock value. We propose an estimation procedure for the direct effect via a partial penalized maximum likelihood method and establish its theoretical properties. We develop a Wald test for the indirect effect and show that the proposed test has a χ 2 limiting null distribution. We also develop a partial penalized likelihood ratio test for the direct effect and show that the proposed test asymptotically follows a χ 2 -distribution under null hypothesis. A more efficient estimator of indirect effect under complete mediation model is also developed. Simulation studies are conducted to examine the finite sample performance of the proposed procedures and compare with some existing methods. We further illustrate the proposed methodology with an empirical analysis of stock reaction to COVID-19 pandemic via exploring the underlying mechanism of the relationship between companies’ sectors and their stock values.},
  archive      = {J_JBES},
  author       = {Xu Guo and Runze Li and Jingyuan Liu and Mudong Zeng},
  doi          = {10.1080/07350015.2023.2174548},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {243-256},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimations and tests for generalized mediation models with high-dimensional potential mediators},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On bivariate time-varying price staleness. <em>JBES</em>,
<em>42</em>(1), 229–242. (<a
href="https://doi.org/10.1080/07350015.2023.2174547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Price staleness refers to the extent of zero returns in price dynamics. Bandi, Pirino, and Reno introduce two types of staleness: systematic and idiosyncratic staleness. In this study, we allow price staleness to be time-varying and study the statistical inference for idiosyncratic and common price staleness between two assets. We propose consistent estimators for both time-varying idiosyncratic and systematic price staleness and derive their asymptotic theory. Moreover, we develop a feasible nonparametric test for the simultaneous constancy of idiosyncratic and common price staleness. Our inference is based on infill asymptotics. Finally, we conduct simulation studies under various scenarios to assess the finite sample performance of the proposed approaches and provide an empirical application of the proposed theory.},
  archive      = {J_JBES},
  author       = {Haibin Zhu and Zhi Liu},
  doi          = {10.1080/07350015.2023.2174547},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {229-242},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {On bivariate time-varying price staleness},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the least squares estimation of
multiple-threshold-variable autoregressive models. <em>JBES</em>,
<em>42</em>(1), 215–228. (<a
href="https://doi.org/10.1080/07350015.2023.2174124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most threshold models to-date contain a single threshold variable. However, in many empirical applications, models with multiple threshold variables may be needed and are the focus of this article. For the sake of readability, we start with the Two-Threshold-Variable Autoregressive (2-TAR) model and study its Least Squares Estimation (LSE). Among others, we show that the respective estimated thresholds are asymptotically independent. We propose a new method, namely the weighted Nadaraya-Watson method, to construct confidence intervals for the threshold parameters, that turns out to be, as far as we know, the only method to-date that enjoys good probability coverage, regardless of whether the threshold variables are endogenous or exogenous. Finally, we describe in some detail how our results can be extended to the K -Threshold-Variable Autoregressive ( K -TAR) model, K &gt; 2. We assess the finite-sample performance of the LSE by simulation and present two real examples to illustrate the efficacy of our modeling.},
  archive      = {J_JBES},
  author       = {Xinyu Zhang and Dong Li and Howell Tong},
  doi          = {10.1080/07350015.2023.2174124},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {215-228},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {On the least squares estimation of multiple-threshold-variable autoregressive models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Specification tests for GARCH processes with nuisance
parameters on the boundary. <em>JBES</em>, <em>42</em>(1), 197–214. (<a
href="https://doi.org/10.1080/07350015.2023.2173206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops tests for the correct specification of the conditional variance function in GARCH models when the true parameter may lie on the boundary of the parameter space. The test statistics considered are of Kolmogorov-Smirnov and Cramér-von Mises type, and are based on empirical processes marked by centered squared residuals. The limiting distributions of the test statistics depend on unknown nuisance parameters in a nontrivial way, making the tests difficult to implement. We therefore introduce a novel bootstrap procedure which is shown to be asymptotically valid under general conditions, irrespective of the presence of nuisance parameters on the boundary. The proposed bootstrap approach is based on shrinking of the parameter estimates used to generate the bootstrap sample toward the boundary of the parameter space at a proper rate. It is simple to implement and fast in applications, as the associated test statistics have simple closed form expressions. Although the bootstrap test is designed for a data generating process with fixed parameters (i.e., independent of the sample size n ), we also discuss how to obtain valid inference for sequences of DGPs with parameters approaching the boundary at the n − 1 / 2 rate. A simulation study demonstrates that the new tests: (i) have excellent finite sample behavior in terms of empirical rejection probabilities under the null as well as under the alternative; (ii) provide a useful complement to existing procedures based on Ljung-Box type approaches. Two data examples illustrate the implementation of the proposed tests in applications.},
  archive      = {J_JBES},
  author       = {Giuseppe Cavaliere and Indeewara Perera and Anders Rahbek},
  doi          = {10.1080/07350015.2023.2173206},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {197-214},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Specification tests for GARCH processes with nuisance parameters on the boundary},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction using many samples with models possibly
containing partially shared parameters. <em>JBES</em>, <em>42</em>(1),
187–196. (<a
href="https://doi.org/10.1080/07350015.2023.2166515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider prediction based on a main model. When the main model shares partial parameters with several other helper models, we make use of the additional information. Specifically, we propose a Model Averaging Prediction (MAP) procedure that takes into account data related to the main model as well as data related to the helper models. We allow the data related to different models to follow different structures, as long as they share some common covariate effect. We show that when the main model is misspecified, MAP yields the optimal weights in terms of prediction. Further, if the main model is correctly specified, then MAP will automatically exclude all incorrect helper models asymptotically. Simulation studies are conducted to demonstrate the superior performance of MAP. We further implement MAP to analyze a dataset related to the probability of credit card default.},
  archive      = {J_JBES},
  author       = {Xinyu Zhang and Huihang Liu and Yizheng Wei and Yanyuan Ma},
  doi          = {10.1080/07350015.2023.2166515},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {187-196},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Prediction using many samples with models possibly containing partially shared parameters},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal subsampling bootstrap for massive data.
<em>JBES</em>, <em>42</em>(1), 174–186. (<a
href="https://doi.org/10.1080/07350015.2023.2166514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bootstrap is a widely used procedure for statistical inference because of its simplicity and attractive statistical properties. However, the vanilla version of bootstrap is no longer feasible computationally for many modern massive datasets due to the need to repeatedly resample the entire data. Therefore, several improvements to the bootstrap method have been made in recent years, which assess the quality of estimators by subsampling the full dataset before resampling the subsamples. Naturally, the performance of these modern subsampling methods is influenced by tuning parameters such as the size of subsamples, the number of subsamples, and the number of resamples per subsample. In this article, we develop a novel hyperparameter selection methodology for selecting these tuning parameters. Formulated as an optimization problem to find the optimal value of some measure of accuracy of an estimator subject to computational cost, our framework provides closed-form solutions for the optimal hyperparameter values for subsampled bootstrap, subsampled double bootstrap and bag of little bootstraps, at no or little extra time cost. Using the mean square errors as a proxy of the accuracy measure, we apply our methodology to study, compare and improve the performance of these modern versions of bootstrap developed for massive data through numerical study. The results are promising.},
  archive      = {J_JBES},
  author       = {Yingying Ma and Chenlei Leng and Hansheng Wang},
  doi          = {10.1080/07350015.2023.2166514},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {174-186},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Optimal subsampling bootstrap for massive data},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low frequency cointegrating regression with local to unity
regressors and unknown form of serial dependence. <em>JBES</em>,
<em>42</em>(1), 160–173. (<a
href="https://doi.org/10.1080/07350015.2023.2166513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops new t and F tests in a low-frequency transformed triangular cointegrating regression when one may not be certain that the economic variables are exact unit root processes. We first show that the low-frequency transformed and augmented OLS (TA-OLS) method exhibits an asymptotic bias term in its limiting distribution. As a result, the test for the cointegration vector can have substantially large size distortion, even with minor deviations from the unit root regressors. To correct the asymptotic bias of the TA-OLS statistics for the cointegration vector, we develop modified TA-OLS statistics that adjust the bias and take account of the estimation uncertainty of the long-run endogeneity arising from the bias correction. Based on the modified test statistics, we provide Bonferroni-based tests of the cointegration vector using standard t and F critical values. Monte Carlo results show that our approach has the correct size and reasonable power for a wide range of local-to-unity parameters. Additionally, our method has advantages over the IVX approach when the serial dependence and the long-run endogeneity in the cointegration system are important.},
  archive      = {J_JBES},
  author       = {Jungbin Hwang and Gonzalo Valdés},
  doi          = {10.1080/07350015.2023.2166513},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {160-173},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Low frequency cointegrating regression with local to unity regressors and unknown form of serial dependence},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-sample testing for tail copulas with an application to
equity indices. <em>JBES</em>, <em>42</em>(1), 147–159. (<a
href="https://doi.org/10.1080/07350015.2023.2166050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel, general two-sample hypothesis testing procedure is established for testing the equality of tail copulas associated with bivariate data. More precisely, using a martingale transformation of a natural two-sample tail copula process, a test process is constructed, which is shown to converge in distribution to a standard Wiener process. Hence, from this test process a myriad of asymptotically distribution-free two-sample tests can be obtained. The good finite-sample behavior of our procedure is demonstrated through Monte Carlo simulations. Using the new testing procedure, no evidence of a difference in the respective tail copulas is found for pairs of negative daily log-returns of equity indices during and after the global financial crisis.},
  archive      = {J_JBES},
  author       = {Sami Umut Can and John H. J. Einmahl and Roger J. A. Laeven},
  doi          = {10.1080/07350015.2023.2166050},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {147-159},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Two-sample testing for tail copulas with an application to equity indices},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian nonparametric panel markov-switching GARCH models.
<em>JBES</em>, <em>42</em>(1), 135–146. (<a
href="https://doi.org/10.1080/07350015.2023.2166049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes Bayesian nonparametric inference for panel Markov-switching GARCH models. The model incorporates series-specific hidden Markov chain processes that drive the GARCH parameters. To cope with the high-dimensionality of the parameter space, the article assumes soft parameter pooling through a hierarchical prior distribution and introduces cross sectional clustering through a Bayesian nonparametric prior distribution. An MCMC posterior approximation algorithm is developed and its efficiency is studied in simulations under alternative settings. An empirical application to financial returns data in the United States is offered with a portfolio performance exercise based on forecasts. A comparison shows that the Bayesian nonparametric panel Markov-switching GARCH model provides good forecasting performances and economic gains in optimal asset allocation.},
  archive      = {J_JBES},
  author       = {Roberto Casarin and Mauro Costantini and Anthony Osuntuyi},
  doi          = {10.1080/07350015.2023.2166049},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {135-146},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bayesian nonparametric panel markov-switching GARCH models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting a nonstationary time series using a mixture of
stationary and nonstationary factors as predictors. <em>JBES</em>,
<em>42</em>(1), 122–134. (<a
href="https://doi.org/10.1080/07350015.2023.2166048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method for constructing prediction intervals for a nonstationary variable, such as GDP. The method uses a Factor Augmented Regression (FAR) model. The predictors in the model include a small number of factors generated to extract most of the information in a set of panel data on a large number of macroeconomic variables that are considered to be potential predictors. The novelty of this article is that it provides a method and justification for a mixture of stationary and nonstationary factors as predictors in the FAR model; we refer to this as mixture-FAR method. This method is important because typically such a large set of panel data, for example the FRED-QD, is likely to contain a mixture of stationary and nonstationary variables. In our simulation study, we observed that the proposed mixture-FAR method performed better than its competitor that requires all the predictors to be nonstationary; the MSE of prediction was at least 33% lower for mixture-FAR. Using the data in FRED-QD for the United States, we evaluated the aforementioned methods for forecasting the nonstationary variables, GDP and Industrial Production. We observed that the mixture-FAR method performed better than its competitors.},
  archive      = {J_JBES},
  author       = {Sium Bodha Hannadige and Jiti Gao and Mervyn J. Silvapulle and Param Silvapulle},
  doi          = {10.1080/07350015.2023.2166048},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {122-134},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Forecasting a nonstationary time series using a mixture of stationary and nonstationary factors as predictors},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Getting the ROC into sync. <em>JBES</em>, <em>42</em>(1),
109–121. (<a
href="https://doi.org/10.1080/07350015.2022.2154778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Judging the conformity of binary events in macroeconomics and finance has often been done with indices that measure synchronization. In recent years, the use of Receiver Operating Characteristic (ROC) curve has become popular for this task. This article shows that the ROC and synchronization approaches are closely related, and each can be derived from a decision-making framework. Furthermore, the resulting global measures of the degree of conformity can be identified and estimated using the standard method of moments estimators. The impact of serial dependence in the underlying series upon inferences can therefore be allowed for. Such serial correlation is common in macroeconomic and financial data.},
  archive      = {J_JBES},
  author       = {Liu Yang and Kajal Lahiri and Adrian Pagan},
  doi          = {10.1080/07350015.2022.2154778},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {109-121},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Getting the ROC into sync},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of a structural break point in linear regression
models. <em>JBES</em>, <em>42</em>(1), 95–108. (<a
href="https://doi.org/10.1080/07350015.2022.2154777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a point estimator of the break location for a one-time structural break in linear regression models. If the break magnitude is small, the least-squares estimator of the break date has two modes at the ends of the finite sample period, regardless of the true break location. To solve this problem, I suggest an alternative estimator based on a modification of the least-squares objective function. The modified objective function incorporates estimation uncertainty that varies across potential break dates. The new break point estimator is consistent and has a unimodal finite sample distribution under small break magnitudes. A limit distribution is provided under an in-fill asymptotic framework. Monte Carlo simulation results suggest that the new estimator outperforms the least-squares estimator. I apply the method to estimate the break date in U.S. and U.K. stock return prediction models.},
  archive      = {J_JBES},
  author       = {Yaein Baek},
  doi          = {10.1080/07350015.2022.2154777},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {95-108},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimation of a structural break point in linear regression models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of time-varying factor models. <em>JBES</em>,
<em>42</em>(1), 76–94. (<a
href="https://doi.org/10.1080/07350015.2022.2151449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of large datasets with long time spans has cast doubt on the assumption of constant loadings in conventional factor models. Being a potential solution, the time-varying factor model (TVFM) has attracted enormous interest in the literature. However, TVFM also suffers from the well-known problem of nonidentifiability. This article considers the situations under which both the factors and factor loadings can be estimated without rotations asymptotically. Asymptotic distributions of the proposed estimators are derived. Theoretical findings are supported by simulations. Finally, we evaluate the forecasting performance of the estimated factors subject to different identification restrictions using an extensive dataset of the U.S. macroeconomic variables. Substantial differences are found among the choices of identification restrictions.},
  archive      = {J_JBES},
  author       = {Ying Lun Cheung},
  doi          = {10.1080/07350015.2022.2151449},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {76-94},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Identification of time-varying factor models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood ratio tests for lorenz dominance. <em>JBES</em>,
<em>42</em>(1), 64–75. (<a
href="https://doi.org/10.1080/07350015.2022.2146696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In testing hypotheses pertaining to Lorenz dominance (LD), researchers have examined second- and third-order stochastic dominance using empirical Lorenz processes and integrated stochastic processes with the aid of bootstrap analysis. Among these topics, analysis of third-order stochastic dominance (TSD) based on the notion of risk aversion has been examined using crossing (generalized) Lorenz curves. These facts motivated the present study to characterize distribution pairs displaying the TSD without second-order (generalized Lorenz) dominance. It further motivated the development of likelihood ratio (LR) goodness-of-fit tests for examining the respective hypotheses of the LD, crossing (generalized) Lorenz curves, and TSD through approximate Chi-squared distributions. The proposed LR tests were assessed using simulated distributions, and applied to examine the COVID-19 regional death counts of bivariate samples collected by the World Health Organization between March 2020 and February 2021.},
  archive      = {J_JBES},
  author       = {Shen-Da Chang and Philip E. Cheng and Michelle Liou},
  doi          = {10.1080/07350015.2022.2146696},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {64-75},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Likelihood ratio tests for lorenz dominance},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphical assistant grouped network autoregression model: A
bayesian nonparametric recourse. <em>JBES</em>, <em>42</em>(1), 49–63.
(<a href="https://doi.org/10.1080/07350015.2022.2143784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector autoregression model is ubiquitous in classical time series data analysis. With the rapid advance of social network sites, time series data over latent graph is becoming increasingly popular. In this article, we develop a novel Bayesian grouped network autoregression model, which can simultaneously estimate group information (number of groups and group configurations) and group-wise parameters. Specifically, a graphically assisted Chinese restaurant process is incorporated under the framework of the network autoregression model to improve the statistical inference performance. An efficient Markov chain Monte Carlo sampling algorithm is used to sample from the posterior distribution. Extensive studies are conducted to evaluate the finite sample performance of our proposed methodology. Additionally, we analyze two real datasets as illustrations of the effectiveness of our approach.},
  archive      = {J_JBES},
  author       = {Yimeng Ren and Xuening Zhu and Xiaoling Lu and Guanyu Hu},
  doi          = {10.1080/07350015.2022.2143784},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {49-63},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Graphical assistant grouped network autoregression model: A bayesian nonparametric recourse},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covariance model with general linear structure and divergent
parameters. <em>JBES</em>, <em>42</em>(1), 36–48. (<a
href="https://doi.org/10.1080/07350015.2022.2142593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For estimating the large covariance matrix with a limited sample size, we propose the covariance model with general linear structure (CMGL) by employing the general link function to connect the covariance of the continuous response vector to a linear combination of weight matrices. Without assuming the distribution of responses, and allowing the number of parameters associated with weight matrices to diverge, we obtain the quasi-maximum likelihood estimators (QMLE) of parameters and show their asymptotic properties. In addition, an extended Bayesian information criteria (EBIC) is proposed to select relevant weight matrices, and the consistency of EBIC is demonstrated. Under the identity link function, we introduce the ordinary least squares estimator (OLS) that has the closed form. Hence, its computational burden is reduced compared to QMLE, and the theoretical properties of OLS are also investigated. To assess the adequacy of the link function, we further propose the quasi-likelihood ratio test and obtain its limiting distribution. Simulation studies are presented to assess the performance of the proposed methods, and the usefulness of generalized covariance models is illustrated by an analysis of the U.S. stock market.},
  archive      = {J_JBES},
  author       = {Xinyan Fan and Wei Lan and Tao Zou and Chih-Ling Tsai},
  doi          = {10.1080/07350015.2022.2142593},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {36-48},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Covariance model with general linear structure and divergent parameters},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Homogeneity and sparsity analysis for high-dimensional panel
data models. <em>JBES</em>, <em>42</em>(1), 26–35. (<a
href="https://doi.org/10.1080/07350015.2022.2140667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we are interested in detecting latent group structures and significant covariates in a high-dimensional panel data model with both individual and time fixed effects. The slope coefficients of the model are assumed to be subject dependent, and there exist group structures where the slope coefficients are homogeneous within groups and heterogeneous between groups. We develop a penalized estimator for recovering the group structures and the sparsity patterns simultaneously. We propose a new algorithm to optimize the objective function. Furthermore, we propose a strategy to reduce the computational complexity by pruning the penalty terms in the objective function, which also improves the accuracy of group structure detection. The proposed estimator can recover the latent group structures and the sparsity patterns consistently in large samples. The finite sample performance of the proposed estimator is evaluated through Monte Carlo studies and illustrated with a real dataset.},
  archive      = {J_JBES},
  author       = {Wu Wang and Zhongyi Zhu},
  doi          = {10.1080/07350015.2022.2140667},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {26-35},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Homogeneity and sparsity analysis for high-dimensional panel data models},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of a triangular two equation system without
instruments. <em>JBES</em>, <em>42</em>(1), 14–25. (<a
href="https://doi.org/10.1080/07350015.2023.2166052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that a standard linear triangular two equation system can be point identified, without the use of instruments or any other side information. We find that the only case where the model is not point identified is when a latent variable that causes endogeneity is normally distributed. In this nonidentified case, we derive the sharp identified set. We apply our results to Acemoglu and Johnson’s model of life expectancy and GDP, obtaining point identification and comparable estimates to theirs, without using their (or any other) instrument.},
  archive      = {J_JBES},
  author       = {Arthur Lewbel and Susanne M. Schennach and Linqi Zhang},
  doi          = {10.1080/07350015.2023.2166052},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {14-25},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Identification of a triangular two equation system without instruments},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing sensitivity to unconfoundedness: Estimation and
inference. <em>JBES</em>, <em>42</em>(1), 1–13. (<a
href="https://doi.org/10.1080/07350015.2023.2183212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a set of methods for quantifying the robustness of treatment effects estimated using the unconfoundedness assumption. Specifically, we estimate and do inference on bounds for various treatment effect parameters, like the Average Treatment Effect (ATE) and the average effect of treatment on the treated (ATT), under nonparametric relaxations of the unconfoundedness assumption indexed by a scalar sensitivity parameter c . These relaxations allow for limited selection on unobservables, depending on the value of c . For large enough c , these bounds equal the no assumptions bounds. Using a nonstandard bootstrap method, we show how to construct confidence bands for these bound functions which are uniform over all values of c . We illustrate these methods with an empirical application to the National Supported Work Demonstration program. We implement these methods in the companion Stata module tesensitivity for easy use in practice.},
  archive      = {J_JBES},
  author       = {Matthew A. Masten and Alexandre Poirier and Linqi Zhang},
  doi          = {10.1080/07350015.2023.2183212},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Assessing sensitivity to unconfoundedness: Estimation and inference},
  volume       = {42},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
