<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COLI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coli---41">COLI - 41</h2>
<ul>
<li><details>
<summary>
(2024). Perception of phonological assimilation by neural speech
recognition models. <em>COLI</em>, <em>50</em>(4), 1557–1585. (<a
href="https://doi.org/10.1162/coli_a_00526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human listeners effortlessly compensate for phonological changes during speech perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as “clea[m] pan”, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural speech recognition model Wav2Vec2 perceives assimilated sounds, and identifies the linguistic knowledge that is implemented by the model to compensate for assimilation during Automatic Speech Recognition (ASR). Using psycholinguistic stimuli, we systematically analyze how various linguistic context cues influence compensation patterns in the model’s output. Complementing these behavioral experiments, our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers. Finally, our causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift. These findings represent a step towards better understanding the similarities and differences in phonological processing between neural ASR models and humans.},
  archive      = {J_COLI},
  author       = {Pouw, Charlotte and Kloots, Marianne de Heer and Alishahi, Afra and Zuidema, Willem},
  doi          = {10.1162/coli_a_00526},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1557-1585},
  shortjournal = {Comput. Lingu.},
  title        = {Perception of phonological assimilation by neural speech recognition models},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From form(s) to meaning: Probing the semantic depths of
language models using multisense consistency. <em>COLI</em>,
<em>50</em>(4), 1507–1556. (<a
href="https://doi.org/10.1162/coli_a_00529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what “understanding” means for a language model and how it compares to human understanding. This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say. In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes—inspired by Fregean senses —of the same meaning. Specifically, we focus on consistency across languages as well as paraphrases. Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks. We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks. We find that the model’s multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding. We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding.},
  archive      = {J_COLI},
  author       = {Ohmer, Xenia and Bruni, Elia and Hupke, Dieuwke},
  doi          = {10.1162/coli_a_00529},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1507-1556},
  shortjournal = {Comput. Lingu.},
  title        = {From form(s) to meaning: Probing the semantic depths of language models using multisense consistency},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring temporal sensitivity in the brain using
multi-timescale language models: An EEG decoding study. <em>COLI</em>,
<em>50</em>(4), 1477–1506. (<a
href="https://doi.org/10.1162/coli_a_00533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain’s ability to perform complex computations at varying timescales is crucial, ranging from understanding single words to grasping the overarching narrative of a story. Recently, multi-timescale long short-term memory (MT-LSTM) models (Mahto et al. 2020 ; Jain et al. 2020 ) have been introduced, which use temporally tuned parameters to induce sensitivity to different timescales of language processing (i.e., related to near/distant words). However, there has not been an exploration of the relationship between such temporally tuned information processing in MT-LSTMs and the brain’s processing of language using high temporal resolution recording modalities, such as electroencephalography (EEG). To bridge this gap, we used an EEG dataset recorded while participants listened to Chapter 1 of “Alice in Wonderland” and trained ridge regression models to predict the temporally tuned MT-LSTM embeddings from EEG responses. Our analysis reveals that EEG signals can be used to predict MT-LSTM embeddings across various timescales. For longer timescales, our models produced accurate predictions within an extended time window of ±2 s around word onset, while for shorter timescales, significant predictions are confined to a narrower window ranging from −180 ms to 790 ms. Intriguingly, we observed that short timescale information is not only processed in the vicinity of word onset but also at more distant time points. These observations underscore the parallels and discrepancies between computational models and the neural mechanisms of the brain. As word embeddings are used more as in silico models of semantic representation in the brain, a more explicit consideration of timescale-dependent processing enables more targeted explorations of language processing in humans and machines.},
  archive      = {J_COLI},
  author       = {Ling, Sijie and Murphy, Alex and Fyshe, Alona},
  doi          = {10.1162/coli_a_00533},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1477-1506},
  shortjournal = {Comput. Lingu.},
  title        = {Exploring temporal sensitivity in the brain using multi-timescale language models: An EEG decoding study},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can language models handle recursively nested grammatical
structures? A case study on comparing models and humans. <em>COLI</em>,
<em>50</em>(4), 1441–1476. (<a
href="https://doi.org/10.1162/coli_a_00525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How should we compare the capabilities of language models (LMs) and humans? In this article, I draw inspiration from comparative psychology to highlight challenges in these comparisons. I focus on a case study: processing of recursively nested grammatical structures. Prior work suggests that LMs cannot process these structures as reliably as humans can. However, the humans were provided with instructions and substantial training, while the LMs were evaluated zero-shot. I therefore match the evaluation more closely. Providing large LMs with a simple prompt—with substantially less content than the human training—allows the LMs to consistently outperform the human results, even in more deeply nested conditions than were tested with humans. Furthermore, the effects of prompting are robust to the particular structures and vocabulary used in the prompt. Finally, reanalyzing the existing human data suggests that the humans may not perform above chance at the difficult structures initially. Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably. This case study highlights how discrepancies in the evaluation methods can confound comparisons of language models and humans. I conclude by reflecting on the broader challenge of comparing human and model capabilities, and highlight an important difference between evaluating cognitive models and foundation models.},
  archive      = {J_COLI},
  author       = {Lampinen, Andrew},
  doi          = {10.1162/coli_a_00525},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1441-1476},
  shortjournal = {Comput. Lingu.},
  title        = {Can language models handle recursively nested grammatical structures? a case study on comparing models and humans},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do multimodal large language models and humans ground
language similarly? <em>COLI</em>, <em>50</em>(4), 1415–1440. (<a
href="https://doi.org/10.1162/coli_a_00531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have been criticized for failing to connect linguistic meaning to the world—for failing to solve the “symbol grounding problem.” Multimodal Large Language Models (MLLMs) offer a potential solution to this challenge by combining linguistic representations and processing with other modalities. However, much is still unknown about exactly how and to what degree MLLMs integrate their distinct modalities—and whether the way they do so mirrors the mechanisms believed to underpin grounding in humans. In humans, it has been hypothesized that linguistic meaning is grounded through “embodied simulation,” the activation of sensorimotor and affective representations reflecting described experiences. Across four pre-registered studies, we adapt experimental techniques originally developed to investigate embodied simulation in human comprehenders to ask whether MLLMs are sensitive to sensorimotor features that are implied but not explicit in descriptions of an event. In Experiment 1, we find sensitivity to some features (color and shape) but not others (size, orientation, and volume). In Experiment 2, we identify likely bottlenecks to explain an MLLM’s lack of sensitivity. In Experiment 3, we find that despite sensitivity to implicit sensorimotor features, MLLMs cannot fully account for human behavior on the same task. Finally, in Experiment 4, we compare the psychometric predictive power of different MLLM architectures and find that ViLT, a single-stream architecture, is more predictive of human responses to one sensorimotor feature (shape) than CLIP, a dual-encoder architecture—despite being trained on orders of magnitude less data. These results reveal strengths and limitations in the ability of current MLLMs to integrate language with other modalities, and also shed light on the likely mechanisms underlying human language comprehension.},
  archive      = {J_COLI},
  author       = {Jones, Cameron R. and Bergen, Benjamin and Trott, Sean},
  doi          = {10.1162/coli_a_00531},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1415-1440},
  shortjournal = {Comput. Lingu.},
  title        = {Do multimodal large language models and humans ground language similarly?},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Usage-based grammar induction from minimal cognitive
principles. <em>COLI</em>, <em>50</em>(4), 1375–1414. (<a
href="https://doi.org/10.1162/coli_a_00528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the cognitive mechanisms underlying human language acquisition through grammar induction by a minimal cognitive architecture, with a short and flexible sequence memory as its most central feature. We use reinforcement learning for the task of identifying sentences in a stream of words from artificial languages. Results demonstrate the model’s ability to identify frequent and informative multi-word chunks, reproducing characteristics of natural language acquisition. The model successfully navigates varying degrees of linguistic complexity, exposing efficient adaptation to combinatorial challenges through the reuse of sequential patterns. The emergence of parsimonious tree structures suggests an optimization for the sentence identification task, balancing economy and information. The cognitive architecture reflects aspects of human memory systems and decision-making processes, enhancing its cognitive plausibility. While the model exhibits limitations in generalization and semantic representation, its minimalist nature offers insights into some fundamental mechanisms of language learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory in language learning. Since other animals do not seem to have faithful sequence memory, this may be a key to understanding why only humans have developed complex languages.},
  archive      = {J_COLI},
  author       = {Jon-And, Anna and Michaud, Jérôme},
  doi          = {10.1162/coli_a_00528},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1375-1414},
  shortjournal = {Comput. Lingu.},
  title        = {Usage-based grammar induction from minimal cognitive principles},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decode, move and speak! Self-supervised learning of speech
units, gestures, and sound relationships using vocal imitation.
<em>COLI</em>, <em>50</em>(4), 1345–1373. (<a
href="https://doi.org/10.1162/coli_a_00532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech learning encompasses mastering a complex motor system to produce speech sounds from articulatory gestures while simultaneously uncovering discrete units that provide entry to the linguistic system. Remarkably, children acquire these associations between speech sounds, articulatory gestures, and linguistic units in a weakly supervised manner, without the need for explicit labeling of auditory inputs or access to target articulatory gestures. This study uses self-supervised deep learning to investigate the respective roles of sounds, gestures, and linguistic units in speech acquisition and control. In a first experiment, we analyzed the quantized representations learned by vector-quantized variational autoencoders (VQ-VAE) from ground truth acoustic and articulatory data using ABX tests. We show an interesting complementarity between acoustic and articulatory modalities that may help in the discovery of phonemes. In a second experiment, we introduce a computational agent that repeats auditory speech inputs by controlling a virtual vocal apparatus. This agent integrates an articulatory synthesizer capable of reproducing diverse speech stimuli from interpretable parameters, along with two internal models implementing the articulatory-to-acoustic (forward) and acoustic-to-articulatory (inverse) mapping, respectively. Additionally, two inductive biases are used to regularize the ill-posed acoustic-to-articulatory inverse mapping. In line with the first experiment, we explore the complementarity between the auditory input and the articulatory parameters inferred by the agent. We also evaluate the impact of discretizing auditory inputs using VQ-VAE. While the majority of the agent’s productions are intelligible (according to perceptual evaluations), our analysis highlights inconsistencies in the underlying articulatory trajectories. In particular, we show that the agent’s productions only partially reproduce the complementarity between the auditory and articulatory modalities observed in humans.},
  archive      = {J_COLI},
  author       = {Georges, Marc-Antoine and Lavechin, Marvin and Schwartz, Jean-Luc and Hueber, Thomas},
  doi          = {10.1162/coli_a_00532},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1345-1373},
  shortjournal = {Comput. Lingu.},
  title        = {Decode, move and speak! self-supervised learning of speech units, gestures, and sound relationships using vocal imitation},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meaning beyond lexicality: Capturing pseudoword definitions
with language models. <em>COLI</em>, <em>50</em>(4), 1313–1343. (<a
href="https://doi.org/10.1162/coli_a_00527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudowords such as “knackets” or “spechy”—letter strings that are consistent with the orthotactical rules of a language but do not appear in its lexicon—are traditionally considered to be meaningless, and used as such in empirical studies. However, recent studies that show specific semantic patterns associated with these words as well as semantic effects on human pseudoword processing have cast doubt on this view. While these studies suggest that pseudowords have meanings, they provide only extremely limited insight as to whether humans are able to ascribe explicit and declarative semantic content to unfamiliar word forms. In the present study, we utilized an exploratory-confirmatory study design to examine this question. In a first exploratory study, we started from a pre-existing dataset of words and pseudowords alongside human-generated definitions for these items. Using 18 different language models, we showed that the definitions actually produced for (pseudo)words were closer to their respective (pseudo)words than the definitions for the other items. Based on these initial results, we conducted a second, pre-registered, high-powered confirmatory study collecting a new, controlled set of (pseudo)word interpretations. This second study confirmed the results of the first one. Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.},
  archive      = {J_COLI},
  author       = {de Varda, Andrea Gregor and Gatti, Daniele and Marelli, Marco and Günther, Fritz},
  doi          = {10.1162/coli_a_00527},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1313-1343},
  shortjournal = {Comput. Lingu.},
  title        = {Meaning beyond lexicality: Capturing pseudoword definitions with language models},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Humans learn language from situated communicative
interactions. What about machines? <em>COLI</em>, <em>50</em>(4),
1277–1311. (<a href="https://doi.org/10.1162/coli_a_00534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans acquire their native languages by taking part in communicative interactions with their caregivers. These interactions are meaningful, intentional, and situated in their everyday environment. The situated and communicative nature of the interactions is essential to the language acquisition process, as language learners depend on clues provided by the communicative environment to make sense of the utterances they perceive. As such, the linguistic knowledge they build up is rooted in linguistic forms, their meaning, and their communicative function. When it comes to machines, the situated, communicative, and interactional aspects of language learning are often passed over. This applies in particular to today’s large language models (LLMs), where the input is predominantly text-based, and where the distribution of character groups or words serves as a basis for modeling the meaning of linguistic expressions. In this article, we argue that this design choice lies at the root of a number of important limitations, in particular regarding the data hungriness of the models, their limited ability to perform human-like logical and pragmatic reasoning, and their susceptibility to biases. At the same time, we make a case for an alternative approach that models how artificial agents can acquire linguistic structures by participating in situated communicative interactions. Through a selection of experiments, we show how the linguistic knowledge that is captured in the resulting models is of a fundamentally different nature than the knowledge captured by LLMs and argue that this change of perspective provides a promising path towards more human-like language processing in machines.},
  archive      = {J_COLI},
  author       = {Beuls, Katrien and Van Eecke, Paul},
  doi          = {10.1162/coli_a_00534},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1277-1311},
  shortjournal = {Comput. Lingu.},
  title        = {Humans learn language from situated communicative interactions. what about machines?},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exceptions, instantiations, and overgeneralization: Insights
into how language models process generics. <em>COLI</em>,
<em>50</em>(4), 1211–1275. (<a
href="https://doi.org/10.1162/coli_a_00530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have garnered a great deal of attention for their exceptional generative performance on commonsense and reasoning tasks. In this work, we investigate LLMs’ capabilities for generalization using a particularly challenging type of statement: generics . Generics express generalizations (e.g., birds can fly) but do so without explicit quantification. They are notable because they generalize over their instantiations (e.g., sparrows can fly) yet hold true even in the presence of exceptions (e.g., penguins do not). For humans, these generic generalizations play a fundamental role in cognition, concept acquisition, and intuitive reasoning. We investigate how LLMs respond to and reason about generics. To this end, we first propose a framework grounded in pragmatics to automatically generate both exceptions and instantiations – collectively exemplars . We make use of focus—a pragmatic phenomenon that highlights meaning-bearing elements in a sentence—to capture the full range of interpretations of generics across different contexts of use. This allows us to derive precise logical definitions for exemplars and operationalize them to automatically generate exemplars from LLMs. Using our system, we generate a dataset of ∼370 k exemplars across ∼17 k generics and conduct a human validation of a sample of the generated data. We use our final generated dataset to investigate how LLMs reason about generics. Humans have a documented tendency to conflate universally quantified statements (e.g., all birds can fly) with generics. Therefore, we probe whether LLMs exhibit similar overgeneralization behavior in terms of quantification and in property inheritance. We find that LLMs do show evidence of overgeneralization, although they sometimes struggle to reason about exceptions . Furthermore, we find that LLMs may exhibit similar non-logical behavior to humans when considering property inheritance from generics.},
  archive      = {J_COLI},
  author       = {Allaway, Emily and Bhagavatula, Chandra and Hwang, Jena D. and McKeown, Kathleen and Leslie, Sarah-Jane},
  doi          = {10.1162/coli_a_00530},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1211-1275},
  shortjournal = {Comput. Lingu.},
  title        = {Exceptions, instantiations, and overgeneralization: Insights into how language models process generics},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language learning, representation, and processing in humans
and machines: Introduction to the special issue. <em>COLI</em>,
<em>50</em>(4), 1201–1210. (<a
href="https://doi.org/10.1162/coli_e_00539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) and humans acquire knowledge about language without direct supervision. LLMs do so by means of specific training objectives, while humans rely on sensory experience and social interaction. This parallelism has created a feeling in NLP and cognitive science that a systematic understanding of how LLMs acquire and use the encoded knowledge could provide useful insights for studying human cognition. Conversely, methods and findings from the field of cognitive science have occasionally inspired language model development. Yet, the differences in the way that language is processed by machines and humans—in terms of learning mechanisms, amounts of data used, grounding and access to different modalities—make a direct translation of insights challenging. The aim of this edited volume has been to create a forum of exchange and debate along this line of research, inviting contributions that further elucidate similarities and differences between humans and LLMs.},
  archive      = {J_COLI},
  author       = {Apidianaki, Marianna and Fourtassi, Abdellah and Padó, Sebastian},
  doi          = {10.1162/coli_e_00539},
  journal      = {Computational Linguistics},
  month        = {12},
  number       = {4},
  pages        = {1201-1210},
  shortjournal = {Comput. Lingu.},
  title        = {Language learning, representation, and processing in humans and machines: Introduction to the special issue},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Do language models’ words refer? <em>COLI</em>,
<em>50</em>(3), 1191–1200. (<a
href="https://doi.org/10.1162/coli_a_00522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What do language models ( LMs ) do with language? They can produce sequences of (mostly) coherent strings closely resembling English. But do those sentences mean something, or are LMs simply babbling in a convincing simulacrum of language use? We address one aspect of this broad question: whether LMs’ words can refer , that is, achieve “word-to-world” connections. There is prima facie reason to think they do not, since LMs do not interact with the world in the way that ordinary language users do. Drawing on the externalist tradition in philosophy of language, we argue that those appearances are misleading: Even if the inputs to LMs are simply strings of text, they are strings of text with natural histories , and that may suffice for LMs’ words to refer.},
  archive      = {J_COLI},
  author       = {Mandelkern, Matthew and Linzen, Tal},
  doi          = {10.1162/coli_a_00522},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1191-1200},
  shortjournal = {Comput. Lingu.},
  title        = {Do language models’ words refer?},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel alignment-based approach for PARSEVAL measuress.
<em>COLI</em>, <em>50</em>(3), 1181–1190. (<a
href="https://doi.org/10.1162/coli_a_00512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for calculating PARSEVAL measures to evaluate constituent parsing results. Previous constituent parsing evaluation techniques were constrained by the requirement for consistent sentence boundaries and tokenization results, proving to be stringent and inconvenient. Our new approach handles constituent parsing results obtained from raw text, even when sentence boundaries and tokenization differ from the preprocessed gold sentence. Implementing this measure is our evaluation by alignment approach. The algorithm enables the alignment of tokens and sentences in the gold and system parse trees. Our proposed algorithm draws on the analogy of sentence and word alignment commonly used in machine translation (MT). To demonstrate the intricacy of calculations and clarify any integration of configurations, we explain the implementations in detailed pseudo-code and provide empirical proof for how sentence and word alignment can improve evaluation reliability.},
  archive      = {J_COLI},
  author       = {Jo, Eunkyul Leah and Park, Angela Yoonseo and Park, Jungyeul},
  doi          = {10.1162/coli_a_00512},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1181-1190},
  shortjournal = {Comput. Lingu.},
  title        = {A novel alignment-based approach for PARSEVAL measuress},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bias and fairness in large language models: A survey.
<em>COLI</em>, <em>50</em>(3), 1097–1179. (<a
href="https://doi.org/10.1162/coli_a_00524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
  archive      = {J_COLI},
  author       = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
  doi          = {10.1162/coli_a_00524},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1097-1179},
  shortjournal = {Comput. Lingu.},
  title        = {Bias and fairness in large language models: A survey},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language model instruction following: A survey of
progresses and challenges. <em>COLI</em>, <em>50</em>(3), 1053–1095. (<a
href="https://doi.org/10.1162/coli_a_00523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task semantics can be expressed by a set of input-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: First, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning to follow task instructions, that is, instruction following . Despite its impressive progress, there are some unsolved research equations that the community struggles with. This survey tries to summarize and provide insights into the current research on instruction following, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How should we model instructions? (iii) What are popular instruction following datasets and evaluation metrics? (iv) What factors influence and explain the instructions’ performance? (v) What challenges remain in instruction following? To our knowledge, this is the first comprehensive survey about instruction following. 1},
  archive      = {J_COLI},
  author       = {Lou, Renze and Zhang, Kai and Yin, Wenpeng},
  doi          = {10.1162/coli_a_00523},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1053-1095},
  shortjournal = {Comput. Lingu.},
  title        = {Large language model instruction following: A survey of progresses and challenges},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cognitive plausibility in natural language processing.
<em>COLI</em>, <em>50</em>(3), 1049–1052. (<a
href="https://doi.org/10.1162/coli_r_00517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent successes in Natural Language Processing (NLP) give rise to more and more computational models aimed at generating and understanding human language. Traditionally, researchers evaluate such models by looking at how accurate they are at a given task, but the focus in the field is slowly shifting to evaluating other characteristics, such as models’ fairness, interpretability, efficiency, and so forth. An often overlooked aspect is a model’s cognitive plausibility: that is, how “human-like” a model is. Cognitive plausibility is a multifaceted concept grounded in the field of cognitive science, which often uses computational models to study various aspects of human cognition, including language production and comprehension. The present book is a contribution to narrowing the wide gap between state-of-the-art NLP architectures and human cognition.In the quickly changing world of NLP, it is not easy to summarize recent advances, nonetheless, this book’s comprehensive selection of examples from various studies offers a solid overview of the current research on models’ cognitively plausibility. The examples are combined with discussions of theoretical and methodological issues at the interface of NLP and cognitive science. Moreover, each of the main content chapters ends with an overview of relevant ethical issues, a necessary consideration in the era of powerful language models. The book will be compelling reading to NLP researchers interested in human cognition and model interpretability, as well as to cognitive scientists and psycholinguists willing to better understand computational modeling approaches in the language domain. Gradual presentation of the material, with two introductory chapters (see below), also makes the book generally accessible to students and those with only basic knowledge of NLP.The book consists of seven chapters. Chapter 1, “Introduction,” explains why cognitive plausibility is important to consider in NLP: In addition to obvious advantages of cognitively plausible models for cognitive science, the behavior of such models is more intuitive to interpret for human speakers. Exploring this link between cognitive plausibility and interpretability is one of the book’s goals, and it is repeatedly—and very successfully—exploited throughout all the chapters. Another important message in the introduction is that cognitive plausibility is a graded concept that involves multiple dimensions, and this book focuses on three of them: the similarity between the decisions made by models and humans, between the representational structures they use, and between their procedural strategies.The five chapters with the main content can be divided into two large sections: the more introductory ones (2–3) and more in-depth ones (4–6). Specifically, Chapter 2, “Foundations of Language Modeling,” provides an overview of basic concepts in language modeling, such as conditional probability of a sentence, model perplexity, recurrent neural networks, pretrained language models, and so on. Particular emphasis is made on modeling choices that link language models to human language processing: For example, a model’s objective function and architecture can determine whether the model processes words sequentially or not, while input units that models are trained on may or may not reflect the underlying linguistic structure. Chapter 3, “Cognitive Signals of Language Processing,” introduces the types of data that can be used for evaluating cognitive plausibility of computational models. Here, the reader is made aware that the focus of the book is on language comprehension rather than production, which some may find slightly disappointing given that many state-of-the-art generative language models are celebrated largely for their ability to produce human-like language. On the brighter side, even in comprehension there is plenty of relevant data sets, which capture both human speakers’ behavioral responses and their brain activity patterns. Moreover, one can combine different types of data for an even more comprehensive model evaluation. Importantly, many data sets collected from human speakers need to be preprocessed or otherwise adapted to NLP settings, and this chapter also offers an overview of common techniques in this area.The following three chapters are more in-depth and discuss the three dimensions of cognitive plausibility mentioned above. Chapter 4, “Behavioral Patterns,” largely focuses on model interpretability methods through the cognitive lens. First of all, one can analyze a model’s input and output: properties of the input data, model’s behavior on well-defined subpopulations of data and its performance on specific instances depending on their difficulty. Second, there is a variety of tests or even out-of-the-box test suites for targeted evaluation of NLP models. Many of them focus on models’ linguistic abilities, while others (e.g., occlusion or perturbation tests) are designed to stress-test models’ robustness on intricately designed examples. Here, the authors call for using finer-grained model evaluation and for developing multilingual models that are not optimized for English data, as is often the case in NLP. Another promising approach to narrowing the gap between models’ and human speakers’ behavior, according to the authors, consists in designing cognitively plausible curricula for model training, but sadly, in the 2023 BabyLM challenge curriculum learning methods only resulted in modest improvements (Warstadt et al. 2023), and the jury is still out on this subject.While the previous chapter focuses on models’ inputs and outputs, Chapter 5, “Representational Structure,” discusses interpretability of models’ internal representations. Neural representations are commonly expressed as vectors in a high-dimensional space, and measuring similarity between them is central to research in this area. From the cognitive perspective, however, similarity—even between words, let alone longer units—is a complicated concept: units can be judged similar for a variety of reasons, which also highly depend on the context. Moving beyond a single representational space, one can also measure the similarity of different spaces, including the ones derived from human speakers’ behavioral or brain responses. In many cases, it is crucial to have a concrete hypothesis about a model’s representations and test it in a targeted way—for example, using probing classifiers, a common method for finding out whether a specific feature is encoded in a model’s representation space. Another fruitful research direction is mapping models’ representations to brain responses: For example, can a probing classifier learn to predict brain activation patterns?The final dimension of cognitive plausibility is discussed in Chapter 6, “Procedural Strategies.” Strategies that a model adopts are determined by its architecture. For the time being, transformers are by far the most common architecture in NLP, and a large part of this chapter discusses the mechanisms of attention and self-attention used in transformers. Somewhat sadly from this book’s perspective, multi-head attention can hardly be considered cognitively plausible, but nevertheless, studying attention weights can still help us understand relative importance of input units (e.g., words), and the model’s importance values can be compared to human data, such as gaze patterns during reading. Overall, this chapter mostly focuses on sentence processing tasks, since they provide a fruitful ground for studying various procedural strategies and effects: incremental processing (cf. Chapter 2), priming, hierarchical processing, and so on. Concrete proposals to improve the cognitive plausibility of models’ algorithms include the use of multi-task and transfer learning setups, through the explicit integration of human problem-solving strategies into models’ training process.The final Chapter 7, “Towards Cognitively More Plausible Models,” is a brief recap of the book. Although the authors conclude they could not find a silver bullet to make a model cognitively plausible on all the three dimensions considered, they nevertheless successfully propose concrete methods for designing more cognitively plausible models. Among others, these proposals include taking into account instance difficulty in model evaluation, developing more context-aware tools for representation analysis, integrating information from multiple modalities into the models, and adopting a truly multilingual perspective on model design.One key takeaway from this book is that cognitive plausibility is a complex concept—not only because there are several dimensions to it, but also because data collected from human speakers is often less straightforward than what’s dictated by existing NLP models’ objective functions. The authors provide plenty of examples of this complexity throughout the book: Human speakers can disagree in their linguistic annotations, their conceptual representations tend to be fluid, their similarity judgments are nuanced and graded. One possible way forward for NLP is to embrace this uncertainty of human language behavior, an avenue that the field is only starting to explore (e.g., Baan et al. 2023; Liu et al. 2023).},
  archive      = {J_COLI},
  author       = {Matusevych, Yevgen},
  doi          = {10.1162/coli_r_00517},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1049-1052},
  shortjournal = {Comput. Lingu.},
  title        = {Cognitive plausibility in natural language processing},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-lingual cross-temporal summarization: Dataset, models,
evaluation. <em>COLI</em>, <em>50</em>(3), 1001–1047. (<a
href="https://doi.org/10.1162/coli_a_00519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While summarization has been extensively researched in natural language processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a largely unexplored area that has the potential to improve cross-cultural accessibility and understanding. This article comprehensively addresses the CLCTS task, including dataset creation, modeling, and evaluation. We (1) build the first CLCTS corpus with 328 instances for hDe-En (extended version with 455 instances) and 289 for hEn-De (extended version with 501 instances), leveraging historical fiction texts and Wikipedia summaries in English and German; (2) examine the effectiveness of popular transformer end-to-end models with different intermediate fine-tuning tasks; (3) explore the potential of GPT-3.5 as a summarizer; and (4) report evaluations from humans, GPT-4, and several recent automatic evaluation metrics. Our results indicate that intermediate task fine-tuned end-to-end models generate bad to moderate quality summaries while GPT-3.5, as a zero-shot summarizer, provides moderate to good quality outputs. GPT-3.5 also seems very adept at normalizing historical text. To assess data contamination in GPT-3.5, we design an adversarial attack scheme in which we find that GPT-3.5 performs slightly worse for unseen source documents compared to seen documents. Moreover, it sometimes hallucinates when the source sentences are inverted against its prior knowledge with a summarization accuracy of 0.67 for plot omission, 0.71 for entity swap, and 0.53 for plot negation. Overall, our regression results of model performances suggest that longer, older, and more complex source texts (all of which are more characteristic for historical language variants) are harder to summarize for all models, indicating the difficulty of the CLCTS task. Regarding evaluation, we observe that both the GPT-4 and BERTScore correlate moderately with human evaluations, implicating great potential for future improvement.},
  archive      = {J_COLI},
  author       = {Zhang, Ran and Ouni, Jihed and Eger, Steffen},
  doi          = {10.1162/coli_a_00519},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1001-1047},
  shortjournal = {Comput. Lingu.},
  title        = {Cross-lingual cross-temporal summarization: Dataset, models, evaluation},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relation extraction in underexplored biomedical domains: A
diversity-optimized sampling and synthetic data generation approach.
<em>COLI</em>, <em>50</em>(3), 953–1000. (<a
href="https://doi.org/10.1162/coli_a_00520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparsity of labeled data is an obstacle to the development of Relation Extraction (RE) models and the completion of databases in various biomedical areas. While being of high interest in drug-discovery, the literature on natural products, reporting the identification of potential bioactive compounds from organisms, is a concrete example of such an overlooked topic. To mark the start of this new task, we created the first curated evaluation dataset and extracted literature items from the LOTUS database to build training sets. To this end, we developed a new sampler, inspired by diversity metrics in ecology, named Greedy Maximum Entropy sampler ( https://github.com/idiap/gme-sampler ). The strategic optimization of both balance and diversity of the selected items in the evaluation set is important given the resource-intensive nature of manual curation. After quantifying the noise in the training set, in the form of discrepancies between the text of input abstracts and the expected output labels, we explored different strategies accordingly. Framing the task as an end-to-end Relation Extraction, we evaluated the performance of standard fine-tuning (BioGPT, GPT-2, and Seq2rel) and few-shot learning with open Large Language Models (LLMs) (LLaMA 7B-65B). In addition to their evaluation in few-shot settings, we explore the potential of open LLMs as synthetic data generators and propose a new workflow for this purpose. All evaluated models exhibited substantial improvements when fine-tuned on synthetic abstracts rather than the original noisy data. We provide our best performing (F1-score = 59.0) BioGPT-Large model for end-to-end RE of natural products relationships along with all the training and evaluation datasets. See more details at https://github.com/idiap/abroad-re .},
  archive      = {J_COLI},
  author       = {Delmas, Maxime and Wysocka, Magdalena and Freitas, André},
  doi          = {10.1162/coli_a_00520},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {953-1000},
  shortjournal = {Comput. Lingu.},
  title        = {Relation extraction in underexplored biomedical domains: A diversity-optimized sampling and synthetic data generation approach},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aligning human and computational coherence evaluations.
<em>COLI</em>, <em>50</em>(3), 893–952. (<a
href="https://doi.org/10.1162/coli_a_00518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated coherence metrics constitute an efficient and popular way to evaluate topic models. Previous work presents a mixed picture of their presumed correlation with human judgment. This work proposes a novel sampling approach to mining topic representations at a large scale while seeking to mitigate bias from sampling, enabling the investigation of widely used automated coherence metrics via large corpora. Additionally, this article proposes a novel user study design, an amalgamation of different proxy tasks, to derive a finer insight into the human decision-making processes. This design subsumes the purpose of simple rating and outlier-detection user studies. Similar to the sampling approach, the user study conducted is extensive, comprising 40 study participants split into eight different study groups tasked with evaluating their respective set of 100 topic representations. Usually, when substantiating the use of these metrics, human responses are treated as the gold standard. This article further investigates the reliability of human judgment by flipping the comparison and conducting a novel extended analysis of human response at the group and individual level against a generic corpus. The investigation results show a moderate to good correlation between these metrics and human judgment, especially for generic corpora, and derive further insights into the human perception of coherence. Analyzing inter-metric correlations across corpora shows moderate to good correlation among these metrics. As these metrics depend on corpus statistics, this article further investigates the topical differences between corpora, revealing nuances in applications of these metrics.},
  archive      = {J_COLI},
  author       = {Lim, Jia Peng and Lauw, Hady W.},
  doi          = {10.1162/coli_a_00518},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {893-952},
  shortjournal = {Comput. Lingu.},
  title        = {Aligning human and computational coherence evaluations},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLM-assisted data augmentation for chinese dialogue-level
dependency parsing. <em>COLI</em>, <em>50</em>(3), 867–891. (<a
href="https://doi.org/10.1162/coli_a_00515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogue-level dependency parsing, despite its growing academic interest, often encounters underperformance issues due to resource shortages. A potential solution to this challenge is data augmentation. In recent years, large language models (LLMs) have demonstrated strong capabilities in generation, which can facilitate data augmentation greatly. In this study, we focus on Chinese dialogue-level dependency parsing, presenting three simple and effective strategies with LLM to augment the original training instances, namely word-level, syntax-level, and discourse-level augmentations, respectively. These strategies enable LLMs to either preserve or modify dependency structures, thereby assuring accuracy while increasing the diversity of instances at different levels. We conduct experiments on the benchmark dataset released by Jiang et al. ( 2023 ) to validate our approach. Results show that our method can greatly boost the parsing performance in various settings, particularly in dependencies among elementary discourse units. Lastly, we provide in-depth analysis to show the key points of our data augmentation strategies.},
  archive      = {J_COLI},
  author       = {Zhang, Meishan and Jiang, Gongyao and Liu, Shuang and Chen, Jing and Zhang, Min},
  doi          = {10.1162/coli_a_00515},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {867-891},
  shortjournal = {Comput. Lingu.},
  title        = {LLM-assisted data augmentation for chinese dialogue-level dependency parsing},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing dataset annotation quality management in the wild.
<em>COLI</em>, <em>50</em>(3), 817–866. (<a
href="https://doi.org/10.1162/coli_a_00516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quality is crucial for training accurate, unbiased, and trustworthy machine learning models as well as for their correct evaluation. Recent work, however, has shown that even popular datasets used to train and evaluate state-of-the-art models contain a non-negligible amount of erroneous annotations, biases, or artifacts. While practices and guidelines regarding dataset creation projects exist, to our knowledge, large-scale analysis has yet to be performed on how quality management is conducted when creating natural language datasets and whether these recommendations are followed. Therefore, we first survey and summarize recommended quality management practices for dataset creation as described in the literature and provide suggestions for applying them. Then, we compile a corpus of 591 scientific publications introducing text datasets and annotate it for quality-related aspects, such as annotator management, agreement, adjudication, or data validation. Using these annotations, we then analyze how quality management is conducted in practice. A majority of the annotated publications apply good or excellent quality management. However, we deem the effort of 30% of the studies as only subpar. Our analysis also shows common errors, especially when using inter-annotator agreement and computing annotation error rates.},
  archive      = {J_COLI},
  author       = {Klie, Jan-Christoph and Castilho, Richard Eckart de and Gurevych, Iryna},
  doi          = {10.1162/coli_a_00516},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {817-866},
  shortjournal = {Comput. Lingu.},
  title        = {Analyzing dataset annotation quality management in the wild},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The pitfalls of defining hallucination. <em>COLI</em>,
<em>50</em>(2), 807–816. (<a
href="https://doi.org/10.1162/coli_a_00509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs.},
  archive      = {J_COLI},
  author       = {Deemter, Kees van},
  doi          = {10.1162/coli_a_00509},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {807-816},
  shortjournal = {Comput. Lingu.},
  title        = {The pitfalls of defining hallucination},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Common flaws in running human evaluation experiments in NLP.
<em>COLI</em>, <em>50</em>(2), 795–805. (<a
href="https://doi.org/10.1162/coli_a_00508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While conducting a coordinated set of repeat runs of human evaluation experiments in NLP, we discovered flaws in every single experiment we selected for inclusion via a systematic process. In this squib, we describe the types of flaws we discovered, which include coding errors (e.g., loading the wrong system outputs to evaluate), failure to follow standard scientific practice (e.g., ad hoc exclusion of participants and responses), and mistakes in reported numerical results (e.g., reported numbers not matching experimental data). If these problems are widespread, it would have worrying implications for the rigor of NLP evaluation experiments as currently conducted. We discuss what researchers can do to reduce the occurrence of such flaws, including pre-registration, better code development practices, increased testing and piloting, and post-publication addressing of errors.},
  archive      = {J_COLI},
  author       = {Thomson, Craig and Reiter, Ehud and Belz, Anya},
  doi          = {10.1162/coli_a_00508},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {795-805},
  shortjournal = {Comput. Lingu.},
  title        = {Common flaws in running human evaluation experiments in NLP},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The role of typological feature prediction in NLP and
linguistics. <em>COLI</em>, <em>50</em>(2), 781–794. (<a
href="https://doi.org/10.1162/coli_a_00498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational typology has gained traction in the field of Natural Language Processing (NLP) in recent years, as evidenced by the increasing number of papers on the topic and the establishment of a Special Interest Group on the topic (SIGTYP), including the organization of successful workshops and shared tasks. A considerable amount of work in this sub-field is concerned with prediction of typological features, for example, for databases such as the World Atlas of Language Structures (WALS) or Grambank. Prediction is argued to be useful either because (1) it allows for obtaining feature values for relatively undocumented languages, alleviating the sparseness in WALS, in turn argued to be useful for both NLP and linguistics; and (2) it allows us to probe models to see whether or not these typological features are encapsulated in, for example, language representations. In this article, we present a critical stance concerning prediction of typological features, investigating to what extent this line of research is aligned with purported needs—both from the perspective of NLP practitioners, and perhaps more importantly, from the perspective of linguists specialized in typology and language documentation. We provide evidence that this line of research in its current state suffers from a lack of interdisciplinary alignment. Based on an extensive survey of the linguistic typology community, we present concrete recommendations for future research in order to improve this alignment between linguists and NLP researchers, beyond the scope of typological feature prediction.},
  archive      = {J_COLI},
  author       = {Bjerva, Johannes},
  doi          = {10.1162/coli_a_00498},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {781-794},
  shortjournal = {Comput. Lingu.},
  title        = {The role of typological feature prediction in NLP and linguistics},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of computational approaches to
deciphering bronze age aegean and cypriot scripts. <em>COLI</em>,
<em>50</em>(2), 725–779. (<a
href="https://doi.org/10.1162/coli_a_00514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a detailed insight into computational approaches for deciphering Bronze Age Aegean and Cypriot scripts, namely, the Archanes script and the Archanes formula, Phaistos Disk, Cretan hieroglyphic (including the Malia Altar Stone and Arkalochori Axe), Linear A, Linear B, Cypro-Minoan, and Cypriot scripts. The unique contributions of this article are threefold: (1) a thorough review of major Bronze Age Aegean and Cypriot scripts and inscriptions, digital data and corpora associated with them, existing computational decipherment methods developed in order to decipher them, and possible links to other scripts and languages; (2) the definition of 15 major challenges that can be encountered in computational decipherments of ancient scripts; and (3) an outline of a computational model that could possibly be used to simulate traditional decipherment processes of ancient scripts based on palaeography and epigraphy. In the context of this article the term decipherment denotes the process of discovery of the language and/or the set of symbols behind an unknown script, and the meaning behind it.},
  archive      = {J_COLI},
  author       = {Braović, Maja and Krstinić, Damir and Štula, Maja and Ivanda, Antonia},
  doi          = {10.1162/coli_a_00514},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {725-779},
  shortjournal = {Comput. Lingu.},
  title        = {A systematic review of computational approaches to deciphering bronze age aegean and cypriot scripts},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards faithful model explanation in NLP: A survey.
<em>COLI</em>, <em>50</em>(2), 657–723. (<a
href="https://doi.org/10.1162/coli_a_00511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness , that is, an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.},
  archive      = {J_COLI},
  author       = {Lyu, Qing and Apidianaki, Marianna and Callison-Burch, Chris},
  doi          = {10.1162/coli_a_00511},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {657-723},
  shortjournal = {Comput. Lingu.},
  title        = {Towards faithful model explanation in NLP: A survey},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topics in the haystack: Enhancing topic quality through
corpus expansion. <em>COLI</em>, <em>50</em>(2), 619–655. (<a
href="https://doi.org/10.1162/coli_a_00506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting and identifying latent topics in large text corpora have gained increasing importance in Natural Language Processing (NLP). Most models, whether probabilistic models similar to Latent Dirichlet Allocation (LDA) or neural topic models, follow the same underlying approach of topic interpretability and topic extraction. We propose a method that incorporates a deeper understanding of both sentence and document themes, and goes beyond simply analyzing word frequencies in the data. Through simple corpus expansion, our model can detect latent topics that may include uncommon words or neologisms, as well as words not present in the documents themselves. Additionally, we propose several new evaluation metrics based on intruder words and similarity measures in the semantic space. We present correlation coefficients with human identification of intruder words and achieve near-human level results at the word-intrusion task. We demonstrate the competitive performance of our method with a large benchmark study, and achieve superior results compared with state-of-the-art topic modeling and document clustering models. The code is available at the following link: https://github.com/AnFreTh/STREAM .},
  archive      = {J_COLI},
  author       = {Thielmann, Anton and Reuter, Arik and Seifert, Quentin and Bergherr, Elisabeth and Säfken, Benjamin},
  doi          = {10.1162/coli_a_00506},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {619-655},
  shortjournal = {Comput. Lingu.},
  title        = {Topics in the haystack: Enhancing topic quality through corpus expansion},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian approach to uncertainty in word embedding bias
estimation. <em>COLI</em>, <em>50</em>(2), 563–617. (<a
href="https://doi.org/10.1162/coli_a_00507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple measures, such as WEAT or MAC, attempt to quantify the magnitude of bias present in word embeddings in terms of a single-number metric. However, such metrics and the related statistical significance calculations rely on treating pre-averaged data as individual data points and utilizing bootstrapping techniques with low sample sizes. We show that similar results can be easily obtained using such methods even if the data are generated by a null model lacking the intended bias. Consequently, we argue that this approach generates false confidence. To address this issue, we propose a Bayesian alternative: hierarchical Bayesian modeling, which enables a more uncertainty-sensitive inspection of bias in word embeddings at different levels of granularity. To showcase our method, we apply it to Religion, Gender, and Race word lists from the original research, together with our control neutral word lists. We deploy the method using Google, GloVe, and Reddit embeddings. Further, we utilize our approach to evaluate a debiasing technique applied to the Reddit word embedding. Our findings reveal a more complex landscape than suggested by the proponents of single-number metrics. The datasets and source code for the paper are publicly available. 1},
  archive      = {J_COLI},
  author       = {Dobrzeniecka, Alicja and Urbaniak, Rafal},
  doi          = {10.1162/coli_a_00507},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {563-617},
  shortjournal = {Comput. Lingu.},
  title        = {A bayesian approach to uncertainty in word embedding bias estimation},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UG-schematic annotation for event nominals: A case study in
mandarin chinese. <em>COLI</em>, <em>50</em>(2), 535–561. (<a
href="https://doi.org/10.1162/coli_a_00504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Divergence of languages observed at the surface level is a major challenge encountered by multilingual data representation, especially when typologically distant languages are involved. Drawing inspiration from a formalist Chomskyan perspective towards language universals, Universal Grammar (UG), this article uses deductively pre-defined universals to analyze a multilingually heterogeneous phenomenon, event nominals. In this way, deeper universality of event nominals beneath their huge divergence in different languages is uncovered, which empowers us to break barriers between languages and thus extend insights from some synthetic languages to a non-inflectional language, Mandarin Chinese. Our empirical investigation also demonstrates this UG-inspired schema is effective: With its assistance, the inter-annotator agreement (IAA) for identifying event nominals in Mandarin grows from 88.02% to 94.99%, and automatic detection of event-reading nominalizations on the newly-established data achieves an accuracy of 94.76% and an F 1 score of 91.3%, which significantly surpass those achieved on the pre-existing resource by 9.8% and 5.2%, respectively. Our systematic analysis also sheds light on nominal semantic role labeling. By providing a clear definition and classification on arguments of event nominal, the IAA of this task significantly increases from 90.46% to 98.04%.},
  archive      = {J_COLI},
  author       = {Li, Wenxi and Zhang, Yutong and Emerson, Guy and Sun, Weiwei},
  doi          = {10.1162/coli_a_00504},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {535-561},
  shortjournal = {Comput. Lingu.},
  title        = {UG-schematic annotation for event nominals: A case study in mandarin chinese},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Context-aware transliteration of romanized south asian
languages. <em>COLI</em>, <em>50</em>(2), 475–534. (<a
href="https://doi.org/10.1162/coli_a_00510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While most transliteration research is focused on single tokens such as named entities—for example, transliteration of from the Gujarati script to the Latin script “Ahmedabad” footnoteThe most populous city in the Indian state of Gujarat. the informal romanization prevalent in South Asia and elsewhere often requires transliteration of full sentences. The lack of large parallel text collections of full sentence (as opposed to single word) transliterations necessitates incorporation of contextual information into transliteration via non-parallel resources, such as via mono-script text collections. In this article, we present a number of methods for improving transliteration in context for such a use scenario. Some of these methods in fact improve performance without making use of sentential context, allowing for better quantification of the degree to which contextual information in particular is responsible for system improvements. Our final systems, which ultimately rely upon ensembles including large pretrained language models fine-tuned on simulated parallel data, yield substantial improvements over the best previously reported results for full sentence transliteration from Latin to native script on all 12 languages in the Dakshina dataset (Roark et al. 2020 ), with an overall 3.3% absolute (18.6% relative) mean word-error rate reduction.},
  archive      = {J_COLI},
  author       = {Kirov, Christo and Johny, Cibu and Katanova, Anna and Gutkin, Alexander and Roark, Brian},
  doi          = {10.1162/coli_a_00510},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {475-534},
  shortjournal = {Comput. Lingu.},
  title        = {Context-aware transliteration of romanized south asian languages},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing the cross-linguistic utility of abstract meaning
representation. <em>COLI</em>, <em>50</em>(2), 419–473. (<a
href="https://doi.org/10.1162/coli_a_00503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic representations capture the meaning of a text. Abstract Meaning Representation (AMR), a type of semantic representation, focuses on predicate-argument structure and abstracts away from surface form. Though AMR was developed initially for English, it has now been adapted to a multitude of languages in the form of non-English annotation schemas, cross-lingual text-to-AMR parsing, and AMR-to-(non-English) text generation. We advance prior work on cross-lingual AMR by thoroughly investigating the amount, types, and causes of differences that appear in AMRs of different languages. Further, we compare how AMR captures meaning in cross-lingual pairs versus strings, and show that AMR graphs are able to draw out fine-grained differences between parallel sentences. We explore three primary research questions: (1) What are the types and causes of differences in parallel AMRs? (2) How can we measure the amount of difference between AMR pairs in different languages? (3) Given that AMR structure is affected by language and exhibits cross-lingual differences, how do cross-lingual AMR pairs compare to string-based representations of cross-lingual sentence pairs? We find that the source language itself does have a measurable impact on AMR structure, and that translation divergences and annotator choices also lead to differences in cross-lingual AMR pairs. We explore the implications of this finding throughout our study, concluding that, although AMR is useful to capture meaning across languages, evaluations need to take into account source language influences if they are to paint an accurate picture of system output, and meaning generally.},
  archive      = {J_COLI},
  author       = {Wein, Shira and Schneider, Nathan},
  doi          = {10.1162/coli_a_00503},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {419-473},
  shortjournal = {Comput. Lingu.},
  title        = {Assessing the cross-linguistic utility of abstract meaning representation},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Polysemy—evidence from linguistics, behavioral science, and
contextualized language models. <em>COLI</em>, <em>50</em>(1), 351–417.
(<a href="https://doi.org/10.1162/coli_a_00500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polysemy is the type of lexical ambiguity where a word has multiple distinct but related interpretations. In the past decade, it has been the subject of a great many studies across multiple disciplines including linguistics, psychology, neuroscience, and computational linguistics, which have made it increasingly clear that the complexity of polysemy precludes simple, universal answers, especially concerning the representation and processing of polysemous words. But fuelled by the growing availability of large, crowdsourced datasets providing substantial empirical evidence; improved behavioral methodology; and the development of contextualized language models capable of encoding the fine-grained meaning of a word within a given context, the literature on polysemy recently has developed more complex theoretical analyses. In this survey we discuss these recent contributions to the investigation of polysemy against the backdrop of a long legacy of research across multiple decades and disciplines. Our aim is to bring together different perspectives to achieve a more complete picture of the heterogeneity and complexity of the phenomenon of polysemy. Specifically, we highlight evidence supporting a range of hybrid models of the mental processing of polysemes. These hybrid models combine elements from different previous theoretical approaches to explain patterns and idiosyncrasies in the processing of polysemous that the best known models so far have failed to account for. Our literature review finds that (i) traditional analyses of polysemy can be limited in their generalizability by loose definitions and selective materials; (ii) linguistic tests provide useful evidence on individual cases, but fail to capture the full range of factors involved in the processing of polysemous sense extensions; and (iii) recent behavioral (psycho) linguistics studies, large-scale annotation efforts, and investigations leveraging contextualized language models provide accumulating evidence suggesting that polysemous sense similarity covers a wide spectrum between identity of sense and homonymy-like unrelatedness of meaning. We hope that the interdisciplinary account of polysemy provided in this survey inspires further fundamental research on the nature of polysemy and better equips applied research to deal with the complexity surrounding the phenomenon, for example, by enabling the development of benchmarks and testing paradigms for large language models informed by a greater portion of the rich evidence on the phenomenon currently available.},
  archive      = {J_COLI},
  author       = {Haber, Janosch and Poesio, Massimo},
  doi          = {10.1162/coli_a_00500},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {351-417},
  shortjournal = {Comput. Lingu.},
  title        = {Polysemy—Evidence from linguistics, behavioral science, and contextualized language models},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language model behavior: A comprehensive survey.
<em>COLI</em>, <em>50</em>(1), 293–350. (<a
href="https://doi.org/10.1162/coli_a_00492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.},
  archive      = {J_COLI},
  author       = {Chang, Tyler A. and Bergen, Benjamin K.},
  doi          = {10.1162/coli_a_00492},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {293-350},
  shortjournal = {Comput. Lingu.},
  title        = {Language model behavior: A comprehensive survey},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can large language models transform computational social
science? <em>COLI</em>, <em>50</em>(1), 237–291. (<a
href="https://doi.org/10.1162/coli_a_00502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot LLMs can also reliably classify and explain social phenomena like persuasiveness and political ideology, then LLMs could augment the computational social science (CSS) pipeline in important ways. This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, LLMs are posed to meaningfully participate in social science analysis in partnership with humans.},
  archive      = {J_COLI},
  author       = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  doi          = {10.1162/coli_a_00502},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {237-291},
  shortjournal = {Comput. Lingu.},
  title        = {Can large language models transform computational social science?},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stance detection with explanations. <em>COLI</em>,
<em>50</em>(1), 193–235. (<a
href="https://doi.org/10.1162/coli_a_00501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of stance has recently gained a lot of attention with the extreme growth of fake news and filter bubbles. Over the last decade, many feature-based and deep-learning approaches have been proposed to solve stance detection. However, almost none of the existing works focus on providing a meaningful explanation for their prediction. In this work, we study stance detection with an emphasis on generating explanations for the predicted stance by capturing the pivotal argumentative structure embedded in a document. We propose to build a stance tree that utilizes rhetorical parsing to construct an evidence tree and to use Dempster Shafer Theory to aggregate the evidence. Human studies show that our unsupervised technique of generating stance explanations outperforms the SOTA extractive summarization method in terms of informativeness, non-redundancy, coverage, and overall quality. Furthermore, experiments show that our explanation-based stance prediction excels or matches the performance of the SOTA model on various benchmark datasets.},
  archive      = {J_COLI},
  author       = {Saha, Rudra Ranajee and Lakshmanan, Laks V. S. and Ng, Raymond T.},
  doi          = {10.1162/coli_a_00501},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {193-235},
  shortjournal = {Comput. Lingu.},
  title        = {Stance detection with explanations},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the role of morphological information for contextual
lemmatization. <em>COLI</em>, <em>50</em>(1), 157–191. (<a
href="https://doi.org/10.1162/coli_a_00497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lemmatization is a natural language processing (NLP) task that consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this article we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish, and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, which constitutes, after all, their most common application use. The results of our study are rather surprising. It turns out that providing lemmatizers with fine-grained morphological features during training is not that beneficial, not even for agglutinative languages. In fact, modern contextual word representations seem to implicitly encode enough morphological information to obtain competitive contextual lemmatizers without seeing any explicit morphological signal. Moreover, our experiments suggest that the best lemmatizers out-of-domain are those using simple UPOS tags or those trained without morphology and, lastly, that current evaluation practices for lemmatization are not adequate to clearly discriminate between models.},
  archive      = {J_COLI},
  author       = {Toporkov, Olia and Agerri, Rodrigo},
  doi          = {10.1162/coli_a_00497},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {157-191},
  shortjournal = {Comput. Lingu.},
  title        = {On the role of morphological information for contextual lemmatization},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing semantic faithfulness of language models via input
intervention on question answering. <em>COLI</em>, <em>50</em>(1),
119–155. (<a href="https://doi.org/10.1162/coli_a_00493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based language models have been shown to be highly effective for several NLP tasks. In this article, we consider three transformer models, BERT, RoBERTa, and XLNet, in both small and large versions, and investigate how faithful their representations are with respect to the semantic content of texts. We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally figure in a model’s inferences in question answering. We then test this notion by observing a model’s behavior on answering questions about a story after performing two novel semantic interventions—deletion intervention and negation intervention. While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (∼ 50% for deletion intervention, and ∼ 20% drop in accuracy for negation intervention). We then propose an intervention-based training regime that can mitigate the undesirable effects for deletion intervention by a significant margin (from ∼ 50% to ∼ 6%). We analyze the inner-workings of the models to better understand the effectiveness of intervention-based training for deletion intervention. But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models’ inability to deal with negation intervention or to capture the predicate–argument structure of texts. We also test InstructGPT, via prompting, for its ability to handle the two interventions and to capture predicate–argument structure. While InstructGPT models do achieve very high performance on predicate–argument structure task, they fail to respond adequately to our deletion and negation interventions.},
  archive      = {J_COLI},
  author       = {Chaturvedi, Akshay and Bhar, Swarnadeep and Saha, Soumadeep and Garain, Utpal and Asher, Nicholas},
  doi          = {10.1162/coli_a_00493},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {119-155},
  shortjournal = {Comput. Lingu.},
  title        = {Analyzing semantic faithfulness of language models via input intervention on question answering},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal generation for optimality theory is
PSPACE-complete. <em>COLI</em>, <em>50</em>(1), 83–117. (<a
href="https://doi.org/10.1162/coli_a_00494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article shows that the universal generation problem for Optimality Theory (OT) is PSPACE-complete. While prior work has shown that universal generation is at least NP-hard and at most EXPSPACE-hard, our results place universal generation in between those two classes, assuming that NP ≠ PSPACE. We additionally show that when the number of constraints is bounded in advance, universal generation is at least NL-hard and at most NP NP -hard. Our proofs rely on a close connection between OT and the intersection non-emptiness problem for finite automata, which is PSPACE-complete in general and NL-complete when the number of automata is bounded. Our analysis shows that constraint interaction is the main contributor to the complexity of OT: The ability to factor transformations into simple, interacting constraints allows OT to furnish compact descriptions of intricate phonological phenomena.},
  archive      = {J_COLI},
  author       = {Hao, Sophie},
  doi          = {10.1162/coli_a_00494},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {83-117},
  shortjournal = {Comput. Lingu.},
  title        = {Universal generation for optimality theory is PSPACE-complete},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How is a “kitchen chair” like a “farm horse”? Exploring the
representation of noun-noun compound semantics in transformer-based
language models. <em>COLI</em>, <em>50</em>(1), 49–81. (<a
href="https://doi.org/10.1162/coli_a_00495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of Transformer-based language models in a wide variety of natural language processing tasks, our understanding of how these models process a given input in order to represent task-relevant information remains incomplete. In this work, we focus on semantic composition and examine how Transformer-based language models represent semantic information related to the meaning of English noun-noun compounds. We probe Transformer-based language models for their knowledge of the thematic relations that link the head nouns and modifier words of compounds (e.g., KITCHEN CHAIR : a chair located in a kitchen). Firstly, using a dataset featuring groups of compounds with shared lexical or semantic features, we find that token representations of six Transformer-based language models distinguish between pairs of compounds based on whether they use the same thematic relation. Secondly, we utilize fine-grained vector representations of compound semantics derived from human annotations, and find that token vectors from several models elicit a strong signal of the semantic relations used in the compounds. In a novel “compositional probe” setting, where we compare the semantic relation signal in mean-pooled token vectors of compounds to mean-pooled token vectors when the two constituent words appear in separate sentences, we find that the Transformer-based language models that best represent the semantics of noun-noun compounds also do so substantially better than in the control condition where the two constituent works are processed separately. Overall, our results shed light on the ability of Transformer-based language models to support compositional semantic processes in representing the meaning of noun-noun compounds.},
  archive      = {J_COLI},
  author       = {Ormerod, Mark and del Rincón, Jesús Martínez and Devereux, Barry},
  doi          = {10.1162/coli_a_00495},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {49-81},
  shortjournal = {Comput. Lingu.},
  title        = {How is a “Kitchen chair” like a “Farm horse”? exploring the representation of noun-noun compound semantics in transformer-based language models},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking the exploitation of monolingual data for
low-resource neural machine translation. <em>COLI</em>, <em>50</em>(1),
25–47. (<a href="https://doi.org/10.1162/coli_a_00496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of monolingual data has been shown to be a promising strategy for addressing low-resource machine translation problems. Previous studies have demonstrated the effectiveness of techniques such as back-translation and self-supervised objectives, including masked language modeling, causal language modeling, and denoise autoencoding, in improving the performance of machine translation models. However, the manner in which these methods contribute to the success of machine translation tasks and how they can be effectively combined remains an under-researched area. In this study, we carry out a systematic investigation of the effects of these techniques on linguistic properties through the use of probing tasks, including source language comprehension, bilingual word alignment, and translation fluency. We further evaluate the impact of pre-training, back-translation, and multi-task learning on bitexts of varying sizes. Our findings inform the design of more effective pipelines for leveraging monolingual data in extremely low-resource and low-resource machine translation tasks. Experiment results show consistent performance gains in seven translation directions, which provide further support for our conclusions and understanding of the role of monolingual data in machine translation.},
  archive      = {J_COLI},
  author       = {Pang, Jianhui and Yang*, Baosong and Wong*, Derek Fai and Wan, Yu and Liu, Dayiheng and Chao, Lidia Sam and Xie, Jun},
  doi          = {10.1162/coli_a_00496},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {25-47},
  shortjournal = {Comput. Lingu.},
  title        = {Rethinking the exploitation of monolingual data for low-resource neural machine translation},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). My big, fat 50-year journey. <em>COLI</em>, <em>50</em>(1),
1–24. (<a href="https://doi.org/10.1162/coli_a_00499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {My most heartfelt thanks to ACL for this tremendous honor. I’m completely thrilled. I cannot tell you how surprised I was when I got Iryna’s email. It is amazing that my first ACL conference since 2019 in Florence includes this award. What a wonderful way to be back with all of my friends and family here at ACL. I’m going to tell you about my big fat 50-year journey. What have I been doing for the last 50 years? Well, finding meaning, quite literally in words. Or in other words, exploring how computational lexical semantics can support natural language understanding. This is going to be quick. Hold onto your hats, here we go.},
  archive      = {J_COLI},
  author       = {Palmer, Martha},
  doi          = {10.1162/coli_a_00499},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Comput. Lingu.},
  title        = {My big, fat 50-year journey},
  volume       = {50},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
